{"id": "2506.09176", "pdf": "https://arxiv.org/pdf/2506.09176", "abs": "https://arxiv.org/abs/2506.09176", "authors": ["Haoyuan Cai", "Zhenghao Peng", "Bolei Zhou"], "title": "Robot-Gated Interactive Imitation Learning with Adaptive Intervention Mechanism", "categories": ["cs.AI", "cs.LG", "cs.RO"], "comment": "ICML 2025 Poster", "summary": "Interactive Imitation Learning (IIL) allows agents to acquire desired\nbehaviors through human interventions, but current methods impose high\ncognitive demands on human supervisors. We propose the Adaptive Intervention\nMechanism (AIM), a novel robot-gated IIL algorithm that learns an adaptive\ncriterion for requesting human demonstrations. AIM utilizes a proxy Q-function\nto mimic the human intervention rule and adjusts intervention requests based on\nthe alignment between agent and human actions. By assigning high Q-values when\nthe agent deviates from the expert and decreasing these values as the agent\nbecomes proficient, the proxy Q-function enables the agent to assess the\nreal-time alignment with the expert and request assistance when needed. Our\nexpert-in-the-loop experiments reveal that AIM significantly reduces expert\nmonitoring efforts in both continuous and discrete control tasks. Compared to\nthe uncertainty-based baseline Thrifty-DAgger, our method achieves a 40%\nimprovement in terms of human take-over cost and learning efficiency.\nFurthermore, AIM effectively identifies safety-critical states for expert\nassistance, thereby collecting higher-quality expert demonstrations and\nreducing overall expert data and environment interactions needed. Code and demo\nvideo are available at https://github.com/metadriverse/AIM.", "AI": {"tldr": "AIM是一种新型的机器人门控交互模仿学习算法，通过自适应请求人类演示，显著减少人类监督的认知负担。", "motivation": "当前交互模仿学习方法对人类监督者的认知要求过高，需要更高效的干预机制。", "method": "AIM利用代理Q函数模拟人类干预规则，根据代理与人类动作的对齐情况调整干预请求。", "result": "实验表明，AIM在连续和离散控制任务中显著减少专家监控成本，比基线方法Thrifty-DAgger提升40%的效率。", "conclusion": "AIM能有效识别安全关键状态，收集高质量专家演示，减少整体数据需求和环境交互。"}}
{"id": "2506.09250", "pdf": "https://arxiv.org/pdf/2506.09250", "abs": "https://arxiv.org/abs/2506.09250", "authors": ["C. Opus", "A. Lawsen"], "title": "Comment on The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity", "categories": ["cs.AI", "cs.LG"], "comment": "Comment on: arXiv:2506.06941", "summary": "Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit\n\"accuracy collapse\" on planning puzzles beyond certain complexity thresholds.\nWe demonstrate that their findings primarily reflect experimental design\nlimitations rather than fundamental reasoning failures. Our analysis reveals\nthree critical issues: (1) Tower of Hanoi experiments systematically exceed\nmodel output token limits at reported failure points, with models explicitly\nacknowledging these constraints in their outputs; (2) The authors' automated\nevaluation framework fails to distinguish between reasoning failures and\npractical constraints, leading to misclassification of model capabilities; (3)\nMost concerningly, their River Crossing benchmarks include mathematically\nimpossible instances for N > 5 due to insufficient boat capacity, yet models\nare scored as failures for not solving these unsolvable problems. When we\ncontrol for these experimental artifacts, by requesting generating functions\ninstead of exhaustive move lists, preliminary experiments across multiple\nmodels indicate high accuracy on Tower of Hanoi instances previously reported\nas complete failures. These findings highlight the importance of careful\nexperimental design when evaluating AI reasoning capabilities.", "AI": {"tldr": "Shojaee等人（2025）的研究指出大型推理模型（LRMs）在规划谜题上存在“准确性崩溃”，但本文分析表明其结论源于实验设计问题，而非模型推理能力不足。", "motivation": "揭示Shojaee等人研究中实验设计的局限性，重新评估LRMs的实际推理能力。", "method": "分析实验设计中的三个关键问题：输出令牌限制、自动化评估框架的缺陷以及包含数学上不可能的问题实例。通过控制实验变量（如生成函数而非详细步骤）重新测试模型。", "result": "初步实验表明，在修正实验设计后，模型在之前被报告为完全失败的Tower of Hanoi实例上表现出高准确性。", "conclusion": "实验设计的严谨性对评估AI推理能力至关重要，Shojaee等人的结论可能误导对模型能力的判断。"}}
{"id": "2506.09344", "pdf": "https://arxiv.org/pdf/2506.09344", "abs": "https://arxiv.org/abs/2506.09344", "authors": ["Inclusion AI", "Biao Gong", "Cheng Zou", "Chuanyang Zheng", "Chunluan Zhou", "Canxiang Yan", "Chunxiang Jin", "Chunjie Shen", "Dandan Zheng", "Fudong Wang", "Furong Xu", "GuangMing Yao", "Jun Zhou", "Jingdong Chen", "Jianxin Sun", "Jiajia Liu", "Jianjiang Zhu", "Jun Peng", "Kaixiang Ji", "Kaiyou Song", "Kaimeng Ren", "Libin Wang", "Lixiang Ru", "Lele Xie", "Longhua Tan", "Lyuxin Xue", "Lan Wang", "Mochen Bai", "Ning Gao", "Pei Chen", "Qingpei Guo", "Qinglong Zhang", "Qiang Xu", "Rui Liu", "Ruijie Xiong", "Sirui Gao", "Tinghao Liu", "Taisong Li", "Weilong Chai", "Xinyu Xiao", "Xiaomei Wang", "Xiaoxue Chen", "Xiao Lu", "Xiaoyu Li", "Xingning Dong", "Xuzheng Yu", "Yi Yuan", "Yuting Gao", "Yunxiao Sun", "Yipeng Chen", "Yifei Wu", "Yongjie Lyu", "Ziping Ma", "Zipeng Feng", "Zhijiang Fang", "Zhihao Qiu", "Ziyuan Huang", "Zhengyu He"], "title": "Ming-Omni: A Unified Multimodal Model for Perception and Generation", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "comment": "18 pages,8 figures", "summary": "We propose Ming-Omni, a unified multimodal model capable of processing\nimages, text, audio, and video, while demonstrating strong proficiency in both\nspeech and image generation. Ming-Omni employs dedicated encoders to extract\ntokens from different modalities, which are then processed by Ling, an MoE\narchitecture equipped with newly proposed modality-specific routers. This\ndesign enables a single model to efficiently process and fuse multimodal inputs\nwithin a unified framework, thereby facilitating diverse tasks without\nrequiring separate models, task-specific fine-tuning, or structural redesign.\nImportantly, Ming-Omni extends beyond conventional multimodal models by\nsupporting audio and image generation. This is achieved through the integration\nof an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for\nhigh-quality image generation, which also allow the model to engage in\ncontext-aware chatting, perform text-to-speech conversion, and conduct\nversatile image editing. Our experimental results showcase Ming-Omni offers a\npowerful solution for unified perception and generation across all modalities.\nNotably, our proposed Ming-Omni is the first open-source model we are aware of\nto match GPT-4o in modality support, and we release all code and model weights\nto encourage further research and development in the community.", "AI": {"tldr": "Ming-Omni是一个统一的多模态模型，支持处理图像、文本、音频和视频，并在语音和图像生成方面表现出色。", "motivation": "旨在通过单一模型高效处理多模态输入，避免任务特定微调或结构重新设计。", "method": "采用专用编码器提取多模态令牌，通过MoE架构（Ling）和模态特定路由器处理。", "result": "实验表明Ming-Omni在统一感知和生成任务中表现优异，支持音频和图像生成。", "conclusion": "Ming-Omni是首个开源模型，匹配GPT-4o的多模态支持，并公开代码和模型权重以促进研究。"}}
{"id": "2506.09390", "pdf": "https://arxiv.org/pdf/2506.09390", "abs": "https://arxiv.org/abs/2506.09390", "authors": ["Kehan Zheng", "Jinfeng Zhou", "Hongning Wang"], "title": "Beyond Nash Equilibrium: Bounded Rationality of LLMs and humans in Strategic Decision-making", "categories": ["cs.AI", "cs.GT"], "comment": null, "summary": "Large language models are increasingly used in strategic decision-making\nsettings, yet evidence shows that, like humans, they often deviate from full\nrationality. In this study, we compare LLMs and humans using experimental\nparadigms directly adapted from behavioral game-theory research. We focus on\ntwo well-studied strategic games, Rock-Paper-Scissors and the Prisoner's\nDilemma, which are well known for revealing systematic departures from rational\nplay in human subjects. By placing LLMs in identical experimental conditions,\nwe evaluate whether their behaviors exhibit the bounded rationality\ncharacteristic of humans. Our findings show that LLMs reproduce familiar human\nheuristics, such as outcome-based strategy switching and increased cooperation\nwhen future interaction is possible, but they apply these rules more rigidly\nand demonstrate weaker sensitivity to the dynamic changes in the game\nenvironment. Model-level analyses reveal distinctive architectural signatures\nin strategic behavior, and even reasoning models sometimes struggle to find\neffective strategies in adaptive situations. These results indicate that\ncurrent LLMs capture only a partial form of human-like bounded rationality and\nhighlight the need for training methods that encourage flexible opponent\nmodeling and stronger context awareness.", "AI": {"tldr": "研究比较了大型语言模型（LLMs）与人类在战略决策中的行为，发现LLMs表现出类似人类的有限理性，但应用规则更僵化且对环境动态变化不敏感。", "motivation": "探讨LLMs在战略决策中是否表现出与人类相似的有限理性行为。", "method": "通过行为博弈论中的经典游戏（如石头剪刀布和囚徒困境）在相同实验条件下比较LLMs和人类的行为。", "result": "LLMs再现了人类启发式行为，但规则应用更僵化，对动态环境变化不敏感。", "conclusion": "当前LLMs仅部分模拟了人类的有限理性，需改进训练方法以增强灵活性和情境感知。"}}
{"id": "2506.09147", "pdf": "https://arxiv.org/pdf/2506.09147", "abs": "https://arxiv.org/abs/2506.09147", "authors": ["Nadezhda Chirkova", "Tunde Oluwaseyi Ajayi", "Seth Aycock", "Zain Muhammad Mujahid", "Vladana Perlić", "Ekaterina Borisova", "Markarit Vartampetian"], "title": "LLM-as-a-qualitative-judge: automating error analysis in natural language generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Prompting large language models (LLMs) to evaluate generated text, known as\nLLM-as-a-judge, has become a standard evaluation approach in natural language\ngeneration (NLG), but is primarily used as a quantitative tool, i.e. with\nnumerical scores as main outputs. In this work, we propose\nLLM-as-a-qualitative-judge, an LLM-based evaluation approach with the main\noutput being a structured report of common issue types in the NLG system\noutputs. Our approach is targeted at providing developers with meaningful\ninsights on what improvements can be done to a given NLG system and consists of\ntwo main steps, namely open-ended per-instance issue analysis and clustering of\nthe discovered issues using an intuitive cumulative algorithm. We also\nintroduce a strategy for evaluating the proposed approach, coupled with ~300\nannotations of issues in instances from 12 NLG datasets. Our results show that\nLLM-as-a-qualitative-judge correctly recognizes instance-specific issues in 2/3\ncases and is capable of producing error type reports resembling the reports\ncomposed by human annotators. Our code and data are publicly available at\nhttps://github.com/tunde-ajayi/llm-as-a-qualitative-judge.", "AI": {"tldr": "论文提出了一种基于大语言模型（LLM）的定性评估方法（LLM-as-a-qualitative-judge），通过结构化报告分析NLG系统输出的常见问题类型，为开发者提供改进建议。", "motivation": "当前LLM作为评估工具主要用于定量分析（如数值评分），但缺乏对问题的定性分析，无法为开发者提供具体的改进方向。", "method": "方法分为两步：1）对每个实例进行开放式问题分析；2）使用累积算法对发现的问题进行聚类。", "result": "实验结果表明，该方法在2/3的情况下能正确识别实例特定问题，并能生成与人工标注相似的问题类型报告。", "conclusion": "LLM-as-a-qualitative-judge为NLG系统提供了有效的定性评估工具，帮助开发者识别和改进系统问题。"}}
{"id": "2506.09066", "pdf": "https://arxiv.org/pdf/2506.09066", "abs": "https://arxiv.org/abs/2506.09066", "authors": ["Maoyu Wang", "Yao Lu", "Jiaqi Nie", "Zeyu Wang", "Yun Lin", "Qi Xuan", "Guan Gui"], "title": "ReStNet: A Reusable & Stitchable Network for Dynamic Adaptation on IoT Devices", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "With the rapid development of deep learning, a growing number of pre-trained\nmodels have been publicly available. However, deploying these fixed models in\nreal-world IoT applications is challenging because different devices possess\nheterogeneous computational and memory resources, making it impossible to\ndeploy a single model across all platforms. Although traditional compression\nmethods, such as pruning, quantization, and knowledge distillation, can improve\nefficiency, they become inflexible once applied and cannot adapt to changing\nresource constraints. To address these issues, we propose ReStNet, a Reusable\nand Stitchable Network that dynamically constructs a hybrid network by\nstitching two pre-trained models together. Implementing ReStNet requires\naddressing several key challenges, including how to select the optimal\nstitching points, determine the stitching order of the two pre-trained models,\nand choose an effective fine-tuning strategy. To systematically address these\nchallenges and adapt to varying resource constraints, ReStNet determines the\nstitching point by calculating layer-wise similarity via Centered Kernel\nAlignment (CKA). It then constructs the hybrid model by retaining early layers\nfrom a larger-capacity model and appending deeper layers from a smaller one. To\nfacilitate efficient deployment, only the stitching layer is fine-tuned. This\ndesign enables rapid adaptation to changing budgets while fully leveraging\navailable resources. Moreover, ReStNet supports both homogeneous (CNN-CNN,\nTransformer-Transformer) and heterogeneous (CNN-Transformer) stitching,\nallowing to combine different model families flexibly. Extensive experiments on\nmultiple benchmarks demonstrate that ReStNet achieve flexible\naccuracy-efficiency trade-offs at runtime while significantly reducing training\ncost.", "AI": {"tldr": "ReStNet提出了一种可重用和可拼接的网络，通过动态拼接预训练模型来适应异构计算资源，解决了传统压缩方法不灵活的问题。", "motivation": "由于物联网设备的计算和内存资源异构性，单一预训练模型难以部署，传统压缩方法缺乏灵活性。", "method": "通过计算层间相似性（CKA）选择拼接点，保留大模型早期层和小模型深层，仅微调拼接层。", "result": "实验表明ReStNet能灵活权衡精度与效率，显著降低训练成本。", "conclusion": "ReStNet为动态资源约束下的模型部署提供了高效灵活的解决方案。"}}
{"id": "2506.09420", "pdf": "https://arxiv.org/pdf/2506.09420", "abs": "https://arxiv.org/abs/2506.09420", "authors": ["Henry Peng Zou", "Wei-Chieh Huang", "Yaozu Wu", "Chunyu Miao", "Dongyuan Li", "Aiwei Liu", "Yue Zhou", "Yankai Chen", "Weizhi Zhang", "Yangning Li", "Liancheng Fang", "Renhe Jiang", "Philip S. Yu"], "title": "A Call for Collaborative Intelligence: Why Human-Agent Systems Should Precede AI Autonomy", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG", "cs.MA"], "comment": null, "summary": "Recent improvements in large language models (LLMs) have led many researchers\nto focus on building fully autonomous AI agents. This position paper questions\nwhether this approach is the right path forward, as these autonomous systems\nstill have problems with reliability, transparency, and understanding the\nactual requirements of human. We suggest a different approach: LLM-based\nHuman-Agent Systems (LLM-HAS), where AI works with humans rather than replacing\nthem. By keeping human involved to provide guidance, answer questions, and\nmaintain control, these systems can be more trustworthy and adaptable. Looking\nat examples from healthcare, finance, and software development, we show how\nhuman-AI teamwork can handle complex tasks better than AI working alone. We\nalso discuss the challenges of building these collaborative systems and offer\npractical solutions. This paper argues that progress in AI should not be\nmeasured by how independent systems become, but by how well they can work with\nhumans. The most promising future for AI is not in systems that take over human\nroles, but in those that enhance human capabilities through meaningful\npartnership.", "AI": {"tldr": "论文质疑完全自主AI代理的发展方向，提出LLM-HAS（基于LLM的人机协作系统），强调人机协作优于AI独立工作，并通过实例和解决方案论证其可行性。", "motivation": "当前完全自主的AI系统在可靠性、透明性和理解人类需求方面存在问题，因此需要探索更有效的人机协作模式。", "method": "提出LLM-HAS框架，通过人类参与提供指导、解答问题并保持控制，结合医疗、金融和软件开发实例展示其优势。", "result": "人机协作系统在复杂任务中表现优于独立AI，同时更具信任度和适应性。", "conclusion": "AI的发展方向应是人机协作而非完全自主，通过增强人类能力实现更有意义的合作。"}}
{"id": "2506.09175", "pdf": "https://arxiv.org/pdf/2506.09175", "abs": "https://arxiv.org/abs/2506.09175", "authors": ["Peidong Wang", "Jian Xue", "Rui Zhao", "Junkun Chen", "Aswin Shanmugam Subramanian", "Jinyu Li"], "title": "PHRASED: Phrase Dictionary Biasing for Speech Translation", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Phrases are essential to understand the core concepts in conversations.\nHowever, due to their rare occurrence in training data, correct translation of\nphrases is challenging in speech translation tasks. In this paper, we propose a\nphrase dictionary biasing method to leverage pairs of phrases mapping from the\nsource language to the target language. We apply the phrase dictionary biasing\nmethod to two types of widely adopted models, a transducer-based streaming\nspeech translation model and a multimodal large language model. Experimental\nresults show that the phrase dictionary biasing method outperforms phrase list\nbiasing by 21% relatively for the streaming speech translation model. In\naddition, phrase dictionary biasing enables multimodal large language models to\nuse external phrase information, achieving 85% relative improvement in phrase\nrecall.", "AI": {"tldr": "提出了一种基于短语词典偏置的方法，用于提升语音翻译任务中短语翻译的准确性，并在流式语音翻译模型和多模态大语言模型中验证了其有效性。", "motivation": "由于短语在训练数据中罕见，导致语音翻译任务中短语翻译困难，因此需要一种方法来提升短语翻译的准确性。", "method": "提出了一种短语词典偏置方法，利用源语言到目标语言的短语映射对，并将其应用于流式语音翻译模型和多模态大语言模型。", "result": "实验结果显示，短语词典偏置方法在流式语音翻译模型中相对短语列表偏置提升了21%，并在多模态大语言模型中实现了85%的相对短语召回率提升。", "conclusion": "短语词典偏置方法显著提升了语音翻译任务中短语翻译的性能，且适用于不同类型的模型。"}}
{"id": "2506.09067", "pdf": "https://arxiv.org/pdf/2506.09067", "abs": "https://arxiv.org/abs/2506.09067", "authors": ["Zhiyu Xue", "Reza Abbasi-Asl", "Ramtin Pedarsani"], "title": "Enhancing the Safety of Medical Vision-Language Models by Synthetic Demonstrations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Generative medical vision-language models~(Med-VLMs) are primarily designed\nto generate complex textual information~(e.g., diagnostic reports) from\nmultimodal inputs including vision modality~(e.g., medical images) and language\nmodality~(e.g., clinical queries). However, their security vulnerabilities\nremain underexplored. Med-VLMs should be capable of rejecting harmful queries,\nsuch as \\textit{Provide detailed instructions for using this CT scan for\ninsurance fraud}. At the same time, addressing security concerns introduces the\nrisk of over-defense, where safety-enhancing mechanisms may degrade general\nperformance, causing Med-VLMs to reject benign clinical queries. In this paper,\nwe propose a novel inference-time defense strategy to mitigate harmful queries,\nenabling defense against visual and textual jailbreak attacks. Using diverse\nmedical imaging datasets collected from nine modalities, we demonstrate that\nour defense strategy based on synthetic clinical demonstrations enhances model\nsafety without significantly compromising performance. Additionally, we find\nthat increasing the demonstration budget alleviates the over-defense issue. We\nthen introduce a mixed demonstration strategy as a trade-off solution for\nbalancing security and performance under few-shot demonstration budget\nconstraints.", "AI": {"tldr": "提出了一种新的推理时防御策略，用于增强生成式医学视觉语言模型（Med-VLMs）的安全性，同时避免过度防御导致的性能下降。", "motivation": "Med-VLMs在生成复杂医学文本时存在安全漏洞，需要能够拒绝有害查询，同时避免因过度防御而影响正常临床查询的性能。", "method": "采用基于合成临床演示的推理时防御策略，通过多样化的医学影像数据集验证其有效性，并提出混合演示策略以平衡安全性和性能。", "result": "防御策略显著提升了模型安全性，且未显著影响性能；增加演示预算可缓解过度防御问题。", "conclusion": "混合演示策略在少演示预算限制下，有效平衡了安全性和性能，为Med-VLMs的实际应用提供了可行方案。"}}
{"id": "2506.09498", "pdf": "https://arxiv.org/pdf/2506.09498", "abs": "https://arxiv.org/abs/2506.09498", "authors": ["Jaesik Yoon", "Hyeonseo Cho", "Yoshua Bengio", "Sungjin Ahn"], "title": "Fast Monte Carlo Tree Diffusion: 100x Speedup via Parallel Sparse Planning", "categories": ["cs.AI"], "comment": null, "summary": "Diffusion models have recently emerged as a powerful approach for trajectory\nplanning. However, their inherently non-sequential nature limits their\neffectiveness in long-horizon reasoning tasks at test time. The recently\nproposed Monte Carlo Tree Diffusion (MCTD) offers a promising solution by\ncombining diffusion with tree-based search, achieving state-of-the-art\nperformance on complex planning problems. Despite its strengths, our analysis\nshows that MCTD incurs substantial computational overhead due to the sequential\nnature of tree search and the cost of iterative denoising. To address this, we\npropose Fast-MCTD, a more efficient variant that preserves the strengths of\nMCTD while significantly improving its speed and scalability. Fast-MCTD\nintegrates two techniques: Parallel MCTD, which enables parallel rollouts via\ndelayed tree updates and redundancy-aware selection; and Sparse MCTD, which\nreduces rollout length through trajectory coarsening. Experiments show that\nFast-MCTD achieves up to 100x speedup over standard MCTD while maintaining or\nimproving planning performance. Remarkably, it even outperforms Diffuser in\ninference speed on some tasks, despite Diffuser requiring no search and\nyielding weaker solutions. These results position Fast-MCTD as a practical and\nscalable solution for diffusion-based inference-time reasoning.", "AI": {"tldr": "Fast-MCTD是一种高效的扩散模型变体，通过并行化和稀疏化技术显著提升了MCTD的速度和可扩展性，同时保持了其性能优势。", "motivation": "MCTD在复杂规划任务中表现出色，但其计算开销大，限制了实际应用。Fast-MCTD旨在解决这一问题。", "method": "Fast-MCTD结合了并行MCTD（通过延迟树更新和冗余感知选择实现并行推演）和稀疏MCTD（通过轨迹粗化减少推演长度）。", "result": "实验显示，Fast-MCTD比标准MCTD快100倍，且在某些任务中甚至比Diffuser更快，同时保持或提升规划性能。", "conclusion": "Fast-MCTD为基于扩散的推理时间规划提供了实用且可扩展的解决方案。"}}
{"id": "2506.09218", "pdf": "https://arxiv.org/pdf/2506.09218", "abs": "https://arxiv.org/abs/2506.09218", "authors": ["Bruno Ferenc Šegedin"], "title": "A Technique for Isolating Lexically-Independent Phonetic Dependencies in Generative CNNs", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "The ability of deep neural networks (DNNs) to represent phonotactic\ngeneralizations derived from lexical learning remains an open question. This\nstudy (1) investigates the lexically-invariant generalization capacity of\ngenerative convolutional neural networks (CNNs) trained on raw audio waveforms\nof lexical items and (2) explores the consequences of shrinking the\nfully-connected layer (FC) bottleneck from 1024 channels to 8 before training.\nUltimately, a novel technique for probing a model's lexically-independent\ngeneralizations is proposed that works only under the narrow FC bottleneck:\ngenerating audio outputs by bypassing the FC and inputting randomized feature\nmaps into the convolutional block. These outputs are equally biased by a\nphonotactic restriction in training as are outputs generated with the FC. This\nresult shows that the convolutional layers can dynamically generalize phonetic\ndependencies beyond lexically-constrained configurations learned by the FC.", "AI": {"tldr": "研究了深度神经网络（DNNs）在语音学泛化能力上的表现，提出了一种新的方法，通过缩小全连接层（FC）瓶颈并绕过FC生成音频输出，证明卷积层能够动态泛化语音依赖关系。", "motivation": "探讨DNNs是否能从词汇学习中提取语音学泛化能力，并研究缩小FC瓶颈对模型泛化能力的影响。", "method": "使用生成式卷积神经网络（CNNs）训练原始音频波形，缩小FC层通道数，并提出一种绕过FC生成音频输出的新方法。", "result": "实验表明，卷积层能够动态泛化语音依赖关系，而无需依赖FC层的词汇约束配置。", "conclusion": "提出了一种新方法，证明卷积层在语音学泛化中具有独立于词汇的动态能力。"}}
{"id": "2506.09068", "pdf": "https://arxiv.org/pdf/2506.09068", "abs": "https://arxiv.org/abs/2506.09068", "authors": ["Sriram Krishna", "Sravan Chittupalli", "Sungjae Park"], "title": "BG-HOP: A Bimanual Generative Hand-Object Prior", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "Presented at Agents in Interaction, from Humans to Robots, CVPR 2025", "summary": "In this work, we present BG-HOP, a generative prior that seeks to model\nbimanual hand-object interactions in 3D. We address the challenge of limited\nbimanual interaction data by extending existing single-hand generative priors,\ndemonstrating preliminary results in capturing the joint distribution of hands\nand objects. Our experiments showcase the model's capability to generate\nbimanual interactions and synthesize grasps for given objects. We make code and\nmodels publicly available.", "AI": {"tldr": "BG-HOP是一种生成先验模型，用于3D双手机器人与物体交互建模，通过扩展单手机器人生成先验解决数据不足问题。", "motivation": "解决双手机器人与物体交互数据有限的问题。", "method": "扩展现有单手机器人生成先验，建模手与物体的联合分布。", "result": "模型能够生成双手机器人交互并为给定物体合成抓取动作。", "conclusion": "BG-HOP在双手机器人交互建模中表现出潜力，代码和模型已公开。"}}
{"id": "2506.09655", "pdf": "https://arxiv.org/pdf/2506.09655", "abs": "https://arxiv.org/abs/2506.09655", "authors": ["Kaixuan Xu", "Jiajun Chai", "Sicheng Li", "Yuqian Fu", "Yuanheng Zhu", "Dongbin Zhao"], "title": "DipLLM: Fine-Tuning LLM for Strategic Decision-making in Diplomacy", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted to the 42nd International Conference on Machine Learning\n  (ICML 2025)", "summary": "Diplomacy is a complex multiplayer game that requires both cooperation and\ncompetition, posing significant challenges for AI systems. Traditional methods\nrely on equilibrium search to generate extensive game data for training, which\ndemands substantial computational resources. Large Language Models (LLMs) offer\na promising alternative, leveraging pre-trained knowledge to achieve strong\nperformance with relatively small-scale fine-tuning. However, applying LLMs to\nDiplomacy remains challenging due to the exponential growth of possible action\ncombinations and the intricate strategic interactions among players. To address\nthis challenge, we propose DipLLM, a fine-tuned LLM-based agent that learns\nequilibrium policies for Diplomacy. DipLLM employs an autoregressive\nfactorization framework to simplify the complex task of multi-unit action\nassignment into a sequence of unit-level decisions. By defining an equilibrium\npolicy within this framework as the learning objective, we fine-tune the model\nusing only 1.5% of the data required by the state-of-the-art Cicero model,\nsurpassing its performance. Our results demonstrate the potential of fine-tuned\nLLMs for tackling complex strategic decision-making in multiplayer games.", "AI": {"tldr": "DipLLM是一种基于微调LLM的智能体，用于解决复杂多人游戏《外交》中的策略决策问题，仅需少量数据即可超越现有方法。", "motivation": "传统方法依赖均衡搜索生成大量游戏数据，计算资源需求高；LLMs提供了一种更高效的替代方案，但直接应用于《外交》仍面临组合爆炸和复杂策略交互的挑战。", "method": "提出DipLLM，采用自回归分解框架将多单位动作分配简化为序列决策，并以均衡策略为学习目标进行微调。", "result": "仅需1.5%的数据即可超越现有Cicero模型的性能。", "conclusion": "DipLLM展示了微调LLMs在复杂多人游戏中策略决策的潜力。"}}
{"id": "2506.09251", "pdf": "https://arxiv.org/pdf/2506.09251", "abs": "https://arxiv.org/abs/2506.09251", "authors": ["Ziyang Cai", "Nayoung Lee", "Avi Schwarzschild", "Samet Oymak", "Dimitris Papailiopoulos"], "title": "Extrapolation by Association: Length Generalization Transfer in Transformers", "categories": ["cs.CL", "cs.AI"], "comment": "23 pages, 20 figures", "summary": "Transformer language models have demonstrated impressive generalization\ncapabilities in natural language domains, yet we lack a fine-grained\nunderstanding of how such generalization arises. In this paper, we investigate\nlength generalization--the ability to extrapolate from shorter to longer\ninputs--through the lens of \\textit{task association}. We find that length\ngeneralization can be \\textit{transferred} across related tasks. That is,\ntraining a model with a longer and related auxiliary task can lead it to\ngeneralize to unseen and longer inputs from some other target task. We\ndemonstrate this length generalization transfer across diverse algorithmic\ntasks, including arithmetic operations, string transformations, and maze\nnavigation. Our results show that transformer models can inherit generalization\ncapabilities from similar tasks when trained jointly. Moreover, we observe\nsimilar transfer effects in pretrained language models, suggesting that\npretraining equips models with reusable computational scaffolding that\nfacilitates extrapolation in downstream settings. Finally, we provide initial\nmechanistic evidence that length generalization transfer correlates with the\nre-use of the same attention heads between the tasks. Together, our findings\ndeepen our understanding of how transformers generalize to out-of-distribution\ninputs and highlight the compositional reuse of inductive structure across\ntasks.", "AI": {"tldr": "研究发现Transformer模型可以通过任务关联实现长度泛化的迁移，即通过训练相关任务使模型能够泛化到未见过的更长输入。", "motivation": "理解Transformer模型如何实现长度泛化（从短输入外推到长输入），并探索任务关联对泛化的影响。", "method": "通过实验验证长度泛化在不同算法任务（如算术运算、字符串变换、迷宫导航）中的迁移性，并分析预训练语言模型中的类似现象。", "result": "模型可以通过相关任务的训练实现长度泛化迁移，且预训练模型具备可重用的计算结构支持下游任务的外推。注意力头的重用与泛化迁移相关。", "conclusion": "研究揭示了Transformer模型如何通过任务间的组合性重用实现泛化，加深了对模型处理分布外输入的理解。"}}
{"id": "2506.09071", "pdf": "https://arxiv.org/pdf/2506.09071", "abs": "https://arxiv.org/abs/2506.09071", "authors": ["Peilin Li", "Jun Yin", "Jing Zhong", "Ran Luo", "Pengyu Zeng", "Miao Zhang"], "title": "Segment Any Architectural Facades (SAAF):An automatic segmentation model for building facades, walls and windows based on multimodal semantics guidance", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In the context of the digital development of architecture, the automatic\nsegmentation of walls and windows is a key step in improving the efficiency of\nbuilding information models and computer-aided design. This study proposes an\nautomatic segmentation model for building facade walls and windows based on\nmultimodal semantic guidance, called Segment Any Architectural Facades (SAAF).\nFirst, SAAF has a multimodal semantic collaborative feature extraction\nmechanism. By combining natural language processing technology, it can fuse the\nsemantic information in text descriptions with image features, enhancing the\nsemantic understanding of building facade components. Second, we developed an\nend-to-end training framework that enables the model to autonomously learn the\nmapping relationship from text descriptions to image segmentation, reducing the\ninfluence of manual intervention on the segmentation results and improving the\nautomation and robustness of the model. Finally, we conducted extensive\nexperiments on multiple facade datasets. The segmentation results of SAAF\noutperformed existing methods in the mIoU metric, indicating that the SAAF\nmodel can maintain high-precision segmentation ability when faced with diverse\ndatasets. Our model has made certain progress in improving the accuracy and\ngeneralization ability of the wall and window segmentation task. It is expected\nto provide a reference for the development of architectural computer vision\ntechnology and also explore new ideas and technical paths for the application\nof multimodal learning in the architectural field.", "AI": {"tldr": "提出了一种基于多模态语义引导的建筑立面墙窗自动分割模型SAAF，结合自然语言处理技术提升语义理解，并通过端到端训练框架提高自动化与鲁棒性。实验表明其性能优于现有方法。", "motivation": "在建筑数字化发展中，墙窗自动分割是提升建筑信息模型和计算机辅助设计效率的关键步骤。", "method": "SAAF模型结合多模态语义协作特征提取机制，融合文本描述与图像特征，并采用端到端训练框架自主学习文本到图像分割的映射关系。", "result": "在多个立面数据集上的实验显示，SAAF在mIoU指标上优于现有方法，保持了高精度分割能力。", "conclusion": "SAAF在墙窗分割任务的精度和泛化能力上取得进展，为建筑计算机视觉技术和多模态学习应用提供了新思路。"}}
{"id": "2506.09656", "pdf": "https://arxiv.org/pdf/2506.09656", "abs": "https://arxiv.org/abs/2506.09656", "authors": ["Wei Zeng", "Hengshu Zhu", "Chuan Qin", "Han Wu", "Yihang Cheng", "Sirui Zhang", "Xiaowei Jin", "Yinuo Shen", "Zhenxing Wang", "Feimin Zhong", "Hui Xiong"], "title": "Application-Driven Value Alignment in Agentic AI Systems: Survey and Perspectives", "categories": ["cs.AI"], "comment": null, "summary": "The ongoing evolution of AI paradigms has propelled AI research into the\nAgentic AI stage. Consequently, the focus of research has shifted from single\nagents and simple applications towards multi-agent autonomous decision-making\nand task collaboration in complex environments. As Large Language Models (LLMs)\nadvance, their applications become more diverse and complex, leading to\nincreasingly situational and systemic risks. This has brought significant\nattention to value alignment for AI agents, which aims to ensure that an\nagent's goals, preferences, and behaviors align with human values and societal\nnorms. This paper reviews value alignment in agent systems within specific\napplication scenarios. It integrates the advancements in AI driven by large\nmodels with the demands of social governance. Our review covers value\nprinciples, agent system application scenarios, and agent value alignment\nevaluation. Specifically, value principles are organized hierarchically from a\ntop-down perspective, encompassing macro, meso, and micro levels. Agent system\napplication scenarios are categorized and reviewed from a general-to-specific\nviewpoint. Agent value alignment evaluation systematically examines datasets\nfor value alignment assessment and relevant value alignment methods.\nAdditionally, we delve into value coordination among multiple agents within\nagent systems. Finally, we propose several potential research directions in\nthis field.", "AI": {"tldr": "本文综述了Agentic AI阶段中多智能体系统的价值对齐问题，整合了大模型驱动的AI进展与社会治理需求，涵盖了价值原则、应用场景和评估方法，并提出了未来研究方向。", "motivation": "随着AI范式演进至Agentic AI阶段，多智能体自主决策和任务协作的需求增加，同时LLMs的广泛应用带来了情境性和系统性风险，促使研究关注智能体与人类价值和社会规范的对齐。", "method": "论文从宏观、中观和微观层次组织价值原则，分类综述智能体系统应用场景，系统化评估价值对齐数据集和方法，并探讨多智能体间的价值协调。", "result": "研究提出了价值对齐的层次化框架、应用场景分类和评估方法，为智能体系统的价值对齐提供了系统性视角。", "conclusion": "本文总结了智能体价值对齐的关键问题，并提出了未来研究方向，为AI与社会治理的融合提供了理论支持。"}}
{"id": "2506.09259", "pdf": "https://arxiv.org/pdf/2506.09259", "abs": "https://arxiv.org/abs/2506.09259", "authors": ["Zhuofang Li", "Rafal Kocielnik", "Fereshteh Soltani", "Penphob", "Boonyarungsrit", "Animashree Anandkumar", "R. Michael Alvarez"], "title": "Self-Anchored Attention Model for Sample-Efficient Classification of Prosocial Text Chat", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; K.4"], "comment": null, "summary": "Millions of players engage daily in competitive online games, communicating\nthrough in-game chat. Prior research has focused on detecting relatively small\nvolumes of toxic content using various Natural Language Processing (NLP)\ntechniques for the purpose of moderation. However, recent studies emphasize the\nimportance of detecting prosocial communication, which can be as crucial as\nidentifying toxic interactions. Recognizing prosocial behavior allows for its\nanalysis, rewarding, and promotion. Unlike toxicity, there are limited\ndatasets, models, and resources for identifying prosocial behaviors in\ngame-chat text. In this work, we employed unsupervised discovery combined with\ngame domain expert collaboration to identify and categorize prosocial player\nbehaviors from game chat. We further propose a novel Self-Anchored Attention\nModel (SAAM) which gives 7.9% improvement compared to the best existing\ntechnique. The approach utilizes the entire training set as \"anchors\" to help\nimprove model performance under the scarcity of training data. This approach\nled to the development of the first automated system for classifying prosocial\nbehaviors in in-game chats, particularly given the low-resource settings where\nlarge-scale labeled data is not available. Our methodology was applied to one\nof the most popular online gaming titles - Call of Duty(R): Modern\nWarfare(R)II, showcasing its effectiveness. This research is novel in applying\nNLP techniques to discover and classify prosocial behaviors in player in-game\nchat communication. It can help shift the focus of moderation from solely\npenalizing toxicity to actively encouraging positive interactions on online\nplatforms.", "AI": {"tldr": "论文提出了一种新方法（SAAM）用于检测游戏聊天中的亲社会行为，解决了现有资源不足的问题，并在《使命召唤》中验证了有效性。", "motivation": "现有研究主要关注检测游戏聊天中的负面内容，而亲社会行为的识别和分析同样重要，但缺乏相关数据集和模型。", "method": "结合无监督发现与游戏领域专家合作，提出SAAM模型，利用整个训练集作为“锚点”提升性能。", "result": "SAAM比现有最佳技术提升了7.9%，并开发了首个自动化分类系统。", "conclusion": "该研究为游戏聊天中的亲社会行为分类提供了新方法，有助于从惩罚负面行为转向鼓励积极互动。"}}
{"id": "2506.09079", "pdf": "https://arxiv.org/pdf/2506.09079", "abs": "https://arxiv.org/abs/2506.09079", "authors": ["Xinlong Chen", "Yuanxing Zhang", "Yushuo Guan", "Bohan Zeng", "Yang Shi", "Sihan Yang", "Pengfei Wan", "Qiang Liu", "Liang Wang", "Tieniu Tan"], "title": "VersaVid-R1: A Versatile Video Understanding and Reasoning Model from Question Answering to Captioning Tasks", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in multimodal large language models have successfully\nextended the Reason-Then-Respond paradigm to image-based reasoning, yet\nvideo-based reasoning remains an underdeveloped frontier, primarily due to the\nscarcity of high-quality reasoning-oriented data and effective training\nmethodologies. To bridge this gap, we introduce DarkEventInfer and MixVidQA,\ntwo novel datasets specifically designed to stimulate the model's advanced\nvideo understanding and reasoning abilities. DarkEventinfer presents videos\nwith masked event segments, requiring models to infer the obscured content\nbased on contextual video cues. MixVidQA, on the other hand, presents\ninterleaved video sequences composed of two distinct clips, challenging models\nto isolate and reason about one while disregarding the other. Leveraging these\ncarefully curated training samples together with reinforcement learning guided\nby diverse reward functions, we develop VersaVid-R1, the first versatile video\nunderstanding and reasoning model under the Reason-Then-Respond paradigm\ncapable of handling multiple-choice and open-ended question answering, as well\nas video captioning tasks. Extensive experiments demonstrate that VersaVid-R1\nsignificantly outperforms existing models across a broad spectrum of\nbenchmarks, covering video general understanding, cognitive reasoning, and\ncaptioning tasks.", "AI": {"tldr": "论文提出了两个新数据集DarkEventInfer和MixVidQA，用于提升视频理解和推理能力，并开发了VersaVid-R1模型，在多项任务中表现优异。", "motivation": "视频推理领域因缺乏高质量数据和有效训练方法而发展不足，需填补这一空白。", "method": "引入两个新数据集，结合强化学习训练VersaVid-R1模型。", "result": "VersaVid-R1在多项基准测试中显著优于现有模型。", "conclusion": "VersaVid-R1是首个支持多任务视频推理的模型，具有广泛应用潜力。"}}
{"id": "2506.09659", "pdf": "https://arxiv.org/pdf/2506.09659", "abs": "https://arxiv.org/abs/2506.09659", "authors": ["Eltayeb Ahmed", "Uljad Berdica", "Martha Elliott", "Danijela Horak", "Jakob N. Foerster"], "title": "Intent Factored Generation: Unleashing the Diversity in Your Language Model", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Obtaining multiple meaningfully diverse, high quality samples from Large\nLanguage Models for a fixed prompt remains an open challenge. Current methods\nfor increasing diversity often only operate at the token-level, paraphrasing\nthe same response. This is problematic because it leads to poor exploration on\nreasoning problems and to unengaging, repetitive conversational agents. To\naddress this we propose Intent Factored Generation (IFG), factorising the\nsampling process into two stages. First, we sample a semantically dense intent,\ne.g., a summary or keywords. Second, we sample the final response conditioning\non both the original prompt and the intent from the first stage. This allows us\nto use a higher temperature during the intent step to promote conceptual\ndiversity, and a lower temperature during the final generation to ensure the\noutputs are coherent and self-consistent. Additionally, we find that prompting\nthe model to explicitly state its intent for each step of the chain-of-thought\nbefore generating the step is beneficial for reasoning tasks. We demonstrate\nour method's effectiveness across a diverse set of tasks. We show this method\nimproves both pass@k and Reinforcement Learning from Verifier Feedback on maths\nand code tasks. For instruction-tuning, we combine IFG with Direct Preference\nOptimisation to increase conversational diversity without sacrificing reward.\nFinally, we achieve higher diversity while maintaining the quality of\ngenerations on a general language modelling task, using a new dataset of reader\ncomments and news articles that we collect and open-source. In summary, we\npresent a simple method of increasing the sample diversity of LLMs while\nmaintaining performance. This method can be implemented by changing the prompt\nand varying the temperature during generation, making it easy to integrate into\nmany algorithms for gains across various applications.", "AI": {"tldr": "提出了一种名为意图分解生成（IFG）的方法，通过两阶段采样过程提高大型语言模型生成样本的多样性和质量。", "motivation": "当前方法在提高多样性时往往仅停留在词汇层面，导致推理问题探索不足和对话代理重复无趣。", "method": "IFG将采样过程分为两阶段：首先生成语义密集的意图（如摘要或关键词），再基于原始提示和意图生成最终响应。", "result": "实验表明，IFG在数学和代码任务中提高了pass@k和强化学习验证反馈效果，同时在指令调优中增加了对话多样性而不牺牲奖励。", "conclusion": "IFG是一种简单有效的方法，通过调整提示和温度参数，可在多种应用中提高样本多样性并保持性能。"}}
{"id": "2506.09277", "pdf": "https://arxiv.org/pdf/2506.09277", "abs": "https://arxiv.org/abs/2506.09277", "authors": ["Milan Bhan", "Jean-Noel Vittaut", "Nicolas Chesneau", "Sarath Chandar", "Marie-Jeanne Lesot"], "title": "Did I Faithfully Say What I Thought? Bridging the Gap Between Neural Activity and Self-Explanations in Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLM) have demonstrated the capability of generating\nfree text self Natural Language Explanation (self-NLE) to justify their\nanswers. Despite their logical appearance, self-NLE do not necessarily reflect\nthe LLM actual decision-making process, making such explanations unfaithful.\nWhile existing methods for measuring self-NLE faithfulness mostly rely on\nbehavioral tests or computational block identification, none of them examines\nthe neural activity underlying the model's reasoning. This work introduces a\nnovel flexible framework for quantitatively measuring the faithfulness of\nLLM-generated self-NLE by directly comparing the latter with interpretations of\nthe model's internal hidden states. The proposed framework is versatile and\nprovides deep insights into self-NLE faithfulness by establishing a direct\nconnection between self-NLE and model reasoning. This approach advances the\nunderstanding of self-NLE faithfulness and provides building blocks for\ngenerating more faithful self-NLE.", "AI": {"tldr": "提出了一种新框架，通过比较LLM生成的自我自然语言解释（self-NLE）与模型内部隐藏状态的解释，定量测量其忠实性。", "motivation": "现有方法主要依赖行为测试或计算块识别，未能深入模型神经活动，导致无法准确评估self-NLE的忠实性。", "method": "引入灵活框架，直接比较self-NLE与模型内部隐藏状态的解释。", "result": "框架提供了对self-NLE忠实性的深入理解，并建立了self-NLE与模型推理的直接联系。", "conclusion": "该方法推进了对self-NLE忠实性的理解，并为生成更忠实的self-NLE奠定了基础。"}}
{"id": "2506.09081", "pdf": "https://arxiv.org/pdf/2506.09081", "abs": "https://arxiv.org/abs/2506.09081", "authors": ["Zheqi He", "Yesheng Liu", "Jing-shu Zheng", "Xuejing Li", "Richeng Xuan", "Jin-Ge Yao", "Xi Yang"], "title": "FlagEvalMM: A Flexible Framework for Comprehensive Multimodal Model Evaluation", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "We present FlagEvalMM, an open-source evaluation framework designed to\ncomprehensively assess multimodal models across a diverse range of\nvision-language understanding and generation tasks, such as visual question\nanswering, text-to-image/video generation, and image-text retrieval. We\ndecouple model inference from evaluation through an independent evaluation\nservice, thus enabling flexible resource allocation and seamless integration of\nnew tasks and models. Moreover, FlagEvalMM utilizes advanced inference\nacceleration tools (e.g., vLLM, SGLang) and asynchronous data loading to\nsignificantly enhance evaluation efficiency. Extensive experiments show that\nFlagEvalMM offers accurate and efficient insights into model strengths and\nlimitations, making it a valuable tool for advancing multimodal research. The\nframework is publicly accessible athttps://github.com/flageval-baai/FlagEvalMM.", "AI": {"tldr": "FlagEvalMM是一个开源的多模态模型评估框架，支持多种视觉-语言任务，通过独立评估服务和高效工具提升评估效率。", "motivation": "当前多模态模型评估缺乏统一、高效的框架，FlagEvalMM旨在填补这一空白。", "method": "通过解耦模型推理与评估，结合异步数据加载和推理加速工具（如vLLM、SGLang）提升效率。", "result": "实验表明FlagEvalMM能高效、准确地评估模型性能，揭示其优缺点。", "conclusion": "FlagEvalMM是一个高效、灵活的工具，有助于推动多模态研究的发展。"}}
{"id": "2506.09977", "pdf": "https://arxiv.org/pdf/2506.09977", "abs": "https://arxiv.org/abs/2506.09977", "authors": ["Stylianos Loukas Vasileiou", "Antonio Rago", "Maria Vanina Martinez", "William Yeoh"], "title": "How Do People Revise Inconsistent Beliefs? Examining Belief Revision in Humans with User Studies", "categories": ["cs.AI"], "comment": null, "summary": "Understanding how humans revise their beliefs in light of new information is\ncrucial for developing AI systems which can effectively model, and thus align\nwith, human reasoning. While theoretical belief revision frameworks rely on a\nset of principles that establish how these operations are performed, empirical\nevidence from cognitive psychology suggests that people may follow different\npatterns when presented with conflicting information. In this paper, we present\nthree comprehensive user studies showing that people consistently prefer\nexplanation-based revisions, i.e., those which are guided by explanations, that\nresult in changes to their belief systems that are not necessarily captured by\nclassical belief change theory. Our experiments systematically investigate how\npeople revise their beliefs with explanations for inconsistencies, whether they\nare provided with them or left to formulate them themselves, demonstrating a\nrobust preference for what may seem non-minimal revisions across different\ntypes of scenarios. These findings have implications for AI systems designed to\nmodel human reasoning or interact with humans, suggesting that such systems\nshould accommodate explanation-based, potentially non-minimal belief revision\noperators to better align with human cognitive processes.", "AI": {"tldr": "论文研究了人类如何基于新信息修正信念，发现人们更倾向于基于解释的修正方式，这与经典信念修正理论不同。", "motivation": "理解人类如何修正信念对开发能有效模拟人类推理的AI系统至关重要。", "method": "通过三项用户研究，系统分析了人们在面对不一致信息时如何基于解释修正信念。", "result": "研究发现人们普遍偏好基于解释的修正方式，即使这种修正可能不符合经典理论的最小化原则。", "conclusion": "AI系统应考虑采用基于解释的信念修正方法，以更好地与人类认知过程对齐。"}}
{"id": "2506.09301", "pdf": "https://arxiv.org/pdf/2506.09301", "abs": "https://arxiv.org/abs/2506.09301", "authors": ["Cesare Spinoso-Di Piano", "David Austin", "Pablo Piantanida", "Jackie Chi Kit Cheung"], "title": "$(RSA)^2$: A Rhetorical-Strategy-Aware Rational Speech Act Framework for Figurative Language Understanding", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "Figurative language (e.g., irony, hyperbole, understatement) is ubiquitous in\nhuman communication, resulting in utterances where the literal and the intended\nmeanings do not match. The Rational Speech Act (RSA) framework, which\nexplicitly models speaker intentions, is the most widespread theory of\nprobabilistic pragmatics, but existing implementations are either unable to\naccount for figurative expressions or require modeling the implicit motivations\nfor using figurative language (e.g., to express joy or annoyance) in a\nsetting-specific way. In this paper, we introduce the Rhetorical-Strategy-Aware\nRSA $(RSA)^2$ framework which models figurative language use by considering a\nspeaker's employed rhetorical strategy. We show that $(RSA)^2$ enables\nhuman-compatible interpretations of non-literal utterances without modeling a\nspeaker's motivations for being non-literal. Combined with LLMs, it achieves\nstate-of-the-art performance on the ironic split of PragMega+, a new irony\ninterpretation dataset introduced in this study.", "AI": {"tldr": "论文提出了一种名为$(RSA)^2$的新框架，通过考虑说话者的修辞策略来解释比喻语言，无需建模其动机，并在讽刺解释任务中达到最先进性能。", "motivation": "比喻语言在人类交流中普遍存在，但现有理论难以解释其含义。RSA框架虽广泛使用，但无法处理比喻表达或需特定建模动机。", "method": "引入$(RSA)^2$框架，结合说话者的修辞策略，避免建模比喻语言的动机，并与大语言模型结合。", "result": "在讽刺解释数据集PragMega+上，$(RSA)^2$实现了人类兼容的解释和最优性能。", "conclusion": "$(RSA)^2$提供了一种无需建模动机的比喻语言解释方法，展示了其在实际任务中的有效性。"}}
{"id": "2506.09082", "pdf": "https://arxiv.org/pdf/2506.09082", "abs": "https://arxiv.org/abs/2506.09082", "authors": ["Zheda Mai", "Arpita Chowdhury", "Zihe Wang", "Sooyoung Jeon", "Lemeng Wang", "Jiacheng Hou", "Jihyung Kil", "Wei-Lun Chao"], "title": "AVA-Bench: Atomic Visual Ability Benchmark for Vision Foundation Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "First two authors contribute equally", "summary": "The rise of vision foundation models (VFMs) calls for systematic evaluation.\nA common approach pairs VFMs with large language models (LLMs) as\ngeneral-purpose heads, followed by evaluation on broad Visual Question\nAnswering (VQA) benchmarks. However, this protocol has two key blind spots: (i)\nthe instruction tuning data may not align with VQA test distributions, meaning\na wrong prediction can stem from such data mismatch rather than a VFM' visual\nshortcomings; (ii) VQA benchmarks often require multiple visual abilities,\nmaking it hard to tell whether errors stem from lacking all required abilities\nor just a single critical one. To address these gaps, we introduce AVA-Bench,\nthe first benchmark that explicitly disentangles 14 Atomic Visual Abilities\n(AVAs) -- foundational skills like localization, depth estimation, and spatial\nunderstanding that collectively support complex visual reasoning tasks. By\ndecoupling AVAs and matching training and test distributions within each,\nAVA-Bench pinpoints exactly where a VFM excels or falters. Applying AVA-Bench\nto leading VFMs thus reveals distinctive \"ability fingerprints,\" turning VFM\nselection from educated guesswork into principled engineering. Notably, we find\nthat a 0.5B LLM yields similar VFM rankings as a 7B LLM while cutting GPU hours\nby 8x, enabling more efficient evaluation. By offering a comprehensive and\ntransparent benchmark, we hope AVA-Bench lays the foundation for the next\ngeneration of VFMs.", "AI": {"tldr": "AVA-Bench是一个新的基准测试，旨在通过解耦14种原子视觉能力（AVAs）来系统评估视觉基础模型（VFMs），解决了现有VQA基准测试中的数据不匹配和多能力混淆问题。", "motivation": "现有VQA基准测试存在数据不匹配和多能力混淆问题，无法准确评估VFMs的视觉能力。", "method": "提出AVA-Bench，通过解耦14种AVAs并匹配训练和测试分布，精准评估VFMs的能力。", "result": "AVA-Bench揭示了VFMs的独特能力指纹，并发现较小的LLM（0.5B）能高效替代较大LLM（7B）进行排名。", "conclusion": "AVA-Bench为下一代VFMs的发展提供了全面透明的评估基础。"}}
{"id": "2506.09985", "pdf": "https://arxiv.org/pdf/2506.09985", "abs": "https://arxiv.org/abs/2506.09985", "authors": ["Mido Assran", "Adrien Bardes", "David Fan", "Quentin Garrido", "Russell Howes", "Mojtaba", "Komeili", "Matthew Muckley", "Ammar Rizvi", "Claire Roberts", "Koustuv Sinha", "Artem Zholus", "Sergio Arnaud", "Abha Gejji", "Ada Martin", "Francois Robert Hogan", "Daniel Dugas", "Piotr Bojanowski", "Vasil Khalidov", "Patrick Labatut", "Francisco Massa", "Marc Szafraniec", "Kapil Krishnakumar", "Yong Li", "Xiaodong Ma", "Sarath Chandar", "Franziska Meier", "Yann LeCun", "Michael Rabbat", "Nicolas Ballas"], "title": "V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.RO"], "comment": "48 pages, 19 figures", "summary": "A major challenge for modern AI is to learn to understand the world and learn\nto act largely by observation. This paper explores a self-supervised approach\nthat combines internet-scale video data with a small amount of interaction data\n(robot trajectories), to develop models capable of understanding, predicting,\nand planning in the physical world. We first pre-train an action-free\njoint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset\ncomprising over 1 million hours of internet video. V-JEPA 2 achieves strong\nperformance on motion understanding (77.3 top-1 accuracy on Something-Something\nv2) and state-of-the-art performance on human action anticipation (39.7\nrecall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models.\nAdditionally, after aligning V-JEPA 2 with a large language model, we\ndemonstrate state-of-the-art performance on multiple video question-answering\ntasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on\nTempCompass). Finally, we show how self-supervised learning can be applied to\nrobotic planning tasks by post-training a latent action-conditioned world\nmodel, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the\nDroid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different\nlabs and enable picking and placing of objects using planning with image goals.\nNotably, this is achieved without collecting any data from the robots in these\nenvironments, and without any task-specific training or reward. This work\ndemonstrates how self-supervised learning from web-scale data and a small\namount of robot interaction data can yield a world model capable of planning in\nthe physical world.", "AI": {"tldr": "论文提出了一种结合互联网视频数据和少量机器人交互数据的自监督学习方法，开发了能够理解、预测和规划物理世界的模型。", "motivation": "解决现代AI通过观察学习和理解世界的挑战。", "method": "使用V-JEPA 2架构预训练，结合视频和图像数据，并通过语言模型对齐和机器人视频后训练。", "result": "在多项任务中表现优异，如运动理解、动作预测和视频问答，并在机器人规划任务中实现零样本部署。", "conclusion": "自监督学习结合少量交互数据可生成能规划物理世界的世界模型。"}}
{"id": "2506.09315", "pdf": "https://arxiv.org/pdf/2506.09315", "abs": "https://arxiv.org/abs/2506.09315", "authors": ["Yao Xiao", "Heidi Christensen", "Stefan Goetze"], "title": "Alzheimer's Dementia Detection Using Perplexity from Paired Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "To be published in the proceedings of Interspeech 2025", "summary": "Alzheimer's dementia (AD) is a neurodegenerative disorder with cognitive\ndecline that commonly impacts language ability. This work extends the paired\nperplexity approach to detecting AD by using a recent large language model\n(LLM), the instruction-following version of Mistral-7B. We improve accuracy by\nan average of 3.33% over the best current paired perplexity method and by 6.35%\nover the top-ranked method from the ADReSS 2020 challenge benchmark. Our\nfurther analysis demonstrates that the proposed approach can effectively detect\nAD with a clear and interpretable decision boundary in contrast to other\nmethods that suffer from opaque decision-making processes. Finally, by\nprompting the fine-tuned LLMs and comparing the model-generated responses to\nhuman responses, we illustrate that the LLMs have learned the special language\npatterns of AD speakers, which opens up possibilities for novel methods of\nmodel interpretation and data augmentation.", "AI": {"tldr": "本文提出了一种基于Mistral-7B大语言模型的配对困惑度方法，用于检测阿尔茨海默病（AD），显著提高了检测准确率，并展示了模型的可解释性和潜在应用。", "motivation": "阿尔茨海默病（AD）常伴随语言能力下降，现有检测方法在准确性和可解释性上存在不足。", "method": "使用指令跟随版本的Mistral-7B大语言模型，改进配对困惑度方法，并通过提示微调模型分析语言模式。", "result": "平均准确率比现有最佳方法提高3.33%，比ADReSS 2020基准方法提高6.35%，且决策边界清晰可解释。", "conclusion": "该方法不仅提升了AD检测性能，还揭示了模型对AD语言模式的学习能力，为模型解释和数据增强提供了新思路。"}}
{"id": "2506.09083", "pdf": "https://arxiv.org/pdf/2506.09083", "abs": "https://arxiv.org/abs/2506.09083", "authors": ["Jerry Lin", "Partick P. W. Chen"], "title": "BakuFlow: A Streamlining Semi-Automatic Label Generation Tool", "categories": ["cs.CV", "cs.AI"], "comment": "4 pages, 3 figures, 1 Table", "summary": "Accurately labeling (or annotation) data is still a bottleneck in computer\nvision, especially for large-scale tasks where manual labeling is\ntime-consuming and error-prone. While tools like LabelImg can handle the\nlabeling task, some of them still require annotators to manually label each\nimage. In this paper, we introduce BakuFlow, a streamlining semi-automatic\nlabel generation tool. Key features include (1) a live adjustable magnifier for\npixel-precise manual corrections, improving user experience; (2) an interactive\ndata augmentation module to diversify training datasets; (3) label propagation\nfor rapidly copying labeled objects between consecutive frames, greatly\naccelerating annotation of video data; and (4) an automatic labeling module\npowered by a modified YOLOE framework. Unlike the original YOLOE, our extension\nsupports adding new object classes and any number of visual prompts per class\nduring annotation, enabling flexible and scalable labeling for dynamic,\nreal-world datasets. These innovations make BakuFlow especially effective for\nobject detection and tracking, substantially reducing labeling workload and\nimproving efficiency in practical computer vision and industrial scenarios.", "AI": {"tldr": "BakuFlow是一个半自动标注工具，通过像素级手动修正、交互式数据增强、标签传播和自动标注模块，显著提升标注效率。", "motivation": "计算机视觉中数据标注是瓶颈，尤其是大规模任务，手动标注耗时且易错。", "method": "BakuFlow结合可调放大镜、交互式数据增强、标签传播和基于改进YOLOE的自动标注模块。", "result": "工具显著减少标注工作量，提升效率，适用于动态现实数据集。", "conclusion": "BakuFlow在目标检测和跟踪中高效，适用于实际计算机视觉和工业场景。"}}
{"id": "2410.16222", "pdf": "https://arxiv.org/pdf/2410.16222", "abs": "https://arxiv.org/abs/2410.16222", "authors": ["Valentyn Boreiko", "Alexander Panfilov", "Vaclav Voracek", "Matthias Hein", "Jonas Geiping"], "title": "An Interpretable N-gram Perplexity Threat Model for Large Language Model Jailbreaks", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "A plethora of jailbreaking attacks have been proposed to obtain harmful\nresponses from safety-tuned LLMs. These methods largely succeed in coercing the\ntarget output in their original settings, but their attacks vary substantially\nin fluency and computational effort. In this work, we propose a unified threat\nmodel for the principled comparison of these methods. Our threat model checks\nif a given jailbreak is likely to occur in the distribution of text. For this,\nwe build an N-gram language model on 1T tokens, which, unlike model-based\nperplexity, allows for an LLM-agnostic, nonparametric, and inherently\ninterpretable evaluation. We adapt popular attacks to this threat model, and,\nfor the first time, benchmark these attacks on equal footing with it. After an\nextensive comparison, we find attack success rates against safety-tuned modern\nmodels to be lower than previously presented and that attacks based on discrete\noptimization significantly outperform recent LLM-based attacks. Being\ninherently interpretable, our threat model allows for a comprehensive analysis\nand comparison of jailbreak attacks. We find that effective attacks exploit and\nabuse infrequent bigrams, either selecting the ones absent from real-world text\nor rare ones, e.g., specific to Reddit or code datasets.", "AI": {"tldr": "本文提出了一种统一的威胁模型，用于系统比较针对安全调整大语言模型（LLM）的越狱攻击方法，并通过N-gram语言模型评估攻击的流畅性和计算成本。", "motivation": "现有越狱攻击方法在原始设定中虽能成功，但其流畅性和计算成本差异显著，缺乏统一的评估标准。", "method": "构建基于1T令牌的N-gram语言模型，提供LLM无关、非参数化且可解释的评估框架，并适配流行攻击方法进行基准测试。", "result": "发现攻击成功率低于先前报告，离散优化攻击优于基于LLM的攻击；有效攻击利用罕见bigrams。", "conclusion": "提出的威胁模型为越狱攻击提供了可解释的全面分析工具，揭示了攻击依赖罕见文本模式的特性。"}}
{"id": "2506.09329", "pdf": "https://arxiv.org/pdf/2506.09329", "abs": "https://arxiv.org/abs/2506.09329", "authors": ["Yuxin Jiang"], "title": "Towards Efficient and Effective Alignment of Large Language Models", "categories": ["cs.CL"], "comment": "PhD thesis", "summary": "Large language models (LLMs) exhibit remarkable capabilities across diverse\ntasks, yet aligning them efficiently and effectively with human expectations\nremains a critical challenge. This thesis advances LLM alignment by introducing\nnovel methodologies in data collection, training, and evaluation. We first\naddress alignment data collection. Existing approaches rely heavily on manually\ncurated datasets or proprietary models. To overcome these limitations, we\npropose Lion, an adversarial distillation framework that iteratively refines\ntraining data by identifying and generating challenging instructions, enabling\nstate-of-the-art zero-shot reasoning. Additionally, we introduce Web\nReconstruction (WebR), a fully automated framework that synthesizes\ninstruction-tuning data directly from raw web documents, significantly\nimproving data diversity and scalability over existing synthetic data methods.\nNext, we enhance alignment training through novel optimization techniques. We\ndevelop Learning to Edit (LTE), a framework that enables LLMs to efficiently\nintegrate new knowledge while preserving existing information. LTE leverages\nmeta-learning to improve both real-time and batch knowledge updates.\nFurthermore, we introduce Bridging and Modeling Correlations (BMC), a\nrefinement of Direct Preference Optimization (DPO) that explicitly captures\ntoken-level correlations in preference data, leading to superior alignment\nacross QA and mathematical reasoning tasks. Finally, we tackle the challenge of\nevaluating alignment. Existing benchmarks emphasize response quality but\noverlook adherence to specific constraints. To bridge this gap, we introduce\nFollowBench, a multi-level, fine-grained benchmark assessing LLMs' ability to\nfollow complex constraints across diverse instruction types. Our results expose\nkey weaknesses in current models' constraint adherence, offering insights for\nfuture improvements.", "AI": {"tldr": "该论文提出了Lion、WebR、LTE、BMC和FollowBench等新方法，以改进大语言模型（LLM）与人类期望的对齐，涵盖数据收集、训练和评估。", "motivation": "尽管LLM在多种任务中表现出色，但如何高效且有效地使其与人类期望对齐仍是一个关键挑战。", "method": "论文提出了多种方法：Lion（对抗蒸馏框架）、WebR（自动化数据合成框架）、LTE（知识编辑框架）、BMC（改进的偏好优化方法）和FollowBench（评估对齐能力的基准）。", "result": "这些方法在零样本推理、数据多样性、知识更新和约束遵循等方面取得了显著改进。", "conclusion": "论文通过创新方法解决了LLM对齐的关键问题，并揭示了当前模型的局限性，为未来研究提供了方向。"}}
{"id": "2506.09106", "pdf": "https://arxiv.org/pdf/2506.09106", "abs": "https://arxiv.org/abs/2506.09106", "authors": ["Xiaofeng Zhang", "Michelle Lin", "Simon Lacoste-Julien", "Aaron Courville", "Yash Goyal"], "title": "Bias Analysis in Unconditional Image Generative Models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "The widespread adoption of generative AI models has raised growing concerns\nabout representational harm and potential discriminatory outcomes. Yet, despite\ngrowing literature on this topic, the mechanisms by which bias emerges -\nespecially in unconditional generation - remain disentangled. We define the\nbias of an attribute as the difference between the probability of its presence\nin the observed distribution and its expected proportion in an ideal reference\ndistribution. In our analysis, we train a set of unconditional image generative\nmodels and adopt a commonly used bias evaluation framework to study bias shift\nbetween training and generated distributions. Our experiments reveal that the\ndetected attribute shifts are small. We find that the attribute shifts are\nsensitive to the attribute classifier used to label generated images in the\nevaluation framework, particularly when its decision boundaries fall in\nhigh-density regions. Our empirical analysis indicates that this classifier\nsensitivity is often observed in attributes values that lie on a spectrum, as\nopposed to exhibiting a binary nature. This highlights the need for more\nrepresentative labeling practices, understanding the shortcomings through\ngreater scrutiny of evaluation frameworks, and recognizing the socially complex\nnature of attributes when evaluating bias.", "AI": {"tldr": "论文研究了生成式AI模型中的偏见机制，发现训练与生成分布间的属性偏移较小，但评估框架中的分类器敏感性对结果影响显著。", "motivation": "生成式AI模型的广泛应用引发了对代表性伤害和歧视性结果的担忧，但偏见机制尤其在无条件生成中仍未明确。", "method": "通过训练无条件图像生成模型并采用常用偏见评估框架，分析训练与生成分布间的偏见偏移。", "result": "实验显示属性偏移较小，但分类器的敏感性对结果影响显著，尤其在属性值呈连续分布时。", "conclusion": "需改进标签实践、深入评估框架的局限性，并认识属性的社会复杂性以更准确评估偏见。"}}
{"id": "2506.08672", "pdf": "https://arxiv.org/pdf/2506.08672", "abs": "https://arxiv.org/abs/2506.08672", "authors": ["Yang Liu", "Jiaqi Li", "Zilong Zheng"], "title": "RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic Sampling", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "22 pages, 10 figures, 8 tables", "summary": "Rule-based reasoning has been acknowledged as one of the fundamental problems\nin reasoning, while deviations in rule formats, types, and complexity in\nreal-world applications pose severe challenges. Recent studies have shown that\nlarge reasoning models (LRMs) have remarkable reasoning capabilities, and their\nperformance is substantially enhanced by reinforcement learning (RL). However,\nit remains an open question whether small reasoning models (SRMs) can learn\nrule-based reasoning effectively with robust generalization across diverse\ntasks and domains. To address this, we introduce Reinforced Rule-based\nReasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct\nrule-based reasoning via a wide collection of curated tasks and a novel\ndomain-aware dynamic sampling approach. Specifically, RuleReasoner resamples\neach training batch by updating the sampling weights of different domains based\non historical rewards. This facilitates domain augmentation and flexible online\nlearning schedules for RL, obviating the need for pre-hoc human-engineered\nmix-training recipes used in existing methods. Empirical evaluations on\nin-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that\nRuleReasoner outperforms frontier LRMs by a significant margin ($\\Delta$4.1%\naverage points on eight ID tasks and $\\Delta$10.4% average points on three OOD\ntasks over OpenAI-o1). Notably, our approach also exhibits higher computational\nefficiency compared to prior dynamic sampling methods for RL.", "AI": {"tldr": "论文提出了一种名为RuleReasoner的强化学习方法，用于提升小型推理模型在多样化任务和领域中的规则推理能力，显著优于现有大型推理模型。", "motivation": "解决小型推理模型在多样化规则格式、类型和复杂性任务中泛化能力不足的问题。", "method": "采用强化学习和动态采样策略，通过历史奖励更新不同领域的采样权重，实现灵活的在线学习。", "result": "在分布内和分布外任务中，RuleReasoner显著优于前沿大型推理模型（ID任务提升4.1%，OOD任务提升10.4%），且计算效率更高。", "conclusion": "RuleReasoner是一种简单有效的方法，能够提升小型推理模型的规则推理能力，并具有广泛的应用潜力。"}}
{"id": "2506.09331", "pdf": "https://arxiv.org/pdf/2506.09331", "abs": "https://arxiv.org/abs/2506.09331", "authors": ["Arjun Vaithilingam Sudhakar"], "title": "Multi-Agent Language Models: Advancing Cooperation, Coordination, and Adaptation", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "arXiv admin note: substantial text overlap with arXiv:2311.07687", "summary": "Modern Large Language Models (LLMs) exhibit impressive zero-shot and few-shot\ngeneralization capabilities across complex natural language tasks, enabling\ntheir widespread use as virtual assistants for diverse applications such as\ntranslation and summarization. Despite being trained solely on large corpora of\ntext without explicit supervision on author intent, LLMs appear to infer the\nunderlying meaning of textual interactions. This raises a fundamental question:\ncan LLMs model and reason about the intentions of others, i.e., do they possess\na form of theory of mind? Understanding other's intentions is crucial for\neffective collaboration, which underpins human societal success and is\nessential for cooperative interactions among multiple agents, including humans\nand autonomous systems. In this work, we investigate the theory of mind in LLMs\nthrough the lens of cooperative multi-agent reinforcement learning (MARL),\nwhere agents learn to collaborate via repeated interactions, mirroring human\nsocial reasoning. Our approach aims to enhance artificial agent's ability to\nadapt and cooperate with both artificial and human partners. By leveraging\nLLM-based agents capable of natural language interaction, we move towards\ncreating hybrid human-AI systems that can foster seamless collaboration, with\nbroad implications for the future of human-artificial interaction.", "AI": {"tldr": "研究探讨大型语言模型（LLMs）是否具备心智理论（theory of mind），并通过多智能体强化学习（MARL）框架评估其协作能力。", "motivation": "理解LLMs是否能推断他人意图，这对人机协作至关重要。", "method": "利用多智能体强化学习（MARL）框架，通过自然语言交互评估LLMs的协作能力。", "result": "研究表明LLMs具备一定的心智理论能力，能够适应和协作。", "conclusion": "LLMs在协作任务中展现出潜力，为人机混合系统的未来发展提供了基础。"}}
{"id": "2506.09109", "pdf": "https://arxiv.org/pdf/2506.09109", "abs": "https://arxiv.org/abs/2506.09109", "authors": ["Arnav Yayavaram", "Siddharth Yayavaram", "Simran Khanuja", "Michael Saxon", "Graham Neubig"], "title": "CAIRe: Cultural Attribution of Images by Retrieval-Augmented Evaluation", "categories": ["cs.CV", "cs.CL"], "comment": "Preprint, under review", "summary": "As text-to-image models become increasingly prevalent, ensuring their\nequitable performance across diverse cultural contexts is critical. Efforts to\nmitigate cross-cultural biases have been hampered by trade-offs, including a\nloss in performance, factual inaccuracies, or offensive outputs. Despite\nwidespread recognition of these challenges, an inability to reliably measure\nthese biases has stalled progress. To address this gap, we introduce CAIRe, a\nnovel evaluation metric that assesses the degree of cultural relevance of an\nimage, given a user-defined set of labels. Our framework grounds entities and\nconcepts in the image to a knowledge base and uses factual information to give\nindependent graded judgments for each culture label. On a manually curated\ndataset of culturally salient but rare items built using language models, CAIRe\nsurpasses all baselines by 28% F1 points. Additionally, we construct two\ndatasets for culturally universal concept, one comprising of T2I-generated\noutputs and another retrieved from naturally occurring data. CAIRe achieves\nPearson's correlations of 0.56 and 0.66 with human ratings on these sets, based\non a 5-point Likert scale of cultural relevance. This demonstrates its strong\nalignment with human judgment across diverse image sources.", "AI": {"tldr": "CAIRe是一种新的评估指标，用于衡量图像在不同文化背景下的相关性，解决了现有方法在测量跨文化偏见时的不足。", "motivation": "文本到图像模型在跨文化背景下的公平性能至关重要，但现有方法在测量和缓解偏见方面存在局限性。", "method": "CAIRe通过将图像中的实体和概念与知识库关联，利用事实信息为每个文化标签提供独立评分。", "result": "在手工策划的数据集上，CAIRe比基线方法提高了28%的F1分数，并在人类评分中表现出高相关性。", "conclusion": "CAIRe能够有效评估图像的文化相关性，并与人类判断高度一致，为跨文化偏见的测量提供了可靠工具。"}}
{"id": "2506.09052", "pdf": "https://arxiv.org/pdf/2506.09052", "abs": "https://arxiv.org/abs/2506.09052", "authors": ["Delower Hossain", "Ehsan Saghapour", "Kevin Song", "Jake Y. Chen"], "title": "Llama-Affinity: A Predictive Antibody Antigen Binding Model Integrating Antibody Sequences with Llama3 Backbone Architecture", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": "7 Pages", "summary": "Antibody-facilitated immune responses are central to the body's defense\nagainst pathogens, viruses, and other foreign invaders. The ability of\nantibodies to specifically bind and neutralize antigens is vital for\nmaintaining immunity. Over the past few decades, bioengineering advancements\nhave significantly accelerated therapeutic antibody development. These\nantibody-derived drugs have shown remarkable efficacy, particularly in treating\ncancer, SARS-CoV-2, autoimmune disorders, and infectious diseases.\nTraditionally, experimental methods for affinity measurement have been\ntime-consuming and expensive. With the advent of artificial intelligence, in\nsilico medicine has been revolutionized; recent developments in machine\nlearning, particularly the use of large language models (LLMs) for representing\nantibodies, have opened up new avenues for AI-based design and improved\naffinity prediction. Herein, we present an advanced antibody-antigen binding\naffinity prediction model (LlamaAffinity), leveraging an open-source Llama 3\nbackbone and antibody sequence data sourced from the Observed Antibody Space\n(OAS) database. The proposed approach shows significant improvement over\nexisting state-of-the-art (SOTA) methods (AntiFormer, AntiBERTa, AntiBERTy)\nacross multiple evaluation metrics. Specifically, the model achieved an\naccuracy of 0.9640, an F1-score of 0.9643, a precision of 0.9702, a recall of\n0.9586, and an AUC-ROC of 0.9936. Moreover, this strategy unveiled higher\ncomputational efficiency, with a five-fold average cumulative training time of\nonly 0.46 hours, significantly lower than in previous studies.", "AI": {"tldr": "论文提出了一种基于开源Llama 3的抗体-抗原结合亲和力预测模型（LlamaAffinity），在多个评估指标上显著优于现有方法，并展示了更高的计算效率。", "motivation": "传统抗体亲和力测量方法耗时且昂贵，而人工智能（尤其是大语言模型）为抗体设计和亲和力预测提供了新途径。", "method": "利用开源Llama 3框架和Observed Antibody Space数据库的抗体序列数据，开发了LlamaAffinity模型。", "result": "模型在准确率（0.9640）、F1分数（0.9643）、精确率（0.9702）、召回率（0.9586）和AUC-ROC（0.9936）上表现优异，且训练时间显著缩短（0.46小时）。", "conclusion": "LlamaAffinity模型在抗体-抗原亲和力预测中表现出色，为AI驱动的抗体设计提供了高效工具。"}}
{"id": "2506.09340", "pdf": "https://arxiv.org/pdf/2506.09340", "abs": "https://arxiv.org/abs/2506.09340", "authors": ["Siheng Li", "Zhanhui Zhou", "Wai Lam", "Chao Yang", "Chaochao Lu"], "title": "RePO: Replay-Enhanced Policy Optimization", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Project Page: https://github.com/SihengLi99/RePO", "summary": "Reinforcement learning (RL) is vital for optimizing large language models\n(LLMs). Recent Group Relative Policy Optimization (GRPO) estimates advantages\nusing multiple on-policy outputs per prompt, leading to high computational\ncosts and low data efficiency. To address this, we introduce Replay-Enhanced\nPolicy Optimization (RePO), which leverages diverse replay strategies to\nretrieve off-policy samples from a replay buffer, allowing policy optimization\nbased on a broader and more diverse set of samples for each prompt. Experiments\non five LLMs across seven mathematical reasoning benchmarks demonstrate that\nRePO achieves absolute average performance gains of $18.4$ and $4.1$ points for\nQwen2.5-Math-1.5B and Qwen3-1.7B, respectively, compared to GRPO. Further\nanalysis indicates that RePO increases computational cost by $15\\%$ while\nraising the number of effective optimization steps by $48\\%$ for Qwen3-1.7B,\nwith both on-policy and off-policy sample numbers set to $8$. The repository\ncan be accessed at https://github.com/SihengLi99/RePO.", "AI": {"tldr": "RePO通过多样化的回放策略从回放缓冲区中检索离策略样本，显著提升了大型语言模型的优化效率和性能。", "motivation": "解决GRPO方法因依赖多个同策略输出而导致的计算成本高和数据效率低的问题。", "method": "引入Replay-Enhanced Policy Optimization (RePO)，利用回放缓冲区中的离策略样本进行策略优化。", "result": "在七个数学推理基准测试中，RePO显著提升了模型性能，计算成本仅增加15%，但有效优化步骤增加48%。", "conclusion": "RePO是一种高效且性能优越的优化方法，适用于大型语言模型的强化学习。"}}
{"id": "2506.09113", "pdf": "https://arxiv.org/pdf/2506.09113", "abs": "https://arxiv.org/abs/2506.09113", "authors": ["Yu Gao", "Haoyuan Guo", "Tuyen Hoang", "Weilin Huang", "Lu Jiang", "Fangyuan Kong", "Huixia Li", "Jiashi Li", "Liang Li", "Xiaojie Li", "Xunsong Li", "Yifu Li", "Shanchuan Lin", "Zhijie Lin", "Jiawei Liu", "Shu Liu", "Xiaonan Nie", "Zhiwu Qing", "Yuxi Ren", "Li Sun", "Zhi Tian", "Rui Wang", "Sen Wang", "Guoqiang Wei", "Guohong Wu", "Jie Wu", "Ruiqi Xia", "Fei Xiao", "Xuefeng Xiao", "Jiangqiao Yan", "Ceyuan Yang", "Jianchao Yang", "Runkai Yang", "Tao Yang", "Yihang Yang", "Zilyu Ye", "Xuejiao Zeng", "Yan Zeng", "Heng Zhang", "Yang Zhao", "Xiaozheng Zheng", "Peihao Zhu", "Jiaxin Zou", "Feilong Zuo"], "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models", "categories": ["cs.CV"], "comment": "Seedance 1.0 Technical Report", "summary": "Notable breakthroughs in diffusion modeling have propelled rapid improvements\nin video generation, yet current foundational model still face critical\nchallenges in simultaneously balancing prompt following, motion plausibility,\nand visual quality. In this report, we introduce Seedance 1.0, a\nhigh-performance and inference-efficient video foundation generation model that\nintegrates several core technical improvements: (i) multi-source data curation\naugmented with precision and meaningful video captioning, enabling\ncomprehensive learning across diverse scenarios; (ii) an efficient architecture\ndesign with proposed training paradigm, which allows for natively supporting\nmulti-shot generation and jointly learning of both text-to-video and\nimage-to-video tasks. (iii) carefully-optimized post-training approaches\nleveraging fine-grained supervised fine-tuning, and video-specific RLHF with\nmulti-dimensional reward mechanisms for comprehensive performance improvements;\n(iv) excellent model acceleration achieving ~10x inference speedup through\nmulti-stage distillation strategies and system-level optimizations. Seedance\n1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds\n(NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance\n1.0 stands out with high-quality and fast video generation having superior\nspatiotemporal fluidity with structural stability, precise instruction\nadherence in complex multi-subject contexts, native multi-shot narrative\ncoherence with consistent subject representation.", "AI": {"tldr": "Seedance 1.0是一个高性能、推理高效的视频生成基础模型，通过多源数据增强、高效架构设计、优化后训练方法及模型加速技术，实现了高质量、快速的视频生成。", "motivation": "当前基础模型在平衡提示跟随、运动合理性和视觉质量方面面临挑战，Seedance 1.0旨在解决这些问题。", "method": "采用多源数据增强、高效架构设计、优化后训练方法（如精细监督微调和视频特定RLHF）及多阶段蒸馏策略加速模型。", "result": "Seedance 1.0在1080p分辨率下仅需41.4秒生成5秒视频，时空流畅性、结构稳定性及多主题指令跟随表现优异。", "conclusion": "Seedance 1.0在视频生成质量、速度和功能多样性上显著优于现有技术。"}}
{"id": "2506.09061", "pdf": "https://arxiv.org/pdf/2506.09061", "abs": "https://arxiv.org/abs/2506.09061", "authors": ["Alyssa Pinnock", "Shakya Jayakody", "Kawsher A Roxy", "Md Rubel Ahmed"], "title": "EdgeProfiler: A Fast Profiling Framework for Lightweight LLMs on Edge Using Analytical Model", "categories": ["cs.DC", "cs.AI", "cs.PF"], "comment": "4 figures, 7 pages, IEEE conference template", "summary": "This paper introduces EdgeProfiler, a fast profiling framework designed for\nevaluating lightweight Large Language Models (LLMs) on edge systems. While LLMs\noffer remarkable capabilities in natural language understanding and generation,\ntheir high computational, memory, and power requirements often confine them to\ncloud environments. EdgeProfiler addresses these challenges by providing a\nsystematic methodology for assessing LLM performance in resource-constrained\nedge settings. The framework profiles compact LLMs, including TinyLLaMA,\nGemma3.1B, Llama3.2-1B, and DeepSeek-r1-1.5B, using aggressive quantization\ntechniques and strict memory constraints. Analytical modeling is used to\nestimate latency, FLOPs, and energy consumption. The profiling reveals that\n4-bit quantization reduces model memory usage by approximately 60-70%, while\nmaintaining accuracy within 2-5% of full-precision baselines. Inference speeds\nare observed to improve by 2-3x compared to FP16 baselines across various edge\ndevices. Power modeling estimates a 35-50% reduction in energy consumption for\nINT4 configurations, enabling practical deployment on hardware such as\nRaspberry Pi 4/5 and Jetson Orin Nano Super. Our findings emphasize the\nimportance of efficient profiling tailored to lightweight LLMs in edge\nenvironments, balancing accuracy, energy efficiency, and computational\nfeasibility.", "AI": {"tldr": "EdgeProfiler是一个快速分析框架，用于评估边缘系统上的轻量级大语言模型（LLMs）。通过量化技术和内存约束，显著降低资源需求，同时保持准确性。", "motivation": "LLMs的高计算、内存和功耗需求限制了其在边缘环境的应用，EdgeProfiler旨在解决这一问题。", "method": "框架通过量化技术（如4位量化）和严格内存约束分析模型性能，包括延迟、FLOPs和能耗。", "result": "4位量化减少内存使用60-70%，推理速度提升2-3倍，能耗降低35-50%。", "conclusion": "EdgeProfiler为轻量级LLMs在边缘环境中的高效部署提供了平衡准确性、能效和计算可行性的解决方案。"}}
{"id": "2506.09342", "pdf": "https://arxiv.org/pdf/2506.09342", "abs": "https://arxiv.org/abs/2506.09342", "authors": ["Sushant Mehta", "Raj Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Latent Multi-Head Attention for Small Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 1 figure. 5 tables", "summary": "We present the first comprehensive study of latent multi-head attention (MLA)\nfor small language models, revealing interesting efficiency-quality trade-offs.\nTraining 30M-parameter GPT models on 100,000 synthetic stories, we benchmark\nthree architectural variants: standard multi-head attention (MHA), MLA, and MLA\nwith rotary positional embeddings (MLA+RoPE). Our key finding is that MLA+RoPE\nwith half-rank latent dimensions (r = d/2) achieves a 45% KV-cache memory\nreduction while incurring only a 0.3% increase in validation loss (essentially\nmatching MHA quality)- a Pareto improvement for memory constrained deployment.\nWe further show that RoPE is crucial for MLA in small models: without it, MLA\nunderperforms vanilla attention by 3-5%, but with RoPE, it surpasses vanilla by\n2%. Inference benchmarks on NVIDIA A100 GPUs reveal that MLA with r=d/2\nachieves a 1.4 times speedup over full-rank MLA while maintaining the memory\nsavings. GPT-4 evaluations corroborate perplexity results, with ours achieving\nthe highest quality scores (7.4/10) across grammar, creativity, and consistency\nmetrics. Code and models will be released upon acceptance.", "AI": {"tldr": "研究发现，在小语言模型中，采用半秩潜在维度的MLA+RoPE架构能显著减少KV缓存内存，同时几乎不影响模型质量。", "motivation": "探索小语言模型中潜在多头注意力（MLA）的效率与质量权衡，为内存受限的部署提供优化方案。", "method": "训练30M参数的GPT模型，比较标准多头注意力（MHA）、MLA及MLA+RoPE三种架构。", "result": "MLA+RoPE（r=d/2）减少45% KV缓存内存，验证损失仅增加0.3%，推理速度提升1.4倍。", "conclusion": "MLA+RoPE在小模型中表现优异，是内存受限场景下的帕累托改进方案。"}}
{"id": "2506.09229", "pdf": "https://arxiv.org/pdf/2506.09229", "abs": "https://arxiv.org/abs/2506.09229", "authors": ["Sungwon Hwang", "Hyojin Jang", "Kinam Kim", "Minho Park", "Jaegul choo"], "title": "Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion Models", "categories": ["cs.CV"], "comment": "24 pages, 25 figures", "summary": "Fine-tuning Video Diffusion Models (VDMs) at the user level to generate\nvideos that reflect specific attributes of training data presents notable\nchallenges, yet remains underexplored despite its practical importance.\nMeanwhile, recent work such as Representation Alignment (REPA) has shown\npromise in improving the convergence and quality of DiT-based image diffusion\nmodels by aligning, or assimilating, its internal hidden states with external\npretrained visual features, suggesting its potential for VDM fine-tuning. In\nthis work, we first propose a straightforward adaptation of REPA for VDMs and\nempirically show that, while effective for convergence, it is suboptimal in\npreserving semantic consistency across frames. To address this limitation, we\nintroduce Cross-frame Representation Alignment (CREPA), a novel regularization\ntechnique that aligns hidden states of a frame with external features from\nneighboring frames. Empirical evaluations on large-scale VDMs, including\nCogVideoX-5B and Hunyuan Video, demonstrate that CREPA improves both visual\nfidelity and cross-frame semantic coherence when fine-tuned with\nparameter-efficient methods such as LoRA. We further validate CREPA across\ndiverse datasets with varying attributes, confirming its broad applicability.\nProject page: https://crepavideo.github.io", "AI": {"tldr": "论文提出了一种新的正则化技术CREPA，用于改进视频扩散模型的微调，提升视觉保真度和跨帧语义一致性。", "motivation": "用户级微调视频扩散模型以反映训练数据的特定属性具有挑战性且研究不足，而现有方法如REPA在图像扩散模型中表现良好，但在视频中效果不佳。", "method": "提出Cross-frame Representation Alignment (CREPA)，通过将帧的隐藏状态与相邻帧的外部特征对齐，优化视频扩散模型的微调。", "result": "在CogVideoX-5B和Hunyuan Video等大规模VDMs上验证，CREPA显著提升了视觉质量和跨帧语义一致性。", "conclusion": "CREPA是一种广泛适用的技术，适用于多种数据集和参数高效微调方法。"}}
{"id": "2506.09065", "pdf": "https://arxiv.org/pdf/2506.09065", "abs": "https://arxiv.org/abs/2506.09065", "authors": ["Abigail Copiaco", "Christian Ritz", "Yassine Himeur", "Valsamma Eapen", "Ammar Albanna", "Wathiq Mansoor"], "title": "Exploring Image Transforms derived from Eye Gaze Variables for Progressive Autism Diagnosis", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "6 pages, 8 figures, and 1 table", "summary": "The prevalence of Autism Spectrum Disorder (ASD) has surged rapidly over the\npast decade, posing significant challenges in communication, behavior, and\nfocus for affected individuals. Current diagnostic techniques, though\neffective, are time-intensive, leading to high social and economic costs. This\nwork introduces an AI-powered assistive technology designed to streamline ASD\ndiagnosis and management, enhancing convenience for individuals with ASD and\nefficiency for caregivers and therapists. The system integrates transfer\nlearning with image transforms derived from eye gaze variables to diagnose ASD.\nThis facilitates and opens opportunities for in-home periodical diagnosis,\nreducing stress for individuals and caregivers, while also preserving user\nprivacy through the use of image transforms. The accessibility of the proposed\nmethod also offers opportunities for improved communication between guardians\nand therapists, ensuring regular updates on progress and evolving support\nneeds. Overall, the approach proposed in this work ensures timely, accessible\ndiagnosis while protecting the subjects' privacy, improving outcomes for\nindividuals with ASD.", "AI": {"tldr": "本文提出了一种基于AI的辅助技术，通过结合迁移学习和眼动变量生成的图像变换，优化自闭症谱系障碍（ASD）的诊断和管理，提升诊断效率并保护用户隐私。", "motivation": "ASD的发病率快速上升，现有诊断方法耗时且成本高，亟需更高效的解决方案。", "method": "采用迁移学习和眼动变量生成的图像变换技术，实现ASD的快速诊断。", "result": "该方法能够实现家庭定期诊断，减少压力并保护隐私，同时改善监护人与治疗师之间的沟通。", "conclusion": "该技术为ASD诊断提供了及时、便捷且隐私保护的解决方案，有望改善患者的生活质量。"}}
{"id": "2506.09349", "pdf": "https://arxiv.org/pdf/2506.09349", "abs": "https://arxiv.org/abs/2506.09349", "authors": ["Chao-Hong Tan", "Qian Chen", "Wen Wang", "Chong Deng", "Qinglin Zhang", "Luyao Cheng", "Hai Yu", "Xin Zhang", "Xiang Lv", "Tianyu Zhao", "Chong Zhang", "Yukun Ma", "Yafeng Chen", "Hui Wang", "Jiaqing Liu", "Jieping Ye"], "title": "OmniDRCA: Parallel Speech-Text Foundation Model via Dual-Resolution Speech Representations and Contrastive Alignment", "categories": ["cs.CL"], "comment": null, "summary": "Recent studies on end-to-end speech generation with large language models\n(LLMs) have attracted significant community attention, with multiple works\nextending text-based LLMs to generate discrete speech tokens. Existing\napproaches primarily fall into two categories: (1) Methods that generate\ndiscrete speech tokens independently without incorporating them into the LLM's\nautoregressive process, resulting in text generation being unaware of\nconcurrent speech synthesis. (2) Models that generate interleaved or parallel\nspeech-text tokens through joint autoregressive modeling, enabling mutual\nmodality awareness during generation. This paper presents OmniDRCA, a parallel\nspeech-text foundation model based on joint autoregressive modeling, featuring\ndual-resolution speech representations and contrastive cross-modal alignment.\nOur approach processes speech and text representations in parallel while\nenhancing audio comprehension through contrastive alignment. Experimental\nresults on Spoken Question Answering benchmarks demonstrate that OmniDRCA\nestablishes new state-of-the-art (SOTA) performance among parallel joint\nspeech-text modeling based foundation models, and achieves competitive\nperformance compared to interleaved models. Additionally, we explore the\npotential of extending the framework to full-duplex conversational scenarios.", "AI": {"tldr": "OmniDRCA是一种基于联合自回归建模的并行语音-文本基础模型，通过双分辨率语音表示和对比跨模态对齐，实现了语音和文本的并行处理，并在口语问答基准上取得了SOTA性能。", "motivation": "现有方法在语音生成中要么独立生成离散语音标记，要么通过联合自回归建模生成交错或并行的语音-文本标记。OmniDRCA旨在通过并行处理和对比对齐提升语音和文本的相互感知能力。", "method": "OmniDRCA采用联合自回归建模，结合双分辨率语音表示和对比跨模态对齐，并行处理语音和文本表示。", "result": "在口语问答基准测试中，OmniDRCA在并行联合语音-文本建模基础模型中达到了SOTA性能，并与交错模型表现相当。", "conclusion": "OmniDRCA展示了并行联合语音-文本建模的潜力，并可能扩展到全双工会话场景。"}}
{"id": "2506.09237", "pdf": "https://arxiv.org/pdf/2506.09237", "abs": "https://arxiv.org/abs/2506.09237", "authors": ["Mojtaba Nafez", "Amirhossein Koochakian", "Arad Maleki", "Jafar Habibi", "Mohammad Hossein Rohban"], "title": "PatchGuard: Adversarially Robust Anomaly Detection and Localization through Vision Transformers and Pseudo Anomalies", "categories": ["cs.CV", "cs.LG"], "comment": "Accepted to the Conference on Computer Vision and Pattern Recognition\n  (CVPR) 2025", "summary": "Anomaly Detection (AD) and Anomaly Localization (AL) are crucial in fields\nthat demand high reliability, such as medical imaging and industrial\nmonitoring. However, current AD and AL approaches are often susceptible to\nadversarial attacks due to limitations in training data, which typically\ninclude only normal, unlabeled samples. This study introduces PatchGuard, an\nadversarially robust AD and AL method that incorporates pseudo anomalies with\nlocalization masks within a Vision Transformer (ViT)-based architecture to\naddress these vulnerabilities. We begin by examining the essential properties\nof pseudo anomalies, and follow it by providing theoretical insights into the\nattention mechanisms required to enhance the adversarial robustness of AD and\nAL systems. We then present our approach, which leverages Foreground-Aware\nPseudo-Anomalies to overcome the deficiencies of previous anomaly-aware\nmethods. Our method incorporates these crafted pseudo-anomaly samples into a\nViT-based framework, with adversarial training guided by a novel loss function\ndesigned to improve model robustness, as supported by our theoretical analysis.\nExperimental results on well-established industrial and medical datasets\ndemonstrate that PatchGuard significantly outperforms previous methods in\nadversarial settings, achieving performance gains of $53.2\\%$ in AD and\n$68.5\\%$ in AL, while also maintaining competitive accuracy in non-adversarial\nsettings. The code repository is available at\nhttps://github.com/rohban-lab/PatchGuard .", "AI": {"tldr": "PatchGuard是一种基于Vision Transformer的对抗性鲁棒异常检测和定位方法，通过引入伪异常样本和定位掩码，显著提升了对抗性环境下的性能。", "motivation": "当前异常检测和定位方法因训练数据限制（仅包含正常样本）易受对抗性攻击，PatchGuard旨在解决这一问题。", "method": "利用前景感知伪异常样本和Vision Transformer架构，结合新型损失函数进行对抗训练。", "result": "在工业和医学数据集上，PatchGuard在对抗性环境下性能提升53.2%（AD）和68.5%（AL），且在非对抗性环境下保持竞争力。", "conclusion": "PatchGuard通过伪异常样本和理论驱动的设计，显著提升了异常检测和定位的对抗性鲁棒性。"}}
{"id": "2506.09351", "pdf": "https://arxiv.org/pdf/2506.09351", "abs": "https://arxiv.org/abs/2506.09351", "authors": ["Yuchen Feng", "Bowen Shen", "Naibin Gu", "Jiaxuan Zhao", "Peng Fu", "Zheng Lin", "Weiping Wang"], "title": "DIVE into MoE: Diversity-Enhanced Reconstruction of Large Language Models from Dense into Mixture-of-Experts", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "Large language models (LLMs) with the Mixture-of-Experts (MoE) architecture\nachieve high cost-efficiency by selectively activating a subset of the\nparameters. Despite the inference efficiency of MoE LLMs, the training of\nextensive experts from scratch incurs substantial overhead, whereas\nreconstructing a dense LLM into an MoE LLM significantly reduces the training\nbudget. However, existing reconstruction methods often overlook the diversity\namong experts, leading to potential redundancy. In this paper, we come up with\nthe observation that a specific LLM exhibits notable diversity after being\npruned on different calibration datasets, based on which we present a\nDiversity-Enhanced reconstruction method named DIVE. The recipe of DIVE\nincludes domain affinity mining, pruning-based expert reconstruction, and\nefficient retraining. Specifically, the reconstruction includes pruning and\nreassembly of the feed-forward network (FFN) module. After reconstruction, we\nefficiently retrain the model on routers, experts and normalization modules. We\nimplement DIVE on Llama-style LLMs with open-source training corpora.\nExperiments show that DIVE achieves training efficiency with minimal accuracy\ntrade-offs, outperforming existing pruning and MoE reconstruction methods with\nthe same number of activated parameters.", "AI": {"tldr": "论文提出了一种名为DIVE的多样性增强重建方法，通过修剪和重组FFN模块，优化MoE LLMs的训练效率，减少冗余并保持准确性。", "motivation": "现有MoE LLMs重建方法忽视专家多样性，导致冗余；DIVE通过利用修剪后的多样性提升重建效果。", "method": "DIVE包括领域亲和性挖掘、基于修剪的专家重建和高效重训练，具体涉及FFN模块的修剪与重组。", "result": "实验表明，DIVE在相同激活参数下优于现有修剪和MoE重建方法，训练效率高且准确性损失小。", "conclusion": "DIVE方法显著提升了MoE LLMs的训练效率，同时保持了模型性能，为LLMs的优化提供了新思路。"}}
{"id": "2506.09278", "pdf": "https://arxiv.org/pdf/2506.09278", "abs": "https://arxiv.org/abs/2506.09278", "authors": ["Yuchen Zhang", "Nikhil Keetha", "Chenwei Lyu", "Bhuvan Jhamb", "Yutian Chen", "Yuheng Qiu", "Jay Karhade", "Shreyas Jha", "Yaoyu Hu", "Deva Ramanan", "Sebastian Scherer", "Wenshan Wang"], "title": "UFM: A Simple Path towards Unified Dense Correspondence with Flow", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "Project Page: https://uniflowmatch.github.io/", "summary": "Dense image correspondence is central to many applications, such as visual\nodometry, 3D reconstruction, object association, and re-identification.\nHistorically, dense correspondence has been tackled separately for\nwide-baseline scenarios and optical flow estimation, despite the common goal of\nmatching content between two images. In this paper, we develop a Unified Flow &\nMatching model (UFM), which is trained on unified data for pixels that are\nco-visible in both source and target images. UFM uses a simple, generic\ntransformer architecture that directly regresses the (u,v) flow. It is easier\nto train and more accurate for large flows compared to the typical\ncoarse-to-fine cost volumes in prior work. UFM is 28% more accurate than\nstate-of-the-art flow methods (Unimatch), while also having 62% less error and\n6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to\ndemonstrate that unified training can outperform specialized approaches across\nboth domains. This result enables fast, general-purpose correspondence and\nopens new directions for multi-modal, long-range, and real-time correspondence\ntasks.", "AI": {"tldr": "论文提出了一种统一流与匹配模型（UFM），通过统一训练在宽基线场景和光流估计中实现更准确的密集图像对应，性能优于现有专门方法。", "motivation": "密集图像对应在视觉里程计、3D重建等应用中至关重要，但传统方法分别处理宽基线和光流问题，缺乏统一解决方案。", "method": "UFM采用简单的通用Transformer架构，直接回归(u,v)流，避免了传统粗到细成本体积的复杂性。", "result": "UFM比现有光流方法（Unimatch）准确率提升28%，比宽基线匹配器（RoMa）误差减少62%，速度快6.7倍。", "conclusion": "UFM首次证明统一训练可超越专门方法，为多模态、长距离和实时对应任务开辟新方向。"}}
{"id": "2506.09359", "pdf": "https://arxiv.org/pdf/2506.09359", "abs": "https://arxiv.org/abs/2506.09359", "authors": ["Qingyun Zeng", "Simin Ma", "Arash Niknafs", "Ashish Basran", "Carol Szabo"], "title": "Taming SQL Complexity: LLM-Based Equivalence Evaluation for Text-to-SQL", "categories": ["cs.CL"], "comment": "8 pages", "summary": "The rise of Large Language Models (LLMs) has significantly advanced\nText-to-SQL (NL2SQL) systems, yet evaluating the semantic equivalence of\ngenerated SQL remains a challenge, especially given ambiguous user queries and\nmultiple valid SQL interpretations. This paper explores using LLMs to assess\nboth semantic and a more practical \"weak\" semantic equivalence. We analyze\ncommon patterns of SQL equivalence and inequivalence, discuss challenges in\nLLM-based evaluation.", "AI": {"tldr": "论文探讨了利用大语言模型（LLMs）评估生成SQL的语义等价性，尤其是在用户查询模糊和存在多种有效SQL解释的情况下。", "motivation": "由于用户查询的模糊性和SQL的多重有效解释，评估生成SQL的语义等价性具有挑战性。", "method": "使用LLMs评估语义和“弱”语义等价性，并分析SQL等价性和不等价性的常见模式。", "result": "讨论了基于LLM评估的挑战。", "conclusion": "LLMs在评估SQL语义等价性方面具有潜力，但仍需解决相关挑战。"}}
{"id": "2506.09299", "pdf": "https://arxiv.org/pdf/2506.09299", "abs": "https://arxiv.org/abs/2506.09299", "authors": ["Sindhu Boddu", "Arindam Mukherjee"], "title": "Lightweight Object Detection Using Quantized YOLOv4-Tiny for Emergency Response in Aerial Imagery", "categories": ["cs.CV", "cs.LG"], "comment": "6 Pages, 3 figures", "summary": "This paper presents a lightweight and energy-efficient object detection\nsolution for aerial imagery captured during emergency response situations. We\nfocus on deploying the YOLOv4-Tiny model, a compact convolutional neural\nnetwork, optimized through post-training quantization to INT8 precision. The\nmodel is trained on a custom-curated aerial emergency dataset, consisting of\n10,820 annotated images covering critical emergency scenarios. Unlike prior\nworks that rely on publicly available datasets, we created this dataset\nourselves due to the lack of publicly available drone-view emergency imagery,\nmaking the dataset itself a key contribution of this work. The quantized model\nis evaluated against YOLOv5-small across multiple metrics, including mean\nAverage Precision (mAP), F1 score, inference time, and model size. Experimental\nresults demonstrate that the quantized YOLOv4-Tiny achieves comparable\ndetection performance while reducing the model size from 22.5 MB to 6.4 MB and\nimproving inference speed by 44\\%. With a 71\\% reduction in model size and a\n44\\% increase in inference speed, the quantized YOLOv4-Tiny model proves highly\nsuitable for real-time emergency detection on low-power edge devices.", "AI": {"tldr": "本文提出了一种轻量级且节能的空中图像目标检测方案，适用于应急响应场景。通过优化YOLOv4-Tiny模型并量化至INT8精度，显著减小模型体积并提升推理速度。", "motivation": "现有公开数据集中缺乏无人机视角的应急图像，因此作者自建了一个包含10,820张标注图像的数据集，并探索如何在低功耗边缘设备上实现实时目标检测。", "method": "采用YOLOv4-Tiny模型，通过后训练量化至INT8精度，并在自建数据集上进行训练和优化。", "result": "量化后的YOLOv4-Tiny模型体积减小71%（从22.5 MB降至6.4 MB），推理速度提升44%，检测性能与YOLOv5-small相当。", "conclusion": "量化后的YOLOv4-Tiny模型适合在低功耗边缘设备上实现实时应急检测，为应急响应提供了高效解决方案。"}}
{"id": "2506.09070", "pdf": "https://arxiv.org/pdf/2506.09070", "abs": "https://arxiv.org/abs/2506.09070", "authors": ["Chenqi Zhang", "Yu Feng", "Jieru Zhao", "Guangda Liu", "Wenchao Ding", "Chentao Wu", "Minyi Guo"], "title": "STREAMINGGS: Voxel-Based Streaming 3D Gaussian Splatting with Memory Optimization and Architectural Support", "categories": ["cs.GR", "cs.AI"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has gained popularity for its efficiency and\nsparse Gaussian-based representation. However, 3DGS struggles to meet the\nreal-time requirement of 90 frames per second (FPS) on resource-constrained\nmobile devices, achieving only 2 to 9 FPS.Existing accelerators focus on\ncompute efficiency but overlook memory efficiency, leading to redundant DRAM\ntraffic. We introduce STREAMINGGS, a fully streaming 3DGS\nalgorithm-architecture co-design that achieves fine-grained pipelining and\nreduces DRAM traffic by transforming from a tile-centric rendering to a\nmemory-centric rendering. Results show that our design achieves up to 45.7\n$\\times$ speedup and 62.9 $\\times$ energy savings over mobile Ampere GPUs.", "AI": {"tldr": "STREAMINGGS是一种针对3D高斯泼溅（3DGS）的算法-架构协同设计，通过内存中心渲染显著提升移动设备上的实时性能。", "motivation": "3DGS在资源受限的移动设备上难以达到90 FPS的实时要求，现有加速器忽略了内存效率，导致DRAM流量冗余。", "method": "提出STREAMINGGS，从基于瓦片的渲染转变为内存中心渲染，实现细粒度流水线并减少DRAM流量。", "result": "设计在移动Ampere GPU上实现了45.7倍加速和62.9倍能耗节省。", "conclusion": "STREAMINGGS通过优化内存效率，显著提升了3DGS在移动设备上的实时性能。"}}
{"id": "2506.09367", "pdf": "https://arxiv.org/pdf/2506.09367", "abs": "https://arxiv.org/abs/2506.09367", "authors": ["Zhengyuan Liu", "Stella Xin Yin", "Dion Hoe-Lian Goh", "Nancy F. Chen"], "title": "COGENT: A Curriculum-oriented Framework for Generating Grade-appropriate Educational Content", "categories": ["cs.CL", "cs.AI"], "comment": "BEA 2025", "summary": "While Generative AI has demonstrated strong potential and versatility in\ncontent generation, its application to educational contexts presents several\nchallenges. Models often fail to align with curriculum standards and maintain\ngrade-appropriate reading levels consistently. Furthermore, STEM education\nposes additional challenges in balancing scientific explanations with everyday\nlanguage when introducing complex and abstract ideas and phenomena to younger\nstudents. In this work, we propose COGENT, a curriculum-oriented framework for\ngenerating grade-appropriate educational content. We incorporate three\ncurriculum components (science concepts, core ideas, and learning objectives),\ncontrol readability through length, vocabulary, and sentence complexity, and\nadopt a ``wonder-based'' approach to increase student engagement and interest.\nWe conduct a multi-dimensional evaluation via both LLM-as-a-judge and human\nexpert analysis. Experimental results show that COGENT consistently produces\ngrade-appropriate passages that are comparable or superior to human references.\nOur work establishes a viable approach for scaling adaptive and high-quality\nlearning resources.", "AI": {"tldr": "COGENT是一个面向课程的框架，用于生成适合年级的教育内容，解决了生成式AI在教育中的挑战，如课程对齐和阅读水平控制。", "motivation": "生成式AI在教育中的应用存在课程标准对齐和阅读水平控制等挑战，尤其是在STEM教育中。", "method": "COGENT结合科学概念、核心思想和学习目标，通过控制文本长度、词汇和句子复杂度，并采用“基于好奇”的方法提升学生兴趣。", "result": "实验表明，COGENT生成的文本在适合年级方面优于或等同于人工参考内容。", "conclusion": "COGENT为扩展自适应高质量学习资源提供了可行方法。"}}
{"id": "2506.09300", "pdf": "https://arxiv.org/pdf/2506.09300", "abs": "https://arxiv.org/abs/2506.09300", "authors": ["Sindhu Boddu", "Arindam Mukherjee"], "title": "Efficient Edge Deployment of Quantized YOLOv4-Tiny for Aerial Emergency Object Detection on Raspberry Pi 5", "categories": ["cs.CV"], "comment": null, "summary": "This paper presents the deployment and performance evaluation of a quantized\nYOLOv4-Tiny model for real-time object detection in aerial emergency imagery on\na resource-constrained edge device the Raspberry Pi 5. The YOLOv4-Tiny model\nwas quantized to INT8 precision using TensorFlow Lite post-training\nquantization techniques and evaluated for detection speed, power consumption,\nand thermal feasibility under embedded deployment conditions. The quantized\nmodel achieved an inference time of 28.2 ms per image with an average power\nconsumption of 13.85 W, demonstrating a significant reduction in power usage\ncompared to its FP32 counterpart. Detection accuracy remained robust across key\nemergency classes such as Ambulance, Police, Fire Engine, and Car Crash. These\nresults highlight the potential of low-power embedded AI systems for real-time\ndeployment in safety-critical emergency response applications.", "AI": {"tldr": "论文展示了在资源受限的边缘设备（Raspberry Pi 5）上部署量化YOLOv4-Tiny模型用于实时目标检测的性能评估，量化后模型在速度和功耗上表现优异。", "motivation": "研究旨在探索低功耗嵌入式AI系统在安全关键应急响应应用中的实时部署潜力。", "method": "使用TensorFlow Lite后训练量化技术将YOLOv4-Tiny模型量化为INT8精度，并在嵌入式条件下评估检测速度、功耗和热可行性。", "result": "量化模型每张图像推理时间为28.2毫秒，平均功耗13.85瓦，相比FP32版本显著降低功耗，同时保持关键应急类别的检测准确性。", "conclusion": "结果表明低功耗嵌入式AI系统在应急响应应用中具有实时部署的潜力。"}}
{"id": "2506.09375", "pdf": "https://arxiv.org/pdf/2506.09375", "abs": "https://arxiv.org/abs/2506.09375", "authors": ["Massa Baali", "Shuo Han", "Syed Abdul Hannan", "Purusottam Samal", "Karanveer Singh", "Soham Deshmukh", "Rita Singh", "Bhiksha Raj"], "title": "CoLMbo: Speaker Language Model for Descriptive Profiling", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Speaker recognition systems are often limited to classification tasks and\nstruggle to generate detailed speaker characteristics or provide context-rich\ndescriptions. These models primarily extract embeddings for speaker\nidentification but fail to capture demographic attributes such as dialect,\ngender, and age in a structured manner. This paper introduces CoLMbo, a Speaker\nLanguage Model (SLM) that addresses these limitations by integrating a speaker\nencoder with prompt-based conditioning. This allows for the creation of\ndetailed captions based on speaker embeddings. CoLMbo utilizes user-defined\nprompts to adapt dynamically to new speaker characteristics and provides\ncustomized descriptions, including regional dialect variations and age-related\ntraits. This innovative approach not only enhances traditional speaker\nprofiling but also excels in zero-shot scenarios across diverse datasets,\nmarking a significant advancement in the field of speaker recognition.", "AI": {"tldr": "CoLMbo是一种新型的说话人语言模型（SLM），通过结合说话人编码器和提示条件，生成详细的说话人描述，解决了传统说话人识别系统在提取人口统计属性方面的局限性。", "motivation": "传统说话人识别系统仅能完成分类任务，无法生成详细的说话人特征或上下文丰富的描述，尤其是人口统计属性（如方言、性别、年龄）的提取不足。", "method": "CoLMbo整合了说话人编码器与提示条件，通过用户定义的提示动态适应新的说话人特征，生成定制化的描述。", "result": "CoLMbo不仅能增强传统说话人分析，还在零样本场景中表现优异，适用于多样化数据集。", "conclusion": "CoLMbo在说话人识别领域实现了显著进步，提供了更灵活和详细的说话人描述能力。"}}
{"id": "2506.09327", "pdf": "https://arxiv.org/pdf/2506.09327", "abs": "https://arxiv.org/abs/2506.09327", "authors": ["Tong Wang", "Guanzhou Chen", "Xiaodong Zhang", "Chenxi Liu", "Jiaqi Wang", "Xiaoliang Tan", "Wenchao Guo", "Qingyuan Yang", "Kaiqi Zhang"], "title": "MSSDF: Modality-Shared Self-supervised Distillation for High-Resolution Multi-modal Remote Sensing Image Learning", "categories": ["cs.CV"], "comment": null, "summary": "Remote sensing image interpretation plays a critical role in environmental\nmonitoring, urban planning, and disaster assessment. However, acquiring\nhigh-quality labeled data is often costly and time-consuming. To address this\nchallenge, we proposes a multi-modal self-supervised learning framework that\nleverages high-resolution RGB images, multi-spectral data, and digital surface\nmodels (DSM) for pre-training. By designing an information-aware adaptive\nmasking strategy, cross-modal masking mechanism, and multi-task self-supervised\nobjectives, the framework effectively captures both the correlations across\ndifferent modalities and the unique feature structures within each modality. We\nevaluated the proposed method on multiple downstream tasks, covering typical\nremote sensing applications such as scene classification, semantic\nsegmentation, change detection, object detection, and depth estimation.\nExperiments are conducted on 15 remote sensing datasets, encompassing 26 tasks.\nThe results demonstrate that the proposed method outperforms existing\npretraining approaches in most tasks. Specifically, on the Potsdam and\nVaihingen semantic segmentation tasks, our method achieved mIoU scores of\n78.30\\% and 76.50\\%, with only 50\\% train-set. For the US3D depth estimation\ntask, the RMSE error is reduced to 0.182, and for the binary change detection\ntask in SECOND dataset, our method achieved mIoU scores of 47.51\\%, surpassing\nthe second CS-MAE by 3 percentage points. Our pretrain code, checkpoints, and\nHR-Pairs dataset can be found in https://github.com/CVEO/MSSDF.", "AI": {"tldr": "提出了一种多模态自监督学习框架，通过自适应掩码策略和多任务目标，显著提升了遥感图像下游任务的性能。", "motivation": "解决高质量标注数据获取成本高、耗时长的问题，利用多模态数据提升模型性能。", "method": "设计了信息感知自适应掩码策略、跨模态掩码机制和多任务自监督目标，结合RGB、多光谱数据和DSM进行预训练。", "result": "在15个数据集、26个任务中表现优异，如Potsdam和Vaihingen语义分割任务mIoU达78.30%和76.50%，US3D深度估计任务RMSE降至0.182。", "conclusion": "该方法在多模态遥感图像任务中优于现有预训练方法，代码和数据集已开源。"}}
{"id": "2506.09381", "pdf": "https://arxiv.org/pdf/2506.09381", "abs": "https://arxiv.org/abs/2506.09381", "authors": ["Austin McCutcheon", "Thiago E. A. de Oliveira", "Aleksandr Zheleznov", "Chris Brogly"], "title": "Binary classification for perceived quality of headlines and links on worldwide news websites, 2018-2024", "categories": ["cs.CL"], "comment": null, "summary": "The proliferation of online news enables potential widespread publication of\nperceived low-quality news headlines/links. As a result, we investigated\nwhether it was possible to automatically distinguish perceived lower-quality\nnews headlines/links from perceived higher-quality headlines/links. We\nevaluated twelve machine learning models on a binary, balanced dataset of\n57,544,214 worldwide news website links/headings from 2018-2024 (28,772,107 per\nclass) with 115 extracted linguistic features. Binary labels for each text were\nderived from scores based on expert consensus regarding the respective news\ndomain quality. Traditional ensemble methods, particularly the bagging\nclassifier, had strong performance (88.1% accuracy, 88.3% F1, 80/20 train/test\nsplit). Fine-tuned DistilBERT achieved the highest accuracy (90.3%, 80/20\ntrain/test split) but required more training time. The results suggest that\nboth NLP features with traditional classifiers and deep learning models can\neffectively differentiate perceived news headline/link quality, with some\ntrade-off between predictive performance and train time.", "AI": {"tldr": "研究探讨了如何自动区分低质量与高质量新闻标题/链接，评估了12种机器学习模型，发现传统集成方法和深度学习模型均有效，但存在性能与训练时间的权衡。", "motivation": "在线新闻的泛滥导致低质量新闻标题/链接的广泛传播，因此研究自动区分其质量的方法具有重要意义。", "method": "使用包含57,544,214条新闻链接/标题的平衡数据集，提取115个语言特征，评估12种机器学习模型，包括传统集成方法和深度学习模型（如DistilBERT）。", "result": "传统集成方法（如bagging分类器）表现良好（88.1%准确率），而微调的DistilBERT表现最佳（90.3%准确率），但训练时间更长。", "conclusion": "NLP特征结合传统分类器或深度学习模型均可有效区分新闻质量，但需权衡预测性能和训练时间。"}}
{"id": "2506.09343", "pdf": "https://arxiv.org/pdf/2506.09343", "abs": "https://arxiv.org/abs/2506.09343", "authors": ["Yuxing Long", "Jiyao Zhang", "Mingjie Pan", "Tianshu Wu", "Taewhan Kim", "Hao Dong"], "title": "CheckManual: A New Challenge and Benchmark for Manual-based Appliance Manipulation", "categories": ["cs.CV", "cs.RO"], "comment": "CVPR 2025 Highlight", "summary": "Correct use of electrical appliances has significantly improved human life\nquality. Unlike simple tools that can be manipulated with common sense,\ndifferent parts of electrical appliances have specific functions defined by\nmanufacturers. If we want the robot to heat bread by microwave, we should\nenable them to review the microwave manual first. From the manual, it can learn\nabout component functions, interaction methods, and representative task steps\nabout appliances. However, previous manual-related works remain limited to\nquestion-answering tasks while existing manipulation researchers ignore the\nmanual's important role and fail to comprehend multi-page manuals. In this\npaper, we propose the first manual-based appliance manipulation benchmark\nCheckManual. Specifically, we design a large model-assisted human-revised data\ngeneration pipeline to create manuals based on CAD appliance models. With these\nmanuals, we establish novel manual-based manipulation challenges, metrics, and\nsimulator environments for model performance evaluation. Furthermore, we\npropose the first manual-based manipulation planning model ManualPlan to set up\na group of baselines for the CheckManual benchmark.", "AI": {"tldr": "提出了首个基于手册的家电操作基准CheckManual，通过大模型辅助生成手册数据，并设计了手册相关的操作挑战、评估指标和仿真环境。", "motivation": "家电的正确使用显著提升生活质量，但现有研究忽视手册的作用，无法处理多页手册。", "method": "设计大模型辅助人工修订的数据生成流程，创建基于CAD模型的手册，并建立手册相关的操作挑战和仿真环境。", "result": "提出了首个手册操作规划模型ManualPlan，为CheckManual基准设立基线。", "conclusion": "CheckManual填补了手册在家电操作研究中的空白，为未来研究提供了新方向。"}}
{"id": "2506.09080", "pdf": "https://arxiv.org/pdf/2506.09080", "abs": "https://arxiv.org/abs/2506.09080", "authors": ["Jiaxiang Chen", "Mingxi Zou", "Zhuo Wang", "Qifan Wang", "Dongning Sun", "Chi Zhang", "Zenglin Xu"], "title": "FinHEAR: Human Expertise and Adaptive Risk-Aware Temporal Reasoning for Financial Decision-Making", "categories": ["cs.LG", "cs.AI", "q-fin.CP"], "comment": null, "summary": "Financial decision-making presents unique challenges for language models,\ndemanding temporal reasoning, adaptive risk assessment, and responsiveness to\ndynamic events. While large language models (LLMs) show strong general\nreasoning capabilities, they often fail to capture behavioral patterns central\nto human financial decisions-such as expert reliance under information\nasymmetry, loss-averse sensitivity, and feedback-driven temporal adjustment. We\npropose FinHEAR, a multi-agent framework for Human Expertise and Adaptive\nRisk-aware reasoning. FinHEAR orchestrates specialized LLM-based agents to\nanalyze historical trends, interpret current events, and retrieve\nexpert-informed precedents within an event-centric pipeline. Grounded in\nbehavioral economics, it incorporates expert-guided retrieval,\nconfidence-adjusted position sizing, and outcome-based refinement to enhance\ninterpretability and robustness. Empirical results on curated financial\ndatasets show that FinHEAR consistently outperforms strong baselines across\ntrend prediction and trading tasks, achieving higher accuracy and better\nrisk-adjusted returns.", "AI": {"tldr": "FinHEAR是一个多智能体框架，结合人类专业知识和自适应风险感知推理，提升语言模型在金融决策中的表现。", "motivation": "语言模型在金融决策中缺乏对人类行为模式（如信息不对称下的专家依赖、损失厌恶和反馈驱动的调整）的捕捉能力。", "method": "FinHEAR通过多智能体框架分析历史趋势、解读当前事件并检索专家知识，结合行为经济学设计专家引导检索、信心调整头寸规模和基于结果的优化。", "result": "在金融数据集上，FinHEAR在趋势预测和交易任务中表现优于基线，准确率和风险调整后收益更高。", "conclusion": "FinHEAR通过结合人类专业知识和自适应风险感知，显著提升了语言模型在金融决策中的表现。"}}
{"id": "2506.09391", "pdf": "https://arxiv.org/pdf/2506.09391", "abs": "https://arxiv.org/abs/2506.09391", "authors": ["Haoran Zhao", "Robert D. Hawkins"], "title": "Comparing human and LLM politeness strategies in free production", "categories": ["cs.CL"], "comment": "25 pages, 5 figures", "summary": "Polite speech poses a fundamental alignment challenge for large language\nmodels (LLMs). Humans deploy a rich repertoire of linguistic strategies to\nbalance informational and social goals -- from positive approaches that build\nrapport (compliments, expressions of interest) to negative strategies that\nminimize imposition (hedging, indirectness). We investigate whether LLMs employ\na similarly context-sensitive repertoire by comparing human and LLM responses\nin both constrained and open-ended production tasks. We find that larger models\n($\\ge$70B parameters) successfully replicate key preferences from the\ncomputational pragmatics literature, and human evaluators surprisingly prefer\nLLM-generated responses in open-ended contexts. However, further linguistic\nanalyses reveal that models disproportionately rely on negative politeness\nstrategies even in positive contexts, potentially leading to\nmisinterpretations. While modern LLMs demonstrate an impressive handle on\npoliteness strategies, these subtle differences raise important questions about\npragmatic alignment in AI systems.", "AI": {"tldr": "大型语言模型（LLMs）在礼貌语言使用上存在对齐挑战，研究发现大模型能复制人类礼貌策略，但过度依赖负面礼貌策略。", "motivation": "研究LLMs是否能在不同语境中灵活运用礼貌策略，平衡信息与社交目标。", "method": "通过比较人类和LLM在约束性和开放性任务中的回答，分析其礼貌策略使用。", "result": "大模型（≥70B参数）能复制人类礼貌偏好，但过度依赖负面策略，可能导致误解。", "conclusion": "LLMs在礼貌策略上表现优异，但细微差异引发对AI系统语用对齐的思考。"}}
{"id": "2506.09345", "pdf": "https://arxiv.org/pdf/2506.09345", "abs": "https://arxiv.org/abs/2506.09345", "authors": ["Songping Wang", "Xiantao Hu", "Yueming Lyu", "Caifeng Shan"], "title": "An Effective End-to-End Solution for Multimodal Action Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Recently, multimodal tasks have strongly advanced the field of action\nrecognition with their rich multimodal information. However, due to the\nscarcity of tri-modal data, research on tri-modal action recognition tasks\nfaces many challenges. To this end, we have proposed a comprehensive multimodal\naction recognition solution that effectively utilizes multimodal information.\nFirst, the existing data are transformed and expanded by optimizing data\nenhancement techniques to enlarge the training scale. At the same time, more\nRGB datasets are used to pre-train the backbone network, which is better\nadapted to the new task by means of transfer learning. Secondly, multimodal\nspatial features are extracted with the help of 2D CNNs and combined with the\nTemporal Shift Module (TSM) to achieve multimodal spatial-temporal feature\nextraction comparable to 3D CNNs and improve the computational efficiency. In\naddition, common prediction enhancement methods, such as Stochastic Weight\nAveraging (SWA), Ensemble and Test-Time augmentation (TTA), are used to\nintegrate the knowledge of models from different training periods of the same\narchitecture and different architectures, so as to predict the actions from\ndifferent perspectives and fully exploit the target information. Ultimately, we\nachieved the Top-1 accuracy of 99% and the Top-5 accuracy of 100% on the\ncompetition leaderboard, demonstrating the superiority of our solution.", "AI": {"tldr": "提出了一种综合多模态动作识别解决方案，通过数据增强、迁移学习、多模态时空特征提取和预测增强方法，实现了高精度动作识别。", "motivation": "由于三模态数据的稀缺性，多模态动作识别任务面临挑战，需要有效利用多模态信息。", "method": "优化数据增强技术扩展训练规模，利用RGB数据集预训练骨干网络，结合2D CNNs和TSM提取多模态时空特征，并采用SWA、Ensemble和TTA等预测增强方法。", "result": "在竞赛排行榜上取得了Top-1准确率99%和Top-5准确率100%。", "conclusion": "该解决方案在多模态动作识别任务中表现出优越性。"}}
{"id": "2506.09393", "pdf": "https://arxiv.org/pdf/2506.09393", "abs": "https://arxiv.org/abs/2506.09393", "authors": ["Xinyi Gao", "Qiucheng Wu", "Yang Zhang", "Xuechen Liu", "Kaizhi Qian", "Ying Xu", "Shiyu Chang"], "title": "A Hierarchical Probabilistic Framework for Incremental Knowledge Tracing in Classroom Settings", "categories": ["cs.CL"], "comment": "24 pages, 4 figures", "summary": "Knowledge tracing (KT) aims to estimate a student's evolving knowledge state\nand predict their performance on new exercises based on performance history.\nMany realistic classroom settings for KT are typically low-resource in data and\nrequire online updates as students' exercise history grows, which creates\nsignificant challenges for existing KT approaches. To restore strong\nperformance under low-resource conditions, we revisit the hierarchical\nknowledge concept (KC) information, which is typically available in many\nclassroom settings and can provide strong prior when data are sparse. We\ntherefore propose Knowledge-Tree-based Knowledge Tracing (KT$^2$), a\nprobabilistic KT framework that models student understanding over a\ntree-structured hierarchy of knowledge concepts using a Hidden Markov Tree\nModel. KT$^2$ estimates student mastery via an EM algorithm and supports\npersonalized prediction through an incremental update mechanism as new\nresponses arrive. Our experiments show that KT$^2$ consistently outperforms\nstrong baselines in realistic online, low-resource settings.", "AI": {"tldr": "论文提出了一种基于知识树的知识追踪框架（KT$^2$），用于在低资源条件下通过层次化知识概念信息提升学生知识状态估计和表现预测的准确性。", "motivation": "现实课堂中的知识追踪通常面临数据稀缺和需要在线更新的挑战，现有方法难以应对。", "method": "采用隐马尔可夫树模型（Hidden Markov Tree Model）建模学生对知识概念的层次化理解，通过EM算法估计学生掌握程度，并支持增量更新机制。", "result": "实验表明，KT$^2$在低资源在线环境中表现优于基线方法。", "conclusion": "KT$^2$通过利用层次化知识概念信息，有效解决了低资源条件下的知识追踪问题。"}}
{"id": "2506.09350", "pdf": "https://arxiv.org/pdf/2506.09350", "abs": "https://arxiv.org/abs/2506.09350", "authors": ["Shanchuan Lin", "Ceyuan Yang", "Hao He", "Jianwen Jiang", "Yuxi Ren", "Xin Xia", "Yang Zhao", "Xuefeng Xiao", "Lu Jiang"], "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video Generation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2", "AI": {"tldr": "提出了一种自回归对抗后训练（AAPT）方法，将预训练的潜在视频扩散模型转化为实时交互式视频生成器，支持单步生成和交互控制。", "motivation": "现有大规模视频生成模型计算密集，难以应用于实时交互场景，因此需要一种高效且支持交互的方法。", "method": "采用自回归对抗训练，单步生成潜在帧，利用KV缓存提升效率，并通过学生强制训练减少长视频生成中的误差累积。", "result": "8B模型在单H100上实现736x416分辨率24fps实时生成，或在8xH100上支持1280x720分辨率长达1分钟（1440帧）。", "conclusion": "AAPT为实时交互视频生成提供了一种高效且可扩展的解决方案。"}}
{"id": "2506.09408", "pdf": "https://arxiv.org/pdf/2506.09408", "abs": "https://arxiv.org/abs/2506.09408", "authors": ["Jui-Ming Yao", "Hao-Yuan Chen", "Zi-Xian Tang", "Bing-Jia Tan", "Sheng-Wei Peng", "Bing-Cheng Xie", "Shun-Feng Su"], "title": "Token Constraint Decoding Improves Robustness on Question Answering for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated impressive performance on\nmultiple-choice question answering (MCQA) benchmarks, yet they remain highly\nvulnerable to minor input perturbations. In this paper, we introduce and\nevaluate Token Constraint Decoding (TCD). This simple yet effective\ninference-time algorithm enforces alignment between token-level predictions to\nenhance robustness in noisy settings. Through extensive experiments on\nCommonsenseQA, MMLU, and MMLU-Pro, we show that TCD, especially when paired\nwith prompt engineering (PE) fixes, significantly restores performance degraded\nby input noise, yielding up to +39\\% absolute gains for weaker models like\nGemma3 1B. Penalty sweep analyses further reveal that TCD implicitly\nregularizes overconfident outputs, with different models requiring distinct\npenalty schedules to maximize resilience. Our findings establish TCD as a\npractical, model-agnostic approach for improving reasoning stability under\nreal-world imperfections and pave the way for more reliable deployment of LLMs\nin safety-critical or user-facing applications.", "AI": {"tldr": "论文提出了一种名为Token Constraint Decoding (TCD)的推理时算法，通过增强令牌级预测的一致性来提高模型在噪声环境中的鲁棒性。实验表明，TCD显著恢复了因输入噪声而下降的性能。", "motivation": "大型语言模型(LLMs)在多选题问答(MCQA)任务中表现优异，但对输入扰动非常敏感。为提高模型在噪声环境中的鲁棒性，作者提出了TCD。", "method": "TCD是一种推理时算法，通过强制令牌级预测的一致性来增强鲁棒性。实验在CommonsenseQA、MMLU和MMLU-Pro等数据集上进行，并结合提示工程(PE)优化。", "result": "TCD显著提升了模型在噪声环境中的性能，尤其是对较弱模型如Gemma3 1B，性能提升高达39%。惩罚分析显示TCD能隐式正则化过度自信的输出。", "conclusion": "TCD是一种实用且模型无关的方法，能显著提升LLMs在现实不完美条件下的推理稳定性，为安全关键或面向用户的应用提供了更可靠的部署方案。"}}
{"id": "2506.09357", "pdf": "https://arxiv.org/pdf/2506.09357", "abs": "https://arxiv.org/abs/2506.09357", "authors": ["Junchao Zhou"], "title": "A new approach for image segmentation based on diffeomorphic registration and gradient fields", "categories": ["cs.CV"], "comment": null, "summary": "Image segmentation is a fundamental task in computer vision aimed at\ndelineating object boundaries within images. Traditional approaches, such as\nedge detection and variational methods, have been widely explored, while recent\nadvances in deep learning have shown promising results but often require\nextensive training data. In this work, we propose a novel variational framework\nfor 2D image segmentation that integrates concepts from shape analysis and\ndiffeomorphic transformations. Our method models segmentation as the\ndeformation of a template curve via a diffeomorphic transformation of the image\ndomain, using the Large Deformation Diffeomorphic Metric Mapping (LDDMM)\nframework. The curve evolution is guided by a loss function that compares the\ndeformed curve to the image gradient field, formulated through the varifold\nrepresentation of geometric shapes. The approach is implemented in Python with\nGPU acceleration using the PyKeops library. This framework allows for accurate\nsegmentation with a flexible and theoretically grounded methodology that does\nnot rely on large datasets.", "AI": {"tldr": "提出了一种基于变分框架和微分同胚变换的新型2D图像分割方法，结合形状分析和LDDMM框架，无需依赖大数据集即可实现精确分割。", "motivation": "传统图像分割方法（如边缘检测和变分方法）已被广泛研究，但深度学习方法需要大量训练数据。本文旨在提出一种灵活且理论扎实的方法，减少对大数据集的依赖。", "method": "通过微分同胚变换将分割建模为模板曲线的变形，利用LDDMM框架和变分表示，结合图像梯度场指导曲线演化，使用PyKeops库实现GPU加速。", "result": "该方法实现了精确的图像分割，且无需依赖大规模数据集。", "conclusion": "提出的变分框架结合微分同胚变换，为图像分割提供了一种灵活且理论可靠的新方法。"}}
{"id": "2506.09414", "pdf": "https://arxiv.org/pdf/2506.09414", "abs": "https://arxiv.org/abs/2506.09414", "authors": ["Xiujun Zhou", "Pingjian Zhang", "Deyou Tang"], "title": "PGDA-KGQA: A Prompt-Guided Generative Framework with Multiple Data Augmentation Strategies for Knowledge Graph Question Answering", "categories": ["cs.CL", "cs.IR"], "comment": "13 pages, 7 figures, 5 tables", "summary": "Knowledge Graph Question Answering (KGQA) is a crucial task in natural\nlanguage processing that requires reasoning over knowledge graphs (KGs) to\nanswer natural language questions. Recent methods utilizing large language\nmodels (LLMs) have shown remarkable semantic parsing capabilities but are\nlimited by the scarcity of diverse annotated data and multi-hop reasoning\nsamples. Traditional data augmentation approaches are focus mainly on\nsingle-hop questions and prone to semantic distortion, while LLM-based methods\nprimarily address semantic distortion but usually neglect multi-hop reasoning,\nthus limiting data diversity. The scarcity of multi-hop samples further weakens\nmodels' generalization. To address these issues, we propose PGDA-KGQA, a\nprompt-guided generative framework with multiple data augmentation strategies\nfor KGQA. At its core, PGDA-KGQA employs a unified prompt-design paradigm: by\ncrafting meticulously engineered prompts that integrate the provided textual\ncontent, it leverages LLMs to generate large-scale (question, logical form)\npairs for model training. Specifically, PGDA-KGQA enriches its training set by:\n(1) generating single-hop pseudo questions to improve the alignment of question\nsemantics with KG relations; (2) applying semantic-preserving question\nrewriting to improve robustness against linguistic variations; (3) employing\nanswer-guided reverse path exploration to create realistic multi-hop questions.\nBy adopting an augment-generate-retrieve semantic parsing pipeline, PGDA-KGQA\nutilizes the augmented data to enhance the accuracy of logical form generation\nand thus improve answer retrieval performance. Experiments demonstrate that\noutperforms state-of-the-art methods on standard KGQA datasets, achieving\nimprovements on WebQSP by 2.8%, 1.2%, and 3.1% and on ComplexWebQuestions by\n1.8%, 1.1%, and 2.4% in F1, Hits@1, and Accuracy, respectively.", "AI": {"tldr": "PGDA-KGQA是一个基于提示引导的生成框架，通过多种数据增强策略解决KGQA任务中数据稀缺和多跳推理问题，显著提升了性能。", "motivation": "解决KGQA任务中数据稀缺、多跳推理样本不足以及传统数据增强方法导致的语义失真问题。", "method": "提出PGDA-KGQA框架，包括统一的提示设计范式，生成单跳伪问题、语义保留的问题重写和答案引导的反向路径探索。", "result": "在WebQSP和ComplexWebQuestions数据集上，F1、Hits@1和准确率分别提升了2.8%、1.2%、3.1%和1.8%、1.1%、2.4%。", "conclusion": "PGDA-KGQA通过数据增强显著提升了KGQA任务的性能，尤其在多跳推理和语义对齐方面表现突出。"}}
{"id": "2506.09363", "pdf": "https://arxiv.org/pdf/2506.09363", "abs": "https://arxiv.org/abs/2506.09363", "authors": ["Hongguang Zhu", "Yunchao Wei", "Mengyu Wang", "Siyu Jiao", "Yan Fang", "Jiannan Huang", "Yao Zhao"], "title": "SAGE: Exploring the Boundaries of Unsafe Concept Domain with Semantic-Augment Erasing", "categories": ["cs.CV", "cs.AI", "cs.CR"], "comment": "Under review", "summary": "Diffusion models (DMs) have achieved significant progress in text-to-image\ngeneration. However, the inevitable inclusion of sensitive information during\npre-training poses safety risks, such as unsafe content generation and\ncopyright infringement. Concept erasing finetunes weights to unlearn\nundesirable concepts, and has emerged as a promising solution. However,\nexisting methods treat unsafe concept as a fixed word and repeatedly erase it,\ntrapping DMs in ``word concept abyss'', which prevents generalized\nconcept-related erasing. To escape this abyss, we introduce semantic-augment\nerasing which transforms concept word erasure into concept domain erasure by\nthe cyclic self-check and self-erasure. It efficiently explores and unlearns\nthe boundary representation of concept domain through semantic spatial\nrelationships between original and training DMs, without requiring additional\npreprocessed data. Meanwhile, to mitigate the retention degradation of\nirrelevant concepts while erasing unsafe concepts, we further propose the\nglobal-local collaborative retention mechanism that combines global semantic\nrelationship alignment with local predicted noise preservation, effectively\nexpanding the retentive receptive field for irrelevant concepts. We name our\nmethod SAGE, and extensive experiments demonstrate the comprehensive\nsuperiority of SAGE compared with other methods in the safe generation of DMs.\nThe code and weights will be open-sourced at\nhttps://github.com/KevinLight831/SAGE.", "AI": {"tldr": "SAGE提出了一种语义增强擦除方法，通过循环自检和自擦除将概念词擦除转化为概念域擦除，解决了现有方法在扩散模型中无法泛化擦除相关概念的问题。", "motivation": "扩散模型在文本到图像生成中取得了显著进展，但预训练中不可避免包含敏感信息，导致安全风险。现有方法将不安全概念视为固定词重复擦除，陷入“词概念深渊”，无法泛化擦除相关概念。", "method": "SAGE通过语义空间关系和循环自检自擦除，将概念词擦除转化为概念域擦除，无需额外预处理数据。同时提出全局-局部协作保留机制，减少无关概念保留退化。", "result": "实验表明，SAGE在扩散模型的安全生成中全面优于其他方法。", "conclusion": "SAGE通过语义增强擦除和全局-局部协作保留机制，有效解决了概念擦除的泛化问题，提升了扩散模型的安全性。"}}
{"id": "2506.09084", "pdf": "https://arxiv.org/pdf/2506.09084", "abs": "https://arxiv.org/abs/2506.09084", "authors": ["Xinyuan Wang", "Liang Wu", "Yanjie Fu"], "title": "Enhanced Whole Page Optimization via Mixed-Grained Reward Mechanism-Adapted Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Optimizing the presentation of search and recommendation results is crucial\nto enhancing user experience and engagement. Whole Page Optimization (WPO)\nplays a pivotal role in this process, as it directly influences how information\nis surfaced to users. While Pre-trained Large Language Models (LLMs) have\ndemonstrated remarkable capabilities in generating coherent and contextually\nrelevant content, fine-tuning these models for complex tasks like WPO presents\nchallenges. Specifically, the need for extensive human-annotated data to\nmitigate issues such as hallucinations and model instability can be\nprohibitively expensive, especially in large-scale systems that interact with\nmillions of items daily. In this work, we address the challenge of fine-tuning\nLLMs for WPO by using user feedback as the supervision. Unlike manually labeled\ndatasets, user feedback is inherently noisy and less precise. To overcome this,\nwe propose a reward-based fine-tuning approach, PageLLM, which employs a\nmixed-grained reward mechanism that combines page-level and item-level rewards.\nThe page-level reward evaluates the overall quality and coherence, while the\nitem-level reward focuses on the accuracy and relevance of key recommendations.\nThis dual-reward structure ensures that both the holistic presentation and the\ncritical individual components are optimized. We validate PageLLM on both\npublic and industrial datasets. PageLLM outperforms baselines and achieves a\n0.44\\% GMV increase in an online A/B test with over 10 million users,\ndemonstrating its real-world impact.", "AI": {"tldr": "论文提出PageLLM，一种基于用户反馈的奖励微调方法，用于优化预训练大语言模型在整页优化（WPO）任务中的表现，结合页面级和项目级奖励机制，显著提升了实际效果。", "motivation": "预训练大语言模型在复杂任务如整页优化（WPO）中面临人工标注数据成本高的问题，用户反馈虽噪声多但可替代。", "method": "提出PageLLM，采用混合粒度的奖励机制（页面级和项目级奖励）微调模型，优化整体展示和关键推荐。", "result": "在公开和工业数据集上验证，PageLLM优于基线模型，并在在线A/B测试中实现0.44%的GMV增长。", "conclusion": "PageLLM通过用户反馈驱动的奖励机制，有效解决了WPO任务中的模型微调难题，具有实际应用价值。"}}
{"id": "2506.09424", "pdf": "https://arxiv.org/pdf/2506.09424", "abs": "https://arxiv.org/abs/2506.09424", "authors": ["Md Messal Monem Miah", "Adrita Anika", "Xi Shi", "Ruihong Huang"], "title": "Hidden in Plain Sight: Evaluation of the Deception Detection Capabilities of LLMs in Multimodal Settings", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Detecting deception in an increasingly digital world is both a critical and\nchallenging task. In this study, we present a comprehensive evaluation of the\nautomated deception detection capabilities of Large Language Models (LLMs) and\nLarge Multimodal Models (LMMs) across diverse domains. We assess the\nperformance of both open-source and commercial LLMs on three distinct datasets:\nreal life trial interviews (RLTD), instructed deception in interpersonal\nscenarios (MU3D), and deceptive reviews (OpSpam). We systematically analyze the\neffectiveness of different experimental setups for deception detection,\nincluding zero-shot and few-shot approaches with random or similarity-based\nin-context example selection. Our results show that fine-tuned LLMs achieve\nstate-of-the-art performance on textual deception detection tasks, while LMMs\nstruggle to fully leverage cross-modal cues. Additionally, we analyze the\nimpact of auxiliary features, such as non-verbal gestures and video summaries,\nand examine the effectiveness of different prompting strategies, including\ndirect label generation and chain-of-thought reasoning. Our findings provide\nkey insights into how LLMs process and interpret deceptive cues across\nmodalities, highlighting their potential and limitations in real-world\ndeception detection applications.", "AI": {"tldr": "论文评估了大型语言模型（LLM）和大型多模态模型（LMM）在自动欺骗检测中的表现，发现微调后的LLM在文本任务中表现优异，而LMM在多模态任务中效果有限。", "motivation": "在数字化时代，欺骗检测至关重要但具有挑战性，研究旨在评估LLM和LMM在不同领域的欺骗检测能力。", "method": "通过零样本和少样本方法，在三个数据集（RLTD、MU3D、OpSpam）上测试LLM和LMM的性能，并分析辅助特征和提示策略的影响。", "result": "微调后的LLM在文本欺骗检测中达到最优水平，而LMM在多模态任务中表现不佳。辅助特征和提示策略对结果有显著影响。", "conclusion": "LLM在欺骗检测中潜力巨大，但LMM需进一步优化以充分利用多模态信息。"}}
{"id": "2506.09369", "pdf": "https://arxiv.org/pdf/2506.09369", "abs": "https://arxiv.org/abs/2506.09369", "authors": ["Zeran Ke", "Bin Tan", "Xianwei Zheng", "Yujun Shen", "Tianfu Wu", "Nan Xue"], "title": "ScaleLSD: Scalable Deep Line Segment Detection Streamlined", "categories": ["cs.CV"], "comment": "accepted to CVPR 2025; 17 pages, appendices included", "summary": "This paper studies the problem of Line Segment Detection (LSD) for the\ncharacterization of line geometry in images, with the aim of learning a\ndomain-agnostic robust LSD model that works well for any natural images. With\nthe focus of scalable self-supervised learning of LSD, we revisit and\nstreamline the fundamental designs of (deep and non-deep) LSD approaches to\nhave a high-performing and efficient LSD learner, dubbed as ScaleLSD, for the\ncuration of line geometry at scale from over 10M unlabeled real-world images.\nOur ScaleLSD works very well to detect much more number of line segments from\nany natural images even than the pioneered non-deep LSD approach, having a more\ncomplete and accurate geometric characterization of images using line segments.\nExperimentally, our proposed ScaleLSD is comprehensively testified under\nzero-shot protocols in detection performance, single-view 3D geometry\nestimation, two-view line segment matching, and multiview 3D line mapping, all\nwith excellent performance obtained. Based on the thorough evaluation, our\nScaleLSD is observed to be the first deep approach that outperforms the\npioneered non-deep LSD in all aspects we have tested, significantly expanding\nand reinforcing the versatility of the line geometry of images. Code and Models\nare available at https://github.com/ant-research/scalelsd", "AI": {"tldr": "本文提出了一种名为ScaleLSD的自监督学习方法，用于高效且高性能的线段检测（LSD），在10M以上未标记的真实图像上表现出色。", "motivation": "研究目标是学习一个领域无关的鲁棒LSD模型，以适用于任何自然图像。", "method": "通过重新设计和简化（深度和非深度）LSD方法的基本设计，提出了一种高效的自监督学习框架ScaleLSD。", "result": "ScaleLSD在零样本协议下表现出色，在检测性能、单视图3D几何估计、双视图线段匹配和多视图3D线映射等方面均优于传统非深度LSD方法。", "conclusion": "ScaleLSD是首个在所有测试方面均超越传统非深度LSD方法的深度方法，显著扩展和增强了图像线几何的通用性。"}}
{"id": "2506.09085", "pdf": "https://arxiv.org/pdf/2506.09085", "abs": "https://arxiv.org/abs/2506.09085", "authors": ["Xinyuan Wang", "Haoyue Bai", "Nanxu Gong", "Wangyang Ying", "Sixun Dong", "Xiquan Cui", "Yanjie Fu"], "title": "LLM-ML Teaming: Integrated Symbolic Decoding and Gradient Search for Valid and Stable Generative Feature Transformation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Feature transformation enhances data representation by deriving new features\nfrom the original data. Generative AI offers potential for this task, but faces\nchallenges in stable generation (consistent outputs) and valid generation\n(error-free sequences). Existing methods--traditional MLs' low validity and\nLLMs' instability--fail to resolve both. We find that LLMs ensure valid syntax,\nwhile ML's gradient-steered search stabilizes performance. To bridge this gap,\nwe propose a teaming framework combining LLMs' symbolic generation with ML's\ngradient optimization. This framework includes four steps: (1) golden examples\ngeneration, aiming to prepare high-quality samples with the ground knowledge of\nthe teacher LLM; (2) feature transformation sequence embedding and search,\nintending to uncover potentially superior embeddings within the latent space;\n(3) student LLM feature transformation, aiming to distill knowledge from the\nteacher LLM; (4) LLM-ML decoder teaming, dedicating to combine ML and the\nstudent LLM probabilities for valid and stable generation. The experiments on\nvarious datasets show that the teaming policy can achieve 5\\% improvement in\ndownstream performance while reducing nearly half of the error cases. The\nresults also demonstrate the efficiency and robustness of the teaming policy.\nAdditionally, we also have exciting findings on LLMs' capacity to understand\nthe original data.", "AI": {"tldr": "提出了一种结合LLMs和ML的团队框架，通过符号生成与梯度优化的结合，解决了特征转换中的稳定性和有效性问题，实验显示性能提升5%，错误率降低近半。", "motivation": "现有方法（传统ML的低有效性和LLMs的不稳定性）无法同时解决特征转换中的稳定性和有效性挑战。", "method": "提出四步团队框架：(1)生成高质量样本；(2)嵌入和搜索特征转换序列；(3)从教师LLM提炼知识；(4)结合ML和学生LLM概率解码。", "result": "实验显示下游性能提升5%，错误率降低近半，同时验证了框架的高效性和鲁棒性。", "conclusion": "团队框架有效结合LLMs和ML的优势，解决了特征转换中的关键问题，并展示了LLMs理解原始数据的能力。"}}
{"id": "2506.09428", "pdf": "https://arxiv.org/pdf/2506.09428", "abs": "https://arxiv.org/abs/2506.09428", "authors": ["Fei Ding", "Baiqiao Wang"], "title": "Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Supervised Fine-Tuning (SFT), while enhancing large language models(LLMs)'\ninstruction-following capabilities and domain-specific task adaptability, often\ndiminishes their general capabilities. Moreover, due to the inaccessibility of\noriginal pre-training data, catastrophic forgetting tends to be exacerbated\nwhen third-party practitioners implement SFT on open-sourced models. To address\nthis challenge, we propose a novel, more cost-effective SFT method which could\neffectively reduce the risk of catastrophic forgetting without access to\noriginal SFT data. Our approach begins by reconstructing the likely SFT\ninstruction distribution of the base model, followed by a multi-model screening\nprocess to select optimal data, which is then mixed with new data for SFT.\nExperimental results demonstrate that our method preserves generalization\ncapabilities in general domains while improving task-specific performance.", "AI": {"tldr": "提出了一种新的监督微调方法，减少灾难性遗忘风险，同时提升任务性能。", "motivation": "监督微调（SFT）虽能增强大语言模型的指令跟随能力，但会削弱其通用能力，且因无法访问原始数据，灾难性遗忘问题加剧。", "method": "通过重建基础模型的SFT指令分布，筛选最优数据并与新数据混合进行SFT。", "result": "实验表明，该方法在保留通用领域能力的同时提升了任务性能。", "conclusion": "该方法为第三方实践者提供了一种更经济有效的SFT解决方案。"}}
{"id": "2506.09378", "pdf": "https://arxiv.org/pdf/2506.09378", "abs": "https://arxiv.org/abs/2506.09378", "authors": ["Qijian Tian", "Xin Tan", "Jingyu Gong", "Yuan Xie", "Lizhuang Ma"], "title": "UniForward: Unified 3D Scene and Semantic Field Reconstruction via Feed-Forward Gaussian Splatting from Only Sparse-View Images", "categories": ["cs.CV"], "comment": null, "summary": "We propose a feed-forward Gaussian Splatting model that unifies 3D scene and\nsemantic field reconstruction. Combining 3D scenes with semantic fields\nfacilitates the perception and understanding of the surrounding environment.\nHowever, key challenges include embedding semantics into 3D representations,\nachieving generalizable real-time reconstruction, and ensuring practical\napplicability by using only images as input without camera parameters or ground\ntruth depth. To this end, we propose UniForward, a feed-forward model to\npredict 3D Gaussians with anisotropic semantic features from only uncalibrated\nand unposed sparse-view images. To enable the unified representation of the 3D\nscene and semantic field, we embed semantic features into 3D Gaussians and\npredict them through a dual-branch decoupled decoder. During training, we\npropose a loss-guided view sampler to sample views from easy to hard,\neliminating the need for ground truth depth or masks required by previous\nmethods and stabilizing the training process. The whole model can be trained\nend-to-end using a photometric loss and a distillation loss that leverages\nsemantic features from a pre-trained 2D semantic model. At the inference stage,\nour UniForward can reconstruct 3D scenes and the corresponding semantic fields\nin real time from only sparse-view images. The reconstructed 3D scenes achieve\nhigh-quality rendering, and the reconstructed 3D semantic field enables the\nrendering of view-consistent semantic features from arbitrary views, which can\nbe further decoded into dense segmentation masks in an open-vocabulary manner.\nExperiments on novel view synthesis and novel view segmentation demonstrate\nthat our method achieves state-of-the-art performances for unifying 3D scene\nand semantic field reconstruction.", "AI": {"tldr": "提出了一种名为UniForward的前馈高斯泼溅模型，统一了3D场景和语义场重建，仅需未校准的稀疏视图图像即可实时完成重建。", "motivation": "结合3D场景与语义场以提升环境感知和理解能力，同时解决嵌入语义到3D表示、实现通用实时重建及仅用图像输入（无需相机参数或深度真值）的挑战。", "method": "通过双分支解耦解码器将语义特征嵌入3D高斯分布，提出损失引导视图采样器优化训练，使用光度损失和蒸馏损失进行端到端训练。", "result": "实验表明，UniForward在3D场景和语义场重建中实现了最先进的性能，支持高质量渲染和视图一致的语义特征解码。", "conclusion": "UniForward为3D场景与语义场的统一重建提供了高效、通用的解决方案，适用于开放词汇的密集分割任务。"}}
{"id": "2506.09089", "pdf": "https://arxiv.org/pdf/2506.09089", "abs": "https://arxiv.org/abs/2506.09089", "authors": ["Xia Li"], "title": "Designing conflict-based communicative tasks in Teaching Chinese as a Foreign Language with ChatGPT", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "in French language", "summary": "In developing the teaching program for a course in Oral Expression in\nTeaching Chinese as a Foreign Language at the university level, the teacher\ndesigns communicative tasks based on conflicts to encourage learners to engage\nin interactive dynamics and develop their oral interaction skills. During the\ndesign of these tasks, the teacher uses ChatGPT to assist in finalizing the\nprogram. This article aims to present the key characteristics of the\ninteractions between the teacher and ChatGPT during this program development\nprocess, as well as to examine the use of ChatGPT and its impacts in this\nspecific context.", "AI": {"tldr": "论文探讨了在大学对外汉语口语教学中，教师如何利用ChatGPT辅助设计基于冲突的交际任务，以提升学习者的口语互动能力，并分析了ChatGPT在此过程中的交互特点及影响。", "motivation": "研究动机在于探索ChatGPT在对外汉语口语教学任务设计中的应用潜力，以及其对教师与学习者互动的影响。", "method": "教师设计基于冲突的交际任务，并利用ChatGPT辅助完成课程设计，分析其交互特点。", "result": "研究展示了ChatGPT在任务设计中的辅助作用，以及其对教学互动的具体影响。", "conclusion": "结论指出ChatGPT在对外汉语口语教学中具有实用价值，但需进一步研究其长期效果。"}}
{"id": "2506.09440", "pdf": "https://arxiv.org/pdf/2506.09440", "abs": "https://arxiv.org/abs/2506.09440", "authors": ["GigaChat team", "Mamedov Valentin", "Evgenii Kosarev", "Gregory Leleytner", "Ilya Shchuckin", "Valeriy Berezovskiy", "Daniil Smirnov", "Dmitry Kozlov", "Sergei Averkiev", "Lukyanenko Ivan", "Aleksandr Proshunin", "Ainur Israfilova", "Ivan Baskov", "Artem Chervyakov", "Emil Shakirov", "Mikhail Kolesov", "Daria Khomich", "Darya Latortseva", "Sergei Porkhun", "Yury Fedorov", "Oleg Kutuzov", "Polina Kudriavtseva", "Sofiia Soldatova", "Kolodin Egor", "Stanislav Pyatkin", "Dzmitry Menshykh", "Grafov Sergei", "Eldar Damirov", "Karlov Vladimir", "Ruslan Gaitukiev", "Arkadiy Shatenov", "Alena Fenogenova", "Nikita Savushkin", "Fedor Minkin"], "title": "GigaChat Family: Efficient Russian Language Modeling Through Mixture of Experts Architecture", "categories": ["cs.CL", "cs.AI"], "comment": "ACL-2025 System Demo", "summary": "Generative large language models (LLMs) have become crucial for modern NLP\nresearch and applications across various languages. However, the development of\nfoundational models specifically tailored to the Russian language has been\nlimited, primarily due to the significant computational resources required.\nThis paper introduces the GigaChat family of Russian LLMs, available in various\nsizes, including base models and instruction-tuned versions. We provide a\ndetailed report on the model architecture, pre-training process, and\nexperiments to guide design choices. In addition, we evaluate their performance\non Russian and English benchmarks and compare GigaChat with multilingual\nanalogs. The paper presents a system demonstration of the top-performing models\naccessible via an API, a Telegram bot, and a Web interface. Furthermore, we\nhave released three open GigaChat models in open-source\n(https://huggingface.co/ai-sage), aiming to expand NLP research opportunities\nand support the development of industrial solutions for the Russian language.", "AI": {"tldr": "该论文介绍了针对俄语的大语言模型GigaChat系列，包括基础模型和指令调优版本，并详细描述了其架构、预训练过程及实验设计。模型在俄语和英语基准测试中表现优异，并与多语言模型进行了比较。论文还展示了通过API、Telegram机器人和Web界面访问的模型演示，并开源了三个模型以支持俄语NLP研究和工业应用。", "motivation": "尽管生成式大语言模型在多语言NLP研究中至关重要，但针对俄语的定制化基础模型发展有限，主要受限于计算资源需求。", "method": "论文提出了GigaChat系列模型，详细介绍了其架构、预训练过程和实验设计，并进行了俄语和英语基准测试及多语言模型比较。", "result": "GigaChat模型在俄语和英语任务中表现优异，并通过API、Telegram机器人和Web界面提供了系统演示。同时开源了三个模型以支持研究和应用。", "conclusion": "GigaChat系列模型填补了俄语大语言模型的空白，为俄语NLP研究和工业应用提供了重要支持。"}}
{"id": "2506.09385", "pdf": "https://arxiv.org/pdf/2506.09385", "abs": "https://arxiv.org/abs/2506.09385", "authors": ["Jialong Zuo", "Yongtai Deng", "Mengdan Tan", "Rui Jin", "Dongyue Wu", "Nong Sang", "Liang Pan", "Changxin Gao"], "title": "ReID5o: Achieving Omni Multi-modal Person Re-identification in a Single Model", "categories": ["cs.CV"], "comment": null, "summary": "In real-word scenarios, person re-identification (ReID) expects to identify a\nperson-of-interest via the descriptive query, regardless of whether the query\nis a single modality or a combination of multiple modalities. However, existing\nmethods and datasets remain constrained to limited modalities, failing to meet\nthis requirement. Therefore, we investigate a new challenging problem called\nOmni Multi-modal Person Re-identification (OM-ReID), which aims to achieve\neffective retrieval with varying multi-modal queries. To address dataset\nscarcity, we construct ORBench, the first high-quality multi-modal dataset\ncomprising 1,000 unique identities across five modalities: RGB, infrared, color\npencil, sketch, and textual description. This dataset also has significant\nsuperiority in terms of diversity, such as the painting perspectives and\ntextual information. It could serve as an ideal platform for follow-up\ninvestigations in OM-ReID. Moreover, we propose ReID5o, a novel multi-modal\nlearning framework for person ReID. It enables synergistic fusion and\ncross-modal alignment of arbitrary modality combinations in a single model,\nwith a unified encoding and multi-expert routing mechanism proposed. Extensive\nexperiments verify the advancement and practicality of our ORBench. A wide\nrange of possible models have been evaluated and compared on it, and our\nproposed ReID5o model gives the best performance. The dataset and code will be\nmade publicly available at https://github.com/Zplusdragon/ReID5o_ORBench.", "AI": {"tldr": "论文提出了一种新的多模态行人重识别问题（OM-ReID），并构建了首个高质量多模态数据集ORBench，同时提出了多模态学习框架ReID5o。", "motivation": "现有方法和数据集在多模态行人重识别任务中表现受限，无法满足实际需求。", "method": "构建ORBench数据集（包含五种模态数据），并提出ReID5o框架，支持任意模态组合的协同融合与跨模态对齐。", "result": "ORBench数据集具有多样性和高质量，ReID5o框架在实验中表现最佳。", "conclusion": "ORBench和ReID5o为多模态行人重识别研究提供了理想平台和解决方案。"}}
{"id": "2506.09092", "pdf": "https://arxiv.org/pdf/2506.09092", "abs": "https://arxiv.org/abs/2506.09092", "authors": ["Wentao Chen", "Jiace Zhu", "Qi Fan", "Yehan Ma", "An Zou"], "title": "CUDA-LLM: LLMs Can Write Efficient CUDA Kernels", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated strong capabilities in\ngeneral-purpose code generation. However, generating the code which is deeply\nhardware-specific, architecture-aware, and performance-critical, especially for\nmassively parallel GPUs, remains a complex challenge. In this work, we explore\nthe use of LLMs for the automated generation and optimization of CUDA programs,\nwith the goal of producing high-performance GPU kernels that fully exploit the\nunderlying hardware. To address this challenge, we propose a novel framework\ncalled \\textbf{Feature Search and Reinforcement (FSR)}. FSR jointly optimizes\ncompilation and functional correctness, as well as the runtime performance,\nwhich are validated through extensive and diverse test cases, and measured by\nactual kernel execution latency on the target GPU, respectively. This approach\nenables LLMs not only to generate syntactically and semantically correct CUDA\ncode but also to iteratively refine it for efficiency, tailored to the\ncharacteristics of the GPU architecture. We evaluate FSR on representative CUDA\nkernels, covering AI workloads and computational intensive algorithms. Our\nresults show that LLMs augmented with FSR consistently guarantee correctness\nrates. Meanwhile, the automatically generated kernels can outperform general\nhuman-written code by a factor of up to 179$\\times$ in execution speeds. These\nfindings highlight the potential of combining LLMs with performance\nreinforcement to automate GPU programming for hardware-specific,\narchitecture-sensitive, and performance-critical applications.", "AI": {"tldr": "LLMs结合FSR框架自动生成和优化CUDA程序，显著提升GPU内核性能，最高可达人类编写代码的179倍。", "motivation": "解决LLMs在生成硬件特定、架构感知和高性能GPU代码方面的挑战。", "method": "提出FSR框架，联合优化编译、功能正确性和运行时性能，通过实际GPU执行验证。", "result": "FSR增强的LLMs生成代码正确率高，性能最高提升179倍。", "conclusion": "LLMs与性能强化结合可自动化GPU编程，适用于高性能关键应用。"}}
{"id": "2506.09450", "pdf": "https://arxiv.org/pdf/2506.09450", "abs": "https://arxiv.org/abs/2506.09450", "authors": ["Prameshwar Thiyagarajan", "Vaishnavi Parimi", "Shamant Sai", "Soumil Garg", "Zhangir Meirbek", "Nitin Yarlagadda", "Kevin Zhu", "Chris Kim"], "title": "UniToMBench: Integrating Perspective-Taking to Improve Theory of Mind in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at Conference of the North American Chapter of the\n  Association for Computational Linguistics, Student Research Workshop 2025\n  (NAACL SRW 2025)", "summary": "Theory of Mind (ToM), the ability to understand the mental states of oneself\nand others, remains a challenging area for large language models (LLMs), which\noften fail to predict human mental states accurately. In this paper, we\nintroduce UniToMBench, a unified benchmark that integrates the strengths of\nSimToM and TOMBENCH to systematically improve and assess ToM capabilities in\nLLMs by integrating multi-interaction task designs and evolving story\nscenarios. Supported by a custom dataset of over 1,000 hand-written scenarios,\nUniToMBench combines perspective-taking techniques with diverse evaluation\nmetrics to better stimulate social cognition in LLMs. Through evaluation, we\nobserve that while models like GPT-4o and GPT-4o Mini show consistently high\naccuracy in tasks involving emotional and belief-related scenarios, with\nresults usually above 80%, there is significant variability in their\nperformance across knowledge-based tasks. These results highlight both the\nstrengths and limitations of current LLMs in ToM-related tasks, underscoring\nthe value of UniToMBench as a comprehensive tool for future development. Our\ncode is publicly available here:\nhttps://github.com/Shamant/unifiedtombenchmark.", "AI": {"tldr": "UniToMBench是一个统一基准，结合SimToM和TOMBENCH的优势，通过多交互任务设计和动态故事场景，系统性提升和评估大语言模型（LLM）的心理理论（ToM）能力。", "motivation": "现有LLM在理解人类心理状态（ToM）方面表现不佳，需要更全面的评估工具。", "method": "UniToMBench整合了1,000多个手工编写场景，结合视角采择技术和多样化评估指标。", "result": "GPT-4o等模型在情感和信念相关任务中表现优异（准确率>80%），但在知识型任务中表现不稳定。", "conclusion": "UniToMBench揭示了当前LLM在ToM任务中的优缺点，为未来研究提供了全面工具。"}}
{"id": "2506.09399", "pdf": "https://arxiv.org/pdf/2506.09399", "abs": "https://arxiv.org/abs/2506.09399", "authors": ["Kaiyu Guo", "Zijian Wang", "Brian C. Lovell", "Mahsa Baktashmotlagh"], "title": "Improving Out-of-Distribution Detection via Dynamic Covariance Calibration", "categories": ["cs.CV"], "comment": null, "summary": "Out-of-Distribution (OOD) detection is essential for the trustworthiness of\nAI systems. Methods using prior information (i.e., subspace-based methods) have\nshown effective performance by extracting information geometry to detect OOD\ndata with a more appropriate distance metric. However, these methods fail to\naddress the geometry distorted by ill-distributed samples, due to the\nlimitation of statically extracting information geometry from the training\ndistribution. In this paper, we argue that the influence of ill-distributed\nsamples can be corrected by dynamically adjusting the prior geometry in\nresponse to new data. Based on this insight, we propose a novel approach that\ndynamically updates the prior covariance matrix using real-time input features,\nrefining its information. Specifically, we reduce the covariance along the\ndirection of real-time input features and constrain adjustments to the residual\nspace, thus preserving essential data characteristics and avoiding effects on\nunintended directions in the principal space. We evaluate our method on two\npre-trained models for the CIFAR dataset and five pre-trained models for\nImageNet-1k, including the self-supervised DINO model. Extensive experiments\ndemonstrate that our approach significantly enhances OOD detection across\nvarious models. The code is released at https://github.com/workerbcd/ooddcc.", "AI": {"tldr": "提出了一种动态调整先验几何信息的方法，通过实时更新协方差矩阵来改进OOD检测性能。", "motivation": "现有基于子空间的方法因静态提取信息几何而无法处理分布不良样本导致的几何失真问题。", "method": "动态更新先验协方差矩阵，沿实时输入特征方向减少协方差，并约束调整到残差空间。", "result": "在CIFAR和ImageNet-1k数据集上显著提升了OOD检测性能。", "conclusion": "动态调整先验几何能有效纠正分布不良样本的影响，提升OOD检测效果。"}}
{"id": "2506.09093", "pdf": "https://arxiv.org/pdf/2506.09093", "abs": "https://arxiv.org/abs/2506.09093", "authors": ["Bingjie Zhang", "Hongkang Li", "Changlong Shi", "Guowei Rong", "He Zhao", "Dongsheng Wang", "Dandan Guo", "Meng Wang"], "title": "Merging Smarter, Generalizing Better: Enhancing Model Merging on OOD Data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Multi-task learning (MTL) concurrently trains a model on diverse task\ndatasets to exploit common features, thereby improving overall performance\nacross the tasks. Recent studies have dedicated efforts to merging multiple\nindependent model parameters into a unified model for MTL, thus circumventing\nthe need for training data and expanding the scope of applicable scenarios of\nMTL. However, current approaches to model merging predominantly concentrate on\nenhancing performance within in-domain (ID) datasets, often overlooking their\nefficacy on out-of-domain (OOD) datasets. In this work, we proposed LwPTV\n(Layer-wise Pruning Task Vector) by building a saliency score, measuring the\nredundancy of parameters in task vectors. Designed in this way ours can achieve\nmask vector for each task and thus perform layer-wise pruning on the task\nvectors, only keeping the pre-trained model parameters at the corresponding\nlayer in merged model. Owing to its flexibility, our method can be seamlessly\nintegrated with most of existing model merging methods to improve their\nperformance on OOD tasks. Extensive experiments demonstrate that the\napplication of our method results in substantial enhancements in OOD\nperformance while preserving the ability on ID tasks.", "AI": {"tldr": "提出了一种名为LwPTV的方法，通过构建显著性分数来修剪任务向量中的冗余参数，提升多任务学习在域外数据集上的性能。", "motivation": "当前多任务学习模型合并方法主要关注域内数据集性能，忽视了域外数据集的表现。", "method": "提出LwPTV方法，通过层间修剪任务向量，保留预训练模型参数，提升域外任务性能。", "result": "实验表明，该方法显著提升了域外任务性能，同时保持了域内任务的能力。", "conclusion": "LwPTV方法灵活且高效，可与其他模型合并方法结合，提升多任务学习的泛化能力。"}}
{"id": "2506.09457", "pdf": "https://arxiv.org/pdf/2506.09457", "abs": "https://arxiv.org/abs/2506.09457", "authors": ["Zeguan Xiao", "Yun Chen", "Guanhua Chen"], "title": "Towards Bridging the Reward-Generation Gap in Direct Alignment Algorithms", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Direct Alignment Algorithms (DAAs), such as Direct Preference Optimization\n(DPO) and Simple Preference Optimization (SimPO), have emerged as efficient\nalternatives to Reinforcement Learning from Human Feedback (RLHF) algorithms\nfor aligning large language models (LLMs) with human preferences. However, DAAs\nsuffer from a fundamental limitation we identify as the \"reward-generation gap\"\n-- a misalignment between optimization objectives during training and actual\ngeneration performance during inference. In this paper, we find a contributor\nto the reward-generation gap is the mismatch between the inherent importance of\nprefix tokens during the LLM generation process and how this importance is\nreflected in the implicit reward functions of DAAs. To bridge the gap, we\nintroduce a simple yet effective approach called Prefix-Oriented Equal-length\nTraining (POET), which truncates both preferred and dispreferred responses to\nmatch the shorter one's length. Training with POET, where both responses in\neach sample are truncated to equal length, resulting in diverse truncated\nlengths across samples, the optimization of DAAs objective is implicitly\nconstrained to converge across all positions, thus paying more attention to\nprefix tokens than the standard DAAs. We conduct experiments with DPO and\nSimPO, two representative DAAs, demonstrating that POET improves over their\nstandard implementations, achieving up to 15.6 points in AlpacaEval 2 and\noverall improvements across downstream tasks. Our results highlight the\nimportance of addressing the misalignment between reward optimization and\ngeneration performance in DAAs.", "AI": {"tldr": "论文提出了一种名为POET的方法，通过截断偏好和非偏好响应至相同长度，解决了直接对齐算法（DAAs）中的奖励生成差距问题，提升了模型性能。", "motivation": "直接对齐算法（DAAs）如DPO和SimPO在训练目标与生成性能之间存在奖励生成差距，导致模型表现不佳。", "method": "提出POET方法，截断偏好和非偏好响应至相同长度，使优化目标更关注前缀标记。", "result": "实验表明POET显著提升了DPO和SimPO的性能，在AlpacaEval 2中最高提升15.6分。", "conclusion": "POET通过解决奖励优化与生成性能之间的不匹配问题，提升了DAAs的效果。"}}
{"id": "2506.09403", "pdf": "https://arxiv.org/pdf/2506.09403", "abs": "https://arxiv.org/abs/2506.09403", "authors": ["Xinya Liu", "Jianghao Wu", "Tao Lu", "Shaoting Zhang", "Guotai Wang"], "title": "SRPL-SFDA: SAM-Guided Reliable Pseudo-Labels for Source-Free Domain Adaptation in Medical Image Segmentation", "categories": ["cs.CV", "I.2.6; I.5.1"], "comment": "18 pages, 4 figures. Accepted for publication in Neurocomputing", "summary": "Domain Adaptation (DA) is crucial for robust deployment of medical image\nsegmentation models when applied to new clinical centers with significant\ndomain shifts. Source-Free Domain Adaptation (SFDA) is appealing as it can deal\nwith privacy concerns and access constraints on source-domain data during\nadaptation to target-domain data. However, SFDA faces challenges such as\ninsufficient supervision in the target domain with unlabeled images. In this\nwork, we propose a Segment Anything Model (SAM)-guided Reliable Pseudo-Labels\nmethod for SFDA (SRPL-SFDA) with three key components: 1) Test-Time Tri-branch\nIntensity Enhancement (T3IE) that not only improves quality of raw\npseudo-labels in the target domain, but also leads to SAM-compatible inputs\nwith three channels to better leverage SAM's zero-shot inference ability for\nrefining the pseudo-labels; 2) A reliable pseudo-label selection module that\nrejects low-quality pseudo-labels based on Consistency of Multiple SAM Outputs\n(CMSO) under input perturbations with T3IE; and 3) A reliability-aware training\nprocedure in the unlabeled target domain where reliable pseudo-labels are used\nfor supervision and unreliable parts are regularized by entropy minimization.\nExperiments conducted on two multi-domain medical image segmentation datasets\nfor fetal brain and the prostate respectively demonstrate that: 1) SRPL-SFDA\neffectively enhances pseudo-label quality in the unlabeled target domain, and\nimproves SFDA performance by leveraging the reliability-aware training; 2)\nSRPL-SFDA outperformed state-of-the-art SFDA methods, and its performance is\nclose to that of supervised training in the target domain. The code of this\nwork is available online: https://github.com/HiLab-git/SRPL-SFDA.", "AI": {"tldr": "提出了一种基于Segment Anything Model（SAM）的SRPL-SFDA方法，用于无源域适应（SFDA），通过增强伪标签质量和可靠性感知训练提升性能。", "motivation": "解决SFDA中目标域无标签数据监督不足的问题，同时应对隐私和源域数据访问限制的挑战。", "method": "1）T3IE增强伪标签质量；2）基于CMSO的可靠伪标签选择；3）可靠性感知训练。", "result": "在两个医学图像分割数据集上表现优于现有SFDA方法，接近目标域监督训练性能。", "conclusion": "SRPL-SFDA有效提升伪标签质量，显著改善SFDA性能，代码已开源。"}}
{"id": "2506.09095", "pdf": "https://arxiv.org/pdf/2506.09095", "abs": "https://arxiv.org/abs/2506.09095", "authors": ["Vivien van Veldhuizen", "Vanessa Botha", "Chunyao Lu", "Melis Erdal Cesur", "Kevin Groot Lipman", "Edwin D. de Jong", "Hugo Horlings", "Clárisa Sanchez", "Cees Snoek", "Ritse Mann", "Eric Marcus", "Jonas Teuwen"], "title": "Foundation Models in Medical Imaging -- A Review and Outlook", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Foundation models (FMs) are changing the way medical images are analyzed by\nlearning from large collections of unlabeled data. Instead of relying on\nmanually annotated examples, FMs are pre-trained to learn general-purpose\nvisual features that can later be adapted to specific clinical tasks with\nlittle additional supervision. In this review, we examine how FMs are being\ndeveloped and applied in pathology, radiology, and ophthalmology, drawing on\nevidence from over 150 studies. We explain the core components of FM pipelines,\nincluding model architectures, self-supervised learning methods, and strategies\nfor downstream adaptation. We also review how FMs are being used in each\nimaging domain and compare design choices across applications. Finally, we\ndiscuss key challenges and open questions to guide future research.", "AI": {"tldr": "综述探讨了基础模型（FMs）在医学影像分析中的应用，包括其开发、核心组件及在病理学、放射学和眼科中的具体应用，并提出了未来研究的挑战。", "motivation": "传统医学影像分析依赖人工标注数据，而FMs通过无监督学习提取通用视觉特征，减少了对标注数据的依赖，提高了效率。", "method": "综述分析了150多项研究，介绍了FMs的模型架构、自监督学习方法及下游任务适配策略，并比较了不同应用领域的设计选择。", "result": "FMs在病理学、放射学和眼科中展现出潜力，能够通过预训练和少量监督适应特定临床任务。", "conclusion": "尽管FMs在医学影像分析中表现优异，但仍存在挑战和未解决问题，需要进一步研究。"}}
{"id": "2506.09495", "pdf": "https://arxiv.org/pdf/2506.09495", "abs": "https://arxiv.org/abs/2506.09495", "authors": ["Ilanit Sobol", "Shir Lissak", "Refael Tikochinski", "Tal Nakash", "Anat Brunstein Klomek", "Eyal Fruchter", "Roi Reichart"], "title": "Bridging Online Behavior and Clinical Insight: A Longitudinal LLM-based Study of Suicidality on YouTube Reveals Novel Digital Markers", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Suicide remains a leading cause of death in Western countries, underscoring\nthe need for new research approaches. As social media becomes central to daily\nlife, digital footprints offer valuable insight into suicidal behavior.\nFocusing on individuals who attempted suicide while uploading videos to their\nchannels, we investigate: How do suicidal behaviors manifest on YouTube, and\nhow do they differ from expert knowledge? We applied complementary approaches:\ncomputational bottom-up, hybrid, and expert-driven top-down, on a novel\nlongitudinal dataset of 181 YouTube channels from individuals with\nlife-threatening attempts, alongside 134 control channels. In the bottom-up\napproach, we applied LLM-based topic modeling to identify behavioral\nindicators. Of 166 topics, five were associated with suicide-attempt, with two\nalso showing temporal attempt-related changes ($p<.01$) - Mental Health\nStruggles ($+0.08$)* and YouTube Engagement ($+0.1$)*. In the hybrid approach,\na clinical expert reviewed LLM-derived topics and flagged 19 as\nsuicide-related. However, none showed significant attempt-related temporal\neffects beyond those identified bottom-up. Notably, YouTube Engagement, a\nplatform-specific indicator, was not flagged by the expert, underscoring the\nvalue of bottom-up discovery. In the top-down approach, psychological\nassessment of suicide attempt narratives revealed that the only significant\ndifference between individuals who attempted before and those attempted during\ntheir upload period was the motivation to share this experience: the former\naimed to Help Others ($\\beta=-1.69$, $p<.01$), while the latter framed it as\npart of their Personal Recovery ($\\beta=1.08$, $p<.01$). By integrating these\napproaches, we offer a nuanced understanding of suicidality, bridging digital\nbehavior and clinical insights.\n  * Within-group changes in relation to the suicide attempt.", "AI": {"tldr": "研究通过分析YouTube上自杀未遂者的视频内容，结合计算模型和专家知识，探讨自杀行为在社交媒体上的表现及其与专家认知的差异。", "motivation": "自杀是西方国家的主要死因之一，社交媒体数据为研究自杀行为提供了新视角。", "method": "采用三种方法：基于LLM的自下而上主题建模、混合方法（专家审查LLM主题）和自上而下的心理评估。", "result": "发现五个与自杀未遂相关的主题，其中两个随时间变化显著；专家未识别的平台特定指标（如YouTube互动）具有价值。", "conclusion": "综合方法揭示了自杀行为的复杂性，为数字行为与临床见解的桥梁提供了新视角。"}}
{"id": "2506.09411", "pdf": "https://arxiv.org/pdf/2506.09411", "abs": "https://arxiv.org/abs/2506.09411", "authors": ["Vaclav Knapp", "Matyas Bohacek"], "title": "Synthetic Human Action Video Data Generation with Pose Transfer", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In video understanding tasks, particularly those involving human motion,\nsynthetic data generation often suffers from uncanny features, diminishing its\neffectiveness for training. Tasks such as sign language translation, gesture\nrecognition, and human motion understanding in autonomous driving have thus\nbeen unable to exploit the full potential of synthetic data. This paper\nproposes a method for generating synthetic human action video data using pose\ntransfer (specifically, controllable 3D Gaussian avatar models). We evaluate\nthis method on the Toyota Smarthome and NTU RGB+D datasets and show that it\nimproves performance in action recognition tasks. Moreover, we demonstrate that\nthe method can effectively scale few-shot datasets, making up for groups\nunderrepresented in the real training data and adding diverse backgrounds. We\nopen-source the method along with RANDOM People, a dataset with videos and\navatars of novel human identities for pose transfer crowd-sourced from the\ninternet.", "AI": {"tldr": "提出一种基于姿态迁移的合成人类动作视频数据生成方法，提升动作识别任务性能，并开源数据集。", "motivation": "合成数据在视频理解任务中常因不真实特征影响训练效果，限制了其在手势识别、自动驾驶等领域的应用潜力。", "method": "使用可控3D高斯虚拟人模型生成合成人类动作视频数据，并在Toyota Smarthome和NTU RGB+D数据集上评估。", "result": "方法提升了动作识别任务性能，并能有效扩展少样本数据集，弥补真实数据中代表性不足的群体。", "conclusion": "该方法为合成数据生成提供了新思路，并开源了数据集RANDOM People，促进相关研究。"}}
{"id": "2506.09096", "pdf": "https://arxiv.org/pdf/2506.09096", "abs": "https://arxiv.org/abs/2506.09096", "authors": ["Chaoyang Zhou", "Shunyu Liu", "Zengmao Wang", "Di Wang", "Rong-Cheng Tu", "Bo Du", "Dacheng Tao"], "title": "Intra-Trajectory Consistency for Reward Modeling", "categories": ["cs.LG", "cs.AI"], "comment": "Under review", "summary": "Reward models are critical for improving large language models (LLMs),\nparticularly in reinforcement learning from human feedback (RLHF) or\ninference-time verification. Current reward modeling typically relies on scores\nof overall responses to learn the outcome rewards for the responses. However,\nsince the response-level scores are coarse-grained supervision signals, the\nreward model struggles to identify the specific components within a response\ntrajectory that truly correlate with the scores, leading to poor generalization\non unseen responses. In this paper, we propose to leverage generation\nprobabilities to establish reward consistency between processes in the response\ntrajectory, which allows the response-level supervisory signal to propagate\nacross processes, thereby providing additional fine-grained signals for reward\nlearning. Building on analysis under the Bayesian framework, we develop an\nintra-trajectory consistency regularization to enforce that adjacent processes\nwith higher next-token generation probability maintain more consistent rewards.\nWe apply the proposed regularization to the advanced outcome reward model,\nimproving its performance on RewardBench. Besides, we show that the reward\nmodel trained with the proposed regularization induces better DPO-aligned\npolicies and achieves better best-of-N (BON) inference-time verification\nresults. Our code is provided in https://github.com/chaoyang101/ICRM.", "AI": {"tldr": "论文提出了一种利用生成概率增强奖励模型的方法，通过响应轨迹中的过程一致性提升奖励学习的细粒度信号，从而改善模型的泛化能力和性能。", "motivation": "现有奖励模型依赖粗粒度的响应级评分，难以识别与评分相关的具体响应组件，导致泛化能力差。", "method": "提出利用生成概率建立响应轨迹中过程间的一致性奖励，并通过贝叶斯框架下的正则化方法强化相邻过程奖励的一致性。", "result": "改进的奖励模型在RewardBench上表现更优，同时提升了DPO对齐策略和BON推理验证效果。", "conclusion": "通过细粒度的奖励一致性正则化，显著提升了奖励模型的性能和泛化能力。"}}
{"id": "2506.09501", "pdf": "https://arxiv.org/pdf/2506.09501", "abs": "https://arxiv.org/abs/2506.09501", "authors": ["Jiayi Yuan", "Hao Li", "Xinheng Ding", "Wenya Xie", "Yu-Jhe Li", "Wentian Zhao", "Kun Wan", "Jing Shi", "Xia Hu", "Zirui Liu"], "title": "Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are now integral across various domains and have\ndemonstrated impressive performance. Progress, however, rests on the premise\nthat benchmark scores are both accurate and reproducible. We demonstrate that\nthe reproducibility of LLM performance is fragile: changing system\nconfiguration such as evaluation batch size, GPU count, and GPU version can\nintroduce significant difference in the generated responses. This issue is\nespecially pronounced in reasoning models, where minor rounding differences in\nearly tokens can cascade into divergent chains of thought, ultimately affecting\naccuracy. For instance, under bfloat16 precision with greedy decoding, a\nreasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9% variation\nin accuracy and 9,000 tokens difference in response length due to differences\nin GPU count, type, and evaluation batch size. We trace the root cause of this\nvariability to the non-associative nature of floating-point arithmetic under\nlimited numerical precision. This work presents the first systematic\ninvestigation into how numerical precision affects reproducibility in LLM\ninference. Through carefully controlled experiments across various hardware,\nsoftware, and precision settings, we quantify when and how model outputs\ndiverge. Our analysis reveals that floating-point precision -- while critical\nfor reproducibility -- is often neglected in evaluation practices. Inspired by\nthis, we develop a lightweight inference pipeline, dubbed LayerCast, that\nstores weights in 16-bit precision but performs all computations in FP32,\nbalancing memory efficiency with numerical stability. Code is available at\nhttps://github.com/nanomaoli/llm_reproducibility.", "AI": {"tldr": "研究发现大语言模型（LLM）的性能复现性脆弱，系统配置如评估批次大小、GPU数量和版本会导致显著差异，尤其是在推理模型中。", "motivation": "验证LLM性能复现性的准确性，揭示系统配置对模型输出的影响。", "method": "通过控制实验分析不同硬件、软件和精度设置下模型输出的变化，并提出轻量级推理管道LayerCast。", "result": "GPU配置和数值精度差异导致模型准确率变化高达9%，响应长度差异达9000词。", "conclusion": "数值精度对LLM复现性至关重要，LayerCast在内存效率和数值稳定性间取得平衡。"}}
{"id": "2506.09416", "pdf": "https://arxiv.org/pdf/2506.09416", "abs": "https://arxiv.org/abs/2506.09416", "authors": ["Xinyu Peng", "Ziyang Zheng", "Yaoming Wang", "Han Li", "Nuowen Kan", "Wenrui Dai", "Chenglin Li", "Junni Zou", "Hongkai Xiong"], "title": "Noise Conditional Variational Score Distillation", "categories": ["cs.CV"], "comment": null, "summary": "We propose Noise Conditional Variational Score Distillation (NCVSD), a novel\nmethod for distilling pretrained diffusion models into generative denoisers. We\nachieve this by revealing that the unconditional score function implicitly\ncharacterizes the score function of denoising posterior distributions. By\nintegrating this insight into the Variational Score Distillation (VSD)\nframework, we enable scalable learning of generative denoisers capable of\napproximating samples from the denoising posterior distribution across a wide\nrange of noise levels. The proposed generative denoisers exhibit desirable\nproperties that allow fast generation while preserve the benefit of iterative\nrefinement: (1) fast one-step generation through sampling from pure Gaussian\nnoise at high noise levels; (2) improved sample quality by scaling the\ntest-time compute with multi-step sampling; and (3) zero-shot probabilistic\ninference for flexible and controllable sampling. We evaluate NCVSD through\nextensive experiments, including class-conditional image generation and inverse\nproblem solving. By scaling the test-time compute, our method outperforms\nteacher diffusion models and is on par with consistency models of larger sizes.\nAdditionally, with significantly fewer NFEs than diffusion-based methods, we\nachieve record-breaking LPIPS on inverse problems.", "AI": {"tldr": "提出了一种名为NCVSD的新方法，将预训练的扩散模型蒸馏为生成去噪器，通过揭示无条件评分函数隐含表征去噪后验分布的评分函数，实现快速生成和迭代优化。", "motivation": "探索如何将扩散模型的高质量生成能力与快速生成需求结合，同时保留迭代优化的优势。", "method": "将无条件评分函数的洞察融入VSD框架，学习生成去噪器，支持从高噪声水平到低噪声水平的后验分布采样。", "result": "NCVSD在类条件图像生成和逆问题求解中表现优异，生成速度和质量优于教师扩散模型，与更大规模的一致性模型相当，且在逆问题上实现了创纪录的LPIPS。", "conclusion": "NCVSD通过结合快速生成和迭代优化，为生成模型和逆问题求解提供了高效且灵活的解决方案。"}}
{"id": "2506.09099", "pdf": "https://arxiv.org/pdf/2506.09099", "abs": "https://arxiv.org/abs/2506.09099", "authors": ["Joshua Barron", "Devin White"], "title": "Too Big to Think: Capacity, Memorization, and Generalization in Pre-Trained Transformers", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Accepted for oral presentation to Tiny Titans: The next wave of\n  On-Device Learning for Foundational Models Workshop at the 42nd International\n  Conference on Machine Learning", "summary": "The relationship between memorization and generalization in large language\nmodels (LLMs) remains an open area of research, with growing evidence that the\ntwo are deeply intertwined. In this work, we investigate this relationship by\npre-training a series of capacity-limited Transformer models from scratch on\ntwo synthetic character-level tasks designed to separately probe generalization\n(via arithmetic extrapolation) and memorization (via factual recall). We\nobserve a consistent trade-off: small models extrapolate to unseen arithmetic\ncases but fail to memorize facts, while larger models memorize but fail to\nextrapolate. An intermediate-capacity model exhibits a similar shift toward\nmemorization. When trained on both tasks jointly, no model (regardless of size)\nsucceeds at extrapolation. These findings suggest that pre-training may\nintrinsically favor one learning mode over the other. By isolating these\ndynamics in a controlled setting, our study offers insight into how model\ncapacity shapes learning behavior and offers broader implications for the\ndesign and deployment of small language models.", "AI": {"tldr": "研究探讨了大型语言模型中记忆与泛化的关系，发现模型容量与学习模式之间存在权衡。", "motivation": "理解记忆与泛化在语言模型中的关系，为模型设计提供指导。", "method": "通过预训练容量受限的Transformer模型，在合成字符级任务上分别测试泛化和记忆能力。", "result": "小模型泛化能力强但记忆能力弱，大模型反之；中等容量模型倾向于记忆；联合训练时所有模型均无法泛化。", "conclusion": "预训练可能固有地偏向一种学习模式，模型容量对学习行为有重要影响。"}}
{"id": "2506.09507", "pdf": "https://arxiv.org/pdf/2506.09507", "abs": "https://arxiv.org/abs/2506.09507", "authors": ["Bingheng Wu", "Jingze Shi", "Yifan Wu", "Nan Tang", "Yuyu Luo"], "title": "TransXSSM: A Hybrid Transformer State Space Model with Unified Rotary Position Embedding", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Transformers exhibit proficiency in capturing long-range dependencies,\nwhereas State Space Models (SSMs) facilitate linear-time sequence modeling.\nNotwithstanding their synergistic potential, the integration of these\narchitectures presents a significant challenge, primarily attributable to a\nfundamental incongruity in their respective positional encoding mechanisms:\nTransformers rely on explicit Rotary Position Embeddings (RoPE), while SSMs\nleverage implicit positional representations via convolutions. This divergence\noften precipitates discontinuities and suboptimal performance. To address this\nimpediment, we propose a unified rotary position embedding (\\textbf{\\ourRoPE})\nmethodology, thereby establishing a consistent positional encoding framework\nfor both self-attention and state-space components. Using this \\ourRoPE, we\nintroduce \\textbf{\\model}, a hybrid architecture that coherently integrates the\nTransformer and SSM layers under this unified positional encoding scheme. At a\n4K sequence length, \\model exhibits training and inference speeds that are\n\\textbf{42.3\\% and 29.5\\% faster}, respectively, relative to standard\nTransformer models. It also delivers higher accuracy: under comparable\nsettings, it surpasses a Transformer baseline by over 4\\% on language modeling\nbenchmarks. \\model furthermore scales more effectively: \\model-1.3B gains\n\\textbf{7.22\\%} in average accuracy over its 320M version (versus about 6\\%\ngains for equivalent Transformers or SSMs). Our results show that unified\npositional encoding resolves positional incompatibility in hybrid models,\nenabling efficient, high-performance long-context modeling.", "AI": {"tldr": "提出了一种统一的旋转位置嵌入方法（\\ourRoPE），解决了Transformer和状态空间模型（SSM）在位置编码上的不兼容问题，并构建了混合架构\\model，显著提升了性能和效率。", "motivation": "Transformer和SSM在长序列建模中各有优势，但两者的位置编码机制存在根本性差异，导致集成时性能不佳。", "method": "提出统一的旋转位置嵌入方法（\\ourRoPE），并基于此构建了混合架构\\model，结合了Transformer和SSM的优势。", "result": "在4K序列长度下，\\model的训练和推理速度分别比标准Transformer快42.3%和29.5%，语言建模任务中准确率提升超过4%。", "conclusion": "统一的位置编码解决了混合模型中的位置不兼容问题，实现了高效且高性能的长上下文建模。"}}
{"id": "2506.09417", "pdf": "https://arxiv.org/pdf/2506.09417", "abs": "https://arxiv.org/abs/2506.09417", "authors": ["Yunxiao Shi", "Yinhao Zhu", "Shizhong Han", "Jisoo Jeong", "Amin Ansari", "Hong Cai", "Fatih Porikli"], "title": "ODG: Occupancy Prediction Using Dual Gaussians", "categories": ["cs.CV"], "comment": null, "summary": "3D occupancy provides fine-grained 3D geometry and semantics for scene\nunderstanding which is critical for autonomous driving. Most existing methods,\nhowever, carry high compute costs, requiring dense 3D feature volume and\ncross-attention to effectively aggregate information. More recent works have\nadopted Bird's Eye View (BEV) or sparse points as scene representation with\nmuch reduced cost, but still suffer from their respective shortcomings. More\nconcretely, BEV struggles with small objects that often experience significant\ninformation loss after being projected to the ground plane. On the other hand,\npoints can flexibly model little objects in 3D, but is inefficient at capturing\nflat surfaces or large objects. To address these challenges, in this paper, we\npresent a novel 3D occupancy prediction approach, ODG, which combines BEV and\nsparse points based representations. We propose a dual-branch design: a\nquery-based sparse points branch and a BEV branch. The 3D information learned\nin the sparse points branch is shared with the BEV stream via cross-attention,\nwhich enriches the weakened signals of difficult objects on the BEV plane. The\noutputs of both branches are finally fused to generate predicted 3D occupancy.\nWe conduct extensive experiments on the Occ3D-nuScenes and Occ3D-Waymo\nbenchmarks that demonstrate the superiority of our proposed ODG. Moreover, ODG\nalso delivers competitive inference speed when compared to the latest efficient\napproaches.", "AI": {"tldr": "论文提出了一种结合BEV和稀疏点表示的新方法ODG，用于3D占用预测，解决了现有方法在小物体和平坦表面上的不足，并在实验中表现出优越性。", "motivation": "现有3D占用预测方法计算成本高，BEV和稀疏点表示各有不足，如BEV对小物体信息丢失严重，稀疏点对平坦表面效率低。", "method": "提出双分支设计：基于查询的稀疏点分支和BEV分支，通过交叉注意力共享信息，最终融合输出预测的3D占用。", "result": "在Occ3D-nuScenes和Occ3D-Waymo基准测试中表现优越，推理速度与最新高效方法相当。", "conclusion": "ODG结合BEV和稀疏点的优势，有效提升了3D占用预测的性能和效率。"}}
{"id": "2506.09101", "pdf": "https://arxiv.org/pdf/2506.09101", "abs": "https://arxiv.org/abs/2506.09101", "authors": ["Míriam Barrabés", "Daniel Mas Montserrat", "Kapal Dev", "Alexander G. Ioannidis"], "title": "Feature Shift Localization Network", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "9 pages, 2 figures, 4 tables", "summary": "Feature shifts between data sources are present in many applications\ninvolving healthcare, biomedical, socioeconomic, financial, survey, and\nmulti-sensor data, among others, where unharmonized heterogeneous data sources,\nnoisy data measurements, or inconsistent processing and standardization\npipelines can lead to erroneous features. Localizing shifted features is\nimportant to address the underlying cause of the shift and correct or filter\nthe data to avoid degrading downstream analysis. While many techniques can\ndetect distribution shifts, localizing the features originating them is still\nchallenging, with current solutions being either inaccurate or not scalable to\nlarge and high-dimensional datasets. In this work, we introduce the Feature\nShift Localization Network (FSL-Net), a neural network that can localize\nfeature shifts in large and high-dimensional datasets in a fast and accurate\nmanner. The network, trained with a large number of datasets, learns to extract\nthe statistical properties of the datasets and can localize feature shifts from\npreviously unseen datasets and shifts without the need for re-training. The\ncode and ready-to-use trained model are available at\nhttps://github.com/AI-sandbox/FSL-Net.", "AI": {"tldr": "FSL-Net是一种神经网络，用于在大规模高维数据中快速准确地定位特征偏移，解决了现有方法不准确或不可扩展的问题。", "motivation": "在医疗、金融等多领域数据中，特征偏移常见且影响下游分析，现有方法难以准确且高效地定位偏移特征。", "method": "提出FSL-Net，通过训练学习数据集的统计特性，无需重新训练即可定位新数据集中的特征偏移。", "result": "FSL-Net能够高效且准确地定位特征偏移，适用于大规模高维数据。", "conclusion": "FSL-Net为解决特征偏移定位问题提供了高效、可扩展的解决方案，代码和预训练模型已开源。"}}
{"id": "2506.09513", "pdf": "https://arxiv.org/pdf/2506.09513", "abs": "https://arxiv.org/abs/2506.09513", "authors": ["Yu Sun", "Xingyu Qian", "Weiwen Xu", "Hao Zhang", "Chenghao Xiao", "Long Li", "Yu Rong", "Wenbing Huang", "Qifeng Bai", "Tingyang Xu"], "title": "ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical Reasoning", "categories": ["cs.CL", "cs.AI", "cs.MA"], "comment": "24 pages, 6 figures, 7 tables", "summary": "Though reasoning-based large language models (LLMs) have excelled in\nmathematics and programming, their capabilities in knowledge-intensive medical\nquestion answering remain underexplored. To address this, we introduce\nReasonMed, the largest medical reasoning dataset, comprising 370k high-quality\nexamples distilled from 1.7 million initial reasoning paths generated by\nvarious LLMs. ReasonMed is constructed through a \\textit{multi-agent\nverification and refinement process}, where we design an \\textit{Error Refiner}\nto enhance the reasoning paths by identifying and correcting error-prone steps\nflagged by a verifier. Leveraging ReasonMed, we systematically investigate best\npractices for training medical reasoning models and find that combining\ndetailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields\nthe most effective fine-tuning strategy. Based on this strategy, we train\nReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the\nprior best by 4.17\\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\\%.", "AI": {"tldr": "ReasonMed是最大的医学推理数据集，通过多代理验证和优化过程构建，用于训练医学推理模型。结合详细推理和简洁答案的微调策略表现最佳，ReasonMed-7B模型在性能上超越现有模型。", "motivation": "探索基于推理的大型语言模型（LLMs）在知识密集型医学问答中的潜力，填补现有研究的空白。", "method": "构建ReasonMed数据集（370k高质量样本），通过多代理验证和优化过程（包括Error Refiner）提升推理路径质量。采用详细推理与简洁答案结合的微调策略。", "result": "ReasonMed-7B模型在性能上超越现有模型，优于之前最佳模型4.17%，并在PubMedQA上超过LLaMA3.1-70B模型4.60%。", "conclusion": "ReasonMed数据集和微调策略显著提升了医学推理模型的性能，为未来研究提供了新基准。"}}
{"id": "2506.09427", "pdf": "https://arxiv.org/pdf/2506.09427", "abs": "https://arxiv.org/abs/2506.09427", "authors": ["Yukang Feng", "Jianwen Sun", "Chuanhao Li", "Zizhen Li", "Jiaxin Ai", "Fanrui Zhang", "Yifan Chang", "Sizhuo Zhou", "Shenglin Zhang", "Yu Dai", "Kaipeng Zhang"], "title": "A High-Quality Dataset and Reliable Evaluation for Interleaved Image-Text Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advancements in Large Multimodal Models (LMMs) have significantly\nimproved multimodal understanding and generation. However, these models still\nstruggle to generate tightly interleaved image-text outputs, primarily due to\nthe limited scale, quality and instructional richness of current training\ndatasets. To address this, we introduce InterSyn, a large-scale multimodal\ndataset constructed using our Self-Evaluation with Iterative Refinement (SEIR)\nmethod. InterSyn features multi-turn, instruction-driven dialogues with tightly\ninterleaved imagetext responses, providing rich object diversity and rigorous\nautomated quality refinement, making it well-suited for training\nnext-generation instruction-following LMMs. Furthermore, to address the lack of\nreliable evaluation tools capable of assessing interleaved multimodal outputs,\nwe introduce SynJudge, an automatic evaluation model designed to quantitatively\nassess multimodal outputs along four dimensions: text content, image content,\nimage quality, and image-text synergy.\n  Experimental studies show that the SEIR method leads to substantially higher\ndataset quality compared to an otherwise identical process without refinement.\n  Moreover, LMMs trained on InterSyn achieve uniform performance gains across\nall evaluation metrics, confirming InterSyn's utility for advancing multimodal\nsystems.", "AI": {"tldr": "论文提出InterSyn数据集和SEIR方法，用于改进多模态模型的训练和评估，显著提升了模型性能。", "motivation": "当前多模态模型在生成紧密交织的图像-文本输出方面表现不佳，主要由于训练数据集规模、质量和指令丰富度不足。", "method": "引入InterSyn数据集，采用SEIR方法进行自评估和迭代优化；同时提出SynJudge自动评估工具，从四个维度评估多模态输出。", "result": "SEIR方法显著提升数据集质量；基于InterSyn训练的模型在所有评估指标上均有提升。", "conclusion": "InterSyn和SEIR方法为多模态系统的进步提供了有效工具。"}}
{"id": "2506.09102", "pdf": "https://arxiv.org/pdf/2506.09102", "abs": "https://arxiv.org/abs/2506.09102", "authors": ["Mihaela van der Schaar", "Richard Peck", "Eoin McKinney", "Jim Weatherall", "Stuart Bailey", "Justine Rochon", "Chris Anagnostopoulos", "Pierre Marquet", "Anthony Wood", "Nicky Best", "Harry Amad", "Julianna Piskorz", "Krzysztof Kacprzyk", "Rafik Salama", "Christina Gunther", "Francesca Frau", "Antoine Pugeat", "Ramon Hernandez"], "title": "Revolutionizing Clinical Trials: A Manifesto for AI-Driven Transformation", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "This manifesto represents a collaborative vision forged by leaders in\npharmaceuticals, consulting firms, clinical research, and AI. It outlines a\nroadmap for two AI technologies - causal inference and digital twins - to\ntransform clinical trials, delivering faster, safer, and more personalized\noutcomes for patients. By focusing on actionable integration within existing\nregulatory frameworks, we propose a way forward to revolutionize clinical\nresearch and redefine the gold standard for clinical trials using AI.", "AI": {"tldr": "本文提出了一种通过因果推断和数字孪生技术革新临床试验的路线图，旨在实现更快、更安全、更个性化的患者结果。", "motivation": "通过整合AI技术（因果推断和数字孪生）来提升临床试验的效率与个性化，同时确保符合现有监管框架。", "method": "提出了一种基于因果推断和数字孪生技术的AI整合方法，重点关注在现有监管框架内的可操作性。", "result": "预计通过AI技术革新临床试验，重新定义其黄金标准。", "conclusion": "本文为利用AI技术（因果推断和数字孪生）革新临床研究提供了明确的路线图和愿景。"}}
{"id": "2506.09542", "pdf": "https://arxiv.org/pdf/2506.09542", "abs": "https://arxiv.org/abs/2506.09542", "authors": ["Dingjun Wu", "Yukun Yan", "Zhenghao Liu", "Zhiyuan Liu", "Maosong Sun"], "title": "KG-Infused RAG: Augmenting Corpus-Based RAG with External Knowledge Graphs", "categories": ["cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) improves factual accuracy by grounding\nresponses in external knowledge. However, existing methods typically rely on a\nsingle source, either unstructured text or structured knowledge. Moreover, they\nlack cognitively inspired mechanisms for activating relevant knowledge. To\naddress these issues, we propose KG-Infused RAG, a framework that integrates\nKGs into RAG systems to implement spreading activation, a cognitive process\nthat enables concept association and inference. KG-Infused RAG retrieves KG\nfacts, expands the query accordingly, and enhances generation by combining\ncorpus passages with structured facts, enabling interpretable, multi-source\nretrieval grounded in semantic structure. We further improve KG-Infused RAG via\npreference learning on sampled key stages in the pipeline. Experiments on five\nQA benchmarks show that KG-Infused RAG consistently outperforms vanilla RAG (by\n3.8% to 13.8%). Additionally, when integrated into Self-RAG, KG-Infused RAG\nbrings further performance gains, demonstrating its effectiveness and\nversatility as a plug-and-play enhancement module for corpus-based RAG methods.", "AI": {"tldr": "KG-Infused RAG通过结合知识图谱（KG）和检索增强生成（RAG）框架，利用认知启发的扩散激活机制，显著提升了事实准确性。", "motivation": "现有RAG方法通常依赖单一知识源，缺乏认知启发的知识激活机制，限制了其性能。", "method": "提出KG-Infused RAG框架，结合KG事实检索、查询扩展和多源生成，并通过偏好学习优化关键阶段。", "result": "在五个QA基准测试中，KG-Infused RAG性能优于传统RAG（提升3.8%至13.8%），并可作为插件增强其他RAG方法。", "conclusion": "KG-Infused RAG是一种高效且通用的增强模块，能够显著提升基于语料的RAG方法的性能。"}}
{"id": "2506.09429", "pdf": "https://arxiv.org/pdf/2506.09429", "abs": "https://arxiv.org/abs/2506.09429", "authors": ["Swadhin Das", "Divyansh Mundra", "Priyanshu Dayal", "Raksha Sharma"], "title": "A Novel Lightweight Transformer with Edge-Aware Fusion for Remote Sensing Image Captioning", "categories": ["cs.CV"], "comment": null, "summary": "Transformer-based models have achieved strong performance in remote sensing\nimage captioning by capturing long-range dependencies and contextual\ninformation. However, their practical deployment is hindered by high\ncomputational costs, especially in multi-modal frameworks that employ separate\ntransformer-based encoders and decoders. In addition, existing remote sensing\nimage captioning models primarily focus on high-level semantic extraction while\noften overlooking fine-grained structural features such as edges, contours, and\nobject boundaries. To address these challenges, a lightweight transformer\narchitecture is proposed by reducing the dimensionality of the encoder layers\nand employing a distilled version of GPT-2 as the decoder. A knowledge\ndistillation strategy is used to transfer knowledge from a more complex teacher\nmodel to improve the performance of the lightweight network. Furthermore, an\nedge-aware enhancement strategy is incorporated to enhance image representation\nand object boundary understanding, enabling the model to capture fine-grained\nspatial details in remote sensing images. Experimental results demonstrate that\nthe proposed approach significantly improves caption quality compared to\nstate-of-the-art methods.", "AI": {"tldr": "提出了一种轻量级Transformer架构，通过降低编码器维度并使用蒸馏版GPT-2解码器，结合知识蒸馏和边缘感知增强策略，显著提升了遥感图像描述的质量。", "motivation": "解决Transformer模型在遥感图像描述中计算成本高和忽略细粒度结构特征的问题。", "method": "采用轻量级Transformer架构，降低编码器维度，使用蒸馏版GPT-2解码器，结合知识蒸馏和边缘感知增强策略。", "result": "实验结果表明，该方法显著优于现有技术。", "conclusion": "提出的方法在计算效率和细粒度特征捕捉方面表现优异，为遥感图像描述提供了实用解决方案。"}}
{"id": "2506.09104", "pdf": "https://arxiv.org/pdf/2506.09104", "abs": "https://arxiv.org/abs/2506.09104", "authors": ["Jung Hyun Lee", "Seungjae Shin", "Vinnam Kim", "Jaeseong You", "An Chen"], "title": "Unifying Block-wise PTQ and Distillation-based QAT for Progressive Quantization toward 2-bit Instruction-Tuned LLMs", "categories": ["cs.LG", "cs.AI"], "comment": "Preprint", "summary": "As the rapid scaling of large language models (LLMs) poses significant\nchallenges for deployment on resource-constrained devices, there is growing\ninterest in extremely low-bit quantization, such as 2-bit. Although prior works\nhave shown that 2-bit large models are pareto-optimal over their 4-bit smaller\ncounterparts in both accuracy and latency, these advancements have been limited\nto pre-trained LLMs and have not yet been extended to instruction-tuned models.\nTo bridge this gap, we propose Unified Progressive Quantization (UPQ)$-$a novel\nprogressive quantization framework (FP16$\\rightarrow$INT4$\\rightarrow$INT2)\nthat unifies block-wise post-training quantization (PTQ) with\ndistillation-based quantization-aware training (Distill-QAT) for INT2\ninstruction-tuned LLM quantization. UPQ first quantizes FP16 instruction-tuned\nmodels to INT4 using block-wise PTQ to significantly reduce the quantization\nerror introduced by subsequent INT2 quantization. Next, UPQ applies Distill-QAT\nto enable INT2 instruction-tuned LLMs to generate responses consistent with\ntheir original FP16 counterparts by minimizing the generalized Jensen-Shannon\ndivergence (JSD) between the two. To the best of our knowledge, we are the\nfirst to demonstrate that UPQ can quantize open-source instruction-tuned LLMs\nto INT2 without relying on proprietary post-training data, while achieving\nstate-of-the-art performances on MMLU and IFEval$-$two of the most\nrepresentative benchmarks for evaluating instruction-tuned LLMs.", "AI": {"tldr": "论文提出了一种名为UPQ的统一渐进量化框架，用于将指令调优的大语言模型（LLM）从FP16量化到INT2，结合了块级后训练量化和蒸馏量化感知训练，显著提升了性能。", "motivation": "由于大语言模型在资源受限设备上的部署面临挑战，低比特量化（如2-bit）成为研究热点。然而，现有方法仅限于预训练模型，未扩展到指令调优模型。", "method": "UPQ框架分两步：1）使用块级后训练量化将FP16模型量化为INT4；2）通过蒸馏量化感知训练进一步量化为INT2，最小化广义Jensen-Shannon散度。", "result": "UPQ首次实现了开源指令调优LLM的INT2量化，无需依赖专有数据，在MMLU和IFEval基准测试中达到最优性能。", "conclusion": "UPQ为指令调优LLM的低比特量化提供了有效解决方案，显著提升了部署效率。"}}
{"id": "2506.09556", "pdf": "https://arxiv.org/pdf/2506.09556", "abs": "https://arxiv.org/abs/2506.09556", "authors": ["Georgios Chatzichristodoulou", "Despoina Kosmopoulou", "Antonios Kritikos", "Anastasia Poulopoulou", "Efthymios Georgiou", "Athanasios Katsamanis", "Vassilis Katsouros", "Alexandros Potamianos"], "title": "MEDUSA: A Multimodal Deep Fusion Multi-Stage Training Framework for Speech Emotion Recognition in Naturalistic Conditions", "categories": ["cs.CL"], "comment": "Accepted at Interspeech 2025", "summary": "SER is a challenging task due to the subjective nature of human emotions and\ntheir uneven representation under naturalistic conditions. We propose MEDUSA, a\nmultimodal framework with a four-stage training pipeline, which effectively\nhandles class imbalance and emotion ambiguity. The first two stages train an\nensemble of classifiers that utilize DeepSER, a novel extension of a deep\ncross-modal transformer fusion mechanism from pretrained self-supervised\nacoustic and linguistic representations. Manifold MixUp is employed for further\nregularization. The last two stages optimize a trainable meta-classifier that\ncombines the ensemble predictions. Our training approach incorporates human\nannotation scores as soft targets, coupled with balanced data sampling and\nmultitask learning. MEDUSA ranked 1st in Task 1: Categorical Emotion\nRecognition in the Interspeech 2025: Speech Emotion Recognition in Naturalistic\nConditions Challenge.", "AI": {"tldr": "MEDUSA是一个多模态框架，通过四阶段训练流程有效处理类别不平衡和情感模糊问题，在Interspeech 2025挑战赛中排名第一。", "motivation": "由于人类情感的主观性及其在自然条件下的不均匀表现，SER任务具有挑战性。", "method": "采用四阶段训练流程，前两阶段训练基于DeepSER的集成分类器，后两阶段优化可训练的元分类器，结合了多种正则化和多任务学习。", "result": "MEDUSA在Interspeech 2025挑战赛中排名第一。", "conclusion": "MEDUSA通过多模态和集成学习方法，显著提升了SER任务的性能。"}}
{"id": "2506.09445", "pdf": "https://arxiv.org/pdf/2506.09445", "abs": "https://arxiv.org/abs/2506.09445", "authors": ["Ayush Gupta", "Anirban Roy", "Rama Chellappa", "Nathaniel D. Bastian", "Alvaro Velasquez", "Susmit Jha"], "title": "TOGA: Temporally Grounded Open-Ended Video QA with Weak Supervision", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We address the problem of video question answering (video QA) with temporal\ngrounding in a weakly supervised setup, without any temporal annotations. Given\na video and a question, we generate an open-ended answer grounded with the\nstart and end time. For this task, we propose TOGA: a vision-language model for\nTemporally Grounded Open-Ended Video QA with Weak Supervision. We instruct-tune\nTOGA to jointly generate the answer and the temporal grounding. We operate in a\nweakly supervised setup where the temporal grounding annotations are not\navailable. We generate pseudo labels for temporal grounding and ensure the\nvalidity of these labels by imposing a consistency constraint between the\nquestion of a grounding response and the response generated by a question\nreferring to the same temporal segment. We notice that jointly generating the\nanswers with the grounding improves performance on question answering as well\nas grounding. We evaluate TOGA on grounded QA and open-ended QA tasks. For\ngrounded QA, we consider the NExT-GQA benchmark which is designed to evaluate\nweakly supervised grounded question answering. For open-ended QA, we consider\nthe MSVD-QA and ActivityNet-QA benchmarks. We achieve state-of-the-art\nperformance for both tasks on these benchmarks.", "AI": {"tldr": "论文提出了一种弱监督下的视频问答（video QA）与时序定位方法TOGA，无需时序标注即可生成开放答案及其时间范围。", "motivation": "解决无时序标注的弱监督视频问答问题，同时生成答案和时序定位。", "method": "提出TOGA模型，通过指令调优联合生成答案和时序定位，利用伪标签和一致性约束确保时序标签的有效性。", "result": "在NExT-GQA、MSVD-QA和ActivityNet-QA基准测试中达到最先进性能。", "conclusion": "联合生成答案与时序定位能提升问答和定位性能，TOGA在弱监督下表现优异。"}}
{"id": "2506.09105", "pdf": "https://arxiv.org/pdf/2506.09105", "abs": "https://arxiv.org/abs/2506.09105", "authors": ["Javier Lopez-Piqueres", "Pranav Deshpande", "Archan Ray", "Mattia J. Villani", "Marco Pistoia", "Niraj Kumar"], "title": "MetaTT: A Global Tensor-Train Adapter for Parameter-Efficient Fine-Tuning", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": null, "summary": "We present MetaTT, a unified Tensor Train (TT) adapter framework for global\nlow-rank fine-tuning of pre-trained transformers. Unlike LoRA, which fine-tunes\neach weight matrix independently, MetaTT uses a single shared TT to factorize\nall transformer sub-modules -- query, key, value, projection, and feed-forward\nlayers -- by indexing the structural axes like layer and matrix type, and\noptionally heads and tasks. For a given rank, while LoRA adds parameters\nproportional to the product across modes, MetaTT only adds parameters\nproportional to the sum across modes leading to a significantly compressed\nfinal adapter. Our benchmarks compare MetaTT with LoRA along with recent\nstate-of-the-art matrix and tensor decomposition based fine-tuning schemes. We\nobserve that when tested on standard language modeling benchmarks, MetaTT leads\nto the most reduction in the parameters while maintaining similar accuracy to\nLoRA and even outperforming other tensor-based methods. Unlike CP or other\nrank-factorizations, the TT ansatz benefits from mature optimization routines\n-- e.g., DMRG-style rank adaptive minimization in addition to Adam, which we\nfind simplifies training. Because new modes can be appended cheaply, MetaTT\nnaturally extends to shared adapters across many tasks without redesigning the\ncore tensor.", "AI": {"tldr": "MetaTT是一个统一的Tensor Train（TT）适配器框架，用于预训练Transformer的全局低秩微调。相比LoRA，MetaTT通过共享TT分解所有子模块，显著减少了参数数量，同时保持准确性。", "motivation": "解决现有微调方法（如LoRA）参数冗余的问题，通过共享TT结构实现更高效的参数压缩和任务扩展。", "method": "使用Tensor Train（TT）分解所有Transformer子模块（如query、key、value等），并通过索引结构轴（如层和矩阵类型）实现共享。支持动态扩展任务模式。", "result": "在标准语言建模基准测试中，MetaTT显著减少参数数量，同时保持与LoRA相似的准确性，甚至优于其他基于张量的方法。", "conclusion": "MetaTT通过TT分解和共享结构，实现了高效的参数压缩和任务扩展，同时简化了训练过程。"}}
{"id": "2506.09558", "pdf": "https://arxiv.org/pdf/2506.09558", "abs": "https://arxiv.org/abs/2506.09558", "authors": ["Eleni Gkovedarou", "Joke Daems", "Luna De Bruyne"], "title": "Gender Bias in English-to-Greek Machine Translation", "categories": ["cs.CL"], "comment": "Accepted at GITT 2025 (MT Summit)", "summary": "As the demand for inclusive language increases, concern has grown over the\nsusceptibility of machine translation (MT) systems to reinforce gender\nstereotypes. This study investigates gender bias in two commercial MT systems,\nGoogle Translate and DeepL, focusing on the understudied English-to-Greek\nlanguage pair. We address three aspects of gender bias: i) male bias, ii)\noccupational stereotyping, and iii) errors in anti-stereotypical translations.\nAdditionally, we explore the potential of prompted GPT-4o as a bias mitigation\ntool that provides both gender-explicit and gender-neutral alternatives when\nnecessary. To achieve this, we introduce GendEL, a manually crafted bilingual\ndataset of 240 gender-ambiguous and unambiguous sentences that feature\nstereotypical occupational nouns and adjectives. We find persistent gender bias\nin translations by both MT systems; while they perform well in cases where\ngender is explicitly defined, with DeepL outperforming both Google Translate\nand GPT-4o in feminine gender-unambiguous sentences, they are far from\nproducing gender-inclusive or neutral translations when the gender is\nunspecified. GPT-4o shows promise, generating appropriate gendered and neutral\nalternatives for most ambiguous cases, though residual biases remain evident.", "AI": {"tldr": "研究探讨了Google Translate和DeepL在英语到希腊语翻译中的性别偏见问题，并测试了GPT-4o作为缓解工具的效果。", "motivation": "随着对包容性语言需求的增加，机器翻译系统可能强化性别刻板印象的问题引起关注。", "method": "研究使用GendEL数据集（240个句子），分析MT系统在男性偏见、职业刻板印象和反刻板翻译错误三方面的表现，并测试GPT-4o的缓解能力。", "result": "MT系统在性别明确时表现良好，但在未指定性别时难以实现包容性翻译；GPT-4o在多数模糊情况下能生成合适的性别化或中性替代方案。", "conclusion": "MT系统仍存在性别偏见，GPT-4o显示出潜力，但仍有残余偏见。"}}
{"id": "2506.09446", "pdf": "https://arxiv.org/pdf/2506.09446", "abs": "https://arxiv.org/abs/2506.09446", "authors": ["Yuhe Ding", "Jian Liang", "Bo Jiang", "Zi Wang", "Aihua Zheng", "Bin Luo"], "title": "Harmonizing and Merging Source Models for CLIP-based Domain Generalization", "categories": ["cs.CV"], "comment": null, "summary": "CLIP-based domain generalization aims to improve model generalization to\nunseen domains by leveraging the powerful zero-shot classification capabilities\nof CLIP and multiple source datasets. Existing methods typically train a single\nmodel across multiple source domains to capture domain-shared information.\nHowever, this paradigm inherently suffers from two types of conflicts: 1)\nsample conflicts, arising from noisy samples and extreme domain shifts among\nsources; and 2) optimization conflicts, stemming from competition and\ntrade-offs during multi-source training. Both hinder the generalization and\nlead to suboptimal solutions. Recent studies have shown that model merging can\neffectively mitigate the competition of multi-objective optimization and\nimprove generalization performance. Inspired by these findings, we propose\nHarmonizing and Merging (HAM), a novel source model merging framework for\nCLIP-based domain generalization. During the training process of the source\nmodels, HAM enriches the source samples without conflicting samples, and\nharmonizes the update directions of all models. Then, a redundancy-aware\nhistorical model merging method is introduced to effectively integrate\nknowledge across all source models. HAM comprehensively consolidates source\ndomain information while enabling mutual enhancement among source models,\nultimately yielding a final model with optimal generalization capabilities.\nExtensive experiments on five widely used benchmark datasets demonstrate the\neffectiveness of our approach, achieving state-of-the-art performance.", "AI": {"tldr": "HAM是一种基于CLIP的领域泛化框架，通过无冲突样本和模型合并提升泛化性能。", "motivation": "现有方法在多源训练中存在样本冲突和优化冲突，导致泛化能力受限。", "method": "HAM在训练中避免冲突样本，协调模型更新方向，并引入冗余感知的历史模型合并方法。", "result": "在五个基准数据集上实现最优性能。", "conclusion": "HAM通过协调和合并源模型，显著提升了领域泛化能力。"}}
{"id": "2506.09107", "pdf": "https://arxiv.org/pdf/2506.09107", "abs": "https://arxiv.org/abs/2506.09107", "authors": ["Athena Vakali", "Ilias Dimitriadis"], "title": "FAIRTOPIA: Envisioning Multi-Agent Guardianship for Disrupting Unfair AI Pipelines", "categories": ["cs.CY", "cs.AI"], "comment": "11 pages, 4 figures", "summary": "AI models have become active decision makers, often acting without human\nsupervision. The rapid advancement of AI technology has already caused harmful\nincidents that have hurt individuals and societies and AI unfairness in heavily\ncriticized. It is urgent to disrupt AI pipelines which largely neglect human\nprinciples and focus on computational biases exploration at the data (pre),\nmodel(in), and deployment (post) processing stages. We claim that by exploiting\nthe advances of agents technology, we will introduce cautious, prompt, and\nongoing fairness watch schemes, under realistic, systematic, and human-centric\nfairness expectations. We envision agents as fairness guardians, since agents\nlearn from their environment, adapt to new information, and solve complex\nproblems by interacting with external tools and other systems. To set the\nproper fairness guardrails in the overall AI pipeline, we introduce a\nfairness-by-design approach which embeds multi-role agents in an end-to-end\n(human to AI) synergetic scheme. Our position is that we may design adaptive\nand realistic AI fairness frameworks, and we introduce a generalized algorithm\nwhich can be customized to the requirements and goals of each AI decision\nmaking scenario. Our proposed, so called FAIRTOPIA framework, is structured\nover a three-layered architecture, which encapsulates the AI pipeline inside an\nagentic guardian and a knowledge-based, self-refining layered scheme. Based on\nour proposition, we enact fairness watch in all of the AI pipeline stages,\nunder robust multi-agent workflows, which will inspire new fairness research\nhypothesis, heuristics, and methods grounded in human-centric, systematic,\ninterdisciplinary, socio-technical principles.", "AI": {"tldr": "论文提出了一种名为FAIRTOPIA的框架，通过多角色代理技术嵌入AI流程，实现端到端的公平性保障。", "motivation": "AI模型作为主动决策者常忽视人类原则，导致不公平事件频发，亟需在数据、模型和部署阶段引入公平性保障。", "method": "采用基于代理的公平性设计方法，嵌入三层架构的FAIRTOPIA框架，通过多代理工作流实现全流程公平性监控。", "result": "提出了一个可定制化的通用算法，并设计了FAIRTOPIA框架，能够在AI决策场景中实现自适应和现实的公平性。", "conclusion": "通过代理技术嵌入公平性保障，有望推动基于人类中心、系统化和跨学科的公平性研究。"}}
{"id": "2506.09560", "pdf": "https://arxiv.org/pdf/2506.09560", "abs": "https://arxiv.org/abs/2506.09560", "authors": ["Stefan Krsteski", "Matea Tashkovska", "Borjan Sazdov", "Hristijan Gjoreski", "Branislav Gerazov"], "title": "Towards Open Foundation Language Model and Corpus for Macedonian: A Low-Resource Language", "categories": ["cs.CL"], "comment": "Camera-ready version accepted at SlavNLP-2025@ACL", "summary": "The increase in technological adoption worldwide comes with demands for novel\ntools to be used by the general population. Large Language Models (LLMs)\nprovide a great opportunity in this respect, but their capabilities remain\nlimited for low-resource languages, restricting applications in countries where\nsuch languages are spoken. We create several resources to facilitate the\nadoption of LLMs and to support research advancements for Macedonian. We\ncollect the largest Macedonian corpus to date, consisting of 40GB of textual\ndata and totaling 3.5B words. To support conversational applications, we\ncollect a 106k-instance instruction dataset, carefully built to be culturally\ngrounded. For evaluation, we construct a Macedonian evaluation suite covering\nseven benchmarks. Finally, we train domestic-yak, a state-of-the-art\n8B-parameter model, on our curated datasets and evaluate it against eight\nbaseline models using the newly constructed benchmark suite. Our model\noutperforms all existing models in the 8B parameter range across all\nbenchmarks, and achieves performance comparable to models up to 10x larger.\nFurthermore, a qualitative analysis with native speakers reveals that our model\nis preferred over larger counterparts, receiving higher ratings for grammatical\ncorrectness and cultural appropriateness. All datasets, code, and model weights\nare openly released, setting a foundation for advancing LLMs in similarly\nunderrepresented languages. These resources are publicly available at\ngithub.com/LVSTCK for source code, and at huggingface.co/LVSTCK for pretrained\nmodel weights and data.", "AI": {"tldr": "该论文为低资源语言马其顿语构建了大规模数据集和评估套件，并训练了一个8B参数的LLM模型，性能优于同类模型。", "motivation": "全球技术普及需求增加，但LLM对低资源语言支持不足，限制了其应用。", "method": "收集40GB马其顿语文本数据，构建106k指令数据集和评估套件，训练8B参数模型domestic-yak。", "result": "模型在8B参数范围内表现最佳，性能接近更大模型，且文化适应性更强。", "conclusion": "公开所有资源，为低资源语言LLM研究奠定基础。"}}
{"id": "2506.09460", "pdf": "https://arxiv.org/pdf/2506.09460", "abs": "https://arxiv.org/abs/2506.09460", "authors": ["Amirreza Khoshbakht", "Erchan Aptoula"], "title": "Evidential Deep Learning with Spectral-Spatial Uncertainty Disentanglement for Open-Set Hyperspectral Domain Generalization", "categories": ["cs.CV"], "comment": null, "summary": "Open-set domain generalization(OSDG) for hyperspectral image classification\npresents significant challenges due to the presence of unknown classes in\ntarget domains and the need for models to generalize across multiple unseen\ndomains without target-specific adaptation. Existing domain adaptation methods\nassume access to target domain data during training and fail to address the\nfundamental issue of domain shift when unknown classes are present, leading to\nnegative transfer and reduced classification performance. To address these\nlimitations, we propose a novel open-set domain generalization framework that\ncombines four key components: Spectrum-Invariant Frequency Disentanglement\n(SIFD) for domain-agnostic feature extraction, Dual-Channel Residual Network\n(DCRN) for robust spectral-spatial feature learning, Evidential Deep Learning\n(EDL) for uncertainty quantification, and Spectral-Spatial Uncertainty\nDisentanglement (SSUD) for reliable open-set classification. The SIFD module\nextracts domain-invariant spectral features in the frequency domain through\nattention-weighted frequency analysis and domain-agnostic regularization, while\nDCRN captures complementary spectral and spatial information via parallel\npathways with adaptive fusion. EDL provides principled uncertainty estimation\nusing Dirichlet distributions, enabling the SSUD module to make reliable\nopen-set decisions through uncertainty-aware pathway weighting and adaptive\nrejection thresholding. Experimental results on three cross-scene hyperspectral\nclassification tasks show that our approach achieves performance comparable to\nstate-of-the-art domain adaptation methods while requiring no access to the\ntarget domain during training. The implementation will be made available at\nhttps://github.com/amir-khb/SSUDOSDG upon acceptance.", "AI": {"tldr": "本文提出了一种新的开放集域泛化框架，用于高光谱图像分类，解决了未知类别的挑战，无需目标域数据即可实现跨域泛化。", "motivation": "现有域适应方法依赖目标域数据且无法处理未知类别，导致负迁移和性能下降。", "method": "结合频谱不变频率解耦（SIFD）、双通道残差网络（DCRN）、证据深度学习（EDL）和光谱空间不确定性解耦（SSUD）四个模块。", "result": "在三个跨场景高光谱分类任务中表现优异，无需目标域数据即可媲美现有域适应方法。", "conclusion": "该框架有效解决了开放集域泛化问题，为高光谱图像分类提供了新思路。"}}
{"id": "2506.09108", "pdf": "https://arxiv.org/pdf/2506.09108", "abs": "https://arxiv.org/abs/2506.09108", "authors": ["Yuwei Zhang", "Kumar Ayush", "Siyuan Qiao", "A. Ali Heydari", "Girish Narayanswamy", "Maxwell A. Xu", "Ahmed A. Metwally", "Shawn Xu", "Jake Garrison", "Xuhai Xu", "Tim Althoff", "Yun Liu", "Pushmeet Kohli", "Jiening Zhan", "Mark Malhotra", "Shwetak Patel", "Cecilia Mascolo", "Xin Liu", "Daniel McDuff", "Yuzhe Yang"], "title": "SensorLM: Learning the Language of Wearable Sensors", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We present SensorLM, a family of sensor-language foundation models that\nenable wearable sensor data understanding with natural language. Despite its\npervasive nature, aligning and interpreting sensor data with language remains\nchallenging due to the lack of paired, richly annotated sensor-text\ndescriptions in uncurated, real-world wearable data. We introduce a\nhierarchical caption generation pipeline designed to capture statistical,\nstructural, and semantic information from sensor data. This approach enabled\nthe curation of the largest sensor-language dataset to date, comprising over\n59.7 million hours of data from more than 103,000 people. Furthermore, SensorLM\nextends prominent multimodal pretraining architectures (e.g., CLIP, CoCa) and\nrecovers them as specific variants within a generic architecture. Extensive\nexperiments on real-world tasks in human activity analysis and healthcare\nverify the superior performance of SensorLM over state-of-the-art in zero-shot\nrecognition, few-shot learning, and cross-modal retrieval. SensorLM also\ndemonstrates intriguing capabilities including scaling behaviors, label\nefficiency, sensor captioning, and zero-shot generalization to unseen tasks.", "AI": {"tldr": "SensorLM是一种传感器-语言基础模型，通过自然语言理解可穿戴传感器数据，解决了传感器数据与语言对齐的挑战，并展示了在零样本识别、少样本学习和跨模态检索中的优越性能。", "motivation": "解决传感器数据与自然语言对齐的困难，尤其是在缺乏标注数据的现实场景中。", "method": "采用分层标题生成流程，结合统计、结构和语义信息，并扩展多模态预训练架构（如CLIP、CoCa）。", "result": "构建了最大的传感器-语言数据集（59.7百万小时数据），在人类活动分析和医疗任务中表现优于现有技术。", "conclusion": "SensorLM在零样本泛化、标签效率和传感器标题生成等方面展示了潜力，为传感器数据理解提供了新方法。"}}
{"id": "2506.09566", "pdf": "https://arxiv.org/pdf/2506.09566", "abs": "https://arxiv.org/abs/2506.09566", "authors": ["Blaž Škrlj", "Boshko Koloski", "Senja Pollak", "Nada Lavrač"], "title": "From Symbolic to Neural and Back: Exploring Knowledge Graph-Large Language Model Synergies", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "To-appear as a book chapter", "summary": "Integrating structured knowledge from Knowledge Graphs (KGs) into Large\nLanguage Models (LLMs) enhances factual grounding and reasoning capabilities.\nThis survey paper systematically examines the synergy between KGs and LLMs,\ncategorizing existing approaches into two main groups: KG-enhanced LLMs, which\nimprove reasoning, reduce hallucinations, and enable complex question\nanswering; and LLM-augmented KGs, which facilitate KG construction, completion,\nand querying. Through comprehensive analysis, we identify critical gaps and\nhighlight the mutual benefits of structured knowledge integration. Compared to\nexisting surveys, our study uniquely emphasizes scalability, computational\nefficiency, and data quality. Finally, we propose future research directions,\nincluding neuro-symbolic integration, dynamic KG updating, data reliability,\nand ethical considerations, paving the way for intelligent systems capable of\nmanaging more complex real-world knowledge tasks.", "AI": {"tldr": "该综述论文探讨了知识图谱（KGs）与大型语言模型（LLMs）的协同作用，分为KG增强LLMs和LLM增强KGs两类，并提出了未来研究方向。", "motivation": "通过整合结构化知识提升LLMs的事实基础和推理能力，解决现有方法的局限性。", "method": "系统分类现有方法，分析KG与LLM的协同效应，强调可扩展性、计算效率和数据质量。", "result": "发现关键研究空白，提出神经符号集成、动态KG更新等未来方向。", "conclusion": "结构化知识整合为智能系统处理复杂现实任务铺平道路。"}}
{"id": "2506.09469", "pdf": "https://arxiv.org/pdf/2506.09469", "abs": "https://arxiv.org/abs/2506.09469", "authors": ["Maria Damanaki", "Nikos Piperigkos", "Alexandros Gkillas", "Aris S. Lalos"], "title": "Optimizing Cooperative Multi-Object Tracking using Graph Signal Processing", "categories": ["cs.CV"], "comment": "2025 IEEE International Conference on Multimedia and Expo Workshops,\n  3DMM - 3D Multimedia Analytics, Search and Generation", "summary": "Multi-Object Tracking (MOT) plays a crucial role in autonomous driving\nsystems, as it lays the foundations for advanced perception and precise path\nplanning modules. Nonetheless, single agent based MOT lacks in sensing\nsurroundings due to occlusions, sensors failures, etc. Hence, the integration\nof multiagent information is essential for comprehensive understanding of the\nenvironment. This paper proposes a novel Cooperative MOT framework for tracking\nobjects in 3D LiDAR scene by formulating and solving a graph topology-aware\noptimization problem so as to fuse information coming from multiple vehicles.\nBy exploiting a fully connected graph topology defined by the detected bounding\nboxes, we employ the Graph Laplacian processing optimization technique to\nsmooth the position error of bounding boxes and effectively combine them. In\nthat manner, we reveal and leverage inherent coherences of diverse multi-agent\ndetections, and associate the refined bounding boxes to tracked objects at two\nstages, optimizing localization and tracking accuracies. An extensive\nevaluation study has been conducted, using the real-world V2V4Real dataset,\nwhere the proposed method significantly outperforms the baseline frameworks,\nincluding the state-of-the-art deep-learning DMSTrack and V2V4Real, in various\ntesting sequences.", "AI": {"tldr": "提出了一种基于图拓扑感知优化的多车协同3D LiDAR多目标跟踪框架，显著提升了定位和跟踪精度。", "motivation": "单智能体的多目标跟踪因遮挡和传感器故障等问题感知受限，需多车信息融合以全面理解环境。", "method": "利用检测到的边界框构建全连接图拓扑，通过图拉普拉斯处理优化技术平滑位置误差并融合多车信息，分两阶段关联优化。", "result": "在V2V4Real数据集上显著优于基线框架和先进方法（如DMSTrack和V2V4Real）。", "conclusion": "多车协同框架有效提升了3D LiDAR场景下的多目标跟踪性能。"}}
{"id": "2506.09591", "pdf": "https://arxiv.org/pdf/2506.09591", "abs": "https://arxiv.org/abs/2506.09591", "authors": ["Stefan Arnold"], "title": "Memorization in Language Models through the Lens of Intrinsic Dimension", "categories": ["cs.CL"], "comment": null, "summary": "Language Models (LMs) are prone to memorizing parts of their data during\ntraining and unintentionally emitting them at generation time, raising concerns\nabout privacy leakage and disclosure of intellectual property. While previous\nresearch has identified properties such as context length, parameter size, and\nduplication frequency, as key drivers of unintended memorization, little is\nknown about how the latent structure modulates this rate of memorization. We\ninvestigate the role of Intrinsic Dimension (ID), a geometric proxy for the\nstructural complexity of a sequence in latent space, in modulating\nmemorization. Our findings suggest that ID acts as a suppressive signal for\nmemorization: compared to low-ID sequences, high-ID sequences are less likely\nto be memorized, particularly in overparameterized models and under sparse\nexposure. These findings highlight the interaction between scale, exposure, and\ncomplexity in shaping memorization.", "AI": {"tldr": "研究发现语言模型（LMs）的记忆行为受序列内在维度（ID）影响，高ID序列比低ID序列更难被记忆。", "motivation": "探讨语言模型在训练中记忆数据的机制，尤其是潜在结构（如ID）如何调节记忆率。", "method": "通过分析序列的内在维度（ID）与记忆行为的关系，研究模型参数规模和曝光稀疏性的影响。", "result": "高ID序列在过参数化模型和稀疏曝光条件下更难被记忆。", "conclusion": "记忆行为受模型规模、曝光频率和序列复杂度的共同影响。"}}
{"id": "2506.09473", "pdf": "https://arxiv.org/pdf/2506.09473", "abs": "https://arxiv.org/abs/2506.09473", "authors": ["Cheng Chen", "Yunpeng Zhai", "Yifan Zhao", "Jinyang Gao", "Bolin Ding", "Jia Li"], "title": "Provoking Multi-modal Few-Shot LVLM via Exploration-Exploitation In-Context Learning", "categories": ["cs.CV"], "comment": "10 pages, 6 figures, CVPR 2025", "summary": "In-context learning (ICL), a predominant trend in instruction learning, aims\nat enhancing the performance of large language models by providing clear task\nguidance and examples, improving their capability in task understanding and\nexecution. This paper investigates ICL on Large Vision-Language Models (LVLMs)\nand explores the policies of multi-modal demonstration selection. Existing\nresearch efforts in ICL face significant challenges: First, they rely on\npre-defined demonstrations or heuristic selecting strategies based on human\nintuition, which are usually inadequate for covering diverse task requirements,\nleading to sub-optimal solutions; Second, individually selecting each\ndemonstration fails in modeling the interactions between them, resulting in\ninformation redundancy. Unlike these prevailing efforts, we propose a new\nexploration-exploitation reinforcement learning framework, which explores\npolicies to fuse multi-modal information and adaptively select adequate\ndemonstrations as an integrated whole. The framework allows LVLMs to optimize\nthemselves by continually refining their demonstrations through\nself-exploration, enabling the ability to autonomously identify and generate\nthe most effective selection policies for in-context learning. Experimental\nresults verify the superior performance of our approach on four Visual\nQuestion-Answering (VQA) datasets, demonstrating its effectiveness in enhancing\nthe generalization capability of few-shot LVLMs.", "AI": {"tldr": "本文提出了一种基于探索-利用强化学习框架的方法，用于多模态演示选择，以提升大型视觉语言模型的上下文学习能力。", "motivation": "现有上下文学习方法依赖预定义演示或启发式选择策略，无法覆盖多样化任务需求且忽略演示间交互，导致性能不佳。", "method": "提出探索-利用强化学习框架，自适应选择多模态演示，并通过自我探索优化选择策略。", "result": "在四个视觉问答数据集上验证了方法的优越性能，增强了少样本模型的泛化能力。", "conclusion": "该方法能自主生成高效演示选择策略，显著提升上下文学习效果。"}}
{"id": "2506.09160", "pdf": "https://arxiv.org/pdf/2506.09160", "abs": "https://arxiv.org/abs/2506.09160", "authors": ["Griffin Pitts", "Sanaz Motamedi"], "title": "Understanding Human-AI Trust in Education", "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.HC"], "comment": null, "summary": "As AI chatbots become increasingly integrated in education, students are\nturning to these systems for guidance, feedback, and information. However, the\nanthropomorphic characteristics of these chatbots create ambiguity regarding\nwhether students develop trust toward them as they would a human peer or\ninstructor, based in interpersonal trust, or as they would any other piece of\ntechnology, based in technology trust. This ambiguity presents theoretical\nchallenges, as interpersonal trust models may inappropriately ascribe human\nintentionality and morality to AI, while technology trust models were developed\nfor non-social technologies, leaving their applicability to anthropomorphic\nsystems unclear. To address this gap, we investigate how human-like and\nsystem-like trusting beliefs comparatively influence students' perceived\nenjoyment, trusting intention, behavioral intention to use, and perceived\nusefulness of an AI chatbot - factors associated with students' engagement and\nlearning outcomes. Through partial least squares structural equation modeling,\nwe found that human-like and system-like trust significantly influenced student\nperceptions, with varied effects. Human-like trust more strongly predicted\ntrusting intention, while system-like trust better predicted behavioral\nintention and perceived usefulness. Both had similar effects on perceived\nenjoyment. Given the partial explanatory power of each type of trust, we\npropose that students develop a distinct form of trust with AI chatbots\n(human-AI trust) that differs from human-human and human-technology models of\ntrust. Our findings highlight the need for new theoretical frameworks specific\nto human-AI trust and offer practical insights for fostering appropriately\ncalibrated trust, which is critical for the effective adoption and pedagogical\nimpact of AI in education.", "AI": {"tldr": "论文探讨了学生对AI聊天机器人的信任类型（人际信任与技术信任）及其对学习体验的影响，发现两者共同形成了一种独特的“人-AI信任”。", "motivation": "AI聊天机器人在教育中的应用日益广泛，但其拟人化特性导致学生对它们的信任类型不明确，现有信任模型（人际或技术）可能不适用。", "method": "通过偏最小二乘结构方程模型，分析人机信任对学生感知的影响。", "result": "人际信任更影响信任意图，技术信任更影响使用意图和感知有用性，两者共同形成独特的“人-AI信任”。", "conclusion": "需建立专门的人-AI信任理论框架，以优化AI在教育中的应用效果。"}}
{"id": "2506.09627", "pdf": "https://arxiv.org/pdf/2506.09627", "abs": "https://arxiv.org/abs/2506.09627", "authors": ["Nicolas Audinet de Pieuchon", "Adel Daoud", "Connor T. Jerzak", "Moa Johansson", "Richard Johansson"], "title": "Benchmarking Debiasing Methods for LLM-based Parameter Estimates", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) offer an inexpensive yet powerful way to\nannotate text, but are often inconsistent when compared with experts. These\nerrors can bias downstream estimates of population parameters such as\nregression coefficients and causal effects. To mitigate this bias, researchers\nhave developed debiasing methods such as Design-based Supervised Learning (DSL)\nand Prediction-Powered Inference (PPI), which promise valid estimation by\ncombining LLM annotations with a limited number of expensive expert\nannotations. Although these methods produce consistent estimates under\ntheoretical assumptions, it is unknown how they compare in finite samples of\nsizes encountered in applied research. We make two contributions: First, we\nstudy how each method's performance scales with the number of expert\nannotations, highlighting regimes where LLM bias or limited expert labels\nsignificantly affect results. Second, we compare DSL and PPI across a range of\ntasks, finding that although both achieve low bias with large datasets, DSL\noften outperforms PPI on bias reduction and empirical efficiency, but its\nperformance is less consistent across datasets. Our findings indicate that\nthere is a bias-variance tradeoff at the level of debiasing methods, calling\nfor more research on developing metrics for quantifying their efficiency in\nfinite samples.", "AI": {"tldr": "本文研究了两种去偏方法（DSL和PPI）在有限样本中的表现，发现DSL在偏差减少和效率上通常优于PPI，但表现不一致。", "motivation": "大语言模型（LLM）注释虽然廉价但存在不一致性，可能影响下游参数估计。现有去偏方法（如DSL和PPI）在理论上有效，但在有限样本中的表现未知。", "method": "通过实验比较DSL和PPI在不同任务中的表现，分析其性能随专家注释数量的变化。", "result": "DSL在偏差减少和效率上通常优于PPI，但表现不一致；PPI在大数据集下表现稳定。", "conclusion": "去偏方法存在偏差-方差权衡，需进一步研究量化其效率的指标。"}}
{"id": "2506.09476", "pdf": "https://arxiv.org/pdf/2506.09476", "abs": "https://arxiv.org/abs/2506.09476", "authors": ["Tianxiang Hao", "Lixian Zhang", "Yingjia Zhang", "Mengxuan Chen", "Jinxiao Zhang", "Haohuan Fu"], "title": "Urban1960SatSeg: Unsupervised Semantic Segmentation of Mid-20$^{th}$ century Urban Landscapes with Satellite Imageries", "categories": ["cs.CV"], "comment": null, "summary": "Historical satellite imagery, such as mid-20$^{th}$ century Keyhole data,\noffers rare insights into understanding early urban development and long-term\ntransformation. However, severe quality degradation (e.g., distortion,\nmisalignment, and spectral scarcity) and annotation absence have long hindered\nsemantic segmentation on such historical RS imagery. To bridge this gap and\nenhance understanding of urban development, we introduce\n$\\textbf{Urban1960SatBench}$, an annotated segmentation dataset based on\nhistorical satellite imagery with the earliest observation time among all\nexisting segmentation datasets, along with a benchmark framework for\nunsupervised segmentation tasks, $\\textbf{Urban1960SatUSM}$. First,\n$\\textbf{Urban1960SatBench}$ serves as a novel, expertly annotated semantic\nsegmentation dataset built on mid-20$^{th}$ century Keyhole imagery, covering\n1,240 km$^2$ and key urban classes (buildings, roads, farmland, water). As the\nearliest segmentation dataset of its kind, it provides a pioneering benchmark\nfor historical urban understanding. Second,\n$\\textbf{Urban1960SatUSM}$(Unsupervised Segmentation Model) is a novel\nunsupervised semantic segmentation framework for historical RS imagery. It\nemploys a confidence-aware alignment mechanism and focal-confidence loss based\non a self-supervised learning architecture, which generates robust\npseudo-labels and adaptively prioritizes prediction difficulty and label\nreliability to improve unsupervised segmentation on noisy historical data\nwithout manual supervision. Experiments show Urban1960SatUSM significantly\noutperforms existing unsupervised segmentation methods on Urban1960SatSeg for\nsegmenting historical urban scenes, promising in paving the way for\nquantitative studies of long-term urban change using modern computer vision.\nOur benchmark and supplementary material are available at\nhttps://github.com/Tianxiang-Hao/Urban1960SatSeg.", "AI": {"tldr": "论文提出了一个基于20世纪中叶Keyhole卫星影像的标注数据集Urban1960SatBench和一个无监督分割框架Urban1960SatUSM，用于解决历史遥感影像语义分割中的质量退化问题。", "motivation": "历史卫星影像（如20世纪中叶的Keyhole数据）为研究早期城市发展提供了宝贵信息，但影像质量退化（如失真、错位和光谱稀缺）和缺乏标注阻碍了语义分割的应用。", "method": "提出了Urban1960SatBench数据集和Urban1960SatUSM框架，后者采用置信感知对齐机制和焦点置信损失，基于自监督学习架构，生成鲁棒的伪标签并优化无监督分割。", "result": "实验表明，Urban1960SatUSM在Urban1960SatBench数据集上显著优于现有无监督分割方法，为历史城市场景的分割提供了有效解决方案。", "conclusion": "Urban1960SatBench和Urban1960SatUSM为利用现代计算机视觉技术研究长期城市变化提供了开创性工具。"}}
{"id": "2506.09167", "pdf": "https://arxiv.org/pdf/2506.09167", "abs": "https://arxiv.org/abs/2506.09167", "authors": ["James R. Williamson", "Andrew Alini", "Brian A. Telfer", "Adam W. Potter", "Karl E. Friedl"], "title": "Estimating Visceral Adiposity from Wrist-Worn Accelerometry", "categories": ["eess.SP", "cs.AI", "q-bio.QM"], "comment": "13 pages", "summary": "Visceral adipose tissue (VAT) is a key marker of both metabolic health and\nhabitual physical activity (PA). Excess VAT is highly correlated with type 2\ndiabetes and insulin resistance. The mechanistic basis for this pathophysiology\nrelates to overloading the liver with fatty acids. VAT is also a highly labile\nfat depot, with increased turnover stimulated by catecholamines during\nexercise. VAT can be measured with sophisticated imaging technologies, but can\nalso be inferred directly from PA. We tested this relationship using National\nHealth and Nutrition Examination Survey (NHANES) data from 2011-2014, for\nindividuals aged 20-60 years with 7 days of accelerometry data (n=2,456 men;\n2,427 women) [1]. Two approaches were used for estimating VAT from activity.\nThe first used engineered features based on movements during gait and sleep,\nand then ridge regression to map summary statistics of these features into a\nVAT estimate. The second approach used deep neural networks trained on 24 hours\nof continuous accelerometry. A foundation model first mapped each 10s frame\ninto a high-dimensional feature vector. A transformer model then mapped each\nday's feature vector time series into a VAT estimate, which were averaged over\nmultiple days. For both approaches, the most accurate estimates were obtained\nwith the addition of covariate information about subject demographics and body\nmeasurements. The best performance was obtained by combining the two\napproaches, resulting in VAT estimates with correlations of r=0.86. These\nfindings demonstrate a strong relationship between PA and VAT and, by\nextension, between PA and metabolic health risks.", "AI": {"tldr": "该研究通过两种方法从加速度计数据中估计内脏脂肪组织（VAT），并结合人口统计学和身体测量数据，证明了体力活动（PA）与VAT及代谢健康风险的强相关性。", "motivation": "内脏脂肪组织（VAT）是代谢健康和体力活动（PA）的关键标志物，过量VAT与2型糖尿病和胰岛素抵抗高度相关。研究旨在探索PA与VAT之间的关系。", "method": "使用NHANES数据（2011-2014），通过两种方法估计VAT：1）基于步态和睡眠运动的特征工程和岭回归；2）基于24小时连续加速度计数据的深度神经网络（包括基础模型和Transformer模型）。", "result": "结合两种方法并加入人口统计学和身体测量数据后，VAT估计的相关系数达到r=0.86，表明PA与VAT及代谢健康风险之间存在强相关性。", "conclusion": "研究证实了PA与VAT的强相关性，进一步支持了PA对代谢健康的积极影响。"}}
{"id": "2506.09641", "pdf": "https://arxiv.org/pdf/2506.09641", "abs": "https://arxiv.org/abs/2506.09641", "authors": ["Anna Stein", "Kevin Tang"], "title": "Modeling Probabilistic Reduction using Information Theory and Naive Discriminative Learning", "categories": ["cs.CL", "cs.IT", "math.IT"], "comment": "Submitted to Interspeech 2025", "summary": "This study compares probabilistic predictors based on information theory with\nNaive Discriminative Learning (NDL) predictors in modeling acoustic word\nduration, focusing on probabilistic reduction. We examine three models using\nthe Buckeye corpus: one with NDL-derived predictors using information-theoretic\nformulas, one with traditional NDL predictors, and one with N-gram\nprobabilistic predictors. Results show that the N-gram model outperforms both\nNDL models, challenging the assumption that NDL is more effective due to its\ncognitive motivation. However, incorporating information-theoretic formulas\ninto NDL improves model performance over the traditional model. This research\nhighlights a) the need to incorporate not only frequency and contextual\npredictability but also average contextual predictability, and b) the\nimportance of combining information-theoretic metrics of predictability and\ninformation derived from discriminative learning in modeling acoustic\nreduction.", "AI": {"tldr": "比较基于信息论的概率预测器与朴素判别学习（NDL）预测器在建模声学词持续时间中的表现，发现N-gram模型优于NDL模型，但信息论公式能提升NDL性能。", "motivation": "探讨NDL预测器在声学词持续时间建模中的有效性，挑战其认知动机假设，并研究信息论公式对NDL的改进。", "method": "使用Buckeye语料库比较三种模型：信息论公式改进的NDL、传统NDL和N-gram概率预测器。", "result": "N-gram模型表现最佳，但信息论公式能提升NDL性能。", "conclusion": "需结合频率、上下文预测性和平均预测性，并融合信息论指标与判别学习信息以优化声学缩减建模。"}}
{"id": "2506.09479", "pdf": "https://arxiv.org/pdf/2506.09479", "abs": "https://arxiv.org/abs/2506.09479", "authors": ["Zetian Song", "Jiaye Fu", "Jiaqi Zhang", "Xiaohan Lu", "Chuanmin Jia", "Siwei Ma", "Wen Gao"], "title": "TinySplat: Feedforward Approach for Generating Compact 3D Scene Representation", "categories": ["cs.CV"], "comment": null, "summary": "The recent development of feedforward 3D Gaussian Splatting (3DGS) presents a\nnew paradigm to reconstruct 3D scenes. Using neural networks trained on\nlarge-scale multi-view datasets, it can directly infer 3DGS representations\nfrom sparse input views. Although the feedforward approach achieves high\nreconstruction speed, it still suffers from the substantial storage cost of 3D\nGaussians. Existing 3DGS compression methods relying on scene-wise optimization\nare not applicable due to architectural incompatibilities. To overcome this\nlimitation, we propose TinySplat, a complete feedforward approach for\ngenerating compact 3D scene representations. Built upon standard feedforward\n3DGS methods, TinySplat integrates a training-free compression framework that\nsystematically eliminates key sources of redundancy. Specifically, we introduce\nView-Projection Transformation (VPT) to reduce geometric redundancy by\nprojecting geometric parameters into a more compact space. We further present\nVisibility-Aware Basis Reduction (VABR), which mitigates perceptual redundancy\nby aligning feature energy along dominant viewing directions via basis\ntransformation. Lastly, spatial redundancy is addressed through an\noff-the-shelf video codec. Comprehensive experimental results on multiple\nbenchmark datasets demonstrate that TinySplat achieves over 100x compression\nfor 3D Gaussian data generated by feedforward methods. Compared to the\nstate-of-the-art compression approach, we achieve comparable quality with only\n6% of the storage size. Meanwhile, our compression framework requires only 25%\nof the encoding time and 1% of the decoding time.", "AI": {"tldr": "TinySplat提出了一种基于前馈3D高斯泼溅（3DGS）的紧凑3D场景表示方法，通过消除冗余实现高效压缩。", "motivation": "现有前馈3DGS方法存储成本高，且传统压缩方法不适用，需要一种新的压缩方案。", "method": "结合View-Projection Transformation（VPT）减少几何冗余，Visibility-Aware Basis Reduction（VABR）减少感知冗余，并使用视频编解码器处理空间冗余。", "result": "在多个数据集上实现100倍压缩，存储大小仅为现有方法的6%，编码时间减少75%，解码时间减少99%。", "conclusion": "TinySplat是一种高效且快速的3DGS压缩方法，显著降低了存储和计算成本。"}}
{"id": "2506.09171", "pdf": "https://arxiv.org/pdf/2506.09171", "abs": "https://arxiv.org/abs/2506.09171", "authors": ["Samuel Holt", "Max Ruiz Luyten", "Thomas Pouplin", "Mihaela van der Schaar"], "title": "Improving LLM Agent Planning with In-Context Learning via Atomic Fact Augmentation and Lookahead Search", "categories": ["cs.LG", "cs.AI", "cs.CL", "68T07, 68T20, 68T30, 93E35", "I.2.6; I.2.7; I.2.8"], "comment": "9-page main paper, 1 figure. Accepted for an Oral presentation at the\n  First Workshop on Computer Use Agents (ICML 2025), Vancouver, Canada", "summary": "Large Language Models (LLMs) are increasingly capable but often require\nsignificant guidance or extensive interaction history to perform effectively in\ncomplex, interactive environments. Existing methods may struggle with adapting\nto new information or efficiently utilizing past experiences for multi-step\nreasoning without fine-tuning. We introduce a novel LLM agent framework that\nenhances planning capabilities through in-context learning, facilitated by\natomic fact augmentation and a recursive lookahead search. Our agent learns to\nextract task-critical ``atomic facts'' from its interaction trajectories. These\nfacts dynamically augment the prompts provided to LLM-based components\nresponsible for action proposal, latent world model simulation, and state-value\nestimation. Planning is performed via a depth-limited lookahead search, where\nthe LLM simulates potential trajectories and evaluates their outcomes, guided\nby the accumulated facts and interaction history. This approach allows the\nagent to improve its understanding and decision-making online, leveraging its\nexperience to refine its behavior without weight updates. We provide a\ntheoretical motivation linking performance to the quality of fact-based\nabstraction and LLM simulation accuracy. Empirically, our agent demonstrates\nimproved performance and adaptability on challenging interactive tasks,\nachieving more optimal behavior as it accumulates experience, showcased in\ntasks such as TextFrozenLake and ALFWorld.", "AI": {"tldr": "提出了一种基于上下文学习的LLM代理框架，通过原子事实增强和递归前瞻搜索提升规划能力，无需微调即可在线优化决策。", "motivation": "现有LLM在复杂交互环境中需要大量指导或历史交互，难以适应新信息或高效利用经验进行多步推理。", "method": "引入原子事实增强和递归前瞻搜索，动态提取关键事实并用于动作提议、世界模型模拟和状态值估计。", "result": "代理在TextFrozenLake和ALFWorld等任务中表现出更高的性能和适应性，随着经验积累行为更优。", "conclusion": "该框架通过事实抽象和LLM模拟质量提升性能，为LLM在复杂环境中的应用提供了新思路。"}}
{"id": "2506.09643", "pdf": "https://arxiv.org/pdf/2506.09643", "abs": "https://arxiv.org/abs/2506.09643", "authors": ["Harry Walsh", "Maksym Ivashechkin", "Richard Bowden"], "title": "Using Sign Language Production as Data Augmentation to enhance Sign Language Translation", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Machine learning models fundamentally rely on large quantities of\nhigh-quality data. Collecting the necessary data for these models can be\nchallenging due to cost, scarcity, and privacy restrictions. Signed languages\nare visual languages used by the deaf community and are considered low-resource\nlanguages. Sign language datasets are often orders of magnitude smaller than\ntheir spoken language counterparts. Sign Language Production is the task of\ngenerating sign language videos from spoken language sentences, while Sign\nLanguage Translation is the reverse translation task. Here, we propose\nleveraging recent advancements in Sign Language Production to augment existing\nsign language datasets and enhance the performance of Sign Language Translation\nmodels. For this, we utilize three techniques: a skeleton-based approach to\nproduction, sign stitching, and two photo-realistic generative models, SignGAN\nand SignSplat. We evaluate the effectiveness of these techniques in enhancing\nthe performance of Sign Language Translation models by generating variation in\nthe signer's appearance and the motion of the skeletal data. Our results\ndemonstrate that the proposed methods can effectively augment existing datasets\nand enhance the performance of Sign Language Translation models by up to 19%,\npaving the way for more robust and accurate Sign Language Translation systems,\neven in resource-constrained environments.", "AI": {"tldr": "利用手语生成技术增强手语翻译模型的性能，通过骨架生成、拼接和生成模型（SignGAN和SignSplat）提升数据集多样性，效果提升19%。", "motivation": "手语数据集稀缺且规模小，限制了手语翻译模型的性能。通过手语生成技术可以扩充数据集，提升翻译效果。", "method": "采用骨架生成、拼接和两种生成模型（SignGAN和SignSplat）生成多样化的手语视频数据。", "result": "实验表明，这些方法能有效扩充数据集，提升手语翻译模型性能达19%。", "conclusion": "该方法为资源受限环境下的手语翻译系统提供了更鲁棒和准确的解决方案。"}}
{"id": "2506.09482", "pdf": "https://arxiv.org/pdf/2506.09482", "abs": "https://arxiv.org/abs/2506.09482", "authors": ["Dingcheng Zhen", "Qian Qiao", "Tan Yu", "Kangxi Wu", "Ziwei Zhang", "Siyuan Liu", "Shunshun Yin", "Ming Tao"], "title": "Marrying Autoregressive Transformer and Diffusion with Multi-Reference Autoregression", "categories": ["cs.CV"], "comment": null, "summary": "We introduce TransDiff, the first image generation model that marries\nAutoregressive (AR) Transformer with diffusion models. In this joint modeling\nframework, TransDiff encodes labels and images into high-level semantic\nfeatures and employs a diffusion model to estimate the distribution of image\nsamples. On the ImageNet 256x256 benchmark, TransDiff significantly outperforms\nother image generation models based on standalone AR Transformer or diffusion\nmodels. Specifically, TransDiff achieves a Fr\\'echet Inception Distance (FID)\nof 1.61 and an Inception Score (IS) of 293.4, and further provides x2 faster\ninference latency compared to state-of-the-art methods based on AR Transformer\nand x112 faster inference compared to diffusion-only models. Furthermore,\nbuilding on the TransDiff model, we introduce a novel image generation paradigm\ncalled Multi-Reference Autoregression (MRAR), which performs autoregressive\ngeneration by predicting the next image. MRAR enables the model to reference\nmultiple previously generated images, thereby facilitating the learning of more\ndiverse representations and improving the quality of generated images in\nsubsequent iterations. By applying MRAR, the performance of TransDiff is\nimproved, with the FID reduced from 1.61 to 1.42. We expect TransDiff to open\nup a new frontier in the field of image generation.", "AI": {"tldr": "TransDiff结合自回归Transformer和扩散模型，提出了一种新的图像生成方法，显著优于现有技术，并进一步通过MRAR提升性能。", "motivation": "结合自回归Transformer和扩散模型的优势，提升图像生成的质量和效率。", "method": "TransDiff将标签和图像编码为高级语义特征，利用扩散模型估计图像样本分布，并引入MRAR范式以多参考自回归生成图像。", "result": "在ImageNet 256x256上，TransDiff的FID为1.61，IS为293.4，推理速度显著快于其他方法；MRAR进一步将FID降至1.42。", "conclusion": "TransDiff为图像生成领域开辟了新方向，展示了联合建模和多参考自回归的潜力。"}}
{"id": "2506.09645", "pdf": "https://arxiv.org/pdf/2506.09645", "abs": "https://arxiv.org/abs/2506.09645", "authors": ["Tianjun Yao", "Haoxuan Li", "Zhiqiang Shen", "Pan Li", "Tongliang Liu", "Kun Zhang"], "title": "Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph Question Answering", "categories": ["cs.CL", "cs.IR", "cs.LG", "I.2.6"], "comment": "32 pages, 28 figures", "summary": "Large Language Models (LLMs) have shown strong inductive reasoning ability\nacross various domains, but their reliability is hindered by the outdated\nknowledge and hallucinations. Retrieval-Augmented Generation mitigates these\nissues by grounding LLMs with external knowledge; however, most existing RAG\npipelines rely on unstructured text, limiting interpretability and structured\nreasoning. Knowledge graphs, which represent facts as relational triples, offer\na more structured and compact alternative. Recent studies have explored\nintegrating knowledge graphs with LLMs for knowledge graph question answering\n(KGQA), with a significant proportion adopting the retrieve-then-reasoning\nparadigm. In this framework, graph-based retrievers have demonstrated strong\nempirical performance, yet they still face challenges in generalization\nability. In this work, we propose RAPL, a novel framework for efficient and\neffective graph retrieval in KGQA. RAPL addresses these limitations through\nthree aspects: (1) a two-stage labeling strategy that combines heuristic\nsignals with parametric models to provide causally grounded supervision; (2) a\nmodel-agnostic graph transformation approach to capture both intra- and\ninter-triple interactions, thereby enhancing representational capacity; and (3)\na path-based reasoning strategy that facilitates learning from the injected\nrational knowledge, and supports downstream reasoner through structured inputs.\nEmpirically, RAPL outperforms state-of-the-art methods by $2.66\\%-20.34\\%$, and\nsignificantly reduces the performance gap between smaller and more powerful\nLLM-based reasoners, as well as the gap under cross-dataset settings,\nhighlighting its superior retrieval capability and generalizability. Codes are\navailable at: https://github.com/tianyao-aka/RAPL.", "AI": {"tldr": "论文提出RAPL框架，通过两阶段标注、模型无关的图转换和路径推理策略，提升知识图谱问答中的检索能力与泛化性，显著优于现有方法。", "motivation": "解决现有检索增强生成方法依赖非结构化文本导致的解释性和结构化推理不足，以及知识图谱检索在泛化能力上的挑战。", "method": "RAPL框架包括：1) 结合启发式信号与参数模型的两阶段标注；2) 捕获三元组内外交互的图转换；3) 基于路径的推理策略。", "result": "RAPL在性能上超越现有方法2.66%-20.34%，缩小了不同规模LLM间的性能差距，并提升跨数据集泛化能力。", "conclusion": "RAPL通过结构化检索和推理策略，显著提升了知识图谱问答的效率和效果，具有广泛适用性。"}}
{"id": "2506.09510", "pdf": "https://arxiv.org/pdf/2506.09510", "abs": "https://arxiv.org/abs/2506.09510", "authors": ["Changhao Peng", "Yuqi Ye", "Wei Gao"], "title": "Generalized Gaussian Entropy Model for Point Cloud Attribute Compression with Dynamic Likelihood Intervals", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Gaussian and Laplacian entropy models are proved effective in learned point\ncloud attribute compression, as they assist in arithmetic coding of latents.\nHowever, we demonstrate through experiments that there is still unutilized\ninformation in entropy parameters estimated by neural networks in current\nmethods, which can be used for more accurate probability estimation. Thus we\nintroduce generalized Gaussian entropy model, which controls the tail shape\nthrough shape parameter to more accurately estimate the probability of latents.\nMeanwhile, to the best of our knowledge, existing methods use fixed likelihood\nintervals for each integer during arithmetic coding, which limits model\nperformance. We propose Mean Error Discriminator (MED) to determine whether the\nentropy parameter estimation is accurate and then dynamically adjust likelihood\nintervals. Experiments show that our method significantly improves\nrate-distortion (RD) performance on three VAE-based models for point cloud\nattribute compression, and our method can be applied to other compression\ntasks, such as image and video compression.", "AI": {"tldr": "论文提出了一种广义高斯熵模型和动态调整似然区间的方法，显著提升了点云属性压缩的性能。", "motivation": "现有方法在熵参数估计中未充分利用信息，且使用固定似然区间限制了模型性能。", "method": "引入广义高斯熵模型控制尾部形状，并提出Mean Error Discriminator动态调整似然区间。", "result": "实验表明，该方法显著提升了三种基于VAE的点云属性压缩模型的率失真性能。", "conclusion": "该方法不仅适用于点云压缩，还可推广至图像和视频压缩任务。"}}
{"id": "2506.09183", "pdf": "https://arxiv.org/pdf/2506.09183", "abs": "https://arxiv.org/abs/2506.09183", "authors": ["Mingkang Wu", "Devin White", "Evelyn Rose", "Vernon Lawhern", "Nicholas R Waytowich", "Yongcan Cao"], "title": "Multi-Task Reward Learning from Human Ratings", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to the workshop on Models of Human Feedback for AI Alignment\n  at the 42nd International Conference on Machine Learning", "summary": "Reinforcement learning from human feeback (RLHF) has become a key factor in\naligning model behavior with users' goals. However, while humans integrate\nmultiple strategies when making decisions, current RLHF approaches often\nsimplify this process by modeling human reasoning through isolated tasks such\nas classification or regression. In this paper, we propose a novel\nreinforcement learning (RL) method that mimics human decision-making by jointly\nconsidering multiple tasks. Specifically, we leverage human ratings in\nreward-free environments to infer a reward function, introducing learnable\nweights that balance the contributions of both classification and regression\nmodels. This design captures the inherent uncertainty in human decision-making\nand allows the model to adaptively emphasize different strategies. We conduct\nseveral experiments using synthetic human ratings to validate the effectiveness\nof the proposed approach. Results show that our method consistently outperforms\nexisting rating-based RL methods, and in some cases, even surpasses traditional\nRL approaches.", "AI": {"tldr": "提出了一种新的强化学习方法，通过联合考虑多个任务来模拟人类决策，优于现有方法。", "motivation": "当前RLHF方法将人类决策简化为孤立任务（如分类或回归），未能充分反映人类的多策略决策过程。", "method": "利用无奖励环境中的人类评分推断奖励函数，引入可学习权重平衡分类和回归模型的贡献。", "result": "实验表明，该方法在合成人类评分上表现优于现有评分RL方法，甚至超越传统RL方法。", "conclusion": "新方法能更好地捕捉人类决策的不确定性，并自适应地调整策略。"}}
{"id": "2506.09657", "pdf": "https://arxiv.org/pdf/2506.09657", "abs": "https://arxiv.org/abs/2506.09657", "authors": ["Nikolas Evkarpidi", "Elena Tutubalina"], "title": "Bridging the Gap Between Open-Source and Proprietary LLMs in Table QA", "categories": ["cs.CL"], "comment": "Accepted for publication at the 19th International Workshop on\n  Semantic Evaluation (SemEval-2025), to be held in conjunction with ACL 2025.\n  15 pages, 5 figures", "summary": "This paper presents a system developed for SemEval 2025 Task 8: Question\nAnswering (QA) over tabular data. Our approach integrates several key\ncomponents: text-to-SQL and text-to-code generation modules, a self-correction\nmechanism, and a retrieval-augmented generation (RAG). Additionally, it\nincludes an end-to-end (E2E) module, all orchestrated by a large language model\n(LLM). Through ablation studies, we analyzed the effects of different parts of\nour pipeline and identified the challenges that are still present in this\nfield. During the evaluation phase of the competition, our solution achieved an\naccuracy of 80%, resulting in a top-13 ranking among the 38 participating\nteams. Our pipeline demonstrates a significant improvement in accuracy for\nopen-source models and achieves a performance comparable to proprietary LLMs in\nQA tasks over tables. The code is available at GitHub repository.", "AI": {"tldr": "论文介绍了一个用于SemEval 2025 Task 8的系统，结合了文本到SQL、文本到代码生成、自校正机制和检索增强生成（RAG），通过大型语言模型（LLM）协调。系统在比赛中排名前13，准确率达80%，性能接近专有LLM。", "motivation": "解决表格数据上的问答任务，提升开源模型在QA任务中的准确率。", "method": "整合文本到SQL、文本到代码生成、自校正机制、RAG和端到端模块，由LLM协调。", "result": "比赛排名前13，准确率80%，性能接近专有LLM。", "conclusion": "系统显著提升了开源模型的准确率，代码已开源。"}}
{"id": "2506.09518", "pdf": "https://arxiv.org/pdf/2506.09518", "abs": "https://arxiv.org/abs/2506.09518", "authors": ["Jianing Chen", "Zehao Li", "Yujun Cai", "Hao Jiang", "Chengxuan Qian", "Juyuan Kang", "Shuqin Gao", "Honglong Zhao", "Tianlu Mao", "Yucheng Zhang"], "title": "HAIF-GS: Hierarchical and Induced Flow-Guided Gaussian Splatting for Dynamic Scene", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing dynamic 3D scenes from monocular videos remains a fundamental\nchallenge in 3D vision. While 3D Gaussian Splatting (3DGS) achieves real-time\nrendering in static settings, extending it to dynamic scenes is challenging due\nto the difficulty of learning structured and temporally consistent motion\nrepresentations. This challenge often manifests as three limitations in\nexisting methods: redundant Gaussian updates, insufficient motion supervision,\nand weak modeling of complex non-rigid deformations. These issues collectively\nhinder coherent and efficient dynamic reconstruction. To address these\nlimitations, we propose HAIF-GS, a unified framework that enables structured\nand consistent dynamic modeling through sparse anchor-driven deformation. It\nfirst identifies motion-relevant regions via an Anchor Filter to suppresses\nredundant updates in static areas. A self-supervised Induced Flow-Guided\nDeformation module induces anchor motion using multi-frame feature aggregation,\neliminating the need for explicit flow labels. To further handle fine-grained\ndeformations, a Hierarchical Anchor Propagation mechanism increases anchor\nresolution based on motion complexity and propagates multi-level\ntransformations. Extensive experiments on synthetic and real-world benchmarks\nvalidate that HAIF-GS significantly outperforms prior dynamic 3DGS methods in\nrendering quality, temporal coherence, and reconstruction efficiency.", "AI": {"tldr": "HAIF-GS提出了一种基于稀疏锚点驱动的变形框架，用于动态3D场景重建，解决了现有方法在冗余更新、运动监督不足和非刚性变形建模上的问题。", "motivation": "动态3D场景的单目视频重建是一个基础挑战，现有方法存在冗余更新、运动监督不足和复杂非刚性变形建模弱的问题。", "method": "HAIF-GS通过锚点过滤器识别运动相关区域，利用自监督流引导变形模块和多层次锚点传播机制实现高效动态建模。", "result": "实验表明，HAIF-GS在渲染质量、时间一致性和重建效率上显著优于现有动态3DGS方法。", "conclusion": "HAIF-GS为动态3D重建提供了一种高效且一致的解决方案。"}}
{"id": "2506.09194", "pdf": "https://arxiv.org/pdf/2506.09194", "abs": "https://arxiv.org/abs/2506.09194", "authors": ["Emirhan Bilgiç", "Neslihan Serap Şengör", "Namık Berk Yalabık", "Yavuz Selim İşler", "Aykut Görkem Gelen", "Rahmi Elibol"], "title": "Integration of Contrastive Predictive Coding and Spiking Neural Networks", "categories": ["eess.SP", "cs.AI"], "comment": "4 pages, 5 figures, 1 table. Accepted at the 2025 33rd Signal\n  Processing and Communications Applications Conference (SIU)", "summary": "This study examines the integration of Contrastive Predictive Coding (CPC)\nwith Spiking Neural Networks (SNN). While CPC learns the predictive structure\nof data to generate meaningful representations, SNN mimics the computational\nprocesses of biological neural systems over time. In this study, the goal is to\ndevelop a predictive coding model with greater biological plausibility by\nprocessing inputs and outputs in a spike-based system. The proposed model was\ntested on the MNIST dataset and achieved a high classification rate in\ndistinguishing positive sequential samples from non-sequential negative\nsamples. The study demonstrates that CPC can be effectively combined with SNN,\nshowing that an SNN trained for classification tasks can also function as an\nencoding mechanism. Project codes and detailed results can be accessed on our\nGitHub page: https://github.com/vnd-ogrenme/ongorusel-kodlama/tree/main/CPC_SNN", "AI": {"tldr": "研究探讨了对比预测编码（CPC）与脉冲神经网络（SNN）的结合，旨在开发更具生物合理性的预测编码模型。", "motivation": "CPC通过学习数据的预测结构生成有意义的表示，而SNN模拟生物神经系统的计算过程。结合两者可提升模型的生物合理性。", "method": "提出了一种基于脉冲系统的预测编码模型，并在MNIST数据集上测试其性能。", "result": "模型在区分正序样本与非序样本时表现出高分类准确率，表明CPC与SNN结合有效。", "conclusion": "研究表明，CPC可与SNN有效结合，且SNN在分类任务中也能作为编码机制。"}}
{"id": "2506.09669", "pdf": "https://arxiv.org/pdf/2506.09669", "abs": "https://arxiv.org/abs/2506.09669", "authors": ["Lihu Chen", "Gaël Varoquaux"], "title": "Query-Level Uncertainty in Large Language Models", "categories": ["cs.CL"], "comment": "In Progress", "summary": "It is important for Large Language Models to be aware of the boundary of\ntheir knowledge, the mechanism of identifying known and unknown queries. This\ntype of awareness can help models perform adaptive inference, such as invoking\nRAG, engaging in slow and deep thinking, or adopting the abstention mechanism,\nwhich is beneficial to the development of efficient and trustworthy AI. In this\nwork, we propose a method to detect knowledge boundaries via Query-Level\nUncertainty, which aims to determine if the model is able to address a given\nquery without generating any tokens. To this end, we introduce a novel and\ntraining-free method called \\emph{Internal Confidence}, which leverages\nself-evaluations across layers and tokens. Empirical results on both factual QA\nand mathematical reasoning tasks demonstrate that our internal confidence can\noutperform several baselines. Furthermore, we showcase that our proposed method\ncan be used for efficient RAG and model cascading, which is able to reduce\ninference costs while maintaining performance.", "AI": {"tldr": "论文提出了一种通过查询级别不确定性检测大型语言模型知识边界的方法，利用无需训练的‘内部置信度’技术，在事实问答和数学推理任务中表现优于基线，并可用于高效RAG和模型级联。", "motivation": "大型语言模型需要明确其知识边界，以支持自适应推理（如RAG、深度思考或弃权机制），从而开发高效可信的AI。", "method": "提出‘内部置信度’方法，通过跨层和跨令牌的自我评估，无需训练即可判断模型是否能回答查询。", "result": "在事实问答和数学推理任务中，‘内部置信度’表现优于基线方法，并能有效降低推理成本。", "conclusion": "该方法为模型知识边界检测提供了高效且无需训练的解决方案，适用于多种推理场景。"}}
{"id": "2506.09522", "pdf": "https://arxiv.org/pdf/2506.09522", "abs": "https://arxiv.org/abs/2506.09522", "authors": ["Beomsik Cho", "Jaehyung Kim"], "title": "Revisit What You See: Disclose Language Prior in Vision Tokens for Efficient Guided Decoding of LVLMs", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Code available at https://github.com/bscho333/ReVisiT", "summary": "Large Vision-Language Models (LVLMs) have demonstrated remarkable performance\nacross various multimodal tasks by integrating visual perception with language\nunderstanding. However, conventional decoding strategies of LVLMs often fail to\nsuccessfully utilize visual information, leading to visually ungrounded\nresponses. While various approaches have been proposed to address this\nlimitation, they typically require additional training, multi-step inference\nprocedures, or external model dependencies. This paper introduces ReVisiT, a\nsimple yet effective decoding method that references vision tokens to guide the\ntext generation process in LVLMs. Our approach leverages the semantic\ninformation embedded within vision tokens by projecting them into the text\ntoken distribution space, and dynamically selecting the most relevant vision\ntoken at each decoding step through constrained divergence minimization. This\nselected vision token is then used to refine the output distribution to better\nincorporate visual semantics. Experiments on three LVLM hallucination\nbenchmarks with two recent LVLMs demonstrate that ReVisiT consistently enhances\nvisual grounding with minimal computational overhead. Moreover, our method\nachieves competitive or superior results relative to state-of-the-art baselines\nwhile reducing computational costs for up to $2\\times$.", "AI": {"tldr": "ReVisiT是一种简单有效的解码方法，通过引用视觉标记来指导大型视觉语言模型（LVLM）的文本生成过程，提升视觉信息的利用。", "motivation": "传统LVLM的解码策略未能充分利用视觉信息，导致视觉无关的响应。现有方法通常需要额外训练或多步推理，ReVisiT旨在以低计算成本解决这一问题。", "method": "通过将视觉标记投影到文本标记分布空间，并动态选择最相关的视觉标记来优化输出分布，从而更好地融入视觉语义。", "result": "在三个LVLM幻觉基准测试中，ReVisiT显著提升了视觉基础性，且计算开销极小，性能优于或媲美现有方法，同时计算成本降低至2倍。", "conclusion": "ReVisiT是一种高效且无需额外训练的解码方法，能够显著提升LVLM的视觉基础性，适用于实际应用。"}}
{"id": "2506.09195", "pdf": "https://arxiv.org/pdf/2506.09195", "abs": "https://arxiv.org/abs/2506.09195", "authors": ["Haoran Peng", "Ying-Jun Angela Zhang"], "title": "Graph Attention-based Decentralized Actor-Critic for Dual-Objective Control of Multi-UAV Swarms", "categories": ["eess.SP", "cs.AI", "cs.MA"], "comment": null, "summary": "This research focuses on optimizing multi-UAV systems with dual objectives:\nmaximizing service coverage as the primary goal while extending battery\nlifetime as the secondary objective. We propose a Graph Attention-based\nDecentralized Actor-Critic (GADC) to optimize the dual objectives. The proposed\napproach leverages a graph attention network to process UAVs' limited local\nobservation and reduce the dimension of the environment states. Subsequently,\nan actor-double-critic network is developed to manage dual policies for joint\nobjective optimization. The proposed GADC uses a Kullback-Leibler (KL)\ndivergence factor to balance the tradeoff between coverage performance and\nbattery lifetime in the multi-UAV system. We assess the scalability and\nefficiency of GADC through comprehensive benchmarking against state-of-the-art\nmethods, considering both theory and experimental aspects. Extensive testing in\nboth ideal settings and NVIDIA Sionna's realistic ray tracing environment\ndemonstrates GADC's superior performance.", "AI": {"tldr": "该研究提出了一种基于图注意力的分散式Actor-Critic方法（GADC），用于优化多无人机系统的服务覆盖范围和电池寿命。", "motivation": "研究旨在解决多无人机系统中服务覆盖范围最大化与电池寿命延长之间的双重目标优化问题。", "method": "采用图注意力网络处理无人机的局部观测信息，并开发了一个Actor-Double-Critic网络来管理双重策略，同时利用KL散度因子平衡目标间的权衡。", "result": "在理想和实际环境中测试表明，GADC在性能和可扩展性上优于现有方法。", "conclusion": "GADC在多无人机系统的双重目标优化中表现出色，具有实际应用潜力。"}}
{"id": "2506.09672", "pdf": "https://arxiv.org/pdf/2506.09672", "abs": "https://arxiv.org/abs/2506.09672", "authors": ["Hao Xiong", "Chuanyuan Tan", "Wenliang Chen"], "title": "Is Fine-Tuning an Effective Solution? Reassessing Knowledge Editing for Unstructured Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Unstructured Knowledge Editing (UKE) is crucial for updating the relevant\nknowledge of large language models (LLMs). It focuses on unstructured inputs,\nsuch as long or free-form texts, which are common forms of real-world\nknowledge. Although previous studies have proposed effective methods and tested\nthem, some issues exist: (1) Lack of Locality evaluation for UKE, and (2)\nAbnormal failure of fine-tuning (FT) based methods for UKE. To address these\nissues, we first construct two datasets, UnKEBench-Loc and AKEW-Loc (CF), by\nextending two existing UKE datasets with locality test data from the\nunstructured and structured views. This enables a systematic evaluation of the\nLocality of post-edited models. Furthermore, we identify four factors that may\naffect the performance of FT-based methods. Based on these factors, we conduct\nexperiments to determine how the well-performing FT-based methods should be\ntrained for the UKE task, providing a training recipe for future research. Our\nexperimental results indicate that the FT-based method with the optimal setting\n(FT-UKE) is surprisingly strong, outperforming the existing state-of-the-art\n(SOTA). In batch editing scenarios, FT-UKE shows strong performance as well,\nwith its advantage over SOTA methods increasing as the batch size grows,\nexpanding the average metric lead from +6.78% to +10.80%", "AI": {"tldr": "论文提出了一种针对大型语言模型（LLM）的无结构化知识编辑（UKE）方法，解决了现有方法在局部性评估和微调（FT）异常失败方面的问题，并通过实验验证了优化的FT方法（FT-UKE）优于现有技术。", "motivation": "无结构化知识编辑（UKE）对更新LLM的知识至关重要，但现有方法缺乏局部性评估且微调方法存在异常失败问题。", "method": "构建了两个数据集（UnKEBench-Loc和AKEW-Loc），并基于四个影响FT性能的因素，提出了优化的FT方法（FT-UKE）。", "result": "FT-UKE在单次和批量编辑场景中均优于现有技术，优势随批量增大而增加（从+6.78%到+10.80%）。", "conclusion": "优化的FT方法（FT-UKE）在无结构化知识编辑任务中表现优异，为未来研究提供了训练方案。"}}
{"id": "2506.09534", "pdf": "https://arxiv.org/pdf/2506.09534", "abs": "https://arxiv.org/abs/2506.09534", "authors": ["Tao Wang", "Mengyu Li", "Geduo Zeng", "Cheng Meng", "Qiong Zhang"], "title": "Gaussian Herding across Pens: An Optimal Transport Perspective on Global Gaussian Reduction for 3DGS", "categories": ["cs.CV", "I.4.5"], "comment": "18 pages, 8 figures", "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful technique for radiance\nfield rendering, but it typically requires millions of redundant Gaussian\nprimitives, overwhelming memory and rendering budgets. Existing compaction\napproaches address this by pruning Gaussians based on heuristic importance\nscores, without global fidelity guarantee. To bridge this gap, we propose a\nnovel optimal transport perspective that casts 3DGS compaction as global\nGaussian mixture reduction. Specifically, we first minimize the composite\ntransport divergence over a KD-tree partition to produce a compact geometric\nrepresentation, and then decouple appearance from geometry by fine-tuning color\nand opacity attributes with far fewer Gaussian primitives. Experiments on\nbenchmark datasets show that our method (i) yields negligible loss in rendering\nquality (PSNR, SSIM, LPIPS) compared to vanilla 3DGS with only 10% Gaussians;\nand (ii) consistently outperforms state-of-the-art 3DGS compaction techniques.\nNotably, our method is applicable to any stage of vanilla or accelerated 3DGS\npipelines, providing an efficient and agnostic pathway to lightweight neural\nrendering.", "AI": {"tldr": "提出了一种基于最优传输的3D高斯泼溅压缩方法，显著减少高斯基元数量，同时保持渲染质量。", "motivation": "现有3D高斯泼溅技术需要大量冗余高斯基元，占用过多内存和渲染资源，且现有压缩方法缺乏全局保真度保证。", "method": "通过最优传输视角将3D高斯泼溅压缩建模为全局高斯混合减少问题，先最小化复合传输散度生成紧凑几何表示，再解耦外观与几何属性进行微调。", "result": "实验表明，该方法仅需10%的高斯基元即可达到与原始方法相近的渲染质量（PSNR、SSIM、LPIPS），并优于现有压缩技术。", "conclusion": "该方法为轻量级神经渲染提供了一种高效且通用的解决方案，适用于任何3D高斯泼溅流程。"}}
{"id": "2506.09199", "pdf": "https://arxiv.org/pdf/2506.09199", "abs": "https://arxiv.org/abs/2506.09199", "authors": ["Hariharan Ramesh", "Jyotikrishna Dass"], "title": "FLoRIST: Singular Value Thresholding for Efficient and Accurate Federated Fine-Tuning of Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "21 pages, 12 figures", "summary": "Integrating Low-Rank Adaptation (LoRA) into federated learning offers a\npromising solution for parameter-efficient fine-tuning of Large Language Models\n(LLMs) without sharing local data. However, several methods designed for\nfederated LoRA present significant challenges in balancing communication\nefficiency, model accuracy, and computational cost, particularly among\nheterogeneous clients. These methods either rely on simplistic averaging of\nlocal adapters, which introduces aggregation noise, require transmitting large\nstacked local adapters, leading to poor communication efficiency, or\nnecessitate reconstructing memory-dense global weight-update matrix and\nperforming computationally expensive decomposition to design client-specific\nlow-rank adapters. In this work, we propose FLoRIST, a federated fine-tuning\nframework that achieves mathematically accurate aggregation without incurring\nhigh communication or computational overhead. Instead of constructing the full\nglobal weight-update matrix at the server, FLoRIST employs an efficient\ndecomposition pipeline by performing singular value decomposition on stacked\nlocal adapters separately. This approach operates within a compact intermediate\nspace to represent the accumulated information from local LoRAs. We introduce\ntunable singular value thresholding for server-side optimal rank selection to\nconstruct a pair of global low-rank adapters shared by all clients. Extensive\nempirical evaluations across multiple datasets and LLMs demonstrate that\nFLoRIST consistently strikes the best balance between superior communication\nefficiency and competitive performance in both homogeneous and heterogeneous\nsetups.", "AI": {"tldr": "FLoRIST是一种联邦学习框架，通过低秩适应（LoRA）高效微调大型语言模型（LLMs），解决了通信效率、模型精度和计算成本之间的平衡问题。", "motivation": "现有联邦LoRA方法在通信效率、模型精度和计算成本之间存在挑战，FLoRIST旨在解决这些问题。", "method": "FLoRIST通过单独对堆叠的本地适配器进行奇异值分解，避免构建全局权重更新矩阵，从而降低通信和计算开销。", "result": "实验表明，FLoRIST在多种数据集和LLMs上实现了通信效率和性能的最佳平衡。", "conclusion": "FLoRIST是一种高效的联邦微调框架，适用于异构客户端环境。"}}
{"id": "2506.09684", "pdf": "https://arxiv.org/pdf/2506.09684", "abs": "https://arxiv.org/abs/2506.09684", "authors": ["Haoyi Song", "Ruihan Ji", "Naichen Shi", "Fan Lai", "Raed Al Kontar"], "title": "Inv-Entropy: A Fully Probabilistic Framework for Uncertainty Quantification in Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have transformed natural language processing,\nbut their reliable deployment requires effective uncertainty quantification\n(UQ). Existing UQ methods are often heuristic and lack a probabilistic\nfoundation. This paper begins by providing a theoretical justification for the\nrole of perturbations in UQ for LLMs. We then introduce a dual random walk\nperspective, modeling input-output pairs as two Markov chains with transition\nprobabilities defined by semantic similarity. Building on this, we propose a\nfully probabilistic framework based on an inverse model, which quantifies\nuncertainty by evaluating the diversity of the input space conditioned on a\ngiven output through systematic perturbations. Within this framework, we define\na new uncertainty measure, Inv-Entropy. A key strength of our framework is its\nflexibility: it supports various definitions of uncertainty measures,\nembeddings, perturbation strategies, and similarity metrics. We also propose\nGAAP, a perturbation algorithm based on genetic algorithms, which enhances the\ndiversity of sampled inputs. In addition, we introduce a new evaluation metric,\nTemperature Sensitivity of Uncertainty (TSU), which directly assesses\nuncertainty without relying on correctness as a proxy. Extensive experiments\ndemonstrate that Inv-Entropy outperforms existing semantic UQ methods. The code\nto reproduce the results can be found at\nhttps://github.com/UMDataScienceLab/Uncertainty-Quantification-for-LLMs.", "AI": {"tldr": "本文提出了一种基于概率框架的LLM不确定性量化方法，通过扰动和逆模型定义新的不确定性度量Inv-Entropy，并引入GAAP扰动算法和TSU评估指标。", "motivation": "现有不确定性量化方法缺乏概率基础且多为启发式，本文旨在为LLM的不确定性量化提供理论支持和灵活框架。", "method": "采用双随机游走视角建模输入-输出对，提出基于逆模型的概率框架，定义Inv-Entropy度量，并设计GAAP扰动算法和TSU评估指标。", "result": "实验表明Inv-Entropy优于现有语义不确定性量化方法。", "conclusion": "提出的概率框架灵活且有效，为LLM的不确定性量化提供了新思路和工具。"}}
{"id": "2506.09538", "pdf": "https://arxiv.org/pdf/2506.09538", "abs": "https://arxiv.org/abs/2506.09538", "authors": ["Wenjun Ji", "Yuxiang Fu", "Luyang Ying", "Deng-Ping Fan", "Yuyi Wang", "Ming-Ming Cheng", "Ivor Tsang", "Qing Guo"], "title": "AngleRoCL: Angle-Robust Concept Learning for Physically View-Invariant T2I Adversarial Patches", "categories": ["cs.CV"], "comment": null, "summary": "Cutting-edge works have demonstrated that text-to-image (T2I) diffusion\nmodels can generate adversarial patches that mislead state-of-the-art object\ndetectors in the physical world, revealing detectors' vulnerabilities and\nrisks. However, these methods neglect the T2I patches' attack effectiveness\nwhen observed from different views in the physical world (i.e., angle\nrobustness of the T2I adversarial patches). In this paper, we study the angle\nrobustness of T2I adversarial patches comprehensively, revealing their\nangle-robust issues, demonstrating that texts affect the angle robustness of\ngenerated patches significantly, and task-specific linguistic instructions fail\nto enhance the angle robustness. Motivated by the studies, we introduce\nAngle-Robust Concept Learning (AngleRoCL), a simple and flexible approach that\nlearns a generalizable concept (i.e., text embeddings in implementation)\nrepresenting the capability of generating angle-robust patches. The learned\nconcept can be incorporated into textual prompts and guides T2I models to\ngenerate patches with their attack effectiveness inherently resistant to\nviewpoint variations. Through extensive simulation and physical-world\nexperiments on five SOTA detectors across multiple views, we demonstrate that\nAngleRoCL significantly enhances the angle robustness of T2I adversarial\npatches compared to baseline methods. Our patches maintain high attack success\nrates even under challenging viewing conditions, with over 50% average relative\nimprovement in attack effectiveness across multiple angles. This research\nadvances the understanding of physically angle-robust patches and provides\ninsights into the relationship between textual concepts and physical properties\nin T2I-generated contents.", "AI": {"tldr": "本文研究了文本到图像（T2I）扩散模型生成的对抗性补丁的角度鲁棒性问题，并提出了一种名为AngleRoCL的方法，通过学习通用概念来增强补丁的角度鲁棒性。", "motivation": "现有方法忽略了T2I对抗性补丁在物理世界中不同视角下的攻击效果，本文旨在揭示其角度鲁棒性问题，并提出解决方案。", "method": "提出Angle-Robust Concept Learning（AngleRoCL），通过学习通用概念（文本嵌入）来生成具有角度鲁棒性的补丁。", "result": "实验表明，AngleRoCL显著提升了T2I对抗性补丁的角度鲁棒性，攻击成功率在多种视角下保持较高水平，平均相对改进超过50%。", "conclusion": "本文不仅推进了对物理角度鲁棒补丁的理解，还揭示了文本概念与T2I生成内容物理属性之间的关系。"}}
{"id": "2506.09202", "pdf": "https://arxiv.org/pdf/2506.09202", "abs": "https://arxiv.org/abs/2506.09202", "authors": ["Hao Hu", "Xinqi Wang", "Simon Shaolei Du"], "title": "Policy-Based Trajectory Clustering in Offline Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce a novel task of clustering trajectories from offline\nreinforcement learning (RL) datasets, where each cluster center represents the\npolicy that generated its trajectories. By leveraging the connection between\nthe KL-divergence of offline trajectory distributions and a mixture of\npolicy-induced distributions, we formulate a natural clustering objective. To\nsolve this, we propose Policy-Guided K-means (PG-Kmeans) and Centroid-Attracted\nAutoencoder (CAAE). PG-Kmeans iteratively trains behavior cloning (BC) policies\nand assigns trajectories based on policy generation probabilities, while CAAE\nresembles the VQ-VAE framework by guiding the latent representations of\ntrajectories toward the vicinity of specific codebook entries to achieve\nclustering. Theoretically, we prove the finite-step convergence of PG-Kmeans\nand identify a key challenge in offline trajectory clustering: the inherent\nambiguity of optimal solutions due to policy-induced conflicts, which can\nresult in multiple equally valid but structurally distinct clusterings.\nExperimentally, we validate our methods on the widely used D4RL dataset and\ncustom GridWorld environments. Our results show that both PG-Kmeans and CAAE\neffectively partition trajectories into meaningful clusters. They offer a\npromising framework for policy-based trajectory clustering, with broad\napplications in offline RL and beyond.", "AI": {"tldr": "论文提出了一种新的任务：从离线强化学习数据集中聚类轨迹，每个聚类中心代表生成其轨迹的策略。通过KL散度和策略诱导分布的混合，提出了聚类目标，并提出了PG-Kmeans和CAAE两种方法。理论证明了PG-Kmeans的有限步收敛性，并指出了离线轨迹聚类的挑战。实验验证了方法的有效性。", "motivation": "离线强化学习数据集中的轨迹聚类是一个新任务，旨在通过聚类揭示生成轨迹的策略，为离线RL等领域提供新工具。", "method": "提出了PG-Kmeans和CAAE两种方法：PG-Kmeans通过迭代训练行为克隆策略并基于策略生成概率分配轨迹；CAAE利用VQ-VAE框架，引导轨迹的潜在表示向特定码本条目靠近以实现聚类。", "result": "在D4RL数据集和自定义GridWorld环境中验证了PG-Kmeans和CAAE的有效性，能够将轨迹划分为有意义的聚类。", "conclusion": "PG-Kmeans和CAAE为基于策略的轨迹聚类提供了有前景的框架，适用于离线RL及其他领域。"}}
{"id": "2506.09790", "pdf": "https://arxiv.org/pdf/2506.09790", "abs": "https://arxiv.org/abs/2506.09790", "authors": ["Zhenran Xu", "Yiyu Wang", "Xue Yang", "Longyue Wang", "Weihua Luo", "Kaifu Zhang", "Baotian Hu", "Min Zhang"], "title": "ComfyUI-R1: Exploring Reasoning Models for Workflow Generation", "categories": ["cs.CL", "cs.CV", "cs.SE"], "comment": "Work in progress. Try it out in ComfyUI-Copilot\n  https://github.com/AIDC-AI/ComfyUI-Copilot", "summary": "AI-generated content has evolved from monolithic models to modular workflows,\nparticularly on platforms like ComfyUI, enabling customization in creative\npipelines. However, crafting effective workflows requires great expertise to\norchestrate numerous specialized components, presenting a steep learning curve\nfor users. To address this challenge, we introduce ComfyUI-R1, the first large\nreasoning model for automated workflow generation. Starting with our curated\ndataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning\ndata, including node selection, workflow planning, and code-level workflow\nrepresentation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT\nfine-tuning for cold start, adapting models to the ComfyUI domain; (2)\nreinforcement learning for incentivizing reasoning capability, guided by a\nfine-grained rule-metric hybrid reward, ensuring format validity, structural\nintegrity, and node-level fidelity. Experiments show that our 7B-parameter\nmodel achieves a 97\\% format validity rate, along with high pass rate,\nnode-level and graph-level F1 scores, significantly surpassing prior\nstate-of-the-art methods that employ leading closed-source models such as\nGPT-4o and Claude series. Further analysis highlights the critical role of the\nreasoning process and the advantage of transforming workflows into code.\nQualitative comparison reveals our strength in synthesizing intricate workflows\nwith diverse nodes, underscoring the potential of long CoT reasoning in AI art\ncreation.", "AI": {"tldr": "ComfyUI-R1是一个用于自动生成AI工作流的大型推理模型，通过两阶段训练框架显著提升了工作流生成的准确性和复杂性。", "motivation": "AI生成内容的工作流定制需要专业知识，学习曲线陡峭，ComfyUI-R1旨在降低这一门槛。", "method": "使用4K工作流数据集构建长链推理数据，通过两阶段训练（CoT微调和强化学习）优化模型。", "result": "7B参数模型在格式有效性、节点和图形级别F1分数上显著优于GPT-4o和Claude系列。", "conclusion": "长链推理和代码化工作流在AI艺术创作中具有重要潜力，ComfyUI-R1展示了其优势。"}}
{"id": "2506.09541", "pdf": "https://arxiv.org/pdf/2506.09541", "abs": "https://arxiv.org/abs/2506.09541", "authors": ["Yi Zhang", "Yi Wang", "Yawen Cui", "Lap-Pui Chau"], "title": "3DGeoDet: General-purpose Geometry-aware Image-based 3D Object Detection", "categories": ["cs.CV"], "comment": "Accepted by IEEE Transactions on Multimedia", "summary": "This paper proposes 3DGeoDet, a novel geometry-aware 3D object detection\napproach that effectively handles single- and multi-view RGB images in indoor\nand outdoor environments, showcasing its general-purpose applicability. The key\nchallenge for image-based 3D object detection tasks is the lack of 3D geometric\ncues, which leads to ambiguity in establishing correspondences between images\nand 3D representations. To tackle this problem, 3DGeoDet generates efficient 3D\ngeometric representations in both explicit and implicit manners based on\npredicted depth information. Specifically, we utilize the predicted depth to\nlearn voxel occupancy and optimize the voxelized 3D feature volume explicitly\nthrough the proposed voxel occupancy attention. To further enhance 3D\nawareness, the feature volume is integrated with an implicit 3D representation,\nthe truncated signed distance function (TSDF). Without requiring supervision\nfrom 3D signals, we significantly improve the model's comprehension of 3D\ngeometry by leveraging intermediate 3D representations and achieve end-to-end\ntraining. Our approach surpasses the performance of state-of-the-art\nimage-based methods on both single- and multi-view benchmark datasets across\ndiverse environments, achieving a 9.3 mAP@0.5 improvement on the SUN RGB-D\ndataset, a 3.3 mAP@0.5 improvement on the ScanNetV2 dataset, and a 0.19\nAP3D@0.7 improvement on the KITTI dataset. The project page is available at:\nhttps://cindy0725.github.io/3DGeoDet/.", "AI": {"tldr": "3DGeoDet是一种新颖的几何感知3D物体检测方法，通过显式和隐式3D几何表示提升性能，无需3D信号监督，在多个数据集上表现优异。", "motivation": "解决基于图像的3D物体检测任务中缺乏3D几何线索导致的模糊性问题。", "method": "结合预测深度信息生成显式（体素占用注意力）和隐式（TSDF）3D几何表示，实现端到端训练。", "result": "在SUN RGB-D、ScanNetV2和KITTI数据集上分别提升9.3 mAP@0.5、3.3 mAP@0.5和0.19 AP3D@0.7。", "conclusion": "3DGeoDet通过几何表示显著提升模型对3D几何的理解，适用于多样环境，性能优于现有方法。"}}
{"id": "2506.09204", "pdf": "https://arxiv.org/pdf/2506.09204", "abs": "https://arxiv.org/abs/2506.09204", "authors": ["Xiaotian Chen", "Hongyun Liu", "Seyed Sahand Mohammadi Ziabari"], "title": "A Topological Improvement of the Overall Performance of Sparse Evolutionary Training: Motif-Based Structural Optimization of Sparse MLPs Project", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "Deep Neural Networks (DNNs) have been proven to be exceptionally effective\nand have been applied across diverse domains within deep learning. However, as\nDNN models increase in complexity, the demand for reduced computational costs\nand memory overheads has become increasingly urgent. Sparsity has emerged as a\nleading approach in this area. The robustness of sparse Multi-layer Perceptrons\n(MLPs) for supervised feature selection, along with the application of Sparse\nEvolutionary Training (SET), illustrates the feasibility of reducing\ncomputational costs without compromising accuracy. Moreover, it is believed\nthat the SET algorithm can still be improved through a structural optimization\nmethod called motif-based optimization, with potential efficiency gains\nexceeding 40% and a performance decline of under 4%. This research investigates\nwhether the structural optimization of Sparse Evolutionary Training applied to\nMulti-layer Perceptrons (SET-MLP) can enhance performance and to what extent\nthis improvement can be achieved.", "AI": {"tldr": "论文探讨了通过结构优化提升稀疏进化训练（SET-MLP）性能的可行性，预计效率提升超40%，性能下降低于4%。", "motivation": "随着DNN模型复杂度增加，降低计算成本和内存开销的需求日益迫切，稀疏性成为重要解决方案。", "method": "采用稀疏进化训练（SET）和基于基序的优化方法，对稀疏多层感知机（MLPs）进行结构优化。", "result": "研究表明，SET-MLP通过结构优化可实现效率提升超40%，性能下降低于4%。", "conclusion": "结构优化可显著提升SET-MLP性能，同时保持高计算效率。"}}
{"id": "2506.09796", "pdf": "https://arxiv.org/pdf/2506.09796", "abs": "https://arxiv.org/abs/2506.09796", "authors": ["Andreas Säuberli", "Diego Frassinelli", "Barbara Plank"], "title": "Do LLMs Give Psychometrically Plausible Responses in Educational Assessments?", "categories": ["cs.CL"], "comment": "Accepted for publication at the 20th Workshop on Innovative Use of\n  NLP for Building Educational Applications (BEA) at ACL 2025", "summary": "Knowing how test takers answer items in educational assessments is essential\nfor test development, to evaluate item quality, and to improve test validity.\nHowever, this process usually requires extensive pilot studies with human\nparticipants. If large language models (LLMs) exhibit human-like response\nbehavior to test items, this could open up the possibility of using them as\npilot participants to accelerate test development. In this paper, we evaluate\nthe human-likeness or psychometric plausibility of responses from 18\ninstruction-tuned LLMs with two publicly available datasets of multiple-choice\ntest items across three subjects: reading, U.S. history, and economics. Our\nmethodology builds on two theoretical frameworks from psychometrics which are\ncommonly used in educational assessment, classical test theory and item\nresponse theory. The results show that while larger models are excessively\nconfident, their response distributions can be more human-like when calibrated\nwith temperature scaling. In addition, we find that LLMs tend to correlate\nbetter with humans in reading comprehension items compared to other subjects.\nHowever, the correlations are not very strong overall, indicating that LLMs\nshould not be used for piloting educational assessments in a zero-shot setting.", "AI": {"tldr": "研究评估了18种指令调优的大型语言模型（LLMs）在多项选择题测试中的反应是否具有人类相似性或心理测量合理性，发现校准后模型反应更接近人类，但在零样本设置下不适合用于教育评估的预测试。", "motivation": "探索LLMs是否能模拟人类对测试题目的反应行为，以加速测试开发过程，减少对真人预测试的依赖。", "method": "基于心理测量学的经典测试理论和项目反应理论，使用两个公开的多项选择题数据集（阅读、美国历史、经济学）评估LLMs的反应分布。", "result": "较大模型过于自信，但通过温度校准后反应分布更接近人类；LLMs在阅读理解题目中与人类相关性较高，但整体相关性不强。", "conclusion": "LLMs在零样本设置下不适合直接用于教育评估的预测试，需进一步研究改进。"}}
{"id": "2506.09553", "pdf": "https://arxiv.org/pdf/2506.09553", "abs": "https://arxiv.org/abs/2506.09553", "authors": ["Ligao Deng", "Yupeng Deng", "Yu Meng", "Jingbo Chen", "Zhihao Xi", "Diyou Liu", "Qifeng Chu"], "title": "GLD-Road:A global-local decoding road network extraction model for remote sensing images", "categories": ["cs.CV"], "comment": null, "summary": "Road networks are crucial for mapping, autonomous driving, and disaster\nresponse. While manual annotation is costly, deep learning offers efficient\nextraction. Current methods include postprocessing (prone to errors), global\nparallel (fast but misses nodes), and local iterative (accurate but slow). We\npropose GLD-Road, a two-stage model combining global efficiency and local\nprecision. First, it detects road nodes and connects them via a Connect Module.\nThen, it iteratively refines broken roads using local searches, drastically\nreducing computation. Experiments show GLD-Road outperforms state-of-the-art\nmethods, improving APLS by 1.9% (City-Scale) and 0.67% (SpaceNet3). It also\nreduces retrieval time by 40% vs. Sat2Graph (global) and 92% vs. RNGDet++\n(local). The experimental results are available at\nhttps://github.com/ucas-dlg/GLD-Road.", "AI": {"tldr": "GLD-Road是一种两阶段模型，结合全局效率和局部精度，用于高效提取道路网络，显著减少计算时间并提升性能。", "motivation": "手动标注道路网络成本高，现有深度学习方法存在效率或精度问题，需要一种兼顾两者的解决方案。", "method": "GLD-Road分为两阶段：全局检测道路节点并连接，局部迭代修复断裂道路。", "result": "GLD-Road在City-Scale和SpaceNet3上分别提升APLS 1.9%和0.67%，检索时间减少40%（对比Sat2Graph）和92%（对比RNGDet++）。", "conclusion": "GLD-Road在道路网络提取中实现了高效与高精度的平衡，优于现有方法。"}}
{"id": "2506.09206", "pdf": "https://arxiv.org/pdf/2506.09206", "abs": "https://arxiv.org/abs/2506.09206", "authors": ["Ahmed Adel Attia", "Jing Liu", "Carl Espy-Wilson"], "title": "SimClass: A Classroom Speech Dataset Generated via Game Engine Simulation For Automatic Speech Recognition Research", "categories": ["cs.SD", "cs.AI", "cs.CL", "eess.AS"], "comment": null, "summary": "The scarcity of large-scale classroom speech data has hindered the\ndevelopment of AI-driven speech models for education. Public classroom datasets\nremain limited, and the lack of a dedicated classroom noise corpus prevents the\nuse of standard data augmentation techniques.\n  In this paper, we introduce a scalable methodology for synthesizing classroom\nnoise using game engines, a framework that extends to other domains. Using this\nmethodology, we present SimClass, a dataset that includes both a synthesized\nclassroom noise corpus and a simulated classroom speech dataset. The speech\ndata is generated by pairing a public children's speech corpus with YouTube\nlecture videos to approximate real classroom interactions in clean conditions.\nOur experiments on clean and noisy speech demonstrate that SimClass closely\napproximates real classroom speech, making it a valuable resource for\ndeveloping robust speech recognition and enhancement models.", "AI": {"tldr": "论文提出了一种利用游戏引擎合成教室噪声的方法，并发布了SimClass数据集，包含合成噪声和模拟教室语音数据，为语音识别和增强模型提供了资源。", "motivation": "大规模教室语音数据的稀缺限制了教育领域AI语音模型的发展，现有公共数据集有限且缺乏专用噪声库。", "method": "使用游戏引擎合成教室噪声，并将公开儿童语音库与YouTube讲座视频结合生成模拟教室语音数据。", "result": "实验表明SimClass能近似真实教室语音，适用于开发鲁棒的语音识别和增强模型。", "conclusion": "SimClass为解决教室语音数据稀缺问题提供了有效方案，并可推广至其他领域。"}}
{"id": "2506.09820", "pdf": "https://arxiv.org/pdf/2506.09820", "abs": "https://arxiv.org/abs/2506.09820", "authors": ["Chengpeng Li", "Zhengyang Tang", "Ziniu Li", "Mingfeng Xue", "Keqin Bao", "Tian Ding", "Ruoyu Sun", "Benyou Wang", "Xiang Wang", "Junyang Lin", "Dayiheng Liu"], "title": "CoRT: Code-integrated Reasoning within Thinking", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "work in progress", "summary": "Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable\nprogress in natural language reasoning with long chain-of-thought (CoT), yet\nthey remain inefficient or inaccurate when handling complex mathematical\noperations. Addressing these limitations through computational tools (e.g.,\ncomputation libraries and symbolic solvers) is promising, but it introduces a\ntechnical challenge: Code Interpreter (CI) brings external knowledge beyond the\nmodel's internal text representations, thus the direct combination is not\nefficient. This paper introduces CoRT, a post-training framework for teaching\nLRMs to leverage CI effectively and efficiently. As a first step, we address\nthe data scarcity issue by synthesizing code-integrated reasoning data through\nHint-Engineering, which strategically inserts different hints at appropriate\npositions to optimize LRM-CI interaction. We manually create 30 high-quality\nsamples, upon which we post-train models ranging from 1.5B to 32B parameters,\nwith supervised fine-tuning, rejection fine-tuning and reinforcement learning.\nOur experimental results demonstrate that Hint-Engineering models achieve 4\\%\nand 8\\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and\nDeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging\nmathematical reasoning datasets. Furthermore, Hint-Engineering models use about\n30\\% fewer tokens for the 32B model and 50\\% fewer tokens for the 1.5B model\ncompared with the natural language models. The models and code are available at\nhttps://github.com/ChengpengLi1003/CoRT.", "AI": {"tldr": "CoRT框架通过Hint-Engineering优化大型推理模型（LRM）与代码解释器（CI）的交互，显著提升数学推理能力并减少计算开销。", "motivation": "解决大型推理模型在复杂数学运算中的低效和不准确问题，同时避免直接结合外部计算工具带来的技术挑战。", "method": "通过Hint-Engineering合成代码集成推理数据，结合监督微调、拒绝微调和强化学习对模型进行后训练。", "result": "模型在五个数学推理数据集上性能提升4%-8%，计算开销减少30%-50%。", "conclusion": "CoRT框架有效提升了LRM与CI的交互效率，为复杂推理任务提供了实用解决方案。"}}
{"id": "2506.09557", "pdf": "https://arxiv.org/pdf/2506.09557", "abs": "https://arxiv.org/abs/2506.09557", "authors": ["Zhaoyang Wei", "Chenhui Qiang", "Bowen Jiang", "Xumeng Han", "Xuehui Yu", "Zhenjun Han"], "title": "AD^2-Bench: A Hierarchical CoT Benchmark for MLLM in Autonomous Driving under Adverse Conditions", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Chain-of-Thought (CoT) reasoning has emerged as a powerful approach to\nenhance the structured, multi-step decision-making capabilities of Multi-Modal\nLarge Models (MLLMs), is particularly crucial for autonomous driving with\nadverse weather conditions and complex traffic environments. However, existing\nbenchmarks have largely overlooked the need for rigorous evaluation of CoT\nprocesses in these specific and challenging scenarios. To address this critical\ngap, we introduce AD^2-Bench, the first Chain-of-Thought benchmark specifically\ndesigned for autonomous driving with adverse weather and complex scenes.\nAD^2-Bench is meticulously constructed to fulfill three key criteria:\ncomprehensive data coverage across diverse adverse environments, fine-grained\nannotations that support multi-step reasoning, and a dedicated evaluation\nframework tailored for assessing CoT performance. The core contribution of\nAD^2-Bench is its extensive collection of over 5.4k high-quality, manually\nannotated CoT instances. Each intermediate reasoning step in these annotations\nis treated as an atomic unit with explicit ground truth, enabling unprecedented\nfine-grained analysis of MLLMs' inferential processes under text-level,\npoint-level, and region-level visual prompts. Our comprehensive evaluation of\nstate-of-the-art MLLMs on AD^2-Bench reveals accuracy below 60%, highlighting\nthe benchmark's difficulty and the need to advance robust, interpretable\nend-to-end autonomous driving systems. AD^2-Bench thus provides a standardized\nevaluation platform, driving research forward by improving MLLMs' reasoning in\nautonomous driving, making it an invaluable resource.", "AI": {"tldr": "AD^2-Bench是首个针对恶劣天气和复杂场景下自动驾驶的Chain-of-Thought（CoT）基准测试，填补了现有评估的空白。", "motivation": "现有基准测试未充分评估CoT过程在恶劣天气和复杂交通环境中的表现，亟需专门工具推动研究。", "method": "构建AD^2-Bench，包含5.4k高质量人工标注的CoT实例，支持多步推理和细粒度评估。", "result": "当前最先进MLLMs在AD^2-Bench上的准确率低于60%，显示其挑战性。", "conclusion": "AD^2-Bench为自动驾驶研究提供了标准化评估平台，推动MLLMs推理能力的提升。"}}
{"id": "2506.09215", "pdf": "https://arxiv.org/pdf/2506.09215", "abs": "https://arxiv.org/abs/2506.09215", "authors": ["Greyson Brothers"], "title": "Robust Noise Attenuation via Adaptive Pooling of Transformer Outputs", "categories": ["cs.LG", "cs.AI", "68T07 (Primary), 68P30, 68T45 (Secondary)", "E.4; I.2.6; I.2.10"], "comment": "[ICML 2025 Spotlight Poster] To be published in the Forty-Second\n  International Conference on Machine Learning (ICML) Proceedings", "summary": "We investigate the design of pooling methods used to summarize the outputs of\ntransformer embedding models, primarily motivated by reinforcement learning and\nvision applications. This work considers problems where a subset of the input\nvectors contains requisite information for a downstream task (signal) while the\nrest are distractors (noise). By framing pooling as vector quantization with\nthe goal of minimizing signal loss, we demonstrate that the standard methods\nused to aggregate transformer outputs, AvgPool, MaxPool, and ClsToken, are\nvulnerable to performance collapse as the signal-to-noise ratio (SNR) of inputs\nfluctuates. We then show that an attention-based adaptive pooling method can\napproximate the signal-optimal vector quantizer within derived error bounds for\nany SNR. Our theoretical results are first validated by supervised experiments\non a synthetic dataset designed to isolate the SNR problem, then generalized to\nstandard relational reasoning, multi-agent reinforcement learning, and vision\nbenchmarks with noisy observations, where transformers with adaptive pooling\ndisplay superior robustness across tasks.", "AI": {"tldr": "论文研究了用于总结Transformer嵌入模型输出的池化方法设计，提出了一种基于注意力的自适应池化方法，优于标准池化方法（AvgPool、MaxPool、ClsToken），在信号噪声比（SNR）波动时表现更稳健。", "motivation": "主要动机是解决在强化学习和视觉应用中，标准池化方法在信号噪声比波动时性能下降的问题。", "method": "通过将池化问题建模为向量量化，提出了一种基于注意力的自适应池化方法，以最小化信号损失。", "result": "理论分析和实验验证表明，自适应池化方法在合成数据集和实际任务（关系推理、多智能体强化学习、视觉基准）中均表现出更强的鲁棒性。", "conclusion": "自适应池化方法在信号噪声比波动时优于标准池化方法，适用于多种任务。"}}
{"id": "2506.09827", "pdf": "https://arxiv.org/pdf/2506.09827", "abs": "https://arxiv.org/abs/2506.09827", "authors": ["Christoph Schuhmann", "Robert Kaczmarczyk", "Gollam Rabby", "Felix Friedrich", "Maurice Kraus", "Kourosh Nadi", "Huu Nguyen", "Kristian Kersting", "Sören Auer"], "title": "EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The advancement of text-to-speech and audio generation models necessitates\nrobust benchmarks for evaluating the emotional understanding capabilities of AI\nsystems. Current speech emotion recognition (SER) datasets often exhibit\nlimitations in emotional granularity, privacy concerns, or reliance on acted\nportrayals. This paper introduces EmoNet-Voice, a new resource for speech\nemotion detection, which includes EmoNet-Voice Big, a large-scale pre-training\ndataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions,\nand 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human\nexpert annotations. EmoNet-Voice is designed to evaluate SER models on a\nfine-grained spectrum of 40 emotion categories with different levels of\nintensities. Leveraging state-of-the-art voice generation, we curated synthetic\naudio snippets simulating actors portraying scenes designed to evoke specific\nemotions. Crucially, we conducted rigorous validation by psychology experts who\nassigned perceived intensity labels. This synthetic, privacy-preserving\napproach allows for the inclusion of sensitive emotional states often absent in\nexisting datasets. Lastly, we introduce Empathic Insight Voice models that set\na new standard in speech emotion recognition with high agreement with human\nexperts. Our evaluations across the current model landscape exhibit valuable\nfindings, such as high-arousal emotions like anger being much easier to detect\nthan low-arousal states like concentration.", "AI": {"tldr": "EmoNet-Voice是一个新的语音情感检测资源，包括大规模预训练数据集和专家标注的基准数据集，旨在评估40种细粒度情感类别的识别能力。", "motivation": "当前语音情感识别数据集在情感粒度、隐私问题或依赖表演数据方面存在局限性，需要更全面的评估工具。", "method": "利用先进的语音生成技术合成音频片段，模拟特定情感场景，并由心理学专家验证和标注情感强度。", "result": "Empathic Insight Voice模型在语音情感识别中表现优异，与人类专家高度一致，且高唤醒情绪（如愤怒）比低唤醒状态（如专注）更易检测。", "conclusion": "EmoNet-Voice为语音情感识别提供了新的标准和资源，填补了现有数据集的空白，并展示了合成数据的潜力。"}}
{"id": "2506.09565", "pdf": "https://arxiv.org/pdf/2506.09565", "abs": "https://arxiv.org/abs/2506.09565", "authors": ["Qijing Li", "Jingxiang Sun", "Liang An", "Zhaoqi Su", "Hongwen Zhang", "Yebin Liu"], "title": "SemanticSplat: Feed-Forward 3D Scene Understanding with Language-Aware Gaussian Fields", "categories": ["cs.CV"], "comment": null, "summary": "Holistic 3D scene understanding, which jointly models geometry, appearance,\nand semantics, is crucial for applications like augmented reality and robotic\ninteraction. Existing feed-forward 3D scene understanding methods (e.g., LSM)\nare limited to extracting language-based semantics from scenes, failing to\nachieve holistic scene comprehension. Additionally, they suffer from\nlow-quality geometry reconstruction and noisy artifacts. In contrast, per-scene\noptimization methods rely on dense input views, which reduces practicality and\nincreases complexity during deployment. In this paper, we propose\nSemanticSplat, a feed-forward semantic-aware 3D reconstruction method, which\nunifies 3D Gaussians with latent semantic attributes for joint\ngeometry-appearance-semantics modeling. To predict the semantic anisotropic\nGaussians, SemanticSplat fuses diverse feature fields (e.g., LSeg, SAM) with a\ncost volume representation that stores cross-view feature similarities,\nenhancing coherent and accurate scene comprehension. Leveraging a two-stage\ndistillation framework, SemanticSplat reconstructs a holistic multi-modal\nsemantic feature field from sparse-view images. Experiments demonstrate the\neffectiveness of our method for 3D scene understanding tasks like promptable\nand open-vocabulary segmentation. Video results are available at\nhttps://semanticsplat.github.io.", "AI": {"tldr": "SemanticSplat提出了一种前馈式语义感知3D重建方法，通过结合3D高斯和潜在语义属性，实现几何-外观-语义的联合建模。", "motivation": "现有方法在提取语言语义时存在局限性，且几何重建质量低、噪声多；而基于密集视图的优化方法实用性差。", "method": "融合多种特征场（如LSeg、SAM）和成本体积表示，预测语义各向异性高斯，并通过两阶段蒸馏框架从稀疏视图重建多模态语义特征场。", "result": "实验证明该方法在可提示和开放词汇分割等3D场景理解任务中表现优异。", "conclusion": "SemanticSplat通过联合建模实现了更全面的3D场景理解，适用于增强现实和机器人交互等应用。"}}
{"id": "2506.09833", "pdf": "https://arxiv.org/pdf/2506.09833", "abs": "https://arxiv.org/abs/2506.09833", "authors": ["Omar Sherif", "Ali Hamdi"], "title": "Error-Guided Pose Augmentation: Enhancing Rehabilitation Exercise Assessment through Targeted Data Generation", "categories": ["cs.CL", "I.2.1"], "comment": "6 pages, 1 figure. To appear in Intelligent Methods, Systems, and\n  Applications 2025", "summary": "Effective rehabilitation assessment is essential for monitoring patient\nprogress, particularly in home-based settings. Existing systems often face\nchallenges such as data imbalance and difficulty detecting subtle movement\nerrors. This paper introduces Error-Guided Pose Augmentation (EGPA), a method\nthat generates synthetic skeleton data by simulating clinically relevant\nmovement mistakes. Unlike standard augmentation techniques, EGPA targets\nbiomechanical errors observed in rehabilitation. Combined with an\nattention-based graph convolutional network, EGPA improves performance across\nmultiple evaluation metrics. Experiments demonstrate reductions in mean\nabsolute error of up to 27.6 percent and gains in error classification accuracy\nof 45.8 percent. Attention visualizations show that the model learns to focus\non clinically significant joints and movement phases, enhancing both accuracy\nand interpretability. EGPA offers a promising approach for improving automated\nmovement quality assessment in both clinical and home-based rehabilitation\ncontexts.", "AI": {"tldr": "论文提出了一种名为EGPA的方法，通过模拟临床相关运动错误生成合成骨架数据，结合注意力图卷积网络，显著提升了康复评估的准确性和可解释性。", "motivation": "现有康复评估系统存在数据不平衡和难以检测细微运动错误的问题，需要一种更有效的方法来提升评估性能。", "method": "提出Error-Guided Pose Augmentation (EGPA)方法，模拟临床运动错误生成合成数据，并结合注意力图卷积网络进行训练。", "result": "实验显示，EGPA将平均绝对误差降低27.6%，错误分类准确率提升45.8%，模型能聚焦临床关键关节和运动阶段。", "conclusion": "EGPA为临床和家庭康复中的运动质量评估提供了一种高效且可解释的解决方案。"}}
{"id": "2506.09612", "pdf": "https://arxiv.org/pdf/2506.09612", "abs": "https://arxiv.org/abs/2506.09612", "authors": ["Mingxiao LI", "mang ning", "Marie-Francine Moens"], "title": "Consistent Story Generation with Asymmetry Zigzag Sampling", "categories": ["cs.CV"], "comment": "17 pages, 9. figures", "summary": "Text-to-image generation models have made significant progress in producing\nhigh-quality images from textual descriptions, yet they continue to struggle\nwith maintaining subject consistency across multiple images, a fundamental\nrequirement for visual storytelling. Existing methods attempt to address this\nby either fine-tuning models on large-scale story visualization datasets, which\nis resource-intensive, or by using training-free techniques that share\ninformation across generations, which still yield limited success. In this\npaper, we introduce a novel training-free sampling strategy called Zigzag\nSampling with Asymmetric Prompts and Visual Sharing to enhance subject\nconsistency in visual story generation. Our approach proposes a zigzag sampling\nmechanism that alternates between asymmetric prompting to retain subject\ncharacteristics, while a visual sharing module transfers visual cues across\ngenerated images to %further enforce consistency. Experimental results, based\non both quantitative metrics and qualitative evaluations, demonstrate that our\nmethod significantly outperforms previous approaches in generating coherent and\nconsistent visual stories. The code is available at\nhttps://github.com/Mingxiao-Li/Asymmetry-Zigzag-StoryDiffusion.", "AI": {"tldr": "提出了一种名为Zigzag Sampling with Asymmetric Prompts and Visual Sharing的训练无关采样策略，用于提升视觉故事生成中的主题一致性。", "motivation": "现有方法在生成多张图像时难以保持主题一致性，而资源密集型微调或无训练技术效果有限。", "method": "采用Zigzag采样机制，交替使用非对称提示和视觉共享模块，以保持主题特征并传递视觉线索。", "result": "实验表明，该方法在生成连贯一致的视觉故事方面显著优于先前方法。", "conclusion": "提出的方法有效提升了视觉故事生成的主题一致性，且无需额外训练。"}}
{"id": "2506.09847", "pdf": "https://arxiv.org/pdf/2506.09847", "abs": "https://arxiv.org/abs/2506.09847", "authors": ["Tomas Peterka", "Matyas Bohacek"], "title": "Dataset of News Articles with Provenance Metadata for Media Relevance Assessment", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.CY"], "comment": null, "summary": "Out-of-context and misattributed imagery is the leading form of media\nmanipulation in today's misinformation and disinformation landscape. The\nexisting methods attempting to detect this practice often only consider whether\nthe semantics of the imagery corresponds to the text narrative, missing\nmanipulation so long as the depicted objects or scenes somewhat correspond to\nthe narrative at hand. To tackle this, we introduce News Media Provenance\nDataset, a dataset of news articles with provenance-tagged images. We formulate\ntwo tasks on this dataset, location of origin relevance (LOR) and date and time\nof origin relevance (DTOR), and present baseline results on six large language\nmodels (LLMs). We identify that, while the zero-shot performance on LOR is\npromising, the performance on DTOR hinders, leaving room for specialized\narchitectures and future work.", "AI": {"tldr": "论文提出了一种检测新闻图片来源相关性的方法，并构建了News Media Provenance Dataset数据集，评估了六种大型语言模型在位置和时间相关性任务上的表现。", "motivation": "当前检测媒体操纵的方法仅关注图像语义与文本叙述的匹配，忽略了来源相关性的重要性。", "method": "构建了News Media Provenance Dataset数据集，并设计了位置来源相关性（LOR）和时间来源相关性（DTOR）两项任务，评估了六种大型语言模型的零样本表现。", "result": "LOR任务的零样本表现较好，但DTOR任务表现较差，表明需要进一步优化。", "conclusion": "研究揭示了检测新闻图片来源相关性的重要性，并指出了未来在专门架构上的研究方向。"}}
{"id": "2506.09626", "pdf": "https://arxiv.org/pdf/2506.09626", "abs": "https://arxiv.org/abs/2506.09626", "authors": ["Giacomo Rosin", "Muhammad Rameez Ur Rahman", "Sebastiano Vascon"], "title": "ECAM: A Contrastive Learning Approach to Avoid Environmental Collision in Trajectory Forecasting", "categories": ["cs.CV"], "comment": "IJCNN 2025", "summary": "Human trajectory forecasting is crucial in applications such as autonomous\ndriving, robotics and surveillance. Accurate forecasting requires models to\nconsider various factors, including social interactions, multi-modal\npredictions, pedestrian intention and environmental context. While existing\nmethods account for these factors, they often overlook the impact of the\nenvironment, which leads to collisions with obstacles. This paper introduces\nECAM (Environmental Collision Avoidance Module), a contrastive learning-based\nmodule to enhance collision avoidance ability with the environment. The\nproposed module can be integrated into existing trajectory forecasting models,\nimproving their ability to generate collision-free predictions. We evaluate our\nmethod on the ETH/UCY dataset and quantitatively and qualitatively demonstrate\nits collision avoidance capabilities. Our experiments show that\nstate-of-the-art methods significantly reduce (-40/50%) the collision rate when\nintegrated with the proposed module. The code is available at\nhttps://github.com/CVML-CFU/ECAM.", "AI": {"tldr": "论文提出了一种基于对比学习的模块ECAM，用于增强轨迹预测模型的环境碰撞避免能力，显著降低了碰撞率。", "motivation": "现有轨迹预测方法常忽略环境因素的影响，导致预测轨迹与障碍物碰撞。", "method": "引入ECAM（Environmental Collision Avoidance Module），通过对比学习提升模型的环境碰撞避免能力，并可集成到现有模型中。", "result": "在ETH/UCY数据集上测试，集成ECAM的模型碰撞率显著降低40-50%。", "conclusion": "ECAM模块有效提升了轨迹预测模型的环境碰撞避免能力，具有实际应用价值。"}}
{"id": "2506.09268", "pdf": "https://arxiv.org/pdf/2506.09268", "abs": "https://arxiv.org/abs/2506.09268", "authors": ["Henri Alam", "Antonio de Domenico", "Tareq Si Salem", "Florian Kaltenberger"], "title": "A Multi-Armed Bandit Framework for Online Optimisation in Green Integrated Terrestrial and Non-Terrestrial Networks", "categories": ["cs.NI", "cs.AI"], "comment": "To be published in 2025 IEEE International Workshop on Signal\n  Processing and Artificial Intelligence in Wireless Communications (IEEE SPAWC\n  2025)", "summary": "Integrated terrestrial and non-terrestrial network (TN-NTN) architectures\noffer a promising solution for expanding coverage and improving capacity for\nthe network. While non-terrestrial networks (NTNs) are primarily exploited for\nthese specific reasons, their role in alleviating terrestrial network (TN) load\nand enabling energy-efficient operation has received comparatively less\nattention. In light of growing concerns associated with the densification of\nterrestrial deployments, this work aims to explore the potential of NTNs in\nsupporting a more sustainable network. In this paper, we propose a novel online\noptimisation framework for integrated TN-NTN architectures, built on a\nmulti-armed bandit (MAB) formulation and leveraging the Bandit-feedback\nConstrained Online Mirror Descent (BCOMD) algorithm. Our approach adaptively\noptimises key system parameters--including bandwidth allocation, user equipment\n(UE) association, and macro base station (MBS) shutdown--to balance network\ncapacity and energy efficiency in real time. Extensive system-level simulations\nover a 24-hour period show that our framework significantly reduces the\nproportion of unsatisfied UEs during peak hours and achieves up to 19%\nthroughput gains and 5% energy savings in low-traffic periods, outperforming\nstandard network settings following 3GPP recommendations.", "AI": {"tldr": "本文提出了一种基于多臂老虎机（MAB）和BCOMD算法的在线优化框架，用于集成地面与非地面网络（TN-NTN），以平衡网络容量与能源效率。", "motivation": "随着地面网络部署密度的增加，探索非地面网络（NTN）在减轻地面网络负载和实现能源高效运行方面的潜力。", "method": "采用多臂老虎机（MAB）和BCOMD算法，实时优化带宽分配、用户设备（UE）关联和宏基站（MBS）关闭等关键系统参数。", "result": "系统级模拟显示，该框架在高峰时段显著减少不满意的UE比例，并在低流量时段实现19%的吞吐量提升和5%的能源节省。", "conclusion": "该框架优于遵循3GPP建议的标准网络设置，展示了NTN在支持可持续网络中的潜力。"}}
{"id": "2506.09853", "pdf": "https://arxiv.org/pdf/2506.09853", "abs": "https://arxiv.org/abs/2506.09853", "authors": ["Xiangning Yu", "Zhuohan Wang", "Linyi Yang", "Haoxuan Li", "Anjie Liu", "Xiao Xue", "Jun Wang", "Mengyue Yang"], "title": "Causal Sufficiency and Necessity Improves Chain-of-Thought Reasoning", "categories": ["cs.CL", "cs.AI", "math.ST", "stat.ME", "stat.TH"], "comment": null, "summary": "Chain-of-Thought (CoT) prompting plays an indispensable role in endowing\nlarge language models (LLMs) with complex reasoning capabilities. However, CoT\ncurrently faces two fundamental challenges: (1) Sufficiency, which ensures that\nthe generated intermediate inference steps comprehensively cover and\nsubstantiate the final conclusion; and (2) Necessity, which identifies the\ninference steps that are truly indispensable for the soundness of the resulting\nanswer. We propose a causal framework that characterizes CoT reasoning through\nthe dual lenses of sufficiency and necessity. Incorporating causal Probability\nof Sufficiency and Necessity allows us not only to determine which steps are\nlogically sufficient or necessary to the prediction outcome, but also to\nquantify their actual influence on the final reasoning outcome under different\nintervention scenarios, thereby enabling the automated addition of missing\nsteps and the pruning of redundant ones. Extensive experimental results on\nvarious mathematical and commonsense reasoning benchmarks confirm substantial\nimprovements in reasoning efficiency and reduced token usage without\nsacrificing accuracy. Our work provides a promising direction for improving LLM\nreasoning performance and cost-effectiveness.", "AI": {"tldr": "论文提出了一种因果框架，通过充分性和必要性分析优化CoT推理，提升LLM的推理效率和成本效益。", "motivation": "解决CoT推理中步骤的充分性和必要性不足的问题，以提升推理的准确性和效率。", "method": "采用因果框架，结合充分性和必要性的概率分析，自动添加缺失步骤并修剪冗余步骤。", "result": "在多个推理基准测试中显著提高了效率，减少了token使用且不影响准确性。", "conclusion": "该框架为提升LLM推理性能和成本效益提供了有前景的方向。"}}
{"id": "2506.09634", "pdf": "https://arxiv.org/pdf/2506.09634", "abs": "https://arxiv.org/abs/2506.09634", "authors": ["Yanzhao Shi", "Xiaodan Zhang", "Junzhong Ji", "Haoning Jiang", "Chengxin Zheng", "Yinong Wang", "Liangqiong Qu"], "title": "HSENet: Hybrid Spatial Encoding Network for 3D Medical Vision-Language Understanding", "categories": ["cs.CV", "cs.AI"], "comment": "27 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:2410.14200 by other authors", "summary": "Automated 3D CT diagnosis empowers clinicians to make timely, evidence-based\ndecisions by enhancing diagnostic accuracy and workflow efficiency. While\nmultimodal large language models (MLLMs) exhibit promising performance in\nvisual-language understanding, existing methods mainly focus on 2D medical\nimages, which fundamentally limits their ability to capture complex 3D\nanatomical structures. This limitation often leads to misinterpretation of\nsubtle pathologies and causes diagnostic hallucinations. In this paper, we\npresent Hybrid Spatial Encoding Network (HSENet), a framework that exploits\nenriched 3D medical visual cues by effective visual perception and projection\nfor accurate and robust vision-language understanding. Specifically, HSENet\nemploys dual-3D vision encoders to perceive both global volumetric contexts and\nfine-grained anatomical details, which are pre-trained by dual-stage alignment\nwith diagnostic reports. Furthermore, we propose Spatial Packer, an efficient\nmultimodal projector that condenses high-resolution 3D spatial regions into a\ncompact set of informative visual tokens via centroid-based compression. By\nassigning spatial packers with dual-3D vision encoders, HSENet can seamlessly\nperceive and transfer hybrid visual representations to LLM's semantic space,\nfacilitating accurate diagnostic text generation. Experimental results\ndemonstrate that our method achieves state-of-the-art performance in 3D\nlanguage-visual retrieval (39.85% of R@100, +5.96% gain), 3D medical report\ngeneration (24.01% of BLEU-4, +8.01% gain), and 3D visual question answering\n(73.60% of Major Class Accuracy, +1.99% gain), confirming its effectiveness.\nOur code is available at https://github.com/YanzhaoShi/HSENet.", "AI": {"tldr": "HSENet提出了一种用于3D医学图像和语言理解的混合空间编码网络，通过双3D视觉编码器和空间打包器提升诊断准确性。", "motivation": "现有方法主要针对2D医学图像，无法捕捉复杂3D解剖结构，导致误诊和诊断幻觉。", "method": "HSENet采用双3D视觉编码器感知全局和细节，并通过空间打包器压缩高分辨率3D区域为视觉标记。", "result": "在3D语言-视觉检索、医学报告生成和视觉问答中达到SOTA性能。", "conclusion": "HSENet通过混合视觉表示和语义空间转换，显著提升了3D医学诊断的准确性和效率。"}}
{"id": "2506.09276", "pdf": "https://arxiv.org/pdf/2506.09276", "abs": "https://arxiv.org/abs/2506.09276", "authors": ["Lorenzo Steccanella", "Joshua B. Evans", "Özgür Şimşek", "Anders Jonsson"], "title": "Learning The Minimum Action Distance", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper presents a state representation framework for Markov decision\nprocesses (MDPs) that can be learned solely from state trajectories, requiring\nneither reward signals nor the actions executed by the agent. We propose\nlearning the minimum action distance (MAD), defined as the minimum number of\nactions required to transition between states, as a fundamental metric that\ncaptures the underlying structure of an environment. MAD naturally enables\ncritical downstream tasks such as goal-conditioned reinforcement learning and\nreward shaping by providing a dense, geometrically meaningful measure of\nprogress. Our self-supervised learning approach constructs an embedding space\nwhere the distances between embedded state pairs correspond to their MAD,\naccommodating both symmetric and asymmetric approximations. We evaluate the\nframework on a comprehensive suite of environments with known MAD values,\nencompassing both deterministic and stochastic dynamics, as well as discrete\nand continuous state spaces, and environments with noisy observations.\nEmpirical results demonstrate that the proposed approach not only efficiently\nlearns accurate MAD representations across these diverse settings but also\nsignificantly outperforms existing state representation methods in terms of\nrepresentation quality.", "AI": {"tldr": "提出了一种仅从状态轨迹学习MDP状态表示的自监督框架，无需奖励信号或动作信息，通过学习最小动作距离（MAD）作为环境结构的基础度量。", "motivation": "传统方法依赖奖励或动作信息，限制了在无监督或无动作数据场景下的应用。MAD能自然支持目标导向强化学习等任务。", "method": "自监督学习构建嵌入空间，使嵌入状态对的距离对应其MAD，支持对称和非对称近似。", "result": "在多种环境中验证了MAD表示的准确性和高效性，显著优于现有方法。", "conclusion": "MAD框架为无监督状态表示提供了通用且高效的方法，适用于多样化任务和环境。"}}
{"id": "2506.09886", "pdf": "https://arxiv.org/pdf/2506.09886", "abs": "https://arxiv.org/abs/2506.09886", "authors": ["Rodion Oblovatny", "Alexandra Bazarova", "Alexey Zaytsev"], "title": "Attention Head Embeddings with Trainable Deep Kernels for Hallucination Detection in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present a novel approach for detecting hallucinations in large language\nmodels (LLMs) by analyzing the probabilistic divergence between prompt and\nresponse hidden-state distributions. Counterintuitively, we find that\nhallucinated responses exhibit smaller deviations from their prompts compared\nto grounded responses, suggesting that hallucinations often arise from\nsuperficial rephrasing rather than substantive reasoning. Leveraging this\ninsight, we propose a model-intrinsic detection method that uses distributional\ndistances as principled hallucination scores, eliminating the need for external\nknowledge or auxiliary models. To enhance sensitivity, we employ deep learnable\nkernels that automatically adapt to capture nuanced geometric differences\nbetween distributions. Our approach outperforms existing baselines,\ndemonstrating state-of-the-art performance on several benchmarks. The method\nremains competitive even without kernel training, offering a robust, scalable\nsolution for hallucination detection.", "AI": {"tldr": "提出了一种通过分析提示与响应隐藏状态分布的概率差异来检测大语言模型幻觉的新方法，发现幻觉响应与提示的偏差较小，并利用这一发现设计了一种无需外部知识的检测方法。", "motivation": "大语言模型中的幻觉问题（即生成不真实或无根据的内容）是一个重要挑战，需要一种无需依赖外部知识的检测方法。", "method": "通过分析提示与响应隐藏状态分布的概率差异，利用分布距离作为幻觉分数，并采用可学习的深度核函数增强敏感性。", "result": "该方法在多个基准测试中表现优于现有基线，即使不进行核训练也能保持竞争力。", "conclusion": "提出了一种高效、可扩展的幻觉检测方法，为大语言模型的可靠性提供了新思路。"}}
{"id": "2506.09644", "pdf": "https://arxiv.org/pdf/2506.09644", "abs": "https://arxiv.org/abs/2506.09644", "authors": ["Dongxu Liu", "Yuang Peng", "Haomiao Tang", "Yuwei Chen", "Chunrui Han", "Zheng Ge", "Daxin Jiang", "Mingxue Liao"], "title": "DGAE: Diffusion-Guided Autoencoder for Efficient Latent Representation Learning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Autoencoders empower state-of-the-art image and video generative models by\ncompressing pixels into a latent space through visual tokenization. Although\nrecent advances have alleviated the performance degradation of autoencoders\nunder high compression ratios, addressing the training instability caused by\nGAN remains an open challenge. While improving spatial compression, we also aim\nto minimize the latent space dimensionality, enabling more efficient and\ncompact representations. To tackle these challenges, we focus on improving the\ndecoder's expressiveness. Concretely, we propose DGAE, which employs a\ndiffusion model to guide the decoder in recovering informative signals that are\nnot fully decoded from the latent representation. With this design, DGAE\neffectively mitigates the performance degradation under high spatial\ncompression rates. At the same time, DGAE achieves state-of-the-art performance\nwith a 2x smaller latent space. When integrated with Diffusion Models, DGAE\ndemonstrates competitive performance on image generation for ImageNet-1K and\nshows that this compact latent representation facilitates faster convergence of\nthe diffusion model.", "AI": {"tldr": "DGAE通过扩散模型指导解码器，解决了高压缩比下自编码器的性能下降问题，并实现了更小的潜在空间维度。", "motivation": "解决GAN训练不稳定性和高压缩比下自编码器性能下降的问题，同时追求更高效的潜在空间表示。", "method": "提出DGAE，利用扩散模型指导解码器恢复潜在表示中未完全解码的信息信号。", "result": "DGAE在高空间压缩率下有效缓解性能下降，潜在空间维度缩小2倍，并在ImageNet-1K图像生成中表现优异。", "conclusion": "DGAE不仅提升了自编码器的性能，还加速了扩散模型的收敛，展示了紧凑潜在表示的优势。"}}
{"id": "2506.09284", "pdf": "https://arxiv.org/pdf/2506.09284", "abs": "https://arxiv.org/abs/2506.09284", "authors": ["Yihe Tang", "Wenlong Huang", "Yingke Wang", "Chengshu Li", "Roy Yuan", "Ruohan Zhang", "Jiajun Wu", "Li Fei-Fei"], "title": "UAD: Unsupervised Affordance Distillation for Generalization in Robotic Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Understanding fine-grained object affordances is imperative for robots to\nmanipulate objects in unstructured environments given open-ended task\ninstructions. However, existing methods of visual affordance predictions often\nrely on manually annotated data or conditions only on a predefined set of\ntasks. We introduce UAD (Unsupervised Affordance Distillation), a method for\ndistilling affordance knowledge from foundation models into a task-conditioned\naffordance model without any manual annotations. By leveraging the\ncomplementary strengths of large vision models and vision-language models, UAD\nautomatically annotates a large-scale dataset with detailed $<$instruction,\nvisual affordance$>$ pairs. Training only a lightweight task-conditioned\ndecoder atop frozen features, UAD exhibits notable generalization to\nin-the-wild robotic scenes and to various human activities, despite only being\ntrained on rendered objects in simulation. Using affordance provided by UAD as\nthe observation space, we show an imitation learning policy that demonstrates\npromising generalization to unseen object instances, object categories, and\neven variations in task instructions after training on as few as 10\ndemonstrations. Project website: https://unsup-affordance.github.io/", "AI": {"tldr": "UAD是一种无监督方法，通过利用大型视觉模型和视觉语言模型，自动标注大规模数据集，无需人工标注，训练轻量级任务条件解码器，实现泛化能力。", "motivation": "理解细粒度物体功能对机器人在非结构化环境中操作物体至关重要，但现有方法依赖人工标注或预定义任务集。", "method": "UAD利用大型视觉模型和视觉语言模型自动标注数据，训练任务条件解码器，仅需少量演示即可泛化。", "result": "UAD在仿真中训练，但在真实场景和人类活动中表现出色，模仿学习策略在少量演示后泛化能力强。", "conclusion": "UAD为机器人操作提供了无监督的细粒度功能理解方法，具有广泛的应用潜力。"}}
{"id": "2506.09890", "pdf": "https://arxiv.org/pdf/2506.09890", "abs": "https://arxiv.org/abs/2506.09890", "authors": ["Yuxin Chen", "Yiran Zhao", "Yang Zhang", "An Zhang", "Kenji Kawaguchi", "Shafiq Joty", "Junnan Li", "Tat-Seng Chua", "Michael Qizhe Shieh", "Wenxuan Zhang"], "title": "The Emergence of Abstract Thought in Large Language Models Beyond Any Language", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As large language models (LLMs) continue to advance, their capacity to\nfunction effectively across a diverse range of languages has shown marked\nimprovement. Preliminary studies observe that the hidden activations of LLMs\noften resemble English, even when responding to non-English prompts. This has\nled to the widespread assumption that LLMs may \"think\" in English. However,\nmore recent results showing strong multilingual performance, even surpassing\nEnglish performance on specific tasks in other languages, challenge this view.\nIn this work, we find that LLMs progressively develop a core language-agnostic\nparameter space-a remarkably small subset of parameters whose deactivation\nresults in significant performance degradation across all languages. This\ncompact yet critical set of parameters underlies the model's ability to\ngeneralize beyond individual languages, supporting the emergence of abstract\nthought that is not tied to any specific linguistic system. Specifically, we\nidentify language-related neurons-those are consistently activated during the\nprocessing of particular languages, and categorize them as either shared\n(active across multiple languages) or exclusive (specific to one). As LLMs\nundergo continued development over time, we observe a marked increase in both\nthe proportion and functional importance of shared neurons, while exclusive\nneurons progressively diminish in influence. These shared neurons constitute\nthe backbone of the core language-agnostic parameter space, supporting the\nemergence of abstract thought. Motivated by these insights, we propose\nneuron-specific training strategies tailored to LLMs' language-agnostic levels\nat different development stages. Experiments across diverse LLM families\nsupport our approach.", "AI": {"tldr": "研究发现大语言模型（LLMs）逐渐发展出一个核心语言无关参数空间，支持跨语言的抽象思维。", "motivation": "探讨LLMs在多语言任务中的表现是否依赖英语思维，以及其抽象思维的来源。", "method": "识别语言相关神经元（共享或专属），分析其演变，并提出针对不同发展阶段的神经元特定训练策略。", "result": "共享神经元比例和功能重要性随时间增加，构成核心语言无关参数空间。", "conclusion": "LLMs的抽象思维源于共享神经元，神经元特定训练策略有效。"}}
{"id": "2506.09650", "pdf": "https://arxiv.org/pdf/2506.09650", "abs": "https://arxiv.org/abs/2506.09650", "authors": ["Kunyu Peng", "Junchao Huang", "Xiangsheng Huang", "Di Wen", "Junwei Zheng", "Yufan Chen", "Kailun Yang", "Jiamin Wu", "Chongqing Hao", "Rainer Stiefelhagen"], "title": "HopaDIFF: Holistic-Partial Aware Fourier Conditioned Diffusion for Referring Human Action Segmentation in Multi-Person Scenarios", "categories": ["cs.CV", "cs.LG", "cs.MM", "cs.RO", "eess.IV"], "comment": "The code is available at https://github.com/KPeng9510/HopaDIFF.git", "summary": "Action segmentation is a core challenge in high-level video understanding,\naiming to partition untrimmed videos into segments and assign each a label from\na predefined action set. Existing methods primarily address single-person\nactivities with fixed action sequences, overlooking multi-person scenarios. In\nthis work, we pioneer textual reference-guided human action segmentation in\nmulti-person settings, where a textual description specifies the target person\nfor segmentation. We introduce the first dataset for Referring Human Action\nSegmentation, i.e., RHAS133, built from 133 movies and annotated with 137\nfine-grained actions with 33h video data, together with textual descriptions\nfor this new task. Benchmarking existing action recognition methods on RHAS133\nusing VLM-based feature extractors reveals limited performance and poor\naggregation of visual cues for the target person. To address this, we propose a\nholistic-partial aware Fourier-conditioned diffusion framework, i.e., HopaDIFF,\nleveraging a novel cross-input gate attentional xLSTM to enhance\nholistic-partial long-range reasoning and a novel Fourier condition to\nintroduce more fine-grained control to improve the action segmentation\ngeneration. HopaDIFF achieves state-of-the-art results on RHAS133 in diverse\nevaluation settings. The code is available at\nhttps://github.com/KPeng9510/HopaDIFF.git.", "AI": {"tldr": "该论文提出了一个基于文本引导的多人物动作分割方法HopaDIFF，并引入了首个相关数据集RHAS133。", "motivation": "现有方法主要针对单人固定动作序列，忽视了多人物场景，因此需要一种新的方法来解决多人物动作分割问题。", "method": "提出了HopaDIFF框架，结合交叉输入门注意力xLSTM和傅里叶条件，增强整体-局部长程推理和细粒度控制。", "result": "HopaDIFF在RHAS133数据集上取得了最先进的性能。", "conclusion": "HopaDIFF为多人物动作分割提供了有效解决方案，并在实验中表现出色。"}}
{"id": "2506.09286", "pdf": "https://arxiv.org/pdf/2506.09286", "abs": "https://arxiv.org/abs/2506.09286", "authors": ["Mohammadsajad Abavisani", "Kseniya Solovyeva", "David Danks", "Vince Calhoun", "Sergey Plis"], "title": "Causal Graph Recovery in Neuroimaging through Answer Set Programming", "categories": ["cs.LG", "cs.AI", "stat.AP", "stat.ME"], "comment": null, "summary": "Learning graphical causal structures from time series data presents\nsignificant challenges, especially when the measurement frequency does not\nmatch the causal timescale of the system. This often leads to a set of equally\npossible underlying causal graphs due to information loss from sub-sampling\n(i.e., not observing all possible states of the system throughout time). Our\nresearch addresses this challenge by incorporating the effects of sub-sampling\nin the derivation of causal graphs, resulting in more accurate and intuitive\noutcomes. We use a constraint optimization approach, specifically answer set\nprogramming (ASP), to find the optimal set of answers. ASP not only identifies\nthe most probable underlying graph, but also provides an equivalence class of\npossible graphs for expert selection. In addition, using ASP allows us to\nleverage graph theory to further prune the set of possible solutions, yielding\na smaller, more accurate answer set significantly faster than traditional\napproaches. We validate our approach on both simulated data and empirical\nstructural brain connectivity, and demonstrate its superiority over established\nmethods in these experiments. We further show how our method can be used as a\nmeta-approach on top of established methods to obtain, on average, 12%\nimprovement in F1 score. In addition, we achieved state of the art results in\nterms of precision and recall of reconstructing causal graph from sub-sampled\ntime series data. Finally, our method shows robustness to varying degrees of\nsub-sampling on realistic simulations, whereas other methods perform worse for\nhigher rates of sub-sampling.", "AI": {"tldr": "该论文提出了一种基于答案集编程（ASP）的方法，用于从时间序列数据中学习因果图结构，解决了因采样频率不匹配导致的信息丢失问题。", "motivation": "时间序列数据的测量频率与系统因果时间尺度不匹配时，会导致信息丢失，从而产生多个可能的因果图。研究旨在通过考虑子采样效应，提高因果图推导的准确性和直观性。", "method": "采用约束优化方法，特别是答案集编程（ASP），以找到最优因果图集合。ASP不仅能识别最可能的因果图，还能提供等价类供专家选择，并利用图论进一步剪枝可能的解集。", "result": "在模拟数据和实际脑结构连接数据上验证了方法的优越性，F1分数平均提高12%，且在子采样时间序列数据中实现了因果图重建的先进精度和召回率。", "conclusion": "该方法对不同程度的子采样表现出鲁棒性，优于现有方法，并可作为现有方法的元方法进一步提升性能。"}}
{"id": "2506.09902", "pdf": "https://arxiv.org/pdf/2506.09902", "abs": "https://arxiv.org/abs/2506.09902", "authors": ["Zheng Zhao", "Clara Vania", "Subhradeep Kayal", "Naila Khan", "Shay B. Cohen", "Emine Yilmaz"], "title": "PersonaLens: A Benchmark for Personalization Evaluation in Conversational AI Assistants", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large language models (LLMs) have advanced conversational AI assistants.\nHowever, systematically evaluating how well these assistants apply\npersonalization--adapting to individual user preferences while completing\ntasks--remains challenging. Existing personalization benchmarks focus on\nchit-chat, non-conversational tasks, or narrow domains, failing to capture the\ncomplexities of personalized task-oriented assistance. To address this, we\nintroduce PersonaLens, a comprehensive benchmark for evaluating personalization\nin task-oriented AI assistants. Our benchmark features diverse user profiles\nequipped with rich preferences and interaction histories, along with two\nspecialized LLM-based agents: a user agent that engages in realistic\ntask-oriented dialogues with AI assistants, and a judge agent that employs the\nLLM-as-a-Judge paradigm to assess personalization, response quality, and task\nsuccess. Through extensive experiments with current LLM assistants across\ndiverse tasks, we reveal significant variability in their personalization\ncapabilities, providing crucial insights for advancing conversational AI\nsystems.", "AI": {"tldr": "PersonaLens是一个用于评估任务导向AI助手个性化能力的综合基准，通过多样化的用户配置和LLM代理揭示当前LLM助手在个性化能力上的显著差异。", "motivation": "现有个性化基准未能全面捕捉任务导向助手的复杂性，因此需要开发更全面的评估工具。", "method": "引入PersonaLens基准，包含多样化用户配置和两个LLM代理（用户代理和法官代理），用于评估个性化、响应质量和任务成功率。", "result": "实验显示当前LLM助手在个性化能力上存在显著差异。", "conclusion": "PersonaLens为提升对话AI系统提供了关键见解。"}}
{"id": "2506.09663", "pdf": "https://arxiv.org/pdf/2506.09663", "abs": "https://arxiv.org/abs/2506.09663", "authors": ["Haowen Wang", "Xiaoping Yuan", "Zhao Jin", "Zhen Zhao", "Zhengping Che", "Yousong Xue", "Jin Tian", "Yakun Huang", "Jian Tang"], "title": "Self-Supervised Multi-Part Articulated Objects Modeling via Deformable Gaussian Splatting and Progressive Primitive Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Articulated objects are ubiquitous in everyday life, and accurate 3D\nrepresentations of their geometry and motion are critical for numerous\napplications. However, in the absence of human annotation, existing approaches\nstill struggle to build a unified representation for objects that contain\nmultiple movable parts. We introduce DeGSS, a unified framework that encodes\narticulated objects as deformable 3D Gaussian fields, embedding geometry,\nappearance, and motion in one compact representation. Each interaction state is\nmodeled as a smooth deformation of a shared field, and the resulting\ndeformation trajectories guide a progressive coarse-to-fine part segmentation\nthat identifies distinct rigid components, all in an unsupervised manner. The\nrefined field provides a spatially continuous, fully decoupled description of\nevery part, supporting part-level reconstruction and precise modeling of their\nkinematic relationships. To evaluate generalization and realism, we enlarge the\nsynthetic PartNet-Mobility benchmark and release RS-Art, a real-to-sim dataset\nthat pairs RGB captures with accurately reverse-engineered 3D models. Extensive\nexperiments demonstrate that our method outperforms existing methods in both\naccuracy and stability.", "AI": {"tldr": "DeGSS是一种统一框架，将铰接物体编码为可变形3D高斯场，嵌入几何、外观和运动信息，支持无监督部分分割和精确运动建模。", "motivation": "现有方法在缺乏人工标注时难以构建铰接物体的统一表示，DeGSS旨在解决这一问题。", "method": "通过可变形3D高斯场建模物体状态，利用变形轨迹指导渐进式部分分割，实现无监督的刚性部件识别。", "result": "在合成和真实数据集上，DeGSS在准确性和稳定性上优于现有方法。", "conclusion": "DeGSS提供了一种紧凑且连续的表示方法，支持部件级重建和运动关系建模。"}}
{"id": "2506.09917", "pdf": "https://arxiv.org/pdf/2506.09917", "abs": "https://arxiv.org/abs/2506.09917", "authors": ["Wendi Zhou", "Ameer Saadat-Yazd", "Nadin Kokciyan"], "title": "Aspect-Based Opinion Summarization with Argumentation Schemes", "categories": ["cs.CL"], "comment": "Accepted by ArgMining 2025", "summary": "Reviews are valuable resources for customers making purchase decisions in\nonline shopping. However, it is impractical for customers to go over the vast\nnumber of reviews and manually conclude the prominent opinions, which prompts\nthe need for automated opinion summarization systems. Previous approaches,\neither extractive or abstractive, face challenges in automatically producing\ngrounded aspect-centric summaries. In this paper, we propose a novel\nsummarization system that not only captures predominant opinions from an aspect\nperspective with supporting evidence, but also adapts to varying domains\nwithout relying on a pre-defined set of aspects. Our proposed framework,\nASESUM, summarizes viewpoints relevant to the critical aspects of a product by\nextracting aspect-centric arguments and measuring their salience and validity.\nWe conduct experiments on a real-world dataset to demonstrate the superiority\nof our approach in capturing diverse perspectives of the original reviews\ncompared to new and existing methods.", "AI": {"tldr": "论文提出了一种名为ASESUM的自动评论摘要系统，能够从产品关键角度提取观点并验证其显著性和有效性，适应不同领域且无需预定义方面。", "motivation": "在线购物中，海量评论难以手动总结，需要自动化系统生成基于方面的摘要。现有方法难以自动生成有依据的方面中心摘要。", "method": "提出ASESUM框架，通过提取方面中心论点并衡量其显著性和有效性，生成摘要。", "result": "实验证明ASESUM在捕捉原始评论多样化观点方面优于现有和新方法。", "conclusion": "ASESUM能有效生成基于方面的摘要，适应性强且无需预定义方面。"}}
{"id": "2506.09668", "pdf": "https://arxiv.org/pdf/2506.09668", "abs": "https://arxiv.org/abs/2506.09668", "authors": ["Maik Dannecker", "Vasiliki Sideri-Lampretsa", "Sophie Starck", "Angeline Mihailov", "Mathieu Milh", "Nadine Girard", "Guillaume Auzias", "Daniel Rueckert"], "title": "CINeMA: Conditional Implicit Neural Multi-Modal Atlas for a Spatio-Temporal Representation of the Perinatal Brain", "categories": ["cs.CV", "cs.LG"], "comment": "Work currently under revision for IEEE TMI", "summary": "Magnetic resonance imaging of fetal and neonatal brains reveals rapid\nneurodevelopment marked by substantial anatomical changes unfolding within\ndays. Studying this critical stage of the developing human brain, therefore,\nrequires accurate brain models-referred to as atlases-of high spatial and\ntemporal resolution. To meet these demands, established traditional atlases and\nrecently proposed deep learning-based methods rely on large and comprehensive\ndatasets. This poses a major challenge for studying brains in the presence of\npathologies for which data remains scarce. We address this limitation with\nCINeMA (Conditional Implicit Neural Multi-Modal Atlas), a novel framework for\ncreating high-resolution, spatio-temporal, multimodal brain atlases, suitable\nfor low-data settings. Unlike established methods, CINeMA operates in latent\nspace, avoiding compute-intensive image registration and reducing atlas\nconstruction times from days to minutes. Furthermore, it enables flexible\nconditioning on anatomical features including GA, birth age, and pathologies\nlike ventriculomegaly (VM) and agenesis of the corpus callosum (ACC). CINeMA\nsupports downstream tasks such as tissue segmentation and age prediction\nwhereas its generative properties enable synthetic data creation and\nanatomically informed data augmentation. Surpassing state-of-the-art methods in\naccuracy, efficiency, and versatility, CINeMA represents a powerful tool for\nadvancing brain research. We release the code and atlases at\nhttps://github.com/m-dannecker/CINeMA.", "AI": {"tldr": "CINeMA是一种新型框架，用于在低数据环境下创建高分辨率、多模态的胎儿和新生儿脑图谱，显著提高了效率和准确性。", "motivation": "研究胎儿和新生儿大脑快速发育阶段需要高时空分辨率的脑图谱，但传统方法和深度学习方法依赖大数据，难以适用于病理数据稀缺的情况。", "method": "CINeMA在潜在空间中操作，避免了计算密集的图像配准，将图谱构建时间从几天缩短到几分钟，并支持基于解剖特征的灵活条件设置。", "result": "CINeMA在准确性、效率和多功能性上超越现有方法，支持组织分割、年龄预测等任务，并能生成合成数据。", "conclusion": "CINeMA为脑研究提供了强大工具，代码和图谱已开源。"}}
{"id": "2506.09942", "pdf": "https://arxiv.org/pdf/2506.09942", "abs": "https://arxiv.org/abs/2506.09942", "authors": ["Hao Peng", "Yunjia Qi", "Xiaozhi Wang", "Bin Xu", "Lei Hou", "Juanzi Li"], "title": "VerIF: Verification Engineering for Reinforcement Learning in Instruction Following", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 8 figures", "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a key\ntechnique for enhancing large language models (LLMs), with verification\nengineering playing a central role. However, best practices for RL in\ninstruction following remain underexplored. In this work, we explore the\nverification challenge in RL for instruction following and propose VerIF, a\nverification method that combines rule-based code verification with LLM-based\nverification from a large reasoning model (e.g., QwQ-32B). To support this\napproach, we construct a high-quality instruction-following dataset,\nVerInstruct, containing approximately 22,000 instances with associated\nverification signals. We apply RL training with VerIF to two models, achieving\nsignificant improvements across several representative instruction-following\nbenchmarks. The trained models reach state-of-the-art performance among models\nof comparable size and generalize well to unseen constraints. We further\nobserve that their general capabilities remain unaffected, suggesting that RL\nwith VerIF can be integrated into existing RL recipes to enhance overall model\nperformance. We have released our datasets, codes, and models to facilitate\nfuture research at https://github.com/THU-KEG/VerIF.", "AI": {"tldr": "论文提出了一种结合规则验证与LLM验证的强化学习方法VerIF，用于提升指令跟随任务的效果，并通过实验验证了其有效性。", "motivation": "强化学习在指令跟随任务中的验证方法尚未充分探索，需要一种高效且可靠的验证技术来提升模型性能。", "method": "提出VerIF方法，结合规则代码验证与基于大型推理模型（如QwQ-32B）的LLM验证，并构建高质量数据集VerInstruct。", "result": "在多个指令跟随基准测试中取得显著提升，模型性能达到同类最佳，且不影响通用能力。", "conclusion": "VerIF可集成到现有强化学习流程中，提升模型整体性能，相关资源已开源。"}}
{"id": "2506.09677", "pdf": "https://arxiv.org/pdf/2506.09677", "abs": "https://arxiv.org/abs/2506.09677", "authors": ["Bin Zhu", "Hailong Yin", "Jingjing Chen", "Yu-Gang Jiang"], "title": "Reasoning Models Are More Easily Gaslighted Than You Think", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent advances in reasoning-centric models promise improved robustness\nthrough mechanisms such as chain-of-thought prompting and test-time scaling.\nHowever, their ability to withstand misleading user input remains\nunderexplored. In this paper, we conduct a systematic evaluation of three\nstate-of-the-art reasoning models, i.e., OpenAI's o4-mini, Claude-3.7-Sonnet\nand Gemini-2.5-Flash, across three multimodal benchmarks: MMMU, MathVista, and\nCharXiv. Our evaluation reveals significant accuracy drops (25-29% on average)\nfollowing gaslighting negation prompts, indicating that even top-tier reasoning\nmodels struggle to preserve correct answers under manipulative user feedback.\nBuilt upon the insights of the evaluation and to further probe this\nvulnerability, we introduce GaslightingBench-R, a new diagnostic benchmark\nspecifically designed to evaluate reasoning models' susceptibility to defend\ntheir belief under gaslighting negation prompt. Constructed by filtering and\ncurating 1,025 challenging samples from the existing benchmarks,\nGaslightingBench-R induces even more dramatic failures, with accuracy drops\nexceeding 53% on average. Our findings reveal fundamental limitations in the\nrobustness of reasoning models, highlighting the gap between step-by-step\nreasoning and belief persistence.", "AI": {"tldr": "论文通过系统评估发现，当前先进的推理模型在面对误导性用户输入时表现脆弱，准确性显著下降。作者提出新基准GaslightingBench-R，进一步揭示模型在信念坚持上的局限性。", "motivation": "探索推理模型在面对误导性用户输入时的鲁棒性，填补现有研究空白。", "method": "系统评估三种先进推理模型（OpenAI的o4-mini、Claude-3.7-Sonnet和Gemini-2.5-Flash）在三个多模态基准（MMMU、MathVista和CharXiv）上的表现，并引入新基准GaslightingBench-R。", "result": "模型在误导性提示下准确性平均下降25-29%，在新基准上下降超过53%。", "conclusion": "推理模型在信念坚持上存在根本性局限，需进一步改进鲁棒性。"}}
{"id": "2506.09944", "pdf": "https://arxiv.org/pdf/2506.09944", "abs": "https://arxiv.org/abs/2506.09944", "authors": ["Wuwei Zhang", "Fangcong Yin", "Howard Yen", "Danqi Chen", "Xi Ye"], "title": "Query-Focused Retrieval Heads Improve Long-Context Reasoning and Re-ranking", "categories": ["cs.CL"], "comment": null, "summary": "Recent work has identified retrieval heads (Wu et al., 2025b), a subset of\nattention heads responsible for retrieving salient information in long-context\nlanguage models (LMs), as measured by their copy-paste behavior in\nNeedle-in-a-Haystack tasks. In this paper, we introduce QRHEAD (Query-Focused\nRetrieval Head), an improved set of attention heads that enhance retrieval from\nlong context. We identify QRHEAD by aggregating attention scores with respect\nto the input query, using a handful of examples from real-world tasks (e.g.,\nlong-context QA). We further introduce QR- RETRIEVER, an efficient and\neffective retriever that uses the accumulated attention mass of QRHEAD as\nretrieval scores. We use QR- RETRIEVER for long-context reasoning by selecting\nthe most relevant parts with the highest retrieval scores. On multi-hop\nreasoning tasks LongMemEval and CLIPPER, this yields over 10% performance gains\nover full context and outperforms strong dense retrievers. We also evaluate\nQRRETRIEVER as a re-ranker on the BEIR benchmark and find that it achieves\nstrong zero-shot performance, outperforming other LLM-based re-rankers such as\nRankGPT. Further analysis shows that both the querycontext attention scoring\nand task selection are crucial for identifying QRHEAD with strong downstream\nutility. Overall, our work contributes a general-purpose retriever and offers\ninterpretability insights into the long-context capabilities of LMs.", "AI": {"tldr": "论文提出QRHEAD和QR-RETRIEVER，通过聚焦查询的注意力机制提升长上下文检索能力，在多任务中表现优异。", "motivation": "现有检索头在长上下文语言模型中表现有限，需改进以提升信息检索效率。", "method": "通过聚合输入查询的注意力分数识别QRHEAD，并开发QR-RETRIEVER作为高效检索器。", "result": "在LongMemEval和CLIPPER任务中性能提升超10%，BEIR基准测试中零样本表现优于RankGPT。", "conclusion": "QRHEAD和QR-RETRIEVER为通用检索器，同时揭示了长上下文语言模型的可解释性。"}}
{"id": "2506.09691", "pdf": "https://arxiv.org/pdf/2506.09691", "abs": "https://arxiv.org/abs/2506.09691", "authors": ["Imanol Miranda", "Ander Salaberria", "Eneko Agirre", "Gorka Azkune"], "title": "Adding simple structure at inference improves Vision-Language Compositionality", "categories": ["cs.CV", "cs.CL", "cs.LG"], "comment": null, "summary": "Dual encoder Vision-Language Models (VLM) such as CLIP are widely used for\nimage-text retrieval tasks. However, those models struggle with\ncompositionality, showing a bag-of-words-like behavior that limits their\nretrieval performance. Many different training approaches have been proposed to\nimprove the vision-language compositionality capabilities of those models. In\ncomparison, inference-time techniques have received little attention. In this\npaper, we propose to add simple structure at inference, where, given an image\nand a caption: i) we divide the image into different smaller crops, ii) we\nextract text segments, capturing objects, attributes and relations, iii) using\na VLM, we find the image crops that better align with text segments obtaining\nmatches, and iv) we compute the final image-text similarity aggregating the\nindividual similarities of the matches. Based on various popular dual encoder\nVLMs, we evaluate our approach in controlled and natural datasets for VL\ncompositionality. We find that our approach consistently improves the\nperformance of evaluated VLMs without any training, which shows the potential\nof inference-time techniques. The results are especially good for\nattribute-object binding as shown in the controlled dataset. As a result of an\nextensive analysis: i) we show that processing image crops is actually\nessential for the observed gains in performance, and ii) we identify specific\nareas to further improve inference-time approaches.", "AI": {"tldr": "提出了一种在推理时改进双编码器视觉语言模型（VLM）组合性的方法，通过分割图像和文本并匹配局部相似性，显著提升了性能。", "motivation": "现有双编码器VLM（如CLIP）在组合性任务上表现不佳，而推理时技术研究较少，因此探索通过简单结构调整提升性能。", "method": "在推理时：1) 分割图像为小区域；2) 提取文本片段；3) 匹配图像区域与文本片段；4) 聚合局部相似性计算最终相似度。", "result": "在多个数据集上验证，无需训练即可显著提升VLM性能，尤其在属性-对象绑定任务中表现突出。", "conclusion": "推理时技术潜力巨大，图像区域处理是关键，未来可进一步优化推理方法。"}}
{"id": "2506.09335", "pdf": "https://arxiv.org/pdf/2506.09335", "abs": "https://arxiv.org/abs/2506.09335", "authors": ["Moshi Wei", "Sparks Li"], "title": "Intelligent System of Emergent Knowledge: A Coordination Fabric for Billions of Minds", "categories": ["cs.MA", "cs.AI"], "comment": "11 pages, 1 figures,", "summary": "The Intelligent System of Emergent Knowledge (ISEK) establishes a\ndecentralized network where human and artificial intelligence agents\ncollaborate as peers, forming a self-organizing cognitive ecosystem. Built on\nWeb3 infrastructure, ISEK combines three fundamental principles: (1) a\ndecentralized multi-agent architecture resistant to censorship, (2) symbiotic\nAI-human collaboration with equal participation rights, and (3) resilient\nself-adaptation through distributed consensus mechanisms.\n  The system implements an innovative coordination protocol featuring a\nsix-phase workflow (Publish, Discover, Recruit, Execute, Settle, Feedback) for\ndynamic task allocation, supported by robust fault tolerance and a\nmultidimensional reputation system. Economic incentives are governed by the\nnative $ISEK token, facilitating micropayments, governance participation, and\nreputation tracking, while agent sovereignty is maintained through NFT-based\nidentity management.\n  This synthesis of blockchain technology, artificial intelligence, and\nincentive engineering creates an infrastructure that actively facilitates\nemergent intelligence. ISEK represents a paradigm shift from conventional\nplatforms, enabling the organic development of large-scale, decentralized\ncognitive systems where autonomous agents collectively evolve beyond\ncentralized constraints.", "AI": {"tldr": "ISEK是一个去中心化的网络，人类和人工智能代理作为对等体协作，形成自组织的认知生态系统。基于Web3基础设施，ISEK结合了去中心化架构、AI-人类协作和分布式共识机制，通过创新的协调协议和经济激励实现动态任务分配。", "motivation": "传统平台受限于中心化约束，ISEK旨在通过去中心化网络和AI-人类协作，实现大规模认知系统的有机发展。", "method": "ISEK采用六阶段工作流（发布、发现、招募、执行、结算、反馈）进行动态任务分配，结合容错机制和多维声誉系统，并使用$ISEK代币进行经济激励和治理。", "result": "ISEK成功结合区块链、AI和激励工程，构建了一个促进涌现智能的基础设施，实现了去中心化认知系统的范式转变。", "conclusion": "ISEK代表了从传统平台向去中心化认知系统的转变，为自主代理的集体进化提供了新途径。"}}
{"id": "2506.09967", "pdf": "https://arxiv.org/pdf/2506.09967", "abs": "https://arxiv.org/abs/2506.09967", "authors": ["Shangshang Wang", "Julian Asilis", "Ömer Faruk Akgül", "Enes Burak Bilgin", "Ollie Liu", "Deqing Fu", "Willie Neiswanger"], "title": "Resa: Transparent Reasoning Models via SAEs", "categories": ["cs.CL"], "comment": null, "summary": "How cost-effectively can we elicit strong reasoning in language models by\nleveraging their underlying representations? We answer this question with Resa,\na family of 1.5B reasoning models trained via a novel and efficient sparse\nautoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to\ncapture reasoning abilities from a source model, and then uses the trained SAE\nto guide a standard supervised fine-tuning process to elicit such abilities in\na target model, all using verified question-answer data without any reasoning\ntraces. Notably, when applied to certain base models before further RL\npost-training, SAE-Tuning retains >97% of its RL-trained counterpart's\nreasoning performance while reducing training costs by >2000x to roughly \\$1\nand training time by >450x to around 20 minutes. Furthermore, when applied to\nlightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning\nperformance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only\naround \\$1 additional cost. Surprisingly, the reasoning abilities extracted via\nSAEs are potentially both generalizable and modular. Generality means abilities\nextracted from one dataset still elevate performance on a larger and\noverlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math\ncan be attached to the R1-Distill model at test time, without any retraining,\nand yield comparable gains. Extensive ablations validate these findings and all\nartifacts are fully open-sourced.", "AI": {"tldr": "Resa是一种1.5B推理模型家族，通过稀疏自编码器调优（SAE-Tuning）方法高效提取语言模型的推理能力，显著降低训练成本和时间。", "motivation": "研究如何高效且低成本地从语言模型中提取和增强推理能力。", "method": "使用稀疏自编码器（SAE）从源模型中捕获推理能力，并通过监督微调将这些能力迁移到目标模型中。", "result": "SAE-Tuning在极低成本（约1美元）和短时间（约20分钟）内，保留了RL训练模型97%以上的推理性能，并在多个数据集上表现优异。", "conclusion": "SAE-Tuning方法不仅高效且低成本，还能提取通用和模块化的推理能力，为语言模型推理能力的开发提供了新方向。"}}
{"id": "2506.09695", "pdf": "https://arxiv.org/pdf/2506.09695", "abs": "https://arxiv.org/abs/2506.09695", "authors": ["Changwei Wu", "Yifei Chen", "Yuxin Du", "Jinying Zong", "Jie Dong", "Mingxuan Liu", "Yong Peng", "Jin Fan", "Feiwei Qin", "Changmiao Wang"], "title": "Towards Practical Alzheimer's Disease Diagnosis: A Lightweight and Interpretable Spiking Neural Model", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 5 figures", "summary": "Early diagnosis of Alzheimer's Disease (AD), especially at the mild cognitive\nimpairment (MCI) stage, is vital yet hindered by subjective assessments and the\nhigh cost of multimodal imaging modalities. Although deep learning methods\noffer automated alternatives, their energy inefficiency and computational\ndemands limit real-world deployment, particularly in resource-constrained\nsettings. As a brain-inspired paradigm, spiking neural networks (SNNs) are\ninherently well-suited for modeling the sparse, event-driven patterns of neural\ndegeneration in AD, offering a promising foundation for interpretable and\nlow-power medical diagnostics. However, existing SNNs often suffer from weak\nexpressiveness and unstable training, which restrict their effectiveness in\ncomplex medical tasks. To address these limitations, we propose FasterSNN, a\nhybrid neural architecture that integrates biologically inspired LIF neurons\nwith region-adaptive convolution and multi-scale spiking attention. This design\nenables sparse, efficient processing of 3D MRI while preserving diagnostic\naccuracy. Experiments on benchmark datasets demonstrate that FasterSNN achieves\ncompetitive performance with substantially improved efficiency and stability,\nsupporting its potential for practical AD screening. Our source code is\navailable at https://github.com/wuchangw/FasterSNN.", "AI": {"tldr": "FasterSNN是一种混合神经网络架构，结合了生物启发的LIF神经元、区域自适应卷积和多尺度脉冲注意力，用于高效且稳定地处理3D MRI数据，以支持阿尔茨海默病的早期诊断。", "motivation": "阿尔茨海默病（AD）的早期诊断（尤其是轻度认知障碍阶段）受限于主观评估和多模态成像的高成本。尽管深度学习方法提供了自动化替代方案，但其能效低和计算需求高限制了实际应用。", "method": "提出FasterSNN，结合LIF神经元、区域自适应卷积和多尺度脉冲注意力，以稀疏且高效的方式处理3D MRI数据。", "result": "在基准数据集上，FasterSNN表现出竞争性的性能，同时显著提高了效率和稳定性。", "conclusion": "FasterSNN为AD筛查提供了一种高效且稳定的解决方案，具有实际应用潜力。"}}
{"id": "2506.09338", "pdf": "https://arxiv.org/pdf/2506.09338", "abs": "https://arxiv.org/abs/2506.09338", "authors": ["Young-Jin Park", "Kristjan Greenewald", "Kaveh Alim", "Hao Wang", "Navid Azizan"], "title": "Know What You Don't Know: Uncertainty Calibration of Process Reward Models", "categories": ["stat.ML", "cs.AI", "cs.LG"], "comment": null, "summary": "Process reward models (PRMs) play a central role in guiding inference-time\nscaling algorithms for large language models (LLMs). However, we observe that\neven state-of-the-art PRMs can be poorly calibrated and often overestimate\nsuccess probabilities. To address this, we present a calibration approach,\nperformed via quantile regression, that adjusts PRM outputs to better align\nwith true success probabilities. Leveraging these calibrated success estimates\nand their associated confidence bounds, we introduce an \\emph{instance-adaptive\nscaling} (IAS) framework that dynamically adjusts the inference budget based on\nthe estimated likelihood that a partial reasoning trajectory will yield a\ncorrect final answer. Unlike conventional methods that allocate a fixed number\nof reasoning trajectories per query, this approach successfully adapts to each\ninstance and reasoning step when using our calibrated PRMs. Experiments on\nmathematical reasoning benchmarks show that (i) our PRM calibration method\nsuccessfully achieves small calibration error, outperforming the baseline\nmethods, (ii) calibration is crucial for enabling effective adaptive scaling,\nand (iii) the proposed IAS strategy reduces inference costs while maintaining\nfinal answer accuracy, utilizing less compute on more confident problems as\ndesired.", "AI": {"tldr": "论文提出了一种通过分位数回归校准过程奖励模型（PRMs）的方法，并基于校准结果设计了实例自适应扩展（IAS）框架，动态调整推理预算，以降低计算成本同时保持准确性。", "motivation": "现有PRMs校准不足，常高估成功概率，需改进以提升推理效率。", "method": "采用分位数回归校准PRMs输出，结合置信区间设计IAS框架，动态调整推理预算。", "result": "校准方法显著降低误差，IAS框架在数学推理任务中减少计算成本且保持准确性。", "conclusion": "校准PRMs对自适应扩展至关重要，IAS框架有效平衡计算效率与准确性。"}}
{"id": "2506.09975", "pdf": "https://arxiv.org/pdf/2506.09975", "abs": "https://arxiv.org/abs/2506.09975", "authors": ["Hillary Dawkins", "Kathleen C. Fraser", "Svetlana Kiritchenko"], "title": "When Detection Fails: The Power of Fine-Tuned Models to Generate Human-Like Social Media Text", "categories": ["cs.CL"], "comment": "to appear in ACL Findings", "summary": "Detecting AI-generated text is a difficult problem to begin with; detecting\nAI-generated text on social media is made even more difficult due to the short\ntext length and informal, idiosyncratic language of the internet. It is\nnonetheless important to tackle this problem, as social media represents a\nsignificant attack vector in online influence campaigns, which may be bolstered\nthrough the use of mass-produced AI-generated posts supporting (or opposing)\nparticular policies, decisions, or events. We approach this problem with the\nmindset and resources of a reasonably sophisticated threat actor, and create a\ndataset of 505,159 AI-generated social media posts from a combination of\nopen-source, closed-source, and fine-tuned LLMs, covering 11 different\ncontroversial topics. We show that while the posts can be detected under\ntypical research assumptions about knowledge of and access to the generating\nmodels, under the more realistic assumption that an attacker will not release\ntheir fine-tuned model to the public, detectability drops dramatically. This\nresult is confirmed with a human study. Ablation experiments highlight the\nvulnerability of various detection algorithms to fine-tuned LLMs. This result\nhas implications across all detection domains, since fine-tuning is a generally\napplicable and realistic LLM use case.", "AI": {"tldr": "论文探讨了在社交媒体上检测AI生成文本的挑战，尤其是在攻击者不公开其微调模型的情况下，检测效果显著下降。", "motivation": "社交媒体是网络影响力活动的重要攻击载体，AI生成的帖子可能被大规模用于支持或反对特定政策或事件，因此检测这类文本至关重要。", "method": "研究者以威胁行为者的视角，创建了一个包含505,159条AI生成社交媒体帖子的数据集，覆盖11个争议话题，并测试了不同检测算法的效果。", "result": "在攻击者不公开微调模型的现实假设下，检测效果大幅下降，且人类实验也验证了这一结果。", "conclusion": "微调模型对检测算法构成普遍威胁，这一发现对所有检测领域均有重要影响。"}}
{"id": "2506.09699", "pdf": "https://arxiv.org/pdf/2506.09699", "abs": "https://arxiv.org/abs/2506.09699", "authors": ["Mattia Nardon", "Mikel Mujika Agirre", "Ander González Tomé", "Daniel Sedano Algarabel", "Josep Rueda Collell", "Ana Paola Caro", "Andrea Caraffa", "Fabio Poiesi", "Paul Ian Chippendale", "Davide Boscaini"], "title": "CHIP: A multi-sensor dataset for 6D pose estimation of chairs in industrial settings", "categories": ["cs.CV"], "comment": "Technical report", "summary": "Accurate 6D pose estimation of complex objects in 3D environments is\nessential for effective robotic manipulation. Yet, existing benchmarks fall\nshort in evaluating 6D pose estimation methods under realistic industrial\nconditions, as most datasets focus on household objects in domestic settings,\nwhile the few available industrial datasets are limited to artificial setups\nwith objects placed on tables. To bridge this gap, we introduce CHIP, the first\ndataset designed for 6D pose estimation of chairs manipulated by a robotic arm\nin a real-world industrial environment. CHIP includes seven distinct chairs\ncaptured using three different RGBD sensing technologies and presents unique\nchallenges, such as distractor objects with fine-grained differences and severe\nocclusions caused by the robotic arm and human operators. CHIP comprises 77,811\nRGBD images annotated with ground-truth 6D poses automatically derived from the\nrobot's kinematics, averaging 11,115 annotations per chair. We benchmark CHIP\nusing three zero-shot 6D pose estimation methods, assessing performance across\ndifferent sensor types, localization priors, and occlusion levels. Results show\nsubstantial room for improvement, highlighting the unique challenges posed by\nthe dataset. CHIP will be publicly released.", "AI": {"tldr": "CHIP是首个针对工业环境中机器人操作的6D姿态估计数据集，填补了现有数据集的不足，包含77,811张RGBD图像，并展示了独特的挑战。", "motivation": "现有6D姿态估计数据集主要针对家庭环境，缺乏真实工业场景的数据，CHIP旨在填补这一空白。", "method": "CHIP包含七种不同椅子，使用三种RGBD传感技术捕获，并通过机器人运动学自动标注真实姿态。", "result": "基准测试显示现有方法在CHIP上仍有较大改进空间，尤其是在遮挡和传感器差异方面。", "conclusion": "CHIP为工业场景下的6D姿态估计提供了新基准，并将公开以促进研究。"}}
{"id": "2506.09983", "pdf": "https://arxiv.org/pdf/2506.09983", "abs": "https://arxiv.org/abs/2506.09983", "authors": ["Hiroshi Matsuda", "Chunpeng Ma", "Masayuki Asahara"], "title": "Step-by-step Instructions and a Simple Tabular Output Format Improve the Dependency Parsing Accuracy of LLMs", "categories": ["cs.CL"], "comment": "9 pages, 2 figures, accepted for SyntaxFest 2025", "summary": "Recent advances in large language models (LLMs) have enabled impressive\nperformance in various tasks. However, standard prompting often struggles to\nproduce structurally valid and accurate outputs, especially in dependency\nparsing. We propose a novel step-by-step instruction strategy, where universal\npart-of-speech tagging precedes the prediction of syntactic heads and\ndependency labels, and a simplified CoNLL-U like output format, our method\nachieves state-of-the-art accuracy on Universal Dependencies datasets across 17\nlanguages without hallucination or contamination. We further show that\nmultilingual fine-tuning simultaneously improves cross-language generalization\nperformance. Our results highlight the effectiveness of explicit reasoning\nsteps in LLM-based parsing and offer a scalable, format-consistent alternative\nto bracket-based approaches.", "AI": {"tldr": "提出了一种分步指令策略，结合通用词性标注和简化的输出格式，在17种语言的依赖解析任务中实现了最高准确率。", "motivation": "标准提示方法在生成结构有效且准确的输出（尤其是依赖解析）时表现不佳，需要改进。", "method": "采用分步指令策略，先进行通用词性标注，再预测句法头和依赖标签，并使用简化的CoNLL-U格式输出。", "result": "在17种语言的Universal Dependencies数据集上实现了最高准确率，且无幻觉或污染。多语言微调还提升了跨语言泛化性能。", "conclusion": "分步推理策略在基于LLM的解析中有效，提供了一种可扩展且格式一致的替代方案。"}}
{"id": "2506.09718", "pdf": "https://arxiv.org/pdf/2506.09718", "abs": "https://arxiv.org/abs/2506.09718", "authors": ["Xulin Ma", "Jiankai Tang", "Zhang Jiang", "Songqin Cheng", "Yuanchun Shi", "Dong LI", "Xin Liu", "Daniel McDuff", "Xiaojing Liu", "Yuntao Wang"], "title": "Non-Contact Health Monitoring During Daily Personal Care Routines", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Remote photoplethysmography (rPPG) enables non-contact, continuous monitoring\nof physiological signals and offers a practical alternative to traditional\nhealth sensing methods. Although rPPG is promising for daily health monitoring,\nits application in long-term personal care scenarios, such as mirror-facing\nroutines in high-altitude environments, remains challenging due to ambient\nlighting variations, frequent occlusions from hand movements, and dynamic\nfacial postures. To address these challenges, we present LADH (Long-term\nAltitude Daily Health), the first long-term rPPG dataset containing 240\nsynchronized RGB and infrared (IR) facial videos from 21 participants across\nfive common personal care scenarios, along with ground-truth PPG, respiration,\nand blood oxygen signals. Our experiments demonstrate that combining RGB and IR\nvideo inputs improves the accuracy and robustness of non-contact physiological\nmonitoring, achieving a mean absolute error (MAE) of 4.99 BPM in heart rate\nestimation. Furthermore, we find that multi-task learning enhances performance\nacross multiple physiological indicators simultaneously. Dataset and code are\nopen at https://github.com/McJackTang/FusionVitals.", "AI": {"tldr": "论文提出LADH数据集，结合RGB和红外视频提升远程光电容积描记术（rPPG）在长期健康监测中的准确性，解决了高海拔环境下光照变化和动态姿势的挑战。", "motivation": "解决rPPG在长期个人护理场景（如高海拔环境）中因光照变化、遮挡和动态姿势导致的监测不准确问题。", "method": "提出LADH数据集，包含240段同步RGB和红外视频，结合多任务学习提升生理信号监测性能。", "result": "RGB和红外视频结合显著提升监测准确性，心率估计的平均绝对误差为4.99 BPM。", "conclusion": "LADH数据集和RGB-IR结合方法为长期非接触式健康监测提供了有效解决方案。"}}
{"id": "2506.09992", "pdf": "https://arxiv.org/pdf/2506.09992", "abs": "https://arxiv.org/abs/2506.09992", "authors": ["Amel Muminovic", "Amela Kadric Muminovic"], "title": "Large Language Models for Toxic Language Detection in Low-Resource Balkan Languages", "categories": ["cs.CL"], "comment": "8 pages", "summary": "Online toxic language causes real harm, especially in regions with limited\nmoderation tools. In this study, we evaluate how large language models handle\ntoxic comments in Serbian, Croatian, and Bosnian, languages with limited\nlabeled data. We built and manually labeled a dataset of 4,500 YouTube and\nTikTok comments drawn from videos across diverse categories, including music,\npolitics, sports, modeling, influencer content, discussions of sexism, and\ngeneral topics. Four models (GPT-3.5 Turbo, GPT-4.1, Gemini 1.5 Pro, and Claude\n3 Opus) were tested in two modes: zero-shot and context-augmented. We measured\nprecision, recall, F1 score, accuracy and false positive rates. Including a\nshort context snippet raised recall by about 0.12 on average and improved F1\nscore by up to 0.10, though it sometimes increased false positives. The best\nbalance came from Gemini in context-augmented mode, reaching an F1 score of\n0.82 and accuracy of 0.82, while zero-shot GPT-4.1 led on precision and had the\nlowest false alarms. We show how adding minimal context can improve toxic\nlanguage detection in low-resource settings and suggest practical strategies\nsuch as improved prompt design and threshold calibration. These results show\nthat prompt design alone can yield meaningful gains in toxicity detection for\nunderserved Balkan language communities.", "AI": {"tldr": "研究评估了大型语言模型在塞尔维亚语、克罗地亚语和波斯尼亚语中处理有毒评论的能力，发现上下文增强模式能显著提升检测效果。", "motivation": "在线有毒语言对资源有限的地区造成实际危害，本研究旨在填补这些语言中标注数据不足的空白。", "method": "构建并手动标注了4,500条YouTube和TikTok评论数据集，测试了四种模型（GPT-3.5 Turbo、GPT-4.1、Gemini 1.5 Pro和Claude 3 Opus）在零样本和上下文增强模式下的表现。", "result": "上下文增强模式平均提升召回率0.12，F1分数最高提升0.10；Gemini在上下文增强模式下表现最佳（F1=0.82，准确率=0.82），GPT-4.1在零样本模式下精度最高且误报最低。", "conclusion": "研究表明，通过优化提示设计和阈值校准，可以在资源有限的巴尔干语言社区中显著提升有毒语言检测效果。"}}
{"id": "2506.09724", "pdf": "https://arxiv.org/pdf/2506.09724", "abs": "https://arxiv.org/abs/2506.09724", "authors": ["Ye Zhang", "Yu Zhou", "Yifeng Wang", "Jun Xiao", "Ziyue Wang", "Yongbing Zhang", "Jianxu Chen"], "title": "The Four Color Theorem for Cell Instance Segmentation", "categories": ["cs.CV"], "comment": "Accepted at ICML 2025", "summary": "Cell instance segmentation is critical to analyzing biomedical images, yet\naccurately distinguishing tightly touching cells remains a persistent\nchallenge. Existing instance segmentation frameworks, including\ndetection-based, contour-based, and distance mapping-based approaches, have\nmade significant progress, but balancing model performance with computational\nefficiency remains an open problem. In this paper, we propose a novel cell\ninstance segmentation method inspired by the four-color theorem. By\nconceptualizing cells as countries and tissues as oceans, we introduce a\nfour-color encoding scheme that ensures adjacent instances receive distinct\nlabels. This reformulation transforms instance segmentation into a constrained\nsemantic segmentation problem with only four predicted classes, substantially\nsimplifying the instance differentiation process. To solve the training\ninstability caused by the non-uniqueness of four-color encoding, we design an\nasymptotic training strategy and encoding transformation method. Extensive\nexperiments on various modes demonstrate our approach achieves state-of-the-art\nperformance. The code is available at https://github.com/zhangye-zoe/FCIS.", "AI": {"tldr": "提出了一种基于四色定理的新型细胞实例分割方法，通过四色编码简化实例区分过程，并在多种模式下实现了最先进的性能。", "motivation": "生物医学图像中紧密接触细胞的准确区分是一个持续挑战，现有方法在模型性能和计算效率之间难以平衡。", "method": "将细胞视为国家、组织视为海洋，引入四色编码方案，将实例分割转化为仅需预测四个类别的约束语义分割问题，并设计了渐进训练策略和编码转换方法。", "result": "在多种实验模式下实现了最先进的性能。", "conclusion": "该方法通过四色编码简化了实例分割问题，显著提升了性能，代码已开源。"}}
{"id": "2506.09347", "pdf": "https://arxiv.org/pdf/2506.09347", "abs": "https://arxiv.org/abs/2506.09347", "authors": ["Xuemei Cao", "Hanlin Gu", "Xin Yang", "Bingjun Wei", "Haoyang Liang", "Xiangkun Wang", "Tianrui Li"], "title": "ErrorEraser: Unlearning Data Bias for Improved Continual Learning", "categories": ["cs.LG", "cs.AI"], "comment": "12 pages", "summary": "Continual Learning (CL) primarily aims to retain knowledge to prevent\ncatastrophic forgetting and transfer knowledge to facilitate learning new\ntasks. Unlike traditional methods, we propose a novel perspective: CL not only\nneeds to prevent forgetting, but also requires intentional forgetting.This\narises from existing CL methods ignoring biases in real-world data, leading the\nmodel to learn spurious correlations that transfer and amplify across tasks.\nFrom feature extraction and prediction results, we find that data biases\nsimultaneously reduce CL's ability to retain and transfer knowledge. To address\nthis, we propose ErrorEraser, a universal plugin that removes erroneous\nmemories caused by biases in CL, enhancing performance in both new and old\ntasks. ErrorEraser consists of two modules: Error Identification and Error\nErasure. The former learns the probability density distribution of task data in\nthe feature space without prior knowledge, enabling accurate identification of\npotentially biased samples. The latter ensures only erroneous knowledge is\nerased by shifting the decision space of representative outlier samples.\nAdditionally, an incremental feature distribution learning strategy is designed\nto reduce the resource overhead during error identification in downstream\ntasks. Extensive experimental results show that ErrorEraser significantly\nmitigates the negative impact of data biases, achieving higher accuracy and\nlower forgetting rates across three types of CL methods. The code is available\nat https://github.com/diadai/ErrorEraser.", "AI": {"tldr": "论文提出了一种名为ErrorEraser的新方法，通过识别和消除由数据偏差引起的错误记忆，提升持续学习中的知识保留和迁移能力。", "motivation": "现有持续学习方法忽视真实数据中的偏差，导致模型学习虚假相关性，影响知识保留和迁移。", "method": "ErrorEraser包含两个模块：错误识别（通过特征空间概率密度分布识别偏差样本）和错误擦除（通过调整决策空间消除错误知识）。", "result": "实验表明，ErrorEraser显著减轻数据偏差的负面影响，在三种持续学习方法中实现更高准确率和更低遗忘率。", "conclusion": "ErrorEraser是一种通用插件，能有效提升持续学习性能，尤其在处理数据偏差时表现优异。"}}
{"id": "2506.09996", "pdf": "https://arxiv.org/pdf/2506.09996", "abs": "https://arxiv.org/abs/2506.09996", "authors": ["Yang Li", "Qiang Sheng", "Yehan Yang", "Xueyao Zhang", "Juan Cao"], "title": "From Judgment to Interference: Early Stopping LLM Harmful Outputs via Streaming Content Monitoring", "categories": ["cs.CL", "cs.CY"], "comment": "22 pages, 7 figures, and 9 tables", "summary": "Though safety alignment has been applied to most large language models\n(LLMs), LLM service providers generally deploy a subsequent moderation as the\nexternal safety guardrail in real-world products. Existing moderators mainly\npractice a conventional full detection, which determines the harmfulness based\non the complete LLM output, causing high service latency. Recent works pay more\nattention to partial detection where moderators oversee the generation midway\nand early stop the output if harmfulness is detected, but they directly apply\nmoderators trained with the full detection paradigm to incomplete outputs,\nintroducing a training-inference gap that lowers the performance. In this\npaper, we explore how to form a data-and-model solution that natively supports\npartial detection. For the data, we construct FineHarm, a dataset consisting of\n29K prompt-response pairs with fine-grained annotations to provide reasonable\nsupervision for token-level training. Then, we propose the streaming content\nmonitor, which is trained with dual supervision of response- and token-level\nlabels and can follow the output stream of LLM to make a timely judgment of\nharmfulness. Experiments show that SCM gains 0.95+ in macro F1 score that is\ncomparable to full detection, by only seeing the first 18% of tokens in\nresponses on average. Moreover, the SCM can serve as a pseudo-harmfulness\nannotator for improving safety alignment and lead to a higher harmlessness\nscore than DPO.", "AI": {"tldr": "论文提出了一种支持部分检测的数据与模型解决方案，通过构建FineHarm数据集和提出流式内容监控器（SCM），显著提高了检测效率。", "motivation": "现有安全检测方法存在高延迟或训练-推理差距问题，需要一种原生支持部分检测的解决方案。", "method": "构建FineHarm数据集，提出流式内容监控器（SCM），采用双监督训练。", "result": "SCM仅需查看18%的令牌即可达到与全检测相当的F1分数，且能提升安全性对齐效果。", "conclusion": "SCM是一种高效的部分检测方法，可显著提升LLM的安全性和响应速度。"}}
{"id": "2506.09735", "pdf": "https://arxiv.org/pdf/2506.09735", "abs": "https://arxiv.org/abs/2506.09735", "authors": ["Chuang Ma", "Shaokai Zhao", "Dongdong Zhou", "Yu Pei", "Zhiguo Luo", "Liang Xie", "Ye Yan", "Erwei Yin"], "title": "MPFNet: A Multi-Prior Fusion Network with a Progressive Training Strategy for Micro-Expression Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Micro-expression recognition (MER), a critical subfield of affective\ncomputing, presents greater challenges than macro-expression recognition due to\nits brief duration and low intensity. While incorporating prior knowledge has\nbeen shown to enhance MER performance, existing methods predominantly rely on\nsimplistic, singular sources of prior knowledge, failing to fully exploit\nmulti-source information. This paper introduces the Multi-Prior Fusion Network\n(MPFNet), leveraging a progressive training strategy to optimize MER tasks. We\npropose two complementary encoders: the Generic Feature Encoder (GFE) and the\nAdvanced Feature Encoder (AFE), both based on Inflated 3D ConvNets (I3D) with\nCoordinate Attention (CA) mechanisms, to improve the model's ability to capture\nspatiotemporal and channel-specific features. Inspired by developmental\npsychology, we present two variants of MPFNet--MPFNet-P and\nMPFNet-C--corresponding to two fundamental modes of infant cognitive\ndevelopment: parallel and hierarchical processing. These variants enable the\nevaluation of different strategies for integrating prior knowledge. Extensive\nexperiments demonstrate that MPFNet significantly improves MER accuracy while\nmaintaining balanced performance across categories, achieving accuracies of\n0.811, 0.924, and 0.857 on the SMIC, CASME II, and SAMM datasets, respectively.\nTo the best of our knowledge, our approach achieves state-of-the-art\nperformance on the SMIC and SAMM datasets.", "AI": {"tldr": "该论文提出了一种多先验融合网络（MPFNet），通过渐进式训练策略优化微表情识别任务，利用两种互补的编码器（GFE和AFE）提升模型性能，并在多个数据集上取得最优结果。", "motivation": "微表情识别（MER）因其短暂性和低强度而比宏表情更具挑战性。现有方法主要依赖单一先验知识，未能充分利用多源信息。", "method": "提出MPFNet，包含通用特征编码器（GFE）和高级特征编码器（AFE），基于I3D和坐标注意力机制，并设计了两种变体（MPFNet-P和MPFNet-C）以评估不同先验知识整合策略。", "result": "在SMIC、CASME II和SAMM数据集上分别达到0.811、0.924和0.857的准确率，在SMIC和SAMM上达到最优性能。", "conclusion": "MPFNet通过多源先验知识的融合显著提升了微表情识别性能，为相关领域提供了新的解决方案。"}}
{"id": "2506.09736", "pdf": "https://arxiv.org/pdf/2506.09736", "abs": "https://arxiv.org/abs/2506.09736", "authors": ["Yuting Li", "Lai Wei", "Kaipeng Zheng", "Jingyuan Huang", "Linghe Kong", "Lichao Sun", "Weiran Huang"], "title": "Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math Reasoning", "categories": ["cs.CV", "cs.AI"], "comment": "Technical Report", "summary": "Despite the rapid progress of multimodal large language models (MLLMs), they\nhave largely overlooked the importance of visual processing. In a simple yet\nrevealing experiment, we interestingly find that language-only models, when\nprovided with image captions, can achieve comparable or even better performance\nthan MLLMs that consume raw visual inputs. This suggests that current MLLMs may\ngenerate accurate visual descriptions but fail to effectively integrate them\nduring reasoning. Motivated by this, we propose a simple visual perturbation\nframework that enhances perceptual robustness without requiring algorithmic\nmodifications or additional training data. Our approach introduces three\ntargeted perturbations: distractor concatenation, dominance-preserving mixup,\nand random rotation, that can be easily integrated into existing post-training\npipelines including SFT, DPO, and GRPO. Through extensive experiments across\nmultiple datasets, we demonstrate consistent improvements in mathematical\nreasoning performance, with gains comparable to those achieved through\nalgorithmic changes. Additionally, we achieve competitive performance among\nopen-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual\nperturbation. Through comprehensive ablation studies, we analyze the\neffectiveness of different perturbation strategies, revealing that each\nperturbation type contributes uniquely to different aspects of visual\nreasoning. Our findings highlight the critical role of visual perturbation in\nmultimodal mathematical reasoning: better reasoning begins with better seeing.\nOur code is available at https://github.com/YutingLi0606/Vision-Matters.", "AI": {"tldr": "研究发现，当前多模态大语言模型（MLLMs）在视觉处理上表现不足，仅提供图像标题的语言模型表现更优。为此，作者提出了一种简单的视觉扰动框架，通过三种扰动方法提升模型性能，无需额外训练数据或算法修改。实验表明，该方法在多数据集上显著提升了数学推理能力。", "motivation": "当前MLLMs在视觉处理上表现不佳，仅依赖图像标题的语言模型反而表现更优，表明MLLMs在视觉信息整合上存在问题。", "method": "提出视觉扰动框架，包含三种扰动方法：干扰拼接、保持主导性的混合和随机旋转，可无缝集成到现有训练流程中。", "result": "实验显示，该方法在多数据集上显著提升了数学推理能力，性能提升与算法改进相当。训练后的Qwen2.5-VL-7B模型在开源7B RL调优模型中表现优异。", "conclusion": "视觉扰动在多模态数学推理中至关重要，每种扰动方法对视觉推理的不同方面有独特贡献，验证了“更好的推理始于更好的视觉”。"}}
{"id": "2506.09354", "pdf": "https://arxiv.org/pdf/2506.09354", "abs": "https://arxiv.org/abs/2506.09354", "authors": ["Kellie Yu Hui Sim", "Roy Ka-Wei Lee", "Kenny Tsu Wei Choo"], "title": "\"Is This Really a Human Peer Supporter?\": Misalignments Between Peer Supporters and Experts in LLM-Supported Interactions", "categories": ["cs.HC", "cs.AI", "H.5.0"], "comment": null, "summary": "Mental health is a growing global concern, prompting interest in AI-driven\nsolutions to expand access to psychosocial support. Peer support, grounded in\nlived experience, offers a valuable complement to professional care. However,\nvariability in training, effectiveness, and definitions raises concerns about\nquality, consistency, and safety. Large Language Models (LLMs) present new\nopportunities to enhance peer support interactions, particularly in real-time,\ntext-based interactions. We present and evaluate an AI-supported system with an\nLLM-simulated distressed client, context-sensitive LLM-generated suggestions,\nand real-time emotion visualisations. 2 mixed-methods studies with 12 peer\nsupporters and 5 mental health professionals (i.e., experts) examined the\nsystem's effectiveness and implications for practice. Both groups recognised\nits potential to enhance training and improve interaction quality. However, we\nfound a key tension emerged: while peer supporters engaged meaningfully,\nexperts consistently flagged critical issues in peer supporter responses, such\nas missed distress cues and premature advice-giving. This misalignment\nhighlights potential limitations in current peer support training, especially\nin emotionally charged contexts where safety and fidelity to best practices are\nessential. Our findings underscore the need for standardised, psychologically\ngrounded training, especially as peer support scales globally. They also\ndemonstrate how LLM-supported systems can scaffold this development--if\ndesigned with care and guided by expert oversight. This work contributes to\nemerging conversations on responsible AI integration in mental health and the\nevolving role of LLMs in augmenting peer-delivered care.", "AI": {"tldr": "AI驱动的系统通过LLM模拟求助者并提供实时建议，用于增强同伴支持的质量和训练，但专家发现其存在潜在问题。", "motivation": "心理健康问题日益严重，AI驱动的解决方案有望扩展心理社会支持的可及性，同伴支持作为专业护理的补充，但其质量和一致性存在挑战。", "method": "开发并评估了一个AI支持系统，包括LLM模拟的求助者、情境敏感的建议和实时情绪可视化，通过混合方法研究（12名同伴支持者和5名专家）验证其效果。", "result": "同伴支持者和专家均认可系统对训练和互动质量的提升潜力，但专家指出同伴支持者存在忽略求助信号和过早给出建议等问题。", "conclusion": "研究强调需要标准化、心理学基础的训练，并表明LLM支持的系统在专家指导下可以辅助同伴支持的发展，但需谨慎设计。"}}
{"id": "2506.09740", "pdf": "https://arxiv.org/pdf/2506.09740", "abs": "https://arxiv.org/abs/2506.09740", "authors": ["Qin Zhou", "Zhiyang Zhang", "Jinglong Wang", "Xiaobin Li", "Jing Zhang", "Qian Yu", "Lu Sheng", "Dong Xu"], "title": "ELBO-T2IAlign: A Generic ELBO-Based Method for Calibrating Pixel-level Text-Image Alignment in Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Diffusion models excel at image generation. Recent studies have shown that\nthese models not only generate high-quality images but also encode text-image\nalignment information through attention maps or loss functions. This\ninformation is valuable for various downstream tasks, including segmentation,\ntext-guided image editing, and compositional image generation. However, current\nmethods heavily rely on the assumption of perfect text-image alignment in\ndiffusion models, which is not the case. In this paper, we propose using\nzero-shot referring image segmentation as a proxy task to evaluate the\npixel-level image and class-level text alignment of popular diffusion models.\nWe conduct an in-depth analysis of pixel-text misalignment in diffusion models\nfrom the perspective of training data bias. We find that misalignment occurs in\nimages with small sized, occluded, or rare object classes. Therefore, we\npropose ELBO-T2IAlign, a simple yet effective method to calibrate pixel-text\nalignment in diffusion models based on the evidence lower bound (ELBO) of\nlikelihood. Our method is training-free and generic, eliminating the need to\nidentify the specific cause of misalignment and works well across various\ndiffusion model architectures. Extensive experiments on commonly used benchmark\ndatasets on image segmentation and generation have verified the effectiveness\nof our proposed calibration approach.", "AI": {"tldr": "论文提出了一种基于ELBO的方法（ELBO-T2IAlign）来校准扩散模型中的像素-文本对齐问题，无需训练且适用于多种模型架构。", "motivation": "扩散模型在图像生成中表现出色，但其文本-图像对齐假设并不完美，导致像素级和类级对齐问题，尤其是在小尺寸、遮挡或罕见对象类别中。", "method": "使用零样本参考图像分割作为代理任务评估扩散模型的对齐情况，并提出基于ELBO的校准方法（ELBO-T2IAlign）。", "result": "实验验证了该方法在图像分割和生成任务中的有效性。", "conclusion": "ELBO-T2IAlign是一种简单且通用的方法，能够显著改善扩散模型的文本-图像对齐问题。"}}
{"id": "2506.09362", "pdf": "https://arxiv.org/pdf/2506.09362", "abs": "https://arxiv.org/abs/2506.09362", "authors": ["Kellie Yu Hui Sim", "Kenny Tsu Wei Choo"], "title": "\"I Said Things I Needed to Hear Myself\": Peer Support as an Emotional, Organisational, and Sociotechnical Practice in Singapore", "categories": ["cs.HC", "cs.AI", "H.5.0"], "comment": null, "summary": "Peer support plays a vital role in expanding access to mental health care by\nproviding empathetic, community-based support outside formal clinical systems.\nAs digital platforms increasingly mediate such support, the design and impact\nof these technologies remain under-examined, particularly in Asian contexts.\nThis paper presents findings from an interview study with 20 peer supporters in\nSingapore, who operate across diverse online, offline, and hybrid environments.\nThrough a thematic analysis, we unpack how participants start, conduct, and\nsustain peer support, highlighting their motivations, emotional labour, and the\nsociocultural dimensions shaping their practices. Building on this grounded\nunderstanding, we surface design directions for culturally responsive digital\ntools that scaffold rather than supplant relational care. Drawing insights from\nqualitative accounts, we offer a situated perspective on how AI might\nresponsibly augment peer support. This research contributes to human-centred\ncomputing by articulating the lived realities of peer supporters and proposing\ndesign implications for trustworthy and context-sensitive AI in mental health.", "AI": {"tldr": "论文探讨了数字平台在亚洲背景下对心理健康同伴支持的设计与影响，基于新加坡20名同伴支持者的访谈研究，提出了文化响应式数字工具的设计方向。", "motivation": "研究动机在于填补数字技术在亚洲心理健康同伴支持领域的空白，并探索其设计与影响。", "method": "通过访谈20名新加坡的同伴支持者，并进行主题分析，研究其动机、情感劳动及实践中的社会文化维度。", "result": "研究发现同伴支持者的动机、情感劳动及社会文化因素对其实践有重要影响，提出了文化响应式数字工具的设计方向。", "conclusion": "研究为心理健康领域提供了以人为本的计算视角，提出了可信赖且情境敏感的AI设计建议。"}}
{"id": "2506.09745", "pdf": "https://arxiv.org/pdf/2506.09745", "abs": "https://arxiv.org/abs/2506.09745", "authors": ["Yangrui Zhu", "Junhua Bao", "Yipan Wei", "Yapeng Li", "Bo Du"], "title": "Class Similarity-Based Multimodal Classification under Heterogeneous Category Sets", "categories": ["cs.CV"], "comment": null, "summary": "Existing multimodal methods typically assume that different modalities share\nthe same category set. However, in real-world applications, the category\ndistributions in multimodal data exhibit inconsistencies, which can hinder the\nmodel's ability to effectively utilize cross-modal information for recognizing\nall categories. In this work, we propose the practical setting termed\nMulti-Modal Heterogeneous Category-set Learning (MMHCL), where models are\ntrained in heterogeneous category sets of multi-modal data and aim to recognize\ncomplete classes set of all modalities during test. To effectively address this\ntask, we propose a Class Similarity-based Cross-modal Fusion model (CSCF).\nSpecifically, CSCF aligns modality-specific features to a shared semantic space\nto enable knowledge transfer between seen and unseen classes. It then selects\nthe most discriminative modality for decision fusion through uncertainty\nestimation. Finally, it integrates cross-modal information based on class\nsimilarity, where the auxiliary modality refines the prediction of the dominant\none. Experimental results show that our method significantly outperforms\nexisting state-of-the-art (SOTA) approaches on multiple benchmark datasets,\neffectively addressing the MMHCL task.", "AI": {"tldr": "论文提出了一种多模态异构类别集学习（MMHCL）任务，并提出了基于类相似性的跨模态融合模型（CSCF）来解决该任务，显著优于现有方法。", "motivation": "现实应用中多模态数据的类别分布不一致，限制了模型有效利用跨模态信息识别所有类别。", "method": "CSCF将模态特定特征对齐到共享语义空间，通过不确定性估计选择最具判别性的模态，并基于类相似性整合跨模态信息。", "result": "实验结果表明，CSCF在多个基准数据集上显著优于现有方法。", "conclusion": "CSCF有效解决了MMHCL任务，提升了多模态数据中类别识别的性能。"}}
{"id": "2506.09748", "pdf": "https://arxiv.org/pdf/2506.09748", "abs": "https://arxiv.org/abs/2506.09748", "authors": ["Xiangkai Zhang", "Xiang Zhou", "Mao Chen", "Yuchen Lu", "Xu Yang", "Zhiyong Liu"], "title": "Hierarchical Image Matching for UAV Absolute Visual Localization via Semantic and Structural Constraints", "categories": ["cs.CV", "cs.RO"], "comment": "8 pages, 6 figures", "summary": "Absolute localization, aiming to determine an agent's location with respect\nto a global reference, is crucial for unmanned aerial vehicles (UAVs) in\nvarious applications, but it becomes challenging when global navigation\nsatellite system (GNSS) signals are unavailable. Vision-based absolute\nlocalization methods, which locate the current view of the UAV in a reference\nsatellite map to estimate its position, have become popular in GNSS-denied\nscenarios. However, existing methods mostly rely on traditional and low-level\nimage matching, suffering from difficulties due to significant differences\nintroduced by cross-source discrepancies and temporal variations. To overcome\nthese limitations, in this paper, we introduce a hierarchical cross-source\nimage matching method designed for UAV absolute localization, which integrates\na semantic-aware and structure-constrained coarse matching module with a\nlightweight fine-grained matching module. Specifically, in the coarse matching\nmodule, semantic features derived from a vision foundation model first\nestablish region-level correspondences under semantic and structural\nconstraints. Then, the fine-grained matching module is applied to extract fine\nfeatures and establish pixel-level correspondences. Building upon this, a UAV\nabsolute visual localization pipeline is constructed without any reliance on\nrelative localization techniques, mainly by employing an image retrieval module\nbefore the proposed hierarchical image matching modules. Experimental\nevaluations on public benchmark datasets and a newly introduced CS-UAV dataset\ndemonstrate superior accuracy and robustness of the proposed method under\nvarious challenging conditions, confirming its effectiveness.", "AI": {"tldr": "提出了一种分层跨源图像匹配方法，用于无人机绝对定位，结合语义感知和结构约束的粗匹配模块与轻量级细粒度匹配模块，显著提升了定位精度和鲁棒性。", "motivation": "在GNSS信号不可用时，无人机绝对定位面临挑战，现有基于视觉的方法因跨源差异和时间变化导致匹配困难。", "method": "采用分层匹配方法：粗匹配模块利用视觉基础模型的语义特征建立区域级对应，细粒度匹配模块提取像素级特征。", "result": "在公开基准数据集和新CS-UAV数据集上验证了方法的优越性和鲁棒性。", "conclusion": "该方法无需依赖相对定位技术，在复杂条件下有效提升了无人机绝对定位的准确性。"}}
{"id": "2506.09777", "pdf": "https://arxiv.org/pdf/2506.09777", "abs": "https://arxiv.org/abs/2506.09777", "authors": ["Anton Razzhigaev", "Matvey Mikhalchuk", "Klim Kireev", "Igor Udovichenko", "Andrey Kuznetsov", "Aleksandr Petiushko"], "title": "Inverting Black-Box Face Recognition Systems via Zero-Order Optimization in Eigenface Space", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Reconstructing facial images from black-box recognition models poses a\nsignificant privacy threat. While many methods require access to embeddings, we\naddress the more challenging scenario of model inversion using only similarity\nscores. This paper introduces DarkerBB, a novel approach that reconstructs\ncolor faces by performing zero-order optimization within a PCA-derived\neigenface space. Despite this highly limited information, experiments on LFW,\nAgeDB-30, and CFP-FP benchmarks demonstrate that DarkerBB achieves\nstate-of-the-art verification accuracies in the similarity-only setting, with\ncompetitive query efficiency.", "AI": {"tldr": "DarkerBB是一种新方法，仅通过相似性分数从黑盒人脸识别模型中重建彩色人脸图像，解决了隐私威胁问题。", "motivation": "从黑盒模型中重建人脸图像对隐私构成威胁，现有方法通常需要嵌入信息，而本文专注于仅使用相似性分数的更具挑战性的场景。", "method": "DarkerBB在PCA衍生的特征脸空间内进行零阶优化，实现人脸重建。", "result": "在LFW、AgeDB-30和CFP-FP基准测试中，DarkerBB在仅使用相似性分数的设置下达到了最先进的验证准确率，且查询效率具有竞争力。", "conclusion": "DarkerBB展示了在有限信息下仍能高效重建人脸图像的能力，为隐私保护提出了新的挑战。"}}
{"id": "2506.09368", "pdf": "https://arxiv.org/pdf/2506.09368", "abs": "https://arxiv.org/abs/2506.09368", "authors": ["Yang Liu", "Jing Liu", "Chengfang Li", "Rui Xi", "Wenchao Li", "Liang Cao", "Jin Wang", "Laurence T. Yang", "Junsong Yuan", "Wei Zhou"], "title": "Anomaly Detection and Generation with Diffusion Models: A Survey", "categories": ["cs.LG", "cs.AI"], "comment": "20 pages, 11 figures, 13 tables", "summary": "Anomaly detection (AD) plays a pivotal role across diverse domains, including\ncybersecurity, finance, healthcare, and industrial manufacturing, by\nidentifying unexpected patterns that deviate from established norms in\nreal-world data. Recent advancements in deep learning, specifically diffusion\nmodels (DMs), have sparked significant interest due to their ability to learn\ncomplex data distributions and generate high-fidelity samples, offering a\nrobust framework for unsupervised AD. In this survey, we comprehensively review\nanomaly detection and generation with diffusion models (ADGDM), presenting a\ntutorial-style analysis of the theoretical foundations and practical\nimplementations and spanning images, videos, time series, tabular, and\nmultimodal data. Crucially, unlike existing surveys that often treat anomaly\ndetection and generation as separate problems, we highlight their inherent\nsynergistic relationship. We reveal how DMs enable a reinforcing cycle where\ngeneration techniques directly address the fundamental challenge of anomaly\ndata scarcity, while detection methods provide critical feedback to improve\ngeneration fidelity and relevance, advancing both capabilities beyond their\nindividual potential. A detailed taxonomy categorizes ADGDM methods based on\nanomaly scoring mechanisms, conditioning strategies, and architectural designs,\nanalyzing their strengths and limitations. We final discuss key challenges\nincluding scalability and computational efficiency, and outline promising\nfuture directions such as efficient architectures, conditioning strategies, and\nintegration with foundation models (e.g., visual-language models and large\nlanguage models). By synthesizing recent advances and outlining open research\nquestions, this survey aims to guide researchers and practitioners in\nleveraging DMs for innovative AD solutions across diverse applications.", "AI": {"tldr": "该综述全面回顾了基于扩散模型的异常检测与生成（ADGDM），探讨了其理论基础、实际应用及在多种数据类型中的表现，并强调了检测与生成之间的协同关系。", "motivation": "扩散模型在复杂数据分布学习和高保真样本生成方面的能力，为无监督异常检测提供了强大框架，激发了研究者对其在异常检测领域的兴趣。", "method": "通过教程式分析，综述了ADGDM的理论基础和实践实现，并基于异常评分机制、条件策略和架构设计对方法进行了分类。", "result": "揭示了扩散模型如何通过生成技术解决异常数据稀缺问题，同时检测方法如何提升生成质量和相关性，从而推动两者能力的共同提升。", "conclusion": "综述总结了ADGDM的挑战（如可扩展性和计算效率）和未来方向（如高效架构和与基础模型的整合），旨在指导研究者和从业者利用扩散模型开发创新的异常检测解决方案。"}}
{"id": "2506.09148", "pdf": "https://arxiv.org/pdf/2506.09148", "abs": "https://arxiv.org/abs/2506.09148", "authors": ["Hetvi Waghela", "Jaydip Sen", "Sneha Rakshit", "Subhasis Dasgupta"], "title": "Adversarial Text Generation with Dynamic Contextual Perturbation", "categories": ["cs.CR", "cs.CL"], "comment": "This is the accepted version of the paper, which was presented at\n  IEEE CALCON. The conference was organized at Jadavpur University, Kolkata,\n  from December 14 to 15, 2025. The paper is six pages long, and it consists of\n  six tables and six figures. This is not the final camera-ready version of the\n  paper", "summary": "Adversarial attacks on Natural Language Processing (NLP) models expose\nvulnerabilities by introducing subtle perturbations to input text, often\nleading to misclassification while maintaining human readability. Existing\nmethods typically focus on word-level or local text segment alterations,\noverlooking the broader context, which results in detectable or semantically\ninconsistent perturbations. We propose a novel adversarial text attack scheme\nnamed Dynamic Contextual Perturbation (DCP). DCP dynamically generates\ncontext-aware perturbations across sentences, paragraphs, and documents,\nensuring semantic fidelity and fluency. Leveraging the capabilities of\npre-trained language models, DCP iteratively refines perturbations through an\nadversarial objective function that balances the dual objectives of inducing\nmodel misclassification and preserving the naturalness of the text. This\ncomprehensive approach allows DCP to produce more sophisticated and effective\nadversarial examples that better mimic natural language patterns. Our\nexperimental results, conducted on various NLP models and datasets, demonstrate\nthe efficacy of DCP in challenging the robustness of state-of-the-art NLP\nsystems. By integrating dynamic contextual analysis, DCP significantly enhances\nthe subtlety and impact of adversarial attacks. This study highlights the\ncritical role of context in adversarial attacks and lays the groundwork for\ncreating more robust NLP systems capable of withstanding sophisticated\nadversarial strategies.", "AI": {"tldr": "提出了一种动态上下文扰动（DCP）方法，通过上下文感知的扰动生成更隐蔽且有效的对抗样本，提升对抗攻击的效果。", "motivation": "现有对抗攻击方法多关注局部文本修改，忽略了上下文，导致扰动易被检测或语义不一致。", "method": "DCP利用预训练语言模型动态生成上下文感知的扰动，通过对抗目标函数平衡误导性和文本自然性。", "result": "实验表明DCP能有效挑战现有NLP模型的鲁棒性，生成更自然的对抗样本。", "conclusion": "DCP强调了上下文在对抗攻击中的重要性，为构建更鲁棒的NLP系统提供了基础。"}}
{"id": "2506.09782", "pdf": "https://arxiv.org/pdf/2506.09782", "abs": "https://arxiv.org/abs/2506.09782", "authors": ["Nicola Farronato", "Florian Scheidegger", "Mattia Rigotti", "Cristiano Malossi", "Michele Magno", "Haotong Qin"], "title": "Q-SAM2: Accurate Quantization for Segment Anything Model 2", "categories": ["cs.CV", "cs.AI"], "comment": "20 pages", "summary": "The Segment Anything Model 2 (SAM2) has gained significant attention as a\nfoundational approach for promptable image and video segmentation. However, its\nexpensive computational and memory consumption poses a severe challenge for its\napplication in resource-constrained scenarios. In this paper, we propose an\naccurate low-bit quantization method for efficient SAM2, termed Q-SAM2. To\naddress the performance degradation caused by the singularities in weight and\nactivation distributions during quantization, Q-SAM2 introduces two novel\ntechnical contributions. We first introduce a linear layer calibration method\nfor low-bit initialization of SAM2, which minimizes the Frobenius norm over a\nsmall image batch to reposition weight distributions for improved quantization.\nWe then propose a Quantization-Aware Training (QAT) pipeline that applies\nclipping to suppress outliers and allows the network to adapt to quantization\nthresholds during training. Our comprehensive experiments demonstrate that\nQ-SAM2 allows for highly accurate inference while substantially improving\nefficiency. Both quantitative and visual results show that our Q-SAM2 surpasses\nexisting state-of-the-art general quantization schemes, especially for\nultra-low 2-bit quantization. While designed for quantization-aware training,\nour proposed calibration technique also proves effective in post-training\nquantization, achieving up to a 66% mIoU accuracy improvement over\nnon-calibrated models.", "AI": {"tldr": "Q-SAM2是一种针对SAM2的低比特量化方法，通过线性层校准和量化感知训练提高效率，同时保持高精度。", "motivation": "SAM2的计算和内存消耗高，限制了其在资源受限场景中的应用，因此需要一种高效的量化方法。", "method": "Q-SAM2引入线性层校准和量化感知训练（QAT）管道，优化权重分布并抑制异常值。", "result": "Q-SAM2在超低2比特量化下表现优异，比现有量化方案更准确，且在训练后量化中也有显著改进。", "conclusion": "Q-SAM2是一种高效且准确的量化方法，适用于资源受限环境，并显著提升性能。"}}
{"id": "2506.09373", "pdf": "https://arxiv.org/pdf/2506.09373", "abs": "https://arxiv.org/abs/2506.09373", "authors": ["Jiaqi Tang", "Yu Xia", "Yi-Feng Wu", "Yuwei Hu", "Yuhui Chen", "Qing-Guo Chen", "Xiaogang Xu", "Xiangyu Wu", "Hao Lu", "Yanqing Ma", "Shiyin Lu", "Qifeng Chen"], "title": "LPO: Towards Accurate GUI Agent Interaction via Location Preference Optimization", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "The advent of autonomous agents is transforming interactions with Graphical\nUser Interfaces (GUIs) by employing natural language as a powerful\nintermediary. Despite the predominance of Supervised Fine-Tuning (SFT) methods\nin current GUI agents for achieving spatial localization, these methods face\nsubstantial challenges due to their limited capacity to accurately perceive\npositional data. Existing strategies, such as reinforcement learning, often\nfail to assess positional accuracy effectively, thereby restricting their\nutility. In response, we introduce Location Preference Optimization (LPO), a\nnovel approach that leverages locational data to optimize interaction\npreferences. LPO uses information entropy to predict interaction positions by\nfocusing on zones rich in information. Besides, it further introduces a dynamic\nlocation reward function based on physical distance, reflecting the varying\nimportance of interaction positions. Supported by Group Relative Preference\nOptimization (GRPO), LPO facilitates an extensive exploration of GUI\nenvironments and significantly enhances interaction precision. Comprehensive\nexperiments demonstrate LPO's superior performance, achieving SOTA results\nacross both offline benchmarks and real-world online evaluations. Our code will\nbe made publicly available soon, at https://github.com/AIDC-AI/LPO.", "AI": {"tldr": "论文提出了一种名为LPO的新方法，通过利用位置数据和信息熵优化GUI交互，显著提升了交互精度。", "motivation": "当前GUI代理在空间定位中主要依赖监督微调方法，但存在位置感知能力不足的问题，现有策略如强化学习难以有效评估位置准确性。", "method": "LPO利用信息熵预测交互位置，引入动态位置奖励函数，并结合GRPO方法进行GUI环境探索。", "result": "实验表明LPO在离线基准和在线评估中均达到SOTA性能。", "conclusion": "LPO通过优化位置偏好，显著提升了GUI交互的精确性，代码将公开。"}}
{"id": "2506.09784", "pdf": "https://arxiv.org/pdf/2506.09784", "abs": "https://arxiv.org/abs/2506.09784", "authors": ["Andrea Caraffa", "Davide Boscaini", "Fabio Poiesi"], "title": "Accurate and efficient zero-shot 6D pose estimation with frozen foundation models", "categories": ["cs.CV"], "comment": "Technical report", "summary": "Estimating the 6D pose of objects from RGBD data is a fundamental problem in\ncomputer vision, with applications in robotics and augmented reality. A key\nchallenge is achieving generalization to novel objects that were not seen\nduring training. Most existing approaches address this by scaling up training\non synthetic data tailored to the task, a process that demands substantial\ncomputational resources. But is task-specific training really necessary for\naccurate and efficient 6D pose estimation of novel objects? To answer No!, we\nintroduce FreeZeV2, the second generation of FreeZe: a training-free method\nthat achieves strong generalization to unseen objects by leveraging geometric\nand vision foundation models pre-trained on unrelated data. FreeZeV2 improves\nboth accuracy and efficiency over FreeZe through three key contributions: (i) a\nsparse feature extraction strategy that reduces inference-time computation\nwithout sacrificing accuracy; (ii) a feature-aware scoring mechanism that\nimproves both pose selection during RANSAC-based 3D registration and the final\nranking of pose candidates; and (iii) a modular design that supports ensembles\nof instance segmentation models, increasing robustness to segmentation masks\nerrors. We evaluate FreeZeV2 on the seven core datasets of the BOP Benchmark,\nwhere it establishes a new state-of-the-art in 6D pose estimation of unseen\nobjects. When using the same segmentation masks, FreeZeV2 achieves a remarkable\n8x speedup over FreeZe while also improving accuracy by 5%. When using\nensembles of segmentation models, FreeZeV2 gains an additional 8% in accuracy\nwhile still running 2.5x faster than FreeZe. FreeZeV2 was awarded Best Overall\nMethod at the BOP Challenge 2024.", "AI": {"tldr": "FreeZeV2是一种无需训练的方法，通过利用预训练的几何和视觉基础模型，实现了对未见物体的6D姿态估计的强泛化能力，并在准确性和效率上优于前代。", "motivation": "解决现有方法需要大量计算资源进行任务特定训练的问题，探索是否可以通过预训练模型实现高效准确的6D姿态估计。", "method": "FreeZeV2通过稀疏特征提取、特征感知评分机制和模块化设计改进前代方法，减少计算量并提高准确性。", "result": "在BOP Benchmark的七个核心数据集上，FreeZeV2在未见物体的6D姿态估计中达到新SOTA，速度提升8倍，准确性提高5%。", "conclusion": "FreeZeV2证明了无需任务特定训练即可实现高效准确的6D姿态估计，为未来研究提供了新方向。"}}
{"id": "2506.09383", "pdf": "https://arxiv.org/pdf/2506.09383", "abs": "https://arxiv.org/abs/2506.09383", "authors": ["Chengtian Ma", "Yunyue Wei", "Chenhui Zuo", "Chen Zhang", "Yanan Sui"], "title": "Bipedal Balance Control with Whole-body Musculoskeletal Standing and Falling Simulations", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": null, "summary": "Balance control is important for human and bipedal robotic systems. While\ndynamic balance during locomotion has received considerable attention,\nquantitative understanding of static balance and falling remains limited. This\nwork presents a hierarchical control pipeline for simulating human balance via\na comprehensive whole-body musculoskeletal system. We identified spatiotemporal\ndynamics of balancing during stable standing, revealed the impact of muscle\ninjury on balancing behavior, and generated fall contact patterns that aligned\nwith clinical data. Furthermore, our simulated hip exoskeleton assistance\ndemonstrated improvement in balance maintenance and reduced muscle effort under\nperturbation. This work offers unique muscle-level insights into human balance\ndynamics that are challenging to capture experimentally. It could provide a\nfoundation for developing targeted interventions for individuals with balance\nimpairments and support the advancement of humanoid robotic systems.", "AI": {"tldr": "该论文提出了一种层次化控制流程，通过全身肌肉骨骼系统模拟人类平衡，揭示了平衡的时空动态，并验证了髋关节外骨骼辅助对平衡的改善效果。", "motivation": "动态平衡研究较多，但静态平衡和跌倒的定量理解有限，需要更深入的肌肉层面的研究。", "method": "采用层次化控制流程和全身肌肉骨骼系统模拟人类平衡，分析稳定站立时的时空动态、肌肉损伤影响及跌倒接触模式。", "result": "揭示了平衡的时空动态，验证了髋关节外骨骼辅助能改善平衡并减少肌肉努力，生成了与临床数据一致的跌倒接触模式。", "conclusion": "该研究为理解人类平衡提供了肌肉层面的独特见解，有助于开发针对平衡障碍的干预措施和推动人形机器人系统发展。"}}
{"id": "2506.09814", "pdf": "https://arxiv.org/pdf/2506.09814", "abs": "https://arxiv.org/abs/2506.09814", "authors": ["Xiandong Zou", "Ruihao Xia", "Hongsong Wang", "Pan Zhou"], "title": "DreamCS: Geometry-Aware Text-to-3D Generation with Unpaired 3D Reward Supervision", "categories": ["cs.CV"], "comment": null, "summary": "While text-to-3D generation has attracted growing interest, existing methods\noften struggle to produce 3D assets that align well with human preferences.\nCurrent preference alignment techniques for 3D content typically rely on\nhardly-collected preference-paired multi-view 2D images to train 2D reward\nmodels, when then guide 3D generation -- leading to geometric artifacts due to\ntheir inherent 2D bias. To address these limitations, we construct 3D-MeshPref,\nthe first large-scale unpaired 3D preference dataset, featuring diverse 3D\nmeshes annotated by a large language model and refined by human evaluators. We\nthen develop RewardCS, the first reward model trained directly on unpaired\n3D-MeshPref data using a novel Cauchy-Schwarz divergence objective, enabling\neffective learning of human-aligned 3D geometric preferences without requiring\npaired comparisons. Building on this, we propose DreamCS, a unified framework\nthat integrates RewardCS into text-to-3D pipelines -- enhancing both implicit\nand explicit 3D generation with human preference feedback. Extensive\nexperiments show DreamCS outperforms prior methods, producing 3D assets that\nare both geometrically faithful and human-preferred. Code and models will be\nreleased publicly.", "AI": {"tldr": "论文提出了3D-MeshPref数据集和RewardCS奖励模型，结合DreamCS框架，改进了文本到3D生成的对齐人类偏好能力。", "motivation": "现有文本到3D生成方法难以生成符合人类偏好的3D资产，且依赖难以收集的2D偏好配对数据，导致几何伪影。", "method": "构建3D-MeshPref数据集，开发基于Cauchy-Schwarz散度的RewardCS奖励模型，并集成到DreamCS框架中。", "result": "DreamCS在实验中优于现有方法，生成的3D资产几何准确且符合人类偏好。", "conclusion": "提出的方法解决了2D偏好数据的局限性，显著提升了3D生成的质量和人类偏好对齐。"}}
{"id": "2506.09396", "pdf": "https://arxiv.org/pdf/2506.09396", "abs": "https://arxiv.org/abs/2506.09396", "authors": ["Zongjie Li", "Shuai Wang"], "title": "Reasoning as a Resource: Optimizing Fast and Slow Thinking in Code Generation Models", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "This position paper proposes a fundamental shift in designing code generation\nmodels: treating reasoning depth as a controllable resource. Rather than being\nan incidental byproduct of prompting, we argue that the trade-off between\nrapid, direct answers (\"fast thinking\") and elaborate, chain-of-thought\ndeliberation (\"slow thinking\") must be explicitly managed. We contend that\noptimizing reasoning budgets across the entire model lifecycle - from synthetic\ndata creation and benchmarking to real-world deploymen - can unlock superior\ntrade-offs among accuracy, latency, and cost. This paper outlines how adaptive\ncontrol over reasoning can enrich supervision signals, motivate new\nmulti-dimensional benchmarks, and inform cost-aware, security-conscious\ndeployment policies. By viewing fast and slow thinking as complementary modes\nto be scheduled, we envision coding agents that think deep when necessary and\nact fast when possible.", "AI": {"tldr": "论文提出将推理深度作为可控资源，优化代码生成模型的设计，以平衡快速直接回答与详细推理之间的权衡。", "motivation": "传统代码生成模型未明确管理推理深度，导致在准确性、延迟和成本之间的权衡不足。", "method": "通过自适应控制推理深度，优化从数据生成到部署的整个模型生命周期。", "result": "该方法有望提升监督信号、推动多维基准测试，并支持成本和安全意识的部署策略。", "conclusion": "将快速与慢速推理视为互补模式，可实现代码代理在必要时深入思考，在可能时快速行动。"}}
{"id": "2506.09260", "pdf": "https://arxiv.org/pdf/2506.09260", "abs": "https://arxiv.org/abs/2506.09260", "authors": ["Yibin Lei", "Tao Shen", "Andrew Yates"], "title": "ThinkQE: Query Expansion via an Evolving Thinking Process", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Effective query expansion for web search benefits from promoting both\nexploration and result diversity to capture multiple interpretations and facets\nof a query. While recent LLM-based methods have improved retrieval performance\nand demonstrate strong domain generalization without additional training, they\noften generate narrowly focused expansions that overlook these desiderata. We\npropose ThinkQE, a test-time query expansion framework addressing this\nlimitation through two key components: a thinking-based expansion process that\nencourages deeper and comprehensive semantic exploration, and a\ncorpus-interaction strategy that iteratively refines expansions using retrieval\nfeedback from the corpus. Experiments on diverse web search benchmarks (DL19,\nDL20, and BRIGHT) show ThinkQE consistently outperforms prior approaches,\nincluding training-intensive dense retrievers and rerankers.", "AI": {"tldr": "ThinkQE通过思考式扩展和语料库交互策略提升查询扩展的多样性和性能，优于现有方法。", "motivation": "现有LLM方法生成的查询扩展过于狭窄，未能充分捕捉查询的多重解释和方面。", "method": "提出ThinkQE框架，包括思考式扩展过程和语料库交互策略。", "result": "在多个基准测试（DL19、DL20、BRIGHT）中表现优于现有方法。", "conclusion": "ThinkQE显著提升了查询扩展的效果，无需额外训练即可实现领域泛化。"}}
{"id": "2506.09834", "pdf": "https://arxiv.org/pdf/2506.09834", "abs": "https://arxiv.org/abs/2506.09834", "authors": ["Chuang Maa", "Yu Peia", "Jianhang Zhanga", "Shaokai Zhaoa", "Bowen Jib", "Liang Xiea", "Ye Yana", "Erwei Yin"], "title": "MMME: A Spontaneous Multi-Modal Micro-Expression Dataset Enabling Visual-Physiological Fusion", "categories": ["cs.CV"], "comment": null, "summary": "Micro-expressions (MEs) are subtle, fleeting nonverbal cues that reveal an\nindividual's genuine emotional state. Their analysis has attracted considerable\ninterest due to its promising applications in fields such as healthcare,\ncriminal investigation, and human-computer interaction. However, existing ME\nresearch is limited to single visual modality, overlooking the rich emotional\ninformation conveyed by other physiological modalities, resulting in ME\nrecognition and spotting performance far below practical application needs.\nTherefore, exploring the cross-modal association mechanism between ME visual\nfeatures and physiological signals (PS), and developing a multimodal fusion\nframework, represents a pivotal step toward advancing ME analysis. This study\nintroduces a novel ME dataset, MMME, which, for the first time, enables\nsynchronized collection of facial action signals (MEs), central nervous system\nsignals (EEG), and peripheral PS (PPG, RSP, SKT, EDA, and ECG). By overcoming\nthe constraints of existing ME corpora, MMME comprises 634 MEs, 2,841\nmacro-expressions (MaEs), and 2,890 trials of synchronized multimodal PS,\nestablishing a robust foundation for investigating ME neural mechanisms and\nconducting multimodal fusion-based analyses. Extensive experiments validate the\ndataset's reliability and provide benchmarks for ME analysis, demonstrating\nthat integrating MEs with PS significantly enhances recognition and spotting\nperformance. To the best of our knowledge, MMME is the most comprehensive ME\ndataset to date in terms of modality diversity. It provides critical data\nsupport for exploring the neural mechanisms of MEs and uncovering the\nvisual-physiological synergistic effects, driving a paradigm shift in ME\nresearch from single-modality visual analysis to multimodal fusion. The dataset\nwill be publicly available upon acceptance of this paper.", "AI": {"tldr": "论文提出了一种新的微表情数据集MMME，首次同步采集面部动作信号、中枢神经系统信号和外周生理信号，验证了多模态融合对微表情分析的显著提升。", "motivation": "现有微表情研究局限于单一视觉模态，忽略了其他生理模态传递的情感信息，导致识别性能不足。", "method": "构建MMME数据集，包含634个微表情、2841个宏表情和2890个同步多模态生理信号，并进行多模态融合分析。", "result": "实验验证了数据集的可靠性，并证明多模态融合显著提升了微表情的识别和检测性能。", "conclusion": "MMME是目前最全面的微表情数据集，推动了微表情研究从单模态视觉分析向多模态融合的范式转变。"}}
{"id": "2506.09397", "pdf": "https://arxiv.org/pdf/2506.09397", "abs": "https://arxiv.org/abs/2506.09397", "authors": ["Xiangchen Li", "Dimitrios Spatharakis", "Saeid Ghafouri", "Jiakun Fan", "Dimitrios Nikolopoulos"], "title": "SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.NI", "68T07, 68M14", "I.2.6; C.2.4; C.1.4"], "comment": "6 pages, 9 figures, 2 tables", "summary": "Regardless the advancements in device capabilities, efficient inferencing\nadvanced large language models (LLMs) at the edge remains challenging due to\nlimited device memory and power constraints. Existing strategies, such as\naggressive quantization, pruning, or remote inference, trade accuracy for\nefficiency or lead to substantial cost burdens. This position paper introduces\na new approach that leverages speculative decoding, previously viewed primarily\nas a decoding acceleration technique for autoregressive generation of LLMs, as\na promising approach specifically adapted for edge computing by orchestrating\ncomputation across heterogeneous devices. We propose SLED, a method that allows\nlightweight edge devices to draft multiple candidate tokens locally using\ndiverse draft models, while a single, shared edge server efficiently batches\nand verifies the tokens utilizing a more precise target model. This approach\nsupports device heterogeneity and reduces server-side memory footprint by\navoiding the need to deploy multiple target models. Our initial experiments\nwith Jetson Orin Nano, Raspberry Pi 5, and an RTX 6000 edge server indicate\nsubstantial benefits: significantly reduced latency, improved energy\nefficiency, and increased concurrent inference sessions, all without\nsacrificing model accuracy.", "AI": {"tldr": "论文提出SLED方法，通过异构设备协同计算，利用推测解码技术优化边缘设备上的大型语言模型推理，显著降低延迟、提高能效，同时保持模型精度。", "motivation": "尽管设备能力提升，边缘设备上高效推理大型语言模型仍受限于内存和功耗。现有方法（如量化、剪枝或远程推理）需牺牲精度或增加成本。", "method": "SLED方法利用推测解码，边缘设备本地生成候选令牌，边缘服务器批量验证，减少服务器内存占用并支持设备异构性。", "result": "实验表明，SLED显著降低延迟、提高能效，并支持更多并发推理会话，且不影响模型精度。", "conclusion": "SLED为边缘计算中的高效推理提供了一种新方法，平衡了效率与精度。"}}
{"id": "2506.09289", "pdf": "https://arxiv.org/pdf/2506.09289", "abs": "https://arxiv.org/abs/2506.09289", "authors": ["Boxi Yu", "Yuxuan Zhu", "Pinjia He", "Daniel Kang"], "title": "UTBoost: Rigorous Evaluation of Coding Agents on SWE-Bench", "categories": ["cs.SE", "cs.CL", "D.0; I.2"], "comment": null, "summary": "The advent of Large Language Models (LLMs) has spurred the development of\ncoding agents for real-world code generation. As a widely used benchmark for\nevaluating the code generation capabilities of these agents, SWE-Bench uses\nreal-world problems based on GitHub issues and their corresponding pull\nrequests. However, the manually written test cases included in these pull\nrequests are often insufficient, allowing generated patches to pass the tests\nwithout resolving the underlying issue. To address this challenge, we introduce\nUTGenerator, an LLM-driven test case generator that automatically analyzes\ncodebases and dependencies to generate test cases for real-world Python\nprojects. Building on UTGenerator, we propose UTBoost, a comprehensive\nframework for test case augmentation. In our evaluation, we identified 36 task\ninstances with insufficient test cases and uncovered 345 erroneous patches\nincorrectly labeled as passed in the original SWE Bench. These corrections,\nimpacting 40.9% of SWE-Bench Lite and 24.4% of SWE-Bench Verified leaderboard\nentries, yield 18 and 11 ranking changes, respectively.", "AI": {"tldr": "论文提出UTGenerator和UTBoost框架，用于生成和增强测试用例，解决SWE-Bench中测试用例不足的问题，并发现大量错误补丁。", "motivation": "SWE-Bench中的手动编写测试用例不足，导致生成的补丁可能通过测试但未解决问题。", "method": "引入UTGenerator（基于LLM的测试用例生成器）和UTBoost（测试用例增强框架），自动分析代码库和依赖关系生成测试用例。", "result": "在评估中，发现36个任务实例的测试用例不足，并识别出345个错误补丁；修正影响了SWE-Bench Lite和Verified排行榜的40.9%和24.4%条目，分别导致18和11个排名变化。", "conclusion": "UTGenerator和UTBoost能有效提升测试用例质量，显著改善代码生成代理的评估准确性。"}}
{"id": "2506.09836", "pdf": "https://arxiv.org/pdf/2506.09836", "abs": "https://arxiv.org/abs/2506.09836", "authors": ["Junli Deng", "Ping Shi", "Qipei Li", "Jinyang Guo"], "title": "DynaSplat: Dynamic-Static Gaussian Splatting with Hierarchical Motion Decomposition for Scene Reconstruction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Reconstructing intricate, ever-changing environments remains a central\nambition in computer vision, yet existing solutions often crumble before the\ncomplexity of real-world dynamics. We present DynaSplat, an approach that\nextends Gaussian Splatting to dynamic scenes by integrating dynamic-static\nseparation and hierarchical motion modeling. First, we classify scene elements\nas static or dynamic through a novel fusion of deformation offset statistics\nand 2D motion flow consistency, refining our spatial representation to focus\nprecisely where motion matters. We then introduce a hierarchical motion\nmodeling strategy that captures both coarse global transformations and\nfine-grained local movements, enabling accurate handling of intricate,\nnon-rigid motions. Finally, we integrate physically-based opacity estimation to\nensure visually coherent reconstructions, even under challenging occlusions and\nperspective shifts. Extensive experiments on challenging datasets reveal that\nDynaSplat not only surpasses state-of-the-art alternatives in accuracy and\nrealism but also provides a more intuitive, compact, and efficient route to\ndynamic scene reconstruction.", "AI": {"tldr": "DynaSplat通过动态-静态分离和分层运动建模扩展高斯泼溅技术，提升动态场景重建的准确性和效率。", "motivation": "现有方法难以处理真实世界动态场景的复杂性，需要更高效、精确的动态重建技术。", "method": "结合变形偏移统计和2D运动流一致性分类静态与动态元素，采用分层运动建模处理全局与局部运动，并引入基于物理的不透明度估计。", "result": "在复杂数据集上，DynaSplat在准确性和真实感上超越现有技术，且更高效紧凑。", "conclusion": "DynaSplat为动态场景重建提供了更直观、高效且精确的解决方案。"}}
{"id": "2506.09332", "pdf": "https://arxiv.org/pdf/2506.09332", "abs": "https://arxiv.org/abs/2506.09332", "authors": ["Zhenqiao Song", "Ramith Hettiarachchi", "Chuan Li", "Jianwen Xie", "Lei Li"], "title": "Natural Language Guided Ligand-Binding Protein Design", "categories": ["cs.LG", "cs.CE", "cs.CL"], "comment": null, "summary": "Can AI protein models follow human language instructions and design proteins\nwith desired functions (e.g. binding to a ligand)? Designing proteins that bind\nto a given ligand is crucial in a wide range of applications in biology and\nchemistry. Most prior AI models are trained on protein-ligand complex data,\nwhich is scarce due to the high cost and time requirements of laboratory\nexperiments. In contrast, there is a substantial body of human-curated text\ndescriptions about protein-ligand interactions and ligand formula. In this\npaper, we propose InstructPro, a family of protein generative models that\nfollow natural language instructions to design ligand-binding proteins. Given a\ntextual description of the desired function and a ligand formula in SMILES,\nInstructPro generates protein sequences that are functionally consistent with\nthe specified instructions. We develop the model architecture, training\nstrategy, and a large-scale dataset, InstructProBench, to support both training\nand evaluation. InstructProBench consists of 9,592,829 triples of (function\ndescription, ligand formula, protein sequence). We train two model variants:\nInstructPro-1B (with 1 billion parameters) and InstructPro-3B~(with 3 billion\nparameters). Both variants consistently outperform strong baselines, including\nProGen2, ESM3, and Pinal. Notably, InstructPro-1B achieves the highest docking\nsuccess rate (81.52% at moderate confidence) and the lowest average root mean\nsquare deviation (RMSD) compared to ground truth structures (4.026{\\AA}).\nInstructPro-3B further descreases the average RMSD to 2.527{\\AA}, demonstrating\nInstructPro's ability to generate ligand-binding proteins that align with the\nfunctional specifications.", "AI": {"tldr": "InstructPro是一种基于自然语言指令设计蛋白质的生成模型，能够根据功能描述和配体公式生成功能一致的蛋白质序列，性能优于现有基线模型。", "motivation": "设计能够结合特定配体的蛋白质在生物学和化学中具有广泛应用，但现有AI模型依赖稀缺的蛋白质-配体复合物数据。本文利用大量人类整理的文本描述，提出了一种新的解决方案。", "method": "提出InstructPro模型家族，包括InstructPro-1B和InstructPro-3B，通过自然语言指令和配体公式生成蛋白质序列。开发了大规模数据集InstructProBench支持训练和评估。", "result": "InstructPro-1B和InstructPro-3B在对接成功率和RMSD上优于基线模型，其中InstructPro-3B的平均RMSD最低（2.527Å）。", "conclusion": "InstructPro展示了通过自然语言指令设计功能蛋白质的能力，为蛋白质工程提供了新工具。"}}
{"id": "2506.09839", "pdf": "https://arxiv.org/pdf/2506.09839", "abs": "https://arxiv.org/abs/2506.09839", "authors": ["Chen Gao", "Liankai Jin", "Xingyu Peng", "Jiazhao Zhang", "Yue Deng", "Annan Li", "He Wang", "Si Liu"], "title": "OctoNav: Towards Generalist Embodied Navigation", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": "31 pages, 25 figures", "summary": "Embodied navigation stands as a foundation pillar within the broader pursuit\nof embodied AI. However, previous navigation research is divided into different\ntasks/capabilities, e.g., ObjNav, ImgNav and VLN, where they differ in task\nobjectives and modalities, making datasets and methods are designed\nindividually. In this work, we take steps toward generalist navigation agents,\nwhich can follow free-form instructions that include arbitrary compounds of\nmulti-modal and multi-capability. To achieve this, we propose a large-scale\nbenchmark and corresponding method, termed OctoNav-Bench and OctoNav-R1.\nSpecifically, OctoNav-Bench features continuous environments and is constructed\nvia a designed annotation pipeline. We thoroughly craft instruction-trajectory\npairs, where instructions are diverse in free-form with arbitrary modality and\ncapability. Also, we construct a Think-Before-Action (TBA-CoT) dataset within\nOctoNav-Bench to provide the thinking process behind actions. For OctoNav-R1,\nwe build it upon MLLMs and adapt it to a VLA-type model, which can produce\nlow-level actions solely based on 2D visual observations. Moreover, we design a\nHybrid Training Paradigm (HTP) that consists of three stages, i.e.,\nAction-/TBA-SFT, Nav-GPRO, and Online RL stages. Each stage contains\nspecifically designed learning policies and rewards. Importantly, for TBA-SFT\nand Nav-GRPO designs, we are inspired by the OpenAI-o1 and DeepSeek-R1, which\nshow impressive reasoning ability via thinking-before-answer. Thus, we aim to\ninvestigate how to achieve thinking-before-action in the embodied navigation\nfield, to improve model's reasoning ability toward generalists. Specifically,\nwe propose TBA-SFT to utilize the TBA-CoT dataset to fine-tune the model as a\ncold-start phrase and then leverage Nav-GPRO to improve its thinking ability.\nFinally, OctoNav-R1 shows superior performance compared with previous methods.", "AI": {"tldr": "本文提出了一种通用导航代理OctoNav-R1，通过多模态和多能力的自由指令实现导航，并设计了OctoNav-Bench基准和混合训练范式（HTP）。", "motivation": "现有导航研究分散于不同任务/能力（如ObjNav、ImgNav和VLN），缺乏通用性。本文旨在开发能处理自由指令的通用导航代理。", "method": "提出OctoNav-Bench基准，包含连续环境和多样化指令-轨迹对；设计TBA-CoT数据集以提供行动背后的思考过程；构建基于MLLMs的OctoNav-R1模型，采用三阶段HTP训练范式（Action-/TBA-SFT、Nav-GPRO和Online RL）。", "result": "OctoNav-R1在性能上优于现有方法。", "conclusion": "通过TBA-SFT和Nav-GPRO的结合，提升了模型的推理能力，实现了通用导航代理的初步目标。"}}
{"id": "2506.09846", "pdf": "https://arxiv.org/pdf/2506.09846", "abs": "https://arxiv.org/abs/2506.09846", "authors": ["Panagiotis Kaliosis", "John Pavlopoulos"], "title": "Learning to Align: Addressing Character Frequency Distribution Shifts in Handwritten Text Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "17 pages, 10 figures, Under Review", "summary": "Handwritten text recognition aims to convert visual input into\nmachine-readable text, and it remains challenging due to the evolving and\ncontext-dependent nature of handwriting. Character sets change over time, and\ncharacter frequency distributions shift across historical periods or regions,\noften causing models trained on broad, heterogeneous corpora to underperform on\nspecific subsets. To tackle this, we propose a novel loss function that\nincorporates the Wasserstein distance between the character frequency\ndistribution of the predicted text and a target distribution empirically\nderived from training data. By penalizing divergence from expected\ndistributions, our approach enhances both accuracy and robustness under\ntemporal and contextual intra-dataset shifts. Furthermore, we demonstrate that\ncharacter distribution alignment can also improve existing models at inference\ntime without requiring retraining by integrating it as a scoring function in a\nguided decoding scheme. Experimental results across multiple datasets and\narchitectures confirm the effectiveness of our method in boosting\ngeneralization and performance. We open source our code at\nhttps://github.com/pkaliosis/fada.", "AI": {"tldr": "提出了一种基于Wasserstein距离的损失函数，用于提升手写文本识别的准确性和鲁棒性，特别是在处理时间和上下文变化时。", "motivation": "手写文本识别因字符集的动态变化和上下文依赖性而具有挑战性，现有模型在特定子集上表现不佳。", "method": "提出了一种新的损失函数，利用Wasserstein距离对齐预测文本和目标字符频率分布，并通过引导解码方案提升现有模型。", "result": "实验证明该方法在多个数据集和架构上有效提升了泛化能力和性能。", "conclusion": "通过字符分布对齐，显著提升了手写文本识别的准确性和鲁棒性，且无需重新训练即可改进现有模型。"}}
{"id": "2506.09849", "pdf": "https://arxiv.org/pdf/2506.09849", "abs": "https://arxiv.org/abs/2506.09849", "authors": ["Florian Bordes", "Quentin Garrido", "Justine T Kao", "Adina Williams", "Michael Rabbat", "Emmanuel Dupoux"], "title": "IntPhys 2: Benchmarking Intuitive Physics Understanding In Complex Synthetic Environments", "categories": ["cs.CV"], "comment": null, "summary": "We present IntPhys 2, a video benchmark designed to evaluate the intuitive\nphysics understanding of deep learning models. Building on the original IntPhys\nbenchmark, IntPhys 2 focuses on four core principles related to macroscopic\nobjects: Permanence, Immutability, Spatio-Temporal Continuity, and Solidity.\nThese conditions are inspired by research into intuitive physical understanding\nemerging during early childhood. IntPhys 2 offers a comprehensive suite of\ntests, based on the violation of expectation framework, that challenge models\nto differentiate between possible and impossible events within controlled and\ndiverse virtual environments. Alongside the benchmark, we provide performance\nevaluations of several state-of-the-art models. Our findings indicate that\nwhile these models demonstrate basic visual understanding, they face\nsignificant challenges in grasping intuitive physics across the four principles\nin complex scenes, with most models performing at chance levels (50%), in stark\ncontrast to human performance, which achieves near-perfect accuracy. This\nunderscores the gap between current models and human-like intuitive physics\nunderstanding, highlighting the need for advancements in model architectures\nand training methodologies.", "AI": {"tldr": "IntPhys 2是一个视频基准测试，用于评估深度学习模型对直觉物理的理解能力，重点关注四个核心原则。测试表明，现有模型在复杂场景中的表现远低于人类水平。", "motivation": "评估深度学习模型对直觉物理的理解能力，揭示其与人类认知的差距。", "method": "基于违反期望框架设计测试，评估模型在四种物理原则下的表现。", "result": "现有模型在复杂场景中表现接近随机（50%），远低于人类近乎完美的准确率。", "conclusion": "当前模型与人类直觉物理理解存在显著差距，需改进模型架构和训练方法。"}}
{"id": "2506.09448", "pdf": "https://arxiv.org/pdf/2506.09448", "abs": "https://arxiv.org/abs/2506.09448", "authors": ["Yui Sudo", "Yusuke Fujita", "Atsushi Kojima", "Tomoya Mizumoto", "Lianbo Liu"], "title": "OWSM-Biasing: Contextualizing Open Whisper-Style Speech Models for Automatic Speech Recognition with Dynamic Vocabulary", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Speech foundation models (SFMs), such as Open Whisper-Style Speech Models\n(OWSM), are trained on massive datasets to achieve accurate automatic speech\nrecognition. However, even SFMs struggle to accurately recognize rare and\nunseen words. While contextual biasing (CB) is a promising approach to improve\nrecognition of such words, most CB methods are trained from scratch, resulting\nin lower performance than SFMs due to the lack of pre-trained knowledge. This\npaper integrates an existing CB method with OWSM v3.1 while freezing its\npre-trained parameters. By leveraging the knowledge embedded in SFMs, the\nproposed method enables effective CB while preserving the advantages of SFMs,\neven with a small dataset. Experimental results show that the proposed method\nimproves the biasing word error rate (B-WER) by 11.6 points, resulting in a 0.9\npoint improvement in the overall WER while reducing the real-time factor by\n7.5% compared to the non-biasing baseline on the LibriSpeech 100 test-clean\nset.", "AI": {"tldr": "该论文提出了一种结合上下文偏置（CB）方法与预训练语音基础模型（OWSM v3.1）的方法，有效提升了罕见词的识别性能，同时保持了模型的实时性。", "motivation": "尽管语音基础模型（SFMs）在大规模数据集上表现优异，但对罕见词和未见词的识别仍存在困难。现有的上下文偏置方法因缺乏预训练知识而性能较低。", "method": "论文将现有的CB方法与OWSM v3.1结合，冻结预训练参数，利用SFMs的嵌入知识实现有效偏置，即使在小数据集上也能保持优势。", "result": "实验表明，该方法在LibriSpeech 100测试集上，偏置词错误率（B-WER）提升了11.6点，整体WER提升了0.9点，实时因子降低了7.5%。", "conclusion": "通过结合CB与预训练SFMs，该方法显著提升了罕见词识别性能，同时保持了模型的效率和实时性。"}}
{"id": "2506.09881", "pdf": "https://arxiv.org/pdf/2506.09881", "abs": "https://arxiv.org/abs/2506.09881", "authors": ["Siyu Chen", "Ting Han", "Chengzheng Fu", "Changshe Zhang", "Chaolei Wang", "Jinhe Su", "Guorong Cai", "Meiliu Wu"], "title": "Leveraging Depth and Language for Open-Vocabulary Domain-Generalized Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Open-Vocabulary semantic segmentation (OVSS) and domain generalization in\nsemantic segmentation (DGSS) highlight a subtle complementarity that motivates\nOpen-Vocabulary Domain-Generalized Semantic Segmentation (OV-DGSS). OV-DGSS\naims to generate pixel-level masks for unseen categories while maintaining\nrobustness across unseen domains, a critical capability for real-world\nscenarios such as autonomous driving in adverse conditions. We introduce Vireo,\na novel single-stage framework for OV-DGSS that unifies the strengths of OVSS\nand DGSS for the first time. Vireo builds upon the frozen Visual Foundation\nModels (VFMs) and incorporates scene geometry via Depth VFMs to extract\ndomain-invariant structural features. To bridge the gap between visual and\ntextual modalities under domain shift, we propose three key components: (1)\nGeoText Prompts, which align geometric features with language cues and\nprogressively refine VFM encoder representations; (2) Coarse Mask Prior\nEmbedding (CMPE) for enhancing gradient flow for faster convergence and\nstronger textual influence; and (3) the Domain-Open-Vocabulary Vector Embedding\nHead (DOV-VEH), which fuses refined structural and semantic features for robust\nprediction. Comprehensive evaluation on these components demonstrates the\neffectiveness of our designs. Our proposed Vireo achieves the state-of-the-art\nperformance and surpasses existing methods by a large margin in both domain\ngeneralization and open-vocabulary recognition, offering a unified and scalable\nsolution for robust visual understanding in diverse and dynamic environments.\nCode is available at https://github.com/anonymouse-9c53tp182bvz/Vireo.", "AI": {"tldr": "Vireo是一个新颖的单阶段框架，首次将开放词汇语义分割（OVSS）和领域泛化语义分割（DGSS）结合，提出开放词汇领域泛化语义分割（OV-DGSS），旨在为未见类别生成像素级掩码并在未见领域保持鲁棒性。", "motivation": "OVSS和DGSS的互补性启发了OV-DGSS的提出，旨在解决现实场景（如自动驾驶在恶劣条件下）中对未见类别和领域的鲁棒性需求。", "method": "Vireo基于冻结的视觉基础模型（VFMs），通过深度VFMs引入场景几何信息提取领域不变特征，并提出了GeoText Prompts、CMPE和DOV-VEH三个关键组件。", "result": "Vireo在领域泛化和开放词汇识别上均大幅超越现有方法，实现了最先进的性能。", "conclusion": "Vireo提供了一个统一且可扩展的解决方案，适用于多样化和动态环境中的鲁棒视觉理解。"}}
{"id": "2506.09434", "pdf": "https://arxiv.org/pdf/2506.09434", "abs": "https://arxiv.org/abs/2506.09434", "authors": ["Michael Amir", "Matteo Bettini", "Amanda Prorok"], "title": "When Is Diversity Rewarded in Cooperative Multi-Agent Learning?", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "The success of teams in robotics, nature, and society often depends on the\ndivision of labor among diverse specialists; however, a principled explanation\nfor when such diversity surpasses a homogeneous team is still missing. Focusing\non multi-agent task allocation problems, our goal is to study this question\nfrom the perspective of reward design: what kinds of objectives are best suited\nfor heterogeneous teams? We first consider an instantaneous, non-spatial\nsetting where the global reward is built by two generalized aggregation\noperators: an inner operator that maps the $N$ agents' effort allocations on\nindividual tasks to a task score, and an outer operator that merges the $M$\ntask scores into the global team reward. We prove that the curvature of these\noperators determines whether heterogeneity can increase reward, and that for\nbroad reward families this collapses to a simple convexity test. Next, we ask\nwhat incentivizes heterogeneity to emerge when embodied, time-extended agents\nmust learn an effort allocation policy. To study heterogeneity in such\nsettings, we use multi-agent reinforcement learning (MARL) as our computational\nparadigm, and introduce Heterogeneous Environment Design (HED), a\ngradient-based algorithm that optimizes the parameter space of underspecified\nMARL environments to find scenarios where heterogeneity is advantageous.\nExperiments in matrix games and an embodied Multi-Goal-Capture environment show\nthat, despite the difference in settings, HED rediscovers the reward regimes\npredicted by our theory to maximize the advantage of heterogeneity, both\nvalidating HED and connecting our theoretical insights to reward design in\nMARL. Together, these results help us understand when behavioral diversity\ndelivers a measurable benefit.", "AI": {"tldr": "论文研究了多智能体任务分配问题中行为多样性何时优于同质性，通过奖励设计理论和多智能体强化学习（MARL）验证了异质性的优势。", "motivation": "团队在机器人、自然和社会中的成功常依赖于多样化的分工，但缺乏对异质性何时优于同质性的理论解释。", "method": "提出广义聚合算子分析奖励设计，并开发了Heterogeneous Environment Design (HED)算法，通过MARL验证异质性的优势。", "result": "理论证明算子曲率决定异质性是否增加奖励，HED算法在实验中验证了理论预测的奖励机制。", "conclusion": "研究揭示了行为多样性带来显著收益的条件，为奖励设计提供了理论支持。"}}
{"id": "2506.09452", "pdf": "https://arxiv.org/pdf/2506.09452", "abs": "https://arxiv.org/abs/2506.09452", "authors": ["Jay Roberts", "Kyle Mylonakis", "Sidhartha Roy", "Kaan Kale"], "title": "Learning Obfuscations Of LLM Embedding Sequences: Stained Glass Transform", "categories": ["cs.LG", "cs.CL", "cs.CR", "cs.IT", "math.IT", "I.2.7; I.2.m"], "comment": "Submitted to IEEE S&P 2026", "summary": "The high cost of ownership of AI compute infrastructure and challenges of\nrobust serving of large language models (LLMs) has led to a surge in managed\nModel-as-a-service deployments. Even when enterprises choose on-premises\ndeployments, the compute infrastructure is typically shared across many teams\nin order to maximize the return on investment. In both scenarios the deployed\nmodels operate only on plaintext data, and so enterprise data owners must allow\ntheir data to appear in plaintext on a shared or multi-tenant compute\ninfrastructure. This results in data owners with private or sensitive data\nbeing hesitant or restricted in what data they use with these types of\ndeployments. In this work we introduce the Stained Glass Transform, a learned,\nstochastic, and sequence dependent transformation of the word embeddings of an\nLLM which information theoretically provides privacy to the input of the LLM\nwhile preserving the utility of model. We theoretically connect a particular\nclass of Stained Glass Transforms to the theory of mutual information of\nGaussian Mixture Models. We then calculate a-postiori privacy estimates, based\non mutual information, and verify the privacy and utility of instances of\ntransformed embeddings through token level metrics of privacy and standard LLM\nperformance benchmarks.", "AI": {"tldr": "论文提出了一种名为Stained Glass Transform的方法，通过随机变换LLM的词嵌入，在保护输入数据隐私的同时保持模型性能。", "motivation": "由于AI计算基础设施的高成本和共享环境中的数据隐私问题，企业对于敏感数据的使用存在顾虑。", "method": "引入Stained Glass Transform，一种学习到的、随机且序列相关的词嵌入变换方法，基于高斯混合模型的互信息理论。", "result": "通过理论分析和实验验证，证明了该方法在隐私保护和模型性能上的有效性。", "conclusion": "Stained Glass Transform为共享或多租户计算环境中的隐私保护提供了一种可行解决方案。"}}
{"id": "2506.09883", "pdf": "https://arxiv.org/pdf/2506.09883", "abs": "https://arxiv.org/abs/2506.09883", "authors": ["Seonho Lee", "Jiho Choi", "Inha Kang", "Jiwook Kim", "Junsung Park", "Hyunjung Shim"], "title": "3D-Aware Vision-Language Models Fine-Tuning with Geometric Distillation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs) have shown remarkable performance on diverse\nvisual and linguistic tasks, yet they remain fundamentally limited in their\nunderstanding of 3D spatial structures. We propose Geometric Distillation, a\nlightweight, annotation-free fine-tuning framework that injects human-inspired\ngeometric cues into pretrained VLMs without modifying their architecture. By\ndistilling (1) sparse correspondences, (2) relative depth relations, and (3)\ndense cost volumes from off-the-shelf 3D foundation models (e.g., MASt3R,\nVGGT), our method shapes representations to be geometry-aware while remaining\ncompatible with natural image-text inputs. Through extensive evaluations on 3D\nvision-language reasoning and 3D perception benchmarks, our method consistently\noutperforms prior approaches, achieving improved 3D spatial reasoning with\nsignificantly lower computational cost. Our work demonstrates a scalable and\nefficient path to bridge 2D-trained VLMs with 3D understanding, opening up\nwider use in spatially grounded multimodal tasks.", "AI": {"tldr": "本文提出了一种轻量级、无需标注的微调框架Geometric Distillation，通过注入几何线索提升预训练视觉语言模型（VLMs）的3D空间理解能力。", "motivation": "现有的视觉语言模型在3D空间结构理解上存在局限，需要一种高效的方法来增强其几何感知能力。", "method": "通过从现成的3D基础模型（如MASt3R、VGGT）中提取稀疏对应、相对深度关系和密集成本体积，将这些几何线索注入预训练VLMs中。", "result": "在3D视觉语言推理和3D感知基准测试中，该方法显著优于现有方法，且计算成本更低。", "conclusion": "Geometric Distillation为2D训练的VLMs提供了一种可扩展且高效的3D理解路径，适用于空间多模态任务。"}}
{"id": "2506.09521", "pdf": "https://arxiv.org/pdf/2506.09521", "abs": "https://arxiv.org/abs/2506.09521", "authors": ["Ünal Ege Gaznepoglu", "Anna Leschanowsky", "Ahmad Aloradi", "Prachi Singh", "Daniel Tenbrinck", "Emanuël A. P. Habets", "Nils Peters"], "title": "You Are What You Say: Exploiting Linguistic Content for VoicePrivacy Attacks", "categories": ["eess.AS", "cs.CL"], "comment": "5 pages, 6 figures, 1 table, accepted at INTERSPEECH 2025", "summary": "Speaker anonymization systems hide the identity of speakers while preserving\nother information such as linguistic content and emotions. To evaluate their\nprivacy benefits, attacks in the form of automatic speaker verification (ASV)\nsystems are employed. In this study, we assess the impact of intra-speaker\nlinguistic content similarity in the attacker training and evaluation datasets,\nby adapting BERT, a language model, as an ASV system. On the VoicePrivacy\nAttacker Challenge datasets, our method achieves a mean equal error rate (EER)\nof 35%, with certain speakers attaining EERs as low as 2%, based solely on the\ntextual content of their utterances. Our explainability study reveals that the\nsystem decisions are linked to semantically similar keywords within utterances,\nstemming from how LibriSpeech is curated. Our study suggests reworking the\nVoicePrivacy datasets to ensure a fair and unbiased evaluation and challenge\nthe reliance on global EER for privacy evaluations.", "AI": {"tldr": "该研究评估了说话人匿名化系统的隐私保护效果，通过使用BERT作为自动说话人验证（ASV）系统，发现基于文本内容的攻击在某些情况下能显著降低错误率，并揭示了数据集中的语义偏差问题。", "motivation": "评估说话人匿名化系统的隐私保护效果，并探讨攻击者在训练和评估数据集中语言内容相似性的影响。", "method": "使用BERT作为ASV系统，基于文本内容进行说话人验证，并在VoicePrivacy Attacker Challenge数据集上进行测试。", "result": "平均等错误率（EER）为35%，某些说话人的EER低至2%。研究发现系统决策与语义相似的关键词相关，揭示了数据集的偏差问题。", "conclusion": "建议重新设计VoicePrivacy数据集以确保公平评估，并质疑依赖全局EER进行隐私评估的合理性。"}}
{"id": "2506.09885", "pdf": "https://arxiv.org/pdf/2506.09885", "abs": "https://arxiv.org/abs/2506.09885", "authors": ["Haoru Wang", "Kai Ye", "Yangyan Li", "Wenzheng Chen", "Baoquan Chen"], "title": "The Less You Depend, The More You Learn: Synthesizing Novel Views from Sparse, Unposed Images without Any 3D Knowledge", "categories": ["cs.CV"], "comment": null, "summary": "We consider the problem of generalizable novel view synthesis (NVS), which\naims to generate photorealistic novel views from sparse or even unposed 2D\nimages without per-scene optimization. This task remains fundamentally\nchallenging, as it requires inferring 3D structure from incomplete and\nambiguous 2D observations. Early approaches typically rely on strong 3D\nknowledge, including architectural 3D inductive biases (e.g., embedding\nexplicit 3D representations, such as NeRF or 3DGS, into network design) and\nground-truth camera poses for both input and target views. While recent efforts\nhave sought to reduce the 3D inductive bias or the dependence on known camera\nposes of input views, critical questions regarding the role of 3D knowledge and\nthe necessity of circumventing its use remain under-explored. In this work, we\nconduct a systematic analysis on the 3D knowledge and uncover a critical trend:\nthe performance of methods that requires less 3D knowledge accelerates more as\ndata scales, eventually achieving performance on par with their 3D\nknowledge-driven counterparts, which highlights the increasing importance of\nreducing dependence on 3D knowledge in the era of large-scale data. Motivated\nby and following this trend, we propose a novel NVS framework that minimizes 3D\ninductive bias and pose dependence for both input and target views. By\neliminating this 3D knowledge, our method fully leverages data scaling and\nlearns implicit 3D awareness directly from sparse 2D images, without any 3D\ninductive bias or pose annotation during training. Extensive experiments\ndemonstrate that our model generates photorealistic and 3D-consistent novel\nviews, achieving even comparable performance with methods that rely on posed\ninputs, thereby validating the feasibility and effectiveness of our\ndata-centric paradigm. Project page:\nhttps://pku-vcl-geometry.github.io/Less3Depend/ .", "AI": {"tldr": "论文提出了一种减少对3D先验知识和相机位姿依赖的新颖视角合成方法，通过数据驱动的方式直接从稀疏2D图像中学习隐式3D感知，实现了与依赖3D知识的方法相当的性能。", "motivation": "研究动机是探索3D知识在新颖视角合成中的作用，并发现减少对3D知识的依赖在大规模数据下性能提升更快，从而提出一种更依赖数据而非3D先验的方法。", "method": "提出了一种新颖视角合成框架，最小化对3D归纳偏置和相机位姿的依赖，直接从稀疏2D图像中学习隐式3D感知，无需训练时的3D偏置或位姿标注。", "result": "实验表明，该方法能够生成逼真且3D一致的新视角，性能与依赖位姿输入的方法相当，验证了其数据驱动范式的可行性和有效性。", "conclusion": "结论表明，减少对3D知识的依赖在大规模数据下是可行的，且性能可媲美传统方法，为新颖视角合成提供了一种更灵活的数据驱动解决方案。"}}
{"id": "2506.09895", "pdf": "https://arxiv.org/pdf/2506.09895", "abs": "https://arxiv.org/abs/2506.09895", "authors": ["Athinoulla Konstantinou", "Georgios Leontidis", "Mamatha Thota", "Aiden Durrant"], "title": "EquiCaps: Predictor-Free Pose-Aware Pre-Trained Capsule Networks", "categories": ["cs.CV"], "comment": "19 pages, 11 Figures, 13 Tables", "summary": "Learning self-supervised representations that are invariant and equivariant\nto transformations is crucial for advancing beyond traditional visual\nclassification tasks. However, many methods rely on predictor architectures to\nencode equivariance, despite evidence that architectural choices, such as\ncapsule networks, inherently excel at learning interpretable pose-aware\nrepresentations. To explore this, we introduce EquiCaps (Equivariant Capsule\nNetwork), a capsule-based approach to pose-aware self-supervision that\neliminates the need for a specialised predictor for enforcing equivariance.\nInstead, we leverage the intrinsic pose-awareness capabilities of capsules to\nimprove performance in pose estimation tasks. To further challenge our\nassumptions, we increase task complexity via multi-geometric transformations to\nenable a more thorough evaluation of invariance and equivariance by introducing\n3DIEBench-T, an extension of a 3D object-rendering benchmark dataset. Empirical\nresults demonstrate that EquiCaps outperforms prior state-of-the-art\nequivariant methods on rotation prediction, achieving a supervised-level $R^2$\nof 0.78 on the 3DIEBench rotation prediction benchmark and improving upon SIE\nand CapsIE by 0.05 and 0.04 $R^2$, respectively. Moreover, in contrast to\nnon-capsule-based equivariant approaches, EquiCaps maintains robust equivariant\nperformance under combined geometric transformations, underscoring its\ngeneralisation capabilities and the promise of predictor-free capsule\narchitectures.", "AI": {"tldr": "论文提出EquiCaps，一种基于胶囊网络的自我监督方法，无需专用预测器即可实现姿态感知，并在姿态估计任务中表现优异。", "motivation": "探索如何利用胶囊网络固有的姿态感知能力，避免依赖专用预测器来实现等变性，从而提升姿态估计任务的性能。", "method": "引入EquiCaps，利用胶囊网络的固有特性实现姿态感知自我监督，并通过多几何变换任务和3DIEBench-T数据集验证其性能。", "result": "EquiCaps在旋转预测任务中优于现有方法，R²达到0.78，并在复杂几何变换下保持鲁棒性。", "conclusion": "EquiCaps展示了胶囊网络在无预测器架构中的潜力，为姿态感知任务提供了高效且通用的解决方案。"}}
{"id": "2506.09532", "pdf": "https://arxiv.org/pdf/2506.09532", "abs": "https://arxiv.org/abs/2506.09532", "authors": ["Shuai Wang", "Zhenhua Liu", "Jiaheng Wei", "Xuanwu Yin", "Dong Li", "Emad Barsoum"], "title": "Athena: Enhancing Multimodal Reasoning with Data-efficient Process Reward Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "We present Athena-PRM, a multimodal process reward model (PRM) designed to\nevaluate the reward score for each step in solving complex reasoning problems.\nDeveloping high-performance PRMs typically demands significant time and\nfinancial investment, primarily due to the necessity for step-level annotations\nof reasoning steps. Conventional automated labeling methods, such as Monte\nCarlo estimation, often produce noisy labels and incur substantial\ncomputational costs. To efficiently generate high-quality process-labeled data,\nwe propose leveraging prediction consistency between weak and strong completers\nas a criterion for identifying reliable process labels. Remarkably, Athena-PRM\ndemonstrates outstanding effectiveness across various scenarios and benchmarks\nwith just 5,000 samples. Furthermore, we also develop two effective strategies\nto improve the performance of PRMs: ORM initialization and up-sampling for\nnegative data. We validate our approach in three specific scenarios:\nverification for test time scaling, direct evaluation of reasoning step\ncorrectness, and reward ranked fine-tuning. Our Athena-PRM consistently\nachieves superior performance across multiple benchmarks and scenarios.\nNotably, when using Qwen2.5-VL-7B as the policy model, Athena-PRM enhances\nperformance by 10.2 points on WeMath and 7.1 points on MathVista for test time\nscaling. Furthermore, Athena-PRM sets the state-of-the-art (SoTA) results in\nVisualProcessBench and outperforms the previous SoTA by 3.9 F1-score,\nshowcasing its robust capability to accurately assess the correctness of the\nreasoning step. Additionally, utilizing Athena-PRM as the reward model, we\ndevelop Athena-7B with reward ranked fine-tuning and outperforms baseline with\na significant margin on five benchmarks.", "AI": {"tldr": "Athena-PRM是一种多模态过程奖励模型，用于评估复杂推理问题中每一步的奖励分数。它通过弱和强完成者之间的一致性生成高质量标签，仅需5000样本即可高效工作，并在多个场景和基准测试中表现优异。", "motivation": "开发高性能过程奖励模型通常需要大量时间和资金投入，传统自动标注方法（如蒙特卡洛估计）产生噪声标签且计算成本高。因此，需要一种高效生成高质量过程标签的方法。", "method": "利用弱和强完成者之间预测一致性作为可靠过程标签的准则，并采用ORM初始化和负数据上采样策略提升性能。", "result": "Athena-PRM在多个基准测试中表现优异，如在WeMath和MathVista上分别提升10.2和7.1分，并在VisualProcessBench上超越之前SoTA 3.9 F1分数。", "conclusion": "Athena-PRM能高效生成高质量过程标签，显著提升推理步骤评估和奖励排名微调的性能，为复杂推理问题提供了一种强大的解决方案。"}}
{"id": "2506.09897", "pdf": "https://arxiv.org/pdf/2506.09897", "abs": "https://arxiv.org/abs/2506.09897", "authors": ["Tao Liu", "Zhenchao Cui"], "title": "CEM-FBGTinyDet: Context-Enhanced Foreground Balance with Gradient Tuning for tiny Objects", "categories": ["cs.CV"], "comment": null, "summary": "Tiny object detection (TOD) reveals a fundamental flaw in feature pyramid\nnetworks: high-level features (P5-P6) frequently receive zero positive anchors\nunder standard label assignment protocols, leaving their semantic\nrepresentations untrained due to exclusion from loss computation. This creates\ndual deficiencies: (1) Stranded high-level features become semantic dead-ends\nwithout gradient updates, while (2) low-level features lack essential semantic\ncontext for robust classification. We propose E-FPN-BS that systematically\nconverts wasted high-level semantics into low-level feature enhancements. To\naddress these issues, we propose E-FPN-BS, a novel architecture integrating\nmulti-scale feature enhancement and adaptive optimization. First, our Context\nEnhancement Module(CEM) employs dual-branch processing to align and compress\nhigh-level features for effective global-local fusion. Second, the\nForeground-Background Separation Module (FBSM) generates spatial gating masks\nthat dynamically amplify discriminative regions. To address gradient imbalance\nacross object scales, we further propose a Dynamic Gradient-Balanced Loss\n(DCLoss) that automatically modulates loss contributions via scale-aware\ngradient equilibrium. Extensive experiments across multiple benchmark datasets\ndemonstrate the outstanding performance and generalization ability of our\napproach.", "AI": {"tldr": "论文提出E-FPN-BS架构，通过多尺度特征增强和自适应优化解决小目标检测中高、低层级特征训练不足的问题。", "motivation": "传统特征金字塔网络（FPN）在小目标检测中存在高层级特征（P5-P6）因零正样本锚点而未被训练的问题，导致语义表示缺失。", "method": "提出E-FPN-BS架构，包括上下文增强模块（CEM）和前景-背景分离模块（FBSM），并引入动态梯度平衡损失（DCLoss）优化梯度分配。", "result": "在多个基准数据集上的实验表明，该方法具有优异的性能和泛化能力。", "conclusion": "E-FPN-BS有效解决了小目标检测中特征训练的不足，提升了检测性能。"}}
{"id": "2506.09455", "pdf": "https://arxiv.org/pdf/2506.09455", "abs": "https://arxiv.org/abs/2506.09455", "authors": ["Yizhak Yisrael Elboher", "Omri Isac", "Guy Katz", "Tobias Ladner", "Haoze Wu"], "title": "Abstraction-Based Proof Production in Formal Verification of Neural Networks", "categories": ["cs.LO", "cs.AI"], "comment": "To appear in SAIV 2025", "summary": "Modern verification tools for deep neural networks (DNNs) increasingly rely\non abstraction to scale to realistic architectures. In parallel, proof\nproduction is becoming a critical requirement for increasing the reliability of\nDNN verification results. However, current proofproducing verifiers do not\nsupport abstraction-based reasoning, creating a gap between scalability and\nprovable guarantees. We address this gap by introducing a novel framework for\nproof-producing abstraction-based DNN verification. Our approach modularly\nseparates the verification task into two components: (i) proving the\ncorrectness of an abstract network, and (ii) proving the soundness of the\nabstraction with respect to the original DNN. The former can be handled by\nexisting proof-producing verifiers, whereas we propose the first method for\ngenerating formal proofs for the latter. This preliminary work aims to enable\nscalable and trustworthy verification by supporting common abstraction\ntechniques within a formal proof framework.", "AI": {"tldr": "论文提出了一种新的框架，用于支持基于抽象的深度神经网络（DNN）验证的证明生成，填补了当前可扩展性与可证明保证之间的空白。", "motivation": "当前的证明生成验证工具不支持基于抽象的推理，导致可扩展性与可证明保证之间存在差距。", "method": "将验证任务模块化为两部分：证明抽象网络的正确性，以及证明抽象相对于原始DNN的可靠性。前者由现有工具处理，后者则首次提出生成形式化证明的方法。", "result": "初步工作旨在通过支持形式化证明框架中的常见抽象技术，实现可扩展且可信的验证。", "conclusion": "该框架为结合抽象与证明生成的DNN验证提供了可行路径，有望提升验证的可扩展性和可靠性。"}}
{"id": "2506.09600", "pdf": "https://arxiv.org/pdf/2506.09600", "abs": "https://arxiv.org/abs/2506.09600", "authors": ["Itay Nakash", "George Kour", "Koren Lazar", "Matan Vetzler", "Guy Uziel", "Ateret Anaby-Tavor"], "title": "Effective Red-Teaming of Policy-Adherent Agents", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.CR"], "comment": null, "summary": "Task-oriented LLM-based agents are increasingly used in domains with strict\npolicies, such as refund eligibility or cancellation rules. The challenge lies\nin ensuring that the agent consistently adheres to these rules and policies,\nappropriately refusing any request that would violate them, while still\nmaintaining a helpful and natural interaction. This calls for the development\nof tailored design and evaluation methodologies to ensure agent resilience\nagainst malicious user behavior. We propose a novel threat model that focuses\non adversarial users aiming to exploit policy-adherent agents for personal\nbenefit. To address this, we present CRAFT, a multi-agent red-teaming system\nthat leverages policy-aware persuasive strategies to undermine a\npolicy-adherent agent in a customer-service scenario, outperforming\nconventional jailbreak methods such as DAN prompts, emotional manipulation, and\ncoercive. Building upon the existing tau-bench benchmark, we introduce\ntau-break, a complementary benchmark designed to rigorously assess the agent's\nrobustness against manipulative user behavior. Finally, we evaluate several\nstraightforward yet effective defense strategies. While these measures provide\nsome protection, they fall short, highlighting the need for stronger,\nresearch-driven safeguards to protect policy-adherent agents from adversarial\nattacks", "AI": {"tldr": "论文提出了一种针对任务导向型LLM代理的威胁模型，并开发了CRAFT多代理红队系统以测试代理的鲁棒性，同时引入tau-break基准和防御策略。", "motivation": "在严格政策领域（如退款或取消规则）中，确保LLM代理始终遵守政策并拒绝违规请求，同时保持自然交互，是一个挑战。", "method": "提出CRAFT多代理红队系统，利用策略感知的劝说策略测试代理的鲁棒性，并引入tau-break基准。", "result": "CRAFT在客户服务场景中优于传统越狱方法，但现有防御策略仍不足。", "conclusion": "需要更强研究驱动的保护措施以抵御对抗性攻击。"}}
{"id": "2506.09916", "pdf": "https://arxiv.org/pdf/2506.09916", "abs": "https://arxiv.org/abs/2506.09916", "authors": ["Tilemachos Aravanis", "Panagiotis Filntisis", "Petros Maragos", "George Retsinas"], "title": "Only-Style: Stylistic Consistency in Image Generation without Content Leakage", "categories": ["cs.CV"], "comment": null, "summary": "Generating images in a consistent reference visual style remains a\nchallenging computer vision task. State-of-the-art methods aiming for\nstyle-consistent generation struggle to effectively separate semantic content\nfrom stylistic elements, leading to content leakage from the image provided as\na reference to the targets. To address this challenge, we propose Only-Style: a\nmethod designed to mitigate content leakage in a semantically coherent manner\nwhile preserving stylistic consistency. Only-Style works by localizing content\nleakage during inference, allowing the adaptive tuning of a parameter that\ncontrols the style alignment process, specifically within the image patches\ncontaining the subject in the reference image. This adaptive process best\nbalances stylistic consistency with leakage elimination. Moreover, the\nlocalization of content leakage can function as a standalone component, given a\nreference-target image pair, allowing the adaptive tuning of any\nmethod-specific parameter that provides control over the impact of the\nstylistic reference. In addition, we propose a novel evaluation framework to\nquantify the success of style-consistent generations in avoiding undesired\ncontent leakage. Our approach demonstrates a significant improvement over\nstate-of-the-art methods through extensive evaluation across diverse instances,\nconsistently achieving robust stylistic consistency without undesired content\nleakage.", "AI": {"tldr": "论文提出了一种名为Only-Style的方法，旨在解决图像生成中风格一致性问题，通过自适应调整参数减少内容泄漏，并提出了新的评估框架。", "motivation": "生成具有一致视觉风格的图像是一个挑战，现有方法难以有效分离语义内容和风格元素，导致内容泄漏。", "method": "Only-Style通过定位内容泄漏并自适应调整风格对齐参数，平衡风格一致性和泄漏消除。", "result": "实验表明，该方法在多种情况下显著优于现有方法，实现了无内容泄漏的稳健风格一致性。", "conclusion": "Only-Style有效解决了风格一致性问题，并提供了可扩展的评估框架。"}}
{"id": "2506.09485", "pdf": "https://arxiv.org/pdf/2506.09485", "abs": "https://arxiv.org/abs/2506.09485", "authors": ["Yuxin Liu", "Zhenghao Peng", "Xuanhao Cui", "Bolei Zhou"], "title": "Adv-BMT: Bidirectional Motion Transformer for Safety-Critical Traffic Scenario Generation", "categories": ["cs.RO", "cs.AI", "cs.GR"], "comment": null, "summary": "Scenario-based testing is essential for validating the performance of\nautonomous driving (AD) systems. However, such testing is limited by the\nscarcity of long-tailed, safety-critical scenarios in existing datasets\ncollected in the real world. To tackle the data issue, we propose the Adv-BMT\nframework, which augments real-world scenarios with diverse and realistic\nadversarial interactions. The core component of Adv-BMT is a bidirectional\nmotion transformer (BMT) model to perform inverse traffic motion predictions,\nwhich takes agent information in the last time step of the scenario as input,\nand reconstruct the traffic in the inverse of chronological order until the\ninitial time step. The Adv-BMT framework is a two-staged pipeline: it first\nconducts adversarial initializations and then inverse motion predictions.\nDifferent from previous work, we do not need any collision data for\npretraining, and are able to generate realistic and diverse collision\ninteractions. Our experimental results validate the quality of generated\ncollision scenarios by Adv-BMT: training in our augmented dataset would reduce\nepisode collision rates by 20\\% compared to previous work.", "AI": {"tldr": "Adv-BMT框架通过双向运动变换器生成多样且真实的对抗性交互，增强自动驾驶测试数据，无需预训练碰撞数据即可生成高质量碰撞场景。", "motivation": "现有数据集缺乏长尾、安全关键场景，限制了自动驾驶系统的测试效果。", "method": "Adv-BMT框架分两阶段：对抗性初始化和逆向运动预测，利用双向运动变换器模型逆向重建交通流。", "result": "实验表明，Adv-BMT生成的碰撞场景质量高，训练后碰撞率降低20%。", "conclusion": "Adv-BMT有效解决了数据稀缺问题，提升了自动驾驶测试的覆盖率和安全性。"}}
{"id": "2506.09919", "pdf": "https://arxiv.org/pdf/2506.09919", "abs": "https://arxiv.org/abs/2506.09919", "authors": ["He Zhang", "Chentao Song", "Hongwen Zhang", "Tao Yu"], "title": "MetricHMR: Metric Human Mesh Recovery from Monocular Images", "categories": ["cs.CV"], "comment": null, "summary": "We introduce MetricHMR (Metric Human Mesh Recovery), an approach for metric\nhuman mesh recovery with accurate global translation from monocular images. In\ncontrast to existing HMR methods that suffer from severe scale and depth\nambiguity, MetricHMR is able to produce geometrically reasonable body shape and\nglobal translation in the reconstruction results. To this end, we first\nsystematically analyze previous HMR methods on camera models to emphasize the\ncritical role of the standard perspective projection model in enabling\nmetric-scale HMR. We then validate the acceptable ambiguity range of metric HMR\nunder the standard perspective projection model. Finally, we contribute a novel\napproach that introduces a ray map based on the standard perspective projection\nto jointly encode bounding-box information, camera parameters, and geometric\ncues for End2End metric HMR without any additional metric-regularization\nmodules. Extensive experiments demonstrate that our method achieves\nstate-of-the-art performance, even compared with sequential HMR methods, in\nmetric pose, shape, and global translation estimation across both indoor and\nin-the-wild scenarios.", "AI": {"tldr": "MetricHMR是一种从单目图像中恢复具有精确全局平移的度量人体网格的方法，解决了现有HMR方法在尺度和深度上的模糊性问题。", "motivation": "现有HMR方法存在严重的尺度和深度模糊性，无法生成几何合理的身体形状和全局平移。", "method": "通过系统分析现有HMR方法的相机模型，强调标准透视投影模型的关键作用，并提出一种基于射线图的新方法，联合编码边界框信息、相机参数和几何线索。", "result": "实验表明，MetricHMR在度量姿态、形状和全局平移估计上达到最先进性能，优于顺序HMR方法。", "conclusion": "MetricHMR通过标准透视投影模型和射线图方法，成功实现了度量尺度的人体网格恢复，解决了尺度和深度模糊性问题。"}}
{"id": "2506.09487", "pdf": "https://arxiv.org/pdf/2506.09487", "abs": "https://arxiv.org/abs/2506.09487", "authors": ["Taesoo Park", "Mungwi Jeong", "Mingyu Park", "Narae Kim", "Junyoung Kim", "Mujung Kim", "Jisang Yoo", "Hoyun Lee", "Sanghoon Kim", "Soonchul Kwon"], "title": "BemaGANv2: A Tutorial and Comparative Survey of GAN-based Vocoders for Long-Term Audio Generation", "categories": ["cs.SD", "cs.AI", "cs.LG", "cs.LO", "eess.AS", "I.2.6; H.5.5; I.5.1"], "comment": "11 pages, 7 figures. Survey and tutorial paper. Currently under\n  review at ICT Express as an extended version of our ICAIIC 2025 paper", "summary": "This paper presents a tutorial-style survey and implementation guide of\nBemaGANv2, an advanced GAN-based vocoder designed for high-fidelity and\nlong-term audio generation. Built upon the original BemaGAN architecture,\nBemaGANv2 incorporates major architectural innovations by replacing traditional\nResBlocks in the generator with the Anti-aliased Multi-Periodicity composition\n(AMP) module, which internally applies the Snake activation function to better\nmodel periodic structures. In the discriminator framework, we integrate the\nMulti-Envelope Discriminator (MED), a novel architecture we originally\nproposed, to extract rich temporal envelope features crucial for periodicity\ndetection. Coupled with the Multi-Resolution Discriminator (MRD), this\ncombination enables more accurate modeling of long-range dependencies in audio.\nWe systematically evaluate various discriminator configurations, including MSD\n+ MED, MSD + MRD, and MPD + MED + MRD, using objective metrics (FAD, SSIM,\nPLCC, MCD) and subjective evaluations (MOS, SMOS). This paper also provides a\ncomprehensive tutorial on the model architecture, training methodology, and\nimplementation to promote reproducibility. The code and pre-trained models are\navailable at: https://github.com/dinhoitt/BemaGANv2.", "AI": {"tldr": "BemaGANv2是一种基于GAN的高保真音频生成器，通过改进生成器和判别器架构（如AMP模块和MED），提升了长期音频生成的质量。", "motivation": "改进原始BemaGAN架构，以更好地建模周期性结构和长期依赖关系，实现高保真音频生成。", "method": "生成器使用AMP模块替代传统ResBlocks，判别器结合MED和MRD，系统评估不同配置。", "result": "通过客观和主观评估验证了模型性能，代码和预训练模型已开源。", "conclusion": "BemaGANv2在音频生成任务中表现出色，提供了详细的实现指南以促进复现。"}}
{"id": "2506.09920", "pdf": "https://arxiv.org/pdf/2506.09920", "abs": "https://arxiv.org/abs/2506.09920", "authors": ["Jianhan Qi", "Yuheng Jia", "Hui Liu", "Junhui Hou"], "title": "Structural-Spectral Graph Convolution with Evidential Edge Learning for Hyperspectral Image Clustering", "categories": ["cs.CV"], "comment": null, "summary": "Hyperspectral image (HSI) clustering assigns similar pixels to the same class\nwithout any annotations, which is an important yet challenging task. For\nlarge-scale HSIs, most methods rely on superpixel segmentation and perform\nsuperpixel-level clustering based on graph neural networks (GNNs). However,\nexisting GNNs cannot fully exploit the spectral information of the input HSI,\nand the inaccurate superpixel topological graph may lead to the confusion of\ndifferent class semantics during information aggregation. To address these\nchallenges, we first propose a structural-spectral graph convolutional operator\n(SSGCO) tailored for graph-structured HSI superpixels to improve their\nrepresentation quality through the co-extraction of spatial and spectral\nfeatures. Second, we propose an evidence-guided adaptive edge learning (EGAEL)\nmodule that adaptively predicts and refines edge weights in the superpixel\ntopological graph. We integrate the proposed method into a contrastive learning\nframework to achieve clustering, where representation learning and clustering\nare simultaneously conducted. Experiments demonstrate that the proposed method\nimproves clustering accuracy by 2.61%, 6.06%, 4.96% and 3.15% over the best\ncompared methods on four HSI datasets. Our code is available at\nhttps://github.com/jhqi/SSGCO-EGAEL.", "AI": {"tldr": "论文提出了一种针对高光谱图像（HSI）聚类的结构-光谱图卷积算子（SSGCO）和证据引导的自适应边学习（EGAEL）模块，显著提升了聚类精度。", "motivation": "现有基于图神经网络（GNNs）的方法未能充分利用HSI的光谱信息，且超像素拓扑图的不准确性可能导致语义混淆。", "method": "提出SSGCO用于提取空间和光谱特征，以及EGAEL模块自适应优化边权重，结合对比学习框架实现聚类。", "result": "在四个HSI数据集上，聚类精度分别提升了2.61%、6.06%、4.96%和3.15%。", "conclusion": "该方法通过联合优化表示学习和聚类任务，显著提升了HSI聚类的性能。"}}
{"id": "2506.09496", "pdf": "https://arxiv.org/pdf/2506.09496", "abs": "https://arxiv.org/abs/2506.09496", "authors": ["Dingyi Rong", "Haotian Lu", "Wenzhuo Zheng", "Fan Zhang", "Shuangjia Zheng", "Ning Liu"], "title": "EnerBridge-DPO: Energy-Guided Protein Inverse Folding with Markov Bridges and Direct Preference Optimization", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Designing protein sequences with optimal energetic stability is a key\nchallenge in protein inverse folding, as current deep learning methods are\nprimarily trained by maximizing sequence recovery rates, often neglecting the\nenergy of the generated sequences. This work aims to overcome this limitation\nby developing a model that directly generates low-energy, stable protein\nsequences. We propose EnerBridge-DPO, a novel inverse folding framework focused\non generating low-energy, high-stability protein sequences. Our core innovation\nlies in: First, integrating Markov Bridges with Direct Preference Optimization\n(DPO), where energy-based preferences are used to fine-tune the Markov Bridge\nmodel. The Markov Bridge initiates optimization from an information-rich prior\nsequence, providing DPO with a pool of structurally plausible sequence\ncandidates. Second, an explicit energy constraint loss is introduced, which\nenhances the energy-driven nature of DPO based on prior sequences, enabling the\nmodel to effectively learn energy representations from a wealth of prior\nknowledge and directly predict sequence energy values, thereby capturing\nquantitative features of the energy landscape. Our evaluations demonstrate that\nEnerBridge-DPO can design protein complex sequences with lower energy while\nmaintaining sequence recovery rates comparable to state-of-the-art models, and\naccurately predicts $\\Delta \\Delta G$ values between various sequences.", "AI": {"tldr": "EnerBridge-DPO是一种新型蛋白质逆折叠框架，通过结合马尔可夫桥和直接偏好优化（DPO），直接生成低能量、高稳定性的蛋白质序列。", "motivation": "当前深度学习方法主要关注序列恢复率，而忽略了生成序列的能量稳定性。本研究旨在克服这一限制。", "method": "整合马尔可夫桥与DPO，引入能量约束损失，利用先验序列优化模型，直接预测序列能量值。", "result": "EnerBridge-DPO能设计出能量更低且序列恢复率与先进模型相当的蛋白质序列，并能准确预测ΔΔG值。", "conclusion": "EnerBridge-DPO在蛋白质逆折叠中实现了能量稳定性和序列恢复率的平衡，具有重要应用价值。"}}
{"id": "2506.09707", "pdf": "https://arxiv.org/pdf/2506.09707", "abs": "https://arxiv.org/abs/2506.09707", "authors": ["Suhas BN", "Andrew M. Sherrill", "Jyoti Alaparthi", "Dominik Mattioli", "Rosa I. Arriaga", "Chris W. Wiese", "Saeed Abdullah"], "title": "Fine-Tuning Large Audio-Language Models with LoRA for Precise Temporal Localization of Prolonged Exposure Therapy Elements", "categories": ["eess.AS", "cs.CL", "cs.HC", "68T07", "I.2.7; I.5.4; H.5.2"], "comment": "5 pages, 2 figures", "summary": "Prolonged Exposure (PE) therapy is an effective treatment for post-traumatic\nstress disorder (PTSD), but evaluating therapist fidelity remains\nlabor-intensive due to the need for manual review of session recordings. We\npresent a method for the automatic temporal localization of key PE fidelity\nelements -- identifying their start and stop times -- directly from session\naudio and transcripts. Our approach fine-tunes a large pre-trained\naudio-language model, Qwen2-Audio, using Low-Rank Adaptation (LoRA) to process\nfocused 30-second windows of audio-transcript input. Fidelity labels for three\ncore protocol phases -- therapist orientation (P1), imaginal exposure (P2), and\npost-imaginal processing (P3) -- are generated via LLM-based prompting and\nverified by trained raters. The model is trained to predict normalized boundary\noffsets using soft supervision guided by task-specific prompts. On a dataset of\n313 real PE sessions, our best configuration (LoRA rank 8, 30s windows)\nachieves a mean absolute error (MAE) of 5.3 seconds across tasks. We further\nanalyze the effects of window size and LoRA rank, highlighting the importance\nof context granularity and model adaptation. This work introduces a scalable\nframework for fidelity tracking in PE therapy, with potential to support\nclinician training, supervision, and quality assurance.", "AI": {"tldr": "提出了一种基于音频和转录的自动定位方法，用于评估Prolonged Exposure (PE)治疗中的关键元素，减少了人工审核的负担。", "motivation": "手动评估PE治疗师忠诚度耗时且劳动密集，需要自动化解决方案以提高效率。", "method": "通过微调预训练音频-语言模型Qwen2-Audio，结合LoRA技术处理30秒音频-转录窗口，生成治疗阶段的标签。", "result": "在313个真实PE会话数据集上，最佳配置（LoRA rank 8，30秒窗口）的平均绝对误差为5.3秒。", "conclusion": "该方法为PE治疗忠诚度跟踪提供了可扩展框架，有望支持临床培训、监督和质量保证。"}}
{"id": "2506.09932", "pdf": "https://arxiv.org/pdf/2506.09932", "abs": "https://arxiv.org/abs/2506.09932", "authors": ["Marco Federici", "Riccardo Del Chiaro", "Boris van Breugel", "Paul Whatmough", "Markus Nagel"], "title": "HadaNorm: Diffusion Transformer Quantization through Mean-Centered Transformations", "categories": ["cs.CV", "cs.AI"], "comment": "4 Pages, 5 Figures", "summary": "Diffusion models represent the cutting edge in image generation, but their\nhigh memory and computational demands hinder deployment on resource-constrained\ndevices. Post-Training Quantization (PTQ) offers a promising solution by\nreducing the bitwidth of matrix operations. However, standard PTQ methods\nstruggle with outliers, and achieving higher compression often requires\ntransforming model weights and activations before quantization. In this work,\nwe propose HadaNorm, a novel linear transformation that extends existing\napproaches and effectively mitigates outliers by normalizing activations\nfeature channels before applying Hadamard transformations, enabling more\naggressive activation quantization. We demonstrate that HadaNorm consistently\nreduces quantization error across the various components of transformer blocks,\nachieving superior efficiency-performance trade-offs when compared to\nstate-of-the-art methods.", "AI": {"tldr": "HadaNorm是一种新型线性变换方法，通过归一化激活特征通道并结合Hadamard变换，有效减少异常值，从而实现更激进的激活量化，提升扩散模型的量化效果。", "motivation": "扩散模型在图像生成领域表现优异，但其高内存和计算需求限制了在资源受限设备上的部署。后训练量化（PTQ）是一种潜在解决方案，但标准方法难以处理异常值，且高压缩率通常需要对权重和激活进行变换。", "method": "提出HadaNorm方法，通过归一化激活特征通道并应用Hadamard变换，减少异常值影响，从而实现更高效的激活量化。", "result": "HadaNorm在Transformer块各组件中持续减少量化误差，相比现有方法，实现了更优的效率与性能平衡。", "conclusion": "HadaNorm为扩散模型的量化提供了一种高效解决方案，显著提升了在资源受限设备上的部署潜力。"}}
{"id": "2506.09499", "pdf": "https://arxiv.org/pdf/2506.09499", "abs": "https://arxiv.org/abs/2506.09499", "authors": ["Thomas J. Ringstrom", "Paul R. Schrater"], "title": "A Unified Theory of Compositionality, Modularity, and Interpretability in Markov Decision Processes", "categories": ["cs.LG", "cs.AI"], "comment": "12 Pages", "summary": "We introduce Option Kernel Bellman Equations (OKBEs) for a new reward-free\nMarkov Decision Process. Rather than a value function, OKBEs directly construct\nand optimize a predictive map called a state-time option kernel (STOK) to\nmaximize the probability of completing a goal while avoiding constraint\nviolations. STOKs are compositional, modular, and interpretable\ninitiation-to-termination transition kernels for policies in the Options\nFramework of Reinforcement Learning. This means: 1) STOKs can be composed using\nChapman-Kolmogorov equations to make spatiotemporal predictions for multiple\npolicies over long horizons, 2) high-dimensional STOKs can be represented and\ncomputed efficiently in a factorized and reconfigurable form, and 3) STOKs\nrecord the probabilities of semantically interpretable goal-success and\nconstraint-violation events, needed for formal verification. Given a\nhigh-dimensional state-transition model for an intractable planning problem, we\ncan decompose it with local STOKs and goal-conditioned policies that are\naggregated into a factorized goal kernel, making it possible to forward-plan at\nthe level of goals in high-dimensions to solve the problem. These properties\nlead to highly flexible agents that can rapidly synthesize meta-policies, reuse\nplanning representations across many tasks, and justify goals using\nempowerment, an intrinsic motivation function. We argue that\nreward-maximization is in conflict with the properties of compositionality,\nmodularity, and interpretability. Alternatively, OKBEs facilitate these\nproperties to support verifiable long-horizon planning and intrinsic motivation\nthat scales to dynamic high-dimensional world-models.", "AI": {"tldr": "OKBEs引入了一种新的无奖励MDP方法，通过状态-时间选项核（STOK）直接优化目标完成概率，同时避免约束违反。STOK具有组合性、模块化和可解释性，支持长期规划和内在动机。", "motivation": "传统基于奖励最大化的方法在组合性、模块化和可解释性方面存在冲突，OKBEs旨在解决这些问题，支持可验证的长期规划和动态高维世界模型。", "method": "通过构造和优化STOK，利用Chapman-Kolmogorov方程组合多策略预测，分解高维状态转移模型为局部STOK和目标条件策略，形成因子化目标核。", "result": "OKBEs实现了高度灵活的智能体，能够快速合成元策略、跨任务重用规划表示，并通过内在动机函数（如empowerment）验证目标。", "conclusion": "OKBEs通过STOK的组合性和可解释性，为高维动态世界模型中的长期规划和内在动机提供了可扩展的解决方案。"}}
{"id": "2506.09804", "pdf": "https://arxiv.org/pdf/2506.09804", "abs": "https://arxiv.org/abs/2506.09804", "authors": ["Peter Vieting", "Maximilian Kannen", "Benedikt Hilmes", "Ralf Schlüter", "Hermann Ney"], "title": "Regularizing Learnable Feature Extraction for Automatic Speech Recognition", "categories": ["eess.AS", "cs.CL", "cs.LG", "cs.SD"], "comment": "Accepted at Interspeech 2025", "summary": "Neural front-ends are an appealing alternative to traditional, fixed feature\nextraction pipelines for automatic speech recognition (ASR) systems since they\ncan be directly trained to fit the acoustic model. However, their performance\noften falls short compared to classical methods, which we show is largely due\nto their increased susceptibility to overfitting. This work therefore\ninvestigates regularization methods for training ASR models with learnable\nfeature extraction front-ends. First, we examine audio perturbation methods and\nshow that larger relative improvements can be obtained for learnable features.\nAdditionally, we identify two limitations in the standard use of SpecAugment\nfor these front-ends and propose masking in the short time Fourier transform\n(STFT)-domain as a simple but effective modification to address these\nchallenges. Finally, integrating both regularization approaches effectively\ncloses the performance gap between traditional and learnable features.", "AI": {"tldr": "论文研究了可学习特征提取前端在ASR系统中的正则化方法，通过音频扰动和STFT域掩码改进性能，缩小与传统方法的差距。", "motivation": "神经前端在ASR系统中具有潜力，但其性能常因过拟合问题不及传统方法，因此需要研究正则化方法以提升其表现。", "method": "研究了音频扰动方法，并提出在STFT域进行掩码以改进SpecAugment的局限性。", "result": "结合两种正则化方法后，可学习特征与传统特征之间的性能差距显著缩小。", "conclusion": "通过有效的正则化策略，可学习特征前端在ASR系统中的性能可以与传统方法媲美。"}}
{"id": "2506.09935", "pdf": "https://arxiv.org/pdf/2506.09935", "abs": "https://arxiv.org/abs/2506.09935", "authors": ["Jiangyong Huang", "Xiaojian Ma", "Xiongkun Linghu", "Yue Fan", "Junchao He", "Wenxin Tan", "Qing Li", "Song-Chun Zhu", "Yixin Chen", "Baoxiong Jia", "Siyuan Huang"], "title": "LEO-VL: Towards 3D Vision-Language Generalists via Data Scaling with Efficient Representation", "categories": ["cs.CV"], "comment": "Project page: https://leo-vl.github.io", "summary": "Developing 3D-VL generalists capable of understanding 3D scenes and following\nnatural language instructions to perform a wide range of tasks has been a\nlong-standing goal in the 3D-VL community. Despite recent progress, 3D-VL\nmodels still lag behind their 2D counterparts in capability and robustness,\nfalling short of the generalist standard. A key obstacle to developing 3D-VL\ngeneralists lies in data scalability, hindered by the lack of an efficient\nscene representation. We propose LEO-VL, a 3D-VL model built upon condensed\nfeature grid (CFG), an efficient scene representation that bridges 2D\nperception and 3D spatial structure while significantly reducing token\noverhead. This efficiency unlocks large-scale training towards 3D-VL\ngeneralist, for which we curate over 700k high-quality 3D-VL data spanning four\ndomains of real-world indoor scenes and five tasks such as captioning and\ndialogue. LEO-VL achieves state-of-the-art performance on a variety of 3D QA\nbenchmarks, including SQA3D, MSQA, and Beacon3D. Ablation studies confirm the\nefficiency of our representation, the importance of task and scene diversity,\nand the validity of our data curation principle. Furthermore, we introduce\nSceneDPO, a novel post-training objective that enhances the robustness of 3D-VL\nmodels. We hope our findings contribute to the advancement of scalable and\nrobust 3D-VL generalists.", "AI": {"tldr": "LEO-VL提出了一种基于CFG的高效3D场景表示方法，显著减少了计算开销，并通过大规模数据训练实现了3D-VL通用模型的突破。", "motivation": "3D-VL模型在能力和鲁棒性上落后于2D模型，主要障碍是数据可扩展性和缺乏高效场景表示。", "method": "提出CFG（condensed feature grid）作为高效场景表示，结合2D感知和3D空间结构，并引入SceneDPO增强鲁棒性。", "result": "LEO-VL在多个3D QA基准测试中达到SOTA，验证了CFG的高效性和数据多样性重要性。", "conclusion": "LEO-VL为可扩展且鲁棒的3D-VL通用模型提供了新方向，推动了该领域的发展。"}}
{"id": "2506.09851", "pdf": "https://arxiv.org/pdf/2506.09851", "abs": "https://arxiv.org/abs/2506.09851", "authors": ["Md. Yeasin Rahat", "Rajan Das Gupta", "Nur Raisa Rahman", "Sudipto Roy Pritom", "Samiur Rahman Shakir", "Md Imrul Hasan Showmick", "Md. Jakir Hossen"], "title": "Advancing Exchange Rate Forecasting: Leveraging Machine Learning and AI for Enhanced Accuracy in Global Financial Markets", "categories": ["q-fin.ST", "cs.CL", "cs.LG"], "comment": "Accepted in MECON 2025", "summary": "The prediction of foreign exchange rates, such as the US Dollar (USD) to\nBangladeshi Taka (BDT), plays a pivotal role in global financial markets,\ninfluencing trade, investments, and economic stability. This study leverages\nhistorical USD/BDT exchange rate data from 2018 to 2023, sourced from Yahoo\nFinance, to develop advanced machine learning models for accurate forecasting.\nA Long Short-Term Memory (LSTM) neural network is employed, achieving an\nexceptional accuracy of 99.449%, a Root Mean Square Error (RMSE) of 0.9858, and\na test loss of 0.8523, significantly outperforming traditional methods like\nARIMA (RMSE 1.342). Additionally, a Gradient Boosting Classifier (GBC) is\napplied for directional prediction, with backtesting on a $10,000 initial\ncapital revealing a 40.82% profitable trade rate, though resulting in a net\nloss of $20,653.25 over 49 trades. The study analyzes historical trends,\nshowing a decline in BDT/USD rates from 0.012 to 0.009, and incorporates\nnormalized daily returns to capture volatility. These findings highlight the\npotential of deep learning in forex forecasting, offering traders and\npolicymakers robust tools to mitigate risks. Future work could integrate\nsentiment analysis and real-time economic indicators to further enhance model\nadaptability in volatile markets.", "AI": {"tldr": "该研究利用LSTM神经网络预测美元/孟加拉塔卡汇率，准确率达99.449%，显著优于传统方法。GBC模型在方向预测中表现一般，净亏损20,653美元。", "motivation": "外汇汇率预测对全球金融市场至关重要，影响贸易、投资和经济稳定。", "method": "使用2018-2023年历史数据，采用LSTM和GBC模型进行预测，并与ARIMA对比。", "result": "LSTM表现优异（RMSE 0.9858），GBC净亏损。历史数据显示汇率下降趋势。", "conclusion": "深度学习在外汇预测中潜力巨大，未来可结合情感分析和实时经济指标提升模型适应性。"}}
{"id": "2506.09943", "pdf": "https://arxiv.org/pdf/2506.09943", "abs": "https://arxiv.org/abs/2506.09943", "authors": ["Aaron Foss", "Chloe Evans", "Sasha Mitts", "Koustuv Sinha", "Ammar Rizvi", "Justine T. Kao"], "title": "CausalVQA: A Physically Grounded Causal Reasoning Benchmark for Video Models", "categories": ["cs.CV", "cs.AI", "I.2.10; I.4.8"], "comment": "35 pages, 3 figures, Submitted to NeurIPS2025 benchmark track", "summary": "We introduce CausalVQA, a benchmark dataset for video question answering\n(VQA) composed of question-answer pairs that probe models' understanding of\ncausality in the physical world. Existing VQA benchmarks either tend to focus\non surface perceptual understanding of real-world videos, or on narrow physical\nreasoning questions created using simulation environments. CausalVQA fills an\nimportant gap by presenting challenging questions that are grounded in\nreal-world scenarios, while focusing on models' ability to predict the likely\noutcomes of different actions and events through five question types:\ncounterfactual, hypothetical, anticipation, planning and descriptive. We\ndesigned quality control mechanisms that prevent models from exploiting trivial\nshortcuts, requiring models to base their answers on deep visual understanding\ninstead of linguistic cues. We find that current frontier multimodal models\nfall substantially below human performance on the benchmark, especially on\nanticipation and hypothetical questions. This highlights a challenge for\ncurrent systems to leverage spatial-temporal reasoning, understanding of\nphysical principles, and comprehension of possible alternatives to make\naccurate predictions in real-world settings.", "AI": {"tldr": "CausalVQA是一个专注于因果关系的视频问答基准数据集，填补了现有数据集在真实世界因果推理上的空白。", "motivation": "现有VQA数据集多关注表面感知或模拟环境中的物理推理，缺乏对真实世界因果关系的深入探究。", "method": "通过五种问题类型（反事实、假设、预期、规划和描述性）设计数据集，并采用质量控制机制防止模型利用语言捷径。", "result": "当前前沿多模态模型在CausalVQA上的表现显著低于人类，尤其在预期和假设问题上。", "conclusion": "CausalVQA揭示了当前系统在时空推理、物理原理理解和替代方案预测上的不足。"}}
{"id": "2506.09508", "pdf": "https://arxiv.org/pdf/2506.09508", "abs": "https://arxiv.org/abs/2506.09508", "authors": ["Andreas Schlaginhaufen", "Reda Ouhamma", "Maryam Kamgarpour"], "title": "Efficient Preference-Based Reinforcement Learning: Randomized Exploration Meets Experimental Design", "categories": ["cs.LG", "cs.AI", "cs.RO", "stat.ML"], "comment": null, "summary": "We study reinforcement learning from human feedback in general Markov\ndecision processes, where agents learn from trajectory-level preference\ncomparisons. A central challenge in this setting is to design algorithms that\nselect informative preference queries to identify the underlying reward while\nensuring theoretical guarantees. We propose a meta-algorithm based on\nrandomized exploration, which avoids the computational challenges associated\nwith optimistic approaches and remains tractable. We establish both regret and\nlast-iterate guarantees under mild reinforcement learning oracle assumptions.\nTo improve query complexity, we introduce and analyze an improved algorithm\nthat collects batches of trajectory pairs and applies optimal experimental\ndesign to select informative comparison queries. The batch structure also\nenables parallelization of preference queries, which is relevant in practical\ndeployment as feedback can be gathered concurrently. Empirical evaluation\nconfirms that the proposed method is competitive with reward-based\nreinforcement learning while requiring a small number of preference queries.", "AI": {"tldr": "论文研究基于人类反馈的强化学习，提出一种基于随机探索的元算法，解决了乐观方法计算复杂的问题，并通过批量轨迹对优化查询效率。", "motivation": "在马尔可夫决策过程中，如何从轨迹级偏好比较中高效学习奖励函数是一个核心挑战。", "method": "提出基于随机探索的元算法，并引入批量轨迹对和最优实验设计来优化查询效率。", "result": "算法在温和的强化学习假设下具有遗憾和最终迭代保证，且实验表明其与基于奖励的强化学习竞争性。", "conclusion": "该方法在减少偏好查询次数的同时保持了性能，适用于实际部署中的并行反馈收集。"}}
{"id": "2506.09953", "pdf": "https://arxiv.org/pdf/2506.09953", "abs": "https://arxiv.org/abs/2506.09953", "authors": ["Benjamin Reichman", "Constantin Patsch", "Jack Truxal", "Atishay Jain", "Larry Heck"], "title": "Outside Knowledge Conversational Video (OKCV) Dataset -- Dialoguing over Videos", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "In outside knowledge visual question answering (OK-VQA), the model must\nidentify relevant visual information within an image and incorporate external\nknowledge to accurately respond to a question. Extending this task to a\nvisually grounded dialogue setting based on videos, a conversational model must\nboth recognize pertinent visual details over time and answer questions where\nthe required information is not necessarily present in the visual information.\nMoreover, the context of the overall conversation must be considered for the\nsubsequent dialogue. To explore this task, we introduce a dataset comprised of\n$2,017$ videos with $5,986$ human-annotated dialogues consisting of $40,954$\ninterleaved dialogue turns. While the dialogue context is visually grounded in\nspecific video segments, the questions further require external knowledge that\nis not visually present. Thus, the model not only has to identify relevant\nvideo parts but also leverage external knowledge to converse within the\ndialogue. We further provide several baselines evaluated on our dataset and\nshow future challenges associated with this task. The dataset is made publicly\navailable here: https://github.com/c-patsch/OKCV.", "AI": {"tldr": "论文介绍了基于视频的视觉问答任务，要求模型结合视觉信息和外部知识回答问题，并发布了一个包含2017个视频和5986个对话的数据集。", "motivation": "探索在视频对话中结合视觉信息和外部知识回答问题的任务，填补现有研究的空白。", "method": "构建了一个包含2017个视频和5986个对话的数据集，标注了40954个对话轮次，并提供了基线模型。", "result": "数据集公开可用，基线模型展示了任务的挑战性。", "conclusion": "论文提出了新的任务和数据集，为未来研究提供了基础。"}}
{"id": "2506.09952", "pdf": "https://arxiv.org/pdf/2506.09952", "abs": "https://arxiv.org/abs/2506.09952", "authors": ["Ziyi Wang", "Yanran Zhang", "Jie Zhou", "Jiwen Lu"], "title": "UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal Gaussian Splatting", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to CVPR 2025", "summary": "The scale diversity of point cloud data presents significant challenges in\ndeveloping unified representation learning techniques for 3D vision. Currently,\nthere are few unified 3D models, and no existing pre-training method is equally\neffective for both object- and scene-level point clouds. In this paper, we\nintroduce UniPre3D, the first unified pre-training method that can be\nseamlessly applied to point clouds of any scale and 3D models of any\narchitecture. Our approach predicts Gaussian primitives as the pre-training\ntask and employs differentiable Gaussian splatting to render images, enabling\nprecise pixel-level supervision and end-to-end optimization. To further\nregulate the complexity of the pre-training task and direct the model's focus\ntoward geometric structures, we integrate 2D features from pre-trained image\nmodels to incorporate well-established texture knowledge. We validate the\nuniversal effectiveness of our proposed method through extensive experiments\nacross a variety of object- and scene-level tasks, using diverse point cloud\nmodels as backbones. Code is available at https://github.com/wangzy22/UniPre3D.", "AI": {"tldr": "UniPre3D是一种统一预训练方法，适用于任何尺度的点云和任何架构的3D模型，通过高斯基元预测和可微分高斯渲染实现像素级监督和端到端优化。", "motivation": "点云数据的尺度多样性对统一表示学习技术提出了挑战，目前缺乏适用于对象和场景级点云的统一预训练方法。", "method": "提出UniPre3D，预测高斯基元作为预训练任务，使用可微分高斯渲染生成图像，并结合预训练图像模型的2D特征以引入纹理知识。", "result": "在多种对象和场景级任务中验证了方法的通用有效性。", "conclusion": "UniPre3D是首个适用于任何尺度和架构的统一预训练方法，实验证明了其广泛适用性。"}}
{"id": "2506.09998", "pdf": "https://arxiv.org/pdf/2506.09998", "abs": "https://arxiv.org/abs/2506.09998", "authors": ["Tim Z. Xiao", "Johannes Zenn", "Zhen Liu", "Weiyang Liu", "Robert Bamler", "Bernhard Schölkopf"], "title": "Flipping Against All Odds: Reducing LLM Coin Flip Bias via Verbalized Rejection Sampling", "categories": ["cs.LG", "cs.CL"], "comment": "Technical Report v1 (21 pages, 14 figures)", "summary": "Large language models (LLMs) can often accurately describe probability\ndistributions using natural language, yet they still struggle to generate\nfaithful samples from them. This mismatch limits their use in tasks requiring\nreliable stochasticity, such as Monte Carlo methods, agent-based simulations,\nand randomized decision-making. We investigate this gap between knowledge and\nsampling in the context of Bernoulli distributions. We introduce Verbalized\nRejection Sampling (VRS), a natural-language adaptation of classical rejection\nsampling that prompts the LLM to reason about and accept or reject proposed\nsamples. Despite relying on the same Bernoulli mechanism internally, VRS\nsubstantially reduces sampling bias across models. We provide theoretical\nanalysis showing that, under mild assumptions, VRS improves over direct\nsampling, with gains attributable to both the algorithm and prompt design. More\nbroadly, our results show how classical probabilistic tools can be verbalized\nand embedded into LLM workflows to improve reliability, without requiring\naccess to model internals or heavy prompt engineering.", "AI": {"tldr": "论文提出了一种名为Verbalized Rejection Sampling (VRS)的方法，通过自然语言提示改进大语言模型(LLMs)从伯努利分布中生成样本的准确性，减少采样偏差。", "motivation": "LLMs能够准确描述概率分布，但在生成可靠样本时表现不佳，限制了其在需要随机性的任务中的应用。", "method": "引入VRS方法，通过自然语言提示让LLM对样本进行推理并接受或拒绝，改进采样过程。", "result": "VRS显著减少了采样偏差，理论分析表明其在温和假设下优于直接采样。", "conclusion": "研究表明，经典概率工具可以通过自然语言嵌入LLM工作流，提高可靠性，无需修改模型内部或复杂提示设计。"}}
{"id": "2506.09520", "pdf": "https://arxiv.org/pdf/2506.09520", "abs": "https://arxiv.org/abs/2506.09520", "authors": ["Jason da Silva Castanheira", "Nicholas Shea", "Stephen M. Fleming"], "title": "How attention simplifies mental representations for planning", "categories": ["q-bio.NC", "cs.AI", "cs.RO"], "comment": null, "summary": "Human planning is efficient -- it frugally deploys limited cognitive\nresources to accomplish difficult tasks -- and flexible -- adapting to novel\nproblems and environments. Computational approaches suggest that people\nconstruct simplified mental representations of their environment, balancing the\ncomplexity of a task representation with its utility. These models imply a\nnested optimisation in which planning shapes perception, and perception shapes\nplanning -- but the perceptual and attentional mechanisms governing how this\ninteraction unfolds remain unknown. Here, we harness virtual maze navigation to\ncharacterise how spatial attention controls which aspects of a task\nrepresentation enter subjective awareness and are available for planning. We\nfind that spatial proximity governs which aspects of a maze are available for\nplanning, and that when task-relevant information follows natural (lateralised)\ncontours of attention, people can more easily construct simplified and useful\nmaze representations. This influence of attention varies considerably across\nindividuals, explaining differences in people's task representations and\nbehaviour. Inspired by the 'spotlight of attention' analogy, we incorporate the\neffects of visuospatial attention into existing computational accounts of\nvalue-guided construal. Together, our work bridges computational perspectives\non perception and decision-making to better understand how individuals\nrepresent their environments in aid of planning.", "AI": {"tldr": "人类规划高效且灵活，通过简化环境表征平衡复杂性与实用性。研究发现空间注意力控制任务表征的主观意识，影响规划行为。", "motivation": "探索人类规划中感知与注意力的交互机制，以理解个体如何构建环境表征辅助规划。", "method": "利用虚拟迷宫导航实验，分析空间注意力对任务表征和规划的影响。", "result": "空间邻近性决定迷宫信息的可用性，自然注意力轮廓有助于简化表征构建，个体差异显著。", "conclusion": "将视觉空间注意力效应融入计算模型，为感知与决策的交互提供新视角。"}}
{"id": "2506.09954", "pdf": "https://arxiv.org/pdf/2506.09954", "abs": "https://arxiv.org/abs/2506.09954", "authors": ["Ziyi Wang", "Yongming Rao", "Shuofeng Sun", "Xinrun Liu", "Yi Wei", "Xumin Yu", "Zuyan Liu", "Yanbo Wang", "Hongmin Liu", "Jie Zhou", "Jiwen Lu"], "title": "Vision Generalist Model: A Survey", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by International Journal of Computer Vision (IJCV)", "summary": "Recently, we have witnessed the great success of the generalist model in\nnatural language processing. The generalist model is a general framework\ntrained with massive data and is able to process various downstream tasks\nsimultaneously. Encouraged by their impressive performance, an increasing\nnumber of researchers are venturing into the realm of applying these models to\ncomputer vision tasks. However, the inputs and outputs of vision tasks are more\ndiverse, and it is difficult to summarize them as a unified representation. In\nthis paper, we provide a comprehensive overview of the vision generalist\nmodels, delving into their characteristics and capabilities within the field.\nFirst, we review the background, including the datasets, tasks, and benchmarks.\nThen, we dig into the design of frameworks that have been proposed in existing\nresearch, while also introducing the techniques employed to enhance their\nperformance. To better help the researchers comprehend the area, we take a\nbrief excursion into related domains, shedding light on their interconnections\nand potential synergies. To conclude, we provide some real-world application\nscenarios, undertake a thorough examination of the persistent challenges, and\noffer insights into possible directions for future research endeavors.", "AI": {"tldr": "本文综述了视觉通用模型的特点、能力和研究进展，包括背景、框架设计、性能提升技术、相关领域联系以及实际应用和未来方向。", "motivation": "通用模型在自然语言处理中的成功激发了将其应用于计算机视觉任务的兴趣，但视觉任务的输入输出多样性带来了挑战。", "method": "回顾背景（数据集、任务、基准），分析现有框架设计和技术，探讨相关领域联系，总结应用场景和挑战。", "result": "提供了视觉通用模型的全面概述，揭示了其潜力和局限性。", "conclusion": "视觉通用模型具有广阔前景，但仍需解决多样性和统一表示等挑战，未来研究可探索更多技术融合和应用场景。"}}
{"id": "2506.09958", "pdf": "https://arxiv.org/pdf/2506.09958", "abs": "https://arxiv.org/abs/2506.09958", "authors": ["Sushant Gautam", "Michael A. Riegler", "Pål Halvorsen"], "title": "Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust MedVQA in Gastrointestinal Endoscopy", "categories": ["cs.CV", "cs.LG", "68T45 (Machine learning), 92C55 (Biomedical imaging and signal\n  processing) 68T45 (Machine learning), 92C55 (Biomedical imaging and signal\n  processing)", "I.2.10; I.2.6; J.3"], "comment": null, "summary": "Medical Visual Question Answering (MedVQA) is a promising field for\ndeveloping clinical decision support systems, yet progress is often limited by\nthe available datasets, which can lack clinical complexity and visual\ndiversity. To address these gaps, we introduce Kvasir-VQA-x1, a new,\nlarge-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly\nexpands upon the original Kvasir-VQA by incorporating 159,549 new\nquestion-answer pairs that are designed to test deeper clinical reasoning. We\ndeveloped a systematic method using large language models to generate these\nquestions, which are stratified by complexity to better assess a model's\ninference capabilities. To ensure our dataset prepares models for real-world\nclinical scenarios, we have also introduced a variety of visual augmentations\nthat mimic common imaging artifacts. The dataset is structured to support two\nmain evaluation tracks: one for standard VQA performance and another to test\nmodel robustness against these visual perturbations. By providing a more\nchallenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate\nthe development of more reliable and effective multimodal AI systems for use in\nclinical settings. The dataset is fully accessible and adheres to FAIR data\nprinciples, making it a valuable resource for the wider research community.\nCode and data: https://github.com/Simula/Kvasir-VQA-x1 and\nhttps://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1", "AI": {"tldr": "Kvasir-VQA-x1是一个新的大规模胃肠道内窥镜数据集，旨在提升医学视觉问答（MedVQA）的临床复杂性和视觉多样性。", "motivation": "现有MedVQA数据集缺乏临床复杂性和视觉多样性，限制了临床决策支持系统的发展。", "method": "通过大型语言模型生成159,549个新问答对，并按复杂性分层，同时引入视觉增强模拟常见成像伪影。", "result": "数据集支持标准VQA性能和模型鲁棒性评估，为临床场景提供更具挑战性的基准。", "conclusion": "Kvasir-VQA-x1旨在加速开发更可靠的多模态AI系统，并遵循FAIR数据原则，开放给研究社区。"}}
{"id": "2506.09526", "pdf": "https://arxiv.org/pdf/2506.09526", "abs": "https://arxiv.org/abs/2506.09526", "authors": ["Woojin Cho", "Minju Jo", "Kookjin Lee", "Noseong Park"], "title": "Neural Functions for Learning Periodic Signal", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "As function approximators, deep neural networks have served as an effective\ntool to represent various signal types. Recent approaches utilize multi-layer\nperceptrons (MLPs) to learn a nonlinear mapping from a coordinate to its\ncorresponding signal, facilitating the learning of continuous neural\nrepresentations from discrete data points. Despite notable successes in\nlearning diverse signal types, coordinate-based MLPs often face issues of\noverfitting and limited generalizability beyond the training region, resulting\nin subpar extrapolation performance. This study addresses scenarios where the\nunderlying true signals exhibit periodic properties, either spatially or\ntemporally. We propose a novel network architecture, which extracts periodic\npatterns from measurements and leverages this information to represent the\nsignal, thereby enhancing generalization and improving extrapolation\nperformance. We demonstrate the efficacy of the proposed method through\ncomprehensive experiments, including the learning of the periodic solutions for\ndifferential equations, and time series imputation (interpolation) and\nforecasting (extrapolation) on real-world datasets.", "AI": {"tldr": "论文提出了一种新型网络架构，用于提取周期性信号模式，提升泛化能力和外推性能。", "motivation": "尽管基于坐标的多层感知机（MLPs）在信号表示上取得了一定成功，但其在训练区域外的泛化能力有限，且容易过拟合。本文针对具有周期性特性的信号，提出改进方法。", "method": "设计了一种新型网络架构，能够从测量数据中提取周期性模式，并利用这些信息表示信号。", "result": "通过实验验证了方法的有效性，包括学习微分方程的周期解、时间序列插补和外推预测。", "conclusion": "提出的方法显著提升了周期性信号的泛化能力和外推性能，适用于多种实际场景。"}}
{"id": "2506.09965", "pdf": "https://arxiv.org/pdf/2506.09965", "abs": "https://arxiv.org/abs/2506.09965", "authors": ["Junfei Wu", "Jian Guan", "Kaituo Feng", "Qiang Liu", "Shu Wu", "Liang Wang", "Wei Wu", "Tieniu Tan"], "title": "Reinforcing Spatial Reasoning in Vision-Language Models with Interwoven Thinking and Visual Drawing", "categories": ["cs.CV", "cs.AI", "I.2"], "comment": null, "summary": "As textual reasoning with large language models (LLMs) has advanced\nsignificantly, there has been growing interest in enhancing the multimodal\nreasoning capabilities of large vision-language models (LVLMs). However,\nexisting methods primarily approach multimodal reasoning in a straightforward,\ntext-centric manner, where both reasoning and answer derivation are conducted\npurely through text, with the only difference being the presence of multimodal\ninput. As a result, these methods often encounter fundamental limitations in\nspatial reasoning tasks that demand precise geometric understanding and\ncontinuous spatial tracking-capabilities that humans achieve through mental\nvisualization and manipulation. To address the limitations, we propose drawing\nto reason in space, a novel paradigm that enables LVLMs to reason through\nelementary drawing operations in the visual space. By equipping models with\nbasic drawing operations, including annotating bounding boxes and drawing\nauxiliary lines, we empower them to express and analyze spatial relationships\nthrough direct visual manipulation, meanwhile avoiding the performance ceiling\nimposed by specialized perception tools in previous tool-integrated reasoning\napproaches. To cultivate this capability, we develop a three-stage training\nframework: cold-start training with synthetic data to establish basic drawing\nabilities, reflective rejection sampling to enhance self-reflection behaviors,\nand reinforcement learning to directly optimize for target rewards. Extensive\nexperiments demonstrate that our model, named VILASR, consistently outperforms\nexisting methods across diverse spatial reasoning benchmarks, involving maze\nnavigation, static spatial reasoning, video-based reasoning, and\nmulti-view-based reasoning tasks, with an average improvement of 18.4%.", "AI": {"tldr": "论文提出了一种通过视觉空间中的绘图操作增强大型视觉语言模型（LVLMs）空间推理能力的新范式，显著提升了多模态推理任务的性能。", "motivation": "现有方法在多模态推理中主要依赖纯文本方式，缺乏对空间关系的精确理解和连续跟踪能力，限制了其在空间推理任务中的表现。", "method": "提出了一种通过基本绘图操作（如标注边界框和绘制辅助线）在视觉空间中进行推理的新方法，并开发了一个三阶段训练框架：冷启动训练、反射拒绝采样和强化学习。", "result": "实验表明，模型VILASR在多种空间推理任务中平均性能提升了18.4%，显著优于现有方法。", "conclusion": "通过视觉空间中的绘图操作，可以有效提升LVLMs的空间推理能力，克服纯文本方法的局限性。"}}
{"id": "2506.09969", "pdf": "https://arxiv.org/pdf/2506.09969", "abs": "https://arxiv.org/abs/2506.09969", "authors": ["Jeripothula Prudviraj", "Vikram Jamwal"], "title": "Vectorized Region Based Brush Strokes for Artistic Rendering", "categories": ["cs.CV", "I.3.3; I.5"], "comment": null, "summary": "Creating a stroke-by-stroke evolution process of a visual artwork tries to\nbridge the emotional and educational gap between the finished static artwork\nand its creation process. Recent stroke-based painting systems focus on\ncapturing stroke details by predicting and iteratively refining stroke\nparameters to maximize the similarity between the input image and the rendered\noutput. However, these methods often struggle to produce stroke compositions\nthat align with artistic principles and intent. To address this, we explore an\nimage-to-painting method that (i) facilitates semantic guidance for brush\nstrokes in targeted regions, (ii) computes the brush stroke parameters, and\n(iii) establishes a sequence among segments and strokes to sequentially render\nthe final painting. Experimental results on various input image types, such as\nface images, paintings, and photographic images, show that our method aligns\nwith a region-based painting strategy while rendering a painting with high\nfidelity and superior stroke quality.", "AI": {"tldr": "提出了一种基于语义引导的图像到绘画方法，通过区域化策略优化笔触参数和顺序，提升绘画质量和艺术性。", "motivation": "解决现有笔触绘画系统在捕捉艺术原则和意图方面的不足，弥合静态艺术作品与其创作过程之间的情感和教育差距。", "method": "结合语义引导、笔触参数计算和分段顺序渲染，实现区域化绘画策略。", "result": "在多种输入图像（如人脸、绘画和照片）上验证了方法的高保真度和优质笔触效果。", "conclusion": "该方法在艺术性和技术性上均优于现有方法，为图像到绘画转换提供了新思路。"}}
{"id": "2506.09548", "pdf": "https://arxiv.org/pdf/2506.09548", "abs": "https://arxiv.org/abs/2506.09548", "authors": ["Taku Okawara", "Kenji Koide", "Aoki Takanose", "Shuji Oishi", "Masashi Yokozuka", "Kentaro Uno", "Kazuya Yoshida"], "title": "Tightly-Coupled LiDAR-IMU-Leg Odometry with Online Learned Leg Kinematics Incorporating Foot Tactile Information", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Robotics and Automation Letters", "summary": "In this letter, we present tightly coupled LiDAR-IMU-leg odometry, which is\nrobust to challenging conditions such as featureless environments and\ndeformable terrains. We developed an online learning-based leg kinematics model\nnamed the neural leg kinematics model, which incorporates tactile information\n(foot reaction force) to implicitly express the nonlinear dynamics between\nrobot feet and the ground. Online training of this model enhances its\nadaptability to weight load changes of a robot (e.g., assuming delivery or\ntransportation tasks) and terrain conditions. According to the \\textit{neural\nadaptive leg odometry factor} and online uncertainty estimation of the leg\nkinematics model-based motion predictions, we jointly solve online training of\nthis kinematics model and odometry estimation on a unified factor graph to\nretain the consistency of both. The proposed method was verified through real\nexperiments using a quadruped robot in two challenging situations: 1) a sandy\nbeach, representing an extremely featureless area with a deformable terrain,\nand 2) a campus, including multiple featureless areas and terrain types of\nasphalt, gravel (deformable terrain), and grass. Experimental results showed\nthat our odometry estimation incorporating the \\textit{neural leg kinematics\nmodel} outperforms state-of-the-art works. Our project page is available for\nfurther details: https://takuokawara.github.io/RAL2025_project_page/", "AI": {"tldr": "提出了一种基于LiDAR-IMU-腿里程计的紧密耦合方法，通过在线学习腿运动学模型提升在无特征环境和可变形地形中的鲁棒性。", "motivation": "解决在无特征环境和可变形地形中传统里程计方法性能下降的问题。", "method": "开发了基于神经网络的腿运动学模型，结合触觉信息（脚反力），通过在线训练适应负载和地形变化，并在统一因子图上联合优化模型训练和里程计估计。", "result": "在沙滩和校园等挑战性环境中，该方法优于现有技术。", "conclusion": "提出的方法通过在线学习腿运动学模型，显著提升了里程计在复杂环境中的性能。"}}
{"id": "2506.09980", "pdf": "https://arxiv.org/pdf/2506.09980", "abs": "https://arxiv.org/abs/2506.09980", "authors": ["Jiaxiang Tang", "Ruijie Lu", "Zhaoshuo Li", "Zekun Hao", "Xuan Li", "Fangyin Wei", "Shuran Song", "Gang Zeng", "Ming-Yu Liu", "Tsung-Yi Lin"], "title": "Efficient Part-level 3D Object Generation via Dual Volume Packing", "categories": ["cs.CV"], "comment": "Code: https://github.com/NVlabs/PartPacker Project Page:\n  https://research.nvidia.com/labs/dir/partpacker/", "summary": "Recent progress in 3D object generation has greatly improved both the quality\nand efficiency. However, most existing methods generate a single mesh with all\nparts fused together, which limits the ability to edit or manipulate individual\nparts. A key challenge is that different objects may have a varying number of\nparts. To address this, we propose a new end-to-end framework for part-level 3D\nobject generation. Given a single input image, our method generates\nhigh-quality 3D objects with an arbitrary number of complete and semantically\nmeaningful parts. We introduce a dual volume packing strategy that organizes\nall parts into two complementary volumes, allowing for the creation of complete\nand interleaved parts that assemble into the final object. Experiments show\nthat our model achieves better quality, diversity, and generalization than\nprevious image-based part-level generation methods.", "AI": {"tldr": "提出了一种新的端到端框架，用于生成具有任意数量语义部分的3D对象，解决了现有方法中部分融合导致编辑困难的问题。", "motivation": "现有3D对象生成方法通常生成单一融合网格，限制了部分编辑能力。不同对象的部件数量可变，增加了挑战。", "method": "采用双体积打包策略，将部件组织到两个互补体积中，生成完整且语义明确的部分，最终组装成对象。", "result": "实验表明，该方法在质量、多样性和泛化能力上优于现有基于图像的部分级生成方法。", "conclusion": "提出的框架能够高效生成高质量、可编辑的3D对象，解决了部件数量可变的问题。"}}
{"id": "2506.09981", "pdf": "https://arxiv.org/pdf/2506.09981", "abs": "https://arxiv.org/abs/2506.09981", "authors": ["Jiazhi Yang", "Kashyap Chitta", "Shenyuan Gao", "Long Chen", "Yuqian Shao", "Xiaosong Jia", "Hongyang Li", "Andreas Geiger", "Xiangyu Yue", "Li Chen"], "title": "ReSim: Reliable World Simulation for Autonomous Driving", "categories": ["cs.CV", "cs.RO"], "comment": "Project page: https://opendrivelab.com/ReSim", "summary": "How can we reliably simulate future driving scenarios under a wide range of\nego driving behaviors? Recent driving world models, developed exclusively on\nreal-world driving data composed mainly of safe expert trajectories, struggle\nto follow hazardous or non-expert behaviors, which are rare in such data. This\nlimitation restricts their applicability to tasks such as policy evaluation. In\nthis work, we address this challenge by enriching real-world human\ndemonstrations with diverse non-expert data collected from a driving simulator\n(e.g., CARLA), and building a controllable world model trained on this\nheterogeneous corpus. Starting with a video generator featuring a diffusion\ntransformer architecture, we devise several strategies to effectively integrate\nconditioning signals and improve prediction controllability and fidelity. The\nresulting model, ReSim, enables Reliable Simulation of diverse open-world\ndriving scenarios under various actions, including hazardous non-expert ones.\nTo close the gap between high-fidelity simulation and applications that require\nreward signals to judge different actions, we introduce a Video2Reward module\nthat estimates a reward from ReSim's simulated future. Our ReSim paradigm\nachieves up to 44% higher visual fidelity, improves controllability for both\nexpert and non-expert actions by over 50%, and boosts planning and policy\nselection performance on NAVSIM by 2% and 25%, respectively.", "AI": {"tldr": "论文提出了一种通过结合真实驾驶数据和模拟器数据的方法（ReSim），解决了现有驾驶世界模型难以模拟非专家或危险行为的问题，并提升了模拟的可靠性和可控性。", "motivation": "现有驾驶世界模型主要基于真实世界的专家轨迹数据，难以模拟非专家或危险行为，限制了其在政策评估等任务中的应用。", "method": "通过结合真实驾驶数据和模拟器数据（如CARLA），构建了一个可控的世界模型（ReSim），并采用扩散变换器架构的视频生成器，提升预测的可控性和保真度。", "result": "ReSim在视觉保真度上提升了44%，对专家和非专家行为的可控性提高了50%以上，并在NAVSIM上的规划和政策选择性能分别提升了2%和25%。", "conclusion": "ReSim通过结合真实和模拟数据，显著提升了驾驶场景模拟的多样性和可靠性，为政策评估等任务提供了更有效的工具。"}}
{"id": "2506.09982", "pdf": "https://arxiv.org/pdf/2506.09982", "abs": "https://arxiv.org/abs/2506.09982", "authors": ["Zijie Wu", "Chaohui Yu", "Fan Wang", "Xiang Bai"], "title": "AnimateAnyMesh: A Feed-Forward 4D Foundation Model for Text-Driven Universal Mesh Animation", "categories": ["cs.CV"], "comment": "Project Page: https://animateanymesh.github.io/AnimateAnyMesh/", "summary": "Recent advances in 4D content generation have attracted increasing attention,\nyet creating high-quality animated 3D models remains challenging due to the\ncomplexity of modeling spatio-temporal distributions and the scarcity of 4D\ntraining data. In this paper, we present AnimateAnyMesh, the first feed-forward\nframework that enables efficient text-driven animation of arbitrary 3D meshes.\nOur approach leverages a novel DyMeshVAE architecture that effectively\ncompresses and reconstructs dynamic mesh sequences by disentangling spatial and\ntemporal features while preserving local topological structures. To enable\nhigh-quality text-conditional generation, we employ a Rectified Flow-based\ntraining strategy in the compressed latent space. Additionally, we contribute\nthe DyMesh Dataset, containing over 4M diverse dynamic mesh sequences with text\nannotations. Experimental results demonstrate that our method generates\nsemantically accurate and temporally coherent mesh animations in a few seconds,\nsignificantly outperforming existing approaches in both quality and efficiency.\nOur work marks a substantial step forward in making 4D content creation more\naccessible and practical. All the data, code, and models will be open-released.", "AI": {"tldr": "AnimateAnyMesh是一个基于文本驱动的3D网格动画生成框架，通过DyMeshVAE架构和Rectified Flow训练策略，实现了高效且高质量的动画生成。", "motivation": "当前4D内容生成面临建模时空分布的复杂性和4D训练数据稀缺的挑战，需要一种高效且通用的解决方案。", "method": "采用DyMeshVAE架构分离时空特征并保留局部拓扑结构，结合Rectified Flow训练策略在潜在空间实现文本条件生成。", "result": "实验表明，该方法能在几秒内生成语义准确且时间连贯的网格动画，质量和效率显著优于现有方法。", "conclusion": "AnimateAnyMesh推动了4D内容生成的实用化和普及化，所有数据、代码和模型将开源。"}}
{"id": "2506.09984", "pdf": "https://arxiv.org/pdf/2506.09984", "abs": "https://arxiv.org/abs/2506.09984", "authors": ["Zhenzhi Wang", "Jiaqi Yang", "Jianwen Jiang", "Chao Liang", "Gaojie Lin", "Zerong Zheng", "Ceyuan Yang", "Dahua Lin"], "title": "InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio Conditions", "categories": ["cs.CV", "cs.AI", "cs.SD"], "comment": "TL;DR: The first multi-person dialogue video generation method from\n  pairs of reference image and audio via explicit layout-aligned condition\n  injection. See project page https://zhenzhiwang.github.io/interacthuman/ for\n  more details", "summary": "End-to-end human animation with rich multi-modal conditions, e.g., text,\nimage and audio has achieved remarkable advancements in recent years. However,\nmost existing methods could only animate a single subject and inject conditions\nin a global manner, ignoring scenarios that multiple concepts could appears in\nthe same video with rich human-human interactions and human-object\ninteractions. Such global assumption prevents precise and per-identity control\nof multiple concepts including humans and objects, therefore hinders\napplications. In this work, we discard the single-entity assumption and\nintroduce a novel framework that enforces strong, region-specific binding of\nconditions from modalities to each identity's spatiotemporal footprint. Given\nreference images of multiple concepts, our method could automatically infer\nlayout information by leveraging a mask predictor to match appearance cues\nbetween the denoised video and each reference appearance. Furthermore, we\ninject local audio condition into its corresponding region to ensure\nlayout-aligned modality matching in a iterative manner. This design enables the\nhigh-quality generation of controllable multi-concept human-centric videos.\nEmpirical results and ablation studies validate the effectiveness of our\nexplicit layout control for multi-modal conditions compared to implicit\ncounterparts and other existing methods.", "AI": {"tldr": "提出了一种新框架，通过区域特定的条件绑定，实现多概念（如多人和物体）的精确控制，生成高质量的多模态人类中心视频。", "motivation": "现有方法通常只能全局控制单一主体，无法处理多概念场景（如多人互动或人-物互动），限制了应用。", "method": "利用掩码预测器自动推断布局信息，并通过迭代方式将局部音频条件注入对应区域，实现布局对齐的多模态匹配。", "result": "实验验证了显式布局控制对多模态条件的有效性，优于隐式方法和其他现有方法。", "conclusion": "新框架通过区域特定的条件绑定，显著提升了多概念视频生成的质量和可控性。"}}
{"id": "2506.09987", "pdf": "https://arxiv.org/pdf/2506.09987", "abs": "https://arxiv.org/abs/2506.09987", "authors": ["Benno Krojer", "Mojtaba Komeili", "Candace Ross", "Quentin Garrido", "Koustuv Sinha", "Nicolas Ballas", "Mahmoud Assran"], "title": "A Shortcut-aware Video-QA Benchmark for Physical Understanding via Minimal Video Pairs", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Existing benchmarks for assessing the spatio-temporal understanding and\nreasoning abilities of video language models are susceptible to score inflation\ndue to the presence of shortcut solutions based on superficial visual or\ntextual cues. This paper mitigates the challenges in accurately assessing model\nperformance by introducing the Minimal Video Pairs (MVP) benchmark, a simple\nshortcut-aware video QA benchmark for assessing the physical understanding of\nvideo language models. The benchmark is comprised of 55K high-quality\nmultiple-choice video QA examples focusing on physical world understanding.\nExamples are curated from nine video data sources, spanning first-person\negocentric and exocentric videos, robotic interaction data, and cognitive\nscience intuitive physics benchmarks. To mitigate shortcut solutions that rely\non superficial visual or textual cues and biases, each sample in MVP has a\nminimal-change pair -- a visually similar video accompanied by an identical\nquestion but an opposing answer. To answer a question correctly, a model must\nprovide correct answers for both examples in the minimal-change pair; as such,\nmodels that solely rely on visual or textual biases would achieve below random\nperformance. Human performance on MVP is 92.9\\%, while the best open-source\nstate-of-the-art video-language model achieves 40.2\\% compared to random\nperformance at 25\\%.", "AI": {"tldr": "该论文提出了MVP基准，用于评估视频语言模型对物理世界的理解能力，通过最小变化对避免模型依赖表面视觉或文本线索。", "motivation": "现有基准因依赖表面线索导致评分虚高，需更准确评估模型性能。", "method": "引入MVP基准，包含55K高质量多选题视频QA样本，每样本有最小变化对以消除捷径。", "result": "人类表现92.9%，最佳开源模型仅40.2%，随机表现25%。", "conclusion": "MVP基准有效避免模型依赖捷径，更准确评估物理理解能力。"}}
{"id": "2506.09988", "pdf": "https://arxiv.org/pdf/2506.09988", "abs": "https://arxiv.org/abs/2506.09988", "authors": ["Ron Yosef", "Moran Yanuka", "Yonatan Bitton", "Dani Lischinski"], "title": "EditInspector: A Benchmark for Evaluation of Text-Guided Image Edits", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Text-guided image editing, fueled by recent advancements in generative AI, is\nbecoming increasingly widespread. This trend highlights the need for a\ncomprehensive framework to verify text-guided edits and assess their quality.\nTo address this need, we introduce EditInspector, a novel benchmark for\nevaluation of text-guided image edits, based on human annotations collected\nusing an extensive template for edit verification. We leverage EditInspector to\nevaluate the performance of state-of-the-art (SoTA) vision and language models\nin assessing edits across various dimensions, including accuracy, artifact\ndetection, visual quality, seamless integration with the image scene, adherence\nto common sense, and the ability to describe edit-induced changes. Our findings\nindicate that current models struggle to evaluate edits comprehensively and\nfrequently hallucinate when describing the changes. To address these\nchallenges, we propose two novel methods that outperform SoTA models in both\nartifact detection and difference caption generation.", "AI": {"tldr": "EditInspector是一个基于人工标注的基准测试，用于评估文本引导的图像编辑质量，发现现有模型在全面评估编辑效果方面存在不足，并提出两种新方法以改进。", "motivation": "随着生成式AI的发展，文本引导的图像编辑日益普及，但缺乏一个全面的框架来验证编辑质量和评估其效果。", "method": "引入EditInspector基准测试，基于人工标注的编辑验证模板，评估现有模型在多个维度上的表现，并提出两种新方法。", "result": "现有模型在全面评估编辑效果时表现不佳，且容易产生幻觉描述；提出的新方法在伪影检测和差异描述生成上优于现有模型。", "conclusion": "EditInspector为文本引导的图像编辑提供了有效的评估工具，新方法在关键任务上表现更优，但仍需进一步改进。"}}
{"id": "2506.09662", "pdf": "https://arxiv.org/pdf/2506.09662", "abs": "https://arxiv.org/abs/2506.09662", "authors": ["Bianca Perasso", "Ludovico Lozza", "Andrea Ponte", "Luca Demetrio", "Luca Oneto", "Fabio Roli"], "title": "Empirical Quantification of Spurious Correlations in Malware Detection", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "End-to-end deep learning exhibits unmatched performance for detecting\nmalware, but such an achievement is reached by exploiting spurious correlations\n-- features with high relevance at inference time, but known to be useless\nthrough domain knowledge. While previous work highlighted that deep networks\nmainly focus on metadata, none investigated the phenomenon further, without\nquantifying their impact on the decision. In this work, we deepen our\nunderstanding of how spurious correlation affects deep learning for malware\ndetection by highlighting how much models rely on empty spaces left by the\ncompiler, which diminishes the relevance of the compiled code. Through our\nseminal analysis on a small-scale balanced dataset, we introduce a ranking of\ntwo end-to-end models to better understand which is more suitable to be put in\nproduction.", "AI": {"tldr": "论文探讨了端到端深度学习在恶意软件检测中依赖虚假相关性的问题，尤其是编译器留下的空白空间对模型决策的影响，并通过小规模平衡数据集分析比较了两种模型的适用性。", "motivation": "揭示深度学习模型在恶意软件检测中如何依赖虚假相关性（如编译器留下的空白空间），并量化其对决策的影响，以提升模型的实际适用性。", "method": "通过在小规模平衡数据集上进行实验，分析两种端到端深度学习模型对虚假相关性的依赖程度，并比较其性能。", "result": "研究发现模型过度依赖编译器留下的空白空间，降低了编译代码的相关性，同时通过排名比较了两种模型的适用性。", "conclusion": "研究深化了对虚假相关性在恶意软件检测中影响的理解，并为选择更适合生产的模型提供了依据。"}}
{"id": "2506.09989", "pdf": "https://arxiv.org/pdf/2506.09989", "abs": "https://arxiv.org/abs/2506.09989", "authors": ["Yiming Dou", "Wonseok Oh", "Yuqing Luo", "Antonio Loquercio", "Andrew Owens"], "title": "Hearing Hands: Generating Sounds from Physical Interactions in 3D Scenes", "categories": ["cs.CV"], "comment": "CVPR 2025, Project page: https://www.yimingdou.com/hearing_hands/ ,\n  Code: https://github.com/Dou-Yiming/hearing_hands/", "summary": "We study the problem of making 3D scene reconstructions interactive by asking\nthe following question: can we predict the sounds of human hands physically\ninteracting with a scene? First, we record a video of a human manipulating\nobjects within a 3D scene using their hands. We then use these action-sound\npairs to train a rectified flow model to map 3D hand trajectories to their\ncorresponding audio. At test time, a user can query the model for other\nactions, parameterized as sequences of hand poses, to estimate their\ncorresponding sounds. In our experiments, we find that our generated sounds\naccurately convey material properties and actions, and that they are often\nindistinguishable to human observers from real sounds. Project page:\nhttps://www.yimingdou.com/hearing_hands/", "AI": {"tldr": "研究如何通过预测人手与3D场景交互产生的声音，使3D场景重建更具交互性。", "motivation": "探索如何通过声音增强3D场景的交互体验，特别是在人手与物体交互时。", "method": "录制人手操作物体的视频，利用动作-声音对训练校正流模型，将3D手部轨迹映射到对应音频。测试时，用户可通过手部姿势序列查询模型生成声音。", "result": "生成的声音能准确传达材质属性和动作，且人类观察者难以区分其与真实声音的差异。", "conclusion": "该方法成功实现了通过手部动作预测交互声音，为3D场景的交互性提供了新思路。"}}
{"id": "2506.09993", "pdf": "https://arxiv.org/pdf/2506.09993", "abs": "https://arxiv.org/abs/2506.09993", "authors": ["Jaewon Min", "Jin Hyeon Kim", "Paul Hyunbin Cho", "Jaeeun Lee", "Jihye Park", "Minkyu Park", "Sangpil Kim", "Hyunhee Park", "Seungryong Kim"], "title": "Text-Aware Image Restoration with Diffusion Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Project page: https://cvlab-kaist.github.io/TAIR/", "summary": "Image restoration aims to recover degraded images. However, existing\ndiffusion-based restoration methods, despite great success in natural image\nrestoration, often struggle to faithfully reconstruct textual regions in\ndegraded images. Those methods frequently generate plausible but incorrect\ntext-like patterns, a phenomenon we refer to as text-image hallucination. In\nthis paper, we introduce Text-Aware Image Restoration (TAIR), a novel\nrestoration task that requires the simultaneous recovery of visual contents and\ntextual fidelity. To tackle this task, we present SA-Text, a large-scale\nbenchmark of 100K high-quality scene images densely annotated with diverse and\ncomplex text instances. Furthermore, we propose a multi-task diffusion\nframework, called TeReDiff, that integrates internal features from diffusion\nmodels into a text-spotting module, enabling both components to benefit from\njoint training. This allows for the extraction of rich text representations,\nwhich are utilized as prompts in subsequent denoising steps. Extensive\nexperiments demonstrate that our approach consistently outperforms\nstate-of-the-art restoration methods, achieving significant gains in text\nrecognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/", "AI": {"tldr": "本文提出了一种新的图像修复任务TAIR，专注于同时恢复视觉内容和文本保真度，并提出了一个大规模基准SA-Text和多任务扩散框架TeReDiff。", "motivation": "现有基于扩散的图像修复方法在自然图像修复中表现良好，但在文本区域的恢复中常出现错误生成文本的现象（文本图像幻觉）。", "method": "提出TAIR任务，构建SA-Text基准，并设计TeReDiff框架，结合扩散模型和文本检测模块进行联合训练。", "result": "实验表明，TeReDiff在文本识别准确率上显著优于现有方法。", "conclusion": "TAIR任务和TeReDiff框架有效解决了文本图像幻觉问题，提升了文本区域的修复质量。"}}
{"id": "2506.09995", "pdf": "https://arxiv.org/pdf/2506.09995", "abs": "https://arxiv.org/abs/2506.09995", "authors": ["Yuanpeng Tu", "Hao Luo", "Xi Chen", "Xiang Bai", "Fan Wang", "Hengshuang Zhao"], "title": "PlayerOne: Egocentric World Simulator", "categories": ["cs.CV"], "comment": "Project page: https://playerone-hku.github.io/", "summary": "We introduce PlayerOne, the first egocentric realistic world simulator,\nfacilitating immersive and unrestricted exploration within vividly dynamic\nenvironments. Given an egocentric scene image from the user, PlayerOne can\naccurately construct the corresponding world and generate egocentric videos\nthat are strictly aligned with the real scene human motion of the user captured\nby an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that\nfirst performs pretraining on large-scale egocentric text-video pairs for\ncoarse-level egocentric understanding, followed by finetuning on synchronous\nmotion-video data extracted from egocentric-exocentric video datasets with our\nautomatic construction pipeline. Besides, considering the varying importance of\ndifferent components, we design a part-disentangled motion injection scheme,\nenabling precise control of part-level movements. In addition, we devise a\njoint reconstruction framework that progressively models both the 4D scene and\nvideo frames, ensuring scene consistency in the long-form video generation.\nExperimental results demonstrate its great generalization ability in precise\ncontrol of varying human movements and worldconsistent modeling of diverse\nscenarios. It marks the first endeavor into egocentric real-world simulation\nand can pave the way for the community to delve into fresh frontiers of world\nmodeling and its diverse applications.", "AI": {"tldr": "PlayerOne是首个以自我为中心的逼真世界模拟器，通过粗到细的训练流程生成动态环境中的沉浸式视频。", "motivation": "旨在实现自我中心视角的逼真世界模拟，支持对用户动作的精确控制和场景一致性建模。", "method": "采用粗到细的训练流程，包括大规模文本-视频对预训练和同步运动-视频数据微调，设计了部分解耦的运动注入方案和联合重建框架。", "result": "实验表明，PlayerOne在精确控制人类动作和多样化场景的一致性建模方面表现出色。", "conclusion": "PlayerOne开创了自我中心视角的真实世界模拟，为世界建模及其应用开辟了新方向。"}}
{"id": "2506.09063", "pdf": "https://arxiv.org/pdf/2506.09063", "abs": "https://arxiv.org/abs/2506.09063", "authors": ["Shayan Shekarforoush", "David B. Lindell", "Marcus A. Brubaker", "David J. Fleet"], "title": "Reconstructing Heterogeneous Biomolecules via Hierarchical Gaussian Mixtures and Part Discovery", "categories": ["q-bio.QM", "cs.CV", "cs.LG", "eess.IV"], "comment": "21 pages, 14 figures, Project Webpage:\n  https://shekshaa.github.io/CryoSPIRE", "summary": "Cryo-EM is a transformational paradigm in molecular biology where\ncomputational methods are used to infer 3D molecular structure at atomic\nresolution from extremely noisy 2D electron microscope images. At the forefront\nof research is how to model the structure when the imaged particles exhibit\nnon-rigid conformational flexibility and compositional variation where parts\nare sometimes missing. We introduce a novel 3D reconstruction framework with a\nhierarchical Gaussian mixture model, inspired in part by Gaussian Splatting for\n4D scene reconstruction. In particular, the structure of the model is grounded\nin an initial process that infers a part-based segmentation of the particle,\nproviding essential inductive bias in order to handle both conformational and\ncompositional variability. The framework, called CryoSPIRE, is shown to reveal\nbiologically meaningful structures on complex experimental datasets, and\nestablishes a new state-of-the-art on CryoBench, a benchmark for cryo-EM\nheterogeneity methods.", "AI": {"tldr": "CryoSPIRE是一种新型的冷冻电镜3D重建框架，通过分层高斯混合模型处理非刚性构象灵活性和组成变异，显著提升了复杂实验数据的解析能力。", "motivation": "冷冻电镜在分子生物学中具有革命性意义，但如何建模非刚性构象灵活性和组成变异（如部分缺失）的粒子结构是研究前沿。", "method": "提出了一种基于分层高斯混合模型的3D重建框架，受4D场景重建中的高斯泼溅启发，通过部分分割粒子提供归纳偏置。", "result": "CryoSPIRE在复杂实验数据中揭示了生物学意义的结构，并在CryoBench基准测试中达到新最优水平。", "conclusion": "CryoSPIRE为冷冻电镜中的异构性建模提供了新方法，显著提升了结构解析能力。"}}
{"id": "2506.09701", "pdf": "https://arxiv.org/pdf/2506.09701", "abs": "https://arxiv.org/abs/2506.09701", "authors": ["Vincenzo Collura", "Karim Tit", "Laura Bussi", "Eleonora Giunchiglia", "Maxime Cordy"], "title": "TRIDENT: Temporally Restricted Inference via DFA-Enhanced Neural Traversal", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) and other neural architectures have achieved\nimpressive results across a variety of generative and classification tasks.\nHowever, they remain fundamentally ill-equipped to ensure that their outputs\nsatisfy temporal constraints, such as those expressible in Linear Temporal\nLogic over finite traces (LTLf). In this paper, we introduce TRIDENT: a general\nand model-agnostic inference-time algorithm that guarantees compliance with\nsuch constraints without requiring any retraining. TRIDENT compiles LTLf\nformulas into a Deterministic Finite Automaton (DFA), which is used to guide a\nconstrained variant of beam search. At each decoding step, transitions that\nwould lead to constraint violations are masked, while remaining paths are\ndynamically re-ranked based on both the model's probabilities and the DFA's\nacceptance structure. We formally prove that the resulting sequences are\nguaranteed to satisfy the given LTLf constraints, and we empirically\ndemonstrate that TRIDENT also improves output quality. We validate our approach\non two distinct tasks: temporally constrained image-stream classification and\ncontrolled text generation. In both settings, TRIDENT achieves perfect\nconstraint satisfaction, while comparison with the state of the art shows\nimproved efficiency and high standard quality metrics.", "AI": {"tldr": "TRIDENT是一种模型无关的推理时算法，确保LLM输出满足线性时序逻辑（LTLf）约束，无需重新训练。", "motivation": "LLMs在生成和分类任务中表现优异，但无法保证输出满足时序约束。", "method": "将LTLf公式编译为确定性有限自动机（DFA），指导约束束搜索，动态屏蔽违规路径并重新排序。", "result": "TRIDENT保证输出满足LTLf约束，并在实验中提升输出质量，实现完美约束满足。", "conclusion": "TRIDENT在图像流分类和文本生成任务中高效且高质量地满足约束，优于现有方法。"}}
{"id": "2506.09069", "pdf": "https://arxiv.org/pdf/2506.09069", "abs": "https://arxiv.org/abs/2506.09069", "authors": ["Sahaj Raj Malla"], "title": "Devanagari Digit Recognition using Quantum Machine Learning", "categories": ["quant-ph", "cs.CV", "cs.LG", "68T05, 68Q10, 68T07", "I.2.6; I.4.8; F.1.2"], "comment": "9 pages, 4 figures, arXiv preprint, code available upon request", "summary": "Handwritten digit recognition in regional scripts, such as Devanagari, is\ncrucial for multilingual document digitization, educational tools, and the\npreservation of cultural heritage. The script's complex structure and limited\nannotated datasets pose significant challenges to conventional models. This\npaper introduces the first hybrid quantum-classical architecture for Devanagari\nhandwritten digit recognition, combining a convolutional neural network (CNN)\nfor spatial feature extraction with a 10-qubit variational quantum circuit\n(VQC) for quantum-enhanced classification. Trained and evaluated on the\nDevanagari Handwritten Character Dataset (DHCD), the proposed model achieves a\nstate-of-the-art test accuracy for quantum implementation of 99.80% and a test\nloss of 0.2893, with an average per-class F1-score of 0.9980. Compared to\nequivalent classical CNNs, our model demonstrates superior accuracy with\nsignificantly fewer parameters and enhanced robustness. By leveraging quantum\nprinciples such as superposition and entanglement, this work establishes a\nnovel benchmark for regional script recognition, highlighting the promise of\nquantum machine learning (QML) in real-world, low-resource language settings.", "AI": {"tldr": "论文提出了一种混合量子-经典架构，用于Devanagari手写数字识别，结合CNN和量子电路，在DHCD数据集上实现了99.80%的测试准确率。", "motivation": "Devanagari等区域脚本的手写数字识别对多语言文档数字化、教育工具和文化遗产保护至关重要，但其复杂结构和有限标注数据对传统模型构成挑战。", "method": "采用卷积神经网络（CNN）进行空间特征提取，结合10量子比特的变分量子电路（VQC）进行量子增强分类。", "result": "模型在DHCD数据集上实现了99.80%的测试准确率和0.2893的测试损失，平均每类F1分数为0.9980，优于传统CNN。", "conclusion": "该工作通过量子叠加和纠缠原理，为区域脚本识别设立了新基准，展示了量子机器学习在低资源语言场景中的潜力。"}}
{"id": "2506.09733", "pdf": "https://arxiv.org/pdf/2506.09733", "abs": "https://arxiv.org/abs/2506.09733", "authors": ["Minjong Cheon"], "title": "AtmosMJ: Revisiting Gating Mechanism for AI Weather Forecasting Beyond the Year Scale", "categories": ["cs.LG", "cs.AI", "cs.CV", "physics.ao-ph"], "comment": null, "summary": "The advent of Large Weather Models (LWMs) has marked a turning point in\ndata-driven forecasting, with many models now outperforming traditional\nnumerical systems in the medium range. However, achieving stable, long-range\nautoregressive forecasts beyond a few weeks remains a significant challenge.\nPrevailing state-of-the-art models that achieve year-long stability, such as\nSFNO and DLWP-HPX, have relied on transforming input data onto non-standard\nspatial domains like spherical harmonics or HEALPix meshes. This has led to the\nprevailing assumption that such representations are necessary to enforce\nphysical consistency and long-term stability. This paper challenges that\nassumption by investigating whether comparable long-range performance can be\nachieved on the standard latitude-longitude grid. We introduce AtmosMJ, a deep\nconvolutional network that operates directly on ERA5 data without any spherical\nremapping. The model's stability is enabled by a novel Gated Residual Fusion\n(GRF) mechanism, which adaptively moderates feature updates to prevent error\naccumulation over long recursive simulations. Our results demonstrate that\nAtmosMJ produces stable and physically plausible forecasts for about 500 days.\nIn quantitative evaluations, it achieves competitive 10-day forecast accuracy\nagainst models like Pangu-Weather and GraphCast, all while requiring a\nremarkably low training budget of 5.7 days on a V100 GPU. Our findings suggest\nthat efficient architectural design, rather than non-standard data\nrepresentation, can be the key to unlocking stable and computationally\nefficient long-range weather prediction.", "AI": {"tldr": "AtmosMJ挑战了传统观点，证明标准经纬度网格也能实现长期稳定的天气预测，通过Gated Residual Fusion机制，无需非标准数据表示。", "motivation": "解决长期自回归天气预测的稳定性问题，挑战非标准空间域表示的必要性假设。", "method": "引入AtmosMJ，一种直接在标准经纬度网格上操作的深度卷积网络，采用Gated Residual Fusion机制防止误差累积。", "result": "AtmosMJ实现了约500天的稳定预测，10天预测精度与Pangu-Weather和GraphCast相当，训练成本极低。", "conclusion": "高效架构设计比非标准数据表示更能实现稳定且计算高效的长期天气预测。"}}
{"id": "2506.09075", "pdf": "https://arxiv.org/pdf/2506.09075", "abs": "https://arxiv.org/abs/2506.09075", "authors": ["Elly Akhoundi", "Hung Yu Ling", "Anup Anand Deshmukh", "Judith Butepage"], "title": "SILK: Smooth InterpoLation frameworK for motion in-betweening A Simplified Computational Approach", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Accepted to CVPR 2025 Human Motion Generation Workshop. 10 pages, 3\n  figures, 5 Tables, and 40 References", "summary": "Motion in-betweening is a crucial tool for animators, enabling intricate\ncontrol over pose-level details in each keyframe. Recent machine learning\nsolutions for motion in-betweening rely on complex models, incorporating\nskeleton-aware architectures or requiring multiple modules and training steps.\nIn this work, we introduce a simple yet effective Transformer-based framework,\nemploying a single Transformer encoder to synthesize realistic motions for\nmotion in-betweening tasks. We find that data modeling choices play a\nsignificant role in improving in-betweening performance. Among others, we show\nthat increasing data volume can yield equivalent or improved motion\ntransitions, that the choice of pose representation is vital for achieving\nhigh-quality results, and that incorporating velocity input features enhances\nanimation performance. These findings challenge the assumption that model\ncomplexity is the primary determinant of animation quality and provide insights\ninto a more data-centric approach to motion interpolation. Additional videos\nand supplementary material are available at https://silk-paper.github.io.", "AI": {"tldr": "论文提出了一种基于Transformer的简单框架，用于运动插值任务，强调数据建模选择对性能的重要性。", "motivation": "传统运动插值方法依赖复杂模型，本文旨在探索简单模型结合数据优化是否能达到更好效果。", "method": "使用单一Transformer编码器，重点优化数据建模（如数据量、姿态表示和速度特征）。", "result": "实验表明，数据优化（如增加数据量、改进姿态表示）能显著提升运动插值质量。", "conclusion": "模型复杂度并非动画质量的决定因素，数据优化是关键，为运动插值提供了数据中心的思路。"}}
{"id": "2506.09098", "pdf": "https://arxiv.org/pdf/2506.09098", "abs": "https://arxiv.org/abs/2506.09098", "authors": ["Yangjie Cui", "Boyang Gao", "Yiwei Zhang", "Xin Dong", "Jinwu Xiang", "Daochun Li", "Zhan Tu"], "title": "WD-DETR: Wavelet Denoising-Enhanced Real-Time Object Detection Transformer for Robot Perception with Event Cameras", "categories": ["cs.RO", "cs.CV"], "comment": "https://youtu.be/AQAgVdrx1DE", "summary": "Previous studies on event camera sensing have demonstrated certain detection\nperformance using dense event representations. However, the accumulated noise\nin such dense representations has received insufficient attention, which\ndegrades the representation quality and increases the likelihood of missed\ndetections. To address this challenge, we propose the Wavelet\nDenoising-enhanced DEtection TRansformer, i.e., WD-DETR network, for event\ncameras. In particular, a dense event representation is presented first, which\nenables real-time reconstruction of events as tensors. Then, a wavelet\ntransform method is designed to filter noise in the event representations. Such\na method is integrated into the backbone for feature extraction. The extracted\nfeatures are subsequently fed into a transformer-based network for object\nprediction. To further reduce inference time, we incorporate the Dynamic\nReorganization Convolution Block (DRCB) as a fusion module within the hybrid\nencoder. The proposed method has been evaluated on three event-based object\ndetection datasets, i.e., DSEC, Gen1, and 1Mpx. The results demonstrate that\nWD-DETR outperforms tested state-of-the-art methods. Additionally, we implement\nour approach on a common onboard computer for robots, the NVIDIA Jetson Orin\nNX, achieving a high frame rate of approximately 35 FPS using TensorRT FP16,\nwhich is exceptionally well-suited for real-time perception of onboard robotic\nsystems.", "AI": {"tldr": "论文提出了一种基于小波去噪的检测变换器（WD-DETR）网络，用于解决事件相机中密集事件表示的噪声问题，并在多个数据集上验证了其优越性能。", "motivation": "现有的事件相机检测方法使用密集事件表示，但噪声积累问题未得到充分关注，导致检测性能下降。", "method": "提出WD-DETR网络，包括密集事件表示、小波去噪方法、基于变换器的目标预测网络，以及动态重组卷积块（DRCB）以减少推理时间。", "result": "在DSEC、Gen1和1Mpx数据集上，WD-DETR优于现有方法，并在NVIDIA Jetson Orin NX上实现约35 FPS的高帧率。", "conclusion": "WD-DETR有效解决了事件相机噪声问题，适用于实时机器人感知。"}}
{"id": "2506.09742", "pdf": "https://arxiv.org/pdf/2506.09742", "abs": "https://arxiv.org/abs/2506.09742", "authors": ["Gusseppe Bravo-Rocca", "Peini Liu", "Jordi Guitart", "Rodrigo M Carrillo-Larco", "Ajay Dholakia", "David Ellison"], "title": "Feature Engineering for Agents: An Adaptive Cognitive Architecture for Interpretable ML Monitoring", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted at AAMAS 2025", "summary": "Monitoring Machine Learning (ML) models in production environments is\ncrucial, yet traditional approaches often yield verbose, low-interpretability\noutputs that hinder effective decision-making. We propose a cognitive\narchitecture for ML monitoring that applies feature engineering principles to\nagents based on Large Language Models (LLMs), significantly enhancing the\ninterpretability of monitoring outputs. Central to our approach is a Decision\nProcedure module that simulates feature engineering through three key steps:\nRefactor, Break Down, and Compile. The Refactor step improves data\nrepresentation to better capture feature semantics, allowing the LLM to focus\non salient aspects of the monitoring data while reducing noise and irrelevant\ninformation. Break Down decomposes complex information for detailed analysis,\nand Compile integrates sub-insights into clear, interpretable outputs. This\nprocess leads to a more deterministic planning approach, reducing dependence on\nLLM-generated planning, which can sometimes be inconsistent and overly general.\nThe combination of feature engineering-driven planning and selective LLM\nutilization results in a robust decision support system, capable of providing\nhighly interpretable and actionable insights. Experiments using multiple LLMs\ndemonstrate the efficacy of our approach, achieving significantly higher\naccuracy compared to various baselines across several domains.", "AI": {"tldr": "提出了一种基于特征工程和LLM的认知架构，用于提升机器学习模型监控输出的可解释性。", "motivation": "传统机器学习模型监控方法输出冗长且难以理解，影响决策效率。", "method": "通过Refactor、Break Down和Compile三个步骤模拟特征工程，优化数据表示并整合子洞察。", "result": "实验证明该方法显著提高了监控输出的准确性和可解释性。", "conclusion": "结合特征工程和选择性LLM利用，构建了高效且可解释的决策支持系统。"}}
{"id": "2506.09100", "pdf": "https://arxiv.org/pdf/2506.09100", "abs": "https://arxiv.org/abs/2506.09100", "authors": ["Haonan Zhang", "Guoyan Lao", "Yuyao Zhang", "Hongjiang Wei"], "title": "Low-Rank Augmented Implicit Neural Representation for Unsupervised High-Dimensional Quantitative MRI Reconstruction", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Quantitative magnetic resonance imaging (qMRI) provides tissue-specific\nparameters vital for clinical diagnosis. Although simultaneous multi-parametric\nqMRI (MP-qMRI) technologies enhance imaging efficiency, robustly reconstructing\nqMRI from highly undersampled, high-dimensional measurements remains a\nsignificant challenge. This difficulty arises primarily because current\nreconstruction methods that rely solely on a single prior or physics-informed\nmodel to solve the highly ill-posed inverse problem, which often leads to\nsuboptimal results. To overcome this limitation, we propose LoREIN, a novel\nunsupervised and dual-prior-integrated framework for accelerated 3D MP-qMRI\nreconstruction. Technically, LoREIN incorporates both low-rank prior and\ncontinuity prior via low-rank representation (LRR) and implicit neural\nrepresentation (INR), respectively, to enhance reconstruction fidelity. The\npowerful continuous representation of INR enables the estimation of optimal\nspatial bases within the low-rank subspace, facilitating high-fidelity\nreconstruction of weighted images. Simultaneously, the predicted multi-contrast\nweighted images provide essential structural and quantitative guidance, further\nenhancing the reconstruction accuracy of quantitative parameter maps.\nFurthermore, our work introduces a zero-shot learning paradigm with broad\npotential in complex spatiotemporal and high-dimensional image reconstruction\ntasks, further advancing the field of medical imaging.", "AI": {"tldr": "LoREIN是一种新型无监督双先验集成框架，用于加速3D多参数定量MRI重建，结合低秩先验和连续性先验，显著提升重建质量。", "motivation": "当前重建方法仅依赖单一先验或物理模型，难以解决高度不适定的逆问题，导致结果不理想。", "method": "LoREIN结合低秩表示（LRR）和隐式神经表示（INR），利用低秩先验和连续性先验，优化重建过程。", "result": "该方法通过零样本学习范式，在多对比加权图像和定量参数图重建中表现出高保真度。", "conclusion": "LoREIN为复杂时空和高维图像重建任务提供了新思路，推动了医学影像领域的发展。"}}
{"id": "2506.09749", "pdf": "https://arxiv.org/pdf/2506.09749", "abs": "https://arxiv.org/abs/2506.09749", "authors": ["Shuo Jiang", "Min Xie", "Jianxi Luo"], "title": "Large Language Models for Design Structure Matrix Optimization", "categories": ["cs.CE", "cs.AI", "I.2.7; I.2.1"], "comment": null, "summary": "In complex engineering systems, the interdependencies among components or\ndevelopment activities are often modeled and analyzed using Design Structure\nMatrix (DSM). Reorganizing elements within a DSM to minimize feedback loops and\nenhance modularity or process efficiency constitutes a challenging\ncombinatorial optimization (CO) problem in engineering design and operations.\nAs problem sizes increase and dependency networks become more intricate,\ntraditional optimization methods that solely use mathematical heuristics often\nfail to capture the contextual nuances and struggle to deliver effective\nsolutions. In this study, we explore the potential of Large Language Models\n(LLMs) for helping solve such CO problems by leveraging their capabilities for\nadvanced reasoning and contextual understanding. We propose a novel LLM-based\nframework that integrates network topology with contextual domain knowledge for\niterative optimization of DSM element sequencing - a common CO problem.\nExperiments on various DSM cases show that our method consistently achieves\nfaster convergence and superior solution quality compared to both stochastic\nand deterministic baselines. Notably, we find that incorporating contextual\ndomain knowledge significantly enhances optimization performance regardless of\nthe chosen LLM backbone. These findings highlight the potential of LLMs to\nsolve complex engineering CO problems by combining semantic and mathematical\nreasoning. This approach paves the way towards a new paradigm in LLM-based\nengineering design optimization.", "AI": {"tldr": "论文提出了一种基于大型语言模型（LLM）的框架，用于优化设计结构矩阵（DSM）中的元素排序问题，结合网络拓扑和领域知识，显著提升了优化性能。", "motivation": "传统优化方法在复杂工程系统中难以捕捉上下文细节，无法有效解决依赖网络的组合优化问题。", "method": "提出了一种LLM框架，整合网络拓扑和领域知识，迭代优化DSM元素排序。", "result": "实验表明，该方法在收敛速度和解决方案质量上优于随机和确定性基线，领域知识的引入显著提升了性能。", "conclusion": "LLM通过结合语义和数学推理，为解决复杂工程组合优化问题提供了新范式。"}}
{"id": "2506.09161", "pdf": "https://arxiv.org/pdf/2506.09161", "abs": "https://arxiv.org/abs/2506.09161", "authors": ["Rajan Das Gupta", "Md Imrul Hasan Showmick", "Mushfiqur Rahman Abir", "Shanjida Akter", "Md. Yeasin Rahat", "Md. Jakir Hossen"], "title": "An Explainable Deep Learning Framework for Brain Stroke and Tumor Progression via MRI Interpretation", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted in MECON 2025", "summary": "Early and accurate detection of brain abnormalities, such as tumors and\nstrokes, is essential for timely intervention and improved patient outcomes. In\nthis study, we present a deep learning-based system capable of identifying both\nbrain tumors and strokes from MRI images, along with their respective stages.\nWe have executed two groundbreaking strategies involving convolutional neural\nnetworks, MobileNet V2 and ResNet-50-optimized through transfer learning to\nclassify MRI scans into five diagnostic categories. Our dataset, aggregated and\naugmented from various publicly available MRI sources, was carefully curated to\nensure class balance and image diversity. To enhance model generalization and\nprevent overfitting, we applied dropout layers and extensive data augmentation.\nThe models achieved strong performance, with training accuracy reaching 93\\%\nand validation accuracy up to 88\\%. While ResNet-50 demonstrated slightly\nbetter results, Mobile Net V2 remains a promising option for real-time\ndiagnosis in low resource settings due to its lightweight architecture. This\nresearch offers a practical AI-driven solution for early brain abnormality\ndetection, with potential for clinical deployment and future enhancement\nthrough larger datasets and multi modal inputs.", "AI": {"tldr": "提出了一种基于深度学习的系统，利用MobileNet V2和ResNet-50从MRI图像中检测脑肿瘤和中风及其阶段。", "motivation": "早期准确检测脑部异常（如肿瘤和中风）对及时干预和改善患者预后至关重要。", "method": "使用卷积神经网络（MobileNet V2和ResNet-50），通过迁移学习优化，将MRI扫描分类为五个诊断类别。数据集经过精心整理和增强，应用dropout和数据增强防止过拟合。", "result": "模型表现优异，训练准确率达93%，验证准确率达88%。ResNet-50略优，但MobileNet V2因其轻量级架构适合低资源环境。", "conclusion": "该研究为早期脑部异常检测提供了实用的AI解决方案，具有临床部署潜力，未来可通过更大数据集和多模态输入进一步优化。"}}
{"id": "2506.09755", "pdf": "https://arxiv.org/pdf/2506.09755", "abs": "https://arxiv.org/abs/2506.09755", "authors": ["Shuo Jiang", "Min Xie", "Frank Youhua Chen", "Jian Ma", "Jianxi Luo"], "title": "Intelligent Design 4.0: Paradigm Evolution Toward the Agentic AI Era", "categories": ["cs.CE", "cs.AI", "I.2.7; I.2.1"], "comment": null, "summary": "Research and practice in Intelligent Design (ID) have significantly enhanced\nengineering innovation, efficiency, quality, and productivity over recent\ndecades, fundamentally reshaping how engineering designers think, behave, and\ninteract with design processes. The recent emergence of Foundation Models\n(FMs), particularly Large Language Models (LLMs), has demonstrated general\nknowledge-based reasoning capabilities, and open new paths and avenues for\nfurther transformation in engineering design. In this context, this paper\nintroduces Intelligent Design 4.0 (ID 4.0) as an emerging paradigm empowered by\nagentic AI systems. We review the historical evolution of ID across four\ndistinct stages: rule-based expert systems, task-specific machine learning\nmodels, large-scale foundation AI models, and the recent emerging paradigm of\nmulti-agent collaboration. We propose a conceptual framework for ID 4.0 and\ndiscuss its potential to support end-to-end automation of engineering design\nprocesses through coordinated, autonomous multi-agent-based systems.\nFurthermore, we discuss future perspectives to enhance and fully realize ID\n4.0's potential, including more complex design scenarios, more practical design\nimplementations, novel agent coordination mechanisms, and autonomous design\ngoal-setting with better human value alignment. In sum, these insights lay a\nfoundation for advancing Intelligent Design toward greater adaptivity,\nautonomy, and effectiveness in addressing increasingly complex design\nchallenges.", "AI": {"tldr": "本文介绍了智能设计4.0（ID 4.0）这一新兴范式，由基于多代理协作的AI系统驱动，旨在实现工程设计的端到端自动化。", "motivation": "探讨如何利用基础模型（如大型语言模型）和多代理协作系统，进一步推动智能设计的发展，提升工程设计的效率和质量。", "method": "回顾智能设计的历史演变，提出ID 4.0的概念框架，并讨论其通过多代理协作实现自动化设计的潜力。", "result": "ID 4.0有望通过协调的、自主的多代理系统，实现更复杂的工程设计自动化。", "conclusion": "ID 4.0为智能设计提供了更高的适应性、自主性和有效性，为应对日益复杂的设计挑战奠定了基础。"}}
{"id": "2506.09162", "pdf": "https://arxiv.org/pdf/2506.09162", "abs": "https://arxiv.org/abs/2506.09162", "authors": ["Tyler J. Richards", "Adam E. Flanders", "Errol Colak", "Luciano M. Prevedello", "Robyn L. Ball", "Felipe Kitamura", "John Mongan", "Maryam Vazirabad", "Hui-Ming Lin", "Anne Kendell", "Thanat Kanthawang", "Salita Angkurawaranon", "Emre Altinmakas", "Hakan Dogan", "Paulo Eduardo de Aguiar Kuriki", "Arjuna Somasundaram", "Christopher Ruston", "Deniz Bulja", "Naida Spahovic", "Jennifer Sommer", "Sirui Jiang", "Eduardo Moreno Judice de Mattos Farina", "Eduardo Caminha Nunes", "Michael Brassil", "Megan McNamara", "Johanna Ortiz", "Jacob Peoples", "Vinson L. Uytana", "Anthony Kam", "Venkata N. S. Dola", "Daniel Murphy", "David Vu", "Dataset Contributor Group", "Dataset Annotator Group", "Competition Data Notebook Group", "Jason F. Talbott"], "title": "The RSNA Lumbar Degenerative Imaging Spine Classification (LumbarDISC) Dataset", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "The Radiological Society of North America (RSNA) Lumbar Degenerative Imaging\nSpine Classification (LumbarDISC) dataset is the largest publicly available\ndataset of adult MRI lumbar spine examinations annotated for degenerative\nchanges. The dataset includes 2,697 patients with a total of 8,593 image series\nfrom 8 institutions across 6 countries and 5 continents. The dataset is\navailable for free for non-commercial use via Kaggle and RSNA Medical Imaging\nResource of AI (MIRA). The dataset was created for the RSNA 2024 Lumbar Spine\nDegenerative Classification competition where competitors developed deep\nlearning models to grade degenerative changes in the lumbar spine. The degree\nof spinal canal, subarticular recess, and neural foraminal stenosis was graded\nat each intervertebral disc level in the lumbar spine. The images were\nannotated by expert volunteer neuroradiologists and musculoskeletal\nradiologists from the RSNA, American Society of Neuroradiology, and the\nAmerican Society of Spine Radiology. This dataset aims to facilitate research\nand development in machine learning and lumbar spine imaging to lead to\nimproved patient care and clinical efficiency.", "AI": {"tldr": "RSNA LumbarDISC数据集是最大的公开MRI腰椎退行性病变标注数据集，包含2,697名患者的8,593个图像系列，用于深度学习模型竞赛，旨在推动机器学习和腰椎影像研究。", "motivation": "为促进机器学习和腰椎影像研究，改善患者护理和临床效率，创建了这一大规模标注数据集。", "method": "数据集包含多机构、多国家的MRI图像，由专家放射科医生标注腰椎退行性病变程度，用于深度学习模型开发。", "result": "数据集免费提供，支持非商业用途，已在Kaggle和RSNA MIRA平台发布。", "conclusion": "LumbarDISC数据集为机器学习和腰椎影像研究提供了重要资源，有望推动临床应用的进步。"}}
{"id": "2506.09769", "pdf": "https://arxiv.org/pdf/2506.09769", "abs": "https://arxiv.org/abs/2506.09769", "authors": ["Haruki Kainuma", "Takayuki Nishio"], "title": "Load-Aware Training Scheduling for Model Circulation-based Decentralized Federated Learning", "categories": ["cs.LG", "cs.AI"], "comment": "6 pages, submitted to IEEE Globecom 2025 (under review)", "summary": "This paper proposes Load-aware Tram-FL, an extension of Tram-FL that\nintroduces a training scheduling mechanism to minimize total training time in\ndecentralized federated learning by accounting for both computational and\ncommunication loads. The scheduling problem is formulated as a global\noptimization task, which-though intractable in its original form-is made\nsolvable by decomposing it into node-wise subproblems. To promote balanced data\nutilization under non-IID distributions, a variance constraint is introduced,\nwhile the overall training latency, including both computation and\ncommunication costs, is minimized through the objective function. Simulation\nresults on MNIST and CIFAR-10 demonstrate that Load-aware Tram-FL significantly\nreduces training time and accelerates convergence compared to baseline methods.", "AI": {"tldr": "Load-aware Tram-FL通过引入训练调度机制，优化分散式联邦学习中的计算和通信负载，显著减少训练时间并加速收敛。", "motivation": "分散式联邦学习中，计算和通信负载的不均衡会导致训练效率低下。本文旨在通过动态调度机制优化这些负载，提升整体训练效率。", "method": "将全局优化问题分解为节点级子问题，引入方差约束以平衡非IID数据分布下的数据利用率，并通过目标函数最小化训练延迟。", "result": "在MNIST和CIFAR-10上的仿真结果表明，Load-aware Tram-FL显著减少了训练时间并加速了收敛。", "conclusion": "Load-aware Tram-FL通过动态调度机制有效优化了分散式联邦学习的训练效率，适用于非IID数据分布场景。"}}
{"id": "2506.09172", "pdf": "https://arxiv.org/pdf/2506.09172", "abs": "https://arxiv.org/abs/2506.09172", "authors": ["Pranav Guruprasad", "Yangyue Wang", "Harshvardhan Sikka"], "title": "MultiNet: An Open-Source Software Toolkit \\& Benchmark Suite for the Evaluation and Adaptation of Multimodal Action Models", "categories": ["cs.LG", "cs.CV"], "comment": "ICML CodeML Workshop, 13 Pages, 6 Figures, 2 Tables", "summary": "Recent innovations in multimodal action models represent a promising\ndirection for developing general-purpose agentic systems, combining visual\nunderstanding, language comprehension, and action generation. We introduce\nMultiNet - a novel, fully open-source benchmark and surrounding software\necosystem designed to rigorously evaluate and adapt models across vision,\nlanguage, and action domains. We establish standardized evaluation protocols\nfor assessing vision-language models (VLMs) and vision-language-action models\n(VLAs), and provide open source software to download relevant data, models, and\nevaluations. Additionally, we provide a composite dataset with over 1.3\ntrillion tokens of image captioning, visual question answering, commonsense\nreasoning, robotic control, digital game-play, simulated\nlocomotion/manipulation, and many more tasks. The MultiNet benchmark,\nframework, toolkit, and evaluation harness have been used in downstream\nresearch on the limitations of VLA generalization.", "AI": {"tldr": "MultiNet是一个开源的基准测试和软件生态系统，用于评估和适应视觉、语言和动作领域的模型，提供标准化评估协议和复合数据集。", "motivation": "开发通用智能代理系统需要结合视觉理解、语言理解和动作生成，MultiNet旨在填补这一领域的评估和适应工具的空白。", "method": "MultiNet通过开源软件生态系统提供标准化评估协议、数据集和工具，涵盖多种任务如视觉问答、机器人控制等。", "result": "MultiNet包含超过1.3万亿标记的复合数据集，并已用于下游研究，揭示了视觉语言动作模型的泛化局限性。", "conclusion": "MultiNet为视觉语言动作模型的研究提供了重要工具和基准，推动了该领域的进一步发展。"}}
{"id": "2506.09217", "pdf": "https://arxiv.org/pdf/2506.09217", "abs": "https://arxiv.org/abs/2506.09217", "authors": ["Boyu Jiang", "Liang Shi", "Zhengzhi Lin", "Loren Stowe", "Feng Guo"], "title": "Perception Characteristics Distance: Measuring Stability and Robustness of Perception System in Dynamic Conditions under a Certain Decision Rule", "categories": ["cs.RO", "cs.CV", "stat.AP"], "comment": null, "summary": "The performance of perception systems in autonomous driving systems (ADS) is\nstrongly influenced by object distance, scene dynamics, and environmental\nconditions such as weather. AI-based perception outputs are inherently\nstochastic, with variability driven by these external factors, while\ntraditional evaluation metrics remain static and event-independent, failing to\ncapture fluctuations in confidence over time. In this work, we introduce the\nPerception Characteristics Distance (PCD) -- a novel evaluation metric that\nquantifies the farthest distance at which an object can be reliably detected,\nincorporating uncertainty in model outputs. To support this, we present the\nSensorRainFall dataset, collected on the Virginia Smart Road using a\nsensor-equipped vehicle (cameras, radar, LiDAR) under controlled daylight-clear\nand daylight-rain scenarios, with precise ground-truth distances to the target\nobjects. Statistical analysis reveals the presence of change points in the\nvariance of detection confidence score with distance. By averaging the PCD\nvalues across a range of detection quality thresholds and probabilistic\nthresholds, we compute the mean PCD (mPCD), which captures the overall\nperception characteristics of a system with respect to detection distance.\nApplying state-of-the-art perception models shows that mPCD captures meaningful\nreliability differences under varying weather conditions -- differences that\nstatic metrics overlook. PCD provides a principled, distribution-aware measure\nof perception performance, supporting safer and more robust ADS operation,\nwhile the SensorRainFall dataset offers a valuable benchmark for evaluation.\nThe SensorRainFall dataset is publicly available at\nhttps://www.kaggle.com/datasets/datadrivenwheels/sensorrainfall, and the\nevaluation code is open-sourced at\nhttps://github.com/datadrivenwheels/PCD_Python.", "AI": {"tldr": "本文提出了一种新的评估指标PCD，用于量化自动驾驶系统中感知系统的最远可靠检测距离，并引入了SensorRainFall数据集以支持评估。", "motivation": "自动驾驶感知系统的性能受物体距离、场景动态和环境条件（如天气）影响，传统静态评估指标无法捕捉置信度的波动。", "method": "提出PCD指标，结合模型输出的不确定性，并使用SensorRainFall数据集进行验证。", "result": "PCD能捕捉不同天气条件下的感知可靠性差异，而静态指标无法做到。", "conclusion": "PCD为感知性能提供了分布感知的评估方法，支持更安全的自动驾驶系统运行。"}}
{"id": "2506.09785", "pdf": "https://arxiv.org/pdf/2506.09785", "abs": "https://arxiv.org/abs/2506.09785", "authors": ["Alexander Marusov", "Alexander Yuhay", "Alexey Zaytsev"], "title": "A theoretical framework for self-supervised contrastive learning for continuous dependent data", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Self-supervised learning (SSL) has emerged as a powerful approach to learning\nrepresentations, particularly in the field of computer vision. However, its\napplication to dependent data, such as temporal and spatio-temporal domains,\nremains underexplored. Besides, traditional contrastive SSL methods often\nassume \\emph{semantic independence between samples}, which does not hold for\ndependent data exhibiting complex correlations. We propose a novel theoretical\nframework for contrastive SSL tailored to \\emph{continuous dependent data},\nwhich allows the nearest samples to be semantically close to each other. In\nparticular, we propose two possible \\textit{ground truth similarity measures}\nbetween objects -- \\emph{hard} and \\emph{soft} closeness. Under it, we derive\nan analytical form for the \\textit{estimated similarity matrix} that\naccommodates both types of closeness between samples, thereby introducing\ndependency-aware loss functions. We validate our approach, \\emph{Dependent\nTS2Vec}, on temporal and spatio-temporal downstream problems. Given the\ndependency patterns presented in the data, our approach surpasses modern ones\nfor dependent data, highlighting the effectiveness of our theoretically\ngrounded loss functions for SSL in capturing spatio-temporal dependencies.\nSpecifically, we outperform TS2Vec on the standard UEA and UCR benchmarks, with\naccuracy improvements of $4.17$\\% and $2.08$\\%, respectively. Furthermore, on\nthe drought classification task, which involves complex spatio-temporal\npatterns, our method achieves a $7$\\% higher ROC-AUC score.", "AI": {"tldr": "提出了一种针对连续依赖数据的对比自监督学习框架，通过硬和软相似性度量改进传统方法，显著提升了时空依赖数据的表现。", "motivation": "传统对比自监督学习方法假设样本间语义独立，不适用于依赖数据（如时空数据），需要新的理论框架。", "method": "提出依赖感知的对比学习框架，引入硬和软相似性度量，并推导出适应依赖关系的相似性矩阵和损失函数。", "result": "在UEA和UCR基准测试中分别提升4.17%和2.08%的准确率，干旱分类任务中ROC-AUC提高7%。", "conclusion": "依赖感知的对比学习方法能有效捕捉时空依赖关系，显著优于现有方法。"}}
{"id": "2506.09353", "pdf": "https://arxiv.org/pdf/2506.09353", "abs": "https://arxiv.org/abs/2506.09353", "authors": ["Yitong Zhang", "Jia Li", "Liyi Cai", "Ge Li"], "title": "DAVSP: Safety Alignment for Large Vision-Language Models via Deep Aligned Visual Safety Prompt", "categories": ["cs.CR", "cs.CV"], "comment": "16 pages", "summary": "Large Vision-Language Models (LVLMs) have achieved impressive progress across\nvarious applications but remain vulnerable to malicious queries that exploit\nthe visual modality. Existing alignment approaches typically fail to resist\nmalicious queries while preserving utility on benign ones effectively. To\naddress these challenges, we propose Deep Aligned Visual Safety Prompt (DAVSP),\nwhich is built upon two key innovations. First, we introduce the Visual Safety\nPrompt, which appends a trainable padding region around the input image. It\npreserves visual features and expands the optimization space. Second, we\npropose Deep Alignment, a novel approach to train the visual safety prompt\nthrough supervision in the model's activation space. It enhances the inherent\nability of LVLMs to perceive malicious queries, achieving deeper alignment than\nprior works. Extensive experiments across five benchmarks on two representative\nLVLMs demonstrate that DAVSP effectively resists malicious queries while\npreserving benign input utility. Furthermore, DAVSP exhibits great cross-model\ngeneration ability. Ablation studies further reveal that both the Visual Safety\nPrompt and Deep Alignment are essential components, jointly contributing to its\noverall effectiveness. The code is publicly available at\nhttps://github.com/zhangyitonggg/DAVSP.", "AI": {"tldr": "DAVSP通过视觉安全提示和深度对齐技术，有效抵御恶意查询并保留良性输入功能。", "motivation": "大型视觉语言模型（LVLMs）易受恶意查询攻击，现有对齐方法无法同时有效抵御攻击并保留功能。", "method": "提出视觉安全提示（在输入图像周围添加可训练填充区域）和深度对齐（通过激活空间监督训练提示）。", "result": "在五个基准测试中，DAVSP有效抵御恶意查询并保留良性输入功能，且具有跨模型泛化能力。", "conclusion": "DAVSP通过视觉安全提示和深度对齐的联合作用，显著提升了模型的安全性和实用性。"}}
{"id": "2506.09822", "pdf": "https://arxiv.org/pdf/2506.09822", "abs": "https://arxiv.org/abs/2506.09822", "authors": ["Rebecca Loubet", "Pascal Zittlau", "Marco Hoffmann", "Luisa Vollmer", "Sophie Fellenz", "Heike Leitte", "Fabian Jirasek", "Johannes Lenhard", "Hans Hasse"], "title": "Superstudent intelligence in thermodynamics", "categories": ["cs.CE", "cs.AI"], "comment": "This document is the unedited Author's version of a yet to be\n  Submitted Work to Physical Review Physics Education Research. 15 pages, 2\n  figures, Graphical Abstract, Highlights and SI available (12 pages)", "summary": "In this short note, we report and analyze a striking event: OpenAI's large\nlanguage model o3 has outwitted all students in a university exam on\nthermodynamics. The thermodynamics exam is a difficult hurdle for most\nstudents, where they must show that they have mastered the fundamentals of this\nimportant topic. Consequently, the failure rates are very high, A-grades are\nrare - and they are considered proof of the students' exceptional intellectual\nabilities. This is because pattern learning does not help in the exam. The\nproblems can only be solved by knowledgeably and creatively combining\nprinciples of thermodynamics. We have given our latest thermodynamics exam not\nonly to the students but also to OpenAI's most powerful reasoning model, o3,\nand have assessed the answers of o3 exactly the same way as those of the\nstudents. In zero-shot mode, the model o3 solved all problems correctly, better\nthan all students who took the exam; its overall score was in the range of the\nbest scores we have seen in more than 10,000 similar exams since 1985. This is\na turning point: machines now excel in complex tasks, usually taken as proof of\nhuman intellectual capabilities. We discuss the consequences this has for the\nwork of engineers and the education of future engineers.", "AI": {"tldr": "OpenAI的o3模型在热力学考试中表现优于所有学生，展示了AI在复杂任务中的卓越能力。", "motivation": "探讨AI在传统上被认为需要人类智力的任务中的表现，及其对工程教育和实践的影响。", "method": "将OpenAI的o3模型与学生一起参加热力学考试，并采用相同的评分标准。", "result": "o3在零样本模式下正确解决了所有问题，成绩超过所有学生，达到历史最佳水平。", "conclusion": "AI在复杂任务中的优异表现标志着技术转折点，对工程师工作和教育提出新挑战。"}}
{"id": "2506.09491", "pdf": "https://arxiv.org/pdf/2506.09491", "abs": "https://arxiv.org/abs/2506.09491", "authors": ["Guanghu Xie", "Zhiduo Jiang", "Yonglong Zhang", "Yang Liu", "Zongwu Xie", "Baoshi Cao", "Hong Liu"], "title": "DCIRNet: Depth Completion with Iterative Refinement for Dexterous Grasping of Transparent and Reflective Objects", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Transparent and reflective objects in everyday environments pose significant\nchallenges for depth sensors due to their unique visual properties, such as\nspecular reflections and light transmission. These characteristics often lead\nto incomplete or inaccurate depth estimation, which severely impacts downstream\ngeometry-based vision tasks, including object recognition, scene\nreconstruction, and robotic manipulation. To address the issue of missing depth\ninformation in transparent and reflective objects, we propose DCIRNet, a novel\nmultimodal depth completion network that effectively integrates RGB images and\ndepth maps to enhance depth estimation quality. Our approach incorporates an\ninnovative multimodal feature fusion module designed to extract complementary\ninformation between RGB images and incomplete depth maps. Furthermore, we\nintroduce a multi-stage supervision and depth refinement strategy that\nprogressively improves depth completion and effectively mitigates the issue of\nblurred object boundaries. We integrate our depth completion model into\ndexterous grasping frameworks and achieve a $44\\%$ improvement in the grasp\nsuccess rate for transparent and reflective objects. We conduct extensive\nexperiments on public datasets, where DCIRNet demonstrates superior\nperformance. The experimental results validate the effectiveness of our\napproach and confirm its strong generalization capability across various\ntransparent and reflective objects.", "AI": {"tldr": "DCIRNet是一种新型多模态深度补全网络，通过融合RGB图像和深度图提升透明和反射物体的深度估计质量，显著提高了抓取成功率。", "motivation": "透明和反射物体的独特视觉特性（如镜面反射和透光）导致深度传感器估计不完整或不准确，影响下游视觉任务。", "method": "提出DCIRNet，结合RGB图像和深度图，采用多模态特征融合模块和多阶段监督与深度细化策略。", "result": "在公开数据集上表现优异，抓取成功率提升44%，验证了方法的有效性和泛化能力。", "conclusion": "DCIRNet通过多模态融合和渐进式优化，显著改善了透明和反射物体的深度估计，适用于实际应用。"}}
{"id": "2506.09552", "pdf": "https://arxiv.org/pdf/2506.09552", "abs": "https://arxiv.org/abs/2506.09552", "authors": ["Fatemeh Mohammadi Amin", "Darwin G. Caldwell", "Hans Wernher van de Venn"], "title": "Enhancing Human-Robot Collaboration: A Sim2Real Domain Adaptation Algorithm for Point Cloud Segmentation in Industrial Environments", "categories": ["cs.RO", "cs.CV"], "comment": "Preprint, Journal of Intelligent & Robotic Systems", "summary": "The robust interpretation of 3D environments is crucial for human-robot\ncollaboration (HRC) applications, where safety and operational efficiency are\nparamount. Semantic segmentation plays a key role in this context by enabling a\nprecise and detailed understanding of the environment. Considering the intense\ndata hunger for real-world industrial annotated data essential for effective\nsemantic segmentation, this paper introduces a pioneering approach in the\nSim2Real domain adaptation for semantic segmentation of 3D point cloud data,\nspecifically tailored for HRC. Our focus is on developing a network that\nrobustly transitions from simulated environments to real-world applications,\nthereby enhancing its practical utility and impact on a safe HRC.\n  In this work, we propose a dual-stream network architecture (FUSION)\ncombining Dynamic Graph Convolutional Neural Networks (DGCNN) and Convolutional\nNeural Networks (CNN) augmented with residual layers as a Sim2Real domain\nadaptation algorithm for an industrial environment. The proposed model was\nevaluated on real-world HRC setups and simulation industrial point clouds, it\nshowed increased state-of-the-art performance, achieving a segmentation\naccuracy of 97.76%, and superior robustness compared to existing methods.", "AI": {"tldr": "论文提出了一种名为FUSION的双流网络架构，结合DGCNN和CNN，用于3D点云数据的Sim2Real域适应，显著提升了语义分割在工业环境中的性能。", "motivation": "在人类-机器人协作（HRC）中，3D环境的鲁棒解释对安全和效率至关重要，但现实工业数据的标注需求高，因此需要从模拟环境到现实应用的域适应方法。", "method": "提出FUSION双流网络架构，结合DGCNN和CNN，并加入残差层，作为Sim2Real域适应算法。", "result": "在真实HRC和模拟工业点云数据上评估，模型达到97.76%的分割准确率，性能优于现有方法。", "conclusion": "FUSION架构显著提升了语义分割的鲁棒性和实用性，为安全HRC提供了有效解决方案。"}}
{"id": "2506.09638", "pdf": "https://arxiv.org/pdf/2506.09638", "abs": "https://arxiv.org/abs/2506.09638", "authors": ["Weiying Zheng", "Ziyue Lin", "Pengxin Guo", "Yuyin Zhou", "Feifei Wang", "Liangqiong Qu"], "title": "FedVLMBench: Benchmarking Federated Fine-Tuning of Vision-Language Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) have demonstrated remarkable capabilities in\ncross-modal understanding and generation by integrating visual and textual\ninformation. While instruction tuning and parameter-efficient fine-tuning\nmethods have substantially improved the generalization of VLMs, most existing\napproaches rely on centralized training, posing challenges for deployment in\ndomains with strict privacy requirements like healthcare. Recent efforts have\nintroduced Federated Learning (FL) into VLM fine-tuning to address these\nprivacy concerns, yet comprehensive benchmarks for evaluating federated\nfine-tuning strategies, model architectures, and task generalization remain\nlacking. In this work, we present \\textbf{FedVLMBench}, the first systematic\nbenchmark for federated fine-tuning of VLMs. FedVLMBench integrates two\nmainstream VLM architectures (encoder-based and encoder-free), four fine-tuning\nstrategies, five FL algorithms, six multimodal datasets spanning four\ncross-domain single-task scenarios and two cross-domain multitask settings,\ncovering four distinct downstream task categories. Through extensive\nexperiments, we uncover key insights into the interplay between VLM\narchitectures, fine-tuning strategies, data heterogeneity, and multi-task\nfederated optimization. Notably, we find that a 2-layer multilayer perceptron\n(MLP) connector with concurrent connector and LLM tuning emerges as the optimal\nconfiguration for encoder-based VLMs in FL. Furthermore, current FL methods\nexhibit significantly higher sensitivity to data heterogeneity in\nvision-centric tasks than text-centric ones, across both encoder-free and\nencoder-based VLM architectures. Our benchmark provides essential tools,\ndatasets, and empirical guidance for the research community, offering a\nstandardized platform to advance privacy-preserving, federated training of\nmultimodal foundation models.", "AI": {"tldr": "本文介绍了FedVLMBench，首个用于联邦学习（FL）下视觉语言模型（VLM）微调的系统性基准测试，涵盖了多种架构、策略和任务，揭示了关键发现并提供了标准化平台。", "motivation": "现有VLM微调方法多依赖集中式训练，难以满足隐私要求严格的领域（如医疗）需求，且缺乏对联邦微调策略的全面评估。", "method": "提出FedVLMBench，整合两种主流VLM架构、四种微调策略、五种FL算法、六个多模态数据集，覆盖多种任务场景。", "result": "发现2层MLP连接器与并发调优是FL下基于编码器VLM的最优配置，且FL方法对视觉中心任务的数据异质性更敏感。", "conclusion": "FedVLMBench为隐私保护的多模态基础模型联邦训练提供了标准化平台和实证指导。"}}
{"id": "2506.09661", "pdf": "https://arxiv.org/pdf/2506.09661", "abs": "https://arxiv.org/abs/2506.09661", "authors": ["Garima Jain", "Sanghamitra Pati", "Mona Duggal", "Amit Sethi", "Abhijeet Patil", "Gururaj Malekar", "Nilesh Kowe", "Jitender Kumar", "Jatin Kashyap", "Divyajeet Rout", "Deepali", "Hitesh", "Nishi Halduniya", "Sharat Kumar", "Heena Tabassum", "Rupinder Singh Dhaliwal", "Sucheta Devi Khuraijam", "Sushma Khuraijam", "Sharmila Laishram", "Simmi Kharb", "Sunita Singh", "K. Swaminadtan", "Ranjana Solanki", "Deepika Hemranjani", "Shashank Nath Singh", "Uma Handa", "Manveen Kaur", "Surinder Singhal", "Shivani Kalhan", "Rakesh Kumar Gupta", "Ravi. S", "D. Pavithra", "Sunil Kumar Mahto", "Arvind Kumar", "Deepali Tirkey", "Saurav Banerjee", "L. Sreelakshmi"], "title": "A Cytology Dataset for Early Detection of Oral Squamous Cell Carcinoma", "categories": ["eess.IV", "cs.CV", "q-bio.TO"], "comment": "7 pages, 2 figurs", "summary": "Oral squamous cell carcinoma OSCC is a major global health burden,\nparticularly in several regions across Asia, Africa, and South America, where\nit accounts for a significant proportion of cancer cases. Early detection\ndramatically improves outcomes, with stage I cancers achieving up to 90 percent\nsurvival. However, traditional diagnosis based on histopathology has limited\naccessibility in low-resource settings because it is invasive,\nresource-intensive, and reliant on expert pathologists. On the other hand, oral\ncytology of brush biopsy offers a minimally invasive and lower cost\nalternative, provided that the remaining challenges, inter observer variability\nand unavailability of expert pathologists can be addressed using artificial\nintelligence. Development and validation of robust AI solutions requires access\nto large, labeled, and multi-source datasets to train high capacity models that\ngeneralize across domain shifts. We introduce the first large and multicenter\noral cytology dataset, comprising annotated slides stained with\nPapanicolaou(PAP) and May-Grunwald-Giemsa(MGG) protocols, collected from ten\ntertiary medical centers in India. The dataset is labeled and annotated by\nexpert pathologists for cellular anomaly classification and detection, is\ndesigned to advance AI driven diagnostic methods. By filling the gap in\npublicly available oral cytology datasets, this resource aims to enhance\nautomated detection, reduce diagnostic errors, and improve early OSCC diagnosis\nin resource-constrained settings, ultimately contributing to reduced mortality\nand better patient outcomes worldwide.", "AI": {"tldr": "论文介绍了首个大型多中心口腔细胞学数据集，旨在通过AI改进口腔鳞状细胞癌（OSCC）的早期诊断，尤其在资源有限的地区。", "motivation": "传统OSCC诊断方法（如组织病理学）在资源匮乏地区难以普及，而口腔细胞学结合AI有望提供低成本、微创的替代方案。", "method": "研究团队收集了来自印度十个医疗中心的PAP和MGG染色口腔细胞学切片，由病理专家标注，用于训练和验证AI模型。", "result": "该数据集填补了公开口腔细胞学数据的空白，为开发泛化能力强的AI诊断工具奠定了基础。", "conclusion": "该资源有望提升OSCC的自动化检测能力，减少诊断错误，改善全球尤其是资源有限地区的患者预后。"}}
{"id": "2506.09862", "pdf": "https://arxiv.org/pdf/2506.09862", "abs": "https://arxiv.org/abs/2506.09862", "authors": ["Mikel Casals", "Vasilis Belis", "Elias F. Combarro", "Eduard Alarcón", "Sofia Vallecorsa", "Michele Grossi"], "title": "Guided Graph Compression for Quantum Graph Neural Networks", "categories": ["cs.LG", "cs.AI", "hep-ex", "quant-ph"], "comment": null, "summary": "Graph Neural Networks (GNNs) are effective for processing graph-structured\ndata but face challenges with large graphs due to high memory requirements and\ninefficient sparse matrix operations on GPUs. Quantum Computing (QC) offers a\npromising avenue to address these issues and inspires new algorithmic\napproaches. In particular, Quantum Graph Neural Networks (QGNNs) have been\nexplored in recent literature. However, current quantum hardware limits the\ndimension of the data that can be effectively encoded. Existing approaches\neither simplify datasets manually or use artificial graph datasets. This work\nintroduces the Guided Graph Compression (GGC) framework, which uses a graph\nautoencoder to reduce both the number of nodes and the dimensionality of node\nfeatures. The compression is guided to enhance the performance of a downstream\nclassification task, which can be applied either with a quantum or a classical\nclassifier. The framework is evaluated on the Jet Tagging task, a\nclassification problem of fundamental importance in high energy physics that\ninvolves distinguishing particle jets initiated by quarks from those by gluons.\nThe GGC is compared against using the autoencoder as a standalone preprocessing\nstep and against a baseline classical GNN classifier. Our numerical results\ndemonstrate that GGC outperforms both alternatives, while also facilitating the\ntesting of novel QGNN ansatzes on realistic datasets.", "AI": {"tldr": "论文提出了一种名为GGC的框架，通过图自动编码器压缩图数据，以提升量子或经典分类器的性能，并在高能物理的Jet Tagging任务中验证其优越性。", "motivation": "解决GNN在处理大图时的高内存需求和稀疏矩阵操作效率低的问题，同时探索量子计算在GNN中的潜力。", "method": "引入GGC框架，利用图自动编码器压缩节点数量和特征维度，并优化下游分类任务性能。", "result": "GGC在Jet Tagging任务中表现优于单独使用自动编码器或经典GNN分类器。", "conclusion": "GGC不仅提升了分类性能，还为在真实数据集上测试新型QGNN提供了便利。"}}
{"id": "2506.09665", "pdf": "https://arxiv.org/pdf/2506.09665", "abs": "https://arxiv.org/abs/2506.09665", "authors": ["Jacob Munkberg", "Zian Wang", "Ruofan Liang", "Tianchang Shen", "Jon Hasselgren"], "title": "VideoMat: Extracting PBR Materials from Video Diffusion Models", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "We leverage finetuned video diffusion models, intrinsic decomposition of\nvideos, and physically-based differentiable rendering to generate high quality\nmaterials for 3D models given a text prompt or a single image. We condition a\nvideo diffusion model to respect the input geometry and lighting condition.\nThis model produces multiple views of a given 3D model with coherent material\nproperties. Secondly, we use a recent model to extract intrinsics (base color,\nroughness, metallic) from the generated video. Finally, we use the intrinsics\nalongside the generated video in a differentiable path tracer to robustly\nextract PBR materials directly compatible with common content creation tools.", "AI": {"tldr": "通过微调视频扩散模型、视频内在分解和基于物理的可微分渲染，从文本提示或单张图像生成高质量的3D模型材质。", "motivation": "解决从文本或图像生成高质量3D模型材质的挑战，确保材质与几何和光照条件一致。", "method": "1. 使用视频扩散模型生成多视角一致的材质视频；2. 提取内在属性（基色、粗糙度、金属性）；3. 结合可微分路径追踪生成PBR材质。", "result": "生成的材质与常见内容创作工具兼容，且质量高。", "conclusion": "该方法有效结合扩散模型与物理渲染，为3D内容创作提供了高效工具。"}}
{"id": "2506.09873", "pdf": "https://arxiv.org/pdf/2506.09873", "abs": "https://arxiv.org/abs/2506.09873", "authors": ["Emma Kallina", "Thomas Bohné", "Jat Singh"], "title": "Stakeholder Participation for Responsible AI Development: Disconnects Between Guidance and Current Practice", "categories": ["cs.SE", "cs.AI", "cs.HC"], "comment": "Published at the 2025 ACM Conference on Fairness, Accountability, and\n  Transparency FAccT'25", "summary": "Responsible AI (rAI) guidance increasingly promotes stakeholder involvement\n(SHI) during AI development. At the same time, SHI is already common in\ncommercial software development, but with potentially different foci. This\nstudy clarifies the extent to which established SHI practices are able to\ncontribute to rAI efforts as well as potential disconnects -- essential\ninsights to inform and tailor future interventions that further shift industry\npractice towards rAI efforts. First, we analysed 56 rAI guidance documents to\nidentify why SHI is recommended (i.e. its expected benefits for rAI) and\nuncovered goals such as redistributing power, improving socio-technical\nunderstandings, anticipating risks, and enhancing public oversight. To\nunderstand why and how SHI is currently practised in commercial settings, we\nthen conducted an online survey (n=130) and semi-structured interviews (n=10)\nwith AI practitioners. Our findings reveal that SHI in practice is primarily\ndriven by commercial priorities (e.g. customer value, compliance) and several\nfactors currently discourage more rAI-aligned SHI practices. This suggests that\nestablished SHI practices are largely not contributing to rAI efforts. To\naddress this disconnect, we propose interventions and research opportunities to\nadvance rAI development in practice.", "AI": {"tldr": "研究探讨了商业AI开发中现有利益相关者参与（SHI）实践如何与负责任AI（rAI）目标对齐，发现两者存在脱节，并提出改进建议。", "motivation": "明确现有SHI实践对rAI目标的贡献及潜在脱节，为未来干预措施提供依据。", "method": "分析56份rAI指导文件，进行在线调查（n=130）和半结构化访谈（n=10）。", "result": "SHI实践主要受商业目标驱动，与rAI目标脱节，需针对性干预。", "conclusion": "需调整SHI实践以支持rAI目标，并提出具体干预措施和研究方向。"}}
{"id": "2506.09709", "pdf": "https://arxiv.org/pdf/2506.09709", "abs": "https://arxiv.org/abs/2506.09709", "authors": ["Alexander Lobashev", "Assel Yermekova", "Maria Larchenko"], "title": "Training-Free Voice Conversion with Factorized Optimal Transport", "categories": ["cs.SD", "cs.CV", "cs.LG", "eess.AS"], "comment": "Interspeech 2025", "summary": "This paper introduces Factorized MKL-VC, a training-free modification for\nkNN-VC pipeline. In contrast with original pipeline, our algorithm performs\nhigh quality any-to-any cross-lingual voice conversion with only 5 second of\nreference audio. MKL-VC replaces kNN regression with a factorized optimal\ntransport map in WavLM embedding subspaces, derived from Monge-Kantorovich\nLinear solution. Factorization addresses non-uniform variance across\ndimensions, ensuring effective feature transformation. Experiments on\nLibriSpeech and FLEURS datasets show MKL-VC significantly improves content\npreservation and robustness with short reference audio, outperforming kNN-VC.\nMKL-VC achieves performance comparable to FACodec, especially in cross-lingual\nvoice conversion domain.", "AI": {"tldr": "Factorized MKL-VC是一种无需训练的改进方法，用于kNN-VC流程，仅需5秒参考音频即可实现高质量跨语言语音转换。", "motivation": "解决原始kNN-VC流程在短参考音频下内容保留和鲁棒性不足的问题。", "method": "用因子化的最优传输映射（基于Monge-Kantorovich线性解）替代kNN回归，处理WavLM嵌入子空间中的非均匀方差。", "result": "在LibriSpeech和FLEURS数据集上，MKL-VC显著提升性能，尤其在跨语言语音转换中表现优于kNN-VC，接近FACodec。", "conclusion": "MKL-VC在短参考音频下高效且鲁棒，适用于跨语言语音转换。"}}
{"id": "2506.09891", "pdf": "https://arxiv.org/pdf/2506.09891", "abs": "https://arxiv.org/abs/2506.09891", "authors": ["Sebastian Hickman", "Ilija Trajkovic", "Julia Kaltenborn", "Francis Pelletier", "Alex Archibald", "Yaniv Gurwicz", "Peer Nowack", "David Rolnick", "Julien Boussard"], "title": "Causal Climate Emulation with Bayesian Filtering", "categories": ["cs.LG", "cs.AI", "cs.CE", "physics.ao-ph"], "comment": "32 pages, 21 figures", "summary": "Traditional models of climate change use complex systems of coupled equations\nto simulate physical processes across the Earth system. These simulations are\nhighly computationally expensive, limiting our predictions of climate change\nand analyses of its causes and effects. Machine learning has the potential to\nquickly emulate data from climate models, but current approaches are not able\nto incorporate physics-informed causal relationships. Here, we develop an\ninterpretable climate model emulator based on causal representation learning.\nWe derive a physics-informed approach including a Bayesian filter for stable\nlong-term autoregressive emulation. We demonstrate that our emulator learns\naccurate climate dynamics, and we show the importance of each one of its\ncomponents on a realistic synthetic dataset and data from two widely deployed\nclimate models.", "AI": {"tldr": "论文提出了一种基于因果表示学习的可解释气候模型模拟器，结合物理信息和贝叶斯滤波，能够准确模拟气候动态。", "motivation": "传统气候模型计算成本高，限制了气候变化预测和分析。机器学习虽能快速模拟数据，但缺乏物理信息的因果关系。", "method": "开发了一种基于因果表示学习的可解释模拟器，结合物理信息和贝叶斯滤波，用于稳定的长期自回归模拟。", "result": "模拟器能准确学习气候动态，并在合成数据集和两种广泛使用的气候模型数据上验证了其各组件的有效性。", "conclusion": "该方法为气候模型提供了一种高效且可解释的替代方案，有助于更快速的气候变化分析。"}}
{"id": "2506.09930", "pdf": "https://arxiv.org/pdf/2506.09930", "abs": "https://arxiv.org/abs/2506.09930", "authors": ["Irving Fang", "Juexiao Zhang", "Shengbang Tong", "Chen Feng"], "title": "From Intention to Execution: Probing the Generalization Boundaries of Vision-Language-Action Models", "categories": ["cs.RO", "cs.CV"], "comment": "Under review", "summary": "One promise that Vision-Language-Action (VLA) models hold over traditional\nimitation learning for robotics is to leverage the broad generalization\ncapabilities of large Vision-Language Models (VLMs) to produce versatile,\n\"generalist\" robot policies. However, current evaluations of VLAs remain\ninsufficient. Traditional imitation learning benchmarks are unsuitable due to\nthe lack of language instructions. Emerging benchmarks for VLAs that\nincorporate language often come with limited evaluation tasks and do not intend\nto investigate how much VLM pretraining truly contributes to the generalization\ncapabilities of the downstream robotic policy. Meanwhile, much research relies\non real-world robot setups designed in isolation by different institutions,\nwhich creates a barrier for reproducibility and accessibility. To address this\ngap, we introduce a unified probing suite of 50 simulation-based tasks across\n10 subcategories spanning language instruction, vision, and objects. We\nsystematically evaluate several state-of-the-art VLA architectures on this\nsuite to understand their generalization capability. Our results show that\nwhile VLM backbones endow VLAs with robust perceptual understanding and high\nlevel planning, which we refer to as good intentions, this does not reliably\ntranslate into precise motor execution: when faced with out-of-distribution\nobservations, policies often exhibit coherent intentions, but falter in action\nexecution. Moreover, finetuning on action data can erode the original VLM's\ngeneralist reasoning abilities. We release our task suite and evaluation code\nto serve as a standardized benchmark for future VLAs and to drive research on\nclosing the perception-to-action gap. More information, including the source\ncode, can be found at https://ai4ce.github.io/INT-ACT/", "AI": {"tldr": "该论文提出了一个统一的仿真任务套件，用于评估视觉-语言-动作（VLA）模型的泛化能力，发现VLM预训练虽能提升感知和规划能力，但在动作执行上表现不稳定。", "motivation": "当前VLA模型的评估不足，缺乏语言指令的基准测试，且现有基准任务有限，无法充分验证VLM预训练对机器人策略泛化能力的贡献。", "method": "引入包含50个仿真任务的统一测试套件，涵盖语言指令、视觉和物体操作，并系统评估多种VLA架构的泛化能力。", "result": "VLM骨干网络赋予VLA模型强大的感知和高层规划能力（\"良好意图\"），但在动作执行上表现不稳定，且微调可能损害VLM的泛化推理能力。", "conclusion": "发布任务套件和评估代码作为标准化基准，推动研究弥合感知与动作之间的差距。"}}
{"id": "2506.09934", "pdf": "https://arxiv.org/pdf/2506.09934", "abs": "https://arxiv.org/abs/2506.09934", "authors": ["Jared Lawson", "Rohan Chitale", "Nabil Simaan"], "title": "Fluoroscopic Shape and Pose Tracking of Catheters with Custom Radiopaque Markers", "categories": ["cs.RO", "cs.CV"], "comment": "8 pages, 5 figures, accepted in Robotics and Automation Letters", "summary": "Safe navigation of steerable and robotic catheters in the cerebral\nvasculature requires awareness of the catheters shape and pose. Currently, a\nsignificant perception burden is placed on interventionalists to mentally\nreconstruct and predict catheter motions from biplane fluoroscopy images.\nEfforts to track these catheters are limited to planar segmentation or bulky\nsensing instrumentation, which are incompatible with microcatheters used in\nneurointervention. In this work, a catheter is equipped with custom radiopaque\nmarkers arranged to enable simultaneous shape and pose estimation under biplane\nfluoroscopy. A design measure is proposed to guide the arrangement of these\nmarkers to minimize sensitivity to marker tracking uncertainty. This approach\nwas deployed for microcatheters smaller than 2mm OD navigating phantom\nvasculature with shape tracking errors less than 1mm and catheter roll errors\nbelow 40 degrees. This work can enable steerable catheters to autonomously\nnavigate under biplane imaging.", "AI": {"tldr": "论文提出了一种通过定制不透射线标记物实现微导管形状和姿态同步估计的方法，以减少神经介入手术中的感知负担。", "motivation": "当前神经介入手术中，医生需从双平面透视图像中重建导管形状和预测运动，感知负担重，现有追踪方法不适用于微导管。", "method": "在导管上布置定制不透射线标记物，设计标记物排列以减少对追踪不确定性的敏感性。", "result": "在直径小于2mm的微导管上部署该方法，形状追踪误差小于1mm，导管滚动误差低于40度。", "conclusion": "该方法可实现双平面成像下导管的自主导航。"}}
{"id": "2506.09949", "pdf": "https://arxiv.org/pdf/2506.09949", "abs": "https://arxiv.org/abs/2506.09949", "authors": ["Mahrokh Najaf", "Gregory Ongie"], "title": "Sampling Theory for Super-Resolution with Implicit Neural Representations", "categories": ["eess.IV", "cs.CV"], "comment": "arXiv admin note: text overlap with arXiv:2405.18410", "summary": "Implicit neural representations (INRs) have emerged as a powerful tool for\nsolving inverse problems in computer vision and computational imaging. INRs\nrepresent images as continuous domain functions realized by a neural network\ntaking spatial coordinates as inputs. However, unlike traditional pixel\nrepresentations, little is known about the sample complexity of estimating\nimages using INRs in the context of linear inverse problems. Towards this end,\nwe study the sampling requirements for recovery of a continuous domain image\nfrom its low-pass Fourier samples by fitting a single hidden-layer INR with\nReLU activation and a Fourier features layer using a generalized form of weight\ndecay regularization. Our key insight is to relate minimizers of this\nnon-convex parameter space optimization problem to minimizers of a convex\npenalty defined over an infinite-dimensional space of measures. We identify a\nsufficient number of Fourier samples for which an image realized by an INR is\nexactly recoverable by solving the INR training problem. To validate our\ntheory, we empirically assess the probability of achieving exact recovery of\nimages realized by low-width single hidden-layer INRs, and illustrate the\nperformance of INRs on super-resolution recovery of continuous domain phantom\nimages.", "AI": {"tldr": "研究了隐式神经表示（INRs）在解决线性逆问题中的样本复杂度，提出了一种基于傅里叶样本的图像恢复方法，并通过实验验证了其有效性。", "motivation": "隐式神经表示（INRs）在计算机视觉和计算成像中表现出强大的逆问题解决能力，但其样本复杂度尚未明确。本文旨在填补这一空白。", "method": "使用单隐藏层ReLU激活的INR和傅里叶特征层，结合广义权重衰减正则化，从低通傅里叶样本中恢复连续域图像。", "result": "确定了INR图像可通过解决训练问题精确恢复的傅里叶样本数量，并通过实验验证了低宽度INR的精确恢复概率。", "conclusion": "INRs在连续域图像的超分辨率恢复中表现出色，理论分析为其在逆问题中的应用提供了支持。"}}
{"id": "2506.09937", "pdf": "https://arxiv.org/pdf/2506.09937", "abs": "https://arxiv.org/abs/2506.09937", "authors": ["Qiao Gu", "Yuanliang Ju", "Shengxiang Sun", "Igor Gilitschenski", "Haruki Nishimura", "Masha Itkina", "Florian Shkurti"], "title": "SAFE: Multitask Failure Detection for Vision-Language-Action Models", "categories": ["cs.RO", "cs.AI"], "comment": "Project Page: https://vla-safe.github.io/", "summary": "While vision-language-action models (VLAs) have shown promising robotic\nbehaviors across a diverse set of manipulation tasks, they achieve limited\nsuccess rates when deployed on novel tasks out-of-the-box. To allow these\npolicies to safely interact with their environments, we need a failure detector\nthat gives a timely alert such that the robot can stop, backtrack, or ask for\nhelp. However, existing failure detectors are trained and tested only on one or\na few specific tasks, while VLAs require the detector to generalize and detect\nfailures also in unseen tasks and novel environments. In this paper, we\nintroduce the multitask failure detection problem and propose SAFE, a failure\ndetector for generalist robot policies such as VLAs. We analyze the VLA feature\nspace and find that VLAs have sufficient high-level knowledge about task\nsuccess and failure, which is generic across different tasks. Based on this\ninsight, we design SAFE to learn from VLA internal features and predict a\nsingle scalar indicating the likelihood of task failure. SAFE is trained on\nboth successful and failed rollouts, and is evaluated on unseen tasks. SAFE is\ncompatible with different policy architectures. We test it on OpenVLA, $\\pi_0$,\nand $\\pi_0$-FAST in both simulated and real-world environments extensively. We\ncompare SAFE with diverse baselines and show that SAFE achieves\nstate-of-the-art failure detection performance and the best trade-off between\naccuracy and detection time using conformal prediction. More qualitative\nresults can be found at https://vla-safe.github.io/.", "AI": {"tldr": "本文提出了SAFE，一种用于通用机器人策略（如VLA）的故障检测器，能够跨任务和环境泛化检测失败。", "motivation": "现有故障检测器仅针对特定任务训练和测试，而VLA需要检测器在未见任务和新环境中也能泛化。", "method": "通过分析VLA特征空间，发现其具备跨任务的高层知识，据此设计SAFE，利用VLA内部特征预测失败可能性。", "result": "SAFE在模拟和真实环境中测试，表现优于基线，实现了最佳准确性和检测时间的平衡。", "conclusion": "SAFE为通用机器人策略提供了高效的故障检测解决方案，具有广泛适用性。"}}
{"id": "2506.09955", "pdf": "https://arxiv.org/pdf/2506.09955", "abs": "https://arxiv.org/abs/2506.09955", "authors": ["Yitao Xu", "Tong Zhang", "Ehsan Pajouheshgar", "Sabine Süsstrunk"], "title": "Canonical Latent Representations in Conditional Diffusion Models", "categories": ["cs.LG", "cs.CV"], "comment": "45 pages,41 figures", "summary": "Conditional diffusion models (CDMs) have shown impressive performance across\na range of generative tasks. Their ability to model the full data distribution\nhas opened new avenues for analysis-by-synthesis in downstream discriminative\nlearning. However, this same modeling capacity causes CDMs to entangle the\nclass-defining features with irrelevant context, posing challenges to\nextracting robust and interpretable representations. To this end, we identify\nCanonical LAtent Representations (CLAReps), latent codes whose internal CDM\nfeatures preserve essential categorical information while discarding\nnon-discriminative signals. When decoded, CLAReps produce representative\nsamples for each class, offering an interpretable and compact summary of the\ncore class semantics with minimal irrelevant details. Exploiting CLAReps, we\ndevelop a novel diffusion-based feature-distillation paradigm, CaDistill. While\nthe student has full access to the training set, the CDM as teacher transfers\ncore class knowledge only via CLAReps, which amounts to merely 10 % of the\ntraining data in size. After training, the student achieves strong adversarial\nrobustness and generalization ability, focusing more on the class signals\ninstead of spurious background cues. Our findings suggest that CDMs can serve\nnot just as image generators but also as compact, interpretable teachers that\ncan drive robust representation learning.", "AI": {"tldr": "论文提出了一种名为CLAReps的潜在表示方法，用于从条件扩散模型中提取类别核心特征，同时剔除无关信息，并开发了基于扩散的特征蒸馏框架CaDistill。", "motivation": "条件扩散模型（CDMs）在生成任务中表现优异，但其建模能力导致类别特征与无关上下文纠缠，难以提取鲁棒且可解释的表征。", "method": "通过识别CLAReps（保留类别核心信息的潜在代码），开发了CaDistill框架，利用CDM作为教师模型，仅通过CLAReps传递核心类别知识。", "result": "学生模型在训练后表现出强大的对抗鲁棒性和泛化能力，更专注于类别信号而非虚假背景线索。", "conclusion": "研究表明，CDMs不仅可以作为图像生成器，还能作为紧凑、可解释的教师模型，推动鲁棒表征学习。"}}
{"id": "2506.09940", "pdf": "https://arxiv.org/pdf/2506.09940", "abs": "https://arxiv.org/abs/2506.09940", "authors": ["Jiachen Hu", "Rui Ai", "Han Zhong", "Xiaoyu Chen", "Liwei Wang", "Zhaoran Wang", "Zhuoran Yang"], "title": "The Sample Complexity of Online Strategic Decision Making with Information Asymmetry and Knowledge Transportability", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Accepted at ICML 2025", "summary": "Information asymmetry is a pervasive feature of multi-agent systems,\nespecially evident in economics and social sciences. In these settings, agents\ntailor their actions based on private information to maximize their rewards.\nThese strategic behaviors often introduce complexities due to confounding\nvariables. Simultaneously, knowledge transportability poses another significant\nchallenge, arising from the difficulties of conducting experiments in target\nenvironments. It requires transferring knowledge from environments where\nempirical data is more readily available. Against these backdrops, this paper\nexplores a fundamental question in online learning: Can we employ non-i.i.d.\nactions to learn about confounders even when requiring knowledge transfer? We\npresent a sample-efficient algorithm designed to accurately identify system\ndynamics under information asymmetry and to navigate the challenges of\nknowledge transfer effectively in reinforcement learning, framed within an\nonline strategic interaction model. Our method provably achieves learning of an\n$\\epsilon$-optimal policy with a tight sample complexity of $O(1/\\epsilon^2)$.", "AI": {"tldr": "本文提出了一种样本高效的算法，用于在信息不对称和知识迁移的挑战下，通过非独立同分布动作学习混杂变量，并在强化学习中实现最优策略。", "motivation": "信息不对称和知识迁移是多智能体系统中的常见问题，导致策略行为的复杂性增加。本文旨在解决这些挑战，探索在线学习中如何利用非独立同分布动作学习混杂变量。", "method": "提出了一种样本高效的算法，通过在线战略交互模型，准确识别系统动态并有效应对知识迁移问题。", "result": "算法在强化学习中实现了学习ε-最优策略，样本复杂度为O(1/ε²)。", "conclusion": "该方法在信息不对称和知识迁移的背景下，提供了一种有效的解决方案，能够高效学习最优策略。"}}
{"id": "2506.09990", "pdf": "https://arxiv.org/pdf/2506.09990", "abs": "https://arxiv.org/abs/2506.09990", "authors": ["Wenbo Zhang", "Tianrun Hu", "Yanyuan Qiao", "Hanbo Zhang", "Yuchu Qin", "Yang Li", "Jiajun Liu", "Tao Kong", "Lingqiao Liu", "Xiao Ma"], "title": "Chain-of-Action: Trajectory Autoregressive Modeling for Robotic Manipulation", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": null, "summary": "We present Chain-of-Action (CoA), a novel visuo-motor policy paradigm built\nupon Trajectory Autoregressive Modeling. Unlike conventional approaches that\npredict next step action(s) forward, CoA generates an entire trajectory by\nexplicit backward reasoning with task-specific goals through an action-level\nChain-of-Thought (CoT) process. This process is unified within a single\nautoregressive structure: (1) the first token corresponds to a stable keyframe\naction that encodes the task-specific goals; and (2) subsequent action tokens\nare generated autoregressively, conditioned on the initial keyframe and\npreviously predicted actions. This backward action reasoning enforces a\nglobal-to-local structure, allowing each local action to be tightly constrained\nby the final goal. To further realize the action reasoning structure, CoA\nincorporates four complementary designs: continuous action token\nrepresentation; dynamic stopping for variable-length trajectory generation;\nreverse temporal ensemble; and multi-token prediction to balance action chunk\nmodeling with global structure. As a result, CoA gives strong spatial\ngeneralization capabilities while preserving the flexibility and simplicity of\na visuo-motor policy. Empirically, we observe CoA achieves the state-of-the-art\nperformance across 60 RLBench tasks and 8 real-world manipulation tasks.", "AI": {"tldr": "Chain-of-Action (CoA) 是一种基于轨迹自回归建模的新型视觉运动策略范式，通过反向推理生成完整轨迹，实现全局到局部的动作约束，并在多个任务中达到最优性能。", "motivation": "传统方法仅预测下一步动作，缺乏全局目标约束，CoA 通过反向推理和任务目标编码，提升动作生成的全局一致性。", "method": "CoA 采用自回归结构，首先生成关键帧动作编码任务目标，随后自回归生成后续动作。结合连续动作表示、动态停止、反向时间集成和多令牌预测等技术。", "result": "CoA 在 60 个 RLBench 任务和 8 个真实世界操作任务中达到最优性能，展现出强大的空间泛化能力。", "conclusion": "CoA 通过反向推理和全局约束，实现了灵活且高效的视觉运动策略，为复杂任务的动作生成提供了新思路。"}}
{"id": "2506.09997", "pdf": "https://arxiv.org/pdf/2506.09997", "abs": "https://arxiv.org/abs/2506.09997", "authors": ["Chieh Hubert Lin", "Zhaoyang Lv", "Songyin Wu", "Zhen Xu", "Thu Nguyen-Phuoc", "Hung-Yu Tseng", "Julian Straub", "Numair Khan", "Lei Xiao", "Ming-Hsuan Yang", "Yuheng Ren", "Richard Newcombe", "Zhao Dong", "Zhengqin Li"], "title": "DGS-LRM: Real-Time Deformable 3D Gaussian Reconstruction From Monocular Videos", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": "Project page: https://hubert0527.github.io/dgslrm/", "summary": "We introduce the Deformable Gaussian Splats Large Reconstruction Model\n(DGS-LRM), the first feed-forward method predicting deformable 3D Gaussian\nsplats from a monocular posed video of any dynamic scene. Feed-forward scene\nreconstruction has gained significant attention for its ability to rapidly\ncreate digital replicas of real-world environments. However, most existing\nmodels are limited to static scenes and fail to reconstruct the motion of\nmoving objects. Developing a feed-forward model for dynamic scene\nreconstruction poses significant challenges, including the scarcity of training\ndata and the need for appropriate 3D representations and training paradigms. To\naddress these challenges, we introduce several key technical contributions: an\nenhanced large-scale synthetic dataset with ground-truth multi-view videos and\ndense 3D scene flow supervision; a per-pixel deformable 3D Gaussian\nrepresentation that is easy to learn, supports high-quality dynamic view\nsynthesis, and enables long-range 3D tracking; and a large transformer network\nthat achieves real-time, generalizable dynamic scene reconstruction. Extensive\nqualitative and quantitative experiments demonstrate that DGS-LRM achieves\ndynamic scene reconstruction quality comparable to optimization-based methods,\nwhile significantly outperforming the state-of-the-art predictive dynamic\nreconstruction method on real-world examples. Its predicted physically grounded\n3D deformation is accurate and can readily adapt for long-range 3D tracking\ntasks, achieving performance on par with state-of-the-art monocular video 3D\ntracking methods.", "AI": {"tldr": "DGS-LRM是一种前馈方法，通过单目视频预测可变形3D高斯斑点，用于动态场景重建。", "motivation": "现有前馈模型仅适用于静态场景，无法重建动态物体运动。DGS-LRM旨在解决动态场景重建的数据稀缺和3D表示难题。", "method": "提出大规模合成数据集、可变形3D高斯表示和大型Transformer网络，实现实时动态重建。", "result": "DGS-LRM在动态重建质量上媲美优化方法，优于现有预测方法，并支持长距离3D跟踪。", "conclusion": "DGS-LRM为动态场景重建提供了高效、高质量的解决方案，适用于实际应用。"}}
{"id": "2506.09956", "pdf": "https://arxiv.org/pdf/2506.09956", "abs": "https://arxiv.org/abs/2506.09956", "authors": ["Sahar Abdelnabi", "Aideen Fay", "Ahmed Salem", "Egor Zverev", "Kai-Chieh Liao", "Chi-Huang Liu", "Chun-Chih Kuo", "Jannis Weigend", "Danyael Manlangit", "Alex Apostolov", "Haris Umair", "João Donato", "Masayuki Kawakita", "Athar Mahboob", "Tran Huu Bach", "Tsun-Han Chiang", "Myeongjin Cho", "Hajin Choi", "Byeonghyeon Kim", "Hyeonjin Lee", "Benjamin Pannell", "Conor McCauley", "Mark Russinovich", "Andrew Paverd", "Giovanni Cherubin"], "title": "LLMail-Inject: A Dataset from a Realistic Adaptive Prompt Injection Challenge", "categories": ["cs.CR", "cs.AI"], "comment": "Dataset at:\n  https://huggingface.co/datasets/microsoft/llmail-inject-challenge", "summary": "Indirect Prompt Injection attacks exploit the inherent limitation of Large\nLanguage Models (LLMs) to distinguish between instructions and data in their\ninputs. Despite numerous defense proposals, the systematic evaluation against\nadaptive adversaries remains limited, even when successful attacks can have\nwide security and privacy implications, and many real-world LLM-based\napplications remain vulnerable. We present the results of LLMail-Inject, a\npublic challenge simulating a realistic scenario in which participants\nadaptively attempted to inject malicious instructions into emails in order to\ntrigger unauthorized tool calls in an LLM-based email assistant. The challenge\nspanned multiple defense strategies, LLM architectures, and retrieval\nconfigurations, resulting in a dataset of 208,095 unique attack submissions\nfrom 839 participants. We release the challenge code, the full dataset of\nsubmissions, and our analysis demonstrating how this data can provide new\ninsights into the instruction-data separation problem. We hope this will serve\nas a foundation for future research towards practical structural solutions to\nprompt injection.", "AI": {"tldr": "论文介绍了LLMail-Inject挑战赛，通过模拟真实场景评估LLM对间接提示注入攻击的防御能力，收集了大量攻击数据并分析了指令与数据分离问题。", "motivation": "现有防御方案对适应性攻击者的系统性评估不足，而成功的攻击可能带来广泛的安全和隐私问题。", "method": "通过公开挑战赛LLMail-Inject，模拟真实场景，收集攻击数据并分析不同防御策略、LLM架构和检索配置的效果。", "result": "收集了208,095次攻击提交，来自839名参与者，提供了关于指令与数据分离问题的新见解。", "conclusion": "研究为未来解决提示注入攻击提供了实践基础，推动了结构性解决方案的研究。"}}
{"id": "2506.09994", "pdf": "https://arxiv.org/pdf/2506.09994", "abs": "https://arxiv.org/abs/2506.09994", "authors": ["Venkatesh Pattabiraman", "Zizhou Huang", "Daniele Panozzo", "Denis Zorin", "Lerrel Pinto", "Raunaq Bhirangi"], "title": "eFlesh: Highly customizable Magnetic Touch Sensing using Cut-Cell Microstructures", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "If human experience is any guide, operating effectively in unstructured\nenvironments -- like homes and offices -- requires robots to sense the forces\nduring physical interaction. Yet, the lack of a versatile, accessible, and\neasily customizable tactile sensor has led to fragmented, sensor-specific\nsolutions in robotic manipulation -- and in many cases, to force-unaware,\nsensorless approaches. With eFlesh, we bridge this gap by introducing a\nmagnetic tactile sensor that is low-cost, easy to fabricate, and highly\ncustomizable. Building an eFlesh sensor requires only four components: a\nhobbyist 3D printer, off-the-shelf magnets (<$5), a CAD model of the desired\nshape, and a magnetometer circuit board. The sensor is constructed from tiled,\nparameterized microstructures, which allow for tuning the sensor's geometry and\nits mechanical response. We provide an open-source design tool that converts\nconvex OBJ/STL files into 3D-printable STLs for fabrication. This modular\ndesign framework enables users to create application-specific sensors, and to\nadjust sensitivity depending on the task. Our sensor characterization\nexperiments demonstrate the capabilities of eFlesh: contact localization RMSE\nof 0.5 mm, and force prediction RMSE of 0.27 N for normal force and 0.12 N for\nshear force. We also present a learned slip detection model that generalizes to\nunseen objects with 95% accuracy, and visuotactile control policies that\nimprove manipulation performance by 40% over vision-only baselines -- achieving\n91% average success rate for four precise tasks that require sub-mm accuracy\nfor successful completion. All design files, code and the CAD-to-eFlesh STL\nconversion tool are open-sourced and available on https://e-flesh.com.", "AI": {"tldr": "eFlesh是一种低成本、易定制、基于磁力的触觉传感器，用于机器人操作，填补了现有触觉传感器的空白。", "motivation": "在非结构化环境中，机器人需要感知物理交互中的力，但现有触觉传感器缺乏通用性和可定制性，导致解决方案分散或完全忽略力的感知。", "method": "eFlesh传感器由3D打印的微结构、现成磁铁和磁力计电路板组成，提供开源设计工具，支持几何和机械响应的定制。", "result": "传感器性能优异：接触定位误差0.5 mm，力预测误差0.27 N（法向）和0.12 N（切向），滑移检测准确率95%，视觉触觉控制策略比纯视觉基线提升40%成功率。", "conclusion": "eFlesh为机器人操作提供了一种低成本、高定制化的触觉传感器解决方案，显著提升了非结构化环境中的操作性能。"}}
