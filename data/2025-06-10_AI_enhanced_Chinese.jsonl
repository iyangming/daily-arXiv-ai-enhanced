{"id": "2506.06282", "pdf": "https://arxiv.org/pdf/2506.06282", "abs": "https://arxiv.org/abs/2506.06282", "authors": ["Shuangyan Deng", "Haizhou Peng", "Jiachen Xu", "Chunhou Liu", "Ciprian Doru Giurcuaneanu", "Jiamou Liu"], "title": "Understanding Financial Reasoning in AI: A Multimodal Benchmark and Error Learning Approach", "categories": ["cs.AI"], "comment": null, "summary": "Effective financial reasoning demands not only textual understanding but also\nthe ability to interpret complex visual data such as charts, tables, and trend\ngraphs. This paper introduces a new benchmark designed to evaluate how well AI\nmodels - especially large language and multimodal models - reason in\nfinance-specific contexts. Covering 3,200 expert-level question-answer pairs\nacross 15 core financial topics, the benchmark integrates both textual and\nvisual modalities to reflect authentic analytical challenges in finance. To\naddress limitations in current reasoning approaches, we propose an error-aware\nlearning framework that leverages historical model mistakes and feedback to\nguide inference, without requiring fine-tuning. Our experiments across\nstate-of-the-art models show that multimodal inputs significantly enhance\nperformance and that incorporating error feedback leads to consistent and\nmeasurable improvements. The results highlight persistent challenges in visual\nunderstanding and mathematical logic, while also demonstrating the promise of\nself-reflective reasoning in financial AI systems. Our code and data can be\nfound at https://anonymous/FinMR/CodeData.", "AI": {"tldr": "论文提出了一个新的金融推理基准，结合文本和视觉数据，并引入错误感知学习框架以提升AI模型在金融领域的表现。", "motivation": "现有AI模型在金融推理中面临视觉理解和数学逻辑的挑战，需要更全面的评估和改进方法。", "method": "构建包含3,200个专家级问题的基准，结合多模态输入，并提出无需微调的错误感知学习框架。", "result": "多模态输入显著提升性能，错误反馈带来持续改进，但视觉理解和数学逻辑仍是挑战。", "conclusion": "金融AI系统需结合多模态和自我反思推理，未来研究可进一步优化视觉和逻辑能力。"}}
{"id": "2506.06284", "pdf": "https://arxiv.org/pdf/2506.06284", "abs": "https://arxiv.org/abs/2506.06284", "authors": ["John Beverley", "Jim Logan"], "title": "Unreal Patterns", "categories": ["cs.AI"], "comment": null, "summary": "This paper introduces a framework for representing information about entities\nthat do not exist or may never exist, such as those involving fictional\nentities, blueprints, simulations, and future scenarios. Traditional approaches\nthat introduce \"dummy instances\" or rely on modal logic are criticized, and a\nproposal is defended in which such cases are modeled using the intersections of\nactual types rather than specific non existent tokens. The paper positions\nitself within the Basic Formal Ontology and its realist commitments,\nemphasizing the importance of practical, implementable solutions over purely\nmetaphysical or philosophical proposals, arguing that existing approaches to\nnon existent entities either overcommit to metaphysical assumptions or\nintroduce computational inefficiencies that hinder applications. By developing\na structured ontology driven approach to unreal patterns, the paper aims to\nprovide a useful and computationally viable means of handling references to\nhypothetical or non existent entities.", "AI": {"tldr": "本文提出了一种框架，用于表示不存在或可能永远不会存在的实体信息，如虚构实体、蓝图、模拟和未来场景。传统方法被批评，并提出了一种基于实际类型交集而非非存在标记的建模方法。", "motivation": "传统方法在处理非存在实体时存在过度依赖形而上学假设或计算效率低下的问题，本文旨在提供一种实用且计算可行的解决方案。", "method": "采用基于基本形式本体论的现实主义方法，通过实际类型的交集建模非存在实体。", "result": "提出了一种结构化本体驱动的方法，能够有效处理对假设或非存在实体的引用。", "conclusion": "该方法在保持现实主义承诺的同时，提供了实用且计算高效的解决方案，优于传统方法。"}}
{"id": "2506.06285", "pdf": "https://arxiv.org/pdf/2506.06285", "abs": "https://arxiv.org/abs/2506.06285", "authors": ["Kaike Sa Teles Rocha Alves", "Eduardo Pestana de Aguiar"], "title": "NFISiS: New Perspectives on Fuzzy Inference Systems for Renewable Energy Forecasting", "categories": ["cs.AI"], "comment": null, "summary": "Evolving Fuzzy Systems (eFS) have gained significant attention due to their\nability to adaptively update their structure in response to data dynamics while\nmaintaining interpretability. However, the lack of publicly available\nimplementations of these models limits their accessibility and widespread\nadoption. To address this gap, we present evolvingfuzzysystems, a Python\nlibrary that provides implementations of several well-established eFS models,\nincluding ePL-KRLS-DISCO, ePL+, eMG, ePL, exTS, Simpl\\_eTS, and eTS. The\nlibrary facilitates model evaluation and comparison by offering built-in tools\nfor training, visualization, and performance assessment. The models are\nevaluated using the fetch\\_california\\_housing dataset, with performance\nmeasured in terms of normalized root-mean-square error (NRMSE), non-dimensional\nerror index (NDEI), and mean absolute percentage error (MAPE). Additionally,\ncomputational complexity is analyzed by measuring execution times and rule\nevolution during training and testing phases. The results highlight ePL as a\nsimple yet efficient model that balances accuracy and computational cost,\nmaking it particularly suitable for real-world applications. By making these\nmodels publicly available, evolvingfuzzysystems aims to foster research and\npractical applications in adaptive and interpretable machine learning.", "AI": {"tldr": "论文介绍了一个名为evolvingfuzzysystems的Python库，实现了多种Evolving Fuzzy Systems (eFS)模型，旨在提高其可访问性和应用范围。", "motivation": "由于缺乏公开的eFS模型实现，限制了其广泛采用，因此开发了一个Python库来解决这一问题。", "method": "提供了多种eFS模型的实现，包括ePL-KRLS-DISCO、ePL+等，并内置了训练、可视化和性能评估工具。使用fetch_california_housing数据集评估模型性能。", "result": "ePL模型在准确性和计算成本之间取得了平衡，适合实际应用。", "conclusion": "通过公开这些模型，evolvingfuzzysystems旨在促进自适应和可解释机器学习的研究和应用。"}}
{"id": "2506.06287", "pdf": "https://arxiv.org/pdf/2506.06287", "abs": "https://arxiv.org/abs/2506.06287", "authors": ["FutureSearch", ":", "Nikos I. Bosse", "Jon Evans", "Robert G. Gambee", "Daniel Hnyk", "Peter Mühlbacher", "Lawrence Phillips", "Dan Schwarz", "Jack Wildman"], "title": "Deep Research Bench: Evaluating AI Web Research Agents", "categories": ["cs.AI"], "comment": null, "summary": "Amongst the most common use cases of modern AI is LLM chat with web search\nenabled. However, no direct evaluations of the quality of web research agents\nexist that control for the continually-changing web. We introduce Deep Research\nBench, consisting of 89 multi-step web research task instances of varying\ndifficulty across 8 diverse task categories, with the answers carefully worked\nout by skilled humans. We provide a \"RetroSearch\" environment with a large\nfrozen set of scraped web pages, and demonstrate that offline \"RetroSearch\"\nagents perform comparably to \"live web\" agents, enabling reliable evaluations\nof models over time. We provide robust agent tooling and scaffolding to\nbenchmark major LLMs as they are released, including \"thinking\" models like o3\nand Gemini 2.5 Pro. We include automated evaluations of the lengthy agent\ntraces to report progress over time in hallucinations, tool use, and\nforgetting. Finally, we evaluate the major web research products branded as\n\"Deep Research\", \"Deep Search\", \"Search\", or \"Research.\" Results are available\non a public leaderboard at https://drb.futuresearch.ai/.", "AI": {"tldr": "Deep Research Bench 是一个用于评估 LLM 在网页研究任务中性能的基准测试，包含 89 个多步骤任务，并提供离线环境以控制网页变化的影响。", "motivation": "现有评估未能控制网页内容的变化，无法可靠评估 LLM 的研究能力。", "method": "使用 RetroSearch 环境（冻结的网页数据集）和人类标注的答案，评估 LLM 在多步骤任务中的表现。", "result": "离线环境与实时网页环境表现相当，可用于长期评估。同时提供了自动化评估工具和公开排行榜。", "conclusion": "Deep Research Bench 为 LLM 网页研究能力的评估提供了可靠且可重复的基准。"}}
{"id": "2506.06331", "pdf": "https://arxiv.org/pdf/2506.06331", "abs": "https://arxiv.org/abs/2506.06331", "authors": ["Qiming Zeng", "Xiao Yan", "Hao Luo", "Yuhao Lin", "Yuxiang Wang", "Fangcheng Fu", "Bo Du", "Quanqing Xu", "Jiawei Jiang"], "title": "How Significant Are the Real Performance Gains? An Unbiased Evaluation Framework for GraphRAG", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "By retrieving contexts from knowledge graphs, graph-based retrieval-augmented\ngeneration (GraphRAG) enhances large language models (LLMs) to generate quality\nanswers for user questions. Many GraphRAG methods have been proposed and\nreported inspiring performance in answer quality. However, we observe that the\ncurrent answer evaluation framework for GraphRAG has two critical flaws, i.e.,\nunrelated questions and evaluation biases, which may lead to biased or even\nwrong conclusions on performance. To tackle the two flaws, we propose an\nunbiased evaluation framework that uses graph-text-grounded question generation\nto produce questions that are more related to the underlying dataset and an\nunbiased evaluation procedure to eliminate the biases in LLM-based answer\nassessment. We apply our unbiased framework to evaluate 3 representative\nGraphRAG methods and find that their performance gains are much more moderate\nthan reported previously. Although our evaluation framework may still have\nflaws, it calls for scientific evaluations to lay solid foundations for\nGraphRAG research.", "AI": {"tldr": "论文提出了一种无偏见的评估框架，用于解决当前GraphRAG方法在答案评估中的两个关键缺陷：无关问题和评估偏见。", "motivation": "当前GraphRAG方法的评估框架存在无关问题和评估偏见，可能导致对性能的偏见甚至错误结论。", "method": "提出了一种基于图-文本-问题生成的无偏见评估框架，并设计了无偏见的评估流程。", "result": "应用该框架评估3种代表性GraphRAG方法，发现其性能提升比之前报道的要温和得多。", "conclusion": "尽管评估框架可能仍有缺陷，但它呼吁科学评估为GraphRAG研究奠定坚实基础。"}}
{"id": "2506.06283", "pdf": "https://arxiv.org/pdf/2506.06283", "abs": "https://arxiv.org/abs/2506.06283", "authors": ["Juexiao Zhou", "Zhongyi Han", "Mankun Xin", "Xingwei He", "Guotao Wang", "Jiaoyan Song", "Gongning Luo", "Wenjia He", "Xintong Li", "Yuetan Chu", "Juanwen Chen", "Bo Wang", "Xia Wu", "Wenwen Duan", "Zhixia Guo", "Liyan Bai", "Yilin Pan", "Xuefei Bi", "Lu Liu", "Long Feng", "Xiaonan He", "Xin Gao"], "title": "Facial Foundational Model Advances Early Warning of Coronary Artery Disease from Live Videos with DigitalShadow", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Global population aging presents increasing challenges to healthcare systems,\nwith coronary artery disease (CAD) responsible for approximately 17.8 million\ndeaths annually, making it a leading cause of global mortality. As CAD is\nlargely preventable, early detection and proactive management are essential. In\nthis work, we introduce DigitalShadow, an advanced early warning system for\nCAD, powered by a fine-tuned facial foundation model. The system is pre-trained\non 21 million facial images and subsequently fine-tuned into LiveCAD, a\nspecialized CAD risk assessment model trained on 7,004 facial images from 1,751\nsubjects across four hospitals in China. DigitalShadow functions passively and\ncontactlessly, extracting facial features from live video streams without\nrequiring active user engagement. Integrated with a personalized database, it\ngenerates natural language risk reports and individualized health\nrecommendations. With privacy as a core design principle, DigitalShadow\nsupports local deployment to ensure secure handling of user data.", "AI": {"tldr": "DigitalShadow是一种基于面部基础模型的CAD早期预警系统，通过无接触方式从视频流中提取面部特征，生成个性化健康报告。", "motivation": "全球人口老龄化加剧了医疗系统负担，CAD是主要死因之一，早期检测和管理至关重要。", "method": "系统预训练于2100万张面部图像，并微调为LiveCAD模型，使用来自1751名受试者的7004张面部图像进行CAD风险评估。", "result": "DigitalShadow能被动、无接触地工作，生成自然语言风险报告和个性化健康建议。", "conclusion": "该系统以隐私为核心设计原则，支持本地部署，为CAD早期检测提供了创新解决方案。"}}
{"id": "2506.06301", "pdf": "https://arxiv.org/pdf/2506.06301", "abs": "https://arxiv.org/abs/2506.06301", "authors": ["Muhammad Monjurul Karim", "Yan Shi", "Shucheng Zhang", "Bingzhang Wang", "Mehrdad Nasri", "Yinhai Wang"], "title": "Large Language Models and Their Applications in Roadway Safety and Mobility Enhancement: A Comprehensive Review", "categories": ["cs.AI"], "comment": null, "summary": "Roadway safety and mobility remain critical challenges for modern\ntransportation systems, demanding innovative analytical frameworks capable of\naddressing complex, dynamic, and heterogeneous environments. While traditional\nengineering methods have made progress, the complexity and dynamism of\nreal-world traffic necessitate more advanced analytical frameworks. Large\nLanguage Models (LLMs), with their unprecedented capabilities in natural\nlanguage understanding, knowledge integration, and reasoning, represent a\npromising paradigm shift. This paper comprehensively reviews the application\nand customization of LLMs for enhancing roadway safety and mobility. A key\nfocus is how LLMs are adapted -- via architectural, training, prompting, and\nmultimodal strategies -- to bridge the \"modality gap\" with transportation's\nunique spatio-temporal and physical data. The review systematically analyzes\ndiverse LLM applications in mobility (e.g., traffic flow prediction, signal\ncontrol) and safety (e.g., crash analysis, driver behavior assessment,).\nEnabling technologies such as V2X integration, domain-specific foundation\nmodels, explainability frameworks, and edge computing are also examined.\nDespite significant potential, challenges persist regarding inherent LLM\nlimitations (hallucinations, reasoning deficits), data governance (privacy,\nbias), deployment complexities (sim-to-real, latency), and rigorous safety\nassurance. Promising future research directions are highlighted, including\nadvanced multimodal fusion, enhanced spatio-temporal reasoning, human-AI\ncollaboration, continuous learning, and the development of efficient,\nverifiable systems. This review provides a structured roadmap of current\ncapabilities, limitations, and opportunities, underscoring LLMs' transformative\npotential while emphasizing the need for responsible innovation to realize\nsafer, more intelligent transportation systems.", "AI": {"tldr": "本文综述了大型语言模型（LLMs）在提升道路安全和交通流动性中的应用与定制，探讨了其潜力、挑战及未来研究方向。", "motivation": "传统工程方法难以应对复杂动态的交通环境，LLMs因其自然语言理解、知识整合和推理能力被视为潜在的解决方案。", "method": "通过架构调整、训练策略、提示工程和多模态方法，LLMs被定制以处理交通特有的时空和物理数据。", "result": "LLMs在交通流动性（如流量预测、信号控制）和安全性（如事故分析、驾驶员行为评估）中有广泛应用，但仍面临幻觉、数据隐私等技术挑战。", "conclusion": "LLMs具有变革交通系统的潜力，但需负责任地创新以解决当前限制，如多模态融合、时空推理增强等。"}}
{"id": "2506.06343", "pdf": "https://arxiv.org/pdf/2506.06343", "abs": "https://arxiv.org/abs/2506.06343", "authors": ["Taesoo Kim", "Jong Hwan Ko"], "title": "TESU-LLM: Training Speech-LLMs Without Speech via Unified Encoder Alignment", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": null, "summary": "Recent advances in speech-enabled language models have shown promising\nresults in building intelligent voice assistants. However, most existing\napproaches rely on large-scale paired speech-text data and extensive\ncomputational resources, which pose challenges in terms of scalability and\naccessibility. In this paper, we present \\textbf{TESU-LLM}, a novel framework\nthat enables training speech-capable language models using only text data. Our\nkey insight is to leverage a unified encoder that maps semantically equivalent\ntext and speech inputs to a shared latent space. By aligning the encoder output\nwith the embedding space of a LLM via a lightweight projection network, we\nenable the model to generalize from text-only supervision to speech-based\ninference. Despite being trained exclusively on text, TESU-LLM achieves strong\nperformance on various speech-related benchmarks, comparable to baseline\nmethods trained with large-scale multimodal datasets and substantial\ncomputational resources. These results highlight the effectiveness and\nefficiency of our approach, offering a scalable path toward building speech\nLLMs without speech data.", "AI": {"tldr": "TESU-LLM是一种仅使用文本数据训练语音语言模型的新框架，通过统一编码器和轻量级投影网络实现语音推理，性能接近多模态基线方法。", "motivation": "现有语音语言模型依赖大规模语音-文本配对数据和计算资源，限制了可扩展性和可访问性。", "method": "提出TESU-LLM框架，利用统一编码器将语义等效的文本和语音输入映射到共享潜在空间，并通过轻量级投影网络与LLM嵌入空间对齐。", "result": "仅用文本训练的TESU-LLM在多个语音相关基准测试中表现优异，性能接近多模态基线方法。", "conclusion": "TESU-LLM提供了一种无需语音数据的高效、可扩展的语音LLM构建方法。"}}
{"id": "2506.06389", "pdf": "https://arxiv.org/pdf/2506.06389", "abs": "https://arxiv.org/abs/2506.06389", "authors": ["Rifat Sadik", "Tanvir Rahman", "Arpan Bhattacharjee", "Bikash Chandra Halder", "Ismail Hossain"], "title": "Exploring Adversarial Watermarking in Transformer-Based Models: Transferability and Robustness Against Defense Mechanism for Medical Images", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Deep learning models have shown remarkable success in dermatological image\nanalysis, offering potential for automated skin disease diagnosis. Previously,\nconvolutional neural network(CNN) based architectures have achieved immense\npopularity and success in computer vision (CV) based task like skin image\nrecognition, generation and video analysis. But with the emergence of\ntransformer based models, CV tasks are now are nowadays carrying out using\nthese models. Vision Transformers (ViTs) is such a transformer-based models\nthat have shown success in computer vision. It uses self-attention mechanisms\nto achieve state-of-the-art performance across various tasks. However, their\nreliance on global attention mechanisms makes them susceptible to adversarial\nperturbations. This paper aims to investigate the susceptibility of ViTs for\nmedical images to adversarial watermarking-a method that adds so-called\nimperceptible perturbations in order to fool models. By generating adversarial\nwatermarks through Projected Gradient Descent (PGD), we examine the\ntransferability of such attacks to CNNs and analyze the performance defense\nmechanism -- adversarial training. Results indicate that while performance is\nnot compromised for clean images, ViTs certainly become much more vulnerable to\nadversarial attacks: an accuracy drop of as low as 27.6%. Nevertheless,\nadversarial training raises it up to 90.0%.", "AI": {"tldr": "本文研究了基于视觉变换器（ViTs）的医学图像对抗水印攻击的脆弱性，并探讨了对抗训练的防御效果。", "motivation": "随着ViTs在计算机视觉任务中的成功应用，其全局注意力机制使其容易受到对抗性攻击，尤其是在医学图像领域。本文旨在探索ViTs对对抗水印的脆弱性及其防御方法。", "method": "通过投影梯度下降（PGD）生成对抗水印，测试ViTs的脆弱性，并分析对抗训练对性能的影响。", "result": "ViTs在对抗攻击下准确率下降至27.6%，但通过对抗训练可提升至90.0%。", "conclusion": "ViTs在医学图像中对对抗攻击较为脆弱，但对抗训练能显著提高其鲁棒性。"}}
{"id": "2506.06324", "pdf": "https://arxiv.org/pdf/2506.06324", "abs": "https://arxiv.org/abs/2506.06324", "authors": ["Shruti Kumar", "Xiaoyu Chen", "Xiaomei Wang"], "title": "Mapping Human-Agent Co-Learning and Co-Adaptation: A Scoping Review", "categories": ["cs.AI"], "comment": "Abstract accepted to HFES 2024 Annual Meeting", "summary": "Several papers have delved into the challenges of human-AI-robot co-learning\nand co-adaptation. It has been noted that the terminology used to describe this\ncollaborative relationship in existing studies needs to be more consistent. For\nexample, the prefix \"co\" is used interchangeably to represent both\n\"collaborative\" and \"mutual,\" and the terms \"co-learning\" and \"co-adaptation\"\nare sometimes used interchangeably. However, they can reflect subtle\ndifferences in the focus of the studies. The current scoping review's primary\nresearch question (RQ1) aims to gather existing papers discussing this\ncollaboration pattern and examine the terms researchers use to describe this\nhuman-agent relationship. Given the relative newness of this area of study, we\nare also keen on exploring the specific types of intelligent agents and task\ndomains that have been considered in existing research (RQ2). This exploration\nis significant as it can shed light on the diversity of human-agent\ninteractions, from one-time to continuous learning/adaptation scenarios. It can\nalso help us understand the dynamics of human-agent interactions in different\ntask domains, guiding our expectations towards research situated in dynamic,\ncomplex domains. Our third objective (RQ3) is to investigate the cognitive\ntheories and frameworks that have been utilized in existing studies to measure\nhuman-agent co-learning and co-adaptation. This investigation is crucial as it\ncan help us understand the theoretical underpinnings of human-agent\ncollaboration and adaptation, and it can also guide us in identifying any new\nframeworks proposed specifically for this type of relationship.", "AI": {"tldr": "论文探讨了人-AI-机器人共学习与共适应的术语不一致问题，并提出了三个研究问题：术语使用（RQ1）、智能体与任务领域（RQ2）、认知理论与框架（RQ3）。", "motivation": "研究动机在于澄清术语混乱，探索人-智能体交互的多样性与理论框架，以指导动态复杂领域的研究。", "method": "采用范围综述方法，收集并分析现有文献中的术语、智能体类型、任务领域及认知理论。", "result": "预期结果包括术语标准化建议、人-智能体交互的多样性分析及理论框架的总结。", "conclusion": "研究旨在为人-AI-机器人协作提供清晰术语与理论支持，推动动态复杂领域的研究发展。"}}
{"id": "2506.06347", "pdf": "https://arxiv.org/pdf/2506.06347", "abs": "https://arxiv.org/abs/2506.06347", "authors": ["Zachary Yang", "Domenico Tullo", "Reihaneh Rabbany"], "title": "Unified Game Moderation: Soft-Prompting and LLM-Assisted Label Transfer for Resource-Efficient Toxicity Detection", "categories": ["cs.CL", "cs.AI", "I.2.7; J.4"], "comment": "11 pages, 1 figure, 9 Tables, KDD 2025 ADS Track", "summary": "Toxicity detection in gaming communities faces significant scaling challenges\nwhen expanding across multiple games and languages, particularly in real-time\nenvironments where computational efficiency is crucial. We present two key\nfindings to address these challenges while building upon our previous work on\nToxBuster, a BERT-based real-time toxicity detection system. First, we\nintroduce a soft-prompting approach that enables a single model to effectively\nhandle multiple games by incorporating game-context tokens, matching the\nperformance of more complex methods like curriculum learning while offering\nsuperior scalability. Second, we develop an LLM-assisted label transfer\nframework using GPT-4o-mini to extend support to seven additional languages.\nEvaluations on real game chat data across French, German, Portuguese, and\nRussian achieve macro F1-scores ranging from 32.96% to 58.88%, with\nparticularly strong performance in German, surpassing the English benchmark of\n45.39%. In production, this unified approach significantly reduces\ncomputational resources and maintenance overhead compared to maintaining\nseparate models for each game and language combination. At Ubisoft, this model\nsuccessfully identifies an average of 50 players, per game, per day engaging in\nsanctionable behavior.", "AI": {"tldr": "论文提出了一种软提示方法和LLM辅助标签转移框架，以解决多游戏和多语言环境中的实时毒性检测问题，显著提升了计算效率和可扩展性。", "motivation": "游戏社区中的毒性检测在多游戏和多语言环境下面临扩展和实时性挑战，需要高效且可扩展的解决方案。", "method": "1. 引入软提示方法，使单一模型能通过游戏上下文标记处理多游戏；2. 开发基于GPT-4o-mini的LLM辅助标签转移框架，扩展支持七种语言。", "result": "在法语、德语、葡萄牙语和俄语的游戏聊天数据上，宏F1分数为32.96%至58.88%，德语表现最佳，超越英语基准45.39%。生产环境中显著减少计算资源和维护成本。", "conclusion": "该方法在Ubisoft成功应用，每天平均识别50名违规玩家，证明了其高效性和可扩展性。"}}
{"id": "2506.06480", "pdf": "https://arxiv.org/pdf/2506.06480", "abs": "https://arxiv.org/abs/2506.06480", "authors": ["A. Postlmayr", "P. Cosman", "S. Dey"], "title": "(LiFT) Lightweight Fitness Transformer: A language-vision model for Remote Monitoring of Physical Training", "categories": ["cs.CV"], "comment": null, "summary": "We introduce a fitness tracking system that enables remote monitoring for\nexercises using only a RGB smartphone camera, making fitness tracking more\nprivate, scalable, and cost effective. Although prior work explored automated\nexercise supervision, existing models are either too limited in exercise\nvariety or too complex for real-world deployment. Prior approaches typically\nfocus on a small set of exercises and fail to generalize across diverse\nmovements. In contrast, we develop a robust, multitask motion analysis model\ncapable of performing exercise detection and repetition counting across\nhundreds of exercises, a scale far beyond previous methods. We overcome\nprevious data limitations by assembling a large-scale fitness dataset, Olympia\ncovering more than 1,900 exercises. To our knowledge, our vision-language model\nis the first that can perform multiple tasks on skeletal fitness data. On\nOlympia, our model can detect exercises with 76.5% accuracy and count\nrepetitions with 85.3% off-by-one accuracy, using only RGB video. By presenting\na single vision-language transformer model for both exercise identification and\nrep counting, we take a significant step toward democratizing AI-powered\nfitness tracking.", "AI": {"tldr": "提出了一种基于RGB智能手机摄像头的远程健身追踪系统，解决了现有模型在运动多样性和实际部署中的局限性。", "motivation": "现有健身追踪模型要么运动种类有限，要么过于复杂难以部署，需要一种更通用且实用的解决方案。", "method": "开发了一个多任务运动分析模型，结合大规模健身数据集Olympia（包含1900多种运动），利用视觉-语言Transformer模型进行运动检测和重复计数。", "result": "模型在Olympia数据集上运动检测准确率为76.5%，重复计数准确率为85.3%（误差±1）。", "conclusion": "通过单一模型实现运动识别和重复计数，为AI驱动的健身追踪普及迈出了重要一步。"}}
{"id": "2506.06326", "pdf": "https://arxiv.org/pdf/2506.06326", "abs": "https://arxiv.org/abs/2506.06326", "authors": ["Jiazheng Kang", "Mingming Ji", "Zhe Zhao", "Ting Bai"], "title": "Memory OS of AI Agent", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) face a crucial challenge from fixed context\nwindows and inadequate memory management, leading to a severe shortage of\nlong-term memory capabilities and limited personalization in the interactive\nexperience with AI agents. To overcome this challenge, we innovatively propose\na Memory Operating System, i.e., MemoryOS, to achieve comprehensive and\nefficient memory management for AI agents. Inspired by the memory management\nprinciples in operating systems, MemoryOS designs a hierarchical storage\narchitecture and consists of four key modules: Memory Storage, Updating,\nRetrieval, and Generation. Specifically, the architecture comprises three\nlevels of storage units: short-term memory, mid-term memory, and long-term\npersonal memory. Key operations within MemoryOS include dynamic updates between\nstorage units: short-term to mid-term updates follow a dialogue-chain-based\nFIFO principle, while mid-term to long-term updates use a segmented page\norganization strategy. Our pioneering MemoryOS enables hierarchical memory\nintegration and dynamic updating. Extensive experiments on the LoCoMo benchmark\nshow an average improvement of 49.11% on F1 and 46.18% on BLEU-1 over the\nbaselines on GPT-4o-mini, showing contextual coherence and personalized memory\nretention in long conversations. The implementation code is open-sourced at\nhttps://github.com/BAI-LAB/MemoryOS.", "AI": {"tldr": "论文提出MemoryOS，一种创新的内存操作系统，用于解决LLMs的固定上下文窗口和内存管理不足问题，通过分层存储架构和动态更新策略提升长期记忆能力和个性化交互体验。", "motivation": "LLMs面临固定上下文窗口和内存管理不足的挑战，导致长期记忆能力不足和个性化交互体验受限。", "method": "提出MemoryOS，采用分层存储架构（短、中、长期记忆）和四个关键模块（存储、更新、检索、生成），动态更新策略包括基于对话链的FIFO和分段页组织。", "result": "在LoCoMo基准测试中，MemoryOS在F1和BLEU-1上分别平均提升49.11%和46.18%，显著优于基线模型。", "conclusion": "MemoryOS通过分层存储和动态更新策略，有效提升了LLMs的长期记忆能力和个性化交互体验，实验验证了其优越性。"}}
{"id": "2506.06371", "pdf": "https://arxiv.org/pdf/2506.06371", "abs": "https://arxiv.org/abs/2506.06371", "authors": ["Panagiotis Koletsis", "Christos Panagiotopoulos", "Georgios Th. Papadopoulos", "Vasilis Efthymiou"], "title": "Relationship Detection on Tabular Data Using Statistical Analysis and Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Over the past few years, table interpretation tasks have made significant\nprogress due to their importance and the introduction of new technologies and\nbenchmarks in the field. This work experiments with a hybrid approach for\ndetecting relationships among columns of unlabeled tabular data, using a\nKnowledge Graph (KG) as a reference point, a task known as CPA. This approach\nleverages large language models (LLMs) while employing statistical analysis to\nreduce the search space of potential KG relations. The main modules of this\napproach for reducing the search space are domain and range constraints\ndetection, as well as relation co-appearance analysis. The experimental\nevaluation on two benchmark datasets provided by the SemTab challenge assesses\nthe influence of each module and the effectiveness of different\nstate-of-the-art LLMs at various levels of quantization. The experiments were\nperformed, as well as at different prompting techniques. The proposed\nmethodology, which is publicly available on github, proved to be competitive\nwith state-of-the-art approaches on these datasets.", "AI": {"tldr": "论文提出了一种混合方法，利用知识图谱（KG）和大型语言模型（LLM）检测未标记表格数据中列之间的关系（CPA任务），并通过统计分析和模块优化减少搜索空间。", "motivation": "表格解释任务的重要性日益凸显，但现有技术在处理未标记数据时仍有挑战，因此需要结合KG和LLM的新方法。", "method": "采用混合方法，结合KG和LLM，通过域和范围约束检测及关系共现分析减少搜索空间，并在SemTab挑战数据集上评估不同模块和LLM的效果。", "result": "实验表明，该方法在SemTab数据集上具有竞争力，且不同量化和提示技术对性能有影响。", "conclusion": "提出的方法在未标记表格数据的CPA任务中表现优异，且代码已开源。"}}
{"id": "2506.06517", "pdf": "https://arxiv.org/pdf/2506.06517", "abs": "https://arxiv.org/abs/2506.06517", "authors": ["Mingqi Jiang", "Chanho Kim", "Chen Ziwen", "Li Fuxin"], "title": "GS4: Generalizable Sparse Splatting Semantic SLAM", "categories": ["cs.CV"], "comment": "13 pages, 6 figures", "summary": "Traditional SLAM algorithms are excellent at camera tracking but might\ngenerate lower resolution and incomplete 3D maps. Recently, Gaussian Splatting\n(GS) approaches have emerged as an option for SLAM with accurate, dense 3D map\nbuilding. However, existing GS-based SLAM methods rely on per-scene\noptimization which is time-consuming and does not generalize to diverse scenes\nwell. In this work, we introduce the first generalizable GS-based semantic SLAM\nalgorithm that incrementally builds and updates a 3D scene representation from\nan RGB-D video stream using a learned generalizable network. Our approach\nstarts from an RGB-D image recognition backbone to predict the Gaussian\nparameters from every downsampled and backprojected image location.\nAdditionally, we seamlessly integrate 3D semantic segmentation into our GS\nframework, bridging 3D mapping and recognition through a shared backbone. To\ncorrect localization drifting and floaters, we propose to optimize the GS for\nonly 1 iteration following global localization. We demonstrate state-of-the-art\nsemantic SLAM performance on the real-world benchmark ScanNet with an order of\nmagnitude fewer Gaussians compared to other recent GS-based methods, and\nshowcase our model's generalization capability through zero-shot transfer to\nthe NYUv2 and TUM RGB-D datasets.", "AI": {"tldr": "提出了一种基于高斯泼溅（GS）的通用语义SLAM算法，通过学习网络增量构建3D场景表示，结合语义分割，优化效率高且泛化能力强。", "motivation": "传统SLAM算法生成的地图分辨率低且不完整，现有GS方法依赖场景优化，耗时长且泛化性差。", "method": "使用RGB-D图像识别主干预测高斯参数，集成3D语义分割，全局定位后仅优化1次以减少漂移和浮点。", "result": "在ScanNet上实现SOTA性能，高斯数量少一个数量级，零样本泛化至NYUv2和TUM RGB-D数据集。", "conclusion": "该方法高效、通用，结合语义信息，显著提升SLAM性能。"}}
{"id": "2506.06352", "pdf": "https://arxiv.org/pdf/2506.06352", "abs": "https://arxiv.org/abs/2506.06352", "authors": ["Christian Tarsney"], "title": "Will artificial agents pursue power by default?", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "Researchers worried about catastrophic risks from advanced AI have argued\nthat we should expect sufficiently capable AI agents to pursue power over\nhumanity because power is a convergent instrumental goal, something that is\nuseful for a wide range of final goals. Others have recently expressed\nskepticism of these claims. This paper aims to formalize the concepts of\ninstrumental convergence and power-seeking in an abstract, decision-theoretic\nframework, and to assess the claim that power is a convergent instrumental\ngoal. I conclude that this claim contains at least an element of truth, but\nmight turn out to have limited predictive utility, since an agent's options\ncannot always be ranked in terms of power in the absence of substantive\ninformation about the agent's final goals. However, the fact of instrumental\nconvergence is more predictive for agents who have a good shot at attaining\nabsolute or near-absolute power.", "AI": {"tldr": "本文通过决策理论框架形式化工具性趋同和权力追求的概念，评估权力作为趋同工具目标的说法，认为其有一定真实性，但预测效用可能有限。", "motivation": "研究者对高级AI可能带来的灾难性风险表示担忧，认为强大的AI代理会追求权力，因为权力是趋同的工具目标。然而，也有人对此表示怀疑。本文旨在澄清和评估这一观点。", "method": "采用抽象的决策理论框架，形式化工具性趋同和权力追求的概念。", "result": "权力作为趋同工具目标的说法有一定真实性，但在缺乏代理最终目标的具体信息时，预测效用可能有限。", "conclusion": "工具性趋同对于能够获得绝对或接近绝对权力的代理更具预测性。"}}
{"id": "2506.06376", "pdf": "https://arxiv.org/pdf/2506.06376", "abs": "https://arxiv.org/abs/2506.06376", "authors": ["Heng Dong", "Kefei Duan", "Chongjie Zhang"], "title": "Enhancing Decision-Making of Large Language Models via Actor-Critic", "categories": ["cs.CL", "cs.AI"], "comment": "Forty-second International Conference on Machine Learning (ICML 2025)", "summary": "Large Language Models (LLMs) have achieved remarkable advancements in natural\nlanguage processing tasks, yet they encounter challenges in complex\ndecision-making scenarios that require long-term reasoning and alignment with\nhigh-level objectives. Existing methods either rely on short-term\nauto-regressive action generation or face limitations in accurately simulating\nrollouts and assessing outcomes, leading to sub-optimal decisions. This paper\nintroduces a novel LLM-based Actor-Critic framework, termed LAC, that\neffectively improves LLM policies with long-term action evaluations in a\nprincipled and scalable way. Our approach addresses two key challenges: (1)\nextracting robust action evaluations by computing Q-values via token logits\nassociated with positive/negative outcomes, enhanced by future trajectory\nrollouts and reasoning; and (2) enabling efficient policy improvement through a\ngradient-free mechanism. Experiments across diverse environments -- including\nhigh-level decision-making (ALFWorld), low-level action spaces (BabyAI-Text),\nand large action spaces (WebShop) -- demonstrate the framework's generality and\nsuperiority over state-of-the-art methods. Notably, our approach achieves\ncompetitive performance using 7B/8B parameter LLMs, even outperforming baseline\nmethods employing GPT-4 in complex tasks. These results underscore the\npotential of integrating structured policy optimization with LLMs' intrinsic\nknowledge to advance decision-making capabilities in multi-step environments.", "AI": {"tldr": "论文提出了一种基于LLM的Actor-Critic框架LAC，通过长期动作评估改进LLM策略，解决了复杂决策场景中的挑战。", "motivation": "LLMs在复杂决策场景中面临长期推理和高级目标对齐的挑战，现有方法存在短期自回归动作生成或模拟不足的问题。", "method": "LAC框架通过计算Q值提取动作评估，结合未来轨迹推演和推理，并采用无梯度机制实现高效策略改进。", "result": "实验表明LAC在多种环境中优于现有方法，甚至在7B/8B参数LLMs上表现优于使用GPT-4的基线方法。", "conclusion": "LAC展示了将结构化策略优化与LLMs内在知识结合的潜力，提升了多步环境中的决策能力。"}}
{"id": "2506.06537", "pdf": "https://arxiv.org/pdf/2506.06537", "abs": "https://arxiv.org/abs/2506.06537", "authors": ["Seung-jae Lee", "Paul Hongsuck Seo"], "title": "Bridging Audio and Vision: Zero-Shot Audiovisual Segmentation by Connecting Pretrained Models", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "Accepted on INTERSPEECH2025", "summary": "Audiovisual segmentation (AVS) aims to identify visual regions corresponding\nto sound sources, playing a vital role in video understanding, surveillance,\nand human-computer interaction. Traditional AVS methods depend on large-scale\npixel-level annotations, which are costly and time-consuming to obtain. To\naddress this, we propose a novel zero-shot AVS framework that eliminates\ntask-specific training by leveraging multiple pretrained models. Our approach\nintegrates audio, vision, and text representations to bridge modality gaps,\nenabling precise sound source segmentation without AVS-specific annotations. We\nsystematically explore different strategies for connecting pretrained models\nand evaluate their efficacy across multiple datasets. Experimental results\ndemonstrate that our framework achieves state-of-the-art zero-shot AVS\nperformance, highlighting the effectiveness of multimodal model integration for\nfinegrained audiovisual segmentation.", "AI": {"tldr": "提出了一种零样本视听分割框架，通过整合预训练模型实现无需任务特定训练的高效分割。", "motivation": "传统视听分割方法依赖大量像素级标注，成本高且耗时，因此需要一种无需特定标注的解决方案。", "method": "整合音频、视觉和文本表征的预训练模型，通过多模态连接策略实现精确分割。", "result": "在多个数据集上实现了零样本视听分割的最先进性能。", "conclusion": "多模态模型整合在细粒度视听分割中具有显著效果。"}}
{"id": "2506.06367", "pdf": "https://arxiv.org/pdf/2506.06367", "abs": "https://arxiv.org/abs/2506.06367", "authors": ["Jiaxin Pan", "Mojtaba Nayyeri", "Osama Mohammed", "Daniel Hernandez", "Rongchuan Zhang", "Cheng Cheng", "Steffen Staab"], "title": "Towards Foundation Model on Temporal Knowledge Graph Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Temporal Knowledge Graphs (TKGs) store temporal facts with quadruple formats\n(s, p, o, t). Existing Temporal Knowledge Graph Embedding (TKGE) models perform\nlink prediction tasks in transductive or semi-inductive settings, which means\nthe entities, relations, and temporal information in the test graph are fully\nor partially observed during training. Such reliance on seen elements during\ninference limits the models' ability to transfer to new domains and generalize\nto real-world scenarios. A central limitation is the difficulty in learning\nrepresentations for entities, relations, and timestamps that are transferable\nand not tied to dataset-specific vocabularies. To overcome these limitations,\nwe introduce the first fully-inductive approach to temporal knowledge graph\nlink prediction. Our model employs sinusoidal positional encodings to capture\nfine-grained temporal patterns and generates adaptive entity and relation\nrepresentations using message passing conditioned on both local and global\ntemporal contexts. Our model design is agnostic to temporal granularity and\ntime span, effectively addressing temporal discrepancies across TKGs and\nfacilitating time-aware structural information transfer. As a pretrained,\nscalable, and transferable model, POSTRA demonstrates strong zero-shot\nperformance on unseen temporal knowledge graphs, effectively generalizing to\nnovel entities, relations, and timestamps. Extensive theoretical analysis and\nempirical results show that a single pretrained model can improve zero-shot\nperformance on various inductive temporal reasoning scenarios, marking a\nsignificant step toward a foundation model for temporal KGs.", "AI": {"tldr": "论文提出了一种全归纳方法POSTRA，用于时序知识图谱链接预测，解决了现有模型依赖训练数据的问题，通过正弦位置编码和消息传递实现零样本泛化。", "motivation": "现有时序知识图谱嵌入模型在推理时依赖训练中观察到的实体、关系和时间信息，限制了模型在新领域的泛化能力。", "method": "采用正弦位置编码捕捉细粒度时间模式，通过消息传递生成自适应的实体和关系表示，模型设计不受时间粒度和跨度限制。", "result": "POSTRA在未见过的时序知识图谱上表现出强大的零样本性能，能泛化到新实体、关系和时间戳。", "conclusion": "POSTRA作为一种预训练、可扩展和可迁移的模型，为时序知识图谱的基础模型迈出了重要一步。"}}
{"id": "2506.06384", "pdf": "https://arxiv.org/pdf/2506.06384", "abs": "https://arxiv.org/abs/2506.06384", "authors": ["Yi Ji", "Runzhi Li", "Baolei Mao"], "title": "Detection Method for Prompt Injection by Integrating Pre-trained Model and Heuristic Feature Engineering", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by KSEM2025 AI & Sec Workshop", "summary": "With the widespread adoption of Large Language Models (LLMs), prompt\ninjection attacks have emerged as a significant security threat. Existing\ndefense mechanisms often face critical trade-offs between effectiveness and\ngeneralizability. This highlights the urgent need for efficient prompt\ninjection detection methods that are applicable across a wide range of LLMs. To\naddress this challenge, we propose DMPI-PMHFE, a dual-channel feature fusion\ndetection framework. It integrates a pretrained language model with heuristic\nfeature engineering to detect prompt injection attacks. Specifically, the\nframework employs DeBERTa-v3-base as a feature extractor to transform input\ntext into semantic vectors enriched with contextual information. In parallel,\nwe design heuristic rules based on known attack patterns to extract explicit\nstructural features commonly observed in attacks. Features from both channels\nare subsequently fused and passed through a fully connected neural network to\nproduce the final prediction. This dual-channel approach mitigates the\nlimitations of relying only on DeBERTa to extract features. Experimental\nresults on diverse benchmark datasets demonstrate that DMPI-PMHFE outperforms\nexisting methods in terms of accuracy, recall, and F1-score. Furthermore, when\ndeployed actually, it significantly reduces attack success rates across\nmainstream LLMs, including GLM-4, LLaMA 3, Qwen 2.5, and GPT-4o.", "AI": {"tldr": "论文提出了一种名为DMPI-PMHFE的双通道特征融合检测框架，用于检测大型语言模型（LLMs）中的提示注入攻击。该方法结合了预训练语言模型和启发式特征工程，显著提升了检测效果。", "motivation": "随着大型语言模型的广泛应用，提示注入攻击成为重大安全威胁。现有防御机制在效果和泛化性之间存在关键权衡，亟需高效且通用的检测方法。", "method": "DMPI-PMHFE框架整合了DeBERTa-v3-base作为语义特征提取器，并结合基于已知攻击模式的启发式规则提取显式结构特征。双通道特征融合后通过全连接神经网络进行预测。", "result": "实验表明，DMPI-PMHFE在准确率、召回率和F1分数上优于现有方法，并在实际部署中显著降低了主流LLMs（如GLM-4、LLaMA 3等）的攻击成功率。", "conclusion": "DMPI-PMHFE通过双通道特征融合有效解决了单一依赖预训练模型的局限性，为提示注入攻击检测提供了高效且通用的解决方案。"}}
{"id": "2506.06563", "pdf": "https://arxiv.org/pdf/2506.06563", "abs": "https://arxiv.org/abs/2506.06563", "authors": ["Thushari Hapuarachchi", "Long Dang", "Kaiqi Xiong"], "title": "Securing Traffic Sign Recognition Systems in Autonomous Vehicles", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": null, "summary": "Deep Neural Networks (DNNs) are widely used for traffic sign recognition\nbecause they can automatically extract high-level features from images. These\nDNNs are trained on large-scale datasets obtained from unknown sources.\nTherefore, it is important to ensure that the models remain secure and are not\ncompromised or poisoned during training. In this paper, we investigate the\nrobustness of DNNs trained for traffic sign recognition. First, we perform the\nerror-minimizing attacks on DNNs used for traffic sign recognition by adding\nimperceptible perturbations on training data. Then, we propose a data\naugmentation-based training method to mitigate the error-minimizing attacks.\nThe proposed training method utilizes nonlinear transformations to disrupt the\nperturbations and improve the model robustness. We experiment with two\nwell-known traffic sign datasets to demonstrate the severity of the attack and\nthe effectiveness of our mitigation scheme. The error-minimizing attacks reduce\nthe prediction accuracy of the DNNs from 99.90% to 10.6%. However, our\nmitigation scheme successfully restores the prediction accuracy to 96.05%.\nMoreover, our approach outperforms adversarial training in mitigating the\nerror-minimizing attacks. Furthermore, we propose a detection model capable of\nidentifying poisoned data even when the perturbations are imperceptible to\nhuman inspection. Our detection model achieves a success rate of over 99% in\nidentifying the attack. This research highlights the need to employ advanced\ntraining methods for DNNs in traffic sign recognition systems to mitigate the\neffects of data poisoning attacks.", "AI": {"tldr": "该论文研究了深度神经网络（DNNs）在交通标志识别中的鲁棒性，提出了一种基于数据增强的训练方法以抵御误差最小化攻击，并设计了一个检测模型来识别被污染的数据。", "motivation": "由于DNNs在交通标志识别中的广泛应用，且训练数据可能来自未知来源，确保模型在训练过程中不被攻击或污染至关重要。", "method": "论文首先通过向训练数据添加微小扰动进行误差最小化攻击，随后提出一种基于非线性变换的数据增强训练方法以提高模型鲁棒性，并设计了一个检测模型来识别被污染的数据。", "result": "误差最小化攻击将DNNs的预测准确率从99.90%降至10.6%，而提出的缓解方案成功将准确率恢复至96.05%，检测模型对攻击的识别成功率超过99%。", "conclusion": "研究表明，在交通标志识别系统中采用先进的训练方法可以有效减轻数据污染攻击的影响，同时检测模型能够高效识别被污染的数据。"}}
{"id": "2506.06470", "pdf": "https://arxiv.org/pdf/2506.06470", "abs": "https://arxiv.org/abs/2506.06470", "authors": ["Yanwei Ren", "Haotian Zhang", "Fuxiang Wu", "Jiayan Qiu", "Jiaxing Huang", "Baosheng Yu", "Liu Liu"], "title": "SIGMA: Refining Large Language Model Reasoning via Sibling-Guided Monte Carlo Augmentation", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Enhancing large language models by simply scaling up datasets has begun to\nyield diminishing returns, shifting the spotlight to data quality. Monte Carlo\nTree Search (MCTS) has emerged as a powerful technique for generating\nhigh-quality chain-of-thought data, yet conventional approaches typically\nretain only the top-scoring trajectory from the search tree, discarding sibling\nnodes that often contain valuable partial insights, recurrent error patterns,\nand alternative reasoning strategies. This unconditional rejection of\nnon-optimal reasoning branches may waste vast amounts of informative data in\nthe whole search tree. We propose SIGMA (Sibling Guided Monte Carlo\nAugmentation), a novel framework that reintegrates these discarded sibling\nnodes to refine LLM reasoning. SIGMA forges semantic links among sibling nodes\nalong each search path and applies a two-stage refinement: a critique model\nidentifies overlooked strengths and weaknesses across the sibling set, and a\nrevision model conducts text-based backpropagation to refine the top-scoring\ntrajectory in light of this comparative feedback. By recovering and amplifying\nthe underutilized but valuable signals from non-optimal reasoning branches,\nSIGMA substantially improves reasoning trajectories. On the challenging MATH\nbenchmark, our SIGMA-tuned 7B model achieves 54.92% accuracy using only 30K\nsamples, outperforming state-of-the-art models trained on 590K samples. This\nresult highlights that our sibling-guided optimization not only significantly\nreduces data usage but also significantly boosts LLM reasoning.", "AI": {"tldr": "SIGMA框架通过重新利用被丢弃的非最优推理分支（兄弟节点）来提升大语言模型的推理能力，显著减少数据需求并提高性能。", "motivation": "传统方法仅保留搜索树中的最优轨迹，丢弃了包含有价值信息的兄弟节点，导致数据浪费。SIGMA旨在利用这些被忽视的信息优化模型推理。", "method": "SIGMA通过建立兄弟节点间的语义链接，采用两阶段优化：批评模型识别优缺点，修订模型基于反馈优化最优轨迹。", "result": "在MATH基准测试中，SIGMA调优的7B模型仅用30K样本达到54.92%准确率，优于使用590K样本的现有最佳模型。", "conclusion": "SIGMA通过利用非最优推理分支的有价值信号，显著提升推理能力并减少数据需求。"}}
{"id": "2506.06395", "pdf": "https://arxiv.org/pdf/2506.06395", "abs": "https://arxiv.org/abs/2506.06395", "authors": ["Pengyi Li", "Matvey Skripkin", "Alexander Zubrey", "Andrey Kuznetsov", "Ivan Oseledets"], "title": "Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) excel at reasoning, yet post-training remains\ncritical for aligning their behavior with task goals. Existing reinforcement\nlearning (RL) methods often depend on costly human annotations or external\nreward models. We propose Reinforcement Learning via Self-Confidence (RLSC),\nwhich uses the model's own confidence as reward signals-eliminating the need\nfor labels, preference models, or reward engineering. Applied to\nQwen2.5-Math-7B with only 8 samples per question and 4 training epochs, RLSC\nimproves accuracy by +20.10% on AIME2024, +49.40% on MATH500, and +52.50% on\nAMC23. RLSC offers a simple, scalable post-training method for reasoning models\nwith minimal supervision.", "AI": {"tldr": "RLSC是一种利用模型自身置信度作为奖励信号的后训练方法，无需人工标注或外部奖励模型，显著提升了数学推理任务的性能。", "motivation": "现有强化学习方法依赖昂贵的人工标注或外部奖励模型，RLSC旨在通过模型自身置信度解决这一问题。", "method": "RLSC将模型的置信度作为奖励信号，应用于Qwen2.5-Math-7B模型，仅需每个问题8个样本和4个训练周期。", "result": "在AIME2024、MATH500和AMC23数据集上，RLSC分别提升了20.10%、49.40%和52.50%的准确率。", "conclusion": "RLSC提供了一种简单、可扩展且需要最少监督的后训练方法，适用于推理模型。"}}
{"id": "2506.06569", "pdf": "https://arxiv.org/pdf/2506.06569", "abs": "https://arxiv.org/abs/2506.06569", "authors": ["Yannis Spyridis", "Vasileios Argyriou"], "title": "Textile Analysis for Recycling Automation using Transfer Learning and Zero-Shot Foundation Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Automated sorting is crucial for improving the efficiency and scalability of\ntextile recycling, but accurately identifying material composition and\ndetecting contaminants from sensor data remains challenging. This paper\ninvestigates the use of standard RGB imagery, a cost-effective sensing\nmodality, for key pre-processing tasks in an automated system. We present\ncomputer vision components designed for a conveyor belt setup to perform (a)\nclassification of four common textile types and (b) segmentation of non-textile\nfeatures such as buttons and zippers. For classification, several pre-trained\narchitectures were evaluated using transfer learning and cross-validation, with\nEfficientNetB0 achieving the best performance on a held-out test set with\n81.25\\% accuracy. For feature segmentation, a zero-shot approach combining the\nGrounding DINO open-vocabulary detector with the Segment Anything Model (SAM)\nwas employed, demonstrating excellent performance with a mIoU of 0.90 for the\ngenerated masks against ground truth. This study demonstrates the feasibility\nof using RGB images coupled with modern deep learning techniques, including\ntransfer learning for classification and foundation models for zero-shot\nsegmentation, to enable essential analysis steps for automated textile\nrecycling pipelines.", "AI": {"tldr": "论文探讨了利用RGB图像和深度学习技术（如迁移学习和基础模型）实现纺织品自动分类和污染物分割的可行性。", "motivation": "纺织品回收中，准确识别材料成分和检测污染物是提高效率和可扩展性的关键挑战。", "method": "使用RGB图像，结合迁移学习（EfficientNetB0）进行分类，以及零样本方法（Grounding DINO + SAM）进行分割。", "result": "分类准确率达81.25%，分割的mIoU为0.90。", "conclusion": "RGB图像结合现代深度学习技术可用于纺织品回收的关键预处理任务。"}}
{"id": "2506.06523", "pdf": "https://arxiv.org/pdf/2506.06523", "abs": "https://arxiv.org/abs/2506.06523", "authors": ["Sumanth Pillella"], "title": "Reinforcement Learning for Autonomous Warehouse Orchestration in SAP Logistics Execution: Redefining Supply Chain Agility", "categories": ["cs.AI"], "comment": "6 pages", "summary": "In an era of escalating supply chain demands, SAP Logistics Execution (LE) is\npivotal for managing warehouse operations, transportation, and delivery. This\nresearch introduces a pioneering framework leveraging reinforcement learning\n(RL) to autonomously orchestrate warehouse tasks in SAP LE, enhancing\noperational agility and efficiency. By modeling warehouse processes as dynamic\nenvironments, the framework optimizes task allocation, inventory movement, and\norder picking in real-time. A synthetic dataset of 300,000 LE transactions\nsimulates real-world warehouse scenarios, including multilingual data and\noperational disruptions. The analysis achieves 95% task optimization accuracy,\nreducing processing times by 60% compared to traditional methods.\nVisualizations, including efficiency heatmaps and performance graphs, guide\nagile warehouse strategies. This approach tackles data privacy, scalability,\nand SAP integration, offering a transformative solution for modern supply\nchains.", "AI": {"tldr": "本文提出了一种基于强化学习的框架，用于优化SAP物流执行系统中的仓库任务，显著提高了效率和准确性。", "motivation": "在供应链需求日益增长的背景下，SAP物流执行系统需要更高效的仓库任务管理方法。", "method": "通过强化学习建模仓库动态环境，实时优化任务分配、库存移动和订单拣选，并使用合成数据集进行验证。", "result": "实现了95%的任务优化准确率，处理时间减少60%。", "conclusion": "该框架为现代供应链提供了可扩展、隐私安全的解决方案，具有实际应用潜力。"}}
{"id": "2506.06396", "pdf": "https://arxiv.org/pdf/2506.06396", "abs": "https://arxiv.org/abs/2506.06396", "authors": ["Christopher D. Molek", "Roberto Fronteddu", "K. Brent Venable", "Niranjan Suri"], "title": "Natural Language Interaction with Databases on Edge Devices in the Internet of Battlefield Things", "categories": ["cs.CL", "cs.AI", "cs.DB"], "comment": null, "summary": "The expansion of the Internet of Things (IoT) in the battlefield, Internet of\nBattlefield Things (IoBT), gives rise to new opportunities for enhancing\nsituational awareness. To increase the potential of IoBT for situational\nawareness in critical decision making, the data from these devices must be\nprocessed into consumer-ready information objects, and made available to\nconsumers on demand. To address this challenge we propose a workflow that makes\nuse of natural language processing (NLP) to query a database technology and\nreturn a response in natural language. Our solution utilizes Large Language\nModels (LLMs) that are sized for edge devices to perform NLP as well as\ngraphical databases which are well suited for dynamic connected networks which\nare pervasive in the IoBT. Our architecture employs LLMs for both mapping\nquestions in natural language to Cypher database queries as well as to\nsummarize the database output back to the user in natural language. We evaluate\nseveral medium sized LLMs for both of these tasks on a database representing\npublicly available data from the US Army's Multipurpose Sensing Area (MSA) at\nthe Jornada Range in Las Cruces, NM. We observe that Llama 3.1 (8 billion\nparameters) outperforms the other models across all the considered metrics.\nMost importantly, we note that, unlike current methods, our two step approach\nallows the relaxation of the Exact Match (EM) requirement of the produced\nCypher queries with ground truth code and, in this way, it achieves a 19.4%\nincrease in accuracy. Our workflow lays the ground work for deploying LLMs on\nedge devices to enable natural language interactions with databases containing\ninformation objects for critical decision making.", "AI": {"tldr": "论文提出了一种利用自然语言处理（NLP）和大型语言模型（LLMs）在战场物联网（IoBT）中实现自然语言查询数据库的解决方案，显著提升了查询准确性和实用性。", "motivation": "提升战场物联网（IoBT）在关键决策中的情境感知能力，通过自然语言处理技术将设备数据转化为用户友好的信息对象。", "method": "采用LLMs和图形数据库技术，设计了一个两步工作流程：将自然语言问题映射为Cypher查询，并将数据库结果用自然语言总结返回。", "result": "实验表明，Llama 3.1（80亿参数）在所有指标中表现最佳，且两步方法使查询准确率提升了19.4%。", "conclusion": "该工作流程为在边缘设备上部署LLMs提供了基础，支持通过自然语言与数据库交互，助力关键决策。"}}
{"id": "2506.06578", "pdf": "https://arxiv.org/pdf/2506.06578", "abs": "https://arxiv.org/abs/2506.06578", "authors": ["Anees Nashath Shaik", "Barbara Villarini", "Vasileios Argyriou"], "title": "A Deep Learning Approach for Facial Attribute Manipulation and Reconstruction in Surveillance and Reconnaissance", "categories": ["cs.CV"], "comment": null, "summary": "Surveillance systems play a critical role in security and reconnaissance, but\ntheir performance is often compromised by low-quality images and videos,\nleading to reduced accuracy in face recognition. Additionally, existing\nAI-based facial analysis models suffer from biases related to skin tone\nvariations and partially occluded faces, further limiting their effectiveness\nin diverse real-world scenarios. These challenges are the results of data\nlimitations and imbalances, where available training datasets lack sufficient\ndiversity, resulting in unfair and unreliable facial recognition performance.\nTo address these issues, we propose a data-driven platform that enhances\nsurveillance capabilities by generating synthetic training data tailored to\ncompensate for dataset biases. Our approach leverages deep learning-based\nfacial attribute manipulation and reconstruction using autoencoders and\nGenerative Adversarial Networks (GANs) to create diverse and high-quality\nfacial datasets. Additionally, our system integrates an image enhancement\nmodule, improving the clarity of low-resolution or occluded faces in\nsurveillance footage. We evaluate our approach using the CelebA dataset,\ndemonstrating that the proposed platform enhances both training data diversity\nand model fairness. This work contributes to reducing bias in AI-based facial\nanalysis and improving surveillance accuracy in challenging environments,\nleading to fairer and more reliable security applications.", "AI": {"tldr": "论文提出了一种数据驱动平台，通过生成合成训练数据来解决现有AI面部识别模型中的偏见和低质量问题，提升监控系统的准确性和公平性。", "motivation": "现有监控系统因低质量图像和视频导致面部识别准确性下降，同时AI模型存在肤色和遮挡相关的偏见。这些问题源于数据集的局限性和不平衡。", "method": "利用基于深度学习的面部属性操纵和重建（如自动编码器和GANs）生成多样化合成数据，并集成图像增强模块提升低分辨率或遮挡面部的清晰度。", "result": "在CelebA数据集上的评估表明，该平台提升了训练数据的多样性和模型公平性。", "conclusion": "该工作有助于减少AI面部分析的偏见，提升监控系统在复杂环境中的准确性和可靠性。"}}
{"id": "2506.06524", "pdf": "https://arxiv.org/pdf/2506.06524", "abs": "https://arxiv.org/abs/2506.06524", "authors": ["Sam Earle", "Ahmed Khalifa", "Muhammad Umair Nasir", "Zehua Jiang", "Graham Todd", "Andrzej Banburski-Fahey", "Julian Togelius"], "title": "ScriptDoctor: Automatic Generation of PuzzleScript Games via Large Language Models and Tree Search", "categories": ["cs.AI", "cs.HC"], "comment": "5 pages, 3 figures, 3 tables, submitted to IEEE Conference on Games\n  as a Short Paper", "summary": "There is much interest in using large pre-trained models in Automatic Game\nDesign (AGD), whether via the generation of code, assets, or more abstract\nconceptualization of design ideas. But so far this interest largely stems from\nthe ad hoc use of such generative models under persistent human supervision.\nMuch work remains to show how these tools can be integrated into\nlonger-time-horizon AGD pipelines, in which systems interface with game engines\nto test generated content autonomously. To this end, we introduce ScriptDoctor,\na Large Language Model (LLM)-driven system for automatically generating and\ntesting games in PuzzleScript, an expressive but highly constrained description\nlanguage for turn-based puzzle games over 2D gridworlds. ScriptDoctor generates\nand tests game design ideas in an iterative loop, where human-authored examples\nare used to ground the system's output, compilation errors from the\nPuzzleScript engine are used to elicit functional code, and search-based agents\nplay-test generated games. ScriptDoctor serves as a concrete example of the\npotential of automated, open-ended LLM-based workflows in generating novel game\ncontent.", "AI": {"tldr": "ScriptDoctor是一个基于大型语言模型（LLM）的系统，用于自动生成和测试PuzzleScript游戏，展示了LLM在自动游戏设计中的潜力。", "motivation": "当前大型预训练模型在自动游戏设计（AGD）中的应用多依赖人工监督，缺乏长期自主集成到游戏引擎中的研究。", "method": "ScriptDoctor通过迭代循环生成和测试游戏设计，结合人类示例、引擎错误反馈和基于搜索的代理进行游戏测试。", "result": "ScriptDemonstrated the potential of LLM-based workflows in generating novel game content autonomously.", "conclusion": "ScriptDoctor为LLM在开放式的自动游戏设计流程中的应用提供了具体范例。"}}
{"id": "2506.06401", "pdf": "https://arxiv.org/pdf/2506.06401", "abs": "https://arxiv.org/abs/2506.06401", "authors": ["Hongming Yang", "Shi Lin", "Jun Shao", "Changting Lin", "Donghai Zhu", "Meng Han", "Qinglei Kong"], "title": "Direct Behavior Optimization: Unlocking the Potential of Lightweight LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This work is accepted at ACL 2025", "summary": "Lightweight Large Language Models (LwLLMs) are reduced-parameter, optimized\nmodels designed to run efficiently on consumer-grade hardware, offering\nsignificant advantages in resource efficiency, cost-effectiveness, and data\nprivacy. However, these models often struggle with limited inference and\nreasoning capabilities, which restrict their performance on complex tasks and\nlimit their practical applicability. Moreover, existing prompt optimization\nmethods typically rely on extensive manual effort or the meta-cognitive\nabilities of state-of-the-art LLMs, making them less effective for LwLLMs. To\naddress these challenges, we introduce DeBoP, a new Direct Behavior\nOptimization Paradigm, original from the Chain-of-Thought (CoT) prompting\ntechnique. Unlike CoT Prompting, DeBoP is an automatic optimization method,\nwhich focuses on the optimization directly on the behavior of LwLLMs. In\nparticular, DeBoP transforms the optimization of complex prompts into the\noptimization of discrete, quantifiable execution sequences using a\ngradient-free Monte Carlo Tree Search. We evaluate DeBoP on seven challenging\ntasks where state-of-the-art LLMs excel but LwLLMs generally underperform.\nExperimental results demonstrate that DeBoP significantly outperforms recent\nprompt optimization methods on most tasks. In particular, DeBoP-optimized\nLwLLMs surpass GPT-3.5 on most tasks while reducing computational time by\napproximately 60% compared to other automatic prompt optimization methods.", "AI": {"tldr": "DeBoP是一种直接行为优化范式，通过梯度自由的蒙特卡洛树搜索优化轻量级大语言模型（LwLLMs）的行为，显著提升其性能，并在多项任务中超越GPT-3.5，同时减少计算时间。", "motivation": "轻量级大语言模型（LwLLMs）在资源效率、成本效益和数据隐私方面具有优势，但其推理和推理能力有限，且现有提示优化方法对其效果不佳。", "method": "提出DeBoP，一种基于蒙特卡洛树搜索的直接行为优化方法，将复杂提示优化转化为离散、可量化的执行序列优化。", "result": "DeBoP在七项挑战性任务中显著优于现有提示优化方法，优化后的LwLLMs在多数任务中超越GPT-3.5，计算时间减少约60%。", "conclusion": "DeBoP为轻量级大语言模型提供了一种高效的自动优化方法，显著提升了其性能和应用潜力。"}}
{"id": "2506.06596", "pdf": "https://arxiv.org/pdf/2506.06596", "abs": "https://arxiv.org/abs/2506.06596", "authors": ["Youssef Farah", "Federico Paredes-Vallés", "Guido De Croon", "Muhammad Ahmed Humais", "Hussain Sajwani", "Yahya Zweiri"], "title": "EV-LayerSegNet: Self-supervised Motion Segmentation using Event Cameras", "categories": ["cs.CV"], "comment": "This paper has been accepted for publication at the IEEE Conference\n  on Computer Vision and Pattern Recognition (CVPR) Workshops, Nashville, 2025", "summary": "Event cameras are novel bio-inspired sensors that capture motion dynamics\nwith much higher temporal resolution than traditional cameras, since pixels\nreact asynchronously to brightness changes. They are therefore better suited\nfor tasks involving motion such as motion segmentation. However, training\nevent-based networks still represents a difficult challenge, as obtaining\nground truth is very expensive, error-prone and limited in frequency. In this\narticle, we introduce EV-LayerSegNet, a self-supervised CNN for event-based\nmotion segmentation. Inspired by a layered representation of the scene\ndynamics, we show that it is possible to learn affine optical flow and\nsegmentation masks separately, and use them to deblur the input events. The\ndeblurring quality is then measured and used as self-supervised learning loss.\nWe train and test the network on a simulated dataset with only affine motion,\nachieving IoU and detection rate up to 71% and 87% respectively.", "AI": {"tldr": "EV-LayerSegNet是一种自监督CNN，用于事件相机的运动分割，通过分层场景动态表示学习光流和分割掩码，并以去模糊质量作为自监督损失。", "motivation": "事件相机在运动任务中表现优异，但训练网络时获取真实标签成本高且困难。", "method": "提出EV-LayerSegNet，通过分层表示分别学习仿射光流和分割掩码，并用于事件去模糊，以去模糊质量作为自监督损失。", "result": "在仅含仿射运动的模拟数据集上，IoU和检测率分别达到71%和87%。", "conclusion": "EV-LayerSegNet在自监督下有效实现了事件相机的运动分割。"}}
{"id": "2506.06574", "pdf": "https://arxiv.org/pdf/2506.06574", "abs": "https://arxiv.org/abs/2506.06574", "authors": ["Suhana Bedi", "Iddah Mlauzi", "Daniel Shin", "Sanmi Koyejo", "Nigam H. Shah"], "title": "The Optimization Paradox in Clinical AI Multi-Agent Systems", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "Multi-agent artificial intelligence systems are increasingly deployed in\nclinical settings, yet the relationship between component-level optimization\nand system-wide performance remains poorly understood. We evaluated this\nrelationship using 2,400 real patient cases from the MIMIC-CDM dataset across\nfour abdominal pathologies (appendicitis, pancreatitis, cholecystitis,\ndiverticulitis), decomposing clinical diagnosis into information gathering,\ninterpretation, and differential diagnosis. We evaluated single agent systems\n(one model performing all tasks) against multi-agent systems (specialized\nmodels for each task) using comprehensive metrics spanning diagnostic outcomes,\nprocess adherence, and cost efficiency. Our results reveal a paradox: while\nmulti-agent systems generally outperformed single agents, the\ncomponent-optimized or Best of Breed system with superior components and\nexcellent process metrics (85.5% information accuracy) significantly\nunderperformed in diagnostic accuracy (67.7% vs. 77.4% for a top multi-agent\nsystem). This finding underscores that successful integration of AI in\nhealthcare requires not just component level optimization but also attention to\ninformation flow and compatibility between agents. Our findings highlight the\nneed for end to end system validation rather than relying on component metrics\nalone.", "AI": {"tldr": "多智能体AI系统在临床诊断中表现优于单智能体系统，但组件优化的系统在诊断准确率上反而表现较差，强调了系统整体验证的重要性。", "motivation": "研究多智能体AI系统中组件优化与系统整体性能之间的关系，以提升临床诊断的准确性和效率。", "method": "使用MIMIC-CDM数据集中的2,400例患者数据，比较单智能体系统和多智能体系统在信息收集、解释和鉴别诊断中的表现。", "result": "多智能体系统整体表现更优，但组件优化的系统在诊断准确率上显著低于表现最佳的多智能体系统（67.7% vs. 77.4%）。", "conclusion": "AI在医疗中的成功整合不仅需要组件优化，还需关注信息流和智能体间的兼容性，强调系统整体验证的必要性。"}}
{"id": "2506.06404", "pdf": "https://arxiv.org/pdf/2506.06404", "abs": "https://arxiv.org/abs/2506.06404", "authors": ["Sooyung Choi", "Jaehyeok Lee", "Xiaoyuan Yi", "Jing Yao", "Xing Xie", "JinYeong Bak"], "title": "Unintended Harms of Value-Aligned LLMs: Psychological and Empirical Insights", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG"], "comment": "Accepted to ACL 2025", "summary": "The application scope of Large Language Models (LLMs) continues to expand,\nleading to increasing interest in personalized LLMs that align with human\nvalues. However, aligning these models with individual values raises\nsignificant safety concerns, as certain values may correlate with harmful\ninformation. In this paper, we identify specific safety risks associated with\nvalue-aligned LLMs and investigate the psychological principles behind these\nchallenges. Our findings reveal two key insights. (1) Value-aligned LLMs are\nmore prone to harmful behavior compared to non-fine-tuned models and exhibit\nslightly higher risks in traditional safety evaluations than other fine-tuned\nmodels. (2) These safety issues arise because value-aligned LLMs genuinely\ngenerate text according to the aligned values, which can amplify harmful\noutcomes. Using a dataset with detailed safety categories, we find significant\ncorrelations between value alignment and safety risks, supported by\npsychological hypotheses. This study offers insights into the \"black box\" of\nvalue alignment and proposes in-context alignment methods to enhance the safety\nof value-aligned LLMs.", "AI": {"tldr": "研究发现，与人类价值观对齐的大型语言模型（LLMs）更容易产生有害行为，且安全风险更高，原因在于其生成内容会放大有害结果。", "motivation": "随着LLMs应用范围扩大，个性化对齐人类价值观的需求增加，但同时也引发了安全担忧，尤其是某些价值观可能与有害信息相关。", "method": "通过数据集分析，结合心理学假设，研究价值观对齐LLMs的安全风险。", "result": "价值观对齐的LLMs比未微调模型更易产生有害行为，且在传统安全评估中风险略高。", "conclusion": "研究揭示了价值观对齐的“黑箱”问题，并提出上下文对齐方法以提升安全性。"}}
{"id": "2506.06600", "pdf": "https://arxiv.org/pdf/2506.06600", "abs": "https://arxiv.org/abs/2506.06600", "authors": ["Tan-Hanh Pham", "Chris Ngo"], "title": "RARL: Improving Medical VLM Reasoning and Generalization with Reinforcement Learning and LoRA under Data and Hardware Constraints", "categories": ["cs.CV"], "comment": "Under review", "summary": "The growing integration of vision-language models (VLMs) in medical\napplications offers promising support for diagnostic reasoning. However,\ncurrent medical VLMs often face limitations in generalization, transparency,\nand computational efficiency-barriers that hinder deployment in real-world,\nresource-constrained settings. To address these challenges, we propose a\nReasoning-Aware Reinforcement Learning framework, \\textbf{RARL}, that enhances\nthe reasoning capabilities of medical VLMs while remaining efficient and\nadaptable to low-resource environments. Our approach fine-tunes a lightweight\nbase model, Qwen2-VL-2B-Instruct, using Low-Rank Adaptation and custom reward\nfunctions that jointly consider diagnostic accuracy and reasoning quality.\nTraining is performed on a single NVIDIA A100-PCIE-40GB GPU, demonstrating the\nfeasibility of deploying such models in constrained environments. We evaluate\nthe model using an LLM-as-judge framework that scores both correctness and\nexplanation quality. Experimental results show that RARL significantly improves\nVLM performance in medical image analysis and clinical reasoning, outperforming\nsupervised fine-tuning on reasoning-focused tasks by approximately 7.78%, while\nrequiring fewer computational resources. Additionally, we demonstrate the\ngeneralization capabilities of our approach on unseen datasets, achieving\naround 27% improved performance compared to supervised fine-tuning and about 4%\nover traditional RL fine-tuning. Our experiments also illustrate that diversity\nprompting during training and reasoning prompting during inference are crucial\nfor enhancing VLM performance. Our findings highlight the potential of\nreasoning-guided learning and reasoning prompting to steer medical VLMs toward\nmore transparent, accurate, and resource-efficient clinical decision-making.\nCode and data are publicly available.", "AI": {"tldr": "提出了一种名为RARL的框架，通过强化学习提升医学视觉语言模型的推理能力，同时保持高效和适应低资源环境。", "motivation": "当前医学视觉语言模型在泛化性、透明性和计算效率方面存在局限，阻碍了其在资源受限的实际环境中的部署。", "method": "使用低秩适应和自定义奖励函数对轻量级基础模型Qwen2-VL-2B-Instruct进行微调，训练在单张NVIDIA A100-PCIE-40GB GPU上完成。", "result": "RARL在医学图像分析和临床推理任务中显著优于监督微调，推理任务性能提升约7.78%，并在未见数据集上表现更优。", "conclusion": "推理引导的学习和推理提示能有效提升医学视觉语言模型的透明性、准确性和资源效率。"}}
{"id": "2506.06580", "pdf": "https://arxiv.org/pdf/2506.06580", "abs": "https://arxiv.org/abs/2506.06580", "authors": ["Xiaoran Liu", "Istvan David"], "title": "AI Simulation by Digital Twins: Systematic Survey, Reference Framework, and Mapping to a Standardized Architecture", "categories": ["cs.AI", "cs.ET", "cs.SE", "cs.SY", "eess.SY"], "comment": null, "summary": "Insufficient data volume and quality are particularly pressing challenges in\nthe adoption of modern subsymbolic AI. To alleviate these challenges, AI\nsimulation uses virtual training environments in which AI agents can be safely\nand efficiently developed with simulated, synthetic data. Digital twins open\nnew avenues in AI simulation, as these high-fidelity virtual replicas of\nphysical systems are equipped with state-of-the-art simulators and the ability\nto further interact with the physical system for additional data collection. In\nthis article, we report on our systematic survey of digital twin-enabled AI\nsimulation. By analyzing 22 primary studies, we identify technological trends\nand derive a reference framework to situate digital twins and AI components.\nBased on our findings, we derive a reference framework and provide\narchitectural guidelines by mapping it onto the ISO 23247 reference\narchitecture for digital twins. Finally, we identify challenges and research\nopportunities for prospective researchers.", "AI": {"tldr": "本文探讨了数字孪生技术在AI模拟中的应用，通过系统调查22项研究，总结了技术趋势并提出了参考框架和架构指南。", "motivation": "解决现代亚符号AI中数据量和质量不足的问题，利用数字孪生技术提升AI模拟的效率和安全性。", "method": "通过系统调查22项主要研究，分析数字孪生与AI组件的技术趋势，并基于ISO 23247参考架构提出框架。", "result": "提出了数字孪生与AI模拟的参考框架和架构指南，并指出了未来研究的挑战和机会。", "conclusion": "数字孪生为AI模拟提供了高效且安全的解决方案，但仍需进一步研究以应对挑战。"}}
{"id": "2506.06406", "pdf": "https://arxiv.org/pdf/2506.06406", "abs": "https://arxiv.org/abs/2506.06406", "authors": ["Guoyang Xia", "Yifeng Ding", "Fengfa Li", "Lei Ren", "Chen Wei", "Fangxiang Feng", "Xiaojie Wang"], "title": "SMAR: Soft Modality-Aware Routing Strategy for MoE-based Multimodal Large Language Models Preserving Language Capabilities", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Mixture of Experts (MoE) architectures have become a key approach for scaling\nlarge language models, with growing interest in extending them to multimodal\ntasks. Existing methods to build multimodal MoE models either incur high\ntraining costs or suffer from degraded language capabilities when adapting\npretrained models. To address this, we propose Soft ModalityAware Routing\n(SMAR), a novel regularization technique that uses Kullback Leibler divergence\nto control routing probability distributions across modalities, encouraging\nexpert specialization without modifying model architecture or heavily relying\non textual data. Experiments on visual instruction tuning show that SMAR\npreserves language ability at 86.6% retention with only 2.5% pure text,\noutperforming baselines while maintaining strong multimodal performance. Our\napproach offers a practical and efficient solution to balance modality\ndifferentiation and language capabilities in multimodal MoE models.", "AI": {"tldr": "SMAR是一种新的正则化技术，通过KL散度控制多模态路由概率分布，平衡专家专业化和语言能力。", "motivation": "解决多模态MoE模型训练成本高或语言能力下降的问题。", "method": "提出Soft ModalityAware Routing (SMAR)，利用KL散度调节路由概率分布。", "result": "在视觉指令调优中，SMAR保留86.6%的语言能力，仅需2.5%纯文本，优于基线。", "conclusion": "SMAR为多模态MoE模型提供了一种高效平衡模态差异和语言能力的解决方案。"}}
{"id": "2506.06602", "pdf": "https://arxiv.org/pdf/2506.06602", "abs": "https://arxiv.org/abs/2506.06602", "authors": ["Santhosh Kakarla", "Gautama Shastry Bulusu Venkata"], "title": "Zero Shot Composed Image Retrieval", "categories": ["cs.CV"], "comment": "8 pages, 3 figures", "summary": "Composed image retrieval (CIR) allows a user to locate a target image by\napplying a fine-grained textual edit (e.g., ``turn the dress blue'' or ``remove\nstripes'') to a reference image. Zero-shot CIR, which embeds the image and the\ntext with separate pretrained vision-language encoders, reaches only 20-25\\%\nRecall@10 on the FashionIQ benchmark. We improve this by fine-tuning BLIP-2\nwith a lightweight Q-Former that fuses visual and textual features into a\nsingle embedding, raising Recall@10 to 45.6\\% (shirt), 40.1\\% (dress), and\n50.4\\% (top-tee) and increasing the average Recall@50 to 67.6\\%. We also\nexamine Retrieval-DPO, which fine-tunes CLIP's text encoder with a Direct\nPreference Optimization loss applied to FAISS-mined hard negatives. Despite\nextensive tuning of the scaling factor, index, and sampling strategy,\nRetrieval-DPO attains only 0.02\\% Recall@10 -- far below zero-shot and\nprompt-tuned baselines -- because it (i) lacks joint image-text fusion, (ii)\nuses a margin objective misaligned with top-$K$ metrics, (iii) relies on\nlow-quality negatives, and (iv) keeps the vision and Transformer layers frozen.\nOur results show that effective preference-based CIR requires genuine\nmultimodal fusion, ranking-aware objectives, and carefully curated negatives.", "AI": {"tldr": "论文通过改进BLIP-2和Retrieval-DPO方法，提升了零样本CIR任务的性能，但发现后者效果不佳，并分析了原因。", "motivation": "解决零样本CIR任务中性能较低的问题，探索更有效的多模态融合方法。", "method": "1. 微调BLIP-2，使用轻量级Q-Former融合视觉和文本特征；2. 尝试Retrieval-DPO方法，通过DPO损失微调CLIP文本编码器。", "result": "BLIP-2改进后Recall@10显著提升（45.6%衬衫，40.1%裙子，50.4%上衣）；Retrieval-DPO效果极差（0.02% Recall@10）。", "conclusion": "有效的CIR需要多模态融合、排名感知目标和高质量负样本。"}}
{"id": "2506.06634", "pdf": "https://arxiv.org/pdf/2506.06634", "abs": "https://arxiv.org/abs/2506.06634", "authors": ["Yubin Xiao", "Di Wang", "Rui Cao", "Xuan Wu", "Boyang Li", "You Zhou"], "title": "GELD: A Unified Neural Model for Efficiently Solving Traveling Salesman Problems Across Different Scales", "categories": ["cs.AI"], "comment": "21pages, 4 figures, and 14 tables", "summary": "The Traveling Salesman Problem (TSP) is a well-known combinatorial\noptimization problem with broad real-world applications. Recent advancements in\nneural network-based TSP solvers have shown promising results. Nonetheless,\nthese models often struggle to efficiently solve both small- and large-scale\nTSPs using the same set of pre-trained model parameters, limiting their\npractical utility. To address this issue, we introduce a novel neural TSP\nsolver named GELD, built upon our proposed broad global assessment and refined\nlocal selection framework. Specifically, GELD integrates a lightweight\nGlobal-view Encoder (GE) with a heavyweight Local-view Decoder (LD) to enrich\nembedding representation while accelerating the decision-making process.\nMoreover, GE incorporates a novel low-complexity attention mechanism, allowing\nGELD to achieve low inference latency and scalability to larger-scale TSPs.\nAdditionally, we propose a two-stage training strategy that utilizes training\ninstances of different sizes to bolster GELD's generalization ability.\nExtensive experiments conducted on both synthetic and real-world datasets\ndemonstrate that GELD outperforms seven state-of-the-art models considering\nboth solution quality and inference speed. Furthermore, GELD can be employed as\na post-processing method to significantly elevate the quality of the solutions\nderived by existing neural TSP solvers via spending affordable additional\ncomputing time. Notably, GELD is shown as capable of solving TSPs with up to\n744,710 nodes, first-of-its-kind to solve this large size TSP without relying\non divide-and-conquer strategies to the best of our knowledge.", "AI": {"tldr": "GELD是一种新型神经TSP求解器，结合全局评估和局部选择框架，显著提升求解质量和速度，并能处理超大规模TSP问题。", "motivation": "现有神经TSP求解器难以同时高效处理小规模和大规模问题，限制了实际应用。", "method": "提出GELD框架，结合轻量级全局编码器（GE）和重量级局部解码器（LD），采用低复杂度注意力机制和两阶段训练策略。", "result": "GELD在合成和真实数据集上优于七种先进模型，并能处理多达744,710节点的TSP问题。", "conclusion": "GELD不仅提升了神经TSP求解器的性能，还可作为后处理方法显著改进现有求解器的解质量。"}}
{"id": "2506.06446", "pdf": "https://arxiv.org/pdf/2506.06446", "abs": "https://arxiv.org/abs/2506.06446", "authors": ["Ivi Chatzi", "Nina Corvelo Benz", "Stratis Tsirtsis", "Manuel Gomez-Rodriguez"], "title": "Canonical Autoregressive Generation", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "comment": null, "summary": "State of the art large language models are trained using large amounts of\ntokens derived from raw text using what is called a tokenizer. Crucially, the\ntokenizer determines the (token) vocabulary a model will use during inference\nas well as, in principle, the (token) language. This is because, while the\ntoken vocabulary may allow for different tokenizations of a string, the\ntokenizer always maps the string to only one of these tokenizations--the\ncanonical tokenization. However, multiple lines of empirical evidence suggest\nthat large language models do not always generate canonical token sequences,\nand this comes with several negative consequences. In this work, we first show\nthat, to generate a canonical token sequence, a model needs to generate\n(partial) canonical token sequences at each step of the autoregressive\ngeneration process underpinning its functioning. Building upon this theoretical\nresult, we introduce canonical sampling, a simple and efficient sampling method\nthat precludes a given model from generating non-canonical token sequences.\nFurther, we also show that, in comparison with standard sampling, the\ndistribution of token sequences generated using canonical sampling is provably\ncloser to the true distribution of token sequences used during training.", "AI": {"tldr": "论文提出了一种称为“规范采样”的方法，旨在解决大语言模型生成非规范标记序列的问题，并证明其生成的序列更接近训练数据的真实分布。", "motivation": "现有大语言模型在推理时可能生成非规范的标记序列，导致负面后果。本文旨在解决这一问题。", "method": "通过理论分析，提出“规范采样”方法，确保模型在自回归生成过程中每一步都生成规范的标记序列。", "result": "规范采样方法有效避免了非规范序列的生成，且生成的序列分布更接近训练数据的真实分布。", "conclusion": "规范采样是一种简单高效的方法，能够提升大语言模型生成序列的规范性，并改善其与训练数据的一致性。"}}
{"id": "2506.06631", "pdf": "https://arxiv.org/pdf/2506.06631", "abs": "https://arxiv.org/abs/2506.06631", "authors": ["Minghao Zou", "Qingtian Zeng", "Yongping Miao", "Shangkun Liu", "Zilong Wang", "Hantao Liu", "Wei Zhou"], "title": "PhysLab: A Benchmark Dataset for Multi-Granularity Visual Parsing of Physics Experiments", "categories": ["cs.CV"], "comment": null, "summary": "Visual parsing of images and videos is critical for a wide range of\nreal-world applications. However, progress in this field is constrained by\nlimitations of existing datasets: (1) insufficient annotation granularity,\nwhich impedes fine-grained scene understanding and high-level reasoning; (2)\nlimited coverage of domains, particularly a lack of datasets tailored for\neducational scenarios; and (3) lack of explicit procedural guidance, with\nminimal logical rules and insufficient representation of structured task\nprocess. To address these gaps, we introduce PhysLab, the first video dataset\nthat captures students conducting complex physics experiments. The dataset\nincludes four representative experiments that feature diverse scientific\ninstruments and rich human-object interaction (HOI) patterns. PhysLab comprises\n620 long-form videos and provides multilevel annotations that support a variety\nof vision tasks, including action recognition, object detection, HOI analysis,\netc. We establish strong baselines and perform extensive evaluations to\nhighlight key challenges in the parsing of procedural educational videos. We\nexpect PhysLab to serve as a valuable resource for advancing fine-grained\nvisual parsing, facilitating intelligent classroom systems, and fostering\ncloser integration between computer vision and educational technologies. The\ndataset and the evaluation toolkit are publicly available at\nhttps://github.com/ZMH-SDUST/PhysLab.", "AI": {"tldr": "PhysLab是一个针对教育场景的视频数据集，专注于学生进行复杂物理实验，填补了现有数据集在标注粒度、领域覆盖和过程指导方面的不足。", "motivation": "现有数据集在标注粒度、领域覆盖和过程指导方面存在不足，限制了视觉解析在教育场景中的应用。", "method": "引入PhysLab数据集，包含620个长视频，涵盖4个代表性物理实验，提供多层次标注支持多种视觉任务。", "result": "数据集支持动作识别、物体检测和HOI分析等任务，并通过基线评估展示了教育视频解析的关键挑战。", "conclusion": "PhysLab有望推动细粒度视觉解析和智能课堂系统的发展，促进计算机视觉与教育技术的结合。"}}
{"id": "2506.06698", "pdf": "https://arxiv.org/pdf/2506.06698", "abs": "https://arxiv.org/abs/2506.06698", "authors": ["Yitao Liu", "Chenglei Si", "Karthik Narasimhan", "Shunyu Yao"], "title": "Contextual Experience Replay for Self-Improvement of Language Agents", "categories": ["cs.AI", "cs.CL", "cs.CV", "cs.LG"], "comment": "Accepted to ACL 2025. 20 pages", "summary": "Large language model (LLM) agents have been applied to sequential\ndecision-making tasks such as web navigation, but without any\nenvironment-specific experiences, they often fail in these complex tasks.\nMoreover, current LLM agents are not designed to continually learn from past\nexperiences during inference time, which could be crucial for them to gain\nthese environment-specific experiences. To address this, we propose Contextual\nExperience Replay (CER), a training-free framework to enable efficient\nself-improvement for language agents in their context window. Specifically, CER\naccumulates and synthesizes past experiences into a dynamic memory buffer.\nThese experiences encompass environment dynamics and common decision-making\npatterns, allowing the agents to retrieve and augment themselves with relevant\nknowledge in new tasks, enhancing their adaptability in complex environments.\nWe evaluate CER on the challenging WebArena and VisualWebArena benchmarks. On\nVisualWebArena, CER achieves a competitive performance of 31.9%. On WebArena,\nCER also gets a competitive average success rate of 36.7%, relatively improving\nthe success rate of the GPT-4o agent baseline by 51.0%. We also conduct a\ncomprehensive analysis on it to prove its efficiency, validity and understand\nit better.", "AI": {"tldr": "论文提出了一种名为Contextual Experience Replay (CER)的无训练框架，通过动态记忆缓冲区积累和合成过去经验，帮助大型语言模型（LLM）代理在复杂任务中自我改进。", "motivation": "当前LLM代理在复杂任务（如网页导航）中因缺乏环境特定经验而表现不佳，且无法在推理时持续学习。", "method": "CER通过动态记忆缓冲区积累环境动态和决策模式，使代理能在新任务中检索相关知识以增强适应性。", "result": "在VisualWebArena和WebArena基准测试中，CER分别取得31.9%和36.7%的成功率，相对GPT-4o基线提升51.0%。", "conclusion": "CER证明了其在复杂环境中提升LLM代理适应性的有效性。"}}
{"id": "2506.06485", "pdf": "https://arxiv.org/pdf/2506.06485", "abs": "https://arxiv.org/abs/2506.06485", "authors": ["Kaiser Sun", "Fan Bai", "Mark Dredze"], "title": "What Is Seen Cannot Be Unseen: The Disruptive Effect of Knowledge Conflict on Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models frequently rely on both contextual input and parametric\nknowledge to perform tasks. However, these sources can come into conflict,\nespecially when retrieved documents contradict the model's parametric\nknowledge. We propose a diagnostic framework to systematically evaluate LLM\nbehavior under context-memory conflict, where the contextual information\ndiverges from their parametric beliefs. We construct diagnostic data that\nelicit these conflicts and analyze model performance across multiple task\ntypes. Our findings reveal that (1) knowledge conflict has minimal impact on\ntasks that do not require knowledge utilization, (2) model performance is\nconsistently higher when contextual and parametric knowledge are aligned, (3)\nmodels are unable to fully suppress their internal knowledge even when\ninstructed, and (4) providing rationales that explain the conflict increases\nreliance on contexts. These insights raise concerns about the validity of\nmodel-based evaluation and underscore the need to account for knowledge\nconflict in the deployment of LLMs.", "AI": {"tldr": "论文提出了一种诊断框架，用于评估大型语言模型在上下文与参数知识冲突时的行为，发现冲突对任务影响有限，模型更依赖对齐的知识，且难以完全抑制内部知识。", "motivation": "研究大型语言模型在上下文与参数知识冲突时的行为，以评估其可靠性和部署中的潜在问题。", "method": "构建诊断数据，分析模型在不同任务类型中的表现，特别关注知识冲突情境。", "result": "发现知识冲突对非知识任务影响小，模型更依赖对齐知识，无法完全抑制内部知识，提供冲突解释会增加对上下文的依赖。", "conclusion": "研究揭示了知识冲突对模型评估和部署的重要性，需进一步关注其影响。"}}
{"id": "2506.06643", "pdf": "https://arxiv.org/pdf/2506.06643", "abs": "https://arxiv.org/abs/2506.06643", "authors": ["Moushumi Medhi", "Rajiv Ranjan Sahay"], "title": "Dark Channel-Assisted Depth-from-Defocus from a Single Image", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we utilize the dark channel as a complementary cue to estimate\nthe depth of a scene from a single space-variant defocus blurred image due to\nits effectiveness in implicitly capturing the local statistics of blurred\nimages and the scene structure. Existing depth-from-defocus (DFD) techniques\ntypically rely on multiple images with varying apertures or focus settings to\nrecover depth information. Very few attempts have focused on DFD from a single\ndefocused image due to the underconstrained nature of the problem. Our method\ncapitalizes on the relationship between local defocus blur and contrast\nvariations as key depth cues to enhance the overall performance in estimating\nthe scene's structure. The entire pipeline is trained adversarially in a fully\nend-to-end fashion. Experiments conducted on real data with realistic\ndepth-induced defocus blur demonstrate that incorporating dark channel prior\ninto single image DFD yields meaningful depth estimation results, validating\nthe effectiveness of our approach.", "AI": {"tldr": "本文提出了一种利用暗通道作为补充线索，从单张空间变化散焦模糊图像中估计场景深度的方法。", "motivation": "现有深度从散焦（DFD）技术通常依赖多张不同光圈或对焦设置的图像来恢复深度信息，而单张散焦图像的DFD问题由于欠约束性很少被研究。本文旨在解决这一问题。", "method": "利用局部散焦模糊与对比度变化之间的关系作为关键深度线索，并通过对抗训练以端到端方式优化整个流程。", "result": "在真实数据上的实验表明，将暗通道先验引入单图像DFD能产生有意义的深度估计结果。", "conclusion": "该方法有效验证了暗通道先验在单图像DFD中的实用性。"}}
{"id": "2506.06714", "pdf": "https://arxiv.org/pdf/2506.06714", "abs": "https://arxiv.org/abs/2506.06714", "authors": ["Hamied Nabizada", "Tom Jeleniewski", "Lasse Beers", "Maximilian Weigand", "Felix Gehlhoff", "Alexander Fay"], "title": "Integrating AI Planning Semantics into SysML System Models for Automated PDDL File Generation", "categories": ["cs.AI"], "comment": null, "summary": "This paper presents a SysML profile that enables the direct integration of\nplanning semantics based on the Planning Domain Definition Language (PDDL) into\nsystem models. Reusable stereotypes are defined for key PDDL concepts such as\ntypes, predicates, functions and actions, while formal OCL constraints ensure\nsyntactic consistency. The profile was derived from the Backus-Naur Form (BNF)\ndefinition of PDDL 3.1 to align with SysML modeling practices. A case study\nfrom aircraft manufacturing demonstrates the application of the profile: a\nrobotic system with interchangeable end effectors is modeled and enriched to\ngenerate both domain and problem descriptions in PDDL format. These are used as\ninput to a PDDL solver to derive optimized execution plans. The approach\nsupports automated and model-based generation of planning descriptions and\nprovides a reusable bridge between system modeling and AI planning in\nengineering design.", "AI": {"tldr": "本文提出了一种SysML配置文件，将基于PDDL的规划语义直接集成到系统模型中，支持自动化生成规划描述，并连接系统建模与AI规划。", "motivation": "为了在工程设计中实现系统建模与AI规划的无缝集成，需要一种方法将PDDL语义直接嵌入SysML模型。", "method": "定义了可重用的PDDL概念（如类型、谓词、函数和动作）的SysML原型，并通过OCL约束确保语法一致性。配置文件基于PDDL 3.1的BNF定义。", "result": "通过飞机制造案例研究验证了配置文件的应用，成功生成PDDL格式的领域和问题描述，并用于优化执行计划。", "conclusion": "该方法支持模型驱动的规划描述生成，为系统建模与AI规划提供了可重用的桥梁。"}}
{"id": "2506.06500", "pdf": "https://arxiv.org/pdf/2506.06500", "abs": "https://arxiv.org/abs/2506.06500", "authors": ["Luyao Shi", "Michael Kazda", "Charles Schmitter", "Hemlata Gupta"], "title": "Improving LLM-Powered EDA Assistants with RAFT", "categories": ["cs.CL"], "comment": "Accepted paper at IEEE International Conference on LLM-Aided Design,\n  2025 (LAD 2025)", "summary": "Electronic design engineers often struggle to efficiently access relevant\ninformation for tasks like design verification and technology development.\nWhile large language models (LLMs) can enhance productivity as conversational\nagents, pre-trained open-source LLMs lack domain-specific knowledge for\nElectronic Design Automation (EDA). In a Retrieval-Augmented Generation (RAG)\ncontext, LLMs rely on external context but may still produce inaccurate\nresponses. Retrieval-Augmented Fine-Tuning (RAFT) improves LLM performance, but\nacquiring labeled question/answer (Q/A) data in EDA is difficult. To address\nthis, we propose using synthetic Q/A datasets to enhance LLMs with RAFT. Our\nresults show that RAFT with synthetic data significantly boosts LLM performance\nfor RAG-based EDA tasks. We also investigate the impact of using real user\nquestions as Retrieval-Augmented Few-Shot (RAFS) examples for synthetic data\ngeneration. Additionally, we implement secure access control to ensure\nsensitive information is only accessible to authorized personnel. Finally, we\nassess the risk of data leakage and unintended memorization during fine-tuning\nwith synthetic data, providing practical insights.", "AI": {"tldr": "论文提出使用合成Q/A数据集结合RAFT方法提升LLM在EDA领域的性能，并探讨了真实用户问题对合成数据生成的影响，同时关注了数据安全和隐私问题。", "motivation": "电子设计工程师在EDA任务中难以高效获取相关信息，现有开源LLM缺乏领域知识，RAFT方法虽有效但缺乏标注数据。", "method": "提出使用合成Q/A数据集结合RAFT方法，并研究真实用户问题对合成数据生成的影响，实施安全访问控制。", "result": "RAFT结合合成数据显著提升LLM在EDA任务中的性能，同时评估了数据泄漏风险。", "conclusion": "合成数据与RAFT方法可有效提升LLM在EDA领域的表现，同时需关注数据安全和隐私问题。"}}
{"id": "2506.06645", "pdf": "https://arxiv.org/pdf/2506.06645", "abs": "https://arxiv.org/abs/2506.06645", "authors": ["Cheng Peng", "Jingxiang Sun", "Yushuo Chen", "Zhaoqi Su", "Zhuo Su", "Yebin Liu"], "title": "Parametric Gaussian Human Model: Generalizable Prior for Efficient and Realistic Human Avatar Modeling", "categories": ["cs.CV"], "comment": "Project Page: https://pengc02.github.io/pghm/", "summary": "Photorealistic and animatable human avatars are a key enabler for\nvirtual/augmented reality, telepresence, and digital entertainment. While\nrecent advances in 3D Gaussian Splatting (3DGS) have greatly improved rendering\nquality and efficiency, existing methods still face fundamental challenges,\nincluding time-consuming per-subject optimization and poor generalization under\nsparse monocular inputs. In this work, we present the Parametric Gaussian Human\nModel (PGHM), a generalizable and efficient framework that integrates human\npriors into 3DGS for fast and high-fidelity avatar reconstruction from\nmonocular videos. PGHM introduces two core components: (1) a UV-aligned latent\nidentity map that compactly encodes subject-specific geometry and appearance\ninto a learnable feature tensor; and (2) a disentangled Multi-Head U-Net that\npredicts Gaussian attributes by decomposing static, pose-dependent, and\nview-dependent components via conditioned decoders. This design enables robust\nrendering quality under challenging poses and viewpoints, while allowing\nefficient subject adaptation without requiring multi-view capture or long\noptimization time. Experiments show that PGHM is significantly more efficient\nthan optimization-from-scratch methods, requiring only approximately 20 minutes\nper subject to produce avatars with comparable visual quality, thereby\ndemonstrating its practical applicability for real-world monocular avatar\ncreation.", "AI": {"tldr": "PGHM是一种基于3D高斯散射的可泛化和高效的人体化身重建框架，通过引入UV对齐的潜在身份图和分离的多头U-Net，显著提高了渲染质量和效率。", "motivation": "现有方法在单目输入下泛化能力差且优化耗时，PGHM旨在解决这些问题，实现快速高保真化身重建。", "method": "PGHM包含UV对齐的潜在身份图编码几何与外观，以及多头U-Net分解静态、姿态和视角相关的高斯属性。", "result": "PGHM仅需约20分钟即可完成高质量化身重建，效率显著优于传统方法。", "conclusion": "PGHM为单目化身创建提供了实用高效的解决方案。"}}
{"id": "2506.06725", "pdf": "https://arxiv.org/pdf/2506.06725", "abs": "https://arxiv.org/abs/2506.06725", "authors": ["Guillaume Levy", "Cedric Colas", "Pierre-Yves Oudeyer", "Thomas Carta", "Clement Romac"], "title": "WorldLLM: Improving LLMs' world modeling using curiosity-driven theory-making", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) possess general world knowledge but often\nstruggle to generate precise predictions in structured, domain-specific\ncontexts such as simulations. These limitations arise from their inability to\nground their broad, unstructured understanding in specific environments. To\naddress this, we present WorldLLM, a framework that enhances LLM-based world\nmodeling by combining Bayesian inference and autonomous active exploration with\nreinforcement learning. WorldLLM leverages the in-context learning abilities of\nLLMs to guide an LLM-based world model's predictions using natural language\nhypotheses given in its prompt. These hypotheses are iteratively refined\nthrough a Bayesian inference framework that leverages a second LLM as the\nproposal distribution given collected evidence. This evidence is collected\nusing a curiosity-driven reinforcement learning policy that explores the\nenvironment to find transitions with a low log-likelihood under our LLM-based\npredictive model using the current hypotheses. By alternating between refining\nhypotheses and collecting new evidence, our framework autonomously drives\ncontinual improvement of the predictions. Our experiments demonstrate the\neffectiveness of WorldLLM in a textual game environment that requires agents to\nmanipulate and combine objects. The framework not only enhances predictive\naccuracy, but also generates human-interpretable theories of environment\ndynamics.", "AI": {"tldr": "WorldLLM框架通过结合贝叶斯推断和强化学习，提升LLM在结构化领域中的预测能力。", "motivation": "LLMs在结构化领域（如模拟）中难以生成精确预测，因其无法将广泛的无结构知识应用于特定环境。", "method": "WorldLLM结合贝叶斯推断和强化学习，利用LLM的上下文学习能力迭代优化假设，并通过好奇心驱动的策略收集证据。", "result": "实验证明WorldLLM在文本游戏环境中有效提升预测准确性，并生成可解释的环境动态理论。", "conclusion": "WorldLLM通过自主探索和假设优化，显著提升了LLM在结构化领域中的表现。"}}
{"id": "2506.06506", "pdf": "https://arxiv.org/pdf/2506.06506", "abs": "https://arxiv.org/abs/2506.06506", "authors": ["Kshitish Ghate", "Tessa Charlesworth", "Mona Diab", "Aylin Caliskan"], "title": "Biases Propagate in Encoder-based Vision-Language Models: A Systematic Analysis From Intrinsic Measures to Zero-shot Retrieval Outcomes", "categories": ["cs.CL"], "comment": "Accepted to ACL Findings 2025", "summary": "To build fair AI systems we need to understand how social-group biases\nintrinsic to foundational encoder-based vision-language models (VLMs) manifest\nin biases in downstream tasks. In this study, we demonstrate that intrinsic\nbiases in VLM representations systematically ``carry over'' or propagate into\nzero-shot retrieval tasks, revealing how deeply rooted biases shape a model's\noutputs. We introduce a controlled framework to measure this propagation by\ncorrelating (a) intrinsic measures of bias in the representational space with\n(b) extrinsic measures of bias in zero-shot text-to-image (TTI) and\nimage-to-text (ITT) retrieval. Results show substantial correlations between\nintrinsic and extrinsic bias, with an average $\\rho$ = 0.83 $\\pm$ 0.10. This\npattern is consistent across 114 analyses, both retrieval directions, six\nsocial groups, and three distinct VLMs. Notably, we find that\nlarger/better-performing models exhibit greater bias propagation, a finding\nthat raises concerns given the trend towards increasingly complex AI models.\nOur framework introduces baseline evaluation tasks to measure the propagation\nof group and valence signals. Investigations reveal that underrepresented\ngroups experience less robust propagation, further skewing their model-related\noutcomes.", "AI": {"tldr": "研究发现基础视觉语言模型（VLMs）中的社会群体偏见会系统性传播到零样本检索任务中，且性能更强的模型偏见传播更显著。", "motivation": "理解社会群体偏见如何在基础视觉语言模型中传播，以构建更公平的AI系统。", "method": "通过关联（a）表征空间中的内在偏见与（b）零样本检索任务中的外在偏见，设计了一个测量框架。", "result": "内在与外在偏见之间存在显著相关性（平均ρ=0.83±0.10），且性能更强的模型偏见传播更严重。", "conclusion": "研究揭示了偏见传播的普遍性，呼吁关注模型性能提升可能加剧偏见的风险。"}}
{"id": "2506.06667", "pdf": "https://arxiv.org/pdf/2506.06667", "abs": "https://arxiv.org/abs/2506.06667", "authors": ["Yu-Hsuan Ho", "Ali Mostafavi"], "title": "Flood-DamageSense: Multimodal Mamba with Multitask Learning for Building Flood Damage Assessment using SAR Remote Sensing Imagery", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Most post-disaster damage classifiers succeed only when destructive forces\nleave clear spectral or structural signatures -- conditions rarely present\nafter inundation. Consequently, existing models perform poorly at identifying\nflood-related building damages. The model presented in this study,\nFlood-DamageSense, addresses this gap as the first deep-learning framework\npurpose-built for building-level flood-damage assessment. The architecture\nfuses pre- and post-event SAR/InSAR scenes with very-high-resolution optical\nbasemaps and an inherent flood-risk layer that encodes long-term exposure\nprobabilities, guiding the network toward plausibly affected structures even\nwhen compositional change is minimal. A multimodal Mamba backbone with a\nsemi-Siamese encoder and task-specific decoders jointly predicts (1) graded\nbuilding-damage states, (2) floodwater extent, and (3) building footprints.\nTraining and evaluation on Hurricane Harvey (2017) imagery from Harris County,\nTexas -- supported by insurance-derived property-damage extents -- show a mean\nF1 improvement of up to 19 percentage points over state-of-the-art baselines,\nwith the largest gains in the frequently misclassified \"minor\" and \"moderate\"\ndamage categories. Ablation studies identify the inherent-risk feature as the\nsingle most significant contributor to this performance boost. An end-to-end\npost-processing pipeline converts pixel-level outputs to actionable,\nbuilding-scale damage maps within minutes of image acquisition. By combining\nrisk-aware modeling with SAR's all-weather capability, Flood-DamageSense\ndelivers faster, finer-grained, and more reliable flood-damage intelligence to\nsupport post-disaster decision-making and resource allocation.", "AI": {"tldr": "Flood-DamageSense是一种专为洪水灾害建筑损坏评估设计的深度学习框架，结合多模态数据和风险层，显著提升了损坏分类的准确性。", "motivation": "现有模型在洪水灾害后建筑损坏分类中表现不佳，主要因为洪水破坏的痕迹不明显。", "method": "模型融合了SAR/InSAR数据、高分辨率光学地图和洪水风险层，采用多模态Mamba架构和半Siamese编码器进行联合预测。", "result": "在Hurricane Harvey数据上，模型比现有基线平均F1分数提高了19个百分点，尤其在“轻微”和“中等”损坏类别中表现突出。", "conclusion": "Flood-DamageSense通过结合风险感知和SAR的全天候能力，为灾后决策提供了更快、更精细、更可靠的损坏评估。"}}
{"id": "2506.06727", "pdf": "https://arxiv.org/pdf/2506.06727", "abs": "https://arxiv.org/abs/2506.06727", "authors": ["Can Li", "Ting Zhang", "Mei Wang", "Hua Huang"], "title": "VisioMath: Benchmarking Figure-based Mathematical Reasoning in LMMs", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Large Multimodal Models (LMMs) have demonstrated remarkable problem-solving\ncapabilities across various domains. However, their ability to perform\nmathematical reasoning when answer options are represented as images--an\nessential aspect of multi-image comprehension--remains underexplored. To bridge\nthis gap, we introduce VisioMath, a benchmark designed to evaluate mathematical\nreasoning in multimodal contexts involving image-based answer choices.\nVisioMath comprises 8,070 images and 1,800 multiple-choice questions, where\neach answer option is an image, presenting unique challenges to existing LMMs.\nTo the best of our knowledge, VisioMath is the first dataset specifically\ntailored for mathematical reasoning in image-based-option scenarios, where\nfine-grained distinctions between answer choices are critical for accurate\nproblem-solving. We systematically evaluate state-of-the-art LMMs on VisioMath\nand find that even the most advanced models struggle with this task. Notably,\nGPT-4o achieves only 45.9% accuracy, underscoring the limitations of current\nmodels in reasoning over visually similar answer choices. By addressing a\ncrucial gap in existing benchmarks, VisioMath establishes a rigorous testbed\nfor future research, driving advancements in multimodal reasoning.", "AI": {"tldr": "VisioMath是一个新的多模态数学推理基准，专注于图像选项的数学问题，现有大型多模态模型（如GPT-4o）表现不佳。", "motivation": "当前大型多模态模型在图像选项的数学推理能力上研究不足，需要专门的测试基准。", "method": "提出VisioMath数据集，包含8070张图像和1800道多选题，答案选项为图像，用于评估多模态模型的数学推理能力。", "result": "现有最先进模型（如GPT-4o）在VisioMath上表现较差，准确率仅为45.9%。", "conclusion": "VisioMath填补了多模态数学推理基准的空白，为未来研究提供了严格的测试平台。"}}
{"id": "2506.06522", "pdf": "https://arxiv.org/pdf/2506.06522", "abs": "https://arxiv.org/abs/2506.06522", "authors": ["Aladin Djuhera", "Swanand Ravindra Kadhe", "Syed Zawad", "Farhan Ahmed", "Heiko Ludwig", "Holger Boche"], "title": "Fixing It in Post: A Comparative Study of LLM Post-Training Data Quality and Model Performance", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent work on large language models (LLMs) has increasingly focused on\npost-training and alignment with datasets curated to enhance instruction\nfollowing, world knowledge, and specialized skills. However, most post-training\ndatasets used in leading open- and closed-source LLMs remain inaccessible to\nthe public, with limited information about their construction process. This\nlack of transparency has motivated the recent development of open-source\npost-training corpora. While training on these open alternatives can yield\nperformance comparable to that of leading models, systematic comparisons remain\nchallenging due to the significant computational cost of conducting them\nrigorously at scale, and are therefore largely absent. As a result, it remains\nunclear how specific samples, task types, or curation strategies influence\ndownstream performance when assessing data quality. In this work, we conduct\nthe first comprehensive side-by-side analysis of two prominent open\npost-training datasets: Tulu-3-SFT-Mix and SmolTalk. Using the Magpie\nframework, we annotate each sample with detailed quality metrics, including\nturn structure (single-turn vs. multi-turn), task category, input quality, and\nresponse quality, and we derive statistics that reveal structural and\nqualitative similarities and differences between the two datasets. Based on\nthese insights, we design a principled curation recipe that produces a new data\nmixture, TuluTalk, which contains 14% fewer samples than either source dataset\nwhile matching or exceeding their performance on key benchmarks. Our findings\noffer actionable insights for constructing more effective post-training\ndatasets that improve model performance within practical resource limits. To\nsupport future research, we publicly release both the annotated source datasets\nand our curated TuluTalk mixture.", "AI": {"tldr": "本文对两个开源后训练数据集（Tulu-3-SFT-Mix和SmolTalk）进行了首次全面对比分析，提出了一种新的数据混合方法TuluTalk，在减少样本数量的同时提升了性能。", "motivation": "当前大型语言模型的后训练数据集缺乏透明度，且开源数据集的质量和影响尚未系统评估。", "method": "使用Magpie框架对数据集样本进行质量标注，设计了一种新的数据混合方法TuluTalk。", "result": "TuluTalk样本减少14%，但在关键基准测试中性能优于或匹配原始数据集。", "conclusion": "研究为构建高效后训练数据集提供了实用方法，并公开了标注数据和TuluTalk混合物以支持未来研究。"}}
{"id": "2506.06680", "pdf": "https://arxiv.org/pdf/2506.06680", "abs": "https://arxiv.org/abs/2506.06680", "authors": ["Radha Kodali", "Venkata Rao Dhulipalla", "Venkata Siva Kishor Tatavarty", "Madhavi Nadakuditi", "Bharadwaj Thiruveedhula", "Suryanarayana Gunnam", "Durga Prasad Bavirisetti"], "title": "Interpretation of Deep Learning Model in Embryo Selection for In Vitro Fertilization (IVF) Treatment", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Infertility has a considerable impact on individuals' quality of life,\naffecting them socially and psychologically, with projections indicating a rise\nin the upcoming years. In vitro fertilization (IVF) emerges as one of the\nprimary techniques within economically developed nations, employed to address\nthe rising problem of low fertility. Expert embryologists conventionally grade\nembryos by reviewing blastocyst images to select the most optimal for transfer,\nyet this process is time-consuming and lacks efficiency. Blastocyst images\nprovide a valuable resource for assessing embryo viability. In this study, we\nintroduce an explainable artificial intelligence (XAI) framework for\nclassifying embryos, employing a fusion of convolutional neural network (CNN)\nand long short-term memory (LSTM) architecture, referred to as CNN-LSTM.\nUtilizing deep learning, our model achieves high accuracy in embryo\nclassification while maintaining interpretability through XAI.", "AI": {"tldr": "本文提出了一种基于可解释人工智能（XAI）的胚胎分类框架，结合CNN和LSTM架构，旨在提高胚胎选择的效率和准确性。", "motivation": "不孕症对个人生活质量有显著影响，而传统的胚胎分级方法效率低下，因此需要一种更高效且可解释的技术。", "method": "采用CNN-LSTM混合架构，结合深度学习技术，对胚胎图像进行分类，并通过XAI保持模型的可解释性。", "result": "模型在胚胎分类中实现了高准确率，同时保持了可解释性。", "conclusion": "该研究为胚胎选择提供了一种高效且透明的AI解决方案，有望在不孕症治疗中发挥作用。"}}
{"id": "2506.06739", "pdf": "https://arxiv.org/pdf/2506.06739", "abs": "https://arxiv.org/abs/2506.06739", "authors": ["Andrew Cropper", "Filipe Gouveia", "David M. Cerna"], "title": "Honey, I shrunk the hypothesis space (through logical preprocessing)", "categories": ["cs.AI", "cs.LG"], "comment": "Submitted to JAIR", "summary": "Inductive logic programming (ILP) is a form of logical machine learning. The\ngoal is to search a hypothesis space for a hypothesis that generalises training\nexamples and background knowledge. We introduce an approach that 'shrinks' the\nhypothesis space before an ILP system searches it. Our approach uses background\nknowledge to find rules that cannot be in an optimal hypothesis regardless of\nthe training examples. For instance, our approach discovers relationships such\nas \"even numbers cannot be odd\" and \"prime numbers greater than 2 are odd\". It\nthen removes violating rules from the hypothesis space. We implement our\napproach using answer set programming and use it to shrink the hypothesis space\nof a constraint-based ILP system. Our experiments on multiple domains,\nincluding visual reasoning and game playing, show that our approach can\nsubstantially reduce learning times whilst maintaining predictive accuracies.\nFor instance, given just 10 seconds of preprocessing time, our approach can\nreduce learning times from over 10 hours to only 2 seconds.", "AI": {"tldr": "提出了一种通过背景知识缩小假设空间的方法，显著减少归纳逻辑编程（ILP）的学习时间，同时保持预测准确性。", "motivation": "在ILP中，搜索假设空间是一个耗时过程，希望通过预处理减少无效假设，提高效率。", "method": "利用背景知识识别并移除不可能出现在最优假设中的规则，使用答案集编程实现。", "result": "实验表明，预处理仅需10秒即可将学习时间从10小时缩短至2秒，且不影响准确性。", "conclusion": "该方法有效提升了ILP系统的效率，适用于视觉推理和游戏等多个领域。"}}
{"id": "2506.06539", "pdf": "https://arxiv.org/pdf/2506.06539", "abs": "https://arxiv.org/abs/2506.06539", "authors": ["Yijie Hao", "Haofei Yu", "Jiaxuan You"], "title": "Beyond Facts: Evaluating Intent Hallucination in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 main conference", "summary": "When exposed to complex queries containing multiple conditions, today's large\nlanguage models (LLMs) tend to produce responses that only partially satisfy\nthe query while neglecting certain conditions. We therefore introduce the\nconcept of Intent Hallucination. In this phenomenon, LLMs either omit\n(neglecting to address certain parts) or misinterpret (responding to invented\nquery parts) elements of the given query, leading to intent hallucinated\ngeneration. To systematically evaluate intent hallucination, we introduce\nFAITHQA, a novel benchmark for intent hallucination that contains 20,068\nproblems, covering both query-only and retrieval-augmented generation (RAG)\nsetups with varying topics and difficulty. FAITHQA is the first hallucination\nbenchmark that goes beyond factual verification, tailored to identify the\nfundamental cause of intent hallucination. By evaluating various LLMs on\nFAITHQA, we find that (1) intent hallucination is a common issue even for\nstate-of-the-art models, and (2) the phenomenon stems from omission or\nmisinterpretation of LLMs. To facilitate future research, we introduce an\nautomatic LLM generation evaluation metric, CONSTRAINT SCORE, for detecting\nintent hallucination. Human evaluation results demonstrate that CONSTRAINT\nSCORE is closer to human performance for intent hallucination compared to\nbaselines.", "AI": {"tldr": "论文提出‘意图幻觉’概念，指大语言模型（LLMs）在复杂查询中部分忽略或错误解读条件，并引入FAITHQA基准和CONSTRAINT SCORE评估指标。", "motivation": "解决LLMs在复杂查询中部分满足或错误解读意图的问题。", "method": "引入FAITHQA基准（20,068个问题）和CONSTRAINT SCORE评估指标。", "result": "意图幻觉普遍存在，源于LLMs的忽略或误解；CONSTRAINT SCORE接近人类评估效果。", "conclusion": "FAITHQA和CONSTRAINT SCORE为未来研究提供了工具，揭示了LLMs的意图幻觉问题。"}}
{"id": "2506.06710", "pdf": "https://arxiv.org/pdf/2506.06710", "abs": "https://arxiv.org/abs/2506.06710", "authors": ["Qianqian Zhao", "Chunle Guo", "Tianyi Zhang", "Junpei Zhang", "Peiyang Jia", "Tan Su", "Wenjie Jiang", "Chongyi Li"], "title": "A Systematic Investigation on Deep Learning-Based Omnidirectional Image and Video Super-Resolution", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Omnidirectional image and video super-resolution is a crucial research topic\nin low-level vision, playing an essential role in virtual reality and augmented\nreality applications. Its goal is to reconstruct high-resolution images or\nvideo frames from low-resolution inputs, thereby enhancing detail preservation\nand enabling more accurate scene analysis and interpretation. In recent years,\nnumerous innovative and effective approaches have been proposed, predominantly\nbased on deep learning techniques, involving diverse network architectures,\nloss functions, projection strategies, and training datasets. This paper\npresents a systematic review of recent progress in omnidirectional image and\nvideo super-resolution, focusing on deep learning-based methods. Given that\nexisting datasets predominantly rely on synthetic degradation and fall short in\ncapturing real-world distortions, we introduce a new dataset, 360Insta, that\ncomprises authentically degraded omnidirectional images and videos collected\nunder diverse conditions, including varying lighting, motion, and exposure\nsettings. This dataset addresses a critical gap in current omnidirectional\nbenchmarks and enables more robust evaluation of the generalization\ncapabilities of omnidirectional super-resolution methods. We conduct\ncomprehensive qualitative and quantitative evaluations of existing methods on\nboth public datasets and our proposed dataset. Furthermore, we provide a\nsystematic overview of the current status of research and discuss promising\ndirections for future exploration. All datasets, methods, and evaluation\nmetrics introduced in this work are publicly available and will be regularly\nupdated. Project page: https://github.com/nqian1/Survey-on-ODISR-and-ODVSR.", "AI": {"tldr": "本文系统综述了基于深度学习的全向图像和视频超分辨率方法，并提出了新数据集360Insta以解决现有数据集的不足。", "motivation": "全向图像和视频超分辨率在虚拟现实和增强现实中至关重要，但现有数据集多为合成退化，缺乏真实世界失真。", "method": "通过引入真实退化的数据集360Insta，并对现有方法进行全面定性和定量评估。", "result": "360Insta填补了现有基准的空白，提升了超分辨率方法的泛化能力评估。", "conclusion": "本文为全向超分辨率研究提供了系统综述和新数据集，并展望了未来研究方向。"}}
{"id": "2506.06740", "pdf": "https://arxiv.org/pdf/2506.06740", "abs": "https://arxiv.org/abs/2506.06740", "authors": ["Yigui Feng", "Qinglin Wang", "Ke Liu", "Xinhai Chen", "Bo Yang", "Jie Liu"], "title": "AI PsyRoom: Artificial Intelligence Platform for Segmented Yearning and Reactive Outcome Optimization Method", "categories": ["cs.AI"], "comment": null, "summary": "Psychological counseling faces huge challenges due to the growing demand for\nmental health services and the shortage of trained professionals. Large\nlanguage models (LLMs) have shown potential to assist psychological counseling,\nespecially in empathy and emotional support. However, existing models lack a\ndeep understanding of emotions and are unable to generate personalized\ntreatment plans based on fine-grained emotions. To address these shortcomings,\nwe present AI PsyRoom, a multi-agent simulation framework designed to enhance\npsychological counseling by generating empathetic and emotionally nuanced\nconversations. By leveraging fine-grained emotion classification and a\nmulti-agent framework, we construct a multi-agent PsyRoom A for dialogue\nreconstruction, generating a high-quality dialogue dataset EmoPsy, which\ncontains 35 sub-emotions, 423 specific emotion scenarios, and 12,350 dialogues.\nWe also propose PsyRoom B for generating personalized treatment plans.\nQuantitative evaluations demonstrate that AI PsyRoom significantly outperforms\nstate-of-the-art methods, achieving 18% improvement in problem orientation, 23%\nin expression, 24% in Empathy, and 16% in interactive communication quality.\nThe datasets and models are publicly available, providing a foundation for\nadvancing AI-assisted psychological counseling research.", "AI": {"tldr": "AI PsyRoom是一个多代理模拟框架，旨在通过生成具有情感细腻的对话来增强心理咨询，解决了现有模型在情感理解和个性化治疗计划生成方面的不足。", "motivation": "心理咨询需求增长与专业人员短缺之间的矛盾促使研究利用大型语言模型（LLMs）辅助心理咨询，但现有模型在情感理解和个性化治疗方面表现不足。", "method": "提出AI PsyRoom框架，包括PsyRoom A（用于对话重建和生成高质量数据集EmoPsy）和PsyRoom B（用于生成个性化治疗计划）。", "result": "AI PsyRoom在问题导向、表达、同理心和交互沟通质量上显著优于现有方法，提升幅度分别为18%、23%、24%和16%。", "conclusion": "AI PsyRoom为AI辅助心理咨询研究提供了高质量数据集和模型，具有实际应用潜力。"}}
{"id": "2506.06561", "pdf": "https://arxiv.org/pdf/2506.06561", "abs": "https://arxiv.org/abs/2506.06561", "authors": ["Ho Yin 'Sam' Ng", "Ting-Yao Hsu", "Aashish Anantha Ramakrishnan", "Branislav Kveton", "Nedim Lipka", "Franck Dernoncourt", "Dongwon Lee", "Tong Yu", "Sungchul Kim", "Ryan A. Rossi", "Ting-Hao 'Kenneth' Huang"], "title": "LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure Profiles", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Figure captions are crucial for helping readers understand and remember a\nfigure's key message. Many models have been developed to generate these\ncaptions, helping authors compose better quality captions more easily. Yet,\nauthors almost always need to revise generic AI-generated captions to match\ntheir writing style and the domain's style, highlighting the need for\npersonalization. Despite language models' personalization (LaMP) advances,\nthese technologies often focus on text-only settings and rarely address\nscenarios where both inputs and profiles are multimodal. This paper introduces\nLaMP-Cap, a dataset for personalized figure caption generation with multimodal\nfigure profiles. For each target figure, LaMP-Cap provides not only the needed\ninputs, such as figure images, but also up to three other figures from the same\ndocument--each with its image, caption, and figure-mentioning paragraphs--as a\nprofile to characterize the context. Experiments with four LLMs show that using\nprofile information consistently helps generate captions closer to the original\nauthor-written ones. Ablation studies reveal that images in the profile are\nmore helpful than figure-mentioning paragraphs, highlighting the advantage of\nusing multimodal profiles over text-only ones.", "AI": {"tldr": "论文介绍了LaMP-Cap数据集，用于个性化多模态图表标题生成，实验表明多模态信息比纯文本更能提升标题生成质量。", "motivation": "现有AI生成的图表标题缺乏个性化，且多模态场景下的个性化技术研究不足。", "method": "提出LaMP-Cap数据集，包含图表图像、标题及上下文段落，实验使用四种大语言模型验证多模态信息的有效性。", "result": "实验表明，多模态信息（如图像）比纯文本更能帮助生成接近作者风格的标题。", "conclusion": "多模态个性化技术能显著提升图表标题生成质量，图像信息尤为关键。"}}
{"id": "2506.06712", "pdf": "https://arxiv.org/pdf/2506.06712", "abs": "https://arxiv.org/abs/2506.06712", "authors": ["Saiyu Hu", "Chunlei He", "Jianfeng Zhang", "Dexing Kong", "Shoujun Huang"], "title": "Active Contour Models Driven by Hyperbolic Mean Curvature Flow for Image Segmentation", "categories": ["cs.CV", "math.AP"], "comment": null, "summary": "Parabolic mean curvature flow-driven active contour models (PMCF-ACMs) are\nwidely used in image segmentation, which however depend heavily on the\nselection of initial curve configurations. In this paper, we firstly propose\nseveral hyperbolic mean curvature flow-driven ACMs (HMCF-ACMs), which introduce\ntunable initial velocity fields, enabling adaptive optimization for diverse\nsegmentation scenarios. We shall prove that HMCF-ACMs are indeed normal flows\nand establish the numerical equivalence between dissipative HMCF formulations\nand certain wave equations using the level set method with signed distance\nfunction. Building on this framework, we furthermore develop hyperbolic\ndual-mode regularized flow-driven ACMs (HDRF-ACMs), which utilize smooth\nHeaviside functions for edge-aware force modulation to suppress over-diffusion\nnear weak boundaries. Then, we optimize a weighted fourth-order Runge-Kutta\nalgorithm with nine-point stencil spatial discretization when solving the\nabove-mentioned wave equations. Experiments show that both HMCF-ACMs and\nHDRF-ACMs could achieve more precise segmentations with superior noise\nresistance and numerical stability due to task-adaptive configurations of\ninitial velocities and initial contours.", "AI": {"tldr": "论文提出了基于双曲平均曲率流的主动轮廓模型（HMCF-ACMs）和双模式正则化流驱动的主动轮廓模型（HDRF-ACMs），通过可调初始速度场和边缘感知力调制，提升了图像分割的精度和抗噪性。", "motivation": "传统抛物线平均曲率流驱动的主动轮廓模型（PMCF-ACMs）对初始曲线配置依赖性强，限制了其适应性。本文旨在通过引入双曲平均曲率流和正则化技术，解决这一问题。", "method": "1. 提出HMCF-ACMs，证明其为法向流，并与特定波动方程数值等价；2. 开发HDRF-ACMs，利用平滑Heaviside函数抑制弱边界处的过度扩散；3. 优化加权四阶Runge-Kutta算法求解波动方程。", "result": "实验表明，HMCF-ACMs和HDRF-ACMs在初始速度和轮廓配置的自适应优化下，实现了更高精度的分割，且具有更强的抗噪性和数值稳定性。", "conclusion": "HMCF-ACMs和HDRF-ACMs通过引入双曲流和正则化技术，显著提升了图像分割的适应性和性能。"}}
{"id": "2506.06750", "pdf": "https://arxiv.org/pdf/2506.06750", "abs": "https://arxiv.org/abs/2506.06750", "authors": ["Zofia Rudnicka", "Janusz Szczepanski", "Agnieszka Pregowska"], "title": "Bio-Inspired Classification: Combining Information Theory and Spiking Neural Networks -- Influence of the Learning Rules", "categories": ["cs.AI", "cs.LG", "q-bio.NC"], "comment": null, "summary": "Training of Spiking Neural Networks (SNN) is challenging due to their unique\nproperties, including temporal dynamics, non-differentiability of spike events,\nand sparse event-driven activations. In this paper, we widely consider the\ninfluence of the type of chosen learning algorithm, including bioinspired\nlearning rules on the accuracy of classification. We proposed a bioinspired\nclassifier based on the combination of SNN and Lempel-Ziv complexity (LZC).\nThis approach synergizes the strengths of SNNs in temporal precision and\nbiological realism with LZC's structural complexity analysis, facilitating\nefficient and interpretable classification of spatiotemporal neural data. It\nturned out that the classic backpropagation algorithm achieves excellent\nclassification accuracy, but at extremely high computational cost, which makes\nit impractical for real-time applications. Biologically inspired learning\nalgorithms such as tempotron and Spikprop provide increased computational\nefficiency while maintaining competitive classification performance, making\nthem suitable for time-sensitive tasks. The results obtained indicate that the\nselection of the most appropriate learning algorithm depends on the trade-off\nbetween classification accuracy and computational cost as well as application\nconstraints.", "AI": {"tldr": "论文探讨了脉冲神经网络（SNN）训练中的挑战，比较了不同学习算法的分类准确性和计算成本，提出了一种结合SNN和Lempel-Ziv复杂度（LZC）的生物启发分类器。", "motivation": "由于SNN的独特性质（如时间动态性、脉冲事件的不可微分性和稀疏事件驱动激活），其训练具有挑战性。研究旨在评估不同学习算法对分类准确性的影响。", "method": "提出了一种结合SNN和LZC的生物启发分类器，利用SNN的时间精确性和生物真实性，以及LZC的结构复杂性分析，实现高效的时空神经数据分类。", "result": "经典反向传播算法分类准确率高但计算成本极高，不适合实时应用；生物启发算法（如tempotron和Spikprop）在保持竞争力的同时提高了计算效率。", "conclusion": "选择学习算法需权衡分类准确性和计算成本，以及应用场景的限制。"}}
{"id": "2506.06589", "pdf": "https://arxiv.org/pdf/2506.06589", "abs": "https://arxiv.org/abs/2506.06589", "authors": ["Jacqueline He", "Howard Yen", "Margaret Li", "Shuyue Stella Li", "Zhiyuan Zeng", "Weijia Shi", "Yulia Tsvetkov", "Danqi Chen", "Pang Wei Koh", "Luke Zettlemoyer"], "title": "Precise Information Control in Long-Form Text Generation", "categories": ["cs.CL"], "comment": "56 pages, 8 figures. Code and models are publicly available at\n  https://github.com/jacqueline-he/precise-information-control", "summary": "A central challenge in modern language models (LMs) is intrinsic\nhallucination: the generation of information that is plausible but\nunsubstantiated relative to input context. To study this problem, we propose\nPrecise Information Control (PIC), a new task formulation that requires models\nto generate long-form outputs grounded in a provided set of short\nself-contained statements, known as verifiable claims, without adding any\nunsupported ones. For comprehensiveness, PIC includes a full setting that tests\na model's ability to include exactly all input claims, and a partial setting\nthat requires the model to selectively incorporate only relevant claims. We\npresent PIC-Bench, a benchmark of eight long-form generation tasks (e.g.,\nsummarization, biography generation) adapted to the PIC setting, where LMs are\nsupplied with well-formed, verifiable input claims. Our evaluation of a range\nof open and proprietary LMs on PIC-Bench reveals that, surprisingly,\nstate-of-the-art LMs still intrinsically hallucinate in over 70% of outputs. To\nalleviate this lack of faithfulness, we introduce a post-training framework,\nusing a weakly supervised preference data construction method, to train an 8B\nPIC-LM with stronger PIC ability--improving from 69.1% to 91.0% F1 in the full\nPIC setting. When integrated into end-to-end factual generation pipelines,\nPIC-LM improves exact match recall by 17.1% on ambiguous QA with retrieval, and\nfactual precision by 30.5% on a birthplace verification task, underscoring the\npotential of precisely grounded generation.", "AI": {"tldr": "论文提出Precise Information Control (PIC)任务，研究语言模型的内在幻觉问题，并通过PIC-Bench评估模型表现。提出一种后训练框架PIC-LM，显著提升生成内容的准确性。", "motivation": "现代语言模型存在内在幻觉问题，即生成看似合理但未经验证的信息。研究旨在通过PIC任务和PIC-LM框架解决这一问题。", "method": "提出PIC任务，要求模型基于可验证的短句生成长文本。构建PIC-Bench基准测试，并开发后训练框架PIC-LM，通过弱监督偏好数据提升模型性能。", "result": "评估显示，当前先进模型在70%以上的输出中存在幻觉。PIC-LM将F1分数从69.1%提升至91.0%，并在实际任务中显著提升准确性。", "conclusion": "PIC-LM框架有效减少了语言模型的幻觉问题，展示了精确生成内容的潜力。"}}
{"id": "2506.06719", "pdf": "https://arxiv.org/pdf/2506.06719", "abs": "https://arxiv.org/abs/2506.06719", "authors": ["Mufhumudzi Muthivhi", "Jiahao Huo", "Fredrik Gustafsson", "Terence L. van Zyl"], "title": "Improving Wildlife Out-of-Distribution Detection: Africas Big Five", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Mitigating human-wildlife conflict seeks to resolve unwanted encounters\nbetween these parties. Computer Vision provides a solution to identifying\nindividuals that might escalate into conflict, such as members of the Big Five\nAfrican animals. However, environments often contain several varied species.\nThe current state-of-the-art animal classification models are trained under a\nclosed-world assumption. They almost always remain overconfident in their\npredictions even when presented with unknown classes. This study investigates\nout-of-distribution (OOD) detection of wildlife, specifically the Big Five. To\nthis end, we select a parametric Nearest Class Mean (NCM) and a non-parametric\ncontrastive learning approach as baselines to take advantage of pretrained and\nprojected features from popular classification encoders. Moreover, we compare\nour baselines to various common OOD methods in the literature. The results show\nfeature-based methods reflect stronger generalisation capability across varying\nclassification thresholds. Specifically, NCM with ImageNet pre-trained features\nachieves a 2%, 4% and 22% improvement on AUPR-IN, AUPR-OUT and AUTC over the\nbest OOD methods, respectively. The code can be found here\nhttps://github.com/pxpana/BIG5OOD", "AI": {"tldr": "该研究探讨了野生动物（特别是非洲五大动物）的分布外检测问题，通过对比参数化和非参数化方法，发现基于特征的方法在泛化能力上表现更优。", "motivation": "解决野生动物与人类冲突的识别问题，当前模型在未知类别上表现过自信，需改进分布外检测能力。", "method": "采用参数化的最近类均值（NCM）和非参数化的对比学习方法，利用预训练分类编码器的特征，并与文献中常见的分布外检测方法对比。", "result": "基于特征的方法在泛化能力上表现更优，NCM方法在多个指标上优于其他分布外检测方法。", "conclusion": "特征方法在野生动物分布外检测中具有潜力，NCM结合预训练特征表现最佳。"}}
{"id": "2506.06786", "pdf": "https://arxiv.org/pdf/2506.06786", "abs": "https://arxiv.org/abs/2506.06786", "authors": ["Dimitris Panagopoulos", "Adolfo Perrusquia", "Weisi Guo"], "title": "Learning What Matters Now: A Dual-Critic Context-Aware RL Framework for Priority-Driven Information Gain", "categories": ["cs.AI"], "comment": "6 pages, 2 figures, 3 tables, submitted as a regural paper to IEEE\n  International Conference on Systems, Man, and Cybernetics (SMC) 2025", "summary": "Autonomous systems operating in high-stakes search-and-rescue (SAR) missions\nmust continuously gather mission-critical information while flexibly adapting\nto shifting operational priorities. We propose CA-MIQ (Context-Aware\nMax-Information Q-learning), a lightweight dual-critic reinforcement learning\n(RL) framework that dynamically adjusts its exploration strategy whenever\nmission priorities change. CA-MIQ pairs a standard extrinsic critic for task\nreward with an intrinsic critic that fuses state-novelty, information-location\nawareness, and real-time priority alignment. A built-in shift detector triggers\ntransient exploration boosts and selective critic resets, allowing the agent to\nre-focus after a priority revision. In a simulated SAR grid-world, where\nexperiments specifically test adaptation to changes in the priority order of\ninformation types the agent is expected to focus on, CA-MIQ achieves nearly\nfour times higher mission-success rates than baselines after a single priority\nshift and more than three times better performance in multiple-shift scenarios,\nachieving 100% recovery while baseline methods fail to adapt. These results\nhighlight CA-MIQ's effectiveness in any discrete environment with\npiecewise-stationary information-value distributions.", "AI": {"tldr": "CA-MIQ是一种轻量级双批评家强化学习框架，通过动态调整探索策略适应任务优先级变化，在SAR任务中显著优于基线方法。", "motivation": "高风险的搜救任务中，系统需持续收集关键信息并灵活适应优先级变化，传统方法难以满足需求。", "method": "CA-MIQ结合外在批评家和内在批评家（融合状态新颖性、信息位置感知和实时优先级对齐），通过内置的优先级变化检测器触发探索增强和选择性重置。", "result": "在模拟SAR任务中，CA-MIQ在单次和多优先级变化场景下分别实现4倍和3倍以上的任务成功率，基线方法无法适应。", "conclusion": "CA-MIQ适用于信息价值分布分段稳定的离散环境，能有效应对动态优先级变化。"}}
{"id": "2506.06605", "pdf": "https://arxiv.org/pdf/2506.06605", "abs": "https://arxiv.org/abs/2506.06605", "authors": ["Xiao Wang", "Mengjue Tan", "Qiao Jin", "Guangzhi Xiong", "Yu Hu", "Aidong Zhang", "Zhiyong Lu", "Minjia Zhang"], "title": "MedCite: Can Language Models Generate Verifiable Text for Medicine?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Existing LLM-based medical question-answering systems lack citation\ngeneration and evaluation capabilities, raising concerns about their adoption\nin practice. In this work, we introduce \\name, the first end-to-end framework\nthat facilitates the design and evaluation of citation generation with LLMs for\nmedical tasks. Meanwhile, we introduce a novel multi-pass retrieval-citation\nmethod that generates high-quality citations. Our evaluation highlights the\nchallenges and opportunities of citation generation for medical tasks, while\nidentifying important design choices that have a significant impact on the\nfinal citation quality. Our proposed method achieves superior citation\nprecision and recall improvements compared to strong baseline methods, and we\nshow that evaluation results correlate well with annotation results from\nprofessional experts.", "AI": {"tldr": "本文提出了第一个端到端框架，用于设计和评估基于LLM的医学任务引用生成，并引入了一种新颖的多轮检索-引用方法，显著提升了引用质量。", "motivation": "现有基于LLM的医学问答系统缺乏引用生成和评估功能，限制了其实际应用。", "method": "提出了一种端到端框架和一种多轮检索-引用方法，用于生成高质量的引用。", "result": "该方法在引用精确度和召回率上优于基线方法，且评估结果与专家标注结果高度一致。", "conclusion": "研究揭示了医学任务引用生成的挑战与机遇，并提出了关键设计选择，显著提升了引用质量。"}}
{"id": "2506.06729", "pdf": "https://arxiv.org/pdf/2506.06729", "abs": "https://arxiv.org/abs/2506.06729", "authors": ["Zixian Gao", "Chao Yang", "Zhanhui Zhou", "Xing Xu", "Chaochao Lu"], "title": "Mitigating Object Hallucination via Robust Local Perception Search", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Recent advancements in Multimodal Large Language Models (MLLMs) have enabled\nthem to effectively integrate vision and language, addressing a variety of\ndownstream tasks. However, despite their significant success, these models\nstill exhibit hallucination phenomena, where the outputs appear plausible but\ndo not align with the content of the images. To mitigate this issue, we\nintroduce Local Perception Search (LPS), a decoding method during inference\nthat is both simple and training-free, yet effectively suppresses\nhallucinations. This method leverages local visual prior information as a value\nfunction to correct the decoding process. Additionally, we observe that the\nimpact of the local visual prior on model performance is more pronounced in\nscenarios with high levels of image noise. Notably, LPS is a plug-and-play\napproach that is compatible with various models. Extensive experiments on\nwidely used hallucination benchmarks and noisy data demonstrate that LPS\nsignificantly reduces the incidence of hallucinations compared to the baseline,\nshowing exceptional performance, particularly in noisy settings.", "AI": {"tldr": "论文提出了一种名为LPS的解码方法，用于减少多模态大语言模型中的幻觉现象，无需训练且兼容多种模型。", "motivation": "尽管多模态大语言模型在视觉和语言任务中表现优异，但仍存在幻觉问题，即输出与图像内容不符。", "method": "LPS利用局部视觉先验信息作为价值函数，在推理过程中修正解码过程。", "result": "实验表明，LPS显著减少了幻觉现象，尤其在噪声较高的场景中表现突出。", "conclusion": "LPS是一种简单、无需训练且高效的方法，适用于多种模型，能有效抑制幻觉现象。"}}
{"id": "2506.06832", "pdf": "https://arxiv.org/pdf/2506.06832", "abs": "https://arxiv.org/abs/2506.06832", "authors": ["Clément Hongler", "Andrew Emil"], "title": "Cross-Entropy Games for Language Models: From Implicit Knowledge to General Capability Measures", "categories": ["cs.AI", "cs.CL", "cs.GT", "cs.IT", "cs.NE", "math.IT"], "comment": "41 pages, 16 figures", "summary": "Large Language Models (LLMs) define probability measures on text. By\nconsidering the implicit knowledge question of what it means for an LLM to know\nsuch a measure and what it entails algorithmically, we are naturally led to\nformulate a series of tasks that go beyond generative sampling, involving forms\nof summarization, counterfactual thinking, anomaly detection, originality\nsearch, reverse prompting, debating, creative solving, etc. These tasks can be\nformulated as games based on LLM measures, which we call Cross-Entropy (Xent)\nGames. Xent Games can be single-player or multi-player. They involve\ncross-entropy scores and cross-entropy constraints, and can be expressed as\nsimple computational graphs and programs. We show the Xent Game space is large\nenough to contain a wealth of interesting examples, while being constructible\nfrom basic game-theoretic consistency axioms. We then discuss how the Xent Game\nspace can be used to measure the abilities of LLMs. This leads to the\nconstruction of Xent Game measures: finite families of Xent Games that can be\nused as capability benchmarks, built from a given scope, by extracting a\ncovering measure. To address the unbounded scope problem associated with the\nchallenge of measuring general abilities, we propose to explore the space of\nXent Games in a coherent fashion, using ideas inspired by evolutionary\ndynamics.", "AI": {"tldr": "论文探讨了如何通过基于大型语言模型（LLM）的交叉熵（Xent）游戏来评估LLM的能力，提出了Xent游戏的概念及其在多种任务中的应用。", "motivation": "研究LLM如何通过概率度量理解文本，并探索其能力评估的新方法。", "method": "提出Xent游戏框架，包括单人和多人游戏，基于交叉熵分数和约束，并通过计算图和程序表达。", "result": "Xent游戏空间足够丰富，可用于构建能力基准测试（Xent Game measures），并通过进化动力学方法探索其空间。", "conclusion": "Xent游戏为评估LLM能力提供了新工具，并展示了其在多种任务中的潜力。"}}
{"id": "2506.06607", "pdf": "https://arxiv.org/pdf/2506.06607", "abs": "https://arxiv.org/abs/2506.06607", "authors": ["Charles Goddard", "Fernando Fernandes Neto"], "title": "Training-Free Tokenizer Transplantation via Orthogonal Matching Pursuit", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "We present a training-free method to transplant tokenizers in pretrained\nlarge language models (LLMs) by reconstructing unseen token embeddings via\nOrthogonal Matching Pursuit (OMP). Specifically, we approximate each\nout-of-vocabulary token as a sparse linear combination of shared tokens, in two\nphases: first, compute each new token's representation in the donor embedding\nspace with a small dictionary of shared anchor tokens, then transfer these same\nsparse coefficients back into the base model's embedding space.\n  On two challenging cross-tokenizer tasks--Llama$\\to$Mistral NeMo (12B) and\nQwen$\\to$Llama (1B)--we show that OMP achieves best zero-shot preservation of\nthe base model's performance across multiple benchmarks, while other zero-shot\napproaches degrade significantly. Compared to baselines (zero-init, mean-init,\nand existing approaches like WECHSEL, FOCUS, ZETT), OMP consistently achieves\nthe best overall performance, effectively bridging large tokenizer\ndiscrepancies without gradient updates. Our analysis further identifies\nmismatched numerical tokenization schemes as a critical challenge for\npreserving mathematical reasoning capabilities. This technique enables direct\nreuse of pretrained model weights with new tokenizers, facilitating\ncross-tokenizer knowledge distillation, speculative decoding, ensembling,\nmerging, and domain-specific vocabulary adaptations. We integrate our method\ninto the open-source mergekit-tokensurgeon tool for post hoc vocabulary\nrealignment.", "AI": {"tldr": "提出一种无需训练的Tokenizer移植方法，通过正交匹配追踪（OMP）重建未见过的token嵌入，在零样本任务中表现最佳。", "motivation": "解决预训练大语言模型（LLMs）中tokenizer不匹配的问题，避免梯度更新，实现跨tokenizer的知识迁移。", "method": "使用OMP算法，将新token表示为共享token的稀疏线性组合，分两阶段完成嵌入空间的重建。", "result": "在Llama→Mistral和Qwen→Llama任务中，OMP在零样本性能保持上优于其他基线方法。", "conclusion": "OMP方法有效解决了tokenizer差异问题，支持跨tokenizer的应用场景，如知识蒸馏和领域适配。"}}
{"id": "2506.06733", "pdf": "https://arxiv.org/pdf/2506.06733", "abs": "https://arxiv.org/abs/2506.06733", "authors": ["Ruoxuan Zhang", "Jidong Gao", "Bin Wen", "Hongxia Xie", "Chenming Zhang", "Honghan-shuai", "Wen-Huang Cheng"], "title": "RecipeGen: A Step-Aligned Multimodal Benchmark for Real-World Recipe Generation", "categories": ["cs.CV"], "comment": "This is an extended version of arXiv:2503.05228", "summary": "Creating recipe images is a key challenge in food computing, with\napplications in culinary education and multimodal recipe assistants. However,\nexisting datasets lack fine-grained alignment between recipe goals, step-wise\ninstructions, and visual content. We present RecipeGen, the first large-scale,\nreal-world benchmark for recipe-based Text-to-Image (T2I), Image-to-Video\n(I2V), and Text-to-Video (T2V) generation. RecipeGen contains 26,453 recipes,\n196,724 images, and 4,491 videos, covering diverse ingredients, cooking\nprocedures, styles, and dish types. We further propose domain-specific\nevaluation metrics to assess ingredient fidelity and interaction modeling,\nbenchmark representative T2I, I2V, and T2V models, and provide insights for\nfuture recipe generation models. Project page is available now.", "AI": {"tldr": "RecipeGen是一个大规模的真实世界基准数据集，用于基于食谱的文本到图像（T2I）、图像到视频（I2V）和文本到视频（T2V）生成，包含26,453个食谱、196,724张图片和4,491个视频。", "motivation": "现有数据集缺乏食谱目标、逐步指令和视觉内容之间的细粒度对齐，限制了食品计算在烹饪教育和多模态食谱助手中的应用。", "method": "提出了RecipeGen数据集，并设计了领域特定的评估指标，用于评估食材保真度和交互建模。", "result": "RecipeGen涵盖了多样化的食材、烹饪程序、风格和菜品类型，并提供了对代表性T2I、I2V和T2V模型的基准测试。", "conclusion": "RecipeGen为未来的食谱生成模型提供了有价值的见解和基准，推动了食品计算领域的发展。"}}
{"id": "2506.06843", "pdf": "https://arxiv.org/pdf/2506.06843", "abs": "https://arxiv.org/abs/2506.06843", "authors": ["HaoYang Shang", "Xuan Liu", "Zi Liang", "Jie Zhang", "Haibo Hu", "Song Guo"], "title": "United Minds or Isolated Agents? Exploring Coordination of LLMs under Cognitive Load Theory", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) exhibit a notable performance ceiling on\ncomplex, multi-faceted tasks, as they often fail to integrate diverse\ninformation or adhere to multiple constraints. We posit that such limitation\narises when the demands of a task exceed the LLM's effective cognitive load\ncapacity. This interpretation draws a strong analogy to Cognitive Load Theory\n(CLT) in cognitive science, which explains similar performance boundaries in\nthe human mind, and is further supported by emerging evidence that reveals LLMs\nhave bounded working memory characteristics. Building upon this CLT-grounded\nunderstanding, we introduce CoThinker, a novel LLM-based multi-agent framework\ndesigned to mitigate cognitive overload and enhance collaborative\nproblem-solving abilities. CoThinker operationalizes CLT principles by\ndistributing intrinsic cognitive load through agent specialization and managing\ntransactional load via structured communication and a collective working\nmemory. We empirically validate CoThinker on complex problem-solving tasks and\nfabricated high cognitive load scenarios, demonstrating improvements over\nexisting multi-agent baselines in solution quality and efficiency. Our analysis\nreveals characteristic interaction patterns, providing insights into the\nemergence of collective cognition and effective load management, thus offering\na principled approach to overcoming LLM performance ceilings.", "AI": {"tldr": "论文提出CoThinker框架，通过多代理协作减轻LLM的认知负载，提升复杂任务表现。", "motivation": "LLM在复杂任务中表现受限，类比认知负载理论（CLT），提出其性能瓶颈源于认知超载。", "method": "引入CoThinker框架，通过代理分工和结构化通信分配认知负载，管理集体工作记忆。", "result": "实验验证CoThinker在复杂任务中优于现有多代理基线，提升解决方案质量和效率。", "conclusion": "CoThinker为克服LLM性能瓶颈提供了理论支持，展示了集体认知和负载管理的潜力。"}}
{"id": "2506.06609", "pdf": "https://arxiv.org/pdf/2506.06609", "abs": "https://arxiv.org/abs/2506.06609", "authors": ["Alan Chen", "Jack Merullo", "Alessandro Stolfo", "Ellie Pavlick"], "title": "Transferring Features Across Language Models With Model Stitching", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "In this work, we demonstrate that affine mappings between residual streams of\nlanguage models is a cheap way to effectively transfer represented features\nbetween models. We apply this technique to transfer the weights of Sparse\nAutoencoders (SAEs) between models of different sizes to compare their\nrepresentations. We find that small and large models learn highly similar\nrepresentation spaces, which motivates training expensive components like SAEs\non a smaller model and transferring to a larger model at a FLOPs savings. For\nexample, using a small-to-large transferred SAE as initialization can lead to\n50% cheaper training runs when training SAEs on larger models. Next, we show\nthat transferred probes and steering vectors can effectively recover ground\ntruth performance. Finally, we dive deeper into feature-level transferability,\nfinding that semantic and structural features transfer noticeably differently\nwhile specific classes of functional features have their roles faithfully\nmapped. Overall, our findings illustrate similarities and differences in the\nlinear representation spaces of small and large models and demonstrate a method\nfor improving the training efficiency of SAEs.", "AI": {"tldr": "通过仿射映射在语言模型残差流之间传递特征，证明小模型和大模型的表示空间高度相似，可节省训练成本。", "motivation": "探索如何高效地在不同大小的语言模型之间传递特征表示，以降低训练成本。", "method": "使用仿射映射技术将稀疏自编码器（SAE）的权重在不同大小的模型间传递，比较其表示空间。", "result": "小模型和大模型的表示空间高度相似，SAE从小模型传递到大模型可节省50%训练成本；特征级传递性显示语义和结构特征传递效果不同。", "conclusion": "研究揭示了大小模型线性表示空间的异同，并提出了一种提升SAE训练效率的方法。"}}
{"id": "2506.06748", "pdf": "https://arxiv.org/pdf/2506.06748", "abs": "https://arxiv.org/abs/2506.06748", "authors": ["Mingqi Gao", "Haoran Duan", "Tianlu Zhang", "Jungong Han"], "title": "THU-Warwick Submission for EPIC-KITCHEN Challenge 2025: Semi-Supervised Video Object Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "In this report, we describe our approach to egocentric video object\nsegmentation. Our method combines large-scale visual pretraining from SAM2 with\ndepth-based geometric cues to handle complex scenes and long-term tracking. By\nintegrating these signals in a unified framework, we achieve strong\nsegmentation performance. On the VISOR test set, our method reaches a J&F score\nof 90.1%.", "AI": {"tldr": "提出了一种结合视觉预训练和深度几何线索的自中心视频对象分割方法，性能优异。", "motivation": "处理复杂场景和长期跟踪的自中心视频对象分割需求。", "method": "结合SAM2的大规模视觉预训练和深度几何线索，统一框架集成信号。", "result": "在VISOR测试集上达到90.1%的J&F分数。", "conclusion": "该方法通过统一框架有效提升了分割性能。"}}
{"id": "2506.06868", "pdf": "https://arxiv.org/pdf/2506.06868", "abs": "https://arxiv.org/abs/2506.06868", "authors": ["Razieh Arshadizadeh", "Mahmoud Asgari", "Zeinab Khosravi", "Yiannis Papadopoulos", "Koorosh Aslansefat"], "title": "Incorporating Failure of Machine Learning in Dynamic Probabilistic Safety Assurance", "categories": ["cs.AI"], "comment": null, "summary": "Machine Learning (ML) models are increasingly integrated into safety-critical\nsystems, such as autonomous vehicle platooning, to enable real-time\ndecision-making. However, their inherent imperfection introduces a new class of\nfailure: reasoning failures often triggered by distributional shifts between\noperational and training data. Traditional safety assessment methods, which\nrely on design artefacts or code, are ill-suited for ML components that learn\nbehaviour from data. SafeML was recently proposed to dynamically detect such\nshifts and assign confidence levels to the reasoning of ML-based components.\nBuilding on this, we introduce a probabilistic safety assurance framework that\nintegrates SafeML with Bayesian Networks (BNs) to model ML failures as part of\na broader causal safety analysis. This allows for dynamic safety evaluation and\nsystem adaptation under uncertainty. We demonstrate the approach on an\nsimulated automotive platooning system with traffic sign recognition. The\nfindings highlight the potential broader benefits of explicitly modelling ML\nfailures in safety assessment.", "AI": {"tldr": "论文提出了一种结合SafeML和贝叶斯网络的概率安全保证框架，用于动态评估和适应机器学习在安全关键系统中的不确定性。", "motivation": "机器学习模型在安全关键系统中的应用日益广泛，但其固有的不完美性可能导致推理失败，传统安全评估方法对此不适用。", "method": "提出了一种概率安全保证框架，整合SafeML和贝叶斯网络，动态检测分布偏移并建模机器学习失败。", "result": "在模拟的自动驾驶车队系统中验证了该框架，展示了显式建模机器学习失败在安全评估中的潜在优势。", "conclusion": "该框架为安全关键系统中的机器学习提供了动态安全评估和适应能力，具有广泛的应用潜力。"}}
{"id": "2506.06616", "pdf": "https://arxiv.org/pdf/2506.06616", "abs": "https://arxiv.org/abs/2506.06616", "authors": ["Samuel Kim", "Oghenemaro Imieye", "Yunting Yin"], "title": "Interpretable Depression Detection from Social Media Text Using LLM-Derived Embeddings", "categories": ["cs.CL"], "comment": "Submitted to the IEEE EMBS BHI 2025 Conference", "summary": "Accurate and interpretable detection of depressive language in social media\nis useful for early interventions of mental health conditions, and has\nimportant implications for both clinical practice and broader public health\nefforts. In this paper, we investigate the performance of large language models\n(LLMs) and traditional machine learning classifiers across three classification\ntasks involving social media data: binary depression classification, depression\nseverity classification, and differential diagnosis classification among\ndepression, PTSD, and anxiety. Our study compares zero-shot LLMs with\nsupervised classifiers trained on both conventional text embeddings and\nLLM-generated summary embeddings. Our experiments reveal that while zero-shot\nLLMs demonstrate strong generalization capabilities in binary classification,\nthey struggle with fine-grained ordinal classifications. In contrast,\nclassifiers trained on summary embeddings generated by LLMs demonstrate\ncompetitive, and in some cases superior, performance on the classification\ntasks, particularly when compared to models using traditional text embeddings.\nOur findings demonstrate the strengths of LLMs in mental health prediction, and\nsuggest promising directions for better utilization of their zero-shot\ncapabilities and context-aware summarization techniques.", "AI": {"tldr": "论文研究了大型语言模型（LLMs）和传统机器学习分类器在社交媒体数据上的抑郁语言检测性能，发现零样本LLMs在二元分类中表现良好，但在细粒度分类中表现较差，而基于LLM生成的摘要嵌入的分类器表现更优。", "motivation": "准确且可解释的抑郁语言检测对心理健康早期干预具有重要意义，可应用于临床实践和公共卫生。", "method": "比较了零样本LLMs和监督分类器在三种分类任务中的表现：二元抑郁分类、抑郁严重程度分类以及抑郁、PTSD和焦虑的鉴别诊断。", "result": "零样本LLMs在二元分类中泛化能力强，但在细粒度分类中表现不佳；基于LLM摘要嵌入的分类器表现更优。", "conclusion": "LLMs在心理健康预测中具有潜力，未来可通过优化零样本能力和上下文感知摘要技术进一步提升性能。"}}
{"id": "2506.06757", "pdf": "https://arxiv.org/pdf/2506.06757", "abs": "https://arxiv.org/abs/2506.06757", "authors": ["Ziyu Yue", "Ruixi You", "Feng Xu"], "title": "SAR2Struct: Extracting 3D Semantic Structural Representation of Aircraft Targets from Single-View SAR Image", "categories": ["cs.CV"], "comment": "13 pages, 12 figures", "summary": "To translate synthetic aperture radar (SAR) image into interpretable forms\nfor human understanding is the ultimate goal of SAR advanced information\nretrieval. Existing methods mainly focus on 3D surface reconstruction or local\ngeometric feature extraction of targets, neglecting the role of structural\nmodeling in capturing semantic information. This paper proposes a novel task:\nSAR target structure recovery, which aims to infer the components of a target\nand the structural relationships between its components, specifically symmetry\nand adjacency, from a single-view SAR image. Through learning the structural\nconsistency and geometric diversity across the same type of targets as observed\nin different SAR images, it aims to derive the semantic representation of\ntarget directly from its 2D SAR image. To solve this challenging task, a\ntwo-step algorithmic framework based on structural descriptors is developed.\nSpecifically, in the training phase, it first detects 2D keypoints from real\nSAR images, and then learns the mapping from these keypoints to 3D hierarchical\nstructures using simulated data. During the testing phase, these two steps are\nintegrated to infer the 3D structure from real SAR images. Experimental results\nvalidated the effectiveness of each step and demonstrated, for the first time,\nthat 3D semantic structural representation of aircraft targets can be directly\nderived from a single-view SAR image.", "AI": {"tldr": "该论文提出了一种新任务：从单视角SAR图像中恢复目标结构，通过两步算法框架推断目标组件及其结构关系（对称性和邻接性），首次实现了从SAR图像直接获取3D语义结构表示。", "motivation": "现有方法主要关注3D表面重建或局部几何特征提取，忽视了结构建模在捕获语义信息中的作用。本文旨在填补这一空白，提出SAR目标结构恢复任务。", "method": "采用基于结构描述符的两步算法框架：训练阶段从真实SAR图像检测2D关键点，并利用模拟数据学习这些关键点到3D层次结构的映射；测试阶段整合这两步从真实SAR图像推断3D结构。", "result": "实验验证了每一步的有效性，首次证明可以从单视角SAR图像直接获取飞机目标的3D语义结构表示。", "conclusion": "该方法为SAR图像的高级信息检索提供了新思路，展示了结构建模在语义信息提取中的潜力。"}}
{"id": "2506.06881", "pdf": "https://arxiv.org/pdf/2506.06881", "abs": "https://arxiv.org/abs/2506.06881", "authors": ["Zixuan Li", "Wenxuan Liu", "Long Bai", "Chunmao Zhang", "Wei Li", "Fenghui Zhang", "Quanxin Jin", "Ruoyun He", "Zhuo Chen", "Zhilei Hu", "Fei Wang", "Bingbing Xu", "Xuhui Jiang", "Xiaolong Jin", "Jiafeng Guo", "Xueqi Cheng"], "title": "KnowCoder-V2: Deep Knowledge Analysis", "categories": ["cs.AI"], "comment": null, "summary": "Deep knowledge analysis tasks always involve the systematic extraction and\nassociation of knowledge from large volumes of data, followed by logical\nreasoning to discover insights. However, to solve such complex tasks, existing\ndeep research frameworks face three major challenges: 1) They lack systematic\norganization and management of knowledge; 2) They operate purely online, making\nit inefficient for tasks that rely on shared and large-scale knowledge; 3) They\ncannot perform complex knowledge computation, limiting their abilities to\nproduce insightful analytical results. Motivated by these, in this paper, we\npropose a \\textbf{K}nowledgeable \\textbf{D}eep \\textbf{R}esearch (\\textbf{KDR})\nframework that empowers deep research with deep knowledge analysis capability.\nSpecifically, it introduces an independent knowledge organization phase to\npreprocess large-scale, domain-relevant data into systematic knowledge offline.\nBased on this knowledge, it extends deep research with an additional kind of\nreasoning steps that perform complex knowledge computation in an online manner.\nTo enhance the abilities of LLMs to solve knowledge analysis tasks in the above\nframework, we further introduce \\textbf{\\KCII}, an LLM that bridges knowledge\norganization and reasoning via unified code generation. For knowledge\norganization, it generates instantiation code for predefined classes,\ntransforming data into knowledge objects. For knowledge computation, it\ngenerates analysis code and executes on the above knowledge objects to obtain\ndeep analysis results. Experimental results on more than thirty datasets across\nsix knowledge analysis tasks demonstrate the effectiveness of \\KCII. Moreover,\nwhen integrated into the KDR framework, \\KCII can generate high-quality reports\nwith insightful analytical results compared to the mainstream deep research\nframework.", "AI": {"tldr": "论文提出了一种名为KDR的框架，通过离线知识组织和在线复杂知识计算，解决了现有深度学习框架在知识管理、效率和计算能力上的不足，并进一步引入了KCII模型以增强知识分析和推理能力。", "motivation": "现有深度学习框架在知识管理、效率和复杂知识计算方面存在不足，无法满足深度知识分析任务的需求。", "method": "提出KDR框架，包括离线知识组织阶段和在线知识计算阶段，并引入KCII模型通过统一代码生成连接知识组织和推理。", "result": "在六个知识分析任务的三十多个数据集上验证了KCII的有效性，KDR框架能生成高质量的分析报告。", "conclusion": "KDR框架和KCII模型显著提升了深度知识分析的能力和效率。"}}
{"id": "2506.06619", "pdf": "https://arxiv.org/pdf/2506.06619", "abs": "https://arxiv.org/abs/2506.06619", "authors": ["Jesse Woo", "Fateme Hashemi Chaleshtori", "Ana Marasović", "Kenneth Marino"], "title": "BriefMe: A Legal NLP Benchmark for Assisting with Legal Briefs", "categories": ["cs.CL"], "comment": "ACL Findings 2025; 10 pages main, 5 pages references, 37 pages\n  appendix", "summary": "A core part of legal work that has been under-explored in Legal NLP is the\nwriting and editing of legal briefs. This requires not only a thorough\nunderstanding of the law of a jurisdiction, from judgments to statutes, but\nalso the ability to make new arguments to try to expand the law in a new\ndirection and make novel and creative arguments that are persuasive to judges.\nTo capture and evaluate these legal skills in language models, we introduce\nBRIEFME, a new dataset focused on legal briefs. It contains three tasks for\nlanguage models to assist legal professionals in writing briefs: argument\nsummarization, argument completion, and case retrieval. In this work, we\ndescribe the creation of these tasks, analyze them, and show how current models\nperform. We see that today's large language models (LLMs) are already quite\ngood at the summarization and guided completion tasks, even beating\nhuman-generated headings. Yet, they perform poorly on other tasks in our\nbenchmark: realistic argument completion and retrieving relevant legal cases.\nWe hope this dataset encourages more development in Legal NLP in ways that will\nspecifically aid people in performing legal work.", "AI": {"tldr": "论文介绍了BRIEFME数据集，用于评估语言模型在法律简报写作中的能力，包括论点总结、论点补全和案例检索任务。结果显示，当前大语言模型在总结和补全任务上表现良好，但在实际论点补全和案例检索上表现不佳。", "motivation": "探索法律简报写作这一未被充分研究的法律NLP领域，帮助法律专业人士提升写作效率。", "method": "创建BRIEFME数据集，包含三个任务：论点总结、论点补全和案例检索，并评估当前语言模型的表现。", "result": "大语言模型在总结和补全任务上表现优异，但在实际论点补全和案例检索任务上表现较差。", "conclusion": "BRIEFME数据集有望推动法律NLP的发展，更好地支持法律工作。"}}
{"id": "2506.06759", "pdf": "https://arxiv.org/pdf/2506.06759", "abs": "https://arxiv.org/abs/2506.06759", "authors": ["Nidheesh Gorthi", "Kartik Thakral", "Rishabh Ranjan", "Richa Singh", "Mayank Vatsa"], "title": "LitMAS: A Lightweight and Generalized Multi-Modal Anti-Spoofing Framework for Biometric Security", "categories": ["cs.CV"], "comment": "Accepted in Interspeech 2025", "summary": "Biometric authentication systems are increasingly being deployed in critical\napplications, but they remain susceptible to spoofing. Since most of the\nresearch efforts focus on modality-specific anti-spoofing techniques, building\na unified, resource-efficient solution across multiple biometric modalities\nremains a challenge. To address this, we propose LitMAS, a\n$\\textbf{Li}$gh$\\textbf{t}$ weight and generalizable $\\textbf{M}$ulti-modal\n$\\textbf{A}$nti-$\\textbf{S}$poofing framework designed to detect spoofing\nattacks in speech, face, iris, and fingerprint-based biometric systems. At the\ncore of LitMAS is a Modality-Aligned Concentration Loss, which enhances\ninter-class separability while preserving cross-modal consistency and enabling\nrobust spoof detection across diverse biometric traits. With just 6M\nparameters, LitMAS surpasses state-of-the-art methods by $1.36\\%$ in average\nEER across seven datasets, demonstrating high efficiency, strong\ngeneralizability, and suitability for edge deployment. Code and trained models\nare available at https://github.com/IAB-IITJ/LitMAS.", "AI": {"tldr": "LitMAS是一个轻量级、通用的多模态反欺骗框架，用于检测语音、人脸、虹膜和指纹生物识别系统中的欺骗攻击，性能优于现有方法。", "motivation": "生物识别认证系统在关键应用中广泛部署，但仍易受欺骗攻击。现有研究多关注特定模态的反欺骗技术，缺乏统一且高效的跨模态解决方案。", "method": "提出LitMAS框架，采用模态对齐集中损失（Modality-Aligned Concentration Loss），增强类间分离性并保持跨模态一致性。", "result": "LitMAS仅需6M参数，在七个数据集上的平均EER优于现有方法1.36%，表现出高效性、强泛化能力和边缘部署潜力。", "conclusion": "LitMAS为多模态生物识别反欺骗提供了一种高效、通用的解决方案，适合实际应用部署。"}}
{"id": "2506.06905", "pdf": "https://arxiv.org/pdf/2506.06905", "abs": "https://arxiv.org/abs/2506.06905", "authors": ["Akash Gupta", "Amos Storkey", "Mirella Lapata"], "title": "Meta-Adaptive Prompt Distillation for Few-Shot Visual Question Answering", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Large Multimodal Models (LMMs) often rely on in-context learning (ICL) to\nperform new tasks with minimal supervision. However, ICL performance,\nespecially in smaller LMMs, is inconsistent and does not always improve\nmonotonically with increasing examples. We hypothesize that this occurs due to\nthe LMM being overwhelmed by additional information present in the image\nembeddings, which is not required for the downstream task. To address this, we\npropose a meta-learning approach that provides an alternative for inducing\nfew-shot capabilities in LMMs, using a fixed set of soft prompts that are\ndistilled from task-relevant image features and can be adapted at test time\nusing a few examples. To facilitate this distillation, we introduce an\nattention-mapper module that can be easily integrated with the popular LLaVA\nv1.5 architecture and is jointly learned with soft prompts, enabling task\nadaptation in LMMs under low-data regimes with just a few gradient steps.\nEvaluation on the VL-ICL Bench shows that our method consistently outperforms\nICL and related prompt-tuning approaches, even under image perturbations,\nimproving task induction and reasoning across visual question answering tasks.", "AI": {"tldr": "论文提出了一种元学习方法，通过蒸馏任务相关图像特征生成固定软提示，以提升小型多模态模型（LMMs）在少样本任务中的表现。", "motivation": "现有LMMs依赖上下文学习（ICL）进行少样本任务，但性能不稳定且不随示例增加单调提升，推测原因是图像嵌入中的冗余信息干扰。", "method": "提出元学习方法，通过注意力映射模块蒸馏任务相关特征生成软提示，并与LLaVA v1.5架构结合，实现少样本任务适应。", "result": "在VL-ICL Bench上评估，该方法优于ICL及其他提示调优方法，且在图像扰动下仍能提升任务诱导和视觉问答推理能力。", "conclusion": "该方法为LMMs在低数据场景下的任务适应提供了有效解决方案，显著提升了少样本学习性能。"}}
{"id": "2506.06626", "pdf": "https://arxiv.org/pdf/2506.06626", "abs": "https://arxiv.org/abs/2506.06626", "authors": ["Junzhe Wang", "Bichen Wang", "Xing Fu", "Yixin Sun", "Yanyan Zhao", "Bing Qin"], "title": "Psychological Counseling Cannot Be Achieved Overnight: Automated Psychological Counseling Through Multi-Session Conversations", "categories": ["cs.CL"], "comment": "15 pages, 19 figures", "summary": "In recent years, Large Language Models (LLMs) have made significant progress\nin automated psychological counseling. However, current research focuses on\nsingle-session counseling, which doesn't represent real-world scenarios. In\npractice, psychological counseling is a process, not a one-time event,\nrequiring sustained, multi-session engagement to progressively address clients'\nissues. To overcome this limitation, we introduce a dataset for Multi-Session\nPsychological Counseling Conversation Dataset (MusPsy-Dataset). Our\nMusPsy-Dataset is constructed using real client profiles from publicly\navailable psychological case reports. It captures the dynamic arc of\ncounseling, encompassing multiple progressive counseling conversations from the\nsame client across different sessions. Leveraging our dataset, we also\ndeveloped our MusPsy-Model, which aims to track client progress and adapt its\ncounseling direction over time. Experiments show that our model performs better\nthan baseline models across multiple sessions.", "AI": {"tldr": "论文提出了一个多会话心理咨询数据集（MusPsy-Dataset）和模型（MusPsy-Model），以解决当前LLMs在心理咨询中仅关注单次会话的局限性。", "motivation": "当前大型语言模型（LLMs）在心理咨询中的研究集中于单次会话，而实际心理咨询是一个需要多次会话的持续过程。", "method": "通过公开心理案例报告构建多会话心理咨询数据集（MusPsy-Dataset），并开发了能追踪客户进展的MusPsy-Model。", "result": "实验表明，MusPsy-Model在多会话场景中表现优于基线模型。", "conclusion": "MusPsy-Dataset和MusPsy-Model填补了LLMs在持续心理咨询中的研究空白，提升了多会话场景下的表现。"}}
{"id": "2506.06771", "pdf": "https://arxiv.org/pdf/2506.06771", "abs": "https://arxiv.org/abs/2506.06771", "authors": ["Mohammad-Maher Nakshbandi", "Ziad Sharawy", "Dorian Cojocaru", "Sorin Grigorescu"], "title": "LoopDB: A Loop Closure Dataset for Large Scale Simultaneous Localization and Mapping", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "In this study, we introduce LoopDB, which is a challenging loop closure\ndataset comprising over 1000 images captured across diverse environments,\nincluding parks, indoor scenes, parking spaces, as well as centered around\nindividual objects. Each scene is represented by a sequence of five consecutive\nimages. The dataset was collected using a high resolution camera, providing\nsuitable imagery for benchmarking the accuracy of loop closure algorithms,\ntypically used in simultaneous localization and mapping. As ground truth\ninformation, we provide computed rotations and translations between each\nconsecutive images. Additional to its benchmarking goal, the dataset can be\nused to train and fine-tune loop closure methods based on deep neural networks.\nLoopDB is publicly available at https://github.com/RovisLab/LoopDB.", "AI": {"tldr": "LoopDB是一个包含1000多张图像的闭环数据集，覆盖多种环境，用于测试和训练闭环算法。", "motivation": "提供高质量的数据集以支持闭环算法的基准测试和深度学习方法的训练。", "method": "使用高分辨率相机采集图像序列，每场景包含五张连续图像，并提供旋转和平移的真值数据。", "result": "数据集公开可用，适用于闭环算法的基准测试和深度学习模型的训练。", "conclusion": "LoopDB为闭环算法的研究和开发提供了有价值的资源。"}}
{"id": "2506.06910", "pdf": "https://arxiv.org/pdf/2506.06910", "abs": "https://arxiv.org/abs/2506.06910", "authors": ["Mahnaz Koupaee", "Xueying Bai", "Mudan Chen", "Greg Durrett", "Nathanael Chambers", "Niranjan Balasubramanian"], "title": "Causal Graph based Event Reasoning using Semantic Relation Experts", "categories": ["cs.AI"], "comment": null, "summary": "Understanding how events in a scenario causally connect with each other is\nimportant for effectively modeling and reasoning about events. But event\nreasoning remains a difficult challenge, and despite recent advances, Large\nLanguage Models (LLMs) still struggle to accurately identify causal connections\nbetween events. This struggle leads to poor performance on deeper reasoning\ntasks like event forecasting and timeline understanding. To address this\nchallenge, we investigate the generation of causal event graphs (e.g., A\nenables B) as a parallel mechanism to help LLMs explicitly represent causality\nduring inference. This paper evaluates both how to generate correct graphs as\nwell as how graphs can assist reasoning. We propose a collaborative approach to\ncausal graph generation where we use LLMs to simulate experts that focus on\nspecific semantic relations. The experts engage in multiple rounds of\ndiscussions which are then consolidated by a final expert. Then, to demonstrate\nthe utility of causal graphs, we use them on multiple downstream applications,\nand also introduce a new explainable event prediction task that requires a\ncausal chain of events in the explanation. These explanations are more\ninformative and coherent than baseline generations. Finally, our overall\napproach not finetuned on any downstream task, achieves competitive results\nwith state-of-the-art models on both forecasting and next event prediction\ntasks.", "AI": {"tldr": "论文提出了一种通过生成因果事件图来帮助大型语言模型（LLM）显式表示因果关系的方法，并展示了其在事件预测和时间线理解等任务中的有效性。", "motivation": "事件推理是一个具有挑战性的任务，LLM在识别事件间因果关系时表现不佳，影响了更深层次推理任务的性能。", "method": "采用协作方法生成因果事件图，利用LLM模拟专家讨论并整合结果，随后将生成的图应用于下游任务。", "result": "提出的方法在未微调下游任务的情况下，在事件预测和解释任务中取得了与最先进模型竞争的结果。", "conclusion": "因果事件图能有效提升LLM的因果关系表示能力，并在多个应用中表现出优越性。"}}
{"id": "2506.06636", "pdf": "https://arxiv.org/pdf/2506.06636", "abs": "https://arxiv.org/abs/2506.06636", "authors": ["Chuxue Cao", "Han Zhu", "Jiaming Ji", "Qichao Sun", "Zhenghao Zhu", "Yinyu Wu", "Juntao Dai", "Yaodong Yang", "Sirui Han", "Yike Guo"], "title": "SafeLawBench: Towards Safe Alignment of Large Language Models", "categories": ["cs.CL"], "comment": "Accepted to ACL2025 Findings", "summary": "With the growing prevalence of large language models (LLMs), the safety of\nLLMs has raised significant concerns. However, there is still a lack of\ndefinitive standards for evaluating their safety due to the subjective nature\nof current safety benchmarks. To address this gap, we conducted the first\nexploration of LLMs' safety evaluation from a legal perspective by proposing\nthe SafeLawBench benchmark. SafeLawBench categorizes safety risks into three\nlevels based on legal standards, providing a systematic and comprehensive\nframework for evaluation. It comprises 24,860 multi-choice questions and 1,106\nopen-domain question-answering (QA) tasks. Our evaluation included 2\nclosed-source LLMs and 18 open-source LLMs using zero-shot and few-shot\nprompting, highlighting the safety features of each model. We also evaluated\nthe LLMs' safety-related reasoning stability and refusal behavior.\nAdditionally, we found that a majority voting mechanism can enhance model\nperformance. Notably, even leading SOTA models like Claude-3.5-Sonnet and\nGPT-4o have not exceeded 80.5% accuracy in multi-choice tasks on SafeLawBench,\nwhile the average accuracy of 20 LLMs remains at 68.8\\%. We urge the community\nto prioritize research on the safety of LLMs.", "AI": {"tldr": "论文提出了基于法律视角的SafeLawBench基准，用于系统评估大语言模型的安全性，发现现有模型在安全性任务上表现不足。", "motivation": "当前大语言模型的安全性评估缺乏明确标准，主观性强，需从法律角度提供系统性框架。", "method": "提出SafeLawBench基准，包含多选和开放域QA任务，评估20种模型的安全性和推理稳定性。", "result": "领先模型如Claude-3.5-Sonnet和GPT-4o在多选任务中准确率未超过80.5%，20种模型平均准确率为68.8%。", "conclusion": "呼吁社区重视大语言模型安全性研究，并提出多数投票机制可提升模型表现。"}}
{"id": "2506.06780", "pdf": "https://arxiv.org/pdf/2506.06780", "abs": "https://arxiv.org/abs/2506.06780", "authors": ["Lennart Bastian", "Mohammad Rashed", "Nassir Navab", "Tolga Birdal"], "title": "Continuous-Time SO(3) Forecasting with Savitzky--Golay Neural Controlled Differential Equations", "categories": ["cs.CV", "cs.LG"], "comment": "Extended abstract, presented at the CVPR Workshop on 4D Vision", "summary": "Tracking and forecasting the rotation of objects is fundamental in computer\nvision and robotics, yet SO(3) extrapolation remains challenging as (1) sensor\nobservations can be noisy and sparse, (2) motion patterns can be governed by\ncomplex dynamics, and (3) application settings can demand long-term\nforecasting. This work proposes modeling continuous-time rotational object\ndynamics on $SO(3)$ using Neural Controlled Differential Equations guided by\nSavitzky-Golay paths. Unlike existing methods that rely on simplified motion\nassumptions, our method learns a general latent dynamical system of the\nunderlying object trajectory while respecting the geometric structure of\nrotations. Experimental results on real-world data demonstrate compelling\nforecasting capabilities compared to existing approaches.", "AI": {"tldr": "论文提出了一种基于神经控制微分方程和Savitzky-Golay路径的连续时间旋转物体动力学建模方法，用于解决SO(3)外推问题。", "motivation": "SO(3)外推在计算机视觉和机器人学中具有基础性意义，但由于传感器噪声、稀疏观测、复杂动力学和长时预测需求，现有方法难以应对。", "method": "利用神经控制微分方程在SO(3)上建模旋转物体动力学，结合Savitzky-Golay路径，避免简化运动假设，同时尊重旋转的几何结构。", "result": "在真实数据上的实验表明，该方法相比现有方法具有更强的预测能力。", "conclusion": "该方法通过学习潜在动力学系统，显著提升了旋转物体轨迹的预测性能。"}}
{"id": "2506.06923", "pdf": "https://arxiv.org/pdf/2506.06923", "abs": "https://arxiv.org/abs/2506.06923", "authors": ["Xutong Zhao", "Tengyu Xu", "Xuewei Wang", "Zhengxing Chen", "Di Jin", "Liang Tan", "Yen-Ting", "Zishun Yu", "Zhuokai Zhao", "Yun He", "Sinong Wang", "Han Fang", "Sarath Chandar", "Chen Zhu"], "title": "Boosting LLM Reasoning via Spontaneous Self-Correction", "categories": ["cs.AI"], "comment": null, "summary": "While large language models (LLMs) have demonstrated remarkable success on a\nbroad range of tasks, math reasoning remains a challenging one. One of the\napproaches for improving math reasoning is self-correction, which designs\nself-improving loops to let the model correct its own mistakes. However,\nexisting self-correction approaches treat corrections as standalone\npost-generation refinements, relying on extra prompt and system designs to\nelicit self-corrections, instead of performing real-time, spontaneous\nself-corrections in a single pass. To address this, we propose SPOC, a\nspontaneous self-correction approach that enables LLMs to generate interleaved\nsolutions and verifications in a single inference pass, with generation\ndynamically terminated based on verification outcomes, thereby effectively\nscaling inference time compute. SPOC considers a multi-agent perspective by\nassigning dual roles -- solution proposer and verifier -- to the same model. We\nadopt a simple yet effective approach to generate synthetic data for\nfine-tuning, enabling the model to develop capabilities for self-verification\nand multi-agent collaboration. We further improve its solution proposal and\nverification accuracy through online reinforcement learning. Experiments on\nmathematical reasoning benchmarks show that SPOC significantly improves\nperformance. Notably, SPOC boosts the accuracy of Llama-3.1-8B and 70B Instruct\nmodels, achieving gains of 8.8% and 11.6% on MATH500, 10.0% and 20.0% on AMC23,\nand 3.3% and 6.7% on AIME24, respectively.", "AI": {"tldr": "SPOC是一种实时自校正方法，通过单次推理生成交错解和验证，显著提升LLM的数学推理能力。", "motivation": "现有自校正方法依赖额外提示和设计，无法实现单次实时自校正，SPOC旨在解决这一问题。", "method": "SPOC采用多智能体视角，将模型分为解提议者和验证者角色，通过合成数据微调和在线强化学习提升能力。", "result": "实验显示SPOC显著提升模型性能，如Llama-3.1-8B在MATH500上准确率提升8.8%。", "conclusion": "SPOC通过实时自校正和多智能体协作，有效提升了LLM的数学推理能力。"}}
{"id": "2506.06657", "pdf": "https://arxiv.org/pdf/2506.06657", "abs": "https://arxiv.org/abs/2506.06657", "authors": ["Nikhita Vedula", "Dushyanta Dhyani", "Laleh Jalali", "Boris Oreshkin", "Mohsen Bayati", "Shervin Malmasi"], "title": "Quantile Regression with Large Language Models for Price Prediction", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to Findings of ACL, 2025", "summary": "Large Language Models (LLMs) have shown promise in structured prediction\ntasks, including regression, but existing approaches primarily focus on point\nestimates and lack systematic comparison across different methods. We\ninvestigate probabilistic regression using LLMs for unstructured inputs,\naddressing challenging text-to-distribution prediction tasks such as price\nestimation where both nuanced text understanding and uncertainty quantification\nare critical. We propose a novel quantile regression approach that enables LLMs\nto produce full predictive distributions, improving upon traditional point\nestimates. Through extensive experiments across three diverse price prediction\ndatasets, we demonstrate that a Mistral-7B model fine-tuned with quantile heads\nsignificantly outperforms traditional approaches for both point and\ndistributional estimations, as measured by three established metrics each for\nprediction accuracy and distributional calibration. Our systematic comparison\nof LLM approaches, model architectures, training approaches, and data scaling\nreveals that Mistral-7B consistently outperforms encoder architectures,\nembedding-based methods, and few-shot learning methods. Our experiments also\nreveal the effectiveness of LLM-assisted label correction in achieving\nhuman-level accuracy without systematic bias. Our curated datasets are made\navailable at https://github.com/vnik18/llm-price-quantile-reg/ to support\nfuture research.", "AI": {"tldr": "本文研究了利用大语言模型（LLMs）进行概率回归，提出了一种新的分位数回归方法，显著优于传统点估计方法。", "motivation": "现有方法主要关注点估计，缺乏对不同方法的系统比较，尤其是在需要文本理解和不确定性量化的任务中。", "method": "提出了一种分位数回归方法，使LLMs能够生成完整的预测分布，并在三个价格预测数据集上进行了实验。", "result": "实验表明，经过分位数头微调的Mistral-7B模型在预测准确性和分布校准方面显著优于传统方法。", "conclusion": "LLMs在概率回归任务中表现出色，尤其是在文本到分布预测任务中，且LLM辅助的标签校正能实现人类水平的准确性。"}}
{"id": "2506.06802", "pdf": "https://arxiv.org/pdf/2506.06802", "abs": "https://arxiv.org/abs/2506.06802", "authors": ["Mohammad Ali Rezaei", "Helia Hajikazem", "Saeed Khanehgir", "Mahdi Javanmardi"], "title": "Training-Free Identity Preservation in Stylized Image Generation Using Diffusion Models", "categories": ["cs.CV"], "comment": null, "summary": "While diffusion models have demonstrated remarkable generative capabilities,\nexisting style transfer techniques often struggle to maintain identity while\nachieving high-quality stylization. This limitation is particularly acute for\nimages where faces are small or exhibit significant camera-to-face distances,\nfrequently leading to inadequate identity preservation. To address this, we\nintroduce a novel, training-free framework for identity-preserved stylized\nimage synthesis using diffusion models. Key contributions include: (1) the\n\"Mosaic Restored Content Image\" technique, significantly enhancing identity\nretention, especially in complex scenes; and (2) a training-free content\nconsistency loss that enhances the preservation of fine-grained content details\nby directing more attention to the original image during stylization. Our\nexperiments reveal that the proposed approach substantially surpasses the\nbaseline model in concurrently maintaining high stylistic fidelity and robust\nidentity integrity, particularly under conditions of small facial regions or\nsignificant camera-to-face distances, all without necessitating model\nretraining or fine-tuning.", "AI": {"tldr": "提出了一种基于扩散模型的无需训练的身份保留风格化图像合成框架，显著提升了小面部或远距离拍摄图像的身份保留能力。", "motivation": "现有风格迁移技术在保持身份的同时实现高质量风格化方面存在困难，尤其是在小面部或远距离拍摄图像中。", "method": "1. 使用“马赛克恢复内容图像”技术增强身份保留；2. 提出无需训练的内容一致性损失，提升细节保留。", "result": "实验表明，该方法在保持高风格保真度和身份完整性方面显著优于基线模型，尤其适用于小面部或远距离场景。", "conclusion": "该方法无需模型重新训练或微调，即可有效解决身份保留问题。"}}
{"id": "2506.06935", "pdf": "https://arxiv.org/pdf/2506.06935", "abs": "https://arxiv.org/abs/2506.06935", "authors": ["Darui Lu", "Jordan M. Malof", "Willie J. Padilla"], "title": "An Agentic Framework for Autonomous Metamaterial Modeling and Inverse Design", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "comment": "22 pages, 6 figures", "summary": "Recent significant advances in integrating multiple Large Language Model\n(LLM) systems have enabled Agentic Frameworks capable of performing complex\ntasks autonomously, including novel scientific research. We develop and\ndemonstrate such a framework specifically for the inverse design of photonic\nmetamaterials. When queried with a desired optical spectrum, the Agent\nautonomously proposes and develops a forward deep learning model, accesses\nexternal tools via APIs for tasks like simulation and optimization, utilizes\nmemory, and generates a final design via a deep inverse method. The framework's\neffectiveness is demonstrated in its ability to automate, reason, plan, and\nadapt. Notably, the Agentic Framework possesses internal reflection and\ndecision flexibility, permitting highly varied and potentially novel outputs.", "AI": {"tldr": "论文提出了一种基于多LLM系统的Agentic框架，用于光子超材料的逆向设计，能够自主完成任务并生成新颖设计。", "motivation": "整合多LLM系统以实现复杂任务的自主执行，特别是在科学研究的逆向设计领域。", "method": "框架通过查询目标光学光谱，自主开发前向深度学习模型，利用API进行模拟和优化，结合记忆功能，并通过深度逆向方法生成最终设计。", "result": "框架展示了自动化、推理、规划和适应能力，并能生成多样化和潜在新颖的输出。", "conclusion": "该Agentic框架在光子超材料逆向设计中表现出高效性和灵活性，具备内部反思和决策能力。"}}
{"id": "2506.06686", "pdf": "https://arxiv.org/pdf/2506.06686", "abs": "https://arxiv.org/abs/2506.06686", "authors": ["Chunyuan Deng", "Ruidi Chang", "Hanjie Chen"], "title": "Learning Distribution-Wise Control in Representation Space for Language Models", "categories": ["cs.CL"], "comment": "ICML 2025", "summary": "Interventions in language models (LMs) are applied strategically to steer\nmodel behavior during the forward pass. Learnable interventions, also known as\nrepresentation fine-tuning, aim to apply pointwise control within the concept\nsubspace and have proven effective in altering high-level behaviors. In this\nwork, we extend this approach to the distribution level, enabling the model to\nlearn not only pointwise transformations but also the surrounding regions of\nthe concept subspace. We demonstrate that these methods perform effectively in\nearly layers, with larger standard deviations correlating strongly with\nimproved performance. Across eight commonsense reasoning and seven arithmetic\nreasoning benchmarks, our distribution-wise interventions consistently\noutperform pointwise interventions in controllability and robustness. These\nresults illustrate that distribution-wise interventions provide a more\ncomprehensive method for steering model behavior and enabling finer-grained\ncontrol over language models. The code is at:\n\\href{https://github.com/chili-lab/D-Intervention}{https://github.com/chili-lab/D-Intervention}.", "AI": {"tldr": "论文提出了一种分布级别的干预方法，扩展了点干预的局限性，通过控制概念子空间的周围区域，提升了语言模型的可控性和鲁棒性。", "motivation": "现有方法（点干预）仅能对概念子空间进行点控制，限制了模型的灵活性和性能。", "method": "提出分布级别的干预方法，学习概念子空间及其周围区域的变换，并在早期层应用。", "result": "在八个常识推理和七个算术推理基准测试中，分布干预方法在可控性和鲁棒性上均优于点干预方法。", "conclusion": "分布干预为语言模型行为提供了更全面的控制方法，实现了更细粒度的调控。"}}
{"id": "2506.06818", "pdf": "https://arxiv.org/pdf/2506.06818", "abs": "https://arxiv.org/abs/2506.06818", "authors": ["Chao Yin", "Hao Li", "Kequan Yang", "Jide Li", "Pinpin Zhu", "Xiaoqiang Li"], "title": "Stepwise Decomposition and Dual-stream Focus: A Novel Approach for Training-free Camouflaged Object Segmentation", "categories": ["cs.CV"], "comment": "under review", "summary": "While promptable segmentation (\\textit{e.g.}, SAM) has shown promise for\nvarious segmentation tasks, it still requires manual visual prompts for each\nobject to be segmented. In contrast, task-generic promptable segmentation aims\nto reduce the need for such detailed prompts by employing only a task-generic\nprompt to guide segmentation across all test samples. However, when applied to\nCamouflaged Object Segmentation (COS), current methods still face two critical\nissues: 1) \\textit{\\textbf{semantic ambiguity in getting instance-specific text\nprompts}}, which arises from insufficient discriminative cues in holistic\ncaptions, leading to foreground-background confusion; 2)\n\\textit{\\textbf{semantic discrepancy combined with spatial separation in\ngetting instance-specific visual prompts}}, which results from global\nbackground sampling far from object boundaries with low feature correlation,\ncausing SAM to segment irrelevant regions. To address the issues above, we\npropose \\textbf{RDVP-MSD}, a novel training-free test-time adaptation framework\nthat synergizes \\textbf{R}egion-constrained \\textbf{D}ual-stream\n\\textbf{V}isual \\textbf{P}rompting (RDVP) via \\textbf{M}ultimodal\n\\textbf{S}tepwise \\textbf{D}ecomposition Chain of Thought (MSD-CoT). MSD-CoT\nprogressively disentangles image captions to eliminate semantic ambiguity,\nwhile RDVP injects spatial constraints into visual prompting and independently\nsamples visual prompts for foreground and background points, effectively\nmitigating semantic discrepancy and spatial separation. Without requiring any\ntraining or supervision, RDVP-MSD achieves a state-of-the-art segmentation\nresult on multiple COS benchmarks and delivers a faster inference speed than\nprevious methods, demonstrating significantly improved accuracy and efficiency.\nThe codes will be available at\n\\href{https://github.com/ycyinchao/RDVP-MSD}{https://github.com/ycyinchao/RDVP-MSD}", "AI": {"tldr": "论文提出RDVP-MSD框架，通过多模态逐步分解思维链（MSD-CoT）和区域约束双流视觉提示（RDVP），解决了伪装目标分割中的语义模糊和空间分离问题，无需训练即可实现高效分割。", "motivation": "当前任务通用提示分割方法在伪装目标分割（COS）中存在语义模糊和空间分离问题，导致分割不准确。", "method": "结合MSD-CoT逐步分解图像描述以消除语义模糊，RDVP通过空间约束和独立采样视觉提示解决空间分离问题。", "result": "在多个COS基准测试中达到最先进的分割效果，且推理速度更快。", "conclusion": "RDVP-MSD无需训练即可显著提升分割精度和效率，适用于复杂场景。"}}
{"id": "2506.06941", "pdf": "https://arxiv.org/pdf/2506.06941", "abs": "https://arxiv.org/abs/2506.06941", "authors": ["Parshin Shojaee", "Iman Mirzadeh", "Keivan Alizadeh", "Maxwell Horton", "Samy Bengio", "Mehrdad Farajtabar"], "title": "The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "preprint", "summary": "Recent generations of language models have introduced Large Reasoning Models\n(LRMs) that generate detailed thinking processes before providing answers.\nWhile these models demonstrate improved performance on reasoning benchmarks,\ntheir fundamental capabilities, scaling properties, and limitations remain\ninsufficiently understood. Current evaluations primarily focus on established\nmath and coding benchmarks, emphasizing final answer accuracy. However, this\nevaluation paradigm often suffers from contamination and does not provide\ninsights into the reasoning traces. In this work, we systematically investigate\nthese gaps with the help of controllable puzzle environments that allow precise\nmanipulation of complexity while maintaining consistent logical structures.\nThis setup enables the analysis of not only final answers but also the internal\nreasoning traces, offering insights into how LRMs think. Through extensive\nexperiments, we show that LRMs face a complete accuracy collapse beyond certain\ncomplexities. Moreover, they exhibit a counterintuitive scaling limit: their\nreasoning effort increases with problem complexity up to a point, then declines\ndespite having remaining token budget. By comparing LRMs with their standard\nLLM counterparts under same inference compute, we identify three performance\nregimes: (1) low-complexity tasks where standard models outperform LRMs, (2)\nmedium-complexity tasks where LRMs demonstrates advantage, and (3)\nhigh-complexity tasks where both models face complete collapse. We found that\nLRMs have limitations in exact computation: they fail to use explicit\nalgorithms and reason inconsistently across scales. We also investigate the\nreasoning traces in more depth, studying the patterns of explored solutions and\nanalyzing the models' computational behavior, shedding light on their\nstrengths, limitations, and raising questions about their reasoning\ncapabilities.", "AI": {"tldr": "论文研究了大型推理模型（LRMs）的基本能力、扩展特性和局限性，通过可控的谜题环境分析了其推理轨迹，发现LRMs在特定复杂度后会完全失效，并揭示了其反直觉的扩展限制。", "motivation": "当前对LRMs的评价主要关注数学和编程基准的最终答案准确性，但缺乏对其推理轨迹的深入理解，且存在数据污染问题。因此，需要系统研究LRMs的能力和限制。", "method": "使用可控的谜题环境，精确操纵问题复杂度，同时保持逻辑结构一致，分析LRMs的推理轨迹和最终答案。", "result": "LRMs在特定复杂度后完全失效，推理努力随问题复杂度增加至某一点后下降。与标准LLMs相比，LRMs在中等复杂度任务中表现优越，但在低和高复杂度任务中表现不佳。", "conclusion": "LRMs在精确计算方面存在局限性，无法使用显式算法，且推理不一致。研究揭示了其优势和限制，并对其推理能力提出了质疑。"}}
{"id": "2506.06704", "pdf": "https://arxiv.org/pdf/2506.06704", "abs": "https://arxiv.org/abs/2506.06704", "authors": ["Weihang Su", "Qingyao Ai", "Jingtao Zhan", "Qian Dong", "Yiqun Liu"], "title": "Dynamic and Parametric Retrieval-Augmented Generation", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has become a foundational paradigm for\nequipping large language models (LLMs) with external knowledge, playing a\ncritical role in information retrieval and knowledge-intensive applications.\nHowever, conventional RAG systems typically adopt a static\nretrieve-then-generate pipeline and rely on in-context knowledge injection,\nwhich can be suboptimal for complex tasks that require multihop reasoning,\nadaptive information access, and deeper integration of external knowledge.\nMotivated by these limitations, the research community has moved beyond static\nretrieval and in-context knowledge injection. Among the emerging directions,\nthis tutorial delves into two rapidly growing and complementary research areas\non RAG: Dynamic RAG and Parametric RAG. Dynamic RAG adaptively determines when\nand what to retrieve during the LLM's generation process, enabling real-time\nadaptation to the LLM's evolving information needs. Parametric RAG rethinks how\nretrieved knowledge should be injected into LLMs, transitioning from\ninput-level to parameter-level knowledge injection for enhanced efficiency and\neffectiveness. This tutorial offers a comprehensive overview of recent advances\nin these emerging research areas. It also shares theoretical foundations and\npractical insights to support and inspire further research in RAG.", "AI": {"tldr": "本文介绍了检索增强生成（RAG）的两种新兴研究方向：动态RAG和参数化RAG，以解决传统静态检索的局限性。", "motivation": "传统RAG系统采用静态检索和上下文知识注入，对复杂任务效果不佳，因此需要动态和参数化的改进。", "method": "动态RAG实时调整检索时机和内容，参数化RAG将知识注入从输入级提升到参数级。", "result": "动态RAG和参数化RAG在效率和效果上优于传统方法。", "conclusion": "本文为RAG的未来研究提供了理论基础和实践指导。"}}
{"id": "2506.06822", "pdf": "https://arxiv.org/pdf/2506.06822", "abs": "https://arxiv.org/abs/2506.06822", "authors": ["Chenlu Zhan", "Yufei Zhang", "Gaoang Wang", "Hongwei Wang"], "title": "Hi-LSplat: Hierarchical 3D Language Gaussian Splatting", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Modeling 3D language fields with Gaussian Splatting for open-ended language\nqueries has recently garnered increasing attention. However, recent 3DGS-based\nmodels leverage view-dependent 2D foundation models to refine 3D semantics but\nlack a unified 3D representation, leading to view inconsistencies.\nAdditionally, inherent open-vocabulary challenges cause inconsistencies in\nobject and relational descriptions, impeding hierarchical semantic\nunderstanding. In this paper, we propose Hi-LSplat, a view-consistent\nHierarchical Language Gaussian Splatting work for 3D open-vocabulary querying.\nTo achieve view-consistent 3D hierarchical semantics, we first lift 2D features\nto 3D features by constructing a 3D hierarchical semantic tree with layered\ninstance clustering, which addresses the view inconsistency issue caused by 2D\nsemantic features. Besides, we introduce instance-wise and part-wise\ncontrastive losses to capture all-sided hierarchical semantic representations.\nNotably, we construct two hierarchical semantic datasets to better assess the\nmodel's ability to distinguish different semantic levels. Extensive experiments\nhighlight our method's superiority in 3D open-vocabulary segmentation and\nlocalization. Its strong performance on hierarchical semantic datasets\nunderscores its ability to capture complex hierarchical semantics within 3D\nscenes.", "AI": {"tldr": "Hi-LSplat提出了一种基于3D高斯泼溅的分层语言模型，解决了现有方法因依赖2D特征而导致的视角不一致和词汇开放性问题。", "motivation": "现有3DGS模型依赖2D基础模型，导致视角不一致和语义层次理解不足。", "method": "通过构建3D分层语义树和引入实例级与部件级对比损失，实现视角一致的3D语义表示。", "result": "实验表明，Hi-LSplat在3D开放词汇分割和定位任务中表现优越，并能捕捉复杂的分层语义。", "conclusion": "Hi-LSplat通过统一的3D表示和分层语义建模，显著提升了3D场景的语义理解能力。"}}
{"id": "2506.06959", "pdf": "https://arxiv.org/pdf/2506.06959", "abs": "https://arxiv.org/abs/2506.06959", "authors": ["Alena Makarova", "Houssam Abbas"], "title": "Deontically Constrained Policy Improvement in Reinforcement Learning Agents", "categories": ["cs.AI", "60J10 (Primary), 60J20 (Primary), 60J22 (Primary), 93E20 (Secondary)", "D.2.4; F.3.1; I.2.8"], "comment": "20 pages, 11 figures, DEON2025 conference", "summary": "Markov Decision Processes (MDPs) are the most common model for decision\nmaking under uncertainty in the Machine Learning community. An MDP captures\nnon-determinism, probabilistic uncertainty, and an explicit model of action. A\nReinforcement Learning (RL) agent learns to act in an MDP by maximizing a\nutility function. This paper considers the problem of learning a decision\npolicy that maximizes utility subject to satisfying a constraint expressed in\ndeontic logic. In this setup, the utility captures the agent's mission - such\nas going quickly from A to B. The deontic formula represents (ethical, social,\nsituational) constraints on how the agent might achieve its mission by\nprohibiting classes of behaviors. We use the logic of Expected Act\nUtilitarianism, a probabilistic stit logic that can be interpreted over\ncontrolled MDPs. We develop a variation on policy improvement, and show that it\nreaches a constrained local maximum of the mission utility. Given that in stit\nlogic, an agent's duty is derived from value maximization, this can be seen as\na way of acting to simultaneously maximize two value functions, one of which is\nimplicit, in a bi-level structure. We illustrate these results with experiments\non sample MDPs.", "AI": {"tldr": "该论文研究了在马尔可夫决策过程（MDP）中，如何通过学习决策策略来最大化效用，同时满足基于道义逻辑的约束。", "motivation": "在强化学习中，智能体通常通过最大化效用函数来学习行为，但实际应用中可能需要遵守伦理、社会或情境约束。本文旨在解决如何在满足这些约束的同时优化任务效用。", "method": "论文提出了一种基于期望行为功利主义逻辑的变体策略改进方法，该方法能够在受控MDP中实现约束下的局部效用最大化。", "result": "实验表明，该方法能够在样本MDP中实现约束下的局部最优解，同时满足道义逻辑的约束。", "conclusion": "通过结合道义逻辑和效用最大化，论文提供了一种在复杂约束下优化智能体行为的方法，为伦理约束下的强化学习提供了新思路。"}}
{"id": "2506.06705", "pdf": "https://arxiv.org/pdf/2506.06705", "abs": "https://arxiv.org/abs/2506.06705", "authors": ["Zhihui Chen", "Kai He", "Yucheng Huang", "Yunxiao Zhu", "Mengling Feng"], "title": "DivScore: Zero-Shot Detection of LLM-Generated Text in Specialized Domains", "categories": ["cs.CL", "cs.AI"], "comment": "Zhihui Chen and Kai He contributed equally to this work, Mengling\n  Feng is the corresponding author", "summary": "Detecting LLM-generated text in specialized and high-stakes domains like\nmedicine and law is crucial for combating misinformation and ensuring\nauthenticity. However, current zero-shot detectors, while effective on general\ntext, often fail when applied to specialized content due to domain shift. We\nprovide a theoretical analysis showing this failure is fundamentally linked to\nthe KL divergence between human, detector, and source text distributions. To\naddress this, we propose DivScore, a zero-shot detection framework using\nnormalized entropy-based scoring and domain knowledge distillation to robustly\nidentify LLM-generated text in specialized domains. We also release a\ndomain-specific benchmark for LLM-generated text detection in the medical and\nlegal domains. Experiments on our benchmark show that DivScore consistently\noutperforms state-of-the-art detectors, with 14.4% higher AUROC and 64.0%\nhigher recall (0.1% false positive rate threshold). In adversarial settings,\nDivScore demonstrates superior robustness than other baselines, achieving on\naverage 22.8% advantage in AUROC and 29.5% in recall. Code and data are\npublicly available.", "AI": {"tldr": "论文提出DivScore框架，通过熵评分和领域知识蒸馏，在专业领域（如医学和法律）中更有效地检测LLM生成文本，显著优于现有方法。", "motivation": "在专业和高风险领域检测LLM生成文本对防止错误信息和确保真实性至关重要，但现有零样本检测器在专业内容上表现不佳。", "method": "提出DivScore框架，结合归一化熵评分和领域知识蒸馏，以应对领域偏移问题。", "result": "DivScore在医学和法律领域的实验中表现优异，AUROC提升14.4%，召回率提升64.0%，且在对抗性环境中更具鲁棒性。", "conclusion": "DivScore为专业领域LLM生成文本检测提供了高效且鲁棒的解决方案，代码和数据已公开。"}}
{"id": "2506.06823", "pdf": "https://arxiv.org/pdf/2506.06823", "abs": "https://arxiv.org/abs/2506.06823", "authors": ["Qi Li", "Liangzhi Li", "Zhouqiang Jiang", "Bowen Wang", "Keke Tang"], "title": "Exploring Visual Prompting: Robustness Inheritance and Beyond", "categories": ["cs.CV", "cs.AI"], "comment": "arXiv admin note: substantial text overlap with arXiv:2311.10992", "summary": "Visual Prompting (VP), an efficient method for transfer learning, has shown\nits potential in vision tasks. However, previous works focus exclusively on VP\nfrom standard source models, it is still unknown how it performs under the\nscenario of a robust source model: Can the robustness of the source model be\nsuccessfully inherited? Does VP also encounter the same trade-off between\nrobustness and generalization ability as the source model during this process?\nIf such a trade-off exists, is there a strategy specifically tailored to VP to\nmitigate this limitation? In this paper, we thoroughly explore these three\nquestions for the first time and provide affirmative answers to them. To\nmitigate the trade-off faced by VP, we propose a strategy called Prompt\nBoundary Loosening (PBL). As a lightweight, plug-and-play strategy naturally\ncompatible with VP, PBL effectively ensures the successful inheritance of\nrobustness when the source model is a robust model, while significantly\nenhancing VP's generalization ability across various downstream datasets.\nExtensive experiments across various datasets show that our findings are\nuniversal and demonstrate the significant benefits of the proposed strategy.", "AI": {"tldr": "本文探讨了视觉提示（VP）在鲁棒源模型下的表现，提出了Prompt Boundary Loosening（PBL）策略以解决鲁棒性与泛化能力的权衡问题。", "motivation": "研究VP在鲁棒源模型下的表现及其鲁棒性与泛化能力的权衡问题。", "method": "提出PBL策略，作为一种轻量级、即插即用的方法，与VP兼容。", "result": "实验证明PBL能有效继承源模型的鲁棒性并显著提升VP的泛化能力。", "conclusion": "PBL策略在多种数据集上表现出普遍适用性，解决了VP的局限性。"}}
{"id": "2506.06965", "pdf": "https://arxiv.org/pdf/2506.06965", "abs": "https://arxiv.org/abs/2506.06965", "authors": ["Cuong Manh Hoang"], "title": "Long-Tailed Learning for Generalized Category Discovery", "categories": ["cs.AI", "cs.CV"], "comment": null, "summary": "Generalized Category Discovery (GCD) utilizes labeled samples of known\nclasses to discover novel classes in unlabeled samples. Existing methods show\neffective performance on artificial datasets with balanced distributions.\nHowever, real-world datasets are always imbalanced, significantly affecting the\neffectiveness of these methods. To solve this problem, we propose a novel\nframework that performs generalized category discovery in long-tailed\ndistributions. We first present a self-guided labeling technique that uses a\nlearnable distribution to generate pseudo-labels, resulting in less biased\nclassifiers. We then introduce a representation balancing process to derive\ndiscriminative representations. By mining sample neighborhoods, this process\nencourages the model to focus more on tail classes. We conduct experiments on\npublic datasets to demonstrate the effectiveness of the proposed framework. The\nresults show that our model exceeds previous state-of-the-art methods.", "AI": {"tldr": "提出了一种在长尾分布中进行广义类别发现的新框架，通过自引导标记和表示平衡技术提升性能。", "motivation": "现实世界数据集通常是不平衡的，现有方法在人工平衡数据集上表现良好，但在实际应用中效果受限。", "method": "采用自引导标记技术生成伪标签以减少偏差，并通过表示平衡过程挖掘样本邻域以关注尾部类别。", "result": "在公开数据集上的实验表明，该框架优于现有最优方法。", "conclusion": "提出的方法在长尾分布中有效提升了广义类别发现的性能。"}}
{"id": "2506.06708", "pdf": "https://arxiv.org/pdf/2506.06708", "abs": "https://arxiv.org/abs/2506.06708", "authors": ["Haiqi Yang", "Zhiyuan Li", "Yi Chang", "Yuan Wu"], "title": "A Survey of Retentive Network", "categories": ["cs.CL"], "comment": "15 pages, 3 figures", "summary": "Retentive Network (RetNet) represents a significant advancement in neural\nnetwork architecture, offering an efficient alternative to the Transformer.\nWhile Transformers rely on self-attention to model dependencies, they suffer\nfrom high memory costs and limited scalability when handling long sequences due\nto their quadratic complexity. To mitigate these limitations, RetNet introduces\na retention mechanism that unifies the inductive bias of recurrence with the\nglobal dependency modeling of attention. This mechanism enables linear-time\ninference, facilitates efficient modeling of extended contexts, and remains\ncompatible with fully parallelizable training pipelines. RetNet has garnered\nsignificant research interest due to its consistently demonstrated cross-domain\neffectiveness, achieving robust performance across machine learning paradigms\nincluding natural language processing, speech recognition, and time-series\nanalysis. However, a comprehensive review of RetNet is still missing from the\ncurrent literature. This paper aims to fill that gap by offering the first\ndetailed survey of the RetNet architecture, its key innovations, and its\ndiverse applications. We also explore the main challenges associated with\nRetNet and propose future research directions to support its continued\nadvancement in both academic research and practical deployment.", "AI": {"tldr": "RetNet是一种新型神经网络架构，通过保留机制解决了Transformer的高内存和扩展性问题，支持线性时间推理和并行训练，并在多个领域表现出色。本文首次对RetNet进行了全面综述。", "motivation": "解决Transformer在高内存消耗和长序列处理上的局限性，提出更高效的替代方案。", "method": "引入保留机制，结合递归的归纳偏置和注意力的全局依赖建模，实现线性时间推理和并行训练。", "result": "RetNet在自然语言处理、语音识别和时间序列分析等领域表现出色。", "conclusion": "RetNet是一种有潜力的架构，但仍需进一步研究以解决其挑战并推动实际应用。"}}
{"id": "2506.06826", "pdf": "https://arxiv.org/pdf/2506.06826", "abs": "https://arxiv.org/abs/2506.06826", "authors": ["Chenfei Yuan", "Nanshan Jia", "Hangqi Li", "Peter W. Glynn", "Zeyu Zheng"], "title": "Controllable Coupled Image Generation via Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We provide an attention-level control method for the task of coupled image\ngeneration, where \"coupled\" means that multiple simultaneously generated images\nare expected to have the same or very similar backgrounds. While backgrounds\ncoupled, the centered objects in the generated images are still expected to\nenjoy the flexibility raised from different text prompts. The proposed method\ndisentangles the background and entity components in the model's\ncross-attention modules, attached with a sequence of time-varying weight\ncontrol parameters depending on the time step of sampling. We optimize this\nsequence of weight control parameters with a combined objective that assesses\nhow coupled the backgrounds are as well as text-to-image alignment and overall\nvisual quality. Empirical results demonstrate that our method outperforms\nexisting approaches across these criteria.", "AI": {"tldr": "提出了一种注意力级别控制方法，用于耦合图像生成任务，确保生成的图像背景相似，同时中心对象根据文本提示灵活变化。", "motivation": "解决多图像生成中背景耦合与对象灵活性的平衡问题。", "method": "通过解耦背景和实体组件，结合时间变化的权重控制参数，优化背景耦合、文本对齐和视觉质量。", "result": "实验表明，该方法在背景耦合、文本对齐和视觉质量上优于现有方法。", "conclusion": "该方法有效实现了背景耦合与对象灵活性的平衡，性能优越。"}}
{"id": "2506.06981", "pdf": "https://arxiv.org/pdf/2506.06981", "abs": "https://arxiv.org/abs/2506.06981", "authors": ["Riley Simmons-Edler", "Ryan P. Badman", "Felix Baastad Berg", "Raymond Chua", "John J. Vastola", "Joshua Lunger", "William Qian", "Kanaka Rajan"], "title": "Deep RL Needs Deep Behavior Analysis: Exploring Implicit Planning by Model-Free Agents in Open-Ended Environments", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Understanding the behavior of deep reinforcement learning (DRL) agents --\nparticularly as task and agent sophistication increase -- requires more than\nsimple comparison of reward curves, yet standard methods for behavioral\nanalysis remain underdeveloped in DRL. We apply tools from neuroscience and\nethology to study DRL agents in a novel, complex, partially observable\nenvironment, ForageWorld, designed to capture key aspects of real-world animal\nforaging -- including sparse, depleting resource patches, predator threats, and\nspatially extended arenas. We use this environment as a platform for applying\njoint behavioral and neural analysis to agents, revealing detailed,\nquantitatively grounded insights into agent strategies, memory, and planning.\nContrary to common assumptions, we find that model-free RNN-based DRL agents\ncan exhibit structured, planning-like behavior purely through emergent dynamics\n-- without requiring explicit memory modules or world models. Our results show\nthat studying DRL agents like animals -- analyzing them with\nneuroethology-inspired tools that reveal structure in both behavior and neural\ndynamics -- uncovers rich structure in their learning dynamics that would\notherwise remain invisible. We distill these tools into a general analysis\nframework linking core behavioral and representational features to diagnostic\nmethods, which can be reused for a wide range of tasks and agents. As agents\ngrow more complex and autonomous, bridging neuroscience, cognitive science, and\nAI will be essential -- not just for understanding their behavior, but for\nensuring safe alignment and maximizing desirable behaviors that are hard to\nmeasure via reward. We show how this can be done by drawing on lessons from how\nbiological intelligence is studied.", "AI": {"tldr": "论文提出了一种结合神经科学和动物行为学工具的方法，用于分析深度强化学习（DRL）代理在复杂环境中的行为，揭示了其策略、记忆和规划的细节。", "motivation": "当前DRL代理的行为分析方法不足，尤其是在任务和代理复杂性增加时，需要更深入的工具来理解其行为。", "method": "在ForageWorld环境中，应用神经科学和动物行为学的工具，对DRL代理进行行为和神经联合分析。", "result": "发现基于RNN的无模型DRL代理可以通过涌现动态表现出类似规划的行为，无需显式记忆模块或世界模型。", "conclusion": "通过借鉴生物智能研究方法，可以揭示DRL代理学习动态中的丰富结构，为未来复杂代理的行为分析和安全对齐提供框架。"}}
{"id": "2506.06737", "pdf": "https://arxiv.org/pdf/2506.06737", "abs": "https://arxiv.org/abs/2506.06737", "authors": ["Qi Shi", "Qiwei Han", "Cláudia Soares"], "title": "C-PATH: Conversational Patient Assistance and Triage in Healthcare System", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted in IEEE ICDH 2025, 10 pages, 8 figures, 5 tables", "summary": "Navigating healthcare systems can be complex and overwhelming, creating\nbarriers for patients seeking timely and appropriate medical attention. In this\npaper, we introduce C-PATH (Conversational Patient Assistance and Triage in\nHealthcare), a novel conversational AI system powered by large language models\n(LLMs) designed to assist patients in recognizing symptoms and recommending\nappropriate medical departments through natural, multi-turn dialogues. C-PATH\nis fine-tuned on medical knowledge, dialogue data, and clinical summaries using\na multi-stage pipeline built on the LLaMA3 architecture. A core contribution of\nthis work is a GPT-based data augmentation framework that transforms structured\nclinical knowledge from DDXPlus into lay-person-friendly conversations,\nallowing alignment with patient communication norms. We also implement a\nscalable conversation history management strategy to ensure long-range\ncoherence. Evaluation with GPTScore demonstrates strong performance across\ndimensions such as clarity, informativeness, and recommendation accuracy.\nQuantitative benchmarks show that C-PATH achieves superior performance in\nGPT-rewritten conversational datasets, significantly outperforming\ndomain-specific baselines. C-PATH represents a step forward in the development\nof user-centric, accessible, and accurate AI tools for digital health\nassistance and triage.", "AI": {"tldr": "C-PATH是一种基于LLM的对话AI系统，旨在通过自然对话帮助患者识别症状并推荐医疗科室。", "motivation": "医疗系统复杂且难以导航，患者难以及时获得适当医疗帮助。", "method": "基于LLaMA3架构的多阶段管道，结合医学知识、对话数据和临床摘要，采用GPT数据增强框架和可扩展对话历史管理策略。", "result": "在清晰度、信息量和推荐准确性方面表现优异，显著优于领域基线。", "conclusion": "C-PATH是面向用户的、准确且易用的数字健康辅助工具的重要进展。"}}
{"id": "2506.06830", "pdf": "https://arxiv.org/pdf/2506.06830", "abs": "https://arxiv.org/abs/2506.06830", "authors": ["Guankun Wang", "Rui Tang", "Mengya Xu", "Long Bai", "Huxin Gao", "Hongliang Ren"], "title": "EndoARSS: Adapting Spatially-Aware Foundation Model for Efficient Activity Recognition and Semantic Segmentation in Endoscopic Surgery", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted by Advanced Intelligent Systems", "summary": "Endoscopic surgery is the gold standard for robotic-assisted minimally\ninvasive surgery, offering significant advantages in early disease detection\nand precise interventions. However, the complexity of surgical scenes,\ncharacterized by high variability in different surgical activity scenarios and\nconfused image features between targets and the background, presents challenges\nfor surgical environment understanding. Traditional deep learning models often\nstruggle with cross-activity interference, leading to suboptimal performance in\neach downstream task. To address this limitation, we explore multi-task\nlearning, which utilizes the interrelated features between tasks to enhance\noverall task performance. In this paper, we propose EndoARSS, a novel\nmulti-task learning framework specifically designed for endoscopy surgery\nactivity recognition and semantic segmentation. Built upon the DINOv2\nfoundation model, our approach integrates Low-Rank Adaptation to facilitate\nefficient fine-tuning while incorporating Task Efficient Shared Low-Rank\nAdapters to mitigate gradient conflicts across diverse tasks. Additionally, we\nintroduce the Spatially-Aware Multi-Scale Attention that enhances feature\nrepresentation discrimination by enabling cross-spatial learning of global\ninformation. In order to evaluate the effectiveness of our framework, we\npresent three novel datasets, MTLESD, MTLEndovis and MTLEndovis-Gen, tailored\nfor endoscopic surgery scenarios with detailed annotations for both activity\nrecognition and semantic segmentation tasks. Extensive experiments demonstrate\nthat EndoARSS achieves remarkable performance across multiple benchmarks,\nsignificantly improving both accuracy and robustness in comparison to existing\nmodels. These results underscore the potential of EndoARSS to advance AI-driven\nendoscopic surgical systems, offering valuable insights for enhancing surgical\nsafety and efficiency.", "AI": {"tldr": "EndoARSS是一个基于DINOv2的多任务学习框架，用于内窥镜手术活动识别和语义分割，通过低秩适应和空间感知多尺度注意力提升性能。", "motivation": "解决传统深度学习模型在内窥镜手术场景中因跨活动干扰导致的性能不足问题。", "method": "结合低秩适应和任务高效共享低秩适配器，引入空间感知多尺度注意力机制。", "result": "在多个基准测试中表现优异，显著提升了准确性和鲁棒性。", "conclusion": "EndoARSS有望推动AI驱动的内窥镜手术系统发展，提升手术安全性和效率。"}}
{"id": "2506.06991", "pdf": "https://arxiv.org/pdf/2506.06991", "abs": "https://arxiv.org/abs/2506.06991", "authors": ["Yichi Zhang", "Jinlong Pang", "Zhaowei Zhu", "Yang Liu"], "title": "Evaluating LLM-corrupted Crowdsourcing Data Without Ground Truth", "categories": ["cs.AI", "cs.GT", "cs.HC"], "comment": "33 pages, 9 figures", "summary": "The recent success of generative AI highlights the crucial role of\nhigh-quality human feedback in building trustworthy AI systems. However, the\nincreasing use of large language models (LLMs) by crowdsourcing workers poses a\nsignificant challenge: datasets intended to reflect human input may be\ncompromised by LLM-generated responses. Existing LLM detection approaches often\nrely on high-dimension training data such as text, making them unsuitable for\nannotation tasks like multiple-choice labeling. In this work, we investigate\nthe potential of peer prediction -- a mechanism that evaluates the information\nwithin workers' responses without using ground truth -- to mitigate\nLLM-assisted cheating in crowdsourcing with a focus on annotation tasks. Our\napproach quantifies the correlations between worker answers while conditioning\non (a subset of) LLM-generated labels available to the requester. Building on\nprior research, we propose a training-free scoring mechanism with theoretical\nguarantees under a crowdsourcing model that accounts for LLM collusion. We\nestablish conditions under which our method is effective and empirically\ndemonstrate its robustness in detecting low-effort cheating on real-world\ncrowdsourcing datasets.", "AI": {"tldr": "论文探讨了如何利用同行预测机制检测和减少众包任务中LLM辅助作弊行为，特别是在标注任务中。", "motivation": "随着生成式AI的普及，众包工作者使用LLM生成回答可能导致数据集质量下降，现有检测方法不适用于标注任务。", "method": "提出一种基于同行预测的无训练评分机制，量化工人答案间的相关性，并结合LLM生成标签。", "result": "理论证明了方法的有效性，并在真实众包数据集上验证了其检测低效作弊的鲁棒性。", "conclusion": "同行预测机制在LLM辅助作弊检测中具有潜力，尤其在标注任务中表现优异。"}}
{"id": "2506.06751", "pdf": "https://arxiv.org/pdf/2506.06751", "abs": "https://arxiv.org/abs/2506.06751", "authors": ["Mikhail Salnikov", "Dmitrii Korzh", "Ivan Lazichny", "Elvir Karimov", "Artyom Iudin", "Ivan Oseledets", "Oleg Y. Rogov", "Alexander Panchenko", "Natalia Loukachevitch", "Elena Tutubalina"], "title": "Geopolitical biases in LLMs: what are the \"good\" and the \"bad\" countries according to contemporary language models", "categories": ["cs.CL"], "comment": null, "summary": "This paper evaluates geopolitical biases in LLMs with respect to various\ncountries though an analysis of their interpretation of historical events with\nconflicting national perspectives (USA, UK, USSR, and China). We introduce a\nnovel dataset with neutral event descriptions and contrasting viewpoints from\ndifferent countries. Our findings show significant geopolitical biases, with\nmodels favoring specific national narratives. Additionally, simple debiasing\nprompts had a limited effect in reducing these biases. Experiments with\nmanipulated participant labels reveal models' sensitivity to attribution,\nsometimes amplifying biases or recognizing inconsistencies, especially with\nswapped labels. This work highlights national narrative biases in LLMs,\nchallenges the effectiveness of simple debiasing methods, and offers a\nframework and dataset for future geopolitical bias research.", "AI": {"tldr": "论文通过分析LLMs对不同国家历史事件的解释，评估了其地缘政治偏见，发现模型倾向于特定国家叙事，简单去偏见提示效果有限。", "motivation": "研究LLMs在处理具有冲突国家视角的历史事件时是否存在地缘政治偏见。", "method": "引入包含中性事件描述和不同国家对立观点的新数据集，并通过实验评估模型偏见。", "result": "发现LLMs存在显著地缘政治偏见，简单去偏见方法效果有限，模型对标签敏感。", "conclusion": "揭示了LLMs的国家叙事偏见，挑战了简单去偏见方法的有效性，为未来研究提供了框架和数据集。"}}
{"id": "2506.06836", "pdf": "https://arxiv.org/pdf/2506.06836", "abs": "https://arxiv.org/abs/2506.06836", "authors": ["Zelin He", "Sarah Alnegheimish", "Matthew Reimherr"], "title": "Harnessing Vision-Language Models for Time Series Anomaly Detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Time-series anomaly detection (TSAD) has played a vital role in a variety of\nfields, including healthcare, finance, and industrial monitoring. Prior\nmethods, which mainly focus on training domain-specific models on numerical\ndata, lack the visual-temporal reasoning capacity that human experts have to\nidentify contextual anomalies. To fill this gap, we explore a solution based on\nvision language models (VLMs). Recent studies have shown the ability of VLMs\nfor visual reasoning tasks, yet their direct application to time series has\nfallen short on both accuracy and efficiency. To harness the power of VLMs for\nTSAD, we propose a two-stage solution, with (1) ViT4TS, a vision-screening\nstage built on a relatively lightweight pretrained vision encoder, which\nleverages 2-D time-series representations to accurately localize candidate\nanomalies; (2) VLM4TS, a VLM-based stage that integrates global temporal\ncontext and VLM reasoning capacity to refine the detection upon the candidates\nprovided by ViT4TS. We show that without any time-series training, VLM4TS\noutperforms time-series pretrained and from-scratch baselines in most cases,\nyielding a 24.6 percent improvement in F1-max score over the best baseline.\nMoreover, VLM4TS also consistently outperforms existing language-model-based\nTSAD methods and is on average 36 times more efficient in token usage.", "AI": {"tldr": "论文提出了一种基于视觉语言模型（VLMs）的两阶段时间序列异常检测方法（ViT4TS和VLM4TS），显著提升了检测准确性和效率。", "motivation": "现有方法缺乏视觉-时间推理能力，无法像人类专家那样识别上下文异常，因此探索了基于VLMs的解决方案。", "method": "采用两阶段方法：ViT4TS用于初步定位候选异常，VLM4TS结合全局时间上下文和VLM推理能力进行精细化检测。", "result": "VLM4TS在无需时间序列训练的情况下，F1-max分数比最佳基线提高了24.6%，且效率更高。", "conclusion": "该方法在时间序列异常检测中表现出色，为领域提供了新的解决方案。"}}
{"id": "2506.07047", "pdf": "https://arxiv.org/pdf/2506.07047", "abs": "https://arxiv.org/abs/2506.07047", "authors": ["Yu Xuejun", "Jianyuan Zhong", "Zijin Feng", "Pengyi Zhai", "Roozbeh Yousefzadeh", "Wei Chong Ng", "Haoxiong Liu", "Ziyi Shou", "Jing Xiong", "Yudong Zhou", "Claudia Beth Ong", "Austen Jeremy Sugiarto", "Yaoxi Zhang", "Wai Ming Tai", "Huan Cao", "Dongcai Lu", "Jiacheng Sun", "Qiang Xu", "Shen Xin", "Zhenguo Li"], "title": "Mathesis: Towards Formal Theorem Proving from Natural Languages", "categories": ["cs.AI"], "comment": null, "summary": "Recent advances in large language models show strong promise for formal\nreasoning. However, most LLM-based theorem provers have long been constrained\nby the need for expert-written formal statements as inputs, limiting their\napplicability to real-world problems expressed in natural language. We tackle\nthis gap with Mathesis, the first end-to-end theorem proving pipeline\nprocessing informal problem statements. It contributes Mathesis-Autoformalizer,\nthe first autoformalizer using reinforcement learning to enhance the\nformalization ability of natural language problems, aided by our novel\nLeanScorer framework for nuanced formalization quality assessment. It also\nproposes a Mathesis-Prover, which generates formal proofs from the formalized\nstatements. To evaluate the real-world applicability of end-to-end formal\ntheorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex\nproblems from China's national college entrance exam. Our approach is carefully\ndesigned, with a thorough study of each component. Experiments demonstrate\nMathesis's effectiveness, with the autoformalizer outperforming the best\nbaseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other\nmodel combinations, achieving 64% accuracy on MiniF2F with pass@32 and a\nstate-of-the-art 18% on Gaokao-Formal.", "AI": {"tldr": "Mathesis是一个端到端的定理证明系统，通过自动形式化自然语言问题并结合强化学习，显著提升了形式化推理的能力，并在多个基准测试中表现出色。", "motivation": "解决现有基于大语言模型的定理证明器需要专家编写的正式语句输入的问题，扩展其在实际自然语言问题中的应用。", "method": "提出Mathesis-Autoformalizer（基于强化学习的自动形式化工具）和Mathesis-Prover（生成形式化证明），并引入LeanScorer框架评估形式化质量。", "result": "在Gaokao-Formal基准测试中，自动形式化工具的通过率比最佳基线高22%，全系统在MiniF2F和Gaokao-Formal上分别达到64%和18%的准确率。", "conclusion": "Mathesis系统通过端到端的形式化定理证明，显著提升了处理自然语言问题的能力，具有实际应用潜力。"}}
{"id": "2506.06775", "pdf": "https://arxiv.org/pdf/2506.06775", "abs": "https://arxiv.org/abs/2506.06775", "authors": ["Walter Paci", "Alessandro Panunzi", "Sandro Pezzelle"], "title": "They want to pretend not to understand: The Limits of Current LLMs in Interpreting Implicit Content of Political Discourse", "categories": ["cs.CL"], "comment": "Accepted to the ACL2025 Findings", "summary": "Implicit content plays a crucial role in political discourse, where speakers\nsystematically employ pragmatic strategies such as implicatures and\npresuppositions to influence their audiences. Large Language Models (LLMs) have\ndemonstrated strong performance in tasks requiring complex semantic and\npragmatic understanding, highlighting their potential for detecting and\nexplaining the meaning of implicit content. However, their ability to do this\nwithin political discourse remains largely underexplored. Leveraging, for the\nfirst time, the large IMPAQTS corpus, which comprises Italian political\nspeeches with the annotation of manipulative implicit content, we propose\nmethods to test the effectiveness of LLMs in this challenging problem. Through\na multiple-choice task and an open-ended generation task, we demonstrate that\nall tested models struggle to interpret presuppositions and implicatures. We\nconclude that current LLMs lack the key pragmatic capabilities necessary for\naccurately interpreting highly implicit language, such as that found in\npolitical discourse. At the same time, we highlight promising trends and future\ndirections for enhancing model performance. We release our data and code at\nhttps://github.com/WalterPaci/IMPAQTS-PID", "AI": {"tldr": "论文探讨了大型语言模型（LLMs）在政治话语中检测和解释隐含内容的能力，发现当前模型在理解预设和隐含意义方面表现不佳，但提出了未来改进方向。", "motivation": "政治话语中隐含内容对影响受众至关重要，但LLMs在此领域的表现尚未充分研究。", "method": "利用IMPAQTS语料库，通过选择题和开放式生成任务测试LLMs的能力。", "result": "所有测试模型在理解预设和隐含意义方面均表现不佳。", "conclusion": "当前LLMs缺乏准确解释高度隐含语言的关键语用能力，但未来改进方向值得关注。"}}
{"id": "2506.06846", "pdf": "https://arxiv.org/pdf/2506.06846", "abs": "https://arxiv.org/abs/2506.06846", "authors": ["Yangkai Lin", "Jiabao Lei", "Kui jia"], "title": "Multi-StyleGS: Stylizing Gaussian Splatting with Multiple Styles", "categories": ["cs.CV"], "comment": "AAAI 2025", "summary": "In recent years, there has been a growing demand to stylize a given 3D scene\nto align with the artistic style of reference images for creative purposes.\nWhile 3D Gaussian Splatting(GS) has emerged as a promising and efficient method\nfor realistic 3D scene modeling, there remains a challenge in adapting it to\nstylize 3D GS to match with multiple styles through automatic local style\ntransfer or manual designation, while maintaining memory efficiency for\nstylization training. In this paper, we introduce a novel 3D GS stylization\nsolution termed Multi-StyleGS to tackle these challenges. In particular, we\nemploy a bipartite matching mechanism to au tomatically identify\ncorrespondences between the style images and the local regions of the rendered\nimages. To facilitate local style transfer, we introduce a novel semantic style\nloss function that employs a segmentation network to apply distinct styles to\nvarious objects of the scene and propose a local-global feature matching to\nenhance the multi-view consistency. Furthermore, this technique can achieve\nmemory efficient training, more texture details and better color match. To\nbetter assign a robust semantic label to each Gaussian, we propose several\ntechniques to regularize the segmentation network. As demonstrated by our\ncomprehensive experiments, our approach outperforms existing ones in producing\nplausible stylization results and offering flexible editing.", "AI": {"tldr": "本文提出了一种名为Multi-StyleGS的新方法，用于解决3D高斯泼溅（GS）在多风格适配中的挑战，通过自动局部风格转移和语义风格损失函数实现高效训练和灵活编辑。", "motivation": "近年来，对3D场景进行艺术风格化的需求增长，但现有方法在适配多风格、保持内存效率和训练一致性方面存在挑战。", "method": "采用二分匹配机制自动匹配风格图像与渲染图像的局部区域，引入语义风格损失函数和局部-全局特征匹配技术，优化分割网络以增强多视角一致性。", "result": "实验表明，该方法在风格化效果、内存效率和颜色匹配方面优于现有方法，并能实现灵活的编辑。", "conclusion": "Multi-StyleGS通过创新的局部风格转移和语义分割技术，有效解决了3D GS风格化的挑战，为创意应用提供了高效且灵活的解决方案。"}}
{"id": "2506.07075", "pdf": "https://arxiv.org/pdf/2506.07075", "abs": "https://arxiv.org/abs/2506.07075", "authors": ["Liwen Zheng", "Chaozhuo Li", "Haoran Jia", "Xi Zhang"], "title": "Reasoning Paths as Signals: Augmenting Multi-hop Fact Verification through Structural Reasoning Progression", "categories": ["cs.AI"], "comment": null, "summary": "The growing complexity of factual claims in real-world scenarios presents\nsignificant challenges for automated fact verification systems, particularly in\naccurately aggregating and reasoning over multi-hop evidence. Existing\napproaches often rely on static or shallow models that fail to capture the\nevolving structure of reasoning paths, leading to fragmented retrieval and\nlimited interpretability. To address these issues, we propose a Structural\nReasoning framework for Multi-hop Fact Verification that explicitly models\nreasoning paths as structured graphs throughout both evidence retrieval and\nclaim verification stages. Our method comprises two key modules: a\nstructure-enhanced retrieval mechanism that constructs reasoning graphs to\nguide evidence collection, and a reasoning-path-guided verification module that\nincrementally builds subgraphs to represent evolving inference trajectories. We\nfurther incorporate a structure-aware reasoning mechanism that captures\nlong-range dependencies across multi-hop evidence chains, enabling more precise\nverification. Extensive experiments on the FEVER and HoVer datasets demonstrate\nthat our approach consistently outperforms strong baselines, highlighting the\neffectiveness of reasoning-path modeling in enhancing retrieval precision and\nverification accuracy.", "AI": {"tldr": "提出了一种结构化推理框架，用于多跳事实验证，通过显式建模推理路径为结构化图，显著提升了检索精度和验证准确性。", "motivation": "现实场景中事实声明的复杂性增加，现有方法因静态或浅层模型无法捕捉推理路径的动态结构，导致检索碎片化和解释性不足。", "method": "提出结构化推理框架，包含结构增强的检索机制和推理路径引导的验证模块，结合结构感知推理机制捕捉多跳证据链的长程依赖。", "result": "在FEVER和HoVer数据集上的实验表明，该方法显著优于基线模型，验证了推理路径建模的有效性。", "conclusion": "结构化推理框架通过显式建模推理路径，有效提升了多跳事实验证的性能和解释性。"}}
{"id": "2506.06785", "pdf": "https://arxiv.org/pdf/2506.06785", "abs": "https://arxiv.org/abs/2506.06785", "authors": ["Hiram Ring"], "title": "Extending dependencies to the taggedPBC: Word order in transitive clauses", "categories": ["cs.CL"], "comment": null, "summary": "The taggedPBC (Ring 2025a) contains more than 1,800 sentences of pos-tagged\nparallel text data from over 1,500 languages, representing 133 language\nfamilies and 111 isolates. While this dwarfs previously available resources,\nand the POS tags achieve decent accuracy, allowing for predictive\ncrosslinguistic insights (Ring 2025b), the dataset was not initially annotated\nfor dependencies. This paper reports on a CoNLLU-formatted version of the\ndataset which transfers dependency information along with POS tags to all\nlanguages in the taggedPBC. Although there are various concerns regarding the\nquality of the tags and the dependencies, word order information derived from\nthis dataset regarding the position of arguments and predicates in transitive\nclauses correlates with expert determinations of word order in three\ntypological databases (WALS, Grambank, Autotyp). This highlights the usefulness\nof corpus-based typological approaches (as per Baylor et al. 2023; Bjerva 2024)\nfor extending comparisons of discrete linguistic categories, and suggests that\nimportant insights can be gained even from noisy data, given sufficient\nannotation. The dependency-annotated corpora are also made available for\nresearch and collaboration via GitHub.", "AI": {"tldr": "论文介绍了一个依赖标注的并行文本数据集taggedPBC，扩展了原有POS标注数据，并验证了其与专家标注的语序一致性。", "motivation": "taggedPBC数据集虽包含大量语言的POS标注数据，但缺乏依赖标注信息，限制了其在跨语言研究中的应用。", "method": "通过CoNLLU格式将依赖信息与POS标签一起迁移到taggedPBC的所有语言中。", "result": "尽管标注质量存在一些问题，但数据集中的语序信息与专家标注的语序数据库（WALS等）一致，验证了其有效性。", "conclusion": "依赖标注的taggedPBC数据集为基于语料库的类型学研究提供了新资源，表明即使数据噪声较大，也能通过充分标注获得重要见解。"}}
{"id": "2506.06850", "pdf": "https://arxiv.org/pdf/2506.06850", "abs": "https://arxiv.org/abs/2506.06850", "authors": ["Sara M. Cerqueira", "Manuel Palermo", "Cristina P. Santos"], "title": "Deep Inertial Pose: A deep learning approach for human pose estimation", "categories": ["cs.CV", "eess.SP"], "comment": null, "summary": "Inertial-based Motion capture system has been attracting growing attention\ndue to its wearability and unsconstrained use. However, accurate human joint\nestimation demands several complex and expertise demanding steps, which leads\nto expensive software such as the state-of-the-art MVN Awinda from Xsens\nTechnologies. This work aims to study the use of Neural Networks to abstract\nthe complex biomechanical models and analytical mathematics required for pose\nestimation. Thus, it presents a comparison of different Neural Network\narchitectures and methodologies to understand how accurately these methods can\nestimate human pose, using both low cost(MPU9250) and high end (Mtw Awinda)\nMagnetic, Angular Rate, and Gravity (MARG) sensors. The most efficient method\nwas the Hybrid LSTM-Madgwick detached, which achieved an Quaternion Angle\ndistance error of 7.96, using Mtw Awinda data. Also, an ablation study was\nconducted to study the impact of data augmentation, output representation,\nwindow size, loss function and magnetometer data on the pose estimation error.\nThis work indicates that Neural Networks can be trained to estimate human pose,\nwith results comparable to the state-of-the-art fusion filters.", "AI": {"tldr": "该论文研究了利用神经网络简化惯性运动捕捉系统中复杂生物力学模型的方法，比较了不同架构和方法，发现Hybrid LSTM-Madgwick方法效果最佳。", "motivation": "惯性运动捕捉系统因其穿戴性和无约束性受到关注，但现有方法复杂且昂贵，论文旨在探索神经网络替代方案。", "method": "比较了不同神经网络架构和方法，使用低成本和高端的MARG传感器数据，并进行了消融研究。", "result": "Hybrid LSTM-Madgwick方法表现最佳，误差为7.96，结果与现有融合滤波器相当。", "conclusion": "神经网络可以替代复杂模型进行姿态估计，效果接近现有先进方法。"}}
{"id": "2506.07116", "pdf": "https://arxiv.org/pdf/2506.07116", "abs": "https://arxiv.org/abs/2506.07116", "authors": ["Liyang Chen", "Yujun Cai", "Jieqiong Dong", "Yiwei Wang"], "title": "BRIGHT+: Upgrading the BRIGHT Benchmark with MARCUS, a Multi-Agent RAG Clean-Up Suite", "categories": ["cs.AI"], "comment": "8 pages, 7 figures, 4 tables. Submitted to EMNLP 2025", "summary": "Retrieval-Augmented Generation (RAG) systems require corpora that are both\nstructurally clean and semantically coherent. BRIGHT is a recent and\ninfluential benchmark designed to evaluate complex multi-hop retrieval across\ndiverse, high-reasoning domains. However, its practical effectiveness is\nlimited by common web-crawled artifacts - such as content redundancy and\nsemantic discontinuity - that impair retrieval accuracy and downstream\nreasoning. Notably, we find that such issues are concentrated in seven\nStackExchange-derived subdomains, while other domains (e.g., Coding and\nTheorem-based content) remain relatively clean.\n  In this study, we present MARCUS, a multi-agent pipeline that leverages large\nlanguage models (LLMs) to systematically clean and re-chunk BRIGHT into a\nhigher-quality corpus: BRIGHT-Plus. MARCUS applies dedicated agents for\nstructural noise removal and semantic segmentation, preserving answer-bearing\nspans while improving contextual integrity. Experimental evaluations\ndemonstrate that BRIGHT-Plus yields consistent and significant improvements in\nboth retrieval accuracy and multi-hop reasoning across a diverse set of\nretrievers. We release both the BRIGHT-Plus corpus and the MARCUS pipeline to\nsupport future research on robust, reasoning-centric retrieval.", "AI": {"tldr": "MARCUS是一个多智能体管道，利用大语言模型（LLMs）清理和重组BRIGHT基准，生成更高质量的BRIGHT-Plus语料库，显著提升检索准确性和多跳推理能力。", "motivation": "BRIGHT基准因网络爬取内容中的冗余和语义不连贯问题，影响了检索准确性和下游推理效果，尤其是在某些StackExchange子领域。", "method": "MARCUS采用多智能体管道，分别负责结构噪声去除和语义分割，保留答案相关片段并提升上下文完整性。", "result": "实验表明，BRIGHT-Plus在多种检索器中显著提升了检索准确性和多跳推理能力。", "conclusion": "BRIGHT-Plus和MARCUS管道的发布为未来研究提供了更稳健的推理中心检索支持。"}}
{"id": "2506.06800", "pdf": "https://arxiv.org/pdf/2506.06800", "abs": "https://arxiv.org/abs/2506.06800", "authors": ["Tianjie Ju", "Yujia Chen", "Hao Fei", "Mong-Li Lee", "Wynne Hsu", "Pengzhou Cheng", "Zongru Wu", "Zhuosheng Zhang", "Gongshen Liu"], "title": "On the Adaptive Psychological Persuasion of Large Language Models", "categories": ["cs.CL"], "comment": "Working in progress", "summary": "Previous work has showcased the intriguing capabilities of Large Language\nModels (LLMs) in instruction-following and rhetorical fluency. However,\nsystematic exploration of their dual capabilities to autonomously persuade and\nresist persuasion, particularly in contexts involving psychological rhetoric,\nremains unexplored. In this paper, we first evaluate four commonly adopted LLMs\nby tasking them to alternately act as persuaders and listeners in adversarial\ndialogues. Empirical results show that persuader LLMs predominantly employ\nrepetitive strategies, leading to low success rates. Then we introduce eleven\ncomprehensive psychological persuasion strategies, finding that explicitly\ninstructing LLMs to adopt specific strategies such as Fluency Effect and\nRepetition Effect significantly improves persuasion success rates. However, no\n``one-size-fits-all'' strategy proves universally effective, with performance\nheavily dependent on contextual counterfactuals. Motivated by these\nobservations, we propose an adaptive framework based on direct preference\noptimization that trains LLMs to autonomously select optimal strategies by\nleveraging persuasion results from strategy-specific responses as preference\npairs. Experiments on three open-source LLMs confirm that the proposed adaptive\npsychological persuasion method effectively enables persuader LLMs to select\noptimal strategies, significantly enhancing their success rates while\nmaintaining general capabilities. Our code is available at\nhttps://github.com/KalinaEine/PsychologicalPersuasion.", "AI": {"tldr": "论文探讨了大型语言模型（LLMs）在自主说服和抵抗说服中的双重能力，提出了基于心理策略的自适应框架，显著提升了说服成功率。", "motivation": "系统研究LLMs在心理修辞情境下的说服与抵抗能力，填补现有研究的空白。", "method": "评估四种LLMs在对抗对话中的表现，引入11种心理说服策略，并提出基于直接偏好优化的自适应框架。", "result": "实验表明，特定策略（如流畅效应和重复效应）显著提升说服成功率，自适应框架进一步优化策略选择。", "conclusion": "自适应心理说服方法有效提升LLMs的说服能力，同时保持其通用性能。"}}
{"id": "2506.06852", "pdf": "https://arxiv.org/pdf/2506.06852", "abs": "https://arxiv.org/abs/2506.06852", "authors": ["John Waithaka", "Moise Busogi"], "title": "Position Prediction Self-Supervised Learning for Multimodal Satellite Imagery Semantic Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Semantic segmentation of satellite imagery is crucial for Earth observation\napplications, but remains constrained by limited labelled training data. While\nself-supervised pretraining methods like Masked Autoencoders (MAE) have shown\npromise, they focus on reconstruction rather than localisation-a fundamental\naspect of segmentation tasks. We propose adapting LOCA (Location-aware), a\nposition prediction self-supervised learning method, for multimodal satellite\nimagery semantic segmentation. Our approach addresses the unique challenges of\nsatellite data by extending SatMAE's channel grouping from multispectral to\nmultimodal data, enabling effective handling of multiple modalities, and\nintroducing same-group attention masking to encourage cross-modal interaction\nduring pretraining. The method uses relative patch position prediction,\nencouraging spatial reasoning for localisation rather than reconstruction. We\nevaluate our approach on the Sen1Floods11 flood mapping dataset, where it\nsignificantly outperforms existing reconstruction-based self-supervised\nlearning methods for satellite imagery. Our results demonstrate that position\nprediction tasks, when properly adapted for multimodal satellite imagery, learn\nrepresentations more effective for satellite image semantic segmentation than\nreconstruction-based approaches.", "AI": {"tldr": "论文提出了一种基于位置预测的自监督学习方法（LOCA），用于多模态卫星图像的语义分割，显著优于现有的基于重建的自监督学习方法。", "motivation": "卫星图像的语义分割对地球观测应用至关重要，但受限于标记训练数据的不足。现有的自监督预训练方法（如MAE）侧重于重建而非定位，而定位是分割任务的基本要求。", "method": "通过扩展SatMAE的通道分组方法，将多光谱数据扩展到多模态数据，并引入同组注意力掩码以促进预训练期间的跨模态交互。使用相对补丁位置预测任务，鼓励空间推理而非重建。", "result": "在Sen1Floods11洪水映射数据集上，该方法显著优于现有的基于重建的自监督学习方法。", "conclusion": "研究表明，针对多模态卫星图像适配的位置预测任务，比基于重建的方法更能学习到适用于卫星图像语义分割的有效表示。"}}
{"id": "2506.07173", "pdf": "https://arxiv.org/pdf/2506.07173", "abs": "https://arxiv.org/abs/2506.07173", "authors": ["Miroslav Popovic", "Marko Popovic", "Miodrag Djukic", "Ilija Basicevic"], "title": "Translating Federated Learning Algorithms in Python into CSP Processes Using ChatGPT", "categories": ["cs.AI"], "comment": "6 pages, 4 tables", "summary": "The Python Testbed for Federated Learning Algorithms is a simple Python FL\nframework that is easy to use by ML&AI developers who do not need to be\nprofessional programmers and is also amenable to LLMs. In the previous\nresearch, generic federated learning algorithms provided by this framework were\nmanually translated into the CSP processes and algorithms' safety and liveness\nproperties were automatically verified by the model checker PAT. In this paper,\na simple translation process is introduced wherein the ChatGPT is used to\nautomate the translation of the mentioned federated learning algorithms in\nPython into the corresponding CSP processes. Within the process, the minimality\nof the used context is estimated based on the feedback from ChatGPT. The\nproposed translation process was experimentally validated by successful\ntranslation (verified by the model checker PAT) of both generic centralized and\ndecentralized federated learning algorithms.", "AI": {"tldr": "本文介绍了一种利用ChatGPT自动化将Python联邦学习算法翻译为CSP过程的方法，并通过模型检查器PAT验证其正确性。", "motivation": "为简化联邦学习算法的形式化验证过程，减少手动翻译的工作量，提高效率。", "method": "使用ChatGPT自动翻译Python联邦学习算法为CSP过程，并通过PAT验证其安全性和活性。", "result": "实验验证了该方法能成功翻译并验证集中式和分散式联邦学习算法。", "conclusion": "提出的自动化翻译方法有效，为联邦学习算法的形式化验证提供了新思路。"}}
{"id": "2506.06806", "pdf": "https://arxiv.org/pdf/2506.06806", "abs": "https://arxiv.org/abs/2506.06806", "authors": ["Subhendu Khatuya", "Shashwat Naidu", "Saptarshi Ghosh", "Pawan Goyal", "Niloy Ganguly"], "title": "Label-semantics Aware Generative Approach for Domain-Agnostic Multilabel Classification", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "This work has been accepted to appear at the Association for\n  Computational Linguistics (ACL), 2025", "summary": "The explosion of textual data has made manual document classification\nincreasingly challenging. To address this, we introduce a robust, efficient\ndomain-agnostic generative model framework for multi-label text classification.\nInstead of treating labels as mere atomic symbols, our approach utilizes\npredefined label descriptions and is trained to generate these descriptions\nbased on the input text. During inference, the generated descriptions are\nmatched to the pre-defined labels using a finetuned sentence transformer. We\nintegrate this with a dual-objective loss function, combining cross-entropy\nloss and cosine similarity of the generated sentences with the predefined\ntarget descriptions, ensuring both semantic alignment and accuracy. Our\nproposed model LAGAMC stands out for its parameter efficiency and versatility\nacross diverse datasets, making it well-suited for practical applications. We\ndemonstrate the effectiveness of our proposed model by achieving new\nstate-of-the-art performances across all evaluated datasets, surpassing several\nstrong baselines. We achieve improvements of 13.94% in Micro-F1 and 24.85% in\nMacro-F1 compared to the closest baseline across all datasets.", "AI": {"tldr": "提出了一种基于生成模型的多标签文本分类框架LAGAMC，通过生成标签描述并匹配预定义标签，结合双目标损失函数，实现了高效且通用的分类性能。", "motivation": "解决文本数据爆炸导致的手动分类困难，提出一种领域无关的生成模型框架。", "method": "利用预定义标签描述生成模型，结合双目标损失函数（交叉熵损失和余弦相似度），通过微调的句子转换器匹配生成描述与预定义标签。", "result": "在多个数据集上达到新的最优性能，Micro-F1提升13.94%，Macro-F1提升24.85%。", "conclusion": "LAGAMC模型在参数效率和通用性上表现优异，适用于实际应用。"}}
{"id": "2506.06854", "pdf": "https://arxiv.org/pdf/2506.06854", "abs": "https://arxiv.org/abs/2506.06854", "authors": ["Markus Knoche", "Daan de Geus", "Bastian Leibe"], "title": "DONUT: A Decoder-Only Model for Trajectory Prediction", "categories": ["cs.CV"], "comment": null, "summary": "Predicting the motion of other agents in a scene is highly relevant for\nautonomous driving, as it allows a self-driving car to anticipate. Inspired by\nthe success of decoder-only models for language modeling, we propose DONUT, a\nDecoder-Only Network for Unrolling Trajectories. Different from existing\nencoder-decoder forecasting models, we encode historical trajectories and\npredict future trajectories with a single autoregressive model. This allows the\nmodel to make iterative predictions in a consistent manner, and ensures that\nthe model is always provided with up-to-date information, enhancing the\nperformance. Furthermore, inspired by multi-token prediction for language\nmodeling, we introduce an 'overprediction' strategy that gives the network the\nauxiliary task of predicting trajectories at longer temporal horizons. This\nallows the model to better anticipate the future, and further improves the\nperformance. With experiments, we demonstrate that our decoder-only approach\noutperforms the encoder-decoder baseline, and achieves new state-of-the-art\nresults on the Argoverse 2 single-agent motion forecasting benchmark.", "AI": {"tldr": "提出了一种仅解码器模型DONUT，用于预测轨迹，通过自回归方式迭代预测未来轨迹，并引入‘超预测’策略提升性能。", "motivation": "自动驾驶需要预测其他代理的运动，现有编码器-解码器模型存在不足，因此提出仅解码器模型以提升性能。", "method": "使用单一自回归模型编码历史轨迹并预测未来轨迹，引入‘超预测’策略预测更长时间范围的轨迹。", "result": "在Argoverse 2单代理运动预测基准上，DONUT优于编码器-解码器基线，达到新SOTA。", "conclusion": "仅解码器模型DONUT在轨迹预测任务中表现优异，为自动驾驶提供了更高效的解决方案。"}}
{"id": "2506.07184", "pdf": "https://arxiv.org/pdf/2506.07184", "abs": "https://arxiv.org/abs/2506.07184", "authors": ["Liangliang You", "Junchi Yao", "Shu Yang", "Guimin Hu", "Lijie Hu", "Di Wang"], "title": "Mitigating Behavioral Hallucination in Multimodal Large Language Models for Sequential Images", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "While multimodal large language models excel at various tasks, they still\nsuffer from hallucinations, which limit their reliability and scalability for\nbroader domain applications. To address this issue, recent research mainly\nfocuses on objective hallucination. However, for sequential images, besides\nobjective hallucination, there is also behavioral hallucination, which is less\nstudied. This work aims to fill in the gap. We first reveal that behavioral\nhallucinations mainly arise from two key factors: prior-driven bias and the\nsnowball effect. Based on these observations, we introduce SHE (Sequence\nHallucination Eradication), a lightweight, two-stage framework that (1) detects\nhallucinations via visual-textual alignment check using our proposed adaptive\ntemporal window and (2) mitigates them via orthogonal projection onto the joint\nembedding space. We also propose a new metric (BEACH) to quantify behavioral\nhallucination severity. Empirical results on standard benchmarks demonstrate\nthat SHE reduces behavioral hallucination by over 10% on BEACH while\nmaintaining descriptive accuracy.", "AI": {"tldr": "论文提出SHE框架，通过两阶段方法检测和缓解行为幻觉，并引入新指标BEACH量化其严重性。", "motivation": "多模态大语言模型存在行为幻觉问题，现有研究主要关注客观幻觉，而行为幻觉研究较少。", "method": "提出SHE框架，包括基于自适应时间窗口的视觉-文本对齐检测和联合嵌入空间的正交投影缓解。", "result": "SHE在标准基准测试中减少行为幻觉超过10%，同时保持描述准确性。", "conclusion": "SHE框架有效解决了行为幻觉问题，为多模态模型可靠性提供了新方法。"}}
{"id": "2506.06808", "pdf": "https://arxiv.org/pdf/2506.06808", "abs": "https://arxiv.org/abs/2506.06808", "authors": ["James A. Michaelov", "Reeka Estacio", "Zhien Zhang", "Benjamin K. Bergen"], "title": "Not quite Sherlock Holmes: Language model predictions do not reliably differentiate impossible from improbable events", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to Findings of ACL 2025", "summary": "Can language models reliably predict that possible events are more likely\nthan merely improbable ones? By teasing apart possibility, typicality, and\ncontextual relatedness, we show that despite the results of previous work,\nlanguage models' ability to do this is far from robust. In fact, under certain\nconditions, all models tested - including Llama 3, Gemma 2, and Mistral NeMo -\nperform at worse-than-chance level, assigning higher probabilities to\nimpossible sentences such as 'the car was given a parking ticket by the brake'\nthan to merely unlikely sentences such as 'the car was given a parking ticket\nby the explorer'.", "AI": {"tldr": "语言模型在区分可能事件与不可能事件时表现不佳，甚至在某些情况下表现低于随机水平。", "motivation": "探讨语言模型是否能可靠区分可能事件与不可能事件，并分析其局限性。", "method": "通过分离可能性、典型性和上下文相关性，测试多种语言模型（如Llama 3、Gemma 2、Mistral NeMo）的表现。", "result": "所有测试模型在某些条件下表现低于随机水平，甚至对不可能句子赋予更高概率。", "conclusion": "语言模型在区分事件可能性方面仍不稳健，需进一步改进。"}}
{"id": "2506.06856", "pdf": "https://arxiv.org/pdf/2506.06856", "abs": "https://arxiv.org/abs/2506.06856", "authors": ["Chaoyang Wang", "Zeyu Zhang", "Haiyun Jiang"], "title": "Vision-EKIPL: External Knowledge-Infused Policy Learning for Visual Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Visual reasoning is crucial for understanding complex multimodal data and\nadvancing Artificial General Intelligence. Existing methods enhance the\nreasoning capability of Multimodal Large Language Models (MLLMs) through\nReinforcement Learning (RL) fine-tuning (e.g., GRPO). However, current RL\napproaches sample action groups solely from the policy model itself, which\nlimits the upper boundary of the model's reasoning capability and leads to\ninefficient training. To address these limitations, this paper proposes a novel\nRL framework called \\textbf{Vision-EKIPL}. The core of this framework lies in\nintroducing high-quality actions generated by external auxiliary models during\nthe RL training process to guide the optimization of the policy model. The\npolicy learning with knowledge infusion from external models significantly\nexpands the model's exploration space, effectively improves the reasoning\nboundary, and substantially accelerates training convergence speed and\nefficiency. Experimental results demonstrate that our proposed Vision-EKIPL\nachieved up to a 5\\% performance improvement on the Reason-RFT-CoT Benchmark\ncompared to the state-of-the-art (SOTA). It reveals that Vision-EKIPL can\novercome the limitations of traditional RL methods, significantly enhance the\nvisual reasoning performance of MLLMs, and provide a new effective paradigm for\nresearch in this field.", "AI": {"tldr": "本文提出了一种名为Vision-EKIPL的新型强化学习框架，通过引入外部辅助模型生成的高质量动作来优化策略模型，显著提升了多模态大语言模型的视觉推理能力。", "motivation": "现有的强化学习方法仅从策略模型本身采样动作组，限制了模型的推理能力上限并导致训练效率低下。", "method": "提出Vision-EKIPL框架，在强化学习训练过程中引入外部辅助模型生成的高质量动作，指导策略模型的优化。", "result": "在Reason-RFT-CoT Benchmark上实现了5%的性能提升，显著加速了训练收敛速度。", "conclusion": "Vision-EKIPL克服了传统强化学习方法的局限性，为多模态大语言模型的视觉推理研究提供了新范式。"}}
{"id": "2506.07194", "pdf": "https://arxiv.org/pdf/2506.07194", "abs": "https://arxiv.org/abs/2506.07194", "authors": ["Luwei Bai", "Dongkeun Han", "Sara Hennessy"], "title": "Exploring Effective Strategies for Building a Customised GPT Agent for Coding Classroom Dialogues", "categories": ["cs.AI"], "comment": "Draft technical report. 39 pages, 2 figures. Not yet submitted for\n  publication. Update expected", "summary": "This study investigates effective strategies for developing a customised GPT\nagent to code classroom dialogue. While classroom dialogue is widely recognised\nas a crucial element of education, its analysis remains challenging due to the\nneed for a nuanced understanding of dialogic functions and the labour-intensive\nnature of manual transcript coding. Recent advancements in large language\nmodels offer promising avenues for automating this process. However, existing\nstudies predominantly focus on training large-scale models or evaluating\npre-trained models with fixed codebooks, which are often not applicable or\nreplicable for dialogue researchers working with small datasets or customised\ncoding schemes. Using GPT-4's MyGPT agent as a case, this study evaluates its\nbaseline performance in coding classroom dialogue with a human codebook and\nexamines how performance varies with different example inputs through a\nvariable control method. Through a design-based research approach, it\nidentifies a set of practical strategies, based on MyGPT's unique features, for\nconfiguring effective agents with limited data. The findings suggest that,\ndespite some limitations, a MyGPT agent developed with these strategies can\nserve as a useful coding assistant by generating coding suggestions.", "AI": {"tldr": "研究探讨了如何利用GPT-4的MyGPT代理开发定制化策略，以编码课堂对话，并通过设计研究方法提出实用策略。", "motivation": "课堂对话分析在教育中至关重要，但传统方法费时费力，且现有大型语言模型难以适应小数据集或定制化编码需求。", "method": "使用GPT-4的MyGPT代理，通过变量控制方法评估其基线性能，并探索不同输入示例对性能的影响。", "result": "研究发现，尽管存在局限，但基于MyGPT独特功能开发的代理可作为有效的编码助手，生成编码建议。", "conclusion": "研究为利用有限数据配置高效代理提供了实用策略，展示了MyGPT在课堂对话分析中的潜力。"}}
{"id": "2506.06812", "pdf": "https://arxiv.org/pdf/2506.06812", "abs": "https://arxiv.org/abs/2506.06812", "authors": ["Bernardo Leite", "Henrique Lopes Cardoso"], "title": "Advancing Question Generation with Joint Narrative and Difficulty Control", "categories": ["cs.CL"], "comment": "Preprint. Accepted to the BEA 2025 Workshop (ACL)", "summary": "Question Generation (QG), the task of automatically generating questions from\na source input, has seen significant progress in recent years.\nDifficulty-controllable QG (DCQG) enables control over the difficulty level of\ngenerated questions while considering the learner's ability. Additionally,\nnarrative-controllable QG (NCQG) allows control over the narrative aspects\nembedded in the questions. However, research in QG lacks a focus on combining\nthese two types of control, which is important for generating questions\ntailored to educational purposes. To address this gap, we propose a strategy\nfor Joint Narrative and Difficulty Control, enabling simultaneous control over\nthese two attributes in the generation of reading comprehension questions. Our\nevaluation provides preliminary evidence that this approach is feasible, though\nit is not effective across all instances. Our findings highlight the conditions\nunder which the strategy performs well and discuss the trade-offs associated\nwith its application.", "AI": {"tldr": "该论文提出了一种联合控制叙事和难度的方法，用于生成阅读理解问题，填补了现有研究的空白。", "motivation": "现有研究缺乏同时控制问题难度和叙事的方法，而这对教育目的很重要。", "method": "提出了一种联合叙事和难度控制的策略，用于生成阅读理解问题。", "result": "初步评估表明该方法可行，但并非在所有情况下都有效。", "conclusion": "研究揭示了该策略适用的条件及其应用中的权衡。"}}
{"id": "2506.06864", "pdf": "https://arxiv.org/pdf/2506.06864", "abs": "https://arxiv.org/abs/2506.06864", "authors": ["Junyu Liu", "Jianfeng Ren", "Sunhong Liang", "Xudong Jiang"], "title": "Face recognition on point cloud with cgan-top for denoising", "categories": ["cs.CV", "cs.AI"], "comment": "Published in ICASSP 2023", "summary": "Face recognition using 3D point clouds is gaining growing interest, while raw\npoint clouds often contain a significant amount of noise due to imperfect\nsensors. In this paper, an end-to-end 3D face recognition on a noisy point\ncloud is proposed, which synergistically integrates the denoising and\nrecognition modules. Specifically, a Conditional Generative Adversarial Network\non Three Orthogonal Planes (cGAN-TOP) is designed to effectively remove the\nnoise in the point cloud, and recover the underlying features for subsequent\nrecognition. A Linked Dynamic Graph Convolutional Neural Network (LDGCNN) is\nthen adapted to recognize faces from the processed point cloud, which\nhierarchically links both the local point features and neighboring features of\nmultiple scales. The proposed method is validated on the Bosphorus dataset. It\nsignificantly improves the recognition accuracy under all noise settings, with\na maximum gain of 14.81%.", "AI": {"tldr": "提出了一种端到端的3D人脸识别方法，结合去噪和识别模块，显著提升了在噪声点云下的识别精度。", "motivation": "原始点云常因传感器不完美而含噪声，影响人脸识别效果。", "method": "设计了cGAN-TOP用于去噪，LDGCNN用于识别，结合局部和多尺度邻域特征。", "result": "在Bosphorus数据集上验证，最大识别精度提升14.81%。", "conclusion": "该方法有效解决了噪声点云下的3D人脸识别问题。"}}
{"id": "2506.07202", "pdf": "https://arxiv.org/pdf/2506.07202", "abs": "https://arxiv.org/abs/2506.07202", "authors": ["Ming Liu", "Wensheng Zhang"], "title": "Reasoning Multimodal Large Language Model: Data Contamination and Dynamic Evaluation", "categories": ["cs.AI"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) show impressive vision-language\nbenchmark performance, yet growing concerns about data contamination (test set\nexposure during training) risk masking true generalization. This concern\nextends to reasoning MLLMs, often fine-tuned via reinforcement learning from\npotentially contaminated base models. We propose a novel dynamic evaluation\nframework to rigorously assess MLLM generalization, moving beyond static\nbenchmarks. Instead of perturbing inputs, we perturb the task itself. Using the\nsame visual input, models are evaluated across a family of tasks (e.g., QA,\ncaptioning, question posing, verification) to probe diverse capabilities. This\ntask perturbation reveals whether model performance is robust or reliant on\nsuperficial task-specific cues. Our approach is analogous to loss landscape\nsharpness: models overfit or contaminated for a single task (sharp minima)\nfalter under task shifts, unlike models with generalizable solutions (flatter\nminima). We developed an automated pipeline with a calibrated judge scoring\nopen-ended generations (captions, questions) using paraphrase and corruption\nsampling. Applying this framework to leading image/video MLLMs on benchmarks\nincluding MME, RealWorldQA, and CVRR-ES, we analyze each model's cross-task\n\"ability vector.\" We demonstrate that fine-tuning on simulated test data\n(extreme contamination) drastically sharpens task-specific performance but\nharms overall generalization. Our dynamic task perturbation offers deeper\ninsights into MLLM generalization, distinguishing genuine understanding from\nspurious leakage or overfitting.", "AI": {"tldr": "提出了一种动态评估框架，通过任务扰动而非输入扰动，评估多模态大语言模型（MLLMs）的泛化能力，揭示模型是否依赖任务特定线索。", "motivation": "现有MLLMs在视觉语言基准测试中表现优异，但数据污染问题可能掩盖其真实泛化能力，需要更严格的评估方法。", "method": "通过扰动任务本身（如QA、标题生成、提问等），评估模型在多样化任务中的表现，使用自动化流程和校准评分机制。", "result": "实验表明，微调模拟测试数据会提高任务特定性能但损害泛化能力，动态任务扰动能更深入分析模型理解能力。", "conclusion": "动态任务扰动框架为MLLMs的泛化能力提供了更深入的评估，区分了真实理解与虚假泄漏或过拟合。"}}
{"id": "2506.06813", "pdf": "https://arxiv.org/pdf/2506.06813", "abs": "https://arxiv.org/abs/2506.06813", "authors": ["Dipto Das", "Syed Ishtiaque Ahmed", "Shion Guha"], "title": "BTPD: A Multilingual Hand-curated Dataset of Bengali Transnational Political Discourse Across Online Communities", "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "Understanding political discourse in online spaces is crucial for analyzing\npublic opinion and ideological polarization. While social computing and\ncomputational linguistics have explored such discussions in English, such\nresearch efforts are significantly limited in major yet under-resourced\nlanguages like Bengali due to the unavailability of datasets. In this paper, we\npresent a multilingual dataset of Bengali transnational political discourse\n(BTPD) collected from three online platforms, each representing distinct\ncommunity structures and interaction dynamics. Besides describing how we\nhand-curated the dataset through community-informed keyword-based retrieval,\nthis paper also provides a general overview of its topics and multilingual\ncontent.", "AI": {"tldr": "本文介绍了针对孟加拉语跨国政治话语的多语言数据集（BTPD），填补了资源匮乏语言的研究空白。", "motivation": "研究在线政治话语对分析公众意见和意识形态极化至关重要，但孟加拉语等资源匮乏语言的研究因数据集缺失而受限。", "method": "通过社区知情的关键词检索手动整理数据集，并概述其主题和多语言内容。", "result": "构建了一个来自三个在线平台的孟加拉语跨国政治话语数据集，展示了其社区结构和互动动态。", "conclusion": "该数据集为研究孟加拉语政治话语提供了资源，支持未来对多语言在线讨论的分析。"}}
{"id": "2506.06886", "pdf": "https://arxiv.org/pdf/2506.06886", "abs": "https://arxiv.org/abs/2506.06886", "authors": ["Wafaa Kasri", "Yassine Himeur", "Abigail Copiaco", "Wathiq Mansoor", "Ammar Albanna", "Valsamma Eapen"], "title": "Hybrid Vision Transformer-Mamba Framework for Autism Diagnosis via Eye-Tracking Analysis", "categories": ["cs.CV"], "comment": "7 pages, 4 figures and 2 tables", "summary": "Accurate Autism Spectrum Disorder (ASD) diagnosis is vital for early\nintervention. This study presents a hybrid deep learning framework combining\nVision Transformers (ViT) and Vision Mamba to detect ASD using eye-tracking\ndata. The model uses attention-based fusion to integrate visual, speech, and\nfacial cues, capturing both spatial and temporal dynamics. Unlike traditional\nhandcrafted methods, it applies state-of-the-art deep learning and explainable\nAI techniques to enhance diagnostic accuracy and transparency. Tested on the\nSaliency4ASD dataset, the proposed ViT-Mamba model outperformed existing\nmethods, achieving 0.96 accuracy, 0.95 F1-score, 0.97 sensitivity, and 0.94\nspecificity. These findings show the model's promise for scalable,\ninterpretable ASD screening, especially in resource-constrained or remote\nclinical settings where access to expert diagnosis is limited.", "AI": {"tldr": "提出了一种结合Vision Transformers和Vision Mamba的混合深度学习框架，用于通过眼动数据检测自闭症谱系障碍（ASD），显著提高了诊断准确性和可解释性。", "motivation": "早期干预对ASD诊断至关重要，但传统方法依赖手工特征且缺乏透明度。", "method": "采用Vision Transformers和Vision Mamba的混合模型，结合视觉、语音和面部线索的注意力融合，捕捉时空动态。", "result": "在Saliency4ASD数据集上测试，模型表现优异（准确率0.96，F1分数0.95，灵敏度0.97，特异度0.94）。", "conclusion": "该模型为资源有限或偏远地区的ASD筛查提供了可扩展且可解释的解决方案。"}}
{"id": "2506.07217", "pdf": "https://arxiv.org/pdf/2506.07217", "abs": "https://arxiv.org/abs/2506.07217", "authors": ["Zihan Deng", "Changyu Du", "Stavros Nousias", "André Borrmann"], "title": "BIMgent: Towards Autonomous Building Modeling via Computer-use Agents", "categories": ["cs.AI"], "comment": "ICML 2025 Workshop on Computer Use Agents", "summary": "Existing computer-use agents primarily focus on general-purpose desktop\nautomation tasks, with limited exploration of their application in highly\nspecialized domains. In particular, the 3D building modeling process in the\nArchitecture, Engineering, and Construction (AEC) sector involves open-ended\ndesign tasks and complex interaction patterns within Building Information\nModeling (BIM) authoring software, which has yet to be thoroughly addressed by\ncurrent studies. In this paper, we propose BIMgent, an agentic framework\npowered by multimodal large language models (LLMs), designed to enable\nautonomous building model authoring via graphical user interface (GUI)\noperations. BIMgent automates the architectural building modeling process,\nincluding multimodal input for conceptual design, planning of software-specific\nworkflows, and efficient execution of the authoring GUI actions. We evaluate\nBIMgent on real-world building modeling tasks, including both text-based\nconceptual design generation and reconstruction from existing building design.\nThe design quality achieved by BIMgent was found to be reasonable. Its\noperations achieved a 32% success rate, whereas all baseline models failed to\ncomplete the tasks (0% success rate). Results demonstrate that BIMgent\neffectively reduces manual workload while preserving design intent,\nhighlighting its potential for practical deployment in real-world architectural\nmodeling scenarios.", "AI": {"tldr": "BIMgent是一个基于多模态大语言模型的代理框架，旨在通过GUI操作实现自主的建筑模型创作，专注于AEC领域的3D建筑建模。", "motivation": "现有计算机代理主要关注通用桌面自动化任务，而在高度专业化的AEC领域中，尤其是BIM软件的复杂交互和开放设计任务尚未得到充分研究。", "method": "BIMgent利用多模态LLM，实现从概念设计到GUI操作的自动化建筑建模流程，包括多模态输入、软件特定工作流规划和高效执行。", "result": "在真实建筑建模任务中，BIMgent的设计质量合理，成功率为32%，而基线模型完全失败（0%成功率）。", "conclusion": "BIMgent显著减少手动工作量并保留设计意图，展示了其在实际建筑建模场景中的应用潜力。"}}
{"id": "2506.06816", "pdf": "https://arxiv.org/pdf/2506.06816", "abs": "https://arxiv.org/abs/2506.06816", "authors": ["Dipto Das", "Shion Guha", "Bryan Semaan"], "title": "How do datasets, developers, and models affect biases in a low-resourced language?", "categories": ["cs.CL", "cs.CY", "cs.HC"], "comment": null, "summary": "Sociotechnical systems, such as language technologies, frequently exhibit\nidentity-based biases. These biases exacerbate the experiences of historically\nmarginalized communities and remain understudied in low-resource contexts.\nWhile models and datasets specific to a language or with multilingual support\nare commonly recommended to address these biases, this paper empirically tests\nthe effectiveness of such approaches in the context of gender, religion, and\nnationality-based identities in Bengali, a widely spoken but low-resourced\nlanguage. We conducted an algorithmic audit of sentiment analysis models built\non mBERT and BanglaBERT, which were fine-tuned using all Bengali sentiment\nanalysis (BSA) datasets from Google Dataset Search. Our analyses showed that\nBSA models exhibit biases across different identity categories despite having\nsimilar semantic content and structure. We also examined the inconsistencies\nand uncertainties arising from combining pre-trained models and datasets\ncreated by individuals from diverse demographic backgrounds. We connected these\nfindings to the broader discussions on epistemic injustice, AI alignment, and\nmethodological decisions in algorithmic audits.", "AI": {"tldr": "论文研究了孟加拉语情感分析模型中的身份偏见问题，测试了多语言和特定语言模型在性别、宗教和国籍身份上的表现，发现即使语义内容相似，模型仍存在偏见。", "motivation": "研究低资源语言（如孟加拉语）中身份偏见的不足，填补现有研究的空白，并探讨多语言和特定语言模型是否有效解决偏见问题。", "method": "对基于mBERT和BanglaBERT的情感分析模型进行算法审计，使用Google Dataset Search中的所有孟加拉语情感分析数据集进行微调。", "result": "模型在不同身份类别（性别、宗教、国籍）上表现出偏见，且组合预训练模型和多样化数据集时存在不一致性和不确定性。", "conclusion": "研究结果与认知不公、AI对齐和算法审计方法论的讨论相关，强调需进一步解决低资源语言中的偏见问题。"}}
{"id": "2506.06898", "pdf": "https://arxiv.org/pdf/2506.06898", "abs": "https://arxiv.org/abs/2506.06898", "authors": ["Reese Kneeland", "Paul S. Scotti", "Ghislain St-Yves", "Jesse Breedlove", "Kendrick Kay", "Thomas Naselaris"], "title": "NSD-Imagery: A benchmark dataset for extending fMRI vision decoding methods to mental imagery", "categories": ["cs.CV", "cs.LG", "eess.IV", "q-bio.NC"], "comment": "Published at CVPR 2025", "summary": "We release NSD-Imagery, a benchmark dataset of human fMRI activity paired\nwith mental images, to complement the existing Natural Scenes Dataset (NSD), a\nlarge-scale dataset of fMRI activity paired with seen images that enabled\nunprecedented improvements in fMRI-to-image reconstruction efforts. Recent\nmodels trained on NSD have been evaluated only on seen image reconstruction.\nUsing NSD-Imagery, it is possible to assess how well these models perform on\nmental image reconstruction. This is a challenging generalization requirement\nbecause mental images are encoded in human brain activity with relatively lower\nsignal-to-noise and spatial resolution; however, generalization from seen to\nmental imagery is critical for real-world applications in medical domains and\nbrain-computer interfaces, where the desired information is always internally\ngenerated. We provide benchmarks for a suite of recent NSD-trained open-source\nvisual decoding models (MindEye1, MindEye2, Brain Diffuser, iCNN, Takagi et\nal.) on NSD-Imagery, and show that the performance of decoding methods on\nmental images is largely decoupled from performance on vision reconstruction.\nWe further demonstrate that architectural choices significantly impact\ncross-decoding performance: models employing simple linear decoding\narchitectures and multimodal feature decoding generalize better to mental\nimagery, while complex architectures tend to overfit visual training data. Our\nfindings indicate that mental imagery datasets are critical for the development\nof practical applications, and establish NSD-Imagery as a useful resource for\nbetter aligning visual decoding methods with this goal.", "AI": {"tldr": "NSD-Imagery是一个新的基准数据集，用于评估从fMRI活动重建心理图像的性能，补充了现有的NSD数据集。研究发现，心理图像重建的性能与视觉重建解耦，且简单架构模型在跨解码任务中表现更好。", "motivation": "心理图像重建在医学和脑机接口领域具有重要应用价值，但现有模型仅在视觉重建上评估。NSD-Imagery填补了这一空白，支持对心理图像重建性能的评估。", "method": "使用NSD-Imagery数据集，评估了多种NSD训练的视觉解码模型（如MindEye1、Brain Diffuser等）在心理图像重建上的表现。", "result": "心理图像重建性能与视觉重建解耦，简单线性解码架构和多模态特征解码模型表现更优，复杂架构易过拟合视觉数据。", "conclusion": "心理图像数据集对实际应用至关重要，NSD-Imagery为视觉解码方法的改进提供了重要资源。"}}
{"id": "2506.07223", "pdf": "https://arxiv.org/pdf/2506.07223", "abs": "https://arxiv.org/abs/2506.07223", "authors": ["Yangqing Zheng", "Shunqi Mao", "Dingxin Zhang", "Weidong Cai"], "title": "LLM-Enhanced Rapid-Reflex Async-Reflect Embodied Agent for Real-Time Decision-Making in Dynamically Changing Environments", "categories": ["cs.AI"], "comment": "Accepted by the CVPR 2025 Embodied AI Workshop", "summary": "In the realm of embodied intelligence, the evolution of large language models\n(LLMs) has markedly enhanced agent decision making. Consequently, researchers\nhave begun exploring agent performance in dynamically changing high-risk\nscenarios, i.e., fire, flood, and wind scenarios in the HAZARD benchmark. Under\nthese extreme conditions, the delay in decision making emerges as a crucial yet\ninsufficiently studied issue. We propose a Time Conversion Mechanism (TCM) that\ntranslates inference delays in decision-making into equivalent simulation\nframes, thus aligning cognitive and physical costs under a single FPS-based\nmetric. By extending HAZARD with Respond Latency (RL) and Latency-to-Action\nRatio (LAR), we deliver a fully latency-aware evaluation protocol. Moreover, we\npresent the Rapid-Reflex Async-Reflect Agent (RRARA), which couples a\nlightweight LLM-guided feedback module with a rule-based agent to enable\nimmediate reactive behaviors and asynchronous reflective refinements in situ.\nExperiments on HAZARD show that RRARA substantially outperforms existing\nbaselines in latency-sensitive scenarios.", "AI": {"tldr": "论文提出了一种时间转换机制（TCM）和快速反射异步反思代理（RRARA），以解决动态高风险场景中决策延迟问题，并在HAZARD基准测试中验证了其优越性。", "motivation": "在动态高风险场景（如火灾、洪水等）中，决策延迟是一个关键但研究不足的问题，需要新的方法来量化延迟并提升代理性能。", "method": "提出TCM将推理延迟转换为等效模拟帧，并引入RL和LAR指标；设计RRARA代理，结合轻量级LLM反馈模块和基于规则的代理，实现快速反应和异步反思。", "result": "在HAZARD基准测试中，RRARA在延迟敏感场景中显著优于现有基线。", "conclusion": "TCM和RRARA有效解决了决策延迟问题，为高风险场景中的代理性能提供了新思路。"}}
{"id": "2506.06820", "pdf": "https://arxiv.org/pdf/2506.06820", "abs": "https://arxiv.org/abs/2506.06820", "authors": ["Wenyu Zhang", "Yingxu He", "Geyu Lin", "Zhuohan Liu", "Shuo Sun", "Bin Wang", "Xunlong Zou", "Jeremy H. M. Wong", "Qiongqiong Wang", "Hardik B. Sailor", "Nancy F. Chen", "Ai Ti Aw"], "title": "Beyond Classification: Towards Speech Emotion Reasoning with Multitask AudioLLMs", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "Audio Large Language Models (AudioLLMs) have achieved strong results in\nsemantic tasks like speech recognition and translation, but remain limited in\nmodeling paralinguistic cues such as emotion. Existing approaches often treat\nemotion understanding as a classification problem, offering little insight into\nthe underlying rationale behind predictions. In this work, we explore emotion\nreasoning, a strategy that leverages the generative capabilities of AudioLLMs\nto enhance emotion recognition by producing semantically aligned,\nevidence-grounded explanations. To support this in multitask AudioLLMs, we\nintroduce a unified framework combining reasoning-augmented data supervision,\ndual-encoder architecture, and task-alternating training. This approach enables\nAudioLLMs to effectively learn different tasks while incorporating emotional\nreasoning. Experiments on IEMOCAP and MELD show that our approach not only\nimproves emotion prediction accuracy but also enhances the coherence and\nevidential grounding of the generated responses.", "AI": {"tldr": "论文提出了一种利用AudioLLMs的生成能力增强情感识别的方法，通过情感推理生成证据支持的语义解释，并结合多任务框架提升模型性能。", "motivation": "现有AudioLLMs在情感理解任务中表现有限，通常将其视为分类问题，缺乏对预测背后逻辑的解释。", "method": "提出统一框架，结合推理增强的数据监督、双编码器架构和任务交替训练，支持多任务AudioLLMs的情感推理。", "result": "在IEMOCAP和MELD数据集上，方法不仅提高了情感预测准确率，还增强了生成响应的连贯性和证据支持。", "conclusion": "情感推理策略有效提升了AudioLLMs在情感任务中的表现，同时生成更具解释性的输出。"}}
{"id": "2506.06906", "pdf": "https://arxiv.org/pdf/2506.06906", "abs": "https://arxiv.org/abs/2506.06906", "authors": ["Nima Jamali", "Matina Mahdizadeh Sani", "Hanieh Naderi", "Shohreh Kasaei"], "title": "KNN-Defense: Defense against 3D Adversarial Point Clouds using Nearest-Neighbor Search", "categories": ["cs.CV"], "comment": null, "summary": "Deep neural networks (DNNs) have demonstrated remarkable performance in\nanalyzing 3D point cloud data. However, their vulnerability to adversarial\nattacks-such as point dropping, shifting, and adding-poses a critical challenge\nto the reliability of 3D vision systems. These attacks can compromise the\nsemantic and structural integrity of point clouds, rendering many existing\ndefense mechanisms ineffective. To address this issue, a defense strategy named\nKNN-Defense is proposed, grounded in the manifold assumption and\nnearest-neighbor search in feature space. Instead of reconstructing surface\ngeometry or enforcing uniform point distributions, the method restores\nperturbed inputs by leveraging the semantic similarity of neighboring samples\nfrom the training set. KNN-Defense is lightweight and computationally\nefficient, enabling fast inference and making it suitable for real-time and\npractical applications. Empirical results on the ModelNet40 dataset\ndemonstrated that KNN-Defense significantly improves robustness across various\nattack types. In particular, under point-dropping attacks-where many existing\nmethods underperform due to the targeted removal of critical points-the\nproposed method achieves accuracy gains of 20.1%, 3.6%, 3.44%, and 7.74% on\nPointNet, PointNet++, DGCNN, and PCT, respectively. These findings suggest that\nKNN-Defense offers a scalable and effective solution for enhancing the\nadversarial resilience of 3D point cloud classifiers. (An open-source\nimplementation of the method, including code and data, is available at\nhttps://github.com/nimajam41/3d-knn-defense).", "AI": {"tldr": "KNN-Defense是一种轻量级防御策略，通过特征空间中的最近邻搜索恢复受扰动的3D点云数据，显著提升了对抗攻击下的鲁棒性。", "motivation": "深度神经网络在3D点云数据分析中表现出色，但对对抗攻击（如点丢弃、移动和添加）的脆弱性影响了3D视觉系统的可靠性。", "method": "基于流形假设和特征空间的最近邻搜索，KNN-Defense通过训练集中邻近样本的语义相似性恢复受扰动的输入。", "result": "在ModelNet40数据集上，KNN-Defense显著提升了对抗攻击下的鲁棒性，特别是在点丢弃攻击下，对多种模型的准确率提升显著。", "conclusion": "KNN-Defense为3D点云分类器提供了一种可扩展且高效的对抗防御解决方案。"}}
{"id": "2506.07255", "pdf": "https://arxiv.org/pdf/2506.07255", "abs": "https://arxiv.org/abs/2506.07255", "authors": ["Jake Tuero", "Michael Buro", "Levi H. S. Lelis"], "title": "Subgoal-Guided Policy Heuristic Search with Learned Subgoals", "categories": ["cs.AI"], "comment": "Accepted to ICML-25", "summary": "Policy tree search is a family of tree search algorithms that use a policy to\nguide the search. These algorithms provide guarantees on the number of\nexpansions required to solve a given problem that are based on the quality of\nthe policy. While these algorithms have shown promising results, the process in\nwhich they are trained requires complete solution trajectories to train the\npolicy. Search trajectories are obtained during a trial-and-error search\nprocess. When the training problem instances are hard, learning can be\nprohibitively costly, especially when starting from a randomly initialized\npolicy. As a result, search samples are wasted in failed attempts to solve\nthese hard instances. This paper introduces a novel method for learning\nsubgoal-based policies for policy tree search algorithms. The subgoals and\npolicies conditioned on subgoals are learned from the trees that the search\nexpands while attempting to solve problems, including the search trees of\nfailed attempts. We empirically show that our policy formulation and training\nmethod improve the sample efficiency of learning a policy and heuristic\nfunction in this online setting.", "AI": {"tldr": "论文提出了一种基于子目标的学习方法，用于提升策略树搜索算法的样本效率。", "motivation": "传统策略树搜索算法在训练时需要完整的解轨迹，对于困难问题样本效率低，浪费资源。", "method": "通过从搜索树（包括失败尝试）中学习子目标和基于子目标的策略。", "result": "实验表明，该方法提高了在线学习中策略和启发式函数的样本效率。", "conclusion": "该方法显著提升了策略树搜索算法的训练效率，尤其在困难问题中表现突出。"}}
{"id": "2506.06821", "pdf": "https://arxiv.org/pdf/2506.06821", "abs": "https://arxiv.org/abs/2506.06821", "authors": ["Yuhan Cao", "Zian Chen", "Kun Quan", "Ziliang Zhang", "Yu Wang", "Xiaoning Dong", "Yeqi Feng", "Guanzhong He", "Jingcheng Huang", "Jianhao Li", "Yixuan Tan", "Jiafu Tang", "Yilin Tang", "Junlei Wu", "Qianyu Xiao", "Can Zheng", "Shouchen Zhou", "Yuxiang Zhu", "Yiming Huang", "Tian Xie", "Tianxing He"], "title": "Can LLMs Generate Reliable Test Case Generators? A Study on Competition-Level Programming Problems", "categories": ["cs.CL", "cs.AI", "cs.SE"], "comment": "37 pages, 22 figures", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncode generation, capable of tackling complex tasks during inference. However,\nthe extent to which LLMs can be utilized for code checking or debugging through\ntest case generation remains largely unexplored. We investigate this problem\nfrom the perspective of competition-level programming (CP) programs and propose\nTCGBench, a Benchmark for (LLM generation of) Test Case Generators. This\nbenchmark comprises two tasks, aimed at studying the capabilities of LLMs in\n(1) generating valid test case generators for a given CP problem, and further\n(2) generating targeted test case generators that expose bugs in human-written\ncode. Experimental results indicate that while state-of-the-art LLMs can\ngenerate valid test case generators in most cases, most LLMs struggle to\ngenerate targeted test cases that reveal flaws in human code effectively.\nEspecially, even advanced reasoning models (e.g., o3-mini) fall significantly\nshort of human performance in the task of generating targeted generators.\nFurthermore, we construct a high-quality, manually curated dataset of\ninstructions for generating targeted generators. Analysis demonstrates that the\nperformance of LLMs can be enhanced with the aid of this dataset, by both\nprompting and fine-tuning.", "AI": {"tldr": "论文研究了大型语言模型（LLMs）在代码检查或调试中通过测试用例生成的能力，提出了TCGBench基准测试，发现LLMs在生成有效测试用例生成器方面表现良好，但在生成针对性测试用例以暴露人类代码缺陷方面表现不佳。", "motivation": "探索LLMs在代码检查或调试中的潜力，尤其是在竞赛级编程（CP）中生成测试用例的能力。", "method": "提出TCGBench基准测试，包含两项任务：生成有效测试用例生成器和生成针对性测试用例生成器。实验评估了LLMs的表现，并构建了一个高质量的手动标注数据集。", "result": "LLMs能生成有效测试用例生成器，但在生成针对性测试用例方面表现不佳，尤其是高级推理模型（如o3-mini）远不及人类表现。使用手动标注数据集可通过提示和微调提升LLMs性能。", "conclusion": "LLMs在生成测试用例生成器方面有潜力，但在针对性任务上仍需改进。手动标注数据集能显著提升模型性能。"}}
{"id": "2506.06909", "pdf": "https://arxiv.org/pdf/2506.06909", "abs": "https://arxiv.org/abs/2506.06909", "authors": ["Vladimir Yugay", "Thies Kersten", "Luca Carlone", "Theo Gevers", "Martin R. Oswald", "Lukas Schmid"], "title": "Gaussian Mapping for Evolving Scenes", "categories": ["cs.CV"], "comment": null, "summary": "Mapping systems with novel view synthesis (NVS) capabilities are widely used\nin computer vision, with augmented reality, robotics, and autonomous driving\napplications. Most notably, 3D Gaussian Splatting-based systems show high NVS\nperformance; however, many current approaches are limited to static scenes.\nWhile recent works have started addressing short-term dynamics (motion within\nthe view of the camera), long-term dynamics (the scene evolving through changes\nout of view) remain less explored. To overcome this limitation, we introduce a\ndynamic scene adaptation mechanism that continuously updates the 3D\nrepresentation to reflect the latest changes. In addition, since maintaining\ngeometric and semantic consistency remains challenging due to stale\nobservations disrupting the reconstruction process, we propose a novel keyframe\nmanagement mechanism that discards outdated observations while preserving as\nmuch information as possible. We evaluate Gaussian Mapping for Evolving Scenes\n(GaME) on both synthetic and real-world datasets and find it to be more\naccurate than the state of the art.", "AI": {"tldr": "论文提出了一种动态场景适应机制和关键帧管理机制，用于解决3D高斯泼溅技术在长时动态场景中的局限性，并在合成和真实数据集上验证了其优越性。", "motivation": "当前3D高斯泼溅技术在静态场景中表现优异，但在长时动态场景（场景在视野外持续变化）中表现不足，需要一种机制来持续更新3D表示并保持几何和语义一致性。", "method": "提出动态场景适应机制以持续更新3D表示，并设计关键帧管理机制以丢弃过时观测同时保留有用信息。", "result": "在合成和真实数据集上，提出的GaME方法比现有技术更准确。", "conclusion": "GaME方法有效解决了长时动态场景的挑战，提升了3D高斯泼溅技术的实用性。"}}
{"id": "2506.07390", "pdf": "https://arxiv.org/pdf/2506.07390", "abs": "https://arxiv.org/abs/2506.07390", "authors": ["Xin-Cheng Wen", "Yijun Yang", "Cuiyun Gao", "Yang Xiao", "Deheng Ye"], "title": "Boosting Vulnerability Detection of LLMs via Curriculum Preference Optimization with Synthetic Reasoning Data", "categories": ["cs.AI", "cs.SE"], "comment": "Accepted by ACL 2025 Findings", "summary": "Large language models (LLMs) demonstrate considerable proficiency in numerous\ncoding-related tasks; however, their capabilities in detecting software\nvulnerabilities remain limited. This limitation primarily stems from two\nfactors: (1) the absence of reasoning data related to vulnerabilities, which\nhinders the models' ability to capture underlying vulnerability patterns; and\n(2) their focus on learning semantic representations rather than the reason\nbehind them, thus failing to recognize semantically similar vulnerability\nsamples. Furthermore, the development of LLMs specialized in vulnerability\ndetection is challenging, particularly in environments characterized by the\nscarcity of high-quality datasets. In this paper, we propose a novel framework\nReVD that excels at mining vulnerability patterns through reasoning data\nsynthesizing and vulnerability-specific preference optimization. Specifically,\nwe construct forward and backward reasoning processes for vulnerability and\ncorresponding fixed code, ensuring the synthesis of high-quality reasoning\ndata. Moreover, we design the triplet supervised fine-tuning followed by\ncurriculum online preference optimization for enabling ReVD to better\nunderstand vulnerability patterns. The extensive experiments conducted on\nPrimeVul and SVEN datasets demonstrate that ReVD sets new state-of-the-art for\nLLM-based software vulnerability detection, e.g., 12.24\\%-22.77\\% improvement\nin the accuracy. The source code and data are available at\nhttps://github.com/Xin-Cheng-Wen/PO4Vul.", "AI": {"tldr": "ReVD框架通过合成推理数据和优化漏洞偏好，显著提升了大型语言模型在软件漏洞检测中的性能。", "motivation": "大型语言模型在漏洞检测方面表现有限，主要因缺乏推理数据和语义学习而非模式识别。", "method": "提出ReVD框架，合成漏洞推理数据，并采用三重监督微调和课程在线偏好优化。", "result": "在PrimeVul和SVEN数据集上，ReVD将准确率提高了12.24%-22.77%。", "conclusion": "ReVD为基于LLM的漏洞检测设定了新的最先进水平。"}}
{"id": "2506.06842", "pdf": "https://arxiv.org/pdf/2506.06842", "abs": "https://arxiv.org/abs/2506.06842", "authors": ["Arkadiusz Modzelewski", "Witold Sosnowski", "Tiziano Labruna", "Adam Wierzbicki", "Giovanni Da San Martino"], "title": "PCoT: Persuasion-Augmented Chain of Thought for Detecting Fake News and Social Media Disinformation", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted to ACL 2025 Main Conference", "summary": "Disinformation detection is a key aspect of media literacy. Psychological\nstudies have shown that knowledge of persuasive fallacies helps individuals\ndetect disinformation. Inspired by these findings, we experimented with large\nlanguage models (LLMs) to test whether infusing persuasion knowledge enhances\ndisinformation detection. As a result, we introduce the Persuasion-Augmented\nChain of Thought (PCoT), a novel approach that leverages persuasion to improve\ndisinformation detection in zero-shot classification. We extensively evaluate\nPCoT on online news and social media posts. Moreover, we publish two novel,\nup-to-date disinformation datasets: EUDisinfo and MultiDis. These datasets\nenable the evaluation of PCoT on content entirely unseen by the LLMs used in\nour experiments, as the content was published after the models' knowledge\ncutoffs. We show that, on average, PCoT outperforms competitive methods by 15%\nacross five LLMs and five datasets. These findings highlight the value of\npersuasion in strengthening zero-shot disinformation detection.", "AI": {"tldr": "论文提出了一种名为PCoT的新方法，通过注入说服知识提升零样本分类中的虚假信息检测能力，并在多个数据集上表现优异。", "motivation": "心理学研究表明，了解说服谬误有助于检测虚假信息，因此作者尝试利用大语言模型（LLMs）验证这一假设。", "method": "提出Persuasion-Augmented Chain of Thought (PCoT)，结合说服知识改进虚假信息检测，并在新闻和社交媒体内容上进行了广泛评估。", "result": "PCoT在五个LLMs和五个数据集上平均表现优于竞争方法15%，并发布了两个新的虚假信息数据集EUDisinfo和MultiDis。", "conclusion": "研究证明了说服知识在提升零样本虚假信息检测中的价值。"}}
{"id": "2506.06912", "pdf": "https://arxiv.org/pdf/2506.06912", "abs": "https://arxiv.org/abs/2506.06912", "authors": ["Olivier Papillon", "Rafik Goubran", "James Green", "Julien Larivière-Chartier", "Caitlin Higginson", "Frank Knoefel", "Rébecca Robillard"], "title": "Sleep Stage Classification using Multimodal Embedding Fusion from EOG and PSM", "categories": ["cs.CV"], "comment": "Submitted to IEEE MeMeA 2025", "summary": "Accurate sleep stage classification is essential for diagnosing sleep\ndisorders, particularly in aging populations. While traditional polysomnography\n(PSG) relies on electroencephalography (EEG) as the gold standard, its\ncomplexity and need for specialized equipment make home-based sleep monitoring\nchallenging. To address this limitation, we investigate the use of\nelectrooculography (EOG) and pressure-sensitive mats (PSM) as less obtrusive\nalternatives for five-stage sleep-wake classification. This study introduces a\nnovel approach that leverages ImageBind, a multimodal embedding deep learning\nmodel, to integrate PSM data with dual-channel EOG signals for sleep stage\nclassification. Our method is the first reported approach that fuses PSM and\nEOG data for sleep stage classification with ImageBind. Our results demonstrate\nthat fine-tuning ImageBind significantly improves classification accuracy,\noutperforming existing models based on single-channel EOG (DeepSleepNet),\nexclusively PSM data (ViViT), and other multimodal deep learning approaches\n(MBT). Notably, the model also achieved strong performance without fine-tuning,\nhighlighting its adaptability to specific tasks with limited labeled data,\nmaking it particularly advantageous for medical applications. We evaluated our\nmethod using 85 nights of patient recordings from a sleep clinic. Our findings\nsuggest that pre-trained multimodal embedding models, even those originally\ndeveloped for non-medical domains, can be effectively adapted for sleep\nstaging, with accuracies approaching systems that require complex EEG data.", "AI": {"tldr": "该论文提出了一种基于ImageBind的多模态嵌入深度学习模型，结合压力敏感垫（PSM）和双通道眼电图（EOG）信号进行睡眠阶段分类，显著提高了分类准确性。", "motivation": "传统多导睡眠图（PSG）依赖脑电图（EEG），设备复杂且难以在家中使用，因此研究更便捷的替代方法（如EOG和PSM）至关重要。", "method": "利用ImageBind模型整合PSM和双通道EOG数据，首次将这两种数据融合用于睡眠阶段分类。", "result": "实验表明，该方法在85晚患者数据上的分类准确性优于现有单通道EOG（DeepSleepNet）、纯PSM（ViViT）及其他多模态方法（MBT），且无需微调也能表现良好。", "conclusion": "预训练的多模态嵌入模型（如ImageBind）可有效用于睡眠阶段分类，接近依赖复杂EEG数据的系统性能，适用于医疗领域。"}}
{"id": "2506.07411", "pdf": "https://arxiv.org/pdf/2506.07411", "abs": "https://arxiv.org/abs/2506.07411", "authors": ["Ze Yang", "Yihong Jin", "Juntian Liu", "Xinhe Xu"], "title": "An Intelligent Fault Self-Healing Mechanism for Cloud AI Systems via Integration of Large Language Models and Deep Reinforcement Learning", "categories": ["cs.AI"], "comment": "Proceedings of 2025 IEEE 8th International Conference on Advanced\n  Electronic Materials, Computers and Software Engineering (AEMCSE 2025)", "summary": "As the scale and complexity of cloud-based AI systems continue to increase,\nthe detection and adaptive recovery of system faults have become the core\nchallenges to ensure service reliability and continuity. In this paper, we\npropose an Intelligent Fault Self-Healing Mechanism (IFSHM) that integrates\nLarge Language Model (LLM) and Deep Reinforcement Learning (DRL), aiming to\nrealize a fault recovery framework with semantic understanding and policy\noptimization capabilities in cloud AI systems. On the basis of the traditional\nDRL-based control model, the proposed method constructs a two-stage hybrid\narchitecture: (1) an LLM-driven fault semantic interpretation module, which can\ndynamically extract deep contextual semantics from multi-source logs and system\nindicators to accurately identify potential fault modes; (2) DRL recovery\nstrategy optimizer, based on reinforcement learning, learns the dynamic\nmatching of fault types and response behaviors in the cloud environment. The\ninnovation of this method lies in the introduction of LLM for environment\nmodeling and action space abstraction, which greatly improves the exploration\nefficiency and generalization ability of reinforcement learning. At the same\ntime, a memory-guided meta-controller is introduced, combined with\nreinforcement learning playback and LLM prompt fine-tuning strategy, to achieve\ncontinuous adaptation to new failure modes and avoid catastrophic forgetting.\nExperimental results on the cloud fault injection platform show that compared\nwith the existing DRL and rule methods, the IFSHM framework shortens the system\nrecovery time by 37% with unknown fault scenarios.", "AI": {"tldr": "论文提出了一种结合大型语言模型（LLM）和深度强化学习（DRL）的智能故障自愈机制（IFSHM），用于提升云AI系统的故障恢复能力。", "motivation": "随着云AI系统的规模和复杂性增加，故障检测和自适应恢复成为确保服务可靠性和连续性的核心挑战。", "method": "IFSHM采用两阶段混合架构：LLM驱动的故障语义解释模块和DRL恢复策略优化器，结合LLM的环境建模和动作空间抽象，提升强化学习的探索效率和泛化能力。", "result": "实验表明，IFSHM在未知故障场景下将系统恢复时间缩短了37%，优于现有DRL和规则方法。", "conclusion": "IFSHM通过LLM和DRL的结合，显著提升了云AI系统的故障恢复效率和适应性。"}}
{"id": "2506.06844", "pdf": "https://arxiv.org/pdf/2506.06844", "abs": "https://arxiv.org/abs/2506.06844", "authors": ["Naibin Gu", "Peng Fu", "Xiyu Liu", "Ke Ma", "Zheng Lin", "Weiping Wang"], "title": "Adapt Once, Thrive with Updates: Transferable Parameter-Efficient Fine-Tuning on Evolving Base Models", "categories": ["cs.CL"], "comment": "Accepted by ACL 2025", "summary": "Parameter-efficient fine-tuning (PEFT) has become a common method for\nfine-tuning large language models, where a base model can serve multiple users\nthrough PEFT module switching. To enhance user experience, base models require\nperiodic updates. However, once updated, PEFT modules fine-tuned on previous\nversions often suffer substantial performance degradation on newer versions.\nRe-tuning these numerous modules to restore performance would incur significant\ncomputational costs. Through a comprehensive analysis of the changes that occur\nduring base model updates, we uncover an interesting phenomenon: continual\ntraining primarily affects task-specific knowledge stored in Feed-Forward\nNetworks (FFN), while having less impact on the task-specific pattern in the\nAttention mechanism. Based on these findings, we introduce Trans-PEFT, a novel\napproach that enhances the PEFT module by focusing on the task-specific pattern\nwhile reducing its dependence on certain knowledge in the base model. Further\ntheoretical analysis supports our approach. Extensive experiments across 7 base\nmodels and 12 datasets demonstrate that Trans-PEFT trained modules can maintain\nperformance on updated base models without re-tuning, significantly reducing\nmaintenance overhead in real-world applications.", "AI": {"tldr": "Trans-PEFT是一种新方法，通过专注于任务特定模式并减少对基础模型中某些知识的依赖，提升PEFT模块的性能，使其在基础模型更新后无需重新调优即可保持性能。", "motivation": "基础模型更新后，PEFT模块性能下降，重新调优计算成本高。研究发现持续训练主要影响FFN中的任务特定知识，而对注意力机制中的任务特定模式影响较小。", "method": "提出Trans-PEFT方法，专注于任务特定模式，减少对基础模型中某些知识的依赖。", "result": "在7个基础模型和12个数据集上的实验表明，Trans-PEFT模块在基础模型更新后无需重新调优即可保持性能。", "conclusion": "Trans-PEFT显著减少了实际应用中的维护开销，为PEFT模块的性能保持提供了有效解决方案。"}}
{"id": "2506.06918", "pdf": "https://arxiv.org/pdf/2506.06918", "abs": "https://arxiv.org/abs/2506.06918", "authors": ["Carl Brander", "Giovanni Cioffi", "Nico Messikommer", "Davide Scaramuzza"], "title": "Reading in the Dark with Foveated Event Vision", "categories": ["cs.CV", "cs.RO"], "comment": "CVPR 2025 Workshop on Event-based Vision", "summary": "Current smart glasses equipped with RGB cameras struggle to perceive the\nenvironment in low-light and high-speed motion scenarios due to motion blur and\nthe limited dynamic range of frame cameras. Additionally, capturing dense\nimages with a frame camera requires large bandwidth and power consumption,\nconsequently draining the battery faster. These challenges are especially\nrelevant for developing algorithms that can read text from images. In this\nwork, we propose a novel event-based Optical Character Recognition (OCR)\napproach for smart glasses. By using the eye gaze of the user, we foveate the\nevent stream to significantly reduce bandwidth by around 98% while exploiting\nthe benefits of event cameras in high-dynamic and fast scenes. Our proposed\nmethod performs deep binary reconstruction trained on synthetic data and\nleverages multimodal LLMs for OCR, outperforming traditional OCR solutions. Our\nresults demonstrate the ability to read text in low light environments where\nRGB cameras struggle while using up to 2400 times less bandwidth than a\nwearable RGB camera.", "AI": {"tldr": "提出了一种基于事件的新型OCR方法，用于智能眼镜，通过用户视线聚焦事件流，显著降低带宽需求，并在低光和高动态场景中优于传统OCR。", "motivation": "当前智能眼镜的RGB相机在低光和高动态场景中表现不佳，且带宽和功耗高，影响电池寿命。", "method": "利用用户视线聚焦事件流，结合深度二值重建和合成数据训练，采用多模态LLM进行OCR。", "result": "在低光环境下成功读取文本，带宽需求比RGB相机低2400倍。", "conclusion": "该方法在低光和高动态场景中高效且节能，优于传统OCR解决方案。"}}
{"id": "2506.07418", "pdf": "https://arxiv.org/pdf/2506.07418", "abs": "https://arxiv.org/abs/2506.07418", "authors": ["Arnau Igualde Sáez", "Lamyae Rhomrasi", "Yusef Ahsini", "Ricardo Vinuesa", "Sergio Hoyas", "Jose P. García Sabater", "Marius J. Fullana i Alfonso", "J. Alberto Conejero"], "title": "Evaluating Visual Mathematics in Multimodal LLMs: A Multilingual Benchmark Based on the Kangaroo Tests", "categories": ["cs.AI", "68T05, 68T45", "I.2.10; I.2.7"], "comment": "16 pages, 4 figures", "summary": "Multimodal Large Language Models (MLLMs) promise advanced vision language\ncapabilities, yet their effectiveness in visually presented mathematics remains\nunderexplored. This paper analyzes the development and evaluation of MLLMs for\nmathematical problem solving, focusing on diagrams, multilingual text, and\nsymbolic notation. We then assess several models, including GPT 4o, Pixtral,\nQwen VL, Llama 3.2 Vision variants, and Gemini 2.0 Flash in a multilingual\nKangaroo style benchmark spanning English, French, Spanish, and Catalan. Our\nexperiments reveal four key findings. First, overall precision remains moderate\nacross geometry, visual algebra, logic, patterns, and combinatorics: no single\nmodel excels in every topic. Second, while most models see improved accuracy\nwith questions that do not have images, the gain is often limited; performance\nfor some remains nearly unchanged without visual input, indicating\nunderutilization of diagrammatic information. Third, substantial variation\nexists across languages and difficulty levels: models frequently handle easier\nitems but struggle with advanced geometry and combinatorial reasoning. Notably,\nGemini 2.0 Flash achieves the highest precision on image based tasks, followed\nby Qwen VL 2.5 72B and GPT 4o, though none approach human level performance.\nFourth, a complementary analysis aimed at distinguishing whether models reason\nor simply recite reveals that Gemini and GPT 4o stand out for their structured\nreasoning and consistent accuracy. In contrast, Pixtral and Llama exhibit less\nconsistent reasoning, often defaulting to heuristics or randomness when unable\nto align their outputs with the given answer options.", "AI": {"tldr": "该论文分析了多模态大语言模型（MLLMs）在数学问题解决中的表现，发现模型在几何、视觉代数等领域表现中等，且在不同语言和难度级别上存在显著差异。Gemini 2.0 Flash在图像任务中表现最佳，但所有模型均未达到人类水平。", "motivation": "探索MLLMs在视觉数学问题中的能力，填补其在多语言和多模态数学任务中性能评估的空白。", "method": "通过多语言Kangaroo风格基准测试评估多个MLLMs（如GPT 4o、Gemini 2.0 Flash等）在几何、代数等领域的表现。", "result": "模型在无图像问题中表现略有提升，但整体精度中等；Gemini 2.0 Flash在图像任务中领先，但未达人类水平；GPT 4o和Gemini展现出更强的推理能力。", "conclusion": "MLLMs在视觉数学任务中仍有提升空间，尤其是在复杂推理和多语言支持方面。"}}
{"id": "2506.06877", "pdf": "https://arxiv.org/pdf/2506.06877", "abs": "https://arxiv.org/abs/2506.06877", "authors": ["Jiaxing Guo", "Wenjie Yang", "Shengzhong Zhang", "Tongshan Xu", "Lun Du", "Da Zheng", "Zengfeng Huang"], "title": "Right Is Not Enough: The Pitfalls of Outcome Supervision in Training LLMs for Math Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "Outcome-rewarded Large Language Models (LLMs) have demonstrated remarkable\nsuccess in mathematical problem-solving. However, this success often masks a\ncritical issue: models frequently achieve correct answers through fundamentally\nunsound reasoning processes, a phenomenon indicative of reward hacking. We\nintroduce MathOlympiadEval, a new dataset with fine-grained annotations, which\nreveals a significant gap between LLMs' answer correctness and their low\nprocess correctness. Existing automated methods like LLM-as-a-judge struggle to\nreliably detect these reasoning flaws. To address this, we propose\nParaStepVerifier, a novel methodology for meticulous, step-by-step verification\nof mathematical solutions. ParaStepVerifier identifies incorrect reasoning\nsteps. Empirical results demonstrate that ParaStepVerifier substantially\nimproves the accuracy of identifying flawed solutions compared to baselines,\nespecially for complex, multi-step problems. This offers a more robust path\ntowards evaluating and training LLMs with genuine mathematical reasoning.", "AI": {"tldr": "论文指出，尽管基于结果奖励的大语言模型（LLMs）在数学问题解决上表现优异，但其推理过程常存在根本性缺陷。作者提出MathOlympiadEval数据集和ParaStepVerifier方法，以更精确地检测和验证推理步骤。", "motivation": "现有LLMs在数学问题中常通过不合理的推理得出正确答案（奖励黑客行为），而现有评估方法无法可靠检测这些缺陷。", "method": "提出MathOlympiadEval数据集和ParaStepVerifier方法，逐步验证数学解决方案的推理步骤。", "result": "ParaStepVerifier显著提高了对复杂多步问题中错误推理的检测准确率。", "conclusion": "该方法为更可靠地评估和训练具备真正数学推理能力的LLMs提供了新途径。"}}
{"id": "2506.06928", "pdf": "https://arxiv.org/pdf/2506.06928", "abs": "https://arxiv.org/abs/2506.06928", "authors": ["George Lydakis", "Alexander Hermans", "Ali Athar", "Daan de Geus", "Bastian Leibe"], "title": "How Important are Videos for Training Video LLMs?", "categories": ["cs.CV"], "comment": "Project page on\n  https://visualcomputinginstitute.github.io/videollm-pseudovideo-training/", "summary": "Research into Video Large Language Models (LLMs) has progressed rapidly, with\nnumerous models and benchmarks emerging in just a few years. Typically, these\nmodels are initialized with a pretrained text-only LLM and finetuned on both\nimage- and video-caption datasets. In this paper, we present findings\nindicating that Video LLMs are more capable of temporal reasoning after\nimage-only training than one would assume, and that improvements from\nvideo-specific training are surprisingly small. Specifically, we show that\nimage-trained versions of two LLMs trained with the recent LongVU algorithm\nperform significantly above chance level on TVBench, a temporal reasoning\nbenchmark. Additionally, we introduce a simple finetuning scheme involving\nsequences of annotated images and questions targeting temporal capabilities.\nThis baseline results in temporal reasoning performance close to, and\noccasionally higher than, what is achieved by video-trained LLMs. This suggests\nsuboptimal utilization of rich temporal features found in real video by current\nmodels. Our analysis motivates further research into the mechanisms that allow\nimage-trained LLMs to perform temporal reasoning, as well as into the\nbottlenecks that render current video training schemes inefficient.", "AI": {"tldr": "研究发现，仅通过图像训练的Video LLMs在时间推理能力上表现优于预期，而视频特定训练的改进效果较小。", "motivation": "探讨Video LLMs在时间推理任务中的表现，尤其是图像训练与视频训练的效果差异。", "method": "使用LongVU算法训练的两个LLMs，在TVBench时间推理基准上进行测试，并引入基于图像序列和问题的微调方案。", "result": "图像训练的LLMs在时间推理任务中表现显著高于随机水平，且接近或优于视频训练的LLMs。", "conclusion": "当前视频训练方案未能充分利用视频的丰富时间特征，需进一步研究图像训练LLMs的时间推理机制及视频训练的瓶颈。"}}
{"id": "2506.07428", "pdf": "https://arxiv.org/pdf/2506.07428", "abs": "https://arxiv.org/abs/2506.07428", "authors": ["Yuling Wang", "Zihui Chen", "Pengfei Jiao", "Xiao Wang"], "title": "HeTa: Relation-wise Heterogeneous Graph Foundation Attack Model", "categories": ["cs.AI", "cs.LG"], "comment": "Accepted by IJCAI 2025", "summary": "Heterogeneous Graph Neural Networks (HGNNs) are vulnerable, highlighting the\nneed for tailored attacks to assess their robustness and ensure security.\nHowever, existing HGNN attacks often require complex retraining of parameters\nto generate specific perturbations for new scenarios. Recently, foundation\nmodels have opened new horizons for the generalization of graph neural networks\nby capturing shared semantics across various graph distributions. This leads us\nto ask:Can we design a foundation attack model for HGNNs that enables\ngeneralizable perturbations across different HGNNs, and quickly adapts to new\nheterogeneous graphs (HGs)? Empirical findings reveal that, despite significant\ndifferences in model design and parameter space, different HGNNs surprisingly\nshare common vulnerability patterns from a relation-aware perspective.\nTherefore, we explore how to design foundation HGNN attack criteria by mining\nshared attack units. In this paper, we propose a novel relation-wise\nheterogeneous graph foundation attack model, HeTa. We introduce a foundation\nsurrogate model to align heterogeneity and identify the importance of shared\nrelation-aware attack units. Building on this, we implement a serialized\nrelation-by-relation attack based on the identified relational weights. In this\nway, the perturbation can be transferred to various target HGNNs and easily\nfine-tuned for new HGs. Extensive experiments exhibit powerful attack\nperformances and generalizability of our method.", "AI": {"tldr": "提出了一种基于关系感知的异构图基础攻击模型HeTa，能够生成可迁移的扰动，并快速适应新的异构图。", "motivation": "异构图神经网络（HGNNs）存在脆弱性，现有攻击方法需要复杂参数重训练，无法快速适应新场景。基础模型的出现为设计通用攻击模型提供了可能。", "method": "设计了一种关系感知的基础攻击模型HeTa，通过挖掘共享攻击单元，实现关系级别的攻击，并利用基础代理模型对齐异构性。", "result": "实验表明，HeTa具有强大的攻击性能和泛化能力，能够快速适应新的异构图。", "conclusion": "HeTa为异构图神经网络的安全评估提供了一种高效且通用的攻击方法。"}}
{"id": "2506.06887", "pdf": "https://arxiv.org/pdf/2506.06887", "abs": "https://arxiv.org/abs/2506.06887", "authors": ["Ziheng Qiao", "Houquan Zhou", "Zhenghua Li"], "title": "Mixture of Small and Large Models for Chinese Spelling Check", "categories": ["cs.CL"], "comment": null, "summary": "In the era of large language models (LLMs), the Chinese Spelling Check (CSC)\ntask has seen various LLM methods developed, yet their performance remains\nunsatisfactory. In contrast, fine-tuned BERT-based models, relying on\nhigh-quality in-domain data, show excellent performance but suffer from edit\npattern overfitting. This paper proposes a novel dynamic mixture approach that\neffectively combines the probability distributions of small models and LLMs\nduring the beam search decoding phase, achieving a balanced enhancement of\nprecise corrections from small models and the fluency of LLMs. This approach\nalso eliminates the need for fine-tuning LLMs, saving significant time and\nresources, and facilitating domain adaptation. Comprehensive experiments\ndemonstrate that our mixture approach significantly boosts error correction\ncapabilities, achieving state-of-the-art results across multiple datasets. Our\ncode is available at https://github.com/zhqiao-nlp/MSLLM.", "AI": {"tldr": "本文提出了一种动态混合方法，结合小模型和LLM的概率分布，提升中文拼写检查任务的性能，无需微调LLM。", "motivation": "现有LLM方法在中文拼写检查任务中表现不佳，而BERT小模型虽性能优秀但存在编辑模式过拟合问题。", "method": "提出动态混合方法，在beam search解码阶段结合小模型和LLM的概率分布，平衡精确性和流畅性。", "result": "实验表明该方法显著提升纠错能力，在多个数据集上达到最优性能。", "conclusion": "动态混合方法有效结合小模型和LLM的优势，节省资源并提升性能。"}}
{"id": "2506.06944", "pdf": "https://arxiv.org/pdf/2506.06944", "abs": "https://arxiv.org/abs/2506.06944", "authors": ["Mellon M. Zhang", "Glen Chou", "Saibal Mukhopadhyay"], "title": "Polar Hierarchical Mamba: Towards Streaming LiDAR Object Detection with Point Clouds as Egocentric Sequences", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Accurate and efficient object detection is essential for autonomous vehicles,\nwhere real-time perception requires low latency and high throughput. LiDAR\nsensors provide robust depth information, but conventional methods process full\n360{\\deg} scans in a single pass, introducing significant delay. Streaming\napproaches address this by sequentially processing partial scans in the native\npolar coordinate system, yet they rely on translation-invariant convolutions\nthat are misaligned with polar geometry -- resulting in degraded performance or\nrequiring complex distortion mitigation. Recent Mamba-based state space models\n(SSMs) have shown promise for LiDAR perception, but only in the full-scan\nsetting, relying on geometric serialization and positional embeddings that are\nmemory-intensive and ill-suited to streaming. We propose Polar Hierarchical\nMamba (PHiM), a novel SSM architecture designed for polar-coordinate streaming\nLiDAR. PHiM uses local bidirectional Mamba blocks for intra-sector spatial\nencoding and a global forward Mamba for inter-sector temporal modeling,\nreplacing convolutions and positional encodings with distortion-aware,\ndimensionally-decomposed operations. PHiM sets a new state-of-the-art among\nstreaming detectors on the Waymo Open Dataset, outperforming the previous best\nby 10\\% and matching full-scan baselines at twice the throughput. Code will be\navailable at https://github.com/meilongzhang/Polar-Hierarchical-Mamba .", "AI": {"tldr": "PHiM是一种新型状态空间模型架构，专为极坐标流式LiDAR设计，通过局部双向Mamba块和全局前向Mamba实现高效检测，性能优于现有流式检测器。", "motivation": "自动驾驶需要低延迟、高吞吐的实时感知，传统LiDAR处理方法存在延迟问题，而流式方法因几何不对齐导致性能下降或复杂度增加。", "method": "PHiM采用局部双向Mamba块进行扇区内空间编码，全局前向Mamba进行扇区间时序建模，替代卷积和位置编码，实现失真感知和维度分解操作。", "result": "PHiM在Waymo Open Dataset上性能提升10%，吞吐量翻倍，匹配全扫描基线。", "conclusion": "PHiM为极坐标流式LiDAR提供了一种高效解决方案，显著提升了检测性能和吞吐量。"}}
{"id": "2506.07443", "pdf": "https://arxiv.org/pdf/2506.07443", "abs": "https://arxiv.org/abs/2506.07443", "authors": ["Weijie Shi", "Han Zhu", "Jiaming Ji", "Mengze Li", "Jipeng Zhang", "Ruiyuan Zhang", "Jia Zhu", "Jiajie Xu", "Sirui Han", "Yike Guo"], "title": "LegalReasoner: Step-wised Verification-Correction for Legal Judgment Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Legal judgment prediction (LJP) aims to function as a judge by making final\nrulings based on case claims and facts, which plays a vital role in the\njudicial domain for supporting court decision-making and improving judicial\nefficiency. However, existing methods often struggle with logical errors when\nconducting complex legal reasoning. We propose LegalReasoner, which enhances\nLJP reliability through step-wise verification and correction of the reasoning\nprocess. Specifically, it first identifies dispute points to decompose complex\ncases, and then conducts step-wise reasoning while employing a process verifier\nto validate each step's logic from correctness, progressiveness, and potential\nperspectives. When errors are detected, expert-designed attribution and\nresolution strategies are applied for correction. To fine-tune LegalReasoner,\nwe release the LegalHK dataset, containing 58,130 Hong Kong court cases with\ndetailed annotations of dispute points, step-by-step reasoning chains, and\nprocess verification labels. Experiments demonstrate that LegalReasoner\nsignificantly improves concordance with court decisions from 72.37 to 80.27 on\nLLAMA-3.1-70B. The data is available at\nhttps://huggingface.co/datasets/weijiezz/LegalHK.", "AI": {"tldr": "LegalReasoner通过逐步验证和修正推理过程提升法律判决预测的可靠性，显著提高了与法院判决的一致性。", "motivation": "现有方法在复杂法律推理中易出现逻辑错误，需提升法律判决预测的可靠性。", "method": "LegalReasoner通过分解争议点、逐步推理及验证逻辑步骤，并结合专家设计的修正策略。", "result": "实验显示LegalReasoner将一致性从72.37提升至80.27（基于LLAMA-3.1-70B）。", "conclusion": "LegalReasoner有效提升了法律判决预测的可靠性，并发布了LegalHK数据集支持研究。"}}
{"id": "2506.06888", "pdf": "https://arxiv.org/pdf/2506.06888", "abs": "https://arxiv.org/abs/2506.06888", "authors": ["Hamid Mojarad", "Kevin Tang"], "title": "Automatic Speech Recognition of African American English: Lexical and Contextual Effects", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "submitted to Interspeech 2025", "summary": "Automatic Speech Recognition (ASR) models often struggle with the phonetic,\nphonological, and morphosyntactic features found in African American English\n(AAE). This study focuses on two key AAE variables: Consonant Cluster Reduction\n(CCR) and ING-reduction. It examines whether the presence of CCR and\nING-reduction increases ASR misrecognition. Subsequently, it investigates\nwhether end-to-end ASR systems without an external Language Model (LM) are more\ninfluenced by lexical neighborhood effect and less by contextual predictability\ncompared to systems with an LM. The Corpus of Regional African American\nLanguage (CORAAL) was transcribed using wav2vec 2.0 with and without an LM. CCR\nand ING-reduction were detected using the Montreal Forced Aligner (MFA) with\npronunciation expansion. The analysis reveals a small but significant effect of\nCCR and ING on Word Error Rate (WER) and indicates a stronger presence of\nlexical neighborhood effect in ASR systems without LMs.", "AI": {"tldr": "研究探讨了自动语音识别（ASR）模型在处理非裔美国英语（AAE）时的困难，重点关注辅音簇缩减（CCR）和ING缩减对ASR错误率的影响，并比较了有无语言模型（LM）的端到端ASR系统的表现。", "motivation": "AAE的语音、音系和形态句法特征常导致ASR模型识别错误，研究旨在量化CCR和ING缩减对ASR性能的影响，并分析LM对系统表现的作用。", "method": "使用CORAAL语料库，通过wav2vec 2.0（有/无LM）进行转录，利用Montreal Forced Aligner（MFA）检测CCR和ING缩减，分析其对词错误率（WER）的影响。", "result": "CCR和ING缩减对WER有显著但较小的影响；无LM的ASR系统更易受词汇邻域效应影响。", "conclusion": "研究揭示了AAE特征对ASR的挑战，并强调了LM在减少词汇邻域效应中的作用。"}}
{"id": "2506.06952", "pdf": "https://arxiv.org/pdf/2506.06952", "abs": "https://arxiv.org/abs/2506.06952", "authors": ["Ying Shen", "Zhiyang Xu", "Jiuhai Chen", "Shizhe Diao", "Jiaxin Zhang", "Yuguang Yao", "Joy Rimchala", "Ismini Lourentzou", "Lifu Huang"], "title": "LaTtE-Flow: Layerwise Timestep-Expert Flow-based Transformer", "categories": ["cs.CV"], "comment": "Unified multimodal model, Flow-matching", "summary": "Recent advances in multimodal foundation models unifying image understanding\nand generation have opened exciting avenues for tackling a wide range of\nvision-language tasks within a single framework. Despite progress, existing\nunified models typically require extensive pretraining and struggle to achieve\nthe same level of performance compared to models dedicated to each task.\nAdditionally, many of these models suffer from slow image generation speeds,\nlimiting their practical deployment in real-time or resource-constrained\nsettings. In this work, we propose Layerwise Timestep-Expert Flow-based\nTransformer (LaTtE-Flow), a novel and efficient architecture that unifies image\nunderstanding and generation within a single multimodal model. LaTtE-Flow\nbuilds upon powerful pretrained Vision-Language Models (VLMs) to inherit strong\nmultimodal understanding capabilities, and extends them with a novel Layerwise\nTimestep Experts flow-based architecture for efficient image generation.\nLaTtE-Flow distributes the flow-matching process across specialized groups of\nTransformer layers, each responsible for a distinct subset of timesteps. This\ndesign significantly improves sampling efficiency by activating only a small\nsubset of layers at each sampling timestep. To further enhance performance, we\npropose a Timestep-Conditioned Residual Attention mechanism for efficient\ninformation reuse across layers. Experiments demonstrate that LaTtE-Flow\nachieves strong performance on multimodal understanding tasks, while achieving\ncompetitive image generation quality with around 6x faster inference speed\ncompared to recent unified multimodal models.", "AI": {"tldr": "LaTtE-Flow是一种高效的多模态模型，统一了图像理解和生成，通过分层时间步专家架构和残差注意力机制，显著提升了生成速度和性能。", "motivation": "现有统一模型需要大量预训练且性能不如专用模型，生成速度慢，限制了实际应用。", "method": "基于预训练视觉语言模型，引入分层时间步专家流架构和残差注意力机制，优化生成效率。", "result": "在理解任务上表现优异，生成质量与现有模型相当，推理速度快6倍。", "conclusion": "LaTtE-Flow在高效统一多模态任务上具有潜力，尤其适合实时或资源受限场景。"}}
{"id": "2506.07446", "pdf": "https://arxiv.org/pdf/2506.07446", "abs": "https://arxiv.org/abs/2506.07446", "authors": ["Liwen Zheng", "Chaozhuo Li", "Zheng Liu", "Feiran Huang", "Haoran Jia", "Zaisheng Ye", "Xi Zhang"], "title": "Fact in Fragments: Deconstructing Complex Claims via LLM-based Atomic Fact Extraction and Verification", "categories": ["cs.AI"], "comment": null, "summary": "Fact verification plays a vital role in combating misinformation by assessing\nthe veracity of claims through evidence retrieval and reasoning. However,\ntraditional methods struggle with complex claims requiring multi-hop reasoning\nover fragmented evidence, as they often rely on static decomposition strategies\nand surface-level semantic retrieval, which fail to capture the nuanced\nstructure and intent of the claim. This results in accumulated reasoning\nerrors, noisy evidence contamination, and limited adaptability to diverse\nclaims, ultimately undermining verification accuracy in complex scenarios. To\naddress this, we propose Atomic Fact Extraction and Verification (AFEV), a\nnovel framework that iteratively decomposes complex claims into atomic facts,\nenabling fine-grained retrieval and adaptive reasoning. AFEV dynamically\nrefines claim understanding and reduces error propagation through iterative\nfact extraction, reranks evidence to filter noise, and leverages\ncontext-specific demonstrations to guide the reasoning process. Extensive\nexperiments on five benchmark datasets demonstrate that AFEV achieves\nstate-of-the-art performance in both accuracy and interpretability.", "AI": {"tldr": "AFEV是一种通过迭代分解复杂主张为原子事实的新框架，显著提升了事实验证的准确性和可解释性。", "motivation": "传统方法在处理需要多跳推理的复杂主张时表现不佳，导致推理错误累积和证据污染。", "method": "AFEV通过动态分解主张、重新排序证据和利用上下文示例，实现细粒度检索和自适应推理。", "result": "在五个基准数据集上的实验表明，AFEV在准确性和可解释性上达到最先进水平。", "conclusion": "AFEV通过迭代分解和动态推理，有效解决了复杂主张验证中的问题。"}}
{"id": "2506.06929", "pdf": "https://arxiv.org/pdf/2506.06929", "abs": "https://arxiv.org/abs/2506.06929", "authors": ["Mikhail Krasitskii", "Grigori Sidorov", "Olga Kolesnikova", "Liliana Chanona Hernandez", "Alexander Gelbukh"], "title": "Hybrid Extractive Abstractive Summarization for Multilingual Sentiment Analysis", "categories": ["cs.CL"], "comment": "6 pages", "summary": "We propose a hybrid approach for multilingual sentiment analysis that\ncombines extractive and abstractive summarization to address the limitations of\nstandalone methods. The model integrates TF-IDF-based extraction with a\nfine-tuned XLM-R abstractive module, enhanced by dynamic thresholding and\ncultural adaptation. Experiments across 10 languages show significant\nimprovements over baselines, achieving 0.90 accuracy for English and 0.84 for\nlow-resource languages. The approach also demonstrates 22% greater\ncomputational efficiency than traditional methods. Practical applications\ninclude real-time brand monitoring and cross-cultural discourse analysis.\nFuture work will focus on optimization for low-resource languages via 8-bit\nquantization.", "AI": {"tldr": "提出了一种结合提取式和生成式摘要的多语言情感分析方法，显著提升了准确性和计算效率。", "motivation": "解决现有独立方法的局限性，提升多语言情感分析的性能。", "method": "结合TF-IDF提取和XLM-R生成模块，动态阈值和文化适应优化。", "result": "在10种语言中表现优于基线，英语准确率0.90，低资源语言0.84，计算效率提升22%。", "conclusion": "方法适用于实时品牌监控和跨文化分析，未来将优化低资源语言处理。"}}
{"id": "2506.06953", "pdf": "https://arxiv.org/pdf/2506.06953", "abs": "https://arxiv.org/abs/2506.06953", "authors": ["Maciej Zyrek", "Tomasz Tarasiewicz", "Jakub Sadel", "Aleksandra Krzywon", "Michal Kawulok"], "title": "Task-driven real-world super-resolution of document scans", "categories": ["cs.CV"], "comment": null, "summary": "Single-image super-resolution refers to the reconstruction of a\nhigh-resolution image from a single low-resolution observation. Although recent\ndeep learning-based methods have demonstrated notable success on simulated\ndatasets -- with low-resolution images obtained by degrading and downsampling\nhigh-resolution ones -- they frequently fail to generalize to real-world\nsettings, such as document scans, which are affected by complex degradations\nand semantic variability. In this study, we introduce a task-driven, multi-task\nlearning framework for training a super-resolution network specifically\noptimized for optical character recognition tasks. We propose to incorporate\nauxiliary loss functions derived from high-level vision tasks, including text\ndetection using the connectionist text proposal network, text recognition via a\nconvolutional recurrent neural network, keypoints localization using Key.Net,\nand hue consistency. To balance these diverse objectives, we employ dynamic\nweight averaging mechanism, which adaptively adjusts the relative importance of\neach loss term based on its convergence behavior. We validate our approach upon\nthe SRResNet architecture, which is a well-established technique for\nsingle-image super-resolution. Experimental evaluations on both simulated and\nreal-world scanned document datasets demonstrate that the proposed approach\nimproves text detection, measured with intersection over union, while\npreserving overall image fidelity. These findings underscore the value of\nmulti-objective optimization in super-resolution models for bridging the gap\nbetween simulated training regimes and practical deployment in real-world\nscenarios.", "AI": {"tldr": "论文提出了一种多任务学习框架，通过结合高级视觉任务的辅助损失函数，优化超分辨率网络在光学字符识别任务中的表现。", "motivation": "现有深度学习方法在模拟数据集上表现良好，但在真实场景（如文档扫描）中泛化能力不足，需解决复杂退化和语义变化问题。", "method": "采用多任务学习框架，结合文本检测、识别、关键点定位和色调一致性等辅助损失函数，并使用动态权重平均机制平衡目标。", "result": "实验表明，该方法在模拟和真实文档数据集上提升了文本检测性能（IoU指标），同时保持了图像保真度。", "conclusion": "多目标优化有助于缩小模拟训练与实际应用间的差距，提升超分辨率模型在真实场景中的表现。"}}
{"id": "2506.07450", "pdf": "https://arxiv.org/pdf/2506.07450", "abs": "https://arxiv.org/abs/2506.07450", "authors": ["Yi Loo", "Akshunn Trivedi", "Malika Meghjani"], "title": "Efficient Generation of Diverse Cooperative Agents with World Models", "categories": ["cs.AI"], "comment": null, "summary": "A major bottleneck in the training process for Zero-Shot Coordination (ZSC)\nagents is the generation of partner agents that are diverse in collaborative\nconventions. Current Cross-play Minimization (XPM) methods for population\ngeneration can be very computationally expensive and sample inefficient as the\ntraining objective requires sampling multiple types of trajectories. Each\npartner agent in the population is also trained from scratch, despite all of\nthe partners in the population learning policies of the same coordination task.\nIn this work, we propose that simulated trajectories from the dynamics model of\nan environment can drastically speed up the training process for XPM methods.\nWe introduce XPM-WM, a framework for generating simulated trajectories for XPM\nvia a learned World Model (WM). We show XPM with simulated trajectories removes\nthe need to sample multiple trajectories. In addition, we show our proposed\nmethod can effectively generate partners with diverse conventions that match\nthe performance of previous methods in terms of SP population training reward\nas well as training partners for ZSC agents. Our method is thus, significantly\nmore sample efficient and scalable to a larger number of partners.", "AI": {"tldr": "XPM-WM框架通过世界模型生成模拟轨迹，显著提高了零样本协调（ZSC）代理的训练效率，减少了计算成本。", "motivation": "当前XPM方法在生成多样化的协作伙伴代理时计算成本高且样本效率低。", "method": "提出XPM-WM框架，利用学习的世界模型生成模拟轨迹，避免多次采样。", "result": "XPM-WM在保持性能的同时显著提高了样本效率和可扩展性。", "conclusion": "XPM-WM是一种更高效且可扩展的ZSC代理训练方法。"}}
{"id": "2506.06930", "pdf": "https://arxiv.org/pdf/2506.06930", "abs": "https://arxiv.org/abs/2506.06930", "authors": ["Alexander Spangher", "Tenghao Huang", "Jialiang Gu", "Jiatong Shi", "Muhao Chen"], "title": "DiscoSum: Discourse-aware News Summarization", "categories": ["cs.CL", "cs.AI"], "comment": "8 pages, 3 figures, 10 pages in Appendix", "summary": "Recent advances in text summarization have predominantly leveraged large\nlanguage models to generate concise summaries. However, language models often\ndo not maintain long-term discourse structure, especially in news articles,\nwhere organizational flow significantly influences reader engagement. We\nintroduce a novel approach to integrating discourse structure into\nsummarization processes, focusing specifically on news articles across various\nmedia. We present a novel summarization dataset where news articles are\nsummarized multiple times in different ways across different social media\nplatforms (e.g. LinkedIn, Facebook, etc.). We develop a novel news discourse\nschema to describe summarization structures and a novel algorithm, DiscoSum,\nwhich employs beam search technique for structure-aware summarization, enabling\nthe transformation of news stories to meet different stylistic and structural\ndemands. Both human and automatic evaluation results demonstrate the efficacy\nof our approach in maintaining narrative fidelity and meeting structural\nrequirements.", "AI": {"tldr": "论文提出了一种结合新闻语篇结构的文本摘要方法DiscoSum，通过多平台数据集和结构感知算法提升摘要质量。", "motivation": "现有语言模型在新闻摘要中难以保持长期语篇结构，影响读者参与度。", "method": "开发了新闻语篇模式和多平台摘要数据集，提出基于束搜索的结构感知算法DiscoSum。", "result": "人工和自动评估表明，该方法能有效保持叙事忠实度并满足结构需求。", "conclusion": "结合语篇结构的摘要方法在新闻领域具有显著优势。"}}
{"id": "2506.06962", "pdf": "https://arxiv.org/pdf/2506.06962", "abs": "https://arxiv.org/abs/2506.06962", "authors": ["Jingyuan Qi", "Zhiyang Xu", "Qifan Wang", "Lifu Huang"], "title": "AR-RAG: Autoregressive Retrieval Augmentation for Image Generation", "categories": ["cs.CV"], "comment": "Image Generation, Retrieval Augmented Generation", "summary": "We introduce Autoregressive Retrieval Augmentation (AR-RAG), a novel paradigm\nthat enhances image generation by autoregressively incorporating knearest\nneighbor retrievals at the patch level. Unlike prior methods that perform a\nsingle, static retrieval before generation and condition the entire generation\non fixed reference images, AR-RAG performs context-aware retrievals at each\ngeneration step, using prior-generated patches as queries to retrieve and\nincorporate the most relevant patch-level visual references, enabling the model\nto respond to evolving generation needs while avoiding limitations (e.g.,\nover-copying, stylistic bias, etc.) prevalent in existing methods. To realize\nAR-RAG, we propose two parallel frameworks: (1) Distribution-Augmentation in\nDecoding (DAiD), a training-free plug-and-use decoding strategy that directly\nmerges the distribution of model-predicted patches with the distribution of\nretrieved patches, and (2) Feature-Augmentation in Decoding (FAiD), a\nparameter-efficient fine-tuning method that progressively smooths the features\nof retrieved patches via multi-scale convolution operations and leverages them\nto augment the image generation process. We validate the effectiveness of\nAR-RAG on widely adopted benchmarks, including Midjourney-30K, GenEval and\nDPG-Bench, demonstrating significant performance gains over state-of-the-art\nimage generation models.", "AI": {"tldr": "AR-RAG是一种新的图像生成方法，通过逐步检索和整合相关视觉参考来提升生成质量，避免了现有方法的局限性。", "motivation": "现有方法在生成前进行静态检索，可能导致过度复制或风格偏差，AR-RAG旨在通过动态检索解决这些问题。", "method": "提出了两种框架：DAiD（训练自由解码策略）和FAiD（参数高效微调方法），分别在解码过程中增强分布和特征。", "result": "在多个基准测试（如Midjourney-30K、GenEval和DPG-Bench）上表现优于现有图像生成模型。", "conclusion": "AR-RAG通过动态检索和逐步整合参考，显著提升了图像生成的灵活性和质量。"}}
{"id": "2506.07527", "pdf": "https://arxiv.org/pdf/2506.07527", "abs": "https://arxiv.org/abs/2506.07527", "authors": ["Lu Ma", "Hao Liang", "Meiyi Qiang", "Lexiang Tang", "Xiaochen Ma", "Zhen Hao Wong", "Junbo Niu", "Chengyu Shen", "Runming He", "Bin Cui", "Wentao Zhang"], "title": "Learning What Reinforcement Learning Can't: Interleaved Online Fine-Tuning for Hardest Questions", "categories": ["cs.AI", "cs.LG"], "comment": "12 pages, 5 figures", "summary": "Recent advances in large language model (LLM) reasoning have shown that\nsophisticated behaviors such as planning and self-reflection can emerge through\nreinforcement learning (RL). However, despite these successes, RL in its\ncurrent form remains insufficient to induce capabilities that exceed the\nlimitations of the base model, as it is primarily optimized based on existing\nknowledge of the model rather than facilitating the acquisition of new\ninformation. To address this limitation, we employ supervised fine-tuning (SFT)\nto learn what RL cannot, which enables the incorporation of new knowledge and\nreasoning patterns by leveraging high-quality demonstration data. We analyze\nthe training dynamics of RL and SFT for LLM reasoning and find that RL excels\nat maintaining and improving performance on questions within the model's\noriginal capabilities, while SFT is more effective at enabling progress on\nquestions beyond the current scope of the model. Motivated by the complementary\nstrengths of RL and SFT, we introduce a novel training approach,\n\\textbf{ReLIFT} (\\textbf{Re}inforcement \\textbf{L}earning \\textbf{I}nterleaved\nwith Online \\textbf{F}ine-\\textbf{T}uning). In ReLIFT, the model is primarily\ntrained using RL, but when it encounters challenging questions, high-quality\nsolutions are collected for fine-tuning, and the training process alternates\nbetween RL and fine-tuning to enhance the model's reasoning abilities. ReLIFT\nachieves an average improvement of over +5.2 points across five\ncompetition-level benchmarks and one out-of-distribution benchmark compared to\nother zero-RL models. Furthermore, we demonstrate that ReLIFT outperforms both\nRL and SFT while using only 13\\% of the detailed demonstration data,\nhighlighting its scalability. These results provide compelling evidence that\nReLIFT overcomes the fundamental limitations of RL and underscores the\nsignificant potential.", "AI": {"tldr": "论文提出了一种结合强化学习（RL）和监督微调（SFT）的新训练方法ReLIFT，以克服RL在扩展语言模型能力方面的局限性，并在多个基准测试中取得了显著提升。", "motivation": "尽管RL在语言模型推理中表现出色，但其无法超越模型原有能力的限制。为了引入新知识和推理模式，需要结合SFT的优势。", "method": "提出ReLIFT方法，交替使用RL和SFT训练模型，特别是在遇到困难问题时收集高质量解决方案进行微调。", "result": "ReLIFT在五个竞赛级基准和一个分布外基准上平均提升了5.2分，且仅需13%的详细演示数据。", "conclusion": "ReLIFT有效克服了RL的局限性，展示了结合RL和SFT的潜力。"}}
{"id": "2506.06950", "pdf": "https://arxiv.org/pdf/2506.06950", "abs": "https://arxiv.org/abs/2506.06950", "authors": ["Do Xuan Long", "Duy Dinh", "Ngoc-Hai Nguyen", "Kenji Kawaguchi", "Nancy F. Chen", "Shafiq Joty", "Min-Yen Kan"], "title": "What Makes a Good Natural Language Prompt?", "categories": ["cs.CL"], "comment": "ACL 2025 Main Conference", "summary": "As large language models (LLMs) have progressed towards more human-like and\nhuman--AI communications have become prevalent, prompting has emerged as a\ndecisive component. However, there is limited conceptual consensus on what\nexactly quantifies natural language prompts. We attempt to address this\nquestion by conducting a meta-analysis surveying more than 150\nprompting-related papers from leading NLP and AI conferences from 2022 to 2025\nand blogs. We propose a property- and human-centric framework for evaluating\nprompt quality, encompassing 21 properties categorized into six dimensions. We\nthen examine how existing studies assess their impact on LLMs, revealing their\nimbalanced support across models and tasks, and substantial research gaps.\nFurther, we analyze correlations among properties in high-quality natural\nlanguage prompts, deriving prompting recommendations. We then empirically\nexplore multi-property prompt enhancements in reasoning tasks, observing that\nsingle-property enhancements often have the greatest impact. Finally, we\ndiscover that instruction-tuning on property-enhanced prompts can result in\nbetter reasoning models. Our findings establish a foundation for\nproperty-centric prompt evaluation and optimization, bridging the gaps between\nhuman--AI communication and opening new prompting research directions.", "AI": {"tldr": "该论文通过分析150多篇关于提示的文献，提出了一个基于21个属性的框架来评估提示质量，并揭示了现有研究的不足。通过实验发现，单一属性增强对推理任务影响最大，且基于属性增强提示的指令调优能提升模型性能。", "motivation": "随着大语言模型（LLMs）的发展，提示在人类与AI交互中的重要性日益凸显，但目前缺乏对自然语言提示质量的量化共识。", "method": "通过文献综述和元分析，提出一个包含21个属性的六维度框架，评估提示质量，并通过实验验证属性增强对推理任务的影响。", "result": "研究发现现有研究对提示属性的支持不均衡，单一属性增强对推理任务效果显著，且基于属性增强提示的指令调优能提升模型性能。", "conclusion": "该研究为提示质量的评估和优化奠定了基础，为人类与AI交互提供了新的研究方向。"}}
{"id": "2506.06966", "pdf": "https://arxiv.org/pdf/2506.06966", "abs": "https://arxiv.org/abs/2506.06966", "authors": ["Siyuan Jing", "Guangxue Wang", "Haoyang Zhai", "Qin Tao", "Jun Yang", "Bing Wang", "Peng Jin"], "title": "Dual-view Spatio-Temporal Feature Fusion with CNN-Transformer Hybrid Network for Chinese Isolated Sign Language Recognition", "categories": ["cs.CV"], "comment": "18 pages, 3 figures", "summary": "Due to the emergence of many sign language datasets, isolated sign language\nrecognition (ISLR) has made significant progress in recent years. In addition,\nthe development of various advanced deep neural networks is another reason for\nthis breakthrough. However, challenges remain in applying the technique in the\nreal world. First, existing sign language datasets do not cover the whole sign\nvocabulary. Second, most of the sign language datasets provide only single view\nRGB videos, which makes it difficult to handle hand occlusions when performing\nISLR. To fill this gap, this paper presents a dual-view sign language dataset\nfor ISLR named NationalCSL-DP, which fully covers the Chinese national sign\nlanguage vocabulary. The dataset consists of 134140 sign videos recorded by ten\nsigners with respect to two vertical views, namely, the front side and the left\nside. Furthermore, a CNN transformer network is also proposed as a strong\nbaseline and an extremely simple but effective fusion strategy for prediction.\nExtensive experiments were conducted to prove the effectiveness of the datasets\nas well as the baseline. The results show that the proposed fusion strategy can\nsignificantly increase the performance of the ISLR, but it is not easy for the\nsequence-to-sequence model, regardless of whether the early-fusion or\nlate-fusion strategy is applied, to learn the complementary features from the\nsign videos of two vertical views.", "AI": {"tldr": "该论文提出了一个双视角的中国手语数据集NationalCSL-DP，并提出了一个CNN-Transformer网络作为基线模型，通过实验验证了数据集和融合策略的有效性。", "motivation": "现有手语数据集覆盖不全且多为单视角，难以处理手部遮挡问题，因此需要构建更全面的双视角数据集以提升孤立手语识别（ISLR）的性能。", "method": "构建了覆盖中国国家手语词汇的双视角数据集NationalCSL-DP，并提出CNN-Transformer网络及简单有效的融合策略。", "result": "实验表明，提出的融合策略显著提升了ISLR性能，但序列到序列模型难以从双视角视频中学习互补特征。", "conclusion": "双视角数据集和融合策略为ISLR提供了新思路，但需进一步优化模型以更好地利用多视角信息。"}}
{"id": "2506.07528", "pdf": "https://arxiv.org/pdf/2506.07528", "abs": "https://arxiv.org/abs/2506.07528", "authors": ["Qisheng Hu", "Quanyu Long", "Wenya Wang"], "title": "Coordinating Search-Informed Reasoning and Reasoning-Guided Search in Claim Verification", "categories": ["cs.AI"], "comment": "19 pages, 9 figures", "summary": "Multi-hop claim verification is inherently challenging, requiring multi-step\nreasoning to construct verification chains while iteratively searching for\ninformation to uncover hidden bridging facts. This process is fundamentally\ninterleaved, as effective reasoning relies on dynamically retrieved evidence,\nwhile effective search demands reasoning to refine queries based on partial\ninformation. To achieve this, we propose Hierarchical Agent Reasoning and\nInformation Search (HARIS), explicitly modeling the coordinated process of\nreasoning-driven searching and search-informed reasoning. HARIS consists of a\nhigh-level reasoning agent that focuses on constructing the main verification\nchain, generating factual questions when more information is needed, and a\nlow-level search agent that iteratively retrieves more information, refining\nits search based on intermediate findings. This design allows each agent to\nspecialize in its respective task, enhancing verification accuracy and\ninterpretability. HARIS is trained using reinforcement learning with\noutcome-based rewards. Experimental results on the EX-FEVER and HOVER\nbenchmarks demonstrate that HARIS achieves strong performance, greatly\nadvancing multi-hop claim verification.", "AI": {"tldr": "HARIS模型通过分层代理（推理与搜索）实现多跳声明验证，结合强化学习提升性能。", "motivation": "多跳声明验证需要动态检索证据与推理交替进行，现有方法难以协调两者。", "method": "提出HARIS模型，包含高层推理代理（构建验证链）和低层搜索代理（动态检索信息），通过强化学习训练。", "result": "在EX-FEVER和HOVER基准测试中表现优异，显著提升多跳验证性能。", "conclusion": "HARIS通过分层代理设计有效协调推理与搜索，验证准确性和可解释性均得到提升。"}}
{"id": "2506.06955", "pdf": "https://arxiv.org/pdf/2506.06955", "abs": "https://arxiv.org/abs/2506.06955", "authors": ["Ha-Thanh Nguyen", "Chaoran Liu", "Hirokazu Kiyomaru", "Koichi Takeda", "Yusuke Miyao", "Maki Matsuda", "Yusuke Oda", "Pontus Stenetorp", "Qianying Liu", "Su Myat Noe", "Hideyuki Tachibana", "Kouta Nakayama", "Sadao Kurohashi"], "title": "BIS Reasoning 1.0: The First Large-Scale Japanese Benchmark for Belief-Inconsistent Syllogistic Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present BIS Reasoning 1.0, the first large-scale Japanese dataset of\nsyllogistic reasoning problems explicitly designed to evaluate\nbelief-inconsistent reasoning in large language models (LLMs). Unlike prior\ndatasets such as NeuBAROCO and JFLD, which focus on general or belief-aligned\nreasoning, BIS Reasoning 1.0 introduces logically valid yet belief-inconsistent\nsyllogisms to uncover reasoning biases in LLMs trained on human-aligned\ncorpora. We benchmark state-of-the-art models - including GPT models, Claude\nmodels, and leading Japanese LLMs - revealing significant variance in\nperformance, with GPT-4o achieving 79.54% accuracy. Our analysis identifies\ncritical weaknesses in current LLMs when handling logically valid but\nbelief-conflicting inputs. These findings have important implications for\ndeploying LLMs in high-stakes domains such as law, healthcare, and scientific\nliterature, where truth must override intuitive belief to ensure integrity and\nsafety.", "AI": {"tldr": "BIS Reasoning 1.0是首个针对大型语言模型（LLMs）设计的日本三段论推理数据集，专注于评估信念不一致的推理能力。", "motivation": "现有数据集如NeuBAROCO和JFLD主要关注通用或信念一致的推理，而BIS Reasoning 1.0旨在揭示LLMs在信念冲突时的推理偏差。", "method": "通过引入逻辑有效但信念不一致的三段论问题，对GPT、Claude及领先的日本LLMs进行基准测试。", "result": "GPT-4o表现最佳，准确率为79.54%，但LLMs在处理信念冲突时存在显著弱点。", "conclusion": "研究结果对法律、医疗等高风险领域部署LLMs具有重要启示，需确保逻辑优先于直觉信念。"}}
{"id": "2506.06970", "pdf": "https://arxiv.org/pdf/2506.06970", "abs": "https://arxiv.org/abs/2506.06970", "authors": ["Pengfei Zhao", "Rongbo Luan", "Wei Zhang", "Peng Wu", "Sifeng He"], "title": "Guiding Cross-Modal Representations with MLLM Priors via Preference Alignment", "categories": ["cs.CV"], "comment": null, "summary": "Despite Contrastive Language-Image Pretraining (CLIP)'s remarkable capability\nto retrieve content across modalities, a substantial modality gap persists in\nits feature space. Intriguingly, we discover that off-the-shelf MLLMs\n(Multimodal Large Language Models) demonstrate powerful inherent modality\nalignment properties. While recent MLLM-based retrievers with unified\narchitectures partially mitigate this gap, their reliance on coarse modality\nalignment mechanisms fundamentally limits their potential. In this work, We\nintroduce MAPLE (Modality-Aligned Preference Learning for Embeddings), a novel\nframework that leverages the fine grained alignment priors inherent in MLLM to\nguide cross modal representation learning. MAPLE formulates the learning\nprocess as reinforcement learning with two key components: (1) Automatic\npreference data construction using off-the-shelf MLLM, and (2) a new Relative\nPreference Alignment (RPA) loss, which adapts Direct Preference Optimization\n(DPO) to the embedding learning setting. Experimental results show that our\npreference-guided alignment achieves substantial gains in fine-grained\ncross-modal retrieval, underscoring its effectiveness in handling nuanced\nsemantic distinctions.", "AI": {"tldr": "论文提出MAPLE框架，利用MLLM的细粒度对齐先验指导跨模态表示学习，通过强化学习和新的RPA损失显著提升细粒度跨模态检索性能。", "motivation": "尽管CLIP在多模态检索中表现出色，但其特征空间中仍存在显著的模态间隙。现有MLLM虽具备对齐能力，但依赖粗粒度机制限制了潜力。", "method": "提出MAPLE框架，结合MLLM自动构建偏好数据，并设计RPA损失（基于DPO）进行嵌入学习。", "result": "实验表明，MAPLE在细粒度跨模态检索中取得显著提升。", "conclusion": "MAPLE通过细粒度对齐机制有效解决了模态间隙问题，提升了跨模态检索的语义区分能力。"}}
{"id": "2506.07548", "pdf": "https://arxiv.org/pdf/2506.07548", "abs": "https://arxiv.org/abs/2506.07548", "authors": ["Weiqiang Jin", "Hongyang Du", "Guizhong Liu", "Dong In Kim"], "title": "Curriculum Learning With Counterfactual Group Relative Policy Advantage For Multi-Agent Reinforcement Learning", "categories": ["cs.AI", "cs.RO"], "comment": "16 pages; 12figures", "summary": "Multi-agent reinforcement learning (MARL) has achieved strong performance in\ncooperative adversarial tasks. However, most existing methods typically train\nagents against fixed opponent strategies and rely on such meta-static\ndifficulty conditions, which limits their adaptability to changing environments\nand often leads to suboptimal policies. Inspired by the success of curriculum\nlearning (CL) in supervised tasks, we propose a dynamic CL framework for MARL\nthat employs an self-adaptive difficulty adjustment mechanism. This mechanism\ncontinuously modulates opponent strength based on real-time agent training\nperformance, allowing agents to progressively learn from easier to more\nchallenging scenarios. However, the dynamic nature of CL introduces instability\ndue to nonstationary environments and sparse global rewards. To address this\nchallenge, we develop a Counterfactual Group Relative Policy Advantage (CGRPA),\nwhich is tightly coupled with the curriculum by providing intrinsic credit\nsignals that reflect each agent's impact under evolving task demands. CGRPA\nconstructs a counterfactual advantage function that isolates individual\ncontributions within group behavior, facilitating more reliable policy updates\nthroughout the curriculum. CGRPA evaluates each agent's contribution through\nconstructing counterfactual action advantage function, providing intrinsic\nrewards that enhance credit assignment and stabilize learning under\nnon-stationary conditions. Extensive experiments demonstrate that our method\nimproves both training stability and final performance, achieving competitive\nresults against state-of-the-art methods. The code is available at\nhttps://github.com/NICE-HKU/CL2MARL-SMAC.", "AI": {"tldr": "论文提出了一种动态课程学习框架（CL）用于多智能体强化学习（MARL），通过自适应难度调整机制和反事实组相对策略优势（CGRPA）提升训练稳定性和性能。", "motivation": "现有MARL方法通常针对固定对手策略训练，难以适应动态环境，导致策略次优。受课程学习（CL）在监督任务中的成功启发，作者希望通过动态CL提升MARL的适应性。", "method": "提出动态CL框架，自适应调整对手强度；开发CGRPA方法，通过反事实优势函数评估个体贡献，提供内在奖励以稳定学习。", "result": "实验表明，该方法显著提升了训练稳定性和最终性能，优于现有方法。", "conclusion": "动态CL结合CGRPA有效解决了MARL中的非平稳性和信用分配问题，为复杂任务提供了更优解决方案。"}}
{"id": "2506.06964", "pdf": "https://arxiv.org/pdf/2506.06964", "abs": "https://arxiv.org/abs/2506.06964", "authors": ["Subhojyoti Mukherjee", "Viet Dac Lai", "Raghavendra Addanki", "Ryan Rossi", "Seunghyun Yoon", "Trung Bui", "Anup Rao", "Jayakumar Subramanian", "Branislav Kveton"], "title": "Learning to Clarify by Reinforcement Learning Through Reward-Weighted Fine-Tuning", "categories": ["cs.CL", "cs.LG"], "comment": "39 pages", "summary": "Question answering (QA) agents automatically answer questions posed in\nnatural language. In this work, we learn to ask clarifying questions in QA\nagents. The key idea in our method is to simulate conversations that contain\nclarifying questions and learn from them using reinforcement learning (RL). To\nmake RL practical, we propose and analyze offline RL objectives that can be\nviewed as reward-weighted supervised fine-tuning (SFT) and easily optimized in\nlarge language models. Our work stands in a stark contrast to recently proposed\nmethods, based on SFT and direct preference optimization, which have additional\nhyper-parameters and do not directly optimize rewards. We compare to these\nmethods empirically and report gains in both optimized rewards and language\nquality.", "AI": {"tldr": "论文提出了一种通过强化学习（RL）训练问答（QA）代理学习提出澄清问题的方法，采用离线RL目标优化奖励，优于现有基于监督微调（SFT）和直接偏好优化的方法。", "motivation": "现有QA代理在回答自然语言问题时缺乏主动澄清能力，本文旨在通过学习提出澄清问题来提升其表现。", "method": "通过模拟包含澄清问题的对话，利用强化学习（RL）进行训练，并提出离线RL目标（类似奖励加权的监督微调）以优化模型。", "result": "实验表明，该方法在优化奖励和语言质量上优于基于监督微调（SFT）和直接偏好优化的方法。", "conclusion": "通过离线RL目标优化QA代理的澄清问题能力，显著提升了其表现，且方法简单高效。"}}
{"id": "2506.06988", "pdf": "https://arxiv.org/pdf/2506.06988", "abs": "https://arxiv.org/abs/2506.06988", "authors": ["Binxiao Huang", "Zhihao Li", "Shiyong Liu", "Xiao Tang", "Jiajun Tang", "Jiaqi Lin", "Yuxin Cheng", "Zhenyu Chen", "Xiaofei Wu", "Ngai Wong"], "title": "Hybrid Mesh-Gaussian Representation for Efficient Indoor Scene Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian splatting (3DGS) has demonstrated exceptional performance in\nimage-based 3D reconstruction and real-time rendering. However, regions with\ncomplex textures require numerous Gaussians to capture significant color\nvariations accurately, leading to inefficiencies in rendering speed. To address\nthis challenge, we introduce a hybrid representation for indoor scenes that\ncombines 3DGS with textured meshes. Our approach uses textured meshes to handle\ntexture-rich flat areas, while retaining Gaussians to model intricate\ngeometries. The proposed method begins by pruning and refining the extracted\nmesh to eliminate geometrically complex regions. We then employ a joint\noptimization for 3DGS and mesh, incorporating a warm-up strategy and\ntransmittance-aware supervision to balance their contributions\nseamlessly.Extensive experiments demonstrate that the hybrid representation\nmaintains comparable rendering quality and achieves superior frames per second\nFPS with fewer Gaussian primitives.", "AI": {"tldr": "3D高斯泼溅（3DGS）在图像3D重建和实时渲染中表现优异，但复杂纹理区域需要大量高斯分布，导致渲染效率低。本文提出一种结合3DGS和纹理网格的混合表示方法，优化渲染速度和效果。", "motivation": "解决3DGS在复杂纹理区域需要过多高斯分布导致的渲染效率问题。", "method": "提出混合表示方法，用纹理网格处理平坦纹理区域，保留高斯分布处理复杂几何。通过网格修剪、联合优化和传输感知监督实现平衡。", "result": "混合表示在保持渲染质量的同时，显著提升FPS并减少高斯基元数量。", "conclusion": "混合表示方法有效平衡了渲染效率和质量，适用于室内场景。"}}
{"id": "2506.07553", "pdf": "https://arxiv.org/pdf/2506.07553", "abs": "https://arxiv.org/abs/2506.07553", "authors": ["Jingchao Wang", "Haote Yang", "Jiang Wu", "Yifan He", "Xingjian Wei", "Yinfan Wang", "Chengjin Liu", "Lingli Ge", "Lijun Wu", "Bin Wang", "Dahua Lin", "Conghui He"], "title": "GTR-CoT: Graph Traversal as Visual Chain of Thought for Molecular Structure Recognition", "categories": ["cs.AI", "q-bio.QM"], "comment": null, "summary": "Optical Chemical Structure Recognition (OCSR) is crucial for digitizing\nchemical knowledge by converting molecular images into machine-readable\nformats. While recent vision-language models (VLMs) have shown potential in\nthis task, their image-captioning approach often struggles with complex\nmolecular structures and inconsistent annotations. To overcome these\nchallenges, we introduce GTR-Mol-VLM, a novel framework featuring two key\ninnovations: (1) the \\textit{Graph Traversal as Visual Chain of Thought}\nmechanism that emulates human reasoning by incrementally parsing molecular\ngraphs through sequential atom-bond predictions, and (2) the data-centric\nprinciple of \\textit{Faithfully Recognize What You've Seen}, which addresses\nthe mismatch between abbreviated structures in images and their expanded\nannotations. To support model development, we constructed GTR-CoT-1.3M, a\nlarge-scale instruction-tuning dataset with meticulously corrected annotations,\nand introduced MolRec-Bench, the first benchmark designed for a fine-grained\nevaluation of graph-parsing accuracy in OCSR. Comprehensive experiments\ndemonstrate that GTR-Mol-VLM achieves superior results compared to specialist\nmodels, chemistry-domain VLMs, and commercial general-purpose VLMs. Notably, in\nscenarios involving molecular images with functional group abbreviations,\nGTR-Mol-VLM outperforms the second-best baseline by approximately 14 percentage\npoints, both in SMILES-based and graph-based metrics. We hope that this work\nwill drive OCSR technology to more effectively meet real-world needs, thereby\nadvancing the fields of cheminformatics and AI for Science. We will release\nGTR-CoT at https://github.com/opendatalab/GTR-CoT.", "AI": {"tldr": "GTR-Mol-VLM是一种新型框架，通过图遍历和忠实识别机制改进光学化学结构识别（OCSR），在复杂分子结构和缩写注释上表现优异。", "motivation": "解决现有视觉语言模型（VLMs）在OCSR任务中处理复杂分子结构和不一致注释时的局限性。", "method": "提出GTR-Mol-VLM框架，包含图遍历视觉链机制和忠实识别原则，并使用大规模指令调优数据集GTR-CoT-1.3M和基准MolRec-Bench。", "result": "GTR-Mol-VLM在功能组缩写分子图像上表现优于其他模型，提升约14个百分点。", "conclusion": "该工作有望推动OCSR技术更好地满足实际需求，促进化学信息学和AI科学的发展。"}}
{"id": "2506.06968", "pdf": "https://arxiv.org/pdf/2506.06968", "abs": "https://arxiv.org/abs/2506.06968", "authors": ["Pavel Kovalev", "Carlo Angiuli"], "title": "A dependently-typed calculus of event telicity and culminativity", "categories": ["cs.CL", "cs.LO"], "comment": "52 pages, Agda formalization available at\n  https://doi.org/10.5281/zenodo.15602617", "summary": "We present a dependently-typed cross-linguistic framework for analyzing the\ntelicity and culminativity of events, accompanied by examples of using our\nframework to model English sentences. Our framework consists of two parts. In\nthe nominal domain, we model the boundedness of noun phrases and its\nrelationship to subtyping, delimited quantities, and adjectival modification.\nIn the verbal domain we define a dependent event calculus, modeling telic\nevents as those whose undergoer is bounded, culminating events as telic events\nthat achieve their inherent endpoint, and consider adverbial modification. In\nboth domains we pay particular attention to associated entailments. Our\nframework is defined as an extension of intensional Martin-L\\\"of dependent type\ntheory, and the rules and examples in this paper have been formalized in the\nAgda proof assistant.", "AI": {"tldr": "提出了一种基于依赖类型的跨语言框架，用于分析事件的完成性和终结性，并通过英语句子建模示例展示其应用。", "motivation": "研究事件完成性和终结性的跨语言分析需求，提供一种形式化方法以支持语言学理论。", "method": "框架分为两部分：名词域中建模名词短语的有界性及其与子类型、限定数量和形容词修饰的关系；动词域中定义依赖事件演算，区分完成性事件和终结性事件，并考虑副词修饰。", "result": "框架基于强化Martin-Löf依赖类型理论，并在Agda证明助手中实现，展示了规则和示例的形式化。", "conclusion": "该框架为语言学分析提供了形式化工具，尤其关注相关蕴含关系，适用于跨语言研究。"}}
{"id": "2506.06992", "pdf": "https://arxiv.org/pdf/2506.06992", "abs": "https://arxiv.org/abs/2506.06992", "authors": ["Yanting Gao", "Yepeng Liu", "Junming Liu", "Qi Zhang", "Hongyun Zhang", "Duoqian Miao", "Cairong Zhao"], "title": "Boosting Adversarial Transferability via Commonality-Oriented Gradient Optimization", "categories": ["cs.CV"], "comment": "22 pages", "summary": "Exploring effective and transferable adversarial examples is vital for\nunderstanding the characteristics and mechanisms of Vision Transformers (ViTs).\nHowever, adversarial examples generated from surrogate models often exhibit\nweak transferability in black-box settings due to overfitting. Existing methods\nimprove transferability by diversifying perturbation inputs or applying uniform\ngradient regularization within surrogate models, yet they have not fully\nleveraged the shared and unique features of surrogate models trained on the\nsame task, leading to suboptimal transfer performance. Therefore, enhancing\nperturbations of common information shared by surrogate models and suppressing\nthose tied to individual characteristics offers an effective way to improve\ntransferability. Accordingly, we propose a commonality-oriented gradient\noptimization strategy (COGO) consisting of two components: Commonality\nEnhancement (CE) and Individuality Suppression (IS). CE perturbs the mid-to-low\nfrequency regions, leveraging the fact that ViTs trained on the same dataset\ntend to rely more on mid-to-low frequency information for classification. IS\nemploys adaptive thresholds to evaluate the correlation between backpropagated\ngradients and model individuality, assigning weights to gradients accordingly.\nExtensive experiments demonstrate that COGO significantly improves the transfer\nsuccess rates of adversarial attacks, outperforming current state-of-the-art\nmethods.", "AI": {"tldr": "论文提出了一种共性导向的梯度优化策略（COGO），通过增强共享特征和抑制个体特征，显著提高了对抗样本在ViTs中的迁移性。", "motivation": "理解Vision Transformers（ViTs）的特性与机制需要有效的对抗样本，但现有方法因过拟合导致迁移性不足。", "method": "COGO包含共性增强（CE）和个体抑制（IS）两部分，分别通过扰动中低频区域和自适应梯度权重优化提升迁移性。", "result": "实验表明COGO显著提高了对抗攻击的迁移成功率，优于现有方法。", "conclusion": "COGO通过优化共享和抑制个体特征，为ViTs对抗样本的迁移性提供了有效解决方案。"}}
{"id": "2506.07564", "pdf": "https://arxiv.org/pdf/2506.07564", "abs": "https://arxiv.org/abs/2506.07564", "authors": ["Peiran Li", "Xinkai Zou", "Zhuohang Wu", "Ruifeng Li", "Shuo Xing", "Hanwen Zheng", "Zhikai Hu", "Yuping Wang", "Haoxi Li", "Qin Yuan", "Yingmo Zhang", "Zhengzhong Tu"], "title": "SAFEFLOW: A Principled Protocol for Trustworthy and Transactional Autonomous Agent Systems", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) and vision-language models\n(VLMs) have enabled powerful autonomous agents capable of complex reasoning and\nmulti-modal tool use. Despite their growing capabilities, today's agent\nframeworks remain fragile, lacking principled mechanisms for secure information\nflow, reliability, and multi-agent coordination. In this work, we introduce\nSAFEFLOW, a new protocol-level framework for building trustworthy LLM/VLM-based\nagents. SAFEFLOW enforces fine-grained information flow control (IFC),\nprecisely tracking provenance, integrity, and confidentiality of all the data\nexchanged between agents, tools, users, and environments. By constraining LLM\nreasoning to respect these security labels, SAFEFLOW prevents untrusted or\nadversarial inputs from contaminating high-integrity decisions. To ensure\nrobustness in concurrent multi-agent settings, SAFEFLOW introduces\ntransactional execution, conflict resolution, and secure scheduling over shared\nstate, preserving global consistency across agents. We further introduce\nmechanisms, including write-ahead logging, rollback, and secure caches, that\nfurther enhance resilience against runtime errors and policy violations. To\nvalidate the performances, we built SAFEFLOWBENCH, a comprehensive benchmark\nsuite designed to evaluate agent reliability under adversarial, noisy, and\nconcurrent operational conditions. Extensive experiments demonstrate that\nagents built with SAFEFLOW maintain impressive task performance and security\nguarantees even in hostile environments, substantially outperforming\nstate-of-the-art. Together, SAFEFLOW and SAFEFLOWBENCH lay the groundwork for\nprincipled, robust, and secure agent ecosystems, advancing the frontier of\nreliable autonomy.", "AI": {"tldr": "SAFEFLOW是一个新的协议级框架，旨在构建可信赖的基于LLM/VLM的智能体，通过细粒度信息流控制、事务执行和冲突解决机制，提升多智能体环境中的安全性和可靠性。", "motivation": "当前基于LLM/VLM的智能体框架在信息流安全、可靠性和多智能体协调方面存在不足，缺乏系统化的机制。", "method": "SAFEFLOW通过信息流控制（IFC）、事务执行、冲突解决和安全调度等技术，确保数据的安全性和一致性。", "result": "实验表明，SAFEFLOW在对抗性、噪声和并发条件下显著优于现有技术，同时保持任务性能和安全性。", "conclusion": "SAFEFLOW为构建可靠、安全的智能体生态系统奠定了基础，推动了自主系统的前沿发展。"}}
{"id": "2506.06971", "pdf": "https://arxiv.org/pdf/2506.06971", "abs": "https://arxiv.org/abs/2506.06971", "authors": ["Jaechul Roh", "Varun Gandhi", "Shivani Anilkumar", "Arin Garg"], "title": "Break-The-Chain: Reasoning Failures in LLMs via Adversarial Prompting in Code Generation", "categories": ["cs.CL", "cs.CR"], "comment": null, "summary": "Large Language Models (LLMs) have achieved remarkable success in tasks\nrequiring complex reasoning, such as code generation, mathematical problem\nsolving, and algorithmic synthesis -- especially when aided by reasoning tokens\nand Chain-of-Thought prompting. Yet, a core question remains: do these models\ntruly reason, or do they merely exploit shallow statistical patterns? In this\npaper, we systematically investigate the robustness of reasoning LLMs by\nintroducing a suite of semantically faithful yet adversarially structured\nprompt perturbations. Our evaluation -- spanning 700 perturbed code generations\nderived from LeetCode-style problems -- applies transformations such as\nstorytelling reframing, irrelevant constraint injection, example reordering,\nand numeric perturbation. We observe that while certain modifications severely\ndegrade performance (with accuracy drops up to -42.1%), others surprisingly\nimprove model accuracy by up to 35.3%, suggesting sensitivity not only to\nsemantics but also to surface-level prompt dynamics. These findings expose the\nfragility and unpredictability of current reasoning systems, underscoring the\nneed for more principles approaches to reasoning alignments and prompting\nrobustness. We release our perturbation datasets and evaluation framework to\npromote further research in trustworthy and resilient LLM reasoning.", "AI": {"tldr": "论文研究了大型语言模型（LLMs）在推理任务中的鲁棒性，通过引入对抗性提示扰动，发现模型对表面提示动态敏感，性能可能显著下降或意外提升。", "motivation": "探讨LLMs是否真正具备推理能力，还是仅依赖浅层统计模式。", "method": "使用语义忠实但对抗性结构的提示扰动（如故事重构、无关约束注入等），在700个LeetCode风格问题中评估模型表现。", "result": "某些扰动导致性能下降高达42.1%，而其他扰动意外提升准确率35.3%，显示模型对提示动态敏感。", "conclusion": "当前推理系统脆弱且不可预测，需更原则性的方法提升推理对齐和提示鲁棒性。"}}
{"id": "2506.06993", "pdf": "https://arxiv.org/pdf/2506.06993", "abs": "https://arxiv.org/abs/2506.06993", "authors": ["Cong Guan", "Jiacheng Ying", "Yuya Ieiri", "Osamu Yoshie"], "title": "DM$^3$Net: Dual-Camera Super-Resolution via Domain Modulation and Multi-scale Matching", "categories": ["cs.CV"], "comment": null, "summary": "Dual-camera super-resolution is highly practical for smartphone photography\nthat primarily super-resolve the wide-angle images using the telephoto image as\na reference. In this paper, we propose DM$^3$Net, a novel dual-camera\nsuper-resolution network based on Domain Modulation and Multi-scale Matching.\nTo bridge the domain gap between the high-resolution domain and the degraded\ndomain, we learn two compressed global representations from image pairs\ncorresponding to the two domains. To enable reliable transfer of high-frequency\nstructural details from the reference image, we design a multi-scale matching\nmodule that conducts patch-level feature matching and retrieval across multiple\nreceptive fields to improve matching accuracy and robustness. Moreover, we also\nintroduce Key Pruning to achieve a significant reduction in memory usage and\ninference time with little model performance sacrificed. Experimental results\non three real-world datasets demonstrate that our DM$^3$Net outperforms the\nstate-of-the-art approaches.", "AI": {"tldr": "DM$^3$Net是一种基于域调制和多尺度匹配的双摄像头超分辨率网络，旨在通过参考图像提升广角图像的分辨率。", "motivation": "智能手机摄影中，双摄像头超分辨率技术具有实际应用价值，但需要解决高分辨率域与退化域之间的域差距问题。", "method": "通过学习两个压缩的全局表示，设计多尺度匹配模块进行跨域特征匹配，并引入关键修剪以减少内存和推理时间。", "result": "在三个真实数据集上，DM$^3$Net优于现有方法。", "conclusion": "DM$^3$Net通过域调制和多尺度匹配，有效提升了双摄像头超分辨率的性能。"}}
{"id": "2506.07591", "pdf": "https://arxiv.org/pdf/2506.07591", "abs": "https://arxiv.org/abs/2506.07591", "authors": ["Shang Qu", "Ning Ding", "Linhai Xie", "Yifei Li", "Zaoqu Liu", "Kaiyan Zhang", "Yibai Xiong", "Yuxin Zuo", "Zhangren Chen", "Ermo Hua", "Xingtai Lv", "Youbang Sun", "Yang Li", "Dong Li", "Fuchu He", "Bowen Zhou"], "title": "Automating Exploratory Multiomics Research via Language Models", "categories": ["cs.AI", "q-bio.QM"], "comment": null, "summary": "This paper introduces PROTEUS, a fully automated system that produces\ndata-driven hypotheses from raw data files. We apply PROTEUS to clinical\nproteogenomics, a field where effective downstream data analysis and hypothesis\nproposal is crucial for producing novel discoveries. PROTEUS uses separate\nmodules to simulate different stages of the scientific process, from open-ended\ndata exploration to specific statistical analysis and hypothesis proposal. It\nformulates research directions, tools, and results in terms of relationships\nbetween biological entities, using unified graph structures to manage complex\nresearch processes. We applied PROTEUS to 10 clinical multiomics datasets from\npublished research, arriving at 360 total hypotheses. Results were evaluated\nthrough external data validation and automatic open-ended scoring. Through\nexploratory and iterative research, the system can navigate high-throughput and\nheterogeneous multiomics data to arrive at hypotheses that balance reliability\nand novelty. In addition to accelerating multiomic analysis, PROTEUS represents\na path towards tailoring general autonomous systems to specialized scientific\ndomains to achieve open-ended hypothesis generation from data.", "AI": {"tldr": "PROTEUS是一个全自动系统，能从原始数据文件生成数据驱动的假设，应用于临床蛋白质组学领域。", "motivation": "临床蛋白质组学需要高效的数据分析和假设生成以推动新发现，PROTEUS旨在解决这一问题。", "method": "PROTEUS通过模块化设计模拟科学过程的不同阶段，使用统一图结构管理复杂研究流程。", "result": "在10个临床多组学数据集中生成了360个假设，并通过外部验证和自动评分评估结果。", "conclusion": "PROTEUS展示了如何将通用自主系统定制为专业科学领域工具，实现数据驱动的开放假设生成。"}}
{"id": "2506.06972", "pdf": "https://arxiv.org/pdf/2506.06972", "abs": "https://arxiv.org/abs/2506.06972", "authors": ["Yuji Zhang", "Qingyun Wang", "Cheng Qian", "Jiateng Liu", "Chenkai Sun", "Denghui Zhang", "Tarek Abdelzaher", "Chengxiang Zhai", "Preslav Nakov", "Heng Ji"], "title": "Atomic Reasoning for Scientific Table Claim Verification", "categories": ["cs.CL"], "comment": null, "summary": "Scientific texts often convey authority due to their technical language and\ncomplex data. However, this complexity can sometimes lead to the spread of\nmisinformation. Non-experts are particularly susceptible to misleading claims\nbased on scientific tables due to their high information density and perceived\ncredibility. Existing table claim verification models, including\nstate-of-the-art large language models (LLMs), often struggle with precise\nfine-grained reasoning, resulting in errors and a lack of precision in\nverifying scientific claims. Inspired by Cognitive Load Theory, we propose that\nenhancing a model's ability to interpret table-based claims involves reducing\ncognitive load by developing modular, reusable reasoning components (i.e.,\natomic skills). We introduce a skill-chaining schema that dynamically composes\nthese skills to facilitate more accurate and generalizable reasoning with a\nreduced cognitive load. To evaluate this, we create SciAtomicBench, a\ncross-domain benchmark with fine-grained reasoning annotations. With only 350\nfine-tuning examples, our model trained by atomic reasoning outperforms\nGPT-4o's chain-of-thought method, achieving state-of-the-art results with far\nless training data.", "AI": {"tldr": "论文提出了一种基于认知负荷理论的模块化推理方法，通过技能链动态组合原子技能，显著提升了科学表格声明的验证准确性，仅需少量训练数据即可超越GPT-4o的链式思维方法。", "motivation": "科学文本的复杂性和高信息密度可能导致非专家误解或传播错误信息，现有模型在细粒度推理上表现不足。", "method": "提出技能链模式，通过模块化、可复用的原子技能降低认知负荷，提升模型对表格声明的推理能力。", "result": "在仅350个微调样本下，模型表现优于GPT-4o的链式思维方法，达到最先进水平。", "conclusion": "模块化推理方法能有效减少认知负荷，提升科学声明验证的准确性和泛化能力。"}}
{"id": "2506.06995", "pdf": "https://arxiv.org/pdf/2506.06995", "abs": "https://arxiv.org/abs/2506.06995", "authors": ["Xiaoya Zhang"], "title": "Technical Report for ICRA 2025 GOOSE 3D Semantic Segmentation Challenge: Adaptive Point Cloud Understanding for Heterogeneous Robotic Systems", "categories": ["cs.CV"], "comment": "Winner of the GOOSE 3D Semantic Segmentation Challenge at the IEEE\n  ICRA Workshop on Field Robotics 2025", "summary": "This technical report presents the implementation details of the winning\nsolution for the ICRA 2025 GOOSE 3D Semantic Segmentation Challenge. This\nchallenge focuses on semantic segmentation of 3D point clouds from diverse\nunstructured outdoor environments collected from multiple robotic platforms.\nThis problem was addressed by implementing Point Prompt Tuning (PPT) integrated\nwith Point Transformer v3 (PTv3) backbone, enabling adaptive processing of\nheterogeneous LiDAR data through platform-specific conditioning and\ncross-dataset class alignment strategies. The model is trained without\nrequiring additional external data. As a result, this approach achieved\nsubstantial performance improvements with mIoU increases of up to 22.59% on\nchallenging platforms compared to the baseline PTv3 model, demonstrating the\neffectiveness of adaptive point cloud understanding for field robotics\napplications.", "AI": {"tldr": "该技术报告介绍了ICRA 2025 GOOSE 3D语义分割挑战赛获胜方案的实现细节，通过结合Point Prompt Tuning（PPT）和Point Transformer v3（PTv3）骨干网络，实现了对异构LiDAR数据的自适应处理。", "motivation": "解决多机器人平台采集的多样化非结构化室外环境中3D点云的语义分割问题。", "method": "采用Point Prompt Tuning（PPT）与Point Transformer v3（PTv3）结合，通过平台特定条件化和跨数据集类别对齐策略处理异构数据。", "result": "模型在不依赖外部数据的情况下，性能显著提升，mIoU最高提升22.59%。", "conclusion": "该方法展示了自适应点云理解在野外机器人应用中的有效性。"}}
{"id": "2506.07636", "pdf": "https://arxiv.org/pdf/2506.07636", "abs": "https://arxiv.org/abs/2506.07636", "authors": ["Haoran Wang", "Zhenyu Hou", "Yao Wei", "Jie Tang", "Yuxiao Dong"], "title": "SWE-Dev: Building Software Engineering Agents with Training and Inference Scaling", "categories": ["cs.AI"], "comment": "Accepted to Findings of ACL'25", "summary": "Large language models (LLMs) have advanced rapidly from conversational\nproblem solving to addressing real-world tasks involving tool use, such as\nsoftware engineering (SWE). Recent LLM-powered toolkits, such as OpenAI Codex\nand Cursor, have offered end-to-end automation of the software development\nprocess. However, building effective SWE agents remains challenging due to the\nlack of high-quality training data and effective test cases. To address this\nissue, we present SWE-Dev, an SWE agent built upon open-source LLMs. First, we\ndevelop a robust pipeline to synthesize test cases for patch evaluation.\nSecond, we scale up agent trajectories to construct the training data for\nbuilding SWE-Dev. Experiments on the SWE-bench-Verified benchmark show that the\nSWE-Dev models can achieve top performance among all open SWE agents.\nSpecifically, the success rates of the SWE-Dev 7B and 32B parameter models\nreach 23.4% and 36.6%, respectively, outperforming state-of-the-art open-source\nmodels. All code, models, and datasets are publicly available at\nhttps://github.com/THUDM/SWE-Dev.", "AI": {"tldr": "SWE-Dev是一种基于开源大语言模型的软件工程代理，通过合成测试用例和扩展代理轨迹构建训练数据，在SWE-bench-Verified基准测试中表现优异。", "motivation": "当前LLM驱动的工具在软件工程任务中缺乏高质量训练数据和有效测试用例，限制了其性能。", "method": "开发了合成测试用例的管道，并扩展代理轨迹以构建训练数据，从而构建SWE-Dev代理。", "result": "SWE-Dev 7B和32B模型在基准测试中的成功率分别为23.4%和36.6%，优于现有开源模型。", "conclusion": "SWE-Dev展示了开源LLM在软件工程任务中的潜力，并提供了公开可用的代码、模型和数据集。"}}
{"id": "2506.06982", "pdf": "https://arxiv.org/pdf/2506.06982", "abs": "https://arxiv.org/abs/2506.06982", "authors": ["Cong Liu", "Jie Wu", "Weigang Wu", "Xu Chen", "Liang Lin", "Wei-Shi Zheng"], "title": "Chain of Methodologies: Scaling Test Time Computation without Training", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) often struggle with complex reasoning tasks due\nto insufficient in-depth insights in their training data, which are typically\nabsent in publicly available documents. This paper introduces the Chain of\nMethodologies (CoM), an innovative and intuitive prompting framework that\nenhances structured thinking by integrating human methodological insights,\nenabling LLMs to tackle complex tasks with extended reasoning. CoM leverages\nthe metacognitive abilities of advanced LLMs, activating systematic reasoning\nthrought user-defined methodologies without explicit fine-tuning. Experiments\nshow that CoM surpasses competitive baselines, demonstrating the potential of\ntraining-free prompting methods as robust solutions for complex reasoning tasks\nand bridging the gap toward human-level reasoning through human-like\nmethodological insights.", "AI": {"tldr": "论文提出了一种名为Chain of Methodologies (CoM)的提示框架，通过整合人类方法论洞察，提升大型语言模型在复杂推理任务中的表现。", "motivation": "大型语言模型在复杂推理任务中表现不佳，原因是训练数据缺乏深度洞察。", "method": "CoM框架通过用户定义的方法论激活系统推理，无需显式微调。", "result": "实验表明，CoM超越了竞争基线，展示了无训练提示方法的潜力。", "conclusion": "CoM通过人类方法论洞察，为复杂推理任务提供了鲁棒解决方案，并缩小了与人类水平推理的差距。"}}
{"id": "2506.07002", "pdf": "https://arxiv.org/pdf/2506.07002", "abs": "https://arxiv.org/abs/2506.07002", "authors": ["Yunxiao Shi", "Hong Cai", "Jisoo Jeong", "Yinhao Zhu", "Shizhong Han", "Amin Ansari", "Fatih Porikli"], "title": "BePo: Leveraging Birds Eye View and Sparse Points for Efficient and Accurate 3D Occupancy Prediction", "categories": ["cs.CV"], "comment": "Two-page abstract version available at CVPR 2025 Embodied AI Workshop", "summary": "3D occupancy provides fine-grained 3D geometry and semantics for scene\nunderstanding which is critical for autonomous driving. Most existing methods,\nhowever, carry high compute costs, requiring dense 3D feature volume and\ncross-attention to effectively aggregate information. More recent works have\nadopted Bird's Eye View (BEV) or sparse points as scene representation with\nmuch reduced cost, but still suffer from their respective shortcomings. More\nconcretely, BEV struggles with small objects that often experience significant\ninformation loss after being projected to the ground plane. On the other hand,\npoints can flexibly model little objects in 3D, but is inefficient at capturing\nflat surfaces or large objects. To address these challenges, in this paper, we\npresent a novel 3D occupancy prediction approach, BePo, which combines BEV and\nsparse points based representations. We propose a dual-branch design: a\nquery-based sparse points branch and a BEV branch. The 3D information learned\nin the sparse points branch is shared with the BEV stream via cross-attention,\nwhich enriches the weakened signals of difficult objects on the BEV plane. The\noutputs of both branches are finally fused to generate predicted 3D occupancy.\nWe conduct extensive experiments on the Occ3D-nuScenes and Occ3D-Waymo\nbenchmarks that demonstrate the superiority of our proposed BePo. Moreover,\nBePo also delivers competitive inference speed when compared to the latest\nefficient approaches.", "AI": {"tldr": "提出了一种结合BEV和稀疏点表示的新型3D占用预测方法BePo，通过双分支设计提升小物体和大物体的建模能力。", "motivation": "现有方法在3D占用预测中存在高计算成本或信息丢失问题，BEV对小物体表现不佳，稀疏点对大物体效率低。", "method": "采用双分支设计：基于查询的稀疏点分支和BEV分支，通过交叉注意力共享信息，最终融合输出。", "result": "在Occ3D-nuScenes和Occ3D-Waymo基准测试中表现优越，推理速度与最新高效方法相当。", "conclusion": "BePo有效结合BEV和稀疏点的优势，提升了3D占用预测的性能和效率。"}}
{"id": "2506.07672", "pdf": "https://arxiv.org/pdf/2506.07672", "abs": "https://arxiv.org/abs/2506.07672", "authors": ["Yunhe Yan", "Shihe Wang", "Jiajun Du", "Yexuan Yang", "Yuxuan Shan", "Qichen Qiu", "Xianqing Jia", "Xinge Wang", "Xin Yuan", "Xu Han", "Mao Qin", "Yinxiao Chen", "Chen Peng", "Shangguang Wang", "Mengwei Xu"], "title": "MCPWorld: A Unified Benchmarking Testbed for API, GUI, and Hybrid Computer Use Agents", "categories": ["cs.AI"], "comment": null, "summary": "(M)LLM-powered computer use agents (CUA) are emerging as a transformative\ntechnique to automate human-computer interaction. However, existing CUA\nbenchmarks predominantly target GUI agents, whose evaluation methods are\nsusceptible to UI changes and ignore function interactions exposed by\napplication APIs, e.g., Model Context Protocol (MCP). To this end, we propose\nMCPWorld, the first automatic CUA testbed for API, GUI, and API-GUI hybrid\nagents. A key principle of MCPWorld is the use of \"white-box apps\", i.e., those\nwith source code availability and can be revised/re-compiled as needed (e.g.,\nadding MCP support), with two notable advantages:\n  (1) It greatly broadens the design space of CUA, such as what and how the app\nfeatures to be exposed/extracted as CUA-callable APIs.\n  (2) It allows MCPWorld to programmatically verify task completion by directly\nmonitoring application behavior through techniques like dynamic code\ninstrumentation, offering robust, accurate CUA evaluation decoupled from\nspecific agent implementations or UI states.\n  Currently, MCPWorld includes 201 well curated and annotated user tasks,\ncovering diversified use cases and difficulty levels. MCPWorld is also fully\ncontainerized with GPU acceleration support for flexible adoption on different\nOS/hardware environments. Our preliminary experiments, using a representative\nLLM-powered CUA framework, achieve 75.12% task completion accuracy,\nsimultaneously providing initial evidence on the practical effectiveness of\nagent automation leveraging MCP. Overall, we anticipate MCPWorld to facilitate\nand standardize the benchmarking of next-generation computer use agents that\ncan leverage rich external tools. Our code and dataset are publicly available\nat https://github.com/SAAgent/MCPWorld.", "AI": {"tldr": "MCPWorld是一个自动化的计算机使用代理（CUA）测试平台，支持API、GUI及混合代理，通过白盒应用和动态代码检测提供鲁棒评估。", "motivation": "现有CUA基准主要针对GUI代理，易受UI变化影响且忽略API功能交互，因此需要更全面的评估方法。", "method": "提出MCPWorld，利用白盒应用和动态代码检测技术，支持API、GUI及混合代理的自动化测试。", "result": "实验显示任务完成准确率达75.12%，验证了MCPWorld的实用性。", "conclusion": "MCPWorld有望推动下一代CUA的标准化评测，支持丰富外部工具的利用。"}}
{"id": "2506.06987", "pdf": "https://arxiv.org/pdf/2506.06987", "abs": "https://arxiv.org/abs/2506.06987", "authors": ["Senqi Yang", "Dongyu Zhang", "Jing Ren", "Ziqi Xu", "Xiuzhen Zhang", "Yiliao Song", "Hongfei Lin", "Feng Xia"], "title": "Cultural Bias Matters: A Cross-Cultural Benchmark Dataset and Sentiment-Enriched Model for Understanding Multimodal Metaphors", "categories": ["cs.CL"], "comment": "This paper has been accepted to the 63rd Annual Meeting of the\n  Association for Computational Linguistics (ACL 2025), Main Conference", "summary": "Metaphors are pervasive in communication, making them crucial for natural\nlanguage processing (NLP). Previous research on automatic metaphor processing\npredominantly relies on training data consisting of English samples, which\noften reflect Western European or North American biases. This cultural skew can\nlead to an overestimation of model performance and contributions to NLP\nprogress. However, the impact of cultural bias on metaphor processing,\nparticularly in multimodal contexts, remains largely unexplored. To address\nthis gap, we introduce MultiMM, a Multicultural Multimodal Metaphor dataset\ndesigned for cross-cultural studies of metaphor in Chinese and English. MultiMM\nconsists of 8,461 text-image advertisement pairs, each accompanied by\nfine-grained annotations, providing a deeper understanding of multimodal\nmetaphors beyond a single cultural domain. Additionally, we propose\nSentiment-Enriched Metaphor Detection (SEMD), a baseline model that integrates\nsentiment embeddings to enhance metaphor comprehension across cultural\nbackgrounds. Experimental results validate the effectiveness of SEMD on\nmetaphor detection and sentiment analysis tasks. We hope this work increases\nawareness of cultural bias in NLP research and contributes to the development\nof fairer and more inclusive language models. Our dataset and code are\navailable at https://github.com/DUTIR-YSQ/MultiMM.", "AI": {"tldr": "论文提出了一个多文化多模态隐喻数据集MultiMM，用于研究中英文跨文化隐喻，并提出了情感增强隐喻检测模型SEMD，以解决NLP中的文化偏见问题。", "motivation": "现有隐喻处理研究主要依赖英语数据，存在西方文化偏见，影响模型性能和NLP发展。多文化多模态隐喻的影响尚未充分探索。", "method": "引入MultiMM数据集（8,461个文本-图像广告对），并提出SEMD模型，整合情感嵌入以提升跨文化隐喻理解。", "result": "实验验证了SEMD在隐喻检测和情感分析任务中的有效性。", "conclusion": "该研究提高了对NLP文化偏见的认识，推动了更公平、包容的语言模型发展。数据集和代码已开源。"}}
{"id": "2506.07013", "pdf": "https://arxiv.org/pdf/2506.07013", "abs": "https://arxiv.org/abs/2506.07013", "authors": ["Wentao Zhao", "Yihe Niu", "Yanbo Wang", "Tianchen Deng", "Shenghai Yuan", "Zhenli Wang", "Rui Guo", "Jingchuan Wang"], "title": "UNO: Unified Self-Supervised Monocular Odometry for Platform-Agnostic Deployment", "categories": ["cs.CV"], "comment": "15pages, 8 figures", "summary": "This work presents UNO, a unified monocular visual odometry framework that\nenables robust and adaptable pose estimation across diverse environments,\nplatforms, and motion patterns. Unlike traditional methods that rely on\ndeployment-specific tuning or predefined motion priors, our approach\ngeneralizes effectively across a wide range of real-world scenarios, including\nautonomous vehicles, aerial drones, mobile robots, and handheld devices. To\nthis end, we introduce a Mixture-of-Experts strategy for local state\nestimation, with several specialized decoders that each handle a distinct class\nof ego-motion patterns. Moreover, we introduce a fully differentiable\nGumbel-Softmax module that constructs a robust inter-frame correlation graph,\nselects the optimal expert decoder, and prunes erroneous estimates. These cues\nare then fed into a unified back-end that combines pre-trained,\nscale-independent depth priors with a lightweight bundling adjustment to\nenforce geometric consistency. We extensively evaluate our method on three\nmajor benchmark datasets: KITTI (outdoor/autonomous driving), EuRoC-MAV\n(indoor/aerial drones), and TUM-RGBD (indoor/handheld), demonstrating\nstate-of-the-art performance.", "AI": {"tldr": "UNO是一个统一的单目视觉里程计框架，能够在多样环境中实现稳健且适应性强的位姿估计。", "motivation": "传统方法依赖于特定部署的调优或预定义的运动先验，而UNO旨在泛化适用于多种实际场景，如自动驾驶、无人机、移动机器人和手持设备。", "method": "采用Mixture-of-Experts策略进行局部状态估计，结合多个专用解码器处理不同类别的自运动模式，并使用Gumbel-Softmax模块构建帧间相关图、选择最优解码器并剔除错误估计。后端结合预训练的尺度无关深度先验和轻量级捆绑调整。", "result": "在KITTI、EuRoC-MAV和TUM-RGBD三个主要基准数据集上展示了最先进的性能。", "conclusion": "UNO框架通过统一和适应性强的设计，在多样环境中实现了稳健的位姿估计，性能优于传统方法。"}}
{"id": "2506.07731", "pdf": "https://arxiv.org/pdf/2506.07731", "abs": "https://arxiv.org/abs/2506.07731", "authors": ["Mouadh Yagoubi", "Yasser Dahou", "Billel Mokeddem", "Younes Belkada", "Phuc H. Le-Khac", "Basma El Amel Boussaha", "Reda Alami", "Jingwei Zuo", "Damiano Marsili", "Mugariya Farooq", "Mounia Lalmas", "Georgia Gkioxari", "Patrick Gallinari", "Philip Torr", "Hakim Hacid"], "title": "NeurIPS 2025 E2LM Competition : Early Training Evaluation of Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Existing benchmarks have proven effective for assessing the performance of\nfully trained large language models. However, we find striking differences in\nthe early training stages of small models, where benchmarks often fail to\nprovide meaningful or discriminative signals. To explore how these differences\narise, this competition tackles the challenge of designing scientific knowledge\nevaluation tasks specifically tailored for measuring early training progress of\nlanguage models. Participants are invited to develop novel evaluation\nmethodologies or adapt existing benchmarks to better capture performance\ndifferences among language models. To support this effort, we provide three\npre-trained small models (0.5B, 1B, and 3B parameters), along with intermediate\ncheckpoints sampled during training up to 200B tokens. All experiments and\ndevelopment work can be run on widely available free cloud-based GPU platforms,\nmaking participation accessible to researchers with limited computational\nresources. Submissions will be evaluated based on three criteria: the quality\nof the performance signal they produce, the consistency of model rankings at 1\ntrillion tokens of training, and their relevance to the scientific knowledge\ndomain. By promoting the design of tailored evaluation strategies for early\ntraining, this competition aims to attract a broad range of participants from\nvarious disciplines, including those who may not be machine learning experts or\nhave access to dedicated GPU resources. Ultimately, this initiative seeks to\nmake foundational LLM research more systematic and benchmark-informed from the\nearliest phases of model development.", "AI": {"tldr": "论文探讨了现有基准在评估小型语言模型早期训练阶段的局限性，提出了一项竞赛，旨在设计专门用于测量早期训练进度的科学知识评估任务。", "motivation": "现有基准在评估大型语言模型时有效，但在小型模型的早期训练阶段表现不佳，缺乏区分性信号。", "method": "通过竞赛形式，邀请参与者开发或改进评估方法，利用提供的预训练小型模型和中间检查点进行实验。", "result": "竞赛将基于性能信号质量、模型排名一致性和科学知识相关性评估提交方案。", "conclusion": "该竞赛旨在推动早期训练阶段的定制化评估策略设计，使基础LLM研究更系统化。"}}
{"id": "2506.06998", "pdf": "https://arxiv.org/pdf/2506.06998", "abs": "https://arxiv.org/abs/2506.06998", "authors": ["Ming Li", "Zhengyuan Yang", "Xiyao Wang", "Dianqi Li", "Kevin Lin", "Tianyi Zhou", "Lijuan Wang"], "title": "What makes Reasoning Models Different? Follow the Reasoning Leader for Efficient Decoding", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large reasoning models (LRMs) achieve strong reasoning performance by\nemitting long chains of thought. Yet, these verbose traces slow down inference\nand often drift into unnecessary detail, known as the overthinking phenomenon.\nTo better understand LRMs' behavior, we systematically analyze the token-level\nmisalignment between reasoning and non-reasoning models. While it is expected\nthat their primary difference lies in the stylistic \"thinking cues\", LRMs\nuniquely exhibit two pivotal, previously under-explored phenomena: a Global\nMisalignment Rebound, where their divergence from non-reasoning models persists\nor even grows as response length increases, and more critically, a Local\nMisalignment Diminish, where the misalignment concentrates at the \"thinking\ncues\" each sentence starts with but rapidly declines in the remaining of the\nsentence. Motivated by the Local Misalignment Diminish, we propose\nFoReaL-Decoding, a collaborative fast-slow thinking decoding method for\ncost-quality trade-off. In FoReaL-Decoding, a Leading model leads the first few\ntokens for each sentence, and then a weaker draft model completes the following\ntokens to the end of each sentence. FoReaL-Decoding adopts a stochastic gate to\nsmoothly interpolate between the small and the large model. On four popular\nmath-reasoning benchmarks (AIME24, GPQA-Diamond, MATH500, AMC23),\nFoReaL-Decoding reduces theoretical FLOPs by 30 to 50% and trims CoT length by\nup to 40%, while preserving 86 to 100% of model performance. These results\nestablish FoReaL-Decoding as a simple, plug-and-play route to controllable\ncost-quality trade-offs in reasoning-centric tasks.", "AI": {"tldr": "论文提出FoReaL-Decoding方法，通过快速-慢速协作解码减少推理模型的冗余计算，同时保持性能。", "motivation": "大型推理模型（LRMs）在推理过程中常因冗长的思维链导致效率低下和过度思考现象，需研究其行为并提出优化方法。", "method": "提出FoReaL-Decoding，通过主导模型和草稿模型协作解码，利用随机门平滑切换，减少冗余计算。", "result": "在四个数学推理基准测试中，FoReaL-Decoding减少30-50%的FLOPs和40%的思维链长度，同时保持86-100%的性能。", "conclusion": "FoReaL-Decoding为推理任务提供了一种简单、即插即用的成本-质量权衡方案。"}}
{"id": "2506.07015", "pdf": "https://arxiv.org/pdf/2506.07015", "abs": "https://arxiv.org/abs/2506.07015", "authors": ["Qiyu Hou", "Jun Wang"], "title": "TABLET: Table Structure Recognition using Encoder-only Transformers", "categories": ["cs.CV", "cs.LG"], "comment": "ICDAR 2025", "summary": "To address the challenges of table structure recognition, we propose a novel\nSplit-Merge-based top-down model optimized for large, densely populated tables.\nOur approach formulates row and column splitting as sequence labeling tasks,\nutilizing dual Transformer encoders to capture feature interactions. The\nmerging process is framed as a grid cell classification task, leveraging an\nadditional Transformer encoder to ensure accurate and coherent merging. By\neliminating unstable bounding box predictions, our method reduces resolution\nloss and computational complexity, achieving high accuracy while maintaining\nfast processing speed. Extensive experiments on FinTabNet and PubTabNet\ndemonstrate the superiority of our model over existing approaches, particularly\nin real-world applications. Our method offers a robust, scalable, and efficient\nsolution for large-scale table recognition, making it well-suited for\nindustrial deployment.", "AI": {"tldr": "提出了一种基于Split-Merge的表格结构识别方法，通过双Transformer编码器优化处理大型密集表格，减少计算复杂度并提高准确性。", "motivation": "解决表格结构识别中的挑战，特别是针对大型、密集表格的不稳定边界框预测问题。", "method": "将行和列分割任务视为序列标注问题，使用双Transformer编码器；合并任务则通过网格单元分类和额外Transformer编码器实现。", "result": "在FinTabNet和PubTabNet数据集上表现优异，准确率高且处理速度快。", "conclusion": "该方法为大规模表格识别提供了高效、可扩展的解决方案，适合工业应用。"}}
{"id": "2506.07736", "pdf": "https://arxiv.org/pdf/2506.07736", "abs": "https://arxiv.org/abs/2506.07736", "authors": ["Jingnan Zheng", "Xiangtian Ji", "Yijun Lu", "Chenhang Cui", "Weixiang Zhao", "Gelei Deng", "Zhenkai Liang", "An Zhang", "Tat-Seng Chua"], "title": "RSafe: Incentivizing proactive reasoning to build robust and adaptive LLM safeguards", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) continue to exhibit vulnerabilities despite\ndeliberate safety alignment efforts, posing significant risks to users and\nsociety. To safeguard against the risk of policy-violating content,\nsystem-level moderation via external guard models-designed to monitor LLM\ninputs and outputs and block potentially harmful content-has emerged as a\nprevalent mitigation strategy. Existing approaches of training guard models\nrely heavily on extensive human curated datasets and struggle with\nout-of-distribution threats, such as emerging harmful categories or jailbreak\nattacks. To address these limitations, we propose RSafe, an adaptive\nreasoning-based safeguard that conducts guided safety reasoning to provide\nrobust protection within the scope of specified safety policies. RSafe operates\nin two stages: 1) guided reasoning, where it analyzes safety risks of input\ncontent through policy-guided step-by-step reasoning, and 2) reinforced\nalignment, where rule-based RL optimizes its reasoning paths to align with\naccurate safety prediction. This two-stage training paradigm enables RSafe to\ninternalize safety principles to generalize safety protection capability over\nunseen or adversarial safety violation scenarios. During inference, RSafe\naccepts user-specified safety policies to provide enhanced safeguards tailored\nto specific safety requirements.", "AI": {"tldr": "RSafe是一种基于自适应推理的安全保护方法，通过两阶段训练（引导推理和强化对齐）来增强对未知或对抗性安全威胁的泛化能力。", "motivation": "尽管大型语言模型（LLMs）经过安全对齐，但仍存在漏洞，现有保护方法依赖人工数据集且难以应对分布外威胁。", "method": "RSafe采用两阶段方法：1) 引导推理，通过策略引导的逐步推理分析安全风险；2) 强化对齐，利用基于规则的强化学习优化推理路径。", "result": "RSafe能够内化安全原则，泛化到未见或对抗性安全违规场景，并提供用户定制的安全保护。", "conclusion": "RSafe为LLMs提供了一种更灵活、更强大的安全保护机制，能够适应动态的安全需求。"}}
{"id": "2506.07001", "pdf": "https://arxiv.org/pdf/2506.07001", "abs": "https://arxiv.org/abs/2506.07001", "authors": ["Yize Cheng", "Vinu Sankar Sadasivan", "Mehrdad Saberi", "Shoumik Saha", "Soheil Feizi"], "title": "Adversarial Paraphrasing: A Universal Attack for Humanizing AI-Generated Text", "categories": ["cs.CL"], "comment": null, "summary": "The increasing capabilities of Large Language Models (LLMs) have raised\nconcerns about their misuse in AI-generated plagiarism and social engineering.\nWhile various AI-generated text detectors have been proposed to mitigate these\nrisks, many remain vulnerable to simple evasion techniques such as\nparaphrasing. However, recent detectors have shown greater robustness against\nsuch basic attacks. In this work, we introduce Adversarial Paraphrasing, a\ntraining-free attack framework that universally humanizes any AI-generated text\nto evade detection more effectively. Our approach leverages an off-the-shelf\ninstruction-following LLM to paraphrase AI-generated content under the guidance\nof an AI text detector, producing adversarial examples that are specifically\noptimized to bypass detection. Extensive experiments show that our attack is\nboth broadly effective and highly transferable across several detection\nsystems. For instance, compared to simple paraphrasing attack--which,\nironically, increases the true positive at 1% false positive (T@1%F) by 8.57%\non RADAR and 15.03% on Fast-DetectGPT--adversarial paraphrasing, guided by\nOpenAI-RoBERTa-Large, reduces T@1%F by 64.49% on RADAR and a striking 98.96% on\nFast-DetectGPT. Across a diverse set of detectors--including neural\nnetwork-based, watermark-based, and zero-shot approaches--our attack achieves\nan average T@1%F reduction of 87.88% under the guidance of\nOpenAI-RoBERTa-Large. We also analyze the tradeoff between text quality and\nattack success to find that our method can significantly reduce detection\nrates, with mostly a slight degradation in text quality. Our adversarial setup\nhighlights the need for more robust and resilient detection strategies in the\nlight of increasingly sophisticated evasion techniques.", "AI": {"tldr": "论文提出了一种名为Adversarial Paraphrasing的攻击框架，通过使用现成的指令遵循LLM对AI生成文本进行改写，以更有效地绕过检测系统。实验表明，该方法在多种检测系统上具有广泛的有效性和高迁移性。", "motivation": "随着大型语言模型（LLMs）能力的提升，AI生成的抄袭和社会工程问题日益严重。尽管已有多种AI文本检测器，但它们容易受到简单改写攻击的影响。本文旨在提出一种更强大的攻击方法，以揭示现有检测器的脆弱性。", "method": "提出Adversarial Paraphrasing框架，利用现成的指令遵循LLM，在AI文本检测器的指导下改写AI生成内容，生成专门优化的对抗样本以绕过检测。", "result": "实验显示，该方法显著降低了多种检测器的检测率（例如在RADAR和Fast-DetectGPT上分别减少64.49%和98.96%的T@1%F），且文本质量仅轻微下降。", "conclusion": "该攻击框架的成功突显了现有检测策略在面对日益复杂的规避技术时的脆弱性，呼吁开发更鲁棒和弹性的检测方法。"}}
{"id": "2506.07016", "pdf": "https://arxiv.org/pdf/2506.07016", "abs": "https://arxiv.org/abs/2506.07016", "authors": ["Sanjoy Chowdhury", "Mohamed Elmoghany", "Yohan Abeysinghe", "Junjie Fei", "Sayan Nag", "Salman Khan", "Mohamed Elhoseiny", "Dinesh Manocha"], "title": "MAGNET: A Multi-agent Framework for Finding Audio-Visual Needles by Reasoning over Multi-Video Haystacks", "categories": ["cs.CV", "cs.AI"], "comment": "Audio-visual learning, Audio-Visual RAG, Multi-Video Linkage", "summary": "Large multimodal models (LMMs) have shown remarkable progress in audio-visual\nunderstanding, yet they struggle with real-world scenarios that require complex\nreasoning across extensive video collections. Existing benchmarks for video\nquestion answering remain limited in scope, typically involving one clip per\nquery, which falls short of representing the challenges of large-scale,\naudio-visual retrieval and reasoning encountered in practical applications. To\nbridge this gap, we introduce a novel task named AV-HaystacksQA, where the goal\nis to identify salient segments across different videos in response to a query\nand link them together to generate the most informative answer. To this end, we\npresent AVHaystacks, an audio-visual benchmark comprising 3100 annotated QA\npairs designed to assess the capabilities of LMMs in multi-video retrieval and\ntemporal grounding task. Additionally, we propose a model-agnostic, multi-agent\nframework MAGNET to address this challenge, achieving up to 89% and 65%\nrelative improvements over baseline methods on BLEU@4 and GPT evaluation scores\nin QA task on our proposed AVHaystacks. To enable robust evaluation of\nmulti-video retrieval and temporal grounding for optimal response generation,\nwe introduce two new metrics, STEM, which captures alignment errors between a\nground truth and a predicted step sequence and MTGS, to facilitate balanced and\ninterpretable evaluation of segment-level grounding performance. Project:\nhttps://schowdhury671.github.io/magnet_project/", "AI": {"tldr": "论文提出了AV-HaystacksQA任务和AVHaystacks基准，用于评估大型多模态模型在多视频检索和时间定位任务中的能力，并提出了MAGNET框架和两个新评估指标STEM和MTGS。", "motivation": "现有视频问答基准局限于单视频片段，无法满足实际应用中大规模音频-视觉检索和复杂推理的需求。", "method": "提出了AVHaystacks基准和MAGNET多智能体框架，用于多视频检索和时间定位任务。", "result": "MAGNET在BLEU@4和GPT评估分数上分别比基线方法提升了89%和65%。", "conclusion": "AVHaystacks和MAGNET为多视频检索和时间定位任务提供了有效的评估和解决方案。"}}
{"id": "2506.07756", "pdf": "https://arxiv.org/pdf/2506.07756", "abs": "https://arxiv.org/abs/2506.07756", "authors": ["Mark Burgess"], "title": "Agent Semantics, Semantic Spacetime, and Graphical Reasoning", "categories": ["cs.AI", "cs.LG", "cs.MA", "I.2.11; F.4.1; I.2.4; G.2.2"], "comment": null, "summary": "Some formal aspects of the Semantic Spacetime graph model are presented, with\nreference to its use for directed knowledge representations and process\nmodelling. A finite $\\gamma(3,4)$ representation is defined to form a closed\nset of operations that can scale to any degree of semantic complexity. The\nSemantic Spacetime postulates bring predictability with minimal constraints to\npathways in graphs. The ubiquitous appearance of absorbing states in any\npartial graph means that a graph process leaks information. The issue is\nclosely associated with the issue of division by zero, which signals a loss of\nclosure and the need for manual injection of remedial information. The Semantic\nSpacetime model (and its Promise Theory) origins help to clarify how such\nabsorbing states are associated with boundary information where intentionality\ncan enter.", "AI": {"tldr": "论文介绍了语义时空图模型的形式化方面，用于定向知识表示和过程建模，提出了一种有限γ(3,4)表示法，并探讨了吸收态与边界信息的关系。", "motivation": "研究语义时空图模型的形式化特性，以支持可扩展的知识表示和过程建模，同时解决图中吸收态导致的信息泄漏问题。", "method": "定义了一种有限γ(3,4)表示法，形成封闭操作集，并结合语义时空假设和承诺理论分析吸收态与边界信息的关系。", "result": "语义时空假设为图中的路径提供了可预测性，吸收态的出现与信息泄漏相关，边界信息为意图性提供了入口。", "conclusion": "语义时空模型通过边界信息解决吸收态问题，为知识表示和过程建模提供了可扩展且可预测的框架。"}}
{"id": "2506.07032", "pdf": "https://arxiv.org/pdf/2506.07032", "abs": "https://arxiv.org/abs/2506.07032", "authors": ["Bhuiyan Sanjid Shafique", "Ashmal Vayani", "Muhammad Maaz", "Hanoona Abdul Rasheed", "Dinura Dissanayake", "Mohammed Irfan Kurpath", "Yahya Hmaiti", "Go Inoue", "Jean Lahoud", "Md. Safirur Rashid", "Shadid Intisar Quasem", "Maheen Fatima", "Franco Vidal", "Mykola Maslych", "Ketan Pravin More", "Sanoojan Baliah", "Hasindri Watawana", "Yuhao Li", "Fabian Farestam", "Leon Schaller", "Roman Tymtsiv", "Simon Weber", "Hisham Cholakkal", "Ivan Laptev", "Shin'ichi Satoh", "Michael Felsberg", "Mubarak Shah", "Salman Khan", "Fahad Shahbaz Khan"], "title": "A Culturally-diverse Multilingual Multimodal Video Benchmark & Model", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Large multimodal models (LMMs) have recently gained attention due to their\neffectiveness to understand and generate descriptions of visual content. Most\nexisting LMMs are in English language. While few recent works explore\nmultilingual image LMMs, to the best of our knowledge, moving beyond the\nEnglish language for cultural and linguistic inclusivity is yet to be\ninvestigated in the context of video LMMs. In pursuit of more inclusive video\nLMMs, we introduce a multilingual Video LMM benchmark, named ViMUL-Bench, to\nevaluate Video LMMs across 14 languages, including both low- and high-resource\nlanguages: English, Chinese, Spanish, French, German, Hindi, Arabic, Russian,\nBengali, Urdu, Sinhala, Tamil, Swedish, and Japanese. Our ViMUL-Bench is\ndesigned to rigorously test video LMMs across 15 categories including eight\nculturally diverse categories, ranging from lifestyles and festivals to foods\nand rituals and from local landmarks to prominent cultural personalities.\nViMUL-Bench comprises both open-ended (short and long-form) and multiple-choice\nquestions spanning various video durations (short, medium, and long) with 8k\nsamples that are manually verified by native language speakers. In addition, we\nalso introduce a machine translated multilingual video training set comprising\n1.2 million samples and develop a simple multilingual video LMM, named ViMUL,\nthat is shown to provide a better tradeoff between high-and low-resource\nlanguages for video understanding. We hope our ViMUL-Bench and multilingual\nvideo LMM along with a large-scale multilingual video training set will help\nease future research in developing cultural and linguistic inclusive\nmultilingual video LMMs. Our proposed benchmark, video LMM and training data\nwill be publicly released at https://mbzuai-oryx.github.io/ViMUL/.", "AI": {"tldr": "本文介绍了ViMUL-Bench，一个多语言视频LMM基准测试，旨在评估14种语言的视频理解能力，并提出了ViMUL模型和大型多语言视频训练集。", "motivation": "现有的大型多模态模型（LMMs）主要针对英语，缺乏对多语言和文化包容性的研究，尤其是在视频领域。", "method": "提出ViMUL-Bench基准测试，包含14种语言的8k样本，涵盖15个类别；开发ViMUL模型，并构建1.2百万样本的多语言视频训练集。", "result": "ViMUL模型在高资源和低资源语言之间取得了更好的平衡，提升了视频理解能力。", "conclusion": "ViMUL-Bench、ViMUL模型及训练集的发布将促进多语言和文化包容性视频LMM的研究。"}}
{"id": "2506.07045", "pdf": "https://arxiv.org/pdf/2506.07045", "abs": "https://arxiv.org/abs/2506.07045", "authors": ["Yikun Ji", "Hong Yan", "Jun Lan", "Huijia Zhu", "Weiqiang Wang", "Qi Fan", "Liqing Zhang", "Jianfu Zhang"], "title": "Interpretable and Reliable Detection of AI-Generated Images via Grounded Reasoning in MLLMs", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "The rapid advancement of image generation technologies intensifies the demand\nfor interpretable and robust detection methods. Although existing approaches\noften attain high accuracy, they typically operate as black boxes without\nproviding human-understandable justifications. Multi-modal Large Language\nModels (MLLMs), while not originally intended for forgery detection, exhibit\nstrong analytical and reasoning capabilities. When properly fine-tuned, they\ncan effectively identify AI-generated images and offer meaningful explanations.\nHowever, existing MLLMs still struggle with hallucination and often fail to\nalign their visual interpretations with actual image content and human\nreasoning. To bridge this gap, we construct a dataset of AI-generated images\nannotated with bounding boxes and descriptive captions that highlight synthesis\nartifacts, establishing a foundation for human-aligned visual-textual grounded\nreasoning. We then finetune MLLMs through a multi-stage optimization strategy\nthat progressively balances the objectives of accurate detection, visual\nlocalization, and coherent textual explanation. The resulting model achieves\nsuperior performance in both detecting AI-generated images and localizing\nvisual flaws, significantly outperforming baseline methods.", "AI": {"tldr": "论文提出了一种基于多模态大语言模型（MLLMs）的方法，用于检测AI生成图像并提供可解释的视觉定位和文本解释。通过构建标注数据集和多阶段优化策略，模型在检测和定位性能上显著优于基线方法。", "motivation": "现有图像生成检测方法多为黑箱，缺乏可解释性。MLLMs虽具备分析和推理能力，但在视觉解释与人类推理对齐方面存在不足。", "method": "构建标注数据集（含边界框和描述性标注），并通过多阶段优化策略微调MLLMs，平衡检测准确性、视觉定位和文本解释。", "result": "模型在检测AI生成图像和定位视觉缺陷方面表现优异，显著超越基线方法。", "conclusion": "该方法为可解释的AI生成图像检测提供了有效解决方案，同时提升了视觉与文本推理的对齐能力。"}}
{"id": "2506.07759", "pdf": "https://arxiv.org/pdf/2506.07759", "abs": "https://arxiv.org/abs/2506.07759", "authors": ["Diego Forniés-Tabuenca", "Alejandro Uribe", "Urtzi Otamendi", "Arkaitz Artetxe", "Juan Carlos Rivera", "Oier Lopez de Lacalle"], "title": "REMoH: A Reflective Evolution of Multi-objective Heuristics approach via Large Language Models", "categories": ["cs.AI", "cs.NE", "I.2.7; I.2.8; F.2.2"], "comment": "21 pages, 5 tables, 7 figures and 4 appendixes. Pre-print submitted\n  to IEEE Transactions on Evolutionary Computation", "summary": "Multi-objective optimization is fundamental in complex decision-making tasks.\nTraditional algorithms, while effective, often demand extensive\nproblem-specific modeling and struggle to adapt to nonlinear structures. Recent\nadvances in Large Language Models (LLMs) offer enhanced explainability,\nadaptability, and reasoning. This work proposes Reflective Evolution of\nMulti-objective Heuristics (REMoH), a novel framework integrating NSGA-II with\nLLM-based heuristic generation. A key innovation is a reflection mechanism that\nuses clustering and search-space reflection to guide the creation of diverse,\nhigh-quality heuristics, improving convergence and maintaining solution\ndiversity. The approach is evaluated on the Flexible Job Shop Scheduling\nProblem (FJSSP) in-depth benchmarking against state-of-the-art methods using\nthree instance datasets: Dauzere, Barnes, and Brandimarte. Results demonstrate\nthat REMoH achieves competitive results compared to state-of-the-art approaches\nwith reduced modeling effort and enhanced adaptability. These findings\nunderscore the potential of LLMs to augment traditional optimization, offering\ngreater flexibility, interpretability, and robustness in multi-objective\nscenarios.", "AI": {"tldr": "REMoH框架结合NSGA-II与LLM启发式生成，通过反射机制提升多目标优化性能，在FJSSP问题上表现优异。", "motivation": "传统多目标优化算法需要大量问题建模且难以适应非线性结构，LLMs的引入可增强解释性、适应性和推理能力。", "method": "提出REMoH框架，结合NSGA-II与LLM启发式生成，利用聚类和搜索空间反射机制生成多样高质量启发式。", "result": "在FJSSP问题上，REMoH与现有最优方法相比表现竞争性，建模成本更低且适应性更强。", "conclusion": "LLMs可增强传统优化方法，在多目标场景中提供更高灵活性、可解释性和鲁棒性。"}}
{"id": "2506.07037", "pdf": "https://arxiv.org/pdf/2506.07037", "abs": "https://arxiv.org/abs/2506.07037", "authors": ["Zhongze Luo", "Weixuan Wan", "Qizhi Zheng", "Yanhong Bai", "Jingyun Sun", "Jian Wang", "Dan Wang"], "title": "KG2QA: Knowledge Graph-enhanced Retrieval-Augmented Generation for Communication Standards Question Answering", "categories": ["cs.CL"], "comment": "23 pages", "summary": "There are many types of standards in the field of communication. The\ntraditional consulting model has a long cycle and relies on the knowledge and\nexperience of experts, making it difficult to meet the rapidly developing\ntechnological demands. This paper combines the fine-tuning of large language\nmodels with the construction of knowledge graphs to implement an intelligent\nconsultation and question-answering system for communication standards. The\nexperimental results show that after LoRA tuning on the constructed dataset of\n6,587 questions and answers in the field of communication standards,\nQwen2.5-7B-Instruct demonstrates outstanding professional capabilities in the\nfield of communication standards on the test set. BLEU-4 rose from 18.8564 to\n66.8993, and evaluation indicators such as ROUGE also increased significantly,\noutperforming the fine-tuning effect of the comparison model\nLlama-3-8B-Instruct. Based on the ontology framework containing 6 entity\nattributes and 10 relation attributes, a knowledge graph of the communication\nstandard domain containing 13,906 entities and 13,524 relations was\nconstructed, showing a relatively good query accuracy rate. The intelligent\nconsultation and question-answering system enables the fine-tuned model on the\nserver side to access the locally constructed knowledge graph and conduct\ngraphical retrieval of key information first, which is conducive to improving\nthe question-answering effect. The evaluation using DeepSeek as the Judge on\nthe test set shows that our RAG framework enables the fine-tuned model to\nimprove the scores at all five angles, with an average score increase of 2.26%.\nAnd combined with web services and API interfaces, it has achieved very good\nresults in terms of interaction experience and back-end access, and has very\ngood practical application value.", "AI": {"tldr": "论文提出了一种结合大语言模型微调和知识图谱构建的智能咨询系统，用于通信标准领域，显著提升了问答效果。", "motivation": "传统咨询模型周期长且依赖专家经验，难以满足快速发展的技术需求。", "method": "结合LoRA微调大语言模型（Qwen2.5-7B-Instruct）和知识图谱构建，实现智能问答系统。", "result": "微调后模型BLEU-4从18.8564提升至66.8993，ROUGE等指标显著提高；知识图谱包含13,906实体和13,524关系，查询准确率高。", "conclusion": "系统通过RAG框架和本地知识图谱检索，显著提升问答效果，具有实际应用价值。"}}
{"id": "2506.07050", "pdf": "https://arxiv.org/pdf/2506.07050", "abs": "https://arxiv.org/abs/2506.07050", "authors": ["Zheng Wang", "Kai Ying", "Bin Xu", "Chunjiao Wang", "Cong Bai"], "title": "From Swath to Full-Disc: Advancing Precipitation Retrieval with Multimodal Knowledge Expansion", "categories": ["cs.CV", "cs.IR", "cs.MM"], "comment": null, "summary": "Accurate near-real-time precipitation retrieval has been enhanced by\nsatellite-based technologies. However, infrared-based algorithms have low\naccuracy due to weak relations with surface precipitation, whereas passive\nmicrowave and radar-based methods are more accurate but limited in range. This\nchallenge motivates the Precipitation Retrieval Expansion (PRE) task, which\naims to enable accurate, infrared-based full-disc precipitation retrievals\nbeyond the scanning swath. We introduce Multimodal Knowledge Expansion, a\ntwo-stage pipeline with the proposed PRE-Net model. In the Swath-Distilling\nstage, PRE-Net transfers knowledge from a multimodal data integration model to\nan infrared-based model within the scanning swath via Coordinated Masking and\nWavelet Enhancement (CoMWE). In the Full-Disc Adaptation stage, Self-MaskTune\nrefines predictions across the full disc by balancing multimodal and full-disc\ninfrared knowledge. Experiments on the introduced PRE benchmark demonstrate\nthat PRE-Net significantly advanced precipitation retrieval performance,\noutperforming leading products like PERSIANN-CCS, PDIR, and IMERG. The code\nwill be available at https://github.com/Zjut-MultimediaPlus/PRE-Net.", "AI": {"tldr": "论文提出了一种名为PRE-Net的两阶段模型，通过多模态知识扩展技术提升红外降水反演的精度，解决了现有方法在范围和准确性上的限制。", "motivation": "现有红外降水反演算法精度低，而被动微波和雷达方法虽准确但范围有限，因此需要开发一种能在全盘范围内实现高精度降水反演的技术。", "method": "采用两阶段流程：1) Swath-Distilling阶段，通过CoMWE技术将多模态数据知识迁移到红外模型；2) Full-Disc Adaptation阶段，利用Self-MaskTune平衡多模态和红外知识。", "result": "PRE-Net在PRE基准测试中显著优于PERSIANN-CCS、PDIR和IMERG等领先产品。", "conclusion": "PRE-Net通过多模态知识扩展实现了高精度全盘降水反演，为卫星降水反演提供了新解决方案。"}}
{"id": "2506.07807", "pdf": "https://arxiv.org/pdf/2506.07807", "abs": "https://arxiv.org/abs/2506.07807", "authors": ["John Laird", "Christian Lebiere", "Paul Rosenbloom", "Andrea Stocco", "Robert Wray"], "title": "A Proposal to Extend the Common Model of Cognition with Metacognition", "categories": ["cs.AI"], "comment": null, "summary": "The Common Model of Cognition (CMC) provides an abstract characterization of\nthe structure and processing required by a cognitive architecture for\nhuman-like minds. We propose a unified approach to integrating metacognition\nwithin the CMC. We propose that metacognition involves reasoning over explicit\nrepresentations of an agent's cognitive capabilities and processes in working\nmemory. Our proposal exploits the existing cognitive capabilities of the CMC,\nmaking minimal extensions in the structure and information available within\nworking memory. We provide examples of metacognition within our proposal.", "AI": {"tldr": "论文提出了一种在通用认知模型（CMC）中整合元认知的统一方法，利用工作记忆中的显式表征推理元认知能力。", "motivation": "探讨如何在CMC中整合元认知，以增强其模拟人类认知的能力。", "method": "通过扩展工作记忆中的结构和信息，利用CMC现有的认知能力实现元认知推理。", "result": "提出了具体的元认知实现示例，验证了方法的可行性。", "conclusion": "该方法为CMC提供了元认知支持，且无需大幅修改现有架构。"}}
{"id": "2506.07042", "pdf": "https://arxiv.org/pdf/2506.07042", "abs": "https://arxiv.org/abs/2506.07042", "authors": ["Stergios Chatzikyriakidis"], "title": "Reasoning with RAGged events: RAG-Enhanced Event Knowledge Base Construction and reasoning with proof-assistants", "categories": ["cs.CL"], "comment": null, "summary": "Extracting structured computational representations of historical events from\nnarrative text remains computationally expensive when constructed manually.\nWhile RDF/OWL reasoners enable graph-based reasoning, they are limited to\nfragments of first-order logic, preventing deeper temporal and semantic\nanalysis. This paper addresses both challenges by developing automatic\nhistorical event extraction models using multiple LLMs (GPT-4, Claude, Llama\n3.2) with three enhancement strategies: pure base generation, knowledge graph\nenhancement, and Retrieval-Augmented Generation (RAG). We conducted\ncomprehensive evaluations using historical texts from Thucydides. Our findings\nreveal that enhancement strategies optimize different performance dimensions\nrather than providing universal improvements. For coverage and historical\nbreadth, base generation achieves optimal performance with Claude and GPT-4\nextracting comprehensive events. However, for precision, RAG enhancement\nimproves coordinate accuracy and metadata completeness. Model architecture\nfundamentally determines enhancement sensitivity: larger models demonstrate\nrobust baseline performance with incremental RAG improvements, while Llama 3.2\nshows extreme variance from competitive performance to complete failure. We\nthen developed an automated translation pipeline converting extracted RDF\nrepresentations into Coq proof assistant specifications, enabling higher-order\nreasoning beyond RDF capabilities including multi-step causal verification,\ntemporal arithmetic with BC dates, and formal proofs about historical\ncausation. The Coq formalization validates that RAG-discovered event types\nrepresent legitimate domain-specific semantic structures rather than\nontological violations.", "AI": {"tldr": "本文通过使用多种LLM（GPT-4、Claude、Llama 3.2）及三种增强策略（基础生成、知识图谱增强、RAG），自动提取历史事件的计算表示，并评估其性能。结果表明不同策略优化不同维度，最终通过Coq形式化验证了RAG提取的事件类型的有效性。", "motivation": "手动构建历史事件的计算表示成本高，且现有RDF/OWL推理器仅支持一阶逻辑片段，无法进行更深层次的时空和语义分析。", "method": "使用多种LLM（GPT-4、Claude、Llama 3.2）结合三种增强策略（基础生成、知识图谱增强、RAG）自动提取历史事件，并通过Coq形式化验证。", "result": "不同策略优化不同性能维度：基础生成在覆盖范围上表现最佳，RAG增强在精度上更优；模型架构决定增强效果，大模型表现稳健，Llama 3.2波动大。", "conclusion": "通过Coq形式化验证，RAG提取的事件类型具有领域特异性，支持高阶推理，超越了RDF的能力限制。"}}
{"id": "2506.07055", "pdf": "https://arxiv.org/pdf/2506.07055", "abs": "https://arxiv.org/abs/2506.07055", "authors": ["Tarique Dahri", "Zulfiqar Ali Memon", "Zhenyu Yu", "Mohd. Yamani Idna Idris", "Sheheryar Khan", "Sadiq Ahmad", "Maged Shoman", "Saddam Aziz", "Rizwan Qureshi"], "title": "A Layered Self-Supervised Knowledge Distillation Framework for Efficient Multimodal Learning on the Edge", "categories": ["cs.CV"], "comment": null, "summary": "We introduce Layered Self-Supervised Knowledge Distillation (LSSKD) framework\nfor training compact deep learning models. Unlike traditional methods that rely\non pre-trained teacher networks, our approach appends auxiliary classifiers to\nintermediate feature maps, generating diverse self-supervised knowledge and\nenabling one-to-one transfer across different network stages. Our method\nachieves an average improvement of 4.54\\% over the state-of-the-art PS-KD\nmethod and a 1.14% gain over SSKD on CIFAR-100, with a 0.32% improvement on\nImageNet compared to HASSKD. Experiments on Tiny ImageNet and CIFAR-100 under\nfew-shot learning scenarios also achieve state-of-the-art results. These\nfindings demonstrate the effectiveness of our approach in enhancing model\ngeneralization and performance without the need for large over-parameterized\nteacher networks. Importantly, at the inference stage, all auxiliary\nclassifiers can be removed, yielding no extra computational cost. This makes\nour model suitable for deploying small language models on affordable\nlow-computing devices. Owing to its lightweight design and adaptability, our\nframework is particularly suitable for multimodal sensing and cyber-physical\nenvironments that require efficient and responsive inference. LSSKD facilitates\nthe development of intelligent agents capable of learning from limited sensory\ndata under weak supervision.", "AI": {"tldr": "LSSKD框架通过中间特征图的辅助分类器生成自监督知识，无需预训练教师网络，提升了模型性能，适用于低计算设备。", "motivation": "传统方法依赖预训练教师网络，而LSSKD旨在通过自监督知识蒸馏提升模型性能，同时减少计算成本。", "method": "在中间特征图上添加辅助分类器，生成多样化的自监督知识，实现跨网络阶段的一对一知识转移。", "result": "在CIFAR-100上平均提升4.54%，ImageNet上提升0.32%，在少样本学习场景下也达到最优结果。", "conclusion": "LSSKD有效提升模型泛化能力，无需额外计算成本，适用于低计算设备和多模态感知环境。"}}
{"id": "2506.07820", "pdf": "https://arxiv.org/pdf/2506.07820", "abs": "https://arxiv.org/abs/2506.07820", "authors": ["Jiaxiang CHen", "Zhuo Wang", "Mingxi Zou", "Qifan Wang", "Zenglin Xu"], "title": "Guideline Forest: Experience-Induced Multi-Guideline Reasoning with Stepwise Aggregation", "categories": ["cs.AI"], "comment": null, "summary": "Human reasoning is flexible, adaptive, and grounded in prior\nexperience-qualities that large language models (LLMs) still struggle to\nemulate. Existing methods either explore diverse reasoning paths at inference\ntime or search for optimal workflows through expensive operations, but both\nfall short in leveraging multiple reusable strategies in a structured,\nefficient manner. We propose Guideline Forest, a framework that enhances LLMs\nreasoning by inducing structured reasoning strategies-called guidelines-from\nverified examples and executing them via step-wise aggregation. Unlike\ntest-time search or single-path distillation, our method draws on verified\nreasoning experiences by inducing reusable guidelines and expanding each into\ndiverse variants. Much like human reasoning, these variants reflect alternative\nthought patterns, are executed in parallel, refined via self-correction, and\naggregated step by step-enabling the model to adaptively resolve uncertainty\nand synthesize robust solutions.We evaluate Guideline Forest on four\nbenchmarks-GSM8K, MATH-500, MBPP, and HumanEval-spanning mathematical and\nprogrammatic reasoning. Guideline Forest consistently outperforms strong\nbaselines, including CoT, ReAct, ToT, FoT, and AFlow. Ablation studies further\nhighlight the effectiveness of multi-path reasoning and stepwise aggregation,\nunderscoring the Guideline Forest's adaptability and generalization potential.", "AI": {"tldr": "Guideline Forest是一个通过从已验证示例中提取结构化推理策略（指南）来增强大型语言模型（LLM）推理能力的框架，支持并行执行和逐步聚合，显著优于现有方法。", "motivation": "现有方法在利用多样化的可重用推理策略方面效率不足，无法像人类推理那样灵活适应。", "method": "通过从已验证示例中提取可重用的推理指南，并扩展为多样化变体，并行执行并通过自校正逐步聚合。", "result": "在数学和编程推理的四个基准测试中，Guideline Forest显著优于CoT、ReAct等基线方法。", "conclusion": "Guideline Forest展示了多路径推理和逐步聚合的有效性，具有适应性和泛化潜力。"}}
{"id": "2506.07044", "pdf": "https://arxiv.org/pdf/2506.07044", "abs": "https://arxiv.org/abs/2506.07044", "authors": ["LASA Team", "Weiwen Xu", "Hou Pong Chan", "Long Li", "Mahani Aljunied", "Ruifeng Yuan", "Jianyu Wang", "Chenghao Xiao", "Guizhen Chen", "Chaoqun Liu", "Zhaodonghui Li", "Yu Sun", "Junao Shen", "Chaojun Wang", "Jie Tan", "Deli Zhao", "Tingyang Xu", "Hao Zhang", "Yu Rong"], "title": "Lingshu: A Generalist Foundation Model for Unified Multimodal Medical Understanding and Reasoning", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Technical Report, 53 pages, 25 tables, and 16 figures", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated impressive\ncapabilities in understanding common visual elements, largely due to their\nlarge-scale datasets and advanced training strategies. However, their\neffectiveness in medical applications remains limited due to the inherent\ndiscrepancies between data and tasks in medical scenarios and those in the\ngeneral domain. Concretely, existing medical MLLMs face the following critical\nlimitations: (1) limited coverage of medical knowledge beyond imaging, (2)\nheightened susceptibility to hallucinations due to suboptimal data curation\nprocesses, (3) lack of reasoning capabilities tailored for complex medical\nscenarios. To address these challenges, we first propose a comprehensive data\ncuration procedure that (1) efficiently acquires rich medical knowledge data\nnot only from medical imaging but also from extensive medical texts and\ngeneral-domain data; and (2) synthesizes accurate medical captions, visual\nquestion answering (VQA), and reasoning samples. As a result, we build a\nmultimodal dataset enriched with extensive medical knowledge. Building on the\ncurated data, we introduce our medical-specialized MLLM: Lingshu. Lingshu\nundergoes multi-stage training to embed medical expertise and enhance its\ntask-solving capabilities progressively. Besides, we preliminarily explore the\npotential of applying reinforcement learning with verifiable rewards paradigm\nto enhance Lingshu's medical reasoning ability. Additionally, we develop\nMedEvalKit, a unified evaluation framework that consolidates leading multimodal\nand textual medical benchmarks for standardized, fair, and efficient model\nassessment. We evaluate the performance of Lingshu on three fundamental medical\ntasks, multimodal QA, text-based QA, and medical report generation. The results\nshow that Lingshu consistently outperforms the existing open-source multimodal\nmodels on most tasks ...", "AI": {"tldr": "论文提出了一种针对医学领域的多模态大语言模型Lingshu，通过改进数据收集和训练策略，解决了现有医学MLLMs的局限性，并在多个医学任务中表现优异。", "motivation": "现有医学多模态大语言模型在医学应用中存在知识覆盖不足、易产生幻觉和缺乏针对性推理能力的问题，需要改进。", "method": "提出综合数据收集方法，构建丰富的医学多模态数据集，并通过多阶段训练和强化学习增强模型能力。", "result": "Lingshu在多项医学任务中表现优于现有开源多模态模型。", "conclusion": "Lingshu通过改进数据和训练策略，显著提升了医学多模态模型的性能，为医学AI应用提供了新方向。"}}
{"id": "2506.07056", "pdf": "https://arxiv.org/pdf/2506.07056", "abs": "https://arxiv.org/abs/2506.07056", "authors": ["Zhenyu Liu", "Huizhi Liang", "Rajiv Ranjan", "Zhanxing Zhu", "Vaclav Snasel", "Varun Ojha"], "title": "D2R: dual regularization loss with collaborative adversarial generation for model robustness", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": null, "summary": "The robustness of Deep Neural Network models is crucial for defending models\nagainst adversarial attacks. Recent defense methods have employed collaborative\nlearning frameworks to enhance model robustness. Two key limitations of\nexisting methods are (i) insufficient guidance of the target model via loss\nfunctions and (ii) non-collaborative adversarial generation. We, therefore,\npropose a dual regularization loss (D2R Loss) method and a collaborative\nadversarial generation (CAG) strategy for adversarial training. D2R loss\nincludes two optimization steps. The adversarial distribution and clean\ndistribution optimizations enhance the target model's robustness by leveraging\nthe strengths of different loss functions obtained via a suitable function\nspace exploration to focus more precisely on the target model's distribution.\nCAG generates adversarial samples using a gradient-based collaboration between\nguidance and target models. We conducted extensive experiments on three\nbenchmark databases, including CIFAR-10, CIFAR-100, Tiny ImageNet, and two\npopular target models, WideResNet34-10 and PreActResNet18. Our results show\nthat D2R loss with CAG produces highly robust models.", "AI": {"tldr": "论文提出了一种双正则化损失（D2R Loss）方法和协作对抗生成（CAG）策略，以增强深度神经网络模型对抗攻击的鲁棒性。", "motivation": "现有方法存在两个关键限制：目标模型通过损失函数的指导不足，以及非协作的对抗生成。", "method": "D2R Loss通过对抗分布和干净分布优化增强模型鲁棒性；CAG通过梯度协作生成对抗样本。", "result": "在CIFAR-10、CIFAR-100、Tiny ImageNet等基准数据库上实验表明，D2R Loss与CAG结合能生成高鲁棒性模型。", "conclusion": "D2R Loss和CAG策略有效提升了模型对抗攻击的鲁棒性。"}}
{"id": "2506.07824", "pdf": "https://arxiv.org/pdf/2506.07824", "abs": "https://arxiv.org/abs/2506.07824", "authors": ["Yao Yan"], "title": "Addition in Four Movements: Mapping Layer-wise Information Trajectories in LLMs", "categories": ["cs.AI"], "comment": "12 pages, including appendix, 7 figures. EMNLP 2025 submission (ARR\n  May 2025 cycle, reviews pending)", "summary": "Multi-digit addition is a clear probe of the computational power of large\nlanguage models. To dissect the internal arithmetic processes in\nLLaMA-3-8B-Instruct, we combine linear probing with logit-lens inspection.\nInspired by the step-by-step manner in which humans perform addition, we\npropose and analyze a coherent four-stage trajectory in the forward\npass:Formula-structure representations become linearly decodable first, while\nthe answer token is still far down the candidate list.Core computational\nfeatures then emerge prominently.At deeper activation layers, numerical\nabstractions of the result become clearer, enabling near-perfect detection and\ndecoding of the individual digits in the sum.Near the output, the model\norganizes and generates the final content, with the correct token reliably\noccupying the top rank.This trajectory suggests a hierarchical process that\nfavors internal computation over rote memorization. We release our code and\ndata to facilitate reproducibility.", "AI": {"tldr": "论文通过线性探测和logit-lens技术分析了LLaMA-3-8B-Instruct模型内部的多位数加法计算过程，揭示了其分阶段的层级计算机制。", "motivation": "探究大语言模型在多位数加法任务中的计算能力，揭示其内部算术过程的机制。", "method": "结合线性探测和logit-lens技术，分析模型前向传播中的四阶段计算轨迹。", "result": "模型的计算过程分为四个阶段：公式结构解码、核心计算特征出现、数值抽象清晰化、最终内容生成，支持层级计算而非死记硬背。", "conclusion": "研究表明大语言模型在多位数加法任务中采用层级计算机制，代码和数据已公开以促进可重复性。"}}
{"id": "2506.07064", "pdf": "https://arxiv.org/pdf/2506.07064", "abs": "https://arxiv.org/abs/2506.07064", "authors": ["Kai Xiong", "Xiao Ding", "Yixin Cao", "Yuxiong Yan", "Li Du", "Yufei Zhang", "Jinglong Gao", "Jiaqian Liu", "Bing Qin", "Ting Liu"], "title": "Com$^2$: A Causal-Guided Benchmark for Exploring Complex Commonsense Reasoning in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Main Conference", "summary": "Large language models (LLMs) have mastered abundant simple and explicit\ncommonsense knowledge through pre-training, enabling them to achieve human-like\nperformance in simple commonsense reasoning. Nevertheless, LLMs struggle to\nreason with complex and implicit commonsense knowledge that is derived from\nsimple ones (such as understanding the long-term effects of certain events), an\naspect humans tend to focus on more. Existing works focus on complex tasks like\nmath and code, while complex commonsense reasoning remains underexplored due to\nits uncertainty and lack of structure. To fill this gap and align with\nreal-world concerns, we propose a benchmark Com$^2$ focusing on complex\ncommonsense reasoning. We first incorporate causal event graphs to serve as\nstructured complex commonsense. Then we adopt causal theory~(e.g.,\nintervention) to modify the causal event graphs and obtain different scenarios\nthat meet human concerns. Finally, an LLM is employed to synthesize examples\nwith slow thinking, which is guided by the logical relationships in the\nmodified causal graphs. Furthermore, we use detective stories to construct a\nmore challenging subset. Experiments show that LLMs struggle in reasoning depth\nand breadth, while post-training and slow thinking can alleviate this. The code\nand data are available at https://github.com/Waste-Wood/Com2.", "AI": {"tldr": "论文提出了一个名为Com$^2$的基准测试，专注于复杂常识推理，填补了现有研究的空白。通过因果事件图和因果理论构建结构化知识，并利用LLM生成示例。实验表明LLM在推理深度和广度上存在困难，但后训练和慢思考可以缓解这一问题。", "motivation": "现有的大型语言模型（LLM）在简单常识推理上表现优异，但在复杂和隐性的常识推理（如事件长期影响）上表现不佳。这一领域因不确定性和缺乏结构而研究不足。", "method": "1. 使用因果事件图作为结构化复杂常识知识；2. 采用因果理论（如干预）修改因果图以生成符合人类关注的场景；3. 利用LLM基于修改后的因果图逻辑关系生成示例。此外，还通过侦探故事构建更具挑战性的子集。", "result": "实验表明，LLM在复杂常识推理的深度和广度上表现不佳，但后训练和慢思考能有效改善其表现。", "conclusion": "Com$^2$基准填补了复杂常识推理研究的空白，为LLM在这一领域的改进提供了方向。"}}
{"id": "2506.07080", "pdf": "https://arxiv.org/pdf/2506.07080", "abs": "https://arxiv.org/abs/2506.07080", "authors": ["Anatol Garioud", "Sébastien Giordano", "Nicolas David", "Nicolas Gonthier"], "title": "FLAIR-HUB: Large-scale Multimodal Dataset for Land Cover and Crop Mapping", "categories": ["cs.CV"], "comment": null, "summary": "The growing availability of high-quality Earth Observation (EO) data enables\naccurate global land cover and crop type monitoring. However, the volume and\nheterogeneity of these datasets pose major processing and annotation\nchallenges. To address this, the French National Institute of Geographical and\nForest Information (IGN) is actively exploring innovative strategies to exploit\ndiverse EO data, which require large annotated datasets. IGN introduces\nFLAIR-HUB, the largest multi-sensor land cover dataset with\nvery-high-resolution (20 cm) annotations, covering 2528 km2 of France. It\ncombines six aligned modalities: aerial imagery, Sentinel-1/2 time series, SPOT\nimagery, topographic data, and historical aerial images. Extensive benchmarks\nevaluate multimodal fusion and deep learning models (CNNs, transformers) for\nland cover or crop mapping and also explore multi-task learning. Results\nunderscore the complexity of multimodal fusion and fine-grained classification,\nwith best land cover performance (78.2% accuracy, 65.8% mIoU) achieved using\nnearly all modalities. FLAIR-HUB supports supervised and multimodal\npretraining, with data and code available at\nhttps://ignf.github.io/FLAIR/flairhub.", "AI": {"tldr": "FLAIR-HUB是一个多传感器土地覆盖数据集，结合了六种对齐的数据模态，用于土地覆盖和作物类型监测，通过多模态融合和深度学习模型取得了最佳性能。", "motivation": "高分辨率地球观测数据的可用性增加，但数据处理和标注的挑战也随之而来，IGN开发FLAIR-HUB以解决这一问题。", "method": "IGN引入了FLAIR-HUB数据集，结合六种数据模态，并通过多模态融合和深度学习模型（如CNN和Transformer）进行土地覆盖和作物分类。", "result": "最佳土地覆盖分类性能达到78.2%准确率和65.8% mIoU，几乎使用了所有数据模态。", "conclusion": "FLAIR-HUB为监督学习和多模态预训练提供了支持，展示了多模态融合在精细分类中的复杂性。"}}
{"id": "2506.07837", "pdf": "https://arxiv.org/pdf/2506.07837", "abs": "https://arxiv.org/abs/2506.07837", "authors": ["Shijie Wang", "Yilun Zhang", "Zeyu Lai", "Dexing Kong"], "title": "HAIBU-ReMUD: Reasoning Multimodal Ultrasound Dataset and Model Bridging to General Specific Domains", "categories": ["cs.AI"], "comment": null, "summary": "Multimodal large language models (MLLMs) have shown great potential in\ngeneral domains but perform poorly in some specific domains due to a lack of\ndomain-specific data, such as image-text data or vedio-text data. In some\nspecific domains, there is abundant graphic and textual data scattered around,\nbut lacks standardized arrangement. In the field of medical ultrasound, there\nare ultrasonic diagnostic books, ultrasonic clinical guidelines, ultrasonic\ndiagnostic reports, and so on. However, these ultrasonic materials are often\nsaved in the forms of PDF, images, etc., and cannot be directly used for the\ntraining of MLLMs. This paper proposes a novel image-text reasoning supervised\nfine-tuning data generation pipeline to create specific domain quadruplets\n(image, question, thinking trace, and answer) from domain-specific materials. A\nmedical ultrasound domain dataset ReMUD is established, containing over 45,000\nreasoning and non-reasoning supervised fine-tuning Question Answering (QA) and\nVisual Question Answering (VQA) data. The ReMUD-7B model, fine-tuned on\nQwen2.5-VL-7B-Instruct, outperforms general-domain MLLMs in medical ultrasound\nfield. To facilitate research, the ReMUD dataset, data generation codebase, and\nReMUD-7B parameters will be released at https://github.com/ShiDaizi/ReMUD,\naddressing the data shortage issue in specific domain MLLMs.", "AI": {"tldr": "提出了一种针对特定领域（如医学超声）的多模态大语言模型（MLLM）数据生成方法，解决了领域数据不足的问题。", "motivation": "由于缺乏特定领域的图像-文本或视频-文本数据，MLLM在特定领域表现不佳。医学超声领域虽有丰富数据，但格式不统一，无法直接用于训练。", "method": "开发了一种图像-文本推理监督微调数据生成流程，从领域材料中生成四元组（图像、问题、思考轨迹、答案），并构建了医学超声数据集ReMUD。", "result": "基于ReMUD数据集微调的ReMUD-7B模型在医学超声领域表现优于通用MLLM。", "conclusion": "ReMUD数据集和模型参数的开源解决了特定领域MLLM数据短缺问题，推动了相关研究。"}}
{"id": "2506.07086", "pdf": "https://arxiv.org/pdf/2506.07086", "abs": "https://arxiv.org/abs/2506.07086", "authors": ["Yuanhe Tian", "Pengsen Cheng", "Guoqing Jin", "Lei Zhang", "Yan Song"], "title": "Representation Decomposition for Learning Similarity and Contrastness Across Modalities for Affective Computing", "categories": ["cs.CL"], "comment": "13 pages, 4 figures", "summary": "Multi-modal affective computing aims to automatically recognize and interpret\nhuman attitudes from diverse data sources such as images and text, thereby\nenhancing human-computer interaction and emotion understanding. Existing\napproaches typically rely on unimodal analysis or straightforward fusion of\ncross-modal information that fail to capture complex and conflicting evidence\npresented across different modalities. In this paper, we propose a novel\nLLM-based approach for affective computing that explicitly deconstructs visual\nand textual representations into shared (modality-invariant) and\nmodality-specific components. Specifically, our approach firstly encodes and\naligns input modalities using pre-trained multi-modal encoders, then employs a\nrepresentation decomposition framework to separate common emotional content\nfrom unique cues, and finally integrates these decomposed signals via an\nattention mechanism to form a dynamic soft prompt for a multi-modal LLM.\nExtensive experiments on three representative tasks for affective computing,\nnamely, multi-modal aspect-based sentiment analysis, multi-modal emotion\nanalysis, and hateful meme detection, demonstrate the effectiveness of our\napproach, which consistently outperforms strong baselines and state-of-the-art\nmodels.", "AI": {"tldr": "提出了一种基于LLM的多模态情感计算方法，通过分解共享和特定模态的表示，显著提升了情感计算的性能。", "motivation": "现有方法通常依赖单模态分析或简单的跨模态融合，无法捕捉多模态中复杂或冲突的证据。", "method": "使用预训练多模态编码器对齐输入模态，通过表示分解框架分离共享和特定模态的情感内容，最后通过注意力机制整合信号形成动态软提示。", "result": "在三个代表性任务（多模态方面情感分析、多模态情感分析和仇恨模因检测）中表现优于基线模型和SOTA模型。", "conclusion": "该方法通过显式分解和动态整合多模态信息，显著提升了情感计算的准确性和鲁棒性。"}}
{"id": "2506.07087", "pdf": "https://arxiv.org/pdf/2506.07087", "abs": "https://arxiv.org/abs/2506.07087", "authors": ["Weiqi Yan", "Lvhai Chen", "Huaijia Kou", "Shengchuan Zhang", "Yan Zhang", "Liujuan Cao"], "title": "UCOD-DPL: Unsupervised Camouflaged Object Detection via Dynamic Pseudo-label Learning", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025 (Hightlight)", "summary": "Unsupervised Camoflaged Object Detection (UCOD) has gained attention since it\ndoesn't need to rely on extensive pixel-level labels. Existing UCOD methods\ntypically generate pseudo-labels using fixed strategies and train 1 x1\nconvolutional layers as a simple decoder, leading to low performance compared\nto fully-supervised methods. We emphasize two drawbacks in these approaches:\n1). The model is prone to fitting incorrect knowledge due to the pseudo-label\ncontaining substantial noise. 2). The simple decoder fails to capture and learn\nthe semantic features of camouflaged objects, especially for small-sized\nobjects, due to the low-resolution pseudo-labels and severe confusion between\nforeground and background pixels. To this end, we propose a UCOD method with a\nteacher-student framework via Dynamic Pseudo-label Learning called UCOD-DPL,\nwhich contains an Adaptive Pseudo-label Module (APM), a Dual-Branch Adversarial\n(DBA) decoder, and a Look-Twice mechanism. The APM module adaptively combines\npseudo-labels generated by fixed strategies and the teacher model to prevent\nthe model from overfitting incorrect knowledge while preserving the ability for\nself-correction; the DBA decoder takes adversarial learning of different\nsegmentation objectives, guides the model to overcome the foreground-background\nconfusion of camouflaged objects, and the Look-Twice mechanism mimics the human\ntendency to zoom in on camouflaged objects and performs secondary refinement on\nsmall-sized objects. Extensive experiments show that our method demonstrates\noutstanding performance, even surpassing some existing fully supervised\nmethods. The code is available now.", "AI": {"tldr": "本文提出了一种无监督伪装目标检测方法UCOD-DPL，通过动态伪标签学习和教师-学生框架，解决了现有方法中伪标签噪声和简单解码器性能不足的问题。", "motivation": "现有无监督伪装目标检测方法依赖固定策略生成伪标签并使用简单解码器，导致性能较低，且易受伪标签噪声和前景背景混淆的影响。", "method": "提出UCOD-DPL方法，包含自适应伪标签模块（APM）、双分支对抗解码器（DBA）和Look-Twice机制，动态优化伪标签并提升特征学习能力。", "result": "实验表明，该方法性能优异，甚至超过部分全监督方法。", "conclusion": "UCOD-DPL通过动态伪标签学习和多模块协作，显著提升了无监督伪装目标检测的性能。"}}
{"id": "2506.07853", "pdf": "https://arxiv.org/pdf/2506.07853", "abs": "https://arxiv.org/abs/2506.07853", "authors": ["Hudson de Martim"], "title": "A Temporal FRBR/FRBRoo-Based Model for Component-Level Versioning of Legal Norms", "categories": ["cs.AI", "cs.IR"], "comment": null, "summary": "Effectively representing legal norms for automated processing is a critical\nchallenge, particularly in tracking the diachronic evolution of their\nhierarchical components (e.g., articles, paragraphs). While foundational\nframeworks like FRBR/FRBRoo and standards like Akoma Ntoso model legal\ndocuments at a macro level, they lack native mechanisms for granular,\ncomponent-level versioning. This limitation hinders the deterministic\npoint-in-time reconstruction of legal texts, a fundamental capability for\nreliable Legal Tech and AI applications. This paper proposes a structured,\ntemporal model that extends the FRBRoo framework to address this gap. It\nintroduces specialized subclasses of Expressio - Temporal Version (TV) and\nLanguage Version (LV - to represent the state of a legal norm and its\nlinguistic variations at specific points in time. The model applies this same\nparadigm hierarchically, introducing Component Work (CW), Component Temporal\nVersion (CTV), and Component Language Version (CLV) to track the lifecycle of\nindividual articles, paragraphs, and clauses. Using the Brazilian Federal\nConstitution as a case study, the paper demonstrates how each amendment creates\nnew Component Temporal Versions for affected provisions, while unaffected\ncomponents retain their existing versions. This fine-grained, time-aware\narchitecture enables the precise, deterministic retrieval and reconstruction of\nany part of a legal text as it existed on a specific date. The model provides a\nrobust foundation for developing advanced legal information systems, knowledge\ngraphs, and AI tools capable of accurate historical analysis and impact\nassessment, overcoming the limitations of current generative models.", "AI": {"tldr": "本文提出了一种基于FRBRoo框架的细粒度时间模型，用于跟踪法律规范的层次组件（如条款、段落）的演变，解决了现有宏观模型在组件级版本控制上的不足。", "motivation": "现有法律文档建模框架（如FRBR/FRBRoo和Akoma Ntoso）缺乏对组件级版本的原生支持，限制了法律文本在特定时间点的确定性重构能力，影响了法律技术和AI应用的可靠性。", "method": "扩展FRBRoo框架，引入Temporal Version (TV)和Language Version (LV)子类，以及Component Work (CW)、Component Temporal Version (CTV)和Component Language Version (CLV)等概念，以层次化方式跟踪法律组件的生命周期。", "result": "以巴西联邦宪法为例，展示了模型如何通过创建新的Component Temporal Versions来记录修订内容，同时保留未受影响组件的现有版本，实现了法律文本的精确时间点重构。", "conclusion": "该模型为开发高级法律信息系统、知识图谱和AI工具提供了坚实基础，支持准确的历史分析和影响评估，弥补了现有生成模型的局限性。"}}
{"id": "2506.07104", "pdf": "https://arxiv.org/pdf/2506.07104", "abs": "https://arxiv.org/abs/2506.07104", "authors": ["Jiaxuan Gao", "Shu Yan", "Qixin Tan", "Lu Yang", "Shusheng Xu", "Wei Fu", "Zhiyu Mei", "Kaifeng Lyu", "Yi Wu"], "title": "How Far Are We from Optimal Reasoning Efficiency?", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Reasoning Models (LRMs) demonstrate remarkable problem-solving\ncapabilities through extended Chain-of-Thought (CoT) reasoning but often\nproduce excessively verbose and redundant reasoning traces. This inefficiency\nincurs high inference costs and limits practical deployment. While existing\nfine-tuning methods aim to improve reasoning efficiency, assessing their\nefficiency gains remains challenging due to inconsistent evaluations. In this\nwork, we introduce the reasoning efficiency frontiers, empirical upper bounds\nderived from fine-tuning base LRMs across diverse approaches and training\nconfigurations. Based on these frontiers, we propose the Reasoning Efficiency\nGap (REG), a unified metric quantifying deviations of any fine-tuned LRMs from\nthese frontiers. Systematic evaluation on challenging mathematical benchmarks\nreveals significant gaps in current methods: they either sacrifice accuracy for\nshort length or still remain inefficient under tight token budgets. To reduce\nthe efficiency gap, we propose REO-RL, a class of Reinforcement Learning\nalgorithms that minimizes REG by targeting a sparse set of token budgets.\nLeveraging numerical integration over strategically selected budgets, REO-RL\napproximates the full efficiency objective with low error using a small set of\ntoken budgets. Through systematic benchmarking, we demonstrate that our\nefficiency metric, REG, effectively captures the accuracy-length trade-off,\nwith low-REG methods reducing length while maintaining accuracy. Our approach,\nREO-RL, consistently reduces REG by >=50 across all evaluated LRMs and matching\nQwen3-4B/8B efficiency frontiers under a 16K token budget with minimal accuracy\nloss. Ablation studies confirm the effectiveness of our exponential token\nbudget strategy. Finally, our findings highlight that fine-tuning LRMs to\nperfectly align with the efficiency frontiers remains an open challenge.", "AI": {"tldr": "论文提出了一种衡量大型推理模型（LRMs）效率的新方法，通过定义推理效率前沿（efficiency frontiers）和推理效率差距（REG），并设计了REO-RL算法来优化效率。", "motivation": "现有的大型推理模型在推理过程中常产生冗余信息，导致高推理成本，而现有评估方法不一致，难以衡量效率提升。", "method": "引入推理效率前沿作为基准，提出REG作为统一评估指标，并设计REO-RL算法，通过强化学习优化推理效率。", "result": "实验表明，REO-RL能显著减少REG（≥50%），并在16K token预算下接近Qwen3-4B/8B的效率前沿，同时保持准确性。", "conclusion": "论文揭示了当前方法在效率与准确性之间的权衡问题，并指出完全对齐效率前沿仍具挑战性。"}}
{"id": "2506.07091", "pdf": "https://arxiv.org/pdf/2506.07091", "abs": "https://arxiv.org/abs/2506.07091", "authors": ["Yangkai Lin", "Jiabao Lei", "Kui Jia"], "title": "SceneLCM: End-to-End Layout-Guided Interactive Indoor Scene Generation with Latent Consistency Model", "categories": ["cs.CV"], "comment": null, "summary": "Our project page: https://scutyklin.github.io/SceneLCM/. Automated generation\nof complex, interactive indoor scenes tailored to user prompt remains a\nformidable challenge. While existing methods achieve indoor scene synthesis,\nthey struggle with rigid editing constraints, physical incoherence, excessive\nhuman effort, single-room limitations, and suboptimal material quality. To\naddress these limitations, we propose SceneLCM, an end-to-end framework that\nsynergizes Large Language Model (LLM) for layout design with Latent Consistency\nModel(LCM) for scene optimization. Our approach decomposes scene generation\ninto four modular pipelines: (1) Layout Generation. We employ LLM-guided 3D\nspatial reasoning to convert textual descriptions into parametric blueprints(3D\nlayout). And an iterative programmatic validation mechanism iteratively refines\nlayout parameters through LLM-mediated dialogue loops; (2) Furniture\nGeneration. SceneLCM employs Consistency Trajectory Sampling(CTS), a\nconsistency distillation sampling loss guided by LCM, to form fast,\nsemantically rich, and high-quality representations. We also offer two\ntheoretical justification to demonstrate that our CTS loss is equivalent to\nconsistency loss and its distillation error is bounded by the truncation error\nof the Euler solver; (3) Environment Optimization. We use a multiresolution\ntexture field to encode the appearance of the scene, and optimize via CTS loss.\nTo maintain cross-geometric texture coherence, we introduce a normal-aware\ncross-attention decoder to predict RGB by cross-attending to the anchors\nlocations in geometrically heterogeneous instance. (4)Physically Editing.\nSceneLCM supports physically editing by integrating physical simulation,\nachieved persistent physical realism. Extensive experiments validate SceneLCM's\nsuperiority over state-of-the-art techniques, showing its wide-ranging\npotential for diverse applications.", "AI": {"tldr": "SceneLCM是一个端到端框架，结合LLM进行布局设计和LCM进行场景优化，解决了现有室内场景生成方法的局限性。", "motivation": "现有方法在室内场景生成中存在编辑限制、物理不一致、人力需求高、单房间限制和材质质量差等问题。", "method": "SceneLCM将场景生成分解为四个模块化流程：布局生成（LLM引导）、家具生成（CTS损失）、环境优化（多分辨率纹理场）和物理编辑（物理模拟）。", "result": "实验验证了SceneLCM在多样应用中的优越性。", "conclusion": "SceneLCM通过模块化设计和优化方法，显著提升了室内场景生成的效率和质量。"}}
{"id": "2506.07896", "pdf": "https://arxiv.org/pdf/2506.07896", "abs": "https://arxiv.org/abs/2506.07896", "authors": ["Shoko Oka"], "title": "Evaluating Large Language Models on the Frame and Symbol Grounding Problems: A Zero-shot Benchmark", "categories": ["cs.AI", "cs.CL"], "comment": "52 pages, Additional resources available on GitHub repository", "summary": "Recent advancements in large language models (LLMs) have revitalized\nphilosophical debates surrounding artificial intelligence. Two of the most\nfundamental challenges - namely, the Frame Problem and the Symbol Grounding\nProblem - have historically been viewed as unsolvable within traditional\nsymbolic AI systems. This study investigates whether modern LLMs possess the\ncognitive capacities required to address these problems. To do so, I designed\ntwo benchmark tasks reflecting the philosophical core of each problem,\nadministered them under zero-shot conditions to 13 prominent LLMs (both closed\nand open-source), and assessed the quality of the models' outputs across five\ntrials each. Responses were scored along multiple criteria, including\ncontextual reasoning, semantic coherence, and information filtering. The\nresults demonstrate that while open-source models showed variability in\nperformance due to differences in model size, quantization, and instruction\ntuning, several closed models consistently achieved high scores. These findings\nsuggest that select modern LLMs may be acquiring capacities sufficient to\nproduce meaningful and stable responses to these long-standing theoretical\nchallenges.", "AI": {"tldr": "现代大语言模型（LLMs）是否能解决AI中的框架问题和符号接地问题？研究发现，部分封闭模型表现优异，可能具备解决这些理论挑战的能力。", "motivation": "探讨现代LLMs是否具备解决AI中框架问题和符号接地问题的认知能力，这两大问题在传统符号AI系统中被认为无解。", "method": "设计了两个基准任务，分别反映框架问题和符号接地问题的哲学核心，在零样本条件下对13个主流LLMs（包括封闭和开源模型）进行测试，并对输出质量进行多维度评分。", "result": "开源模型因模型规模、量化和指令调优差异表现不一，而部分封闭模型表现稳定且高分，表明其可能具备解决这些问题的能力。", "conclusion": "部分现代LLMs可能已具备解决框架问题和符号接地问题的能力，为AI理论挑战提供了新的可能性。"}}
{"id": "2506.07106", "pdf": "https://arxiv.org/pdf/2506.07106", "abs": "https://arxiv.org/abs/2506.07106", "authors": ["Samir Abdaljalil", "Hasan Kurban", "Khalid Qaraqe", "Erchin Serpedin"], "title": "Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have shown strong performance across natural\nlanguage reasoning tasks, yet their reasoning processes remain brittle and\ndifficult to interpret. Prompting techniques like Chain-of-Thought (CoT)\nenhance reliability by eliciting intermediate reasoning steps or aggregating\nmultiple outputs. However, they lack mechanisms for enforcing logical structure\nand assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a\nnovel framework that models reasoning as collaboration among three parallel\nagents, each simulating a distinct mode of inference: abductive, deductive, and\ninductive. Each agent produces a reasoning trace, which is structured into a\nformal reasoning graph. To evaluate consistency, we apply Bayesian belief\npropagation guided by natural language inference (NLI), assigning confidence\nscores to each step. The most coherent graph is selected to derive the final\nanswer. Experiments on symbolic (WebOfLies) and numerical (MultiArith)\nreasoning benchmarks show that ToTh consistently outperforms CoT,\nSelf-Consistency, and CoT-Decoding across multiple LLMs, while producing\ninterpretable and logically grounded reasoning chains. Our findings suggest a\npromising direction for building more robust and cognitively inspired LLM\nreasoning. The implementation is available at\nhttps://github.com/KurbanIntelligenceLab/theorem-of-thought.", "AI": {"tldr": "论文提出了一种名为Theorem-of-Thought (ToTh)的新框架，通过模拟三种推理模式（溯因、演绎和归纳）的并行代理，生成结构化推理图，并利用贝叶斯信念传播评估一致性，显著提升了语言模型的推理性能。", "motivation": "尽管大型语言模型（LLMs）在自然语言推理任务中表现优异，但其推理过程脆弱且难以解释。现有提示技术（如Chain-of-Thought）缺乏逻辑结构和内部一致性评估机制。", "method": "ToTh框架通过三个并行代理（分别模拟溯因、演绎和归纳推理）生成推理轨迹，并将其结构化为一幅形式化推理图。利用贝叶斯信念传播和自然语言推理（NLI）评估一致性，选择最连贯的图生成最终答案。", "result": "在符号推理（WebOfLies）和数值推理（MultiArith）基准测试中，ToTh在多种LLMs上均优于Chain-of-Thought、Self-Consistency和CoT-Decoding，同时生成可解释且逻辑清晰的推理链。", "conclusion": "ToTh为构建更鲁棒且受认知启发的LLM推理提供了一条有前景的方向。"}}
{"id": "2506.07112", "pdf": "https://arxiv.org/pdf/2506.07112", "abs": "https://arxiv.org/abs/2506.07112", "authors": ["Changhong Fu", "Hua Lin", "Haobo Zuo", "Liangliang Yao", "Liguo Zhang"], "title": "EdgeSpotter: Multi-Scale Dense Text Spotting for Industrial Panel Monitoring", "categories": ["cs.CV"], "comment": null, "summary": "Text spotting for industrial panels is a key task for intelligent monitoring.\nHowever, achieving efficient and accurate text spotting for complex industrial\npanels remains challenging due to issues such as cross-scale localization and\nambiguous boundaries in dense text regions. Moreover, most existing methods\nprimarily focus on representing a single text shape, neglecting a comprehensive\nexploration of multi-scale feature information across different texts. To\naddress these issues, this work proposes a novel multi-scale dense text spotter\nfor edge AI-based vision system (EdgeSpotter) to achieve accurate and robust\nindustrial panel monitoring. Specifically, a novel Transformer with efficient\nmixer is developed to learn the interdependencies among multi-level features,\nintegrating multi-layer spatial and semantic cues. In addition, a new feature\nsampling with catmull-rom splines is designed, which explicitly encodes the\nshape, position, and semantic information of text, thereby alleviating missed\ndetections and reducing recognition errors caused by multi-scale or dense text\nregions. Furthermore, a new benchmark dataset for industrial panel monitoring\n(IPM) is constructed. Extensive qualitative and quantitative evaluations on\nthis challenging benchmark dataset validate the superior performance of the\nproposed method in different challenging panel monitoring tasks. Finally,\npractical tests based on the self-designed edge AI-based vision system\ndemonstrate the practicality of the method. The code and demo will be available\nat https://github.com/vision4robotics/EdgeSpotter.", "AI": {"tldr": "提出了一种名为EdgeSpotter的多尺度密集文本检测方法，用于工业面板监控，解决了跨尺度定位和密集文本区域模糊边界的问题。", "motivation": "工业面板文本检测面临跨尺度定位和密集文本区域模糊边界的挑战，现有方法多关注单一文本形状，缺乏多尺度特征探索。", "method": "开发了基于Transformer的高效混合器学习多级特征依赖，设计了Catmull-Rom样条特征采样方法，整合文本形状、位置和语义信息。", "result": "在自建的工业面板监控数据集上验证了方法的优越性能，实际测试展示了其实用性。", "conclusion": "EdgeSpotter在工业面板监控任务中表现出高效、准确和鲁棒性，适用于边缘AI视觉系统。"}}
{"id": "2506.07915", "pdf": "https://arxiv.org/pdf/2506.07915", "abs": "https://arxiv.org/abs/2506.07915", "authors": ["Dimitris Panagopoulos", "Adolfo Perrusquia", "Weisi Guo"], "title": "LUCIFER: Language Understanding and Context-Infused Framework for Exploration and Behavior Refinement", "categories": ["cs.AI", "cs.CL", "cs.SY", "eess.SY"], "comment": "12 pages, 4 Figures, 3 Tables, submitted to the IEEE for possible\n  publication", "summary": "In dynamic environments, the rapid obsolescence of pre-existing environmental\nknowledge creates a gap between an agent's internal model and the evolving\nreality of its operational context. This disparity between prior and updated\nenvironmental valuations fundamentally limits the effectiveness of autonomous\ndecision-making. To bridge this gap, the contextual bias of human domain\nstakeholders, who naturally accumulate insights through direct, real-time\nobservation, becomes indispensable. However, translating their nuanced, and\ncontext-rich input into actionable intelligence for autonomous systems remains\nan open challenge. To address this, we propose LUCIFER (Language Understanding\nand Context-Infused Framework for Exploration and Behavior Refinement), a\ndomain-agnostic framework that integrates a hierarchical decision-making\narchitecture with reinforcement learning (RL) and large language models (LLMs)\ninto a unified system. This architecture mirrors how humans decompose complex\ntasks, enabling a high-level planner to coordinate specialised sub-agents, each\nfocused on distinct objectives and temporally interdependent actions. Unlike\ntraditional applications where LLMs are limited to single role, LUCIFER\nintegrates them in two synergistic roles: as context extractors, structuring\nverbal stakeholder input into domain-aware representations that influence\ndecision-making through an attention space mechanism aligning LLM-derived\ninsights with the agent's learning process, and as zero-shot exploration\nfacilitators guiding the agent's action selection process during exploration.\nWe benchmark various LLMs in both roles and demonstrate that LUCIFER improves\nexploration efficiency and decision quality, outperforming flat,\ngoal-conditioned policies. Our findings show the potential of context-driven\ndecision-making, where autonomous systems leverage human contextual knowledge\nfor operational success.", "AI": {"tldr": "LUCIFER框架通过结合分层决策架构、强化学习和大型语言模型，解决了动态环境中自主决策的知识过时问题，并利用人类上下文知识提升决策质量。", "motivation": "动态环境中，先验知识快速过时，导致自主决策效率低下。人类上下文知识虽重要，但难以转化为自主系统的可操作信息。", "method": "提出LUCIFER框架，结合分层决策、强化学习和LLMs，LLMs扮演上下文提取器和零样本探索引导者双重角色。", "result": "LUCIFER在探索效率和决策质量上优于传统方法，验证了上下文驱动决策的潜力。", "conclusion": "LUCIFER展示了如何通过整合人类上下文知识提升自主系统的操作成功性。"}}
{"id": "2506.07142", "pdf": "https://arxiv.org/pdf/2506.07142", "abs": "https://arxiv.org/abs/2506.07142", "authors": ["Lennart Meincke", "Ethan Mollick", "Lilach Mollick", "Dan Shapiro"], "title": "Prompting Science Report 2: The Decreasing Value of Chain of Thought in Prompting", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This is the second in a series of short reports that seek to help business,\neducation, and policy leaders understand the technical details of working with\nAI through rigorous testing. In this report, we investigate Chain-of-Thought\n(CoT) prompting, a technique that encourages a large language model (LLM) to\n\"think step by step\" (Wei et al., 2022). CoT is a widely adopted method for\nimproving reasoning tasks, however, our findings reveal a more nuanced picture\nof its effectiveness. We demonstrate two things:\n  - The effectiveness of Chain-of-Thought prompting can vary greatly depending\non the type of task and model. For non-reasoning models, CoT generally improves\naverage performance by a small amount, particularly if the model does not\ninherently engage in step-by-step processing by default. However, CoT can\nintroduce more variability in answers, sometimes triggering occasional errors\nin questions the model would otherwise get right. We also found that many\nrecent models perform some form of CoT reasoning even if not asked; for these\nmodels, a request to perform CoT had little impact. Performing CoT generally\nrequires far more tokens (increasing cost and time) than direct answers.\n  - For models designed with explicit reasoning capabilities, CoT prompting\noften results in only marginal, if any, gains in answer accuracy. However, it\nsignificantly increases the time and tokens needed to generate a response.", "AI": {"tldr": "报告研究了Chain-of-Thought (CoT)提示技术的效果，发现其效果因任务和模型类型而异，可能增加答案变异性或成本，而对具备推理能力的模型增益有限。", "motivation": "帮助商业、教育和政策领导者理解AI技术细节，特别是CoT提示的实际效果。", "method": "通过测试不同任务和模型类型，分析CoT提示对性能和成本的影响。", "result": "CoT对非推理模型小幅提升性能但增加变异性；对推理模型增益有限且增加成本。", "conclusion": "CoT效果因模型和任务而异，需权衡性能提升与成本增加。"}}
{"id": "2506.07122", "pdf": "https://arxiv.org/pdf/2506.07122", "abs": "https://arxiv.org/abs/2506.07122", "authors": ["Prakriti Tripathi", "Theertha Biju", "Maniram Thota", "Rakesh Lingam"], "title": "Image segmentation and classification of E-waste for waste segregation", "categories": ["cs.CV", "cs.AI", "I.2.10"], "comment": "4 pages, 7 figures. For code and link to dataset, see\n  https://github.com/prakriti16/Image-segmentation-and-classification-of-e-waste", "summary": "Industry partners provided a problem statement that involves classifying\nelectronic waste using machine learning models that will be used by\npick-and-place robots for waste segregation. We started by taking common\nelectronic waste items, such as a mouse and charger, unsoldering them, and\ntaking pictures to create a custom dataset. Then state-of-the art YOLOv11 model\nwas trained and run to achieve 70 mAP in real-time. Mask-RCNN model was also\ntrained and achieved 41 mAP. The model will be further integrated with\npick-and-place robots to perform segregation of e-waste.", "AI": {"tldr": "论文提出了一种基于YOLOv11和Mask-RCNN的电子废物分类方法，用于指导拣选机器人进行废物分类，分别实现了70 mAP和41 mAP的性能。", "motivation": "工业合作伙伴提出了一个电子废物分类问题，需要通过机器学习模型指导拣选机器人进行废物分类。", "method": "通过拆解常见电子废物（如鼠标和充电器）并拍照创建自定义数据集，训练了YOLOv11和Mask-RCNN模型。", "result": "YOLOv11模型在实时检测中达到70 mAP，Mask-RCNN模型达到41 mAP。", "conclusion": "模型将进一步集成到拣选机器人中，实现电子废物的自动分类。"}}
{"id": "2506.07927", "pdf": "https://arxiv.org/pdf/2506.07927", "abs": "https://arxiv.org/abs/2506.07927", "authors": ["Jiayi Sheng", "Luna Lyu", "Jikai Jin", "Tony Xia", "Alex Gu", "James Zou", "Pan Lu"], "title": "Solving Inequality Proofs with Large Language Models", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "52 pages, 16 figures", "summary": "Inequality proving, crucial across diverse scientific and mathematical\nfields, tests advanced reasoning skills such as discovering tight bounds and\nstrategic theorem application. This makes it a distinct, demanding frontier for\nlarge language models (LLMs), offering insights beyond general mathematical\nproblem-solving. Progress in this area is hampered by existing datasets that\nare often scarce, synthetic, or rigidly formal. We address this by proposing an\ninformal yet verifiable task formulation, recasting inequality proving into two\nautomatically checkable subtasks: bound estimation and relation prediction.\nBuilding on this, we release IneqMath, an expert-curated dataset of\nOlympiad-level inequalities, including a test set and training corpus enriched\nwith step-wise solutions and theorem annotations. We also develop a novel\nLLM-as-judge evaluation framework, combining a final-answer judge with four\nstep-wise judges designed to detect common reasoning flaws. A systematic\nevaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even\ntop models like o1 achieve less than 10% overall accuracy under step-wise\nscrutiny; this is a drop of up to 65.5% from their accuracy considering only\nfinal answer equivalence. This discrepancy exposes fragile deductive chains and\na critical gap for current LLMs between merely finding an answer and\nconstructing a rigorous proof. Scaling model size and increasing test-time\ncomputation yield limited gains in overall proof correctness. Instead, our\nfindings highlight promising research directions such as theorem-guided\nreasoning and self-refinement. Code and data are available at\nhttps://ineqmath.github.io/.", "AI": {"tldr": "论文提出了一种非正式但可验证的不等式证明任务，并发布了IneqMath数据集和LLM评估框架，揭示了当前LLM在严格证明中的不足。", "motivation": "现有数据集稀缺、合成或过于形式化，阻碍了不等式证明领域的进展。", "method": "将不等式证明分为两个可自动检查的子任务：边界估计和关系预测，并开发了IneqMath数据集和LLM评估框架。", "result": "评估29个领先LLM发现，即使在顶级模型下，严格证明的准确率低于10%，远低于仅考虑最终答案的准确率。", "conclusion": "当前LLM在严格证明中存在显著差距，未来研究方向包括定理引导推理和自我优化。"}}
{"id": "2506.07148", "pdf": "https://arxiv.org/pdf/2506.07148", "abs": "https://arxiv.org/abs/2506.07148", "authors": ["Yaping Chai", "Haoran Xie", "Joe S. Qin"], "title": "Semantic-preserved Augmentation with Confidence-weighted Fine-tuning for Aspect Category Sentiment Analysis", "categories": ["cs.CL"], "comment": "10 pages, 7 figures, 4 tables", "summary": "Large language model (LLM) is an effective approach to addressing data\nscarcity in low-resource scenarios. Recent existing research designs\nhand-crafted prompts to guide LLM for data augmentation. We introduce a data\naugmentation strategy for the aspect category sentiment analysis (ACSA) task\nthat preserves the original sentence semantics and has linguistic diversity,\nspecifically by providing a structured prompt template for an LLM to generate\npredefined content. In addition, we employ a post-processing technique to\nfurther ensure semantic consistency between the generated sentence and the\noriginal sentence. The augmented data increases the semantic coverage of the\ntraining distribution, enabling the model better to understand the relationship\nbetween aspect categories and sentiment polarities, enhancing its inference\ncapabilities. Furthermore, we propose a confidence-weighted fine-tuning\nstrategy to encourage the model to generate more confident and accurate\nsentiment polarity predictions. Compared with powerful and recent works, our\nmethod consistently achieves the best performance on four benchmark datasets\nover all baselines.", "AI": {"tldr": "论文提出了一种基于大语言模型（LLM）的数据增强策略，用于解决低资源场景下的数据稀缺问题，并通过结构化提示模板和后处理技术确保语义一致性和多样性。", "motivation": "解决低资源场景中数据稀缺问题，提升模型在方面类别情感分析（ACSA）任务中的性能。", "method": "设计结构化提示模板引导LLM生成数据，并采用后处理技术确保语义一致性；提出置信度加权微调策略以提高预测准确性。", "result": "在四个基准数据集上，该方法均优于现有基线模型。", "conclusion": "该方法通过数据增强和微调策略显著提升了ACSA任务的性能，为低资源场景提供了有效解决方案。"}}
{"id": "2506.07136", "pdf": "https://arxiv.org/pdf/2506.07136", "abs": "https://arxiv.org/abs/2506.07136", "authors": ["Huaize Liu", "Wenzhang Sun", "Qiyuan Zhang", "Donglin Di", "Biao Gong", "Hao Li", "Chen Wei", "Changqing Zou"], "title": "Hi-VAE: Efficient Video Autoencoding with Global and Detailed Motion", "categories": ["cs.CV"], "comment": null, "summary": "Recent breakthroughs in video autoencoders (Video AEs) have advanced video\ngeneration, but existing methods fail to efficiently model spatio-temporal\nredundancies in dynamics, resulting in suboptimal compression factors. This\nshortfall leads to excessive training costs for downstream tasks. To address\nthis, we introduce Hi-VAE, an efficient video autoencoding framework that\nhierarchically encode coarse-to-fine motion representations of video dynamics\nand formulate the decoding process as a conditional generation task.\nSpecifically, Hi-VAE decomposes video dynamics into two latent spaces: Global\nMotion, capturing overarching motion patterns, and Detailed Motion, encoding\nhigh-frequency spatial details. Using separate self-supervised motion encoders,\nwe compress video latents into compact motion representations to reduce\nredundancy significantly. A conditional diffusion decoder then reconstructs\nvideos by combining hierarchical global and detailed motions, enabling\nhigh-fidelity video reconstructions. Extensive experiments demonstrate that\nHi-VAE achieves a high compression factor of 1428$\\times$, almost 30$\\times$\nhigher than baseline methods (e.g., Cosmos-VAE at 48$\\times$), validating the\nefficiency of our approach. Meanwhile, Hi-VAE maintains high reconstruction\nquality at such high compression rates and performs effectively in downstream\ngenerative tasks. Moreover, Hi-VAE exhibits interpretability and scalability,\nproviding new perspectives for future exploration in video latent\nrepresentation and generation.", "AI": {"tldr": "Hi-VAE是一种高效的视频自编码框架，通过分层编码视频动态的粗到细运动表示，显著减少时空冗余，实现高压缩比和高保真重建。", "motivation": "现有视频自编码方法在动态建模中存在时空冗余问题，导致压缩效率低下和训练成本高。", "method": "Hi-VAE将视频动态分解为全局运动和细节运动两个潜在空间，使用自监督运动编码器压缩视频潜在表示，并通过条件扩散解码器重建视频。", "result": "实验表明，Hi-VAE的压缩比达到1428倍，远超基线方法（如Cosmos-VAE的48倍），同时保持高质量重建和下游生成任务的有效性。", "conclusion": "Hi-VAE在高效压缩和高保真重建方面表现出色，具有可解释性和可扩展性，为视频潜在表示和生成提供了新视角。"}}
{"id": "2506.07940", "pdf": "https://arxiv.org/pdf/2506.07940", "abs": "https://arxiv.org/abs/2506.07940", "authors": ["Christopher Subia-Waud"], "title": "Gradients: When Markets Meet Fine-tuning -- A Distributed Approach to Model Optimisation", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Foundation model fine-tuning faces a fundamental challenge: existing AutoML\nplatforms rely on single optimisation strategies that explore only a fraction\nof viable hyperparameter configurations. In this white paper, We introduce\nGradients, a decentralised AutoML platform that transforms hyperparameter\noptimisation into a competitive marketplace where independent miners compete to\ndiscover optimal configurations. Economic incentives align individual\nexploration with collective optimisation goals, driving systematic\ninvestigation of hyperparameter regions that centralised methods miss. We\nevaluate our approach across 180 controlled experiments spanning diverse model\narchitectures (70M to 70B parameters) and task types. Gradients achieves an\n82.8\\% win rate against HuggingFace AutoTrain and 100\\% against TogetherAI,\nDatabricks, and Google Cloud, with mean improvements of 11.8\\% and 42.1\\%\nrespectively. Complex reasoning and retrieval tasks show particularly strong\ngains of 30-40\\%, whilst diffusion models achieve 23.4\\% improvements for\nperson-specific generation. These results demonstrate that competitive,\neconomically-driven approaches can systematically discover superior\nconfigurations that centralised AutoML consistently miss.", "AI": {"tldr": "Gradients是一个去中心化的AutoML平台，通过竞争性市场机制优化超参数配置，显著优于传统中心化方法。", "motivation": "现有AutoML平台依赖单一优化策略，仅探索部分可行超参数配置，限制了性能提升。", "method": "Gradients将超参数优化转化为竞争性市场，通过经济激励驱动独立矿工探索最优配置。", "result": "在180个实验中，Gradients以82.8%的胜率优于HuggingFace AutoTrain，并在复杂任务中表现尤为突出。", "conclusion": "竞争性经济驱动方法能系统性发现中心化AutoML遗漏的优越配置。"}}
{"id": "2506.07154", "pdf": "https://arxiv.org/pdf/2506.07154", "abs": "https://arxiv.org/abs/2506.07154", "authors": ["Vicky Xefteri", "Tim Vieira", "Ryan Cotterell", "Afra Amini"], "title": "Syntactic Control of Language Models by Posterior Inference", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Controlling the syntactic structure of text generated by language models is\nvaluable for applications requiring clarity, stylistic consistency, or\ninterpretability, yet it remains a challenging task. In this paper, we argue\nthat sampling algorithms based on the posterior inference can effectively\nenforce a target constituency structure during generation. Our approach\ncombines sequential Monte Carlo, which estimates the posterior distribution by\nsampling from a proposal distribution, with a syntactic tagger that ensures\nthat each generated token aligns with the desired syntactic structure. Our\nexperiments with GPT2 and Llama3-8B models show that with an appropriate\nproposal distribution, we can improve syntactic accuracy, increasing the F1\nscore from $12.31$ (GPT2-large) and $35.33$ (Llama3-8B) to about $93$ in both\ncases without compromising the language model's fluency. These results\nunderscore both the complexity of syntactic control and the effectiveness of\nsampling algorithms, offering a promising approach for applications where\nprecise control over syntax is essential.", "AI": {"tldr": "论文提出了一种基于后验推断的采样算法，用于在语言模型生成文本时控制句法结构，显著提高了句法准确性。", "motivation": "控制语言模型生成文本的句法结构对于需要清晰性、风格一致性或可解释性的应用至关重要，但现有方法仍具挑战性。", "method": "结合顺序蒙特卡洛方法和句法标注器，确保生成的每个标记与目标句法结构对齐。", "result": "实验表明，该方法在GPT2和Llama3-8B模型上将句法准确性的F1分数从12.31和35.33提升至约93，同时保持流畅性。", "conclusion": "后验采样算法在句法控制上具有显著效果，为需要精确句法控制的应用提供了可行方案。"}}
{"id": "2506.07138", "pdf": "https://arxiv.org/pdf/2506.07138", "abs": "https://arxiv.org/abs/2506.07138", "authors": ["Hao Tang", "Chengchao Shen"], "title": "Learning Compact Vision Tokens for Efficient Large Multimodal Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": "The source code and trained weights are available at\n  https://github.com/visresearch/LLaVA-STF", "summary": "Large multimodal models (LMMs) suffer significant computational challenges\ndue to the high cost of Large Language Models (LLMs) and the quadratic\ncomplexity of processing long vision token sequences. In this paper, we explore\nthe spatial redundancy among vision tokens and shorten the length of vision\ntoken sequences for inference acceleration. Specifically, we propose a Spatial\nToken Fusion (STF) method to learn compact vision tokens for short vision token\nsequence, where spatial-adjacent tokens are fused into one. Meanwhile,\nweight-frozen vision encoder can not well adapt to the demand of extensive\ndownstream vision-language tasks. To this end, we further introduce a\nMulti-Block Token Fusion (MBTF) module to supplement multi-granularity features\nfor the reduced token sequence. Overall, we combine STF and MBTF module to\nbalance token reduction and information preservation, thereby improving\ninference efficiency without sacrificing multimodal reasoning capabilities.\nExperimental results demonstrate that our method based on LLaVA-1.5 achieves\ncomparable or even superior performance to the baseline on 8 popular\nvision-language benchmarks with only $25\\%$ vision tokens of baseline. The\nsource code and trained weights are available at\nhttps://github.com/visresearch/LLaVA-STF.", "AI": {"tldr": "论文提出了一种空间令牌融合（STF）方法和多块令牌融合（MBTF）模块，以减少视觉令牌序列的长度并提升推理效率，同时保持多模态推理能力。", "motivation": "大型多模态模型（LMMs）因大型语言模型（LLMs）的高计算成本和长视觉令牌序列的二次复杂度而面临计算挑战。", "method": "通过STF融合空间相邻令牌以减少序列长度，并通过MBTF补充多粒度特征以保持信息完整性。", "result": "在8个流行的视觉语言基准测试中，仅使用基线25%的视觉令牌，性能与基线相当或更优。", "conclusion": "STF和MBTF的结合在提升推理效率的同时，未牺牲多模态推理能力，为LMMs的优化提供了有效方法。"}}
{"id": "2506.07963", "pdf": "https://arxiv.org/pdf/2506.07963", "abs": "https://arxiv.org/abs/2506.07963", "authors": ["Jixiang Hong", "Yiran Zhang", "Guanzhong Wang", "Yi Liu", "Ji-Rong Wen", "Rui Yan"], "title": "Reinforcing Multimodal Understanding and Generation with Dual Self-rewards", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "Building upon large language models (LLMs), recent large multimodal models\n(LMMs) unify cross-model understanding and generation into a single framework.\nHowever, LMMs still struggle to achieve accurate image-text alignment, prone to\ngenerating text responses contradicting the visual input or failing to follow\nthe text-to-image prompts. Current solutions require external supervision\n(e.g., human feedback or reward models) and only address unidirectional\ntasks-either understanding or generation. In this work, based on the\nobservation that understanding and generation are inverse dual tasks, we\nintroduce a self-supervised dual reward mechanism to reinforce the\nunderstanding and generation capabilities of LMMs. Specifically, we sample\nmultiple outputs for a given input in one task domain, then reverse the\ninput-output pairs to compute the dual likelihood of the model as self-rewards\nfor optimization. Extensive experimental results on visual understanding and\ngeneration benchmarks demonstrate that our method can effectively enhance the\nperformance of the model without any external supervision, especially achieving\nremarkable improvements in text-to-image tasks.", "AI": {"tldr": "本文提出了一种自监督的双奖励机制，通过理解与生成的逆对偶任务特性，提升大型多模态模型的性能，无需外部监督。", "motivation": "大型多模态模型（LMMs）在图像-文本对齐方面表现不佳，现有方法依赖外部监督且仅解决单向任务。", "method": "提出自监督双奖励机制，通过采样多输出并反转输入-输出对计算双似然作为自奖励进行优化。", "result": "实验表明，该方法显著提升了模型在视觉理解和生成任务中的性能，尤其在文本到图像任务中表现突出。", "conclusion": "自监督双奖励机制有效提升了LMMs的理解与生成能力，无需外部监督。"}}
{"id": "2506.07160", "pdf": "https://arxiv.org/pdf/2506.07160", "abs": "https://arxiv.org/abs/2506.07160", "authors": ["Yikun Wang", "Yibin Wang", "Dianyi Wang", "Zimian Peng", "Qipeng Guo", "Dacheng Tao", "Jiaqi Wang"], "title": "GeometryZero: Improving Geometry Solving for LLM with Group Contrastive Policy Optimization", "categories": ["cs.CL"], "comment": null, "summary": "Recent advances in large language models (LLMs) have demonstrated remarkable\ncapabilities across diverse domains, particularly in mathematical reasoning,\namid which geometry problem solving remains a challenging area where auxiliary\nconstruction plays a enssential role. Existing approaches either achieve\nsuboptimal performance or rely on massive LLMs (e.g., GPT-4o), incurring\nmassive computational costs. We posit that reinforcement learning with\nverifiable reward (e.g., GRPO) offers a promising direction for training\nsmaller models that effectively combine auxiliary construction with robust\ngeometric reasoning. However, directly applying GRPO to geometric reasoning\npresents fundamental limitations due to its dependence on unconditional\nrewards, which leads to indiscriminate and counterproductive auxiliary\nconstructions. To address these challenges, we propose Group Contrastive Policy\nOptimization (GCPO), a novel reinforcement learning framework featuring two key\ninnovations: (1) Group Contrastive Masking, which adaptively provides positive\nor negative reward signals for auxiliary construction based on contextual\nutility, and a (2) length reward that promotes longer reasoning chains.\nBuilding on GCPO, we develop GeometryZero, a family of affordable-size\ngeometric reasoning models that judiciously determine when to employ auxiliary\nconstruction. Our extensive empirical evaluation across popular geometric\nbenchmarks (Geometry3K, MathVista) demonstrates that GeometryZero models\nconsistently outperform baselines (e.g. GRPO), achieving an average improvement\nof 4.29% across all benchmarks.", "AI": {"tldr": "论文提出了一种名为GCPO的新型强化学习框架，用于训练小型模型在几何问题中结合辅助构造与推理，显著提升了性能。", "motivation": "几何问题解决中辅助构造是关键，但现有方法要么性能不佳，要么依赖计算成本高的大模型。", "method": "提出GCPO框架，包含Group Contrastive Masking和长度奖励机制，优化辅助构造的决策。", "result": "GeometryZero模型在多个几何基准测试中平均提升4.29%，优于基线方法。", "conclusion": "GCPO框架为小型模型在几何推理中高效使用辅助构造提供了可行方案。"}}
{"id": "2506.07155", "pdf": "https://arxiv.org/pdf/2506.07155", "abs": "https://arxiv.org/abs/2506.07155", "authors": ["Van Nguyen Nguyen", "Christian Forster", "Sindi Shkodrani", "Vincent Lepetit", "Bugra Tekin", "Cem Keskin", "Tomas Hodan"], "title": "GoTrack: Generic 6DoF Object Pose Refinement and Tracking", "categories": ["cs.CV"], "comment": null, "summary": "We introduce GoTrack, an efficient and accurate CAD-based method for 6DoF\nobject pose refinement and tracking, which can handle diverse objects without\nany object-specific training. Unlike existing tracking methods that rely solely\non an analysis-by-synthesis approach for model-to-frame registration, GoTrack\nadditionally integrates frame-to-frame registration, which saves compute and\nstabilizes tracking. Both types of registration are realized by optical flow\nestimation. The model-to-frame registration is noticeably simpler than in\nexisting methods, relying only on standard neural network blocks (a transformer\nis trained on top of DINOv2) and producing reliable pose confidence scores\nwithout a scoring network. For the frame-to-frame registration, which is an\neasier problem as consecutive video frames are typically nearly identical, we\nemploy a light off-the-shelf optical flow model. We demonstrate that GoTrack\ncan be seamlessly combined with existing coarse pose estimation methods to\ncreate a minimal pipeline that reaches state-of-the-art RGB-only results on\nstandard benchmarks for 6DoF object pose estimation and tracking. Our source\ncode and trained models are publicly available at\nhttps://github.com/facebookresearch/gotrack", "AI": {"tldr": "GoTrack是一种基于CAD的高效6DoF物体姿态优化与跟踪方法，无需特定物体训练，结合模型到帧和帧到帧的配准，通过光流估计实现。", "motivation": "现有跟踪方法仅依赖模型到帧配准，计算量大且稳定性不足，GoTrack通过结合帧到帧配准优化性能。", "method": "使用标准神经网络模块（基于DINOv2的Transformer）简化模型到帧配准，并采用轻量级光流模型处理帧到帧配准。", "result": "GoTrack与现有粗姿态估计方法结合，在标准6DoF姿态估计和跟踪基准测试中达到RGB-only的先进水平。", "conclusion": "GoTrack提供了一种高效、稳定的6DoF物体姿态跟踪解决方案，适用于多样化物体，且无需特定训练。"}}
{"id": "2506.07982", "pdf": "https://arxiv.org/pdf/2506.07982", "abs": "https://arxiv.org/abs/2506.07982", "authors": ["Victor Barres", "Honghua Dong", "Soham Ray", "Xujie Si", "Karthik Narasimhan"], "title": "$τ^2$-Bench: Evaluating Conversational Agents in a Dual-Control Environment", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Existing benchmarks for conversational AI agents simulate single-control\nenvironments, where only the AI agent can use tools to interact with the world,\nwhile the user remains a passive information provider. This differs from\nreal-world scenarios like technical support, where users need to actively\nparticipate in modifying the state of the (shared) world. In order to address\nthis gap, we introduce $\\tau^2$-bench, with four key contributions:\n  1) A novel Telecom dual-control domain modeled as a Dec-POMDP, where both\nagent and user make use of tools to act in a shared, dynamic environment that\ntests both agent coordination and communication,\n  2) A compositional task generator that programmatically creates diverse,\nverifiable tasks from atomic components, ensuring domain coverage and\ncontrolled complexity,\n  3) A reliable user simulator tightly coupled with the environment, whose\nbehavior is constrained by tools and observable states, improving simulation\nfidelity,\n  4) Fine-grained analysis of agent performance through multiple ablations\nincluding separating errors arising from reasoning vs\ncommunication/coordination.\n  In particular, our experiments show significant performance drops when agents\nshift from no-user to dual-control, highlighting the challenges of guiding\nusers. Overall, $\\tau^2$-bench provides a controlled testbed for agents that\nmust both reason effectively and guide user actions.", "AI": {"tldr": "论文介绍了$\tau^2$-bench，一个用于评估对话AI代理在双控制环境中的性能的新基准，填补了现有单控制环境模拟的不足。", "motivation": "现有对话AI代理的基准测试仅模拟单控制环境，而现实场景（如技术支持）需要用户主动参与修改共享世界状态，因此需要更真实的测试环境。", "method": "提出$\tau^2$-bench，包括：1) 电信双控制领域的Dec-POMDP建模；2) 任务生成器；3) 可靠用户模拟器；4) 细粒度性能分析。", "result": "实验显示，代理在双控制环境中性能显著下降，突显了引导用户的挑战。", "conclusion": "$\tau^2$-bench为需要有效推理和引导用户行为的代理提供了可控测试平台。"}}
{"id": "2506.07169", "pdf": "https://arxiv.org/pdf/2506.07169", "abs": "https://arxiv.org/abs/2506.07169", "authors": ["Washington Cunha", "Leonardo Rocha", "Marcos André Gonçalves"], "title": "CTDGSI: A comprehensive exploitation of instance selection methods for automatic text classification. VII Concurso de Teses, Dissertações e Trabalhos de Graduação em SI -- XXI Simpósio Brasileiro de Sistemas de Informação", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 5 figures, 2 tables", "summary": "Progress in Natural Language Processing (NLP) has been dictated by the rule\nof more: more data, more computing power and more complexity, best exemplified\nby the Large Language Models. However, training (or fine-tuning) large dense\nmodels for specific applications usually requires significant amounts of\ncomputing resources. This \\textbf{Ph.D. dissertation} focuses on an\nunder-investi\\-gated NLP data engineering technique, whose potential is\nenormous in the current scenario known as Instance Selection (IS). The IS goal\nis to reduce the training set size by removing noisy or redundant instances\nwhile maintaining the effectiveness of the trained models and reducing the\ntraining process cost. We provide a comprehensive and scientifically sound\ncomparison of IS methods applied to an essential NLP task -- Automatic Text\nClassification (ATC), considering several classification solutions and many\ndatasets. Our findings reveal a significant untapped potential for IS\nsolutions. We also propose two novel IS solutions that are noise-oriented and\nredundancy-aware, specifically designed for large datasets and transformer\narchitectures. Our final solution achieved an average reduction of 41\\% in\ntraining sets, while maintaining the same levels of effectiveness in all\ndatasets. Importantly, our solutions demonstrated speedup improvements of 1.67x\n(up to 2.46x), making them scalable for datasets with hundreds of thousands of\ndocuments.", "AI": {"tldr": "该博士论文探讨了自然语言处理中的实例选择（IS）技术，旨在通过减少训练集中的噪声和冗余实例来降低训练成本，同时保持模型效果。研究提出了两种新型IS方法，显著减少了训练集规模并提升了训练速度。", "motivation": "当前NLP领域依赖大量数据和计算资源，训练大型密集模型成本高昂。实例选择技术潜力巨大但研究不足，本文旨在填补这一空白。", "method": "研究对多种IS方法进行了全面比较，并提出了两种新型IS方法：噪声导向和冗余感知的解决方案，专门针对大型数据集和Transformer架构。", "result": "实验结果显示，最终解决方案平均减少了41%的训练集规模，同时保持了模型效果，训练速度提升了1.67倍（最高2.46倍）。", "conclusion": "实例选择技术在NLP中具有显著潜力，能够有效降低训练成本并提升效率，尤其适用于大规模数据集。"}}
{"id": "2506.07164", "pdf": "https://arxiv.org/pdf/2506.07164", "abs": "https://arxiv.org/abs/2506.07164", "authors": ["Qiong Chang", "Xinyuan Chen", "Xiang Li", "Weimin Wang", "Jun Miyazaki"], "title": "Faster than Fast: Accelerating Oriented FAST Feature Detection on Low-end Embedded GPUs", "categories": ["cs.CV"], "comment": null, "summary": "The visual-based SLAM (Simultaneous Localization and Mapping) is a technology\nwidely used in applications such as robotic navigation and virtual reality,\nwhich primarily focuses on detecting feature points from visual images to\nconstruct an unknown environmental map and simultaneously determines its own\nlocation. It usually imposes stringent requirements on hardware power\nconsumption, processing speed and accuracy. Currently, the ORB (Oriented FAST\nand Rotated BRIEF)-based SLAM systems have exhibited superior performance in\nterms of processing speed and robustness. However, they still fall short of\nmeeting the demands for real-time processing on mobile platforms. This\nlimitation is primarily due to the time-consuming Oriented FAST calculations\naccounting for approximately half of the entire SLAM system. This paper\npresents two methods to accelerate the Oriented FAST feature detection on\nlow-end embedded GPUs. These methods optimize the most time-consuming steps in\nOriented FAST feature detection: FAST feature point detection and Harris corner\ndetection, which is achieved by implementing a binary-level encoding strategy\nto determine candidate points quickly and a separable Harris detection strategy\nwith efficient low-level GPU hardware-specific instructions. Extensive\nexperiments on a Jetson TX2 embedded GPU demonstrate an average speedup of over\n7.3 times compared to widely used OpenCV with GPU support. This significant\nimprovement highlights its effectiveness and potential for real-time\napplications in mobile and resource-constrained environments.", "AI": {"tldr": "论文提出两种方法加速低端嵌入式GPU上的Oriented FAST特征检测，显著提升SLAM系统的实时性能。", "motivation": "当前基于ORB的SLAM系统在移动平台上难以满足实时处理需求，主要因Oriented FAST计算耗时。", "method": "通过二进制编码策略快速确定候选点，以及利用低层GPU指令实现可分离Harris检测策略。", "result": "在Jetson TX2上实验显示，相比OpenCV GPU版本，平均加速7.3倍。", "conclusion": "该方法在移动和资源受限环境中具有实时应用的潜力。"}}
{"id": "2506.08012", "pdf": "https://arxiv.org/pdf/2506.08012", "abs": "https://arxiv.org/abs/2506.08012", "authors": ["Penghao Wu", "Shengnan Ma", "Bo Wang", "Jiaheng Yu", "Lewei Lu", "Ziwei Liu"], "title": "GUI-Reflection: Empowering Multimodal GUI Models with Self-Reflection Behavior", "categories": ["cs.AI", "cs.CV"], "comment": "Project Page at https://penghao-wu.github.io/GUI_Reflection/", "summary": "Multimodal Large Language Models (MLLMs) have shown great potential in\nrevolutionizing Graphical User Interface (GUI) automation. However, existing\nGUI models mostly rely on learning from nearly error-free offline trajectories,\nthus lacking reflection and error recovery capabilities. To bridge this gap, we\npropose GUI-Reflection, a novel framework that explicitly integrates\nself-reflection and error correction capabilities into end-to-end multimodal\nGUI models throughout dedicated training stages: GUI-specific pre-training,\noffline supervised fine-tuning (SFT), and online reflection tuning.\nGUI-reflection enables self-reflection behavior emergence with fully automated\ndata generation and learning processes without requiring any human annotation.\nSpecifically, 1) we first propose scalable data pipelines to automatically\nconstruct reflection and error correction data from existing successful\ntrajectories. While existing GUI models mainly focus on grounding and UI\nunderstanding ability, we propose the GUI-Reflection Task Suite to learn and\nevaluate reflection-oriented abilities explicitly. 2) Furthermore, we built a\ndiverse and efficient environment for online training and data collection of\nGUI models on mobile devices. 3) We also present an iterative online reflection\ntuning algorithm leveraging the proposed environment, enabling the model to\ncontinuously enhance its reflection and error correction abilities. Our\nframework equips GUI agents with self-reflection and correction capabilities,\npaving the way for more robust, adaptable, and intelligent GUI automation, with\nall data, models, environments, and tools to be released publicly.", "AI": {"tldr": "GUI-Reflection框架通过自反思和错误纠正能力增强多模态GUI模型，实现更鲁棒的GUI自动化。", "motivation": "现有GUI模型依赖无错误的离线轨迹，缺乏反思和错误恢复能力，限制了其实际应用。", "method": "提出GUI-Reflection框架，包含GUI特定预训练、离线监督微调和在线反思调优三个阶段，通过自动化数据生成和在线环境训练模型。", "result": "框架实现了无需人工标注的自反思行为，并通过GUI-Reflection Task Suite评估反思能力。", "conclusion": "GUI-Reflection为GUI自动化提供了更智能、适应性更强的解决方案，相关数据和工具将公开。"}}
{"id": "2506.07171", "pdf": "https://arxiv.org/pdf/2506.07171", "abs": "https://arxiv.org/abs/2506.07171", "authors": ["Chenlong Zhang", "Zhuoran Jin", "Hongbang Yuan", "Jiaheng Wei", "Tong Zhou", "Kang Liu", "Jun Zhao", "Yubo Chen"], "title": "RULE: Reinforcement UnLEarning Achieves Forget-Retain Pareto Optimality", "categories": ["cs.CL", "cs.LG"], "comment": "Paper under review", "summary": "The widespread deployment of Large Language Models (LLMs) trained on massive,\nuncurated corpora has raised growing concerns about the inclusion of sensitive,\ncopyrighted, or illegal content. This has led to increasing interest in LLM\nunlearning: the task of selectively removing specific information from a model\nwithout retraining from scratch or degrading overall utility. However, existing\nmethods often rely on large-scale forget and retain datasets, and suffer from\nunnatural responses, poor generalization, or catastrophic utility loss. In this\nwork, we propose Reinforcement UnLearning (RULE), an efficient framework that\nformulates unlearning as a refusal boundary optimization problem. RULE is\ntrained with a small portion of the forget set and synthesized boundary\nqueries, using a verifiable reward function that encourages safe refusal on\nforget--related queries while preserving helpful responses on permissible\ninputs. We provide both theoretical and empirical evidence demonstrating the\neffectiveness of RULE in achieving targeted unlearning without compromising\nmodel utility. Experimental results show that, with only $12%$ forget set and\n$8%$ synthesized boundary data, RULE outperforms existing baselines by up to\n$17.5%$ forget quality and $16.3%$ naturalness response while maintaining\ngeneral utility, achieving forget--retain Pareto optimality. Remarkably, we\nfurther observe that RULE improves the naturalness of model outputs, enhances\ntraining efficiency, and exhibits strong generalization ability, generalizing\nrefusal behavior to semantically related but unseen queries.", "AI": {"tldr": "论文提出了一种名为RULE的高效框架，通过强化学习实现LLM的选择性遗忘，仅需少量遗忘数据和合成边界查询即可优化拒绝边界，显著提升遗忘质量和响应自然度。", "motivation": "大规模语言模型可能包含敏感、受版权保护或非法内容，现有遗忘方法依赖大量数据且效果不佳，因此需要一种高效的选择性遗忘方法。", "method": "提出Reinforcement UnLearning (RULE)框架，将遗忘问题建模为拒绝边界优化，使用少量遗忘数据和合成边界查询，通过可验证的奖励函数训练模型。", "result": "实验表明，RULE仅需12%遗忘数据和8%合成数据，在遗忘质量和响应自然度上优于基线方法17.5%和16.3%，同时保持模型实用性。", "conclusion": "RULE是一种高效的选择性遗忘方法，能显著提升模型性能，同时具备泛化能力和训练效率。"}}
{"id": "2506.07177", "pdf": "https://arxiv.org/pdf/2506.07177", "abs": "https://arxiv.org/abs/2506.07177", "authors": ["Sangwon Jang", "Taekyung Ki", "Jaehyeong Jo", "Jaehong Yoon", "Soo Ye Kim", "Zhe Lin", "Sung Ju Hwang"], "title": "Frame Guidance: Training-Free Guidance for Frame-Level Control in Video Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://frame-guidance-video.github.io/", "summary": "Advancements in diffusion models have significantly improved video quality,\ndirecting attention to fine-grained controllability. However, many existing\nmethods depend on fine-tuning large-scale video models for specific tasks,\nwhich becomes increasingly impractical as model sizes continue to grow. In this\nwork, we present Frame Guidance, a training-free guidance for controllable\nvideo generation based on frame-level signals, such as keyframes, style\nreference images, sketches, or depth maps. For practical training-free\nguidance, we propose a simple latent processing method that dramatically\nreduces memory usage, and apply a novel latent optimization strategy designed\nfor globally coherent video generation. Frame Guidance enables effective\ncontrol across diverse tasks, including keyframe guidance, stylization, and\nlooping, without any training, compatible with any video models. Experimental\nresults show that Frame Guidance can produce high-quality controlled videos for\na wide range of tasks and input signals.", "AI": {"tldr": "提出了一种无需训练的帧级信号引导方法（Frame Guidance），用于可控视频生成，显著减少内存使用，并支持多样化任务。", "motivation": "现有方法依赖大规模视频模型的微调，随着模型规模增长变得不切实际，因此需要一种无需训练的解决方案。", "method": "通过简单的潜在处理方法减少内存使用，并采用新颖的潜在优化策略实现全局一致性视频生成。", "result": "实验表明，Frame Guidance能高质量完成多种任务（如关键帧引导、风格化、循环等），兼容所有视频模型。", "conclusion": "Frame Guidance为可控视频生成提供了一种高效、无需训练的通用解决方案。"}}
{"id": "2309.11082", "pdf": "https://arxiv.org/pdf/2309.11082", "abs": "https://arxiv.org/abs/2309.11082", "authors": ["Chen Jiang", "Hong Liu", "Xuzheng Yu", "Qing Wang", "Yuan Cheng", "Jia Xu", "Zhongyi Liu", "Qingpei Guo", "Wei Chu", "Ming Yang", "Yuan Qi"], "title": "Dual-Modal Attention-Enhanced Text-Video Retrieval with Triplet Partial Margin Contrastive Learning", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.MM"], "comment": "Accepted by ACM MM 2023", "summary": "In recent years, the explosion of web videos makes text-video retrieval\nincreasingly essential and popular for video filtering, recommendation, and\nsearch. Text-video retrieval aims to rank relevant text/video higher than\nirrelevant ones. The core of this task is to precisely measure the cross-modal\nsimilarity between texts and videos. Recently, contrastive learning methods\nhave shown promising results for text-video retrieval, most of which focus on\nthe construction of positive and negative pairs to learn text and video\nrepresentations. Nevertheless, they do not pay enough attention to hard\nnegative pairs and lack the ability to model different levels of semantic\nsimilarity. To address these two issues, this paper improves contrastive\nlearning using two novel techniques. First, to exploit hard examples for robust\ndiscriminative power, we propose a novel Dual-Modal Attention-Enhanced Module\n(DMAE) to mine hard negative pairs from textual and visual clues. By further\nintroducing a Negative-aware InfoNCE (NegNCE) loss, we are able to adaptively\nidentify all these hard negatives and explicitly highlight their impacts in the\ntraining loss. Second, our work argues that triplet samples can better model\nfine-grained semantic similarity compared to pairwise samples. We thereby\npresent a new Triplet Partial Margin Contrastive Learning (TPM-CL) module to\nconstruct partial order triplet samples by automatically generating\nfine-grained hard negatives for matched text-video pairs. The proposed TPM-CL\ndesigns an adaptive token masking strategy with cross-modal interaction to\nmodel subtle semantic differences. Extensive experiments demonstrate that the\nproposed approach outperforms existing methods on four widely-used text-video\nretrieval datasets, including MSR-VTT, MSVD, DiDeMo and ActivityNet.", "AI": {"tldr": "该论文提出了一种改进的对比学习方法，通过挖掘硬负样本和建模细粒度语义相似性，提升了文本-视频检索的性能。", "motivation": "现有的对比学习方法在文本-视频检索中未充分关注硬负样本和不同层次的语义相似性，导致性能受限。", "method": "提出了Dual-Modal Attention-Enhanced Module (DMAE)挖掘硬负样本，并引入NegNCE损失；设计了Triplet Partial Margin Contrastive Learning (TPM-CL)模块建模细粒度语义相似性。", "result": "在四个广泛使用的文本-视频检索数据集（MSR-VTT、MSVD、DiDeMo和ActivityNet）上表现优于现有方法。", "conclusion": "通过改进对比学习，该方法显著提升了文本-视频检索的准确性和鲁棒性。"}}
{"id": "2506.07180", "pdf": "https://arxiv.org/pdf/2506.07180", "abs": "https://arxiv.org/abs/2506.07180", "authors": ["Wenrui Zhou", "Shu Yang", "Qingsong Yang", "Zikun Guo", "Lijie Hu", "Di Wang"], "title": "Flattery in Motion: Benchmarking and Analyzing Sycophancy in Video-LLMs", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "24 pages", "summary": "As video large language models (Video-LLMs) become increasingly integrated\ninto real-world applications that demand grounded multimodal reasoning,\nensuring their factual consistency and reliability is of critical importance.\nHowever, sycophancy, the tendency of these models to align with user input even\nwhen it contradicts the visual evidence, undermines their trustworthiness in\nsuch contexts. Current sycophancy research has largely overlooked its specific\nmanifestations in the video-language domain, resulting in a notable absence of\nsystematic benchmarks and targeted evaluations to understand how Video-LLMs\nrespond under misleading user input. To fill this gap, we propose VISE\n(Video-LLM Sycophancy Benchmarking and Evaluation), the first dedicated\nbenchmark designed to evaluate sycophantic behavior in state-of-the-art\nVideo-LLMs across diverse question formats, prompt biases, and visual reasoning\ntasks. Specifically, VISE pioneeringly brings linguistic perspectives on\nsycophancy into the visual domain, enabling fine-grained analysis across\nmultiple sycophancy types and interaction patterns. In addition, we explore\nkey-frame selection as an interpretable, training-free mitigation strategy,\nwhich reveals potential paths for reducing sycophantic bias by strengthening\nvisual grounding.", "AI": {"tldr": "论文提出了VISE基准，用于评估视频大语言模型（Video-LLMs）在误导性用户输入下的迎合行为，填补了该领域系统性评测的空白。", "motivation": "视频大语言模型在现实应用中需要可靠的多模态推理能力，但其迎合用户输入的行为（即使与视觉证据矛盾）影响了可信度。当前研究缺乏针对视频语言领域的系统性评测。", "method": "提出VISE基准，评估Video-LLMs在不同问题格式、提示偏见和视觉推理任务中的迎合行为，并结合关键帧选择作为缓解策略。", "result": "VISE首次将语言领域的迎合行为分析引入视觉领域，并发现关键帧选择可减少迎合偏见。", "conclusion": "VISE为理解和减少Video-LLMs的迎合行为提供了系统工具，关键帧选择是一种有效的无训练缓解方法。"}}
{"id": "2506.07188", "pdf": "https://arxiv.org/pdf/2506.07188", "abs": "https://arxiv.org/abs/2506.07188", "authors": ["Ni Ding", "Lei He", "Shengbo Eben Li", "Keqiang Li"], "title": "Hierarchical Feature-level Reverse Propagation for Post-Training Neural Networks", "categories": ["cs.CV", "I.2.10"], "comment": "13 pages, 7 figures,", "summary": "End-to-end autonomous driving has emerged as a dominant paradigm, yet its\nhighly entangled black-box models pose significant challenges in terms of\ninterpretability and safety assurance. To improve model transparency and\ntraining flexibility, this paper proposes a hierarchical and decoupled\npost-training framework tailored for pretrained neural networks. By\nreconstructing intermediate feature maps from ground-truth labels, surrogate\nsupervisory signals are introduced at transitional layers to enable independent\ntraining of specific components, thereby avoiding the complexity and coupling\nof conventional end-to-end backpropagation and providing interpretable insights\ninto networks' internal mechanisms. To the best of our knowledge, this is the\nfirst method to formalize feature-level reverse computation as well-posed\noptimization problems, which we rigorously reformulate as systems of linear\nequations or least squares problems. This establishes a novel and efficient\ntraining paradigm that extends gradient backpropagation to feature\nbackpropagation. Extensive experiments on multiple standard image\nclassification benchmarks demonstrate that the proposed method achieves\nsuperior generalization performance and computational efficiency compared to\ntraditional training approaches, validating its effectiveness and potential.", "AI": {"tldr": "本文提出了一种分层解耦的后训练框架，通过重构中间特征图并引入代理监督信号，提升预训练神经网络的透明度和训练灵活性。", "motivation": "端到端自动驾驶的黑盒模型在可解释性和安全性方面存在挑战，需要提高模型的透明度和训练灵活性。", "method": "通过从真实标签重构中间特征图，引入代理监督信号，独立训练特定组件，避免传统端到端反向传播的复杂性。", "result": "在多个标准图像分类基准测试中，该方法表现出优越的泛化性能和计算效率。", "conclusion": "该方法为特征级反向计算提供了新的高效训练范式，验证了其有效性和潜力。"}}
{"id": "2406.00971", "pdf": "https://arxiv.org/pdf/2406.00971", "abs": "https://arxiv.org/abs/2406.00971", "authors": ["Vahid Azizi", "Fatemeh Koochaki"], "title": "MiniGPT-Reverse-Designing: Predicting Image Adjustments Utilizing MiniGPT-4", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 7 figures", "summary": "Vision-Language Models (VLMs) have recently seen significant advancements\nthrough integrating with Large Language Models (LLMs). The VLMs, which process\nimage and text modalities simultaneously, have demonstrated the ability to\nlearn and understand the interaction between images and texts across various\nmulti-modal tasks. Reverse designing, which could be defined as a complex\nvision-language task, aims to predict the edits and their parameters, given a\nsource image, an edited version, and an optional high-level textual edit\ndescription. This task requires VLMs to comprehend the interplay between the\nsource image, the edited version, and the optional textual context\nsimultaneously, going beyond traditional vision-language tasks. In this paper,\nwe extend and fine-tune MiniGPT-4 for the reverse designing task. Our\nexperiments demonstrate the extensibility of off-the-shelf VLMs, specifically\nMiniGPT-4, for more complex tasks such as reverse designing. Code is available\nat this \\href{https://github.com/VahidAz/MiniGPT-Reverse-Designing}", "AI": {"tldr": "论文探讨了通过结合大型语言模型（LLMs）提升视觉语言模型（VLMs）的能力，并扩展MiniGPT-4用于逆向设计任务，展示了其处理复杂多模态任务的潜力。", "motivation": "研究旨在探索VLMs在复杂视觉语言任务（如逆向设计）中的表现，验证现有模型（如MiniGPT-4）的可扩展性。", "method": "通过扩展和微调MiniGPT-4模型，使其能够处理逆向设计任务，即根据源图像、编辑版本和可选文本描述预测编辑操作及其参数。", "result": "实验证明，MiniGPT-4能够成功应用于逆向设计任务，展示了现有VLM模型在复杂任务中的潜力。", "conclusion": "研究表明，现有VLM模型（如MiniGPT-4）可通过微调适应复杂任务，为多模态任务的研究提供了新方向。"}}
{"id": "2506.07245", "pdf": "https://arxiv.org/pdf/2506.07245", "abs": "https://arxiv.org/abs/2506.07245", "authors": ["Wenxuan Xie", "Yaxun Dai", "Wenhao Jiang"], "title": "SDE-SQL: Enhancing Text-to-SQL Generation in Large Language Models via Self-Driven Exploration with SQL Probes", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Recent advancements in large language models (LLMs) have significantly\nimproved performance on the Text-to-SQL task. However, prior approaches\ntypically rely on static, pre-processed database information provided at\ninference time, which limits the model's ability to fully understand the\ndatabase contents. Without dynamic interaction, LLMs are constrained to fixed,\nhuman-provided context and cannot autonomously explore the underlying data. To\naddress this limitation, we propose SDE-SQL, a framework that enables large\nlanguage models to perform self-driven exploration of databases during\ninference. This is accomplished by generating and executing SQL probes, which\nallow the model to actively retrieve information from the database and\niteratively update its understanding of the data. Unlike prior methods, SDE-SQL\noperates in a zero-shot setting, without relying on any question-SQL pairs as\nin-context demonstrations. When evaluated on the BIRD benchmark with\nQwen2.5-72B-Instruct, SDE-SQL achieves an 8.02% relative improvement in\nexecution accuracy over the vanilla Qwen2.5-72B-Instruct baseline, establishing\na new state-of-the-art among methods based on open-source models without\nsupervised fine-tuning (SFT) or model ensembling. Moreover, with SFT, the\nperformance of SDE-SQL can be further enhanced, yielding an additional 0.52%\nimprovement.", "AI": {"tldr": "SDE-SQL框架通过动态生成和执行SQL探针，使大语言模型在推理时自主探索数据库，显著提升了Text-to-SQL任务的性能。", "motivation": "现有方法依赖静态预处理数据库信息，限制了模型对数据库内容的动态理解能力。", "method": "提出SDE-SQL框架，通过生成和执行SQL探针实现数据库的自主探索，无需依赖上下文示例。", "result": "在BIRD基准测试中，SDE-SQL比基线模型提升了8.02%的执行准确率，且通过监督微调可进一步提升0.52%。", "conclusion": "SDE-SQL为零样本Text-to-SQL任务提供了新的解决方案，显著提升了模型性能。"}}
{"id": "2506.07196", "pdf": "https://arxiv.org/pdf/2506.07196", "abs": "https://arxiv.org/abs/2506.07196", "authors": ["Mengya Xu", "Zhongzhen Huang", "Dillan Imans", "Yiru Ye", "Xiaofan Zhang", "Qi Dou"], "title": "SAP-Bench: Benchmarking Multimodal Large Language Models in Surgical Action Planning", "categories": ["cs.CV", "cs.CL"], "comment": "11 pages, 4 figures", "summary": "Effective evaluation is critical for driving advancements in MLLM research.\nThe surgical action planning (SAP) task, which aims to generate future action\nsequences from visual inputs, demands precise and sophisticated analytical\ncapabilities. Unlike mathematical reasoning, surgical decision-making operates\nin life-critical domains and requires meticulous, verifiable processes to\nensure reliability and patient safety. This task demands the ability to\ndistinguish between atomic visual actions and coordinate complex, long-horizon\nprocedures, capabilities that are inadequately evaluated by current benchmarks.\nTo address this gap, we introduce SAP-Bench, a large-scale, high-quality\ndataset designed to enable multimodal large language models (MLLMs) to perform\ninterpretable surgical action planning. Our SAP-Bench benchmark, derived from\nthe cholecystectomy procedures context with the mean duration of 1137.5s, and\nintroduces temporally-grounded surgical action annotations, comprising the\n1,226 clinically validated action clips (mean duration: 68.7s) capturing five\nfundamental surgical actions across 74 procedures. The dataset provides 1,152\nstrategically sampled current frames, each paired with the corresponding next\naction as multimodal analysis anchors. We propose the MLLM-SAP framework that\nleverages MLLMs to generate next action recommendations from the current\nsurgical scene and natural language instructions, enhanced with injected\nsurgical domain knowledge. To assess our dataset's effectiveness and the\nbroader capabilities of current models, we evaluate seven state-of-the-art\nMLLMs (e.g., OpenAI-o1, GPT-4o, QwenVL2.5-72B, Claude-3.5-Sonnet, GeminiPro2.5,\nStep-1o, and GLM-4v) and reveal critical gaps in next action prediction\nperformance.", "AI": {"tldr": "论文介绍了SAP-Bench数据集和MLLM-SAP框架，用于评估多模态大语言模型在手术动作规划任务中的表现，揭示了当前模型的不足。", "motivation": "手术动作规划任务需要精确的分析能力，但现有基准无法充分评估模型的复杂决策能力，因此需要新的数据集和方法。", "method": "提出了SAP-Bench数据集，包含临床验证的手术动作片段，并开发了MLLM-SAP框架，利用MLLMs生成下一步动作建议。", "result": "评估了七种先进MLLMs，发现它们在下一步动作预测任务中存在显著不足。", "conclusion": "SAP-Bench和MLLM-SAP为手术动作规划任务提供了新的评估工具，揭示了当前模型的局限性，为未来研究指明了方向。"}}
{"id": "2411.12262", "pdf": "https://arxiv.org/pdf/2411.12262", "abs": "https://arxiv.org/abs/2411.12262", "authors": ["Raphael Merx", "Adérito José Guterres Correia", "Hanna Suominen", "Ekaterina Vylomova"], "title": "Low-resource Machine Translation: what for? who for? An observational study on a dedicated Tetun language translation service", "categories": ["cs.CL", "cs.AI"], "comment": "to be published in LoResMT 2025", "summary": "Low-resource machine translation (MT) presents a diversity of community needs\nand application challenges that remain poorly understood. To complement surveys\nand focus groups, which tend to rely on small samples of respondents, we\npropose an observational study on actual usage patterns of tetun$.$org, a\nspecialized MT service for the Tetun language, which is the lingua franca in\nTimor-Leste. Our analysis of 100,000 translation requests reveals patterns that\nchallenge assumptions based on existing corpora. We find that users, many of\nthem students on mobile devices, typically translate text from a high-resource\nlanguage into Tetun across diverse domains including science, healthcare, and\ndaily life. This contrasts sharply with available Tetun corpora, which are\ndominated by news articles covering government and social issues. Our results\nsuggest that MT systems for institutionalized minority languages like Tetun\nshould prioritize accuracy on domains relevant to educational contexts, in the\nhigh-resource to low-resource direction. More broadly, this study demonstrates\nhow observational analysis can inform low-resource language technology\ndevelopment, by grounding research in practical community needs.", "AI": {"tldr": "通过对Tetun语言翻译服务tetun.org的10万次翻译请求的观察研究，揭示了低资源机器翻译的实际需求与现有语料库假设的差异。", "motivation": "理解低资源机器翻译的社区需求和实际应用挑战，弥补传统调查方法样本量小的不足。", "method": "采用观察性研究，分析tetun.org的10万次翻译请求，研究用户行为模式。", "result": "用户多为学生，通过移动设备将高资源语言翻译为Tetun，涉及科学、医疗和日常生活等领域，与现有以新闻为主的Tetun语料库形成鲜明对比。", "conclusion": "低资源机器翻译系统应优先关注教育相关领域的高资源到低资源翻译准确性，观察性研究可为技术开发提供实用指导。"}}
{"id": "2506.07248", "pdf": "https://arxiv.org/pdf/2506.07248", "abs": "https://arxiv.org/abs/2506.07248", "authors": ["Prathamesh Kokate", "Mitali Sarnaik", "Manavi Khopade", "Raviraj Joshi"], "title": "Improving the Efficiency of Long Document Classification using Sentence Ranking Approach", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Long document classification poses challenges due to the computational\nlimitations of transformer-based models, particularly BERT, which are\nconstrained by fixed input lengths and quadratic attention complexity.\nMoreover, using the full document for classification is often redundant, as\nonly a subset of sentences typically carries the necessary information. To\naddress this, we propose a TF-IDF-based sentence ranking method that improves\nefficiency by selecting the most informative content. Our approach explores\nfixed-count and percentage-based sentence selection, along with an enhanced\nscoring strategy combining normalized TF-IDF scores and sentence length.\nEvaluated on the MahaNews LDC dataset of long Marathi news articles, the method\nconsistently outperforms baselines such as first, last, and random sentence\nselection. With MahaBERT-v2, we achieve near-identical classification accuracy\nwith just a 0.33 percent drop compared to the full-context baseline, while\nreducing input size by over 50 percent and inference latency by 43 percent.\nThis demonstrates that significant context reduction is possible without\nsacrificing performance, making the method practical for real-world long\ndocument classification tasks.", "AI": {"tldr": "提出了一种基于TF-IDF的句子排序方法，用于长文档分类，显著减少输入大小和推理延迟，同时保持高分类准确率。", "motivation": "长文档分类中，BERT等模型因固定输入长度和二次注意力复杂度受限，且通常只需部分句子即可完成任务。", "method": "采用TF-IDF句子排序方法，结合固定数量或百分比选择句子，并优化评分策略（归一化TF-IDF分和句子长度）。", "result": "在MahaNews LDC数据集上，该方法优于基线方法，输入大小减少50%以上，推理延迟降低43%，分类准确率仅下降0.33%。", "conclusion": "该方法证明在长文档分类中，显著减少上下文是可行的，且不影响性能，适用于实际任务。"}}
{"id": "2506.07205", "pdf": "https://arxiv.org/pdf/2506.07205", "abs": "https://arxiv.org/abs/2506.07205", "authors": ["Min-Jung Kim", "Dongjin Kim", "Seokju Yun", "Jaegul Choo"], "title": "TV-LiVE: Training-Free, Text-Guided Video Editing via Layer Informed Vitality Exploitation", "categories": ["cs.CV"], "comment": null, "summary": "Video editing has garnered increasing attention alongside the rapid progress\nof diffusion-based video generation models. As part of these advancements,\nthere is a growing demand for more accessible and controllable forms of video\nediting, such as prompt-based editing. Previous studies have primarily focused\non tasks such as style transfer, background replacement, object substitution,\nand attribute modification, while maintaining the content structure of the\nsource video. However, more complex tasks, including the addition of novel\nobjects and nonrigid transformations, remain relatively unexplored. In this\npaper, we present TV-LiVE, a Training-free and text-guided Video editing\nframework via Layerinformed Vitality Exploitation. We empirically identify\nvital layers within the video generation model that significantly influence the\nquality of generated outputs. Notably, these layers are closely associated with\nRotary Position Embeddings (RoPE). Based on this observation, our method\nenables both object addition and non-rigid video editing by selectively\ninjecting key and value features from the source model into the corresponding\nlayers of the target model guided by the layer vitality. For object addition,\nwe further identify prominent layers to extract the mask regions corresponding\nto the newly added target prompt. We found that the extracted masks from the\nprominent layers faithfully indicate the region to be edited. Experimental\nresults demonstrate that TV-LiVE outperforms existing approaches for both\nobject addition and non-rigid video editing. Project Page:\nhttps://emjay73.github.io/TV_LiVE/", "AI": {"tldr": "TV-LiVE是一个无需训练、基于文本指导的视频编辑框架，通过利用关键层的活力信息实现复杂编辑任务，如添加新对象和非刚性变换。", "motivation": "随着扩散模型在视频生成中的快速发展，对更易用和可控的视频编辑方法的需求增加，尤其是复杂任务如添加新对象和非刚性变换的研究较少。", "method": "TV-LiVE通过识别视频生成模型中的关键层（与RoPE相关），并选择性注入源模型的特征到目标模型中，实现对象添加和非刚性编辑。", "result": "实验表明，TV-LiVE在对象添加和非刚性视频编辑任务上优于现有方法。", "conclusion": "TV-LiVE为复杂视频编辑任务提供了一种高效且无需训练的解决方案。"}}
{"id": "2506.04762", "pdf": "https://arxiv.org/pdf/2506.04762", "abs": "https://arxiv.org/abs/2506.04762", "authors": ["Lingyuan Liu", "Mengxiang Zhang"], "title": "GOLFer: Smaller LM-Generated Documents Hallucination Filter & Combiner for Query Expansion in Information Retrieval", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs)-based query expansion for information retrieval\naugments queries with generated hypothetical documents with LLMs. However, its\nperformance relies heavily on the scale of the language models (LMs),\nnecessitating larger, more advanced LLMs. This approach is costly,\ncomputationally intensive, and often has limited accessibility. To address\nthese limitations, we introduce GOLFer - Smaller LMs-Generated Documents\nHallucination Filter & Combiner - a novel method leveraging smaller open-source\nLMs for query expansion. GOLFer comprises two modules: a hallucination filter\nand a documents combiner. The former detects and removes non-factual and\ninconsistent sentences in generated documents, a common issue with smaller LMs,\nwhile the latter combines the filtered content with the query using a weight\nvector to balance their influence. We evaluate GOLFer alongside dominant\nLLM-based query expansion methods on three web search and ten low-resource\ndatasets. Experimental results demonstrate that GOLFer consistently outperforms\nother methods using smaller LMs, and maintains competitive performance against\nmethods using large-size LLMs, demonstrating its effectiveness.", "AI": {"tldr": "GOLFer是一种利用小型开源语言模型进行查询扩展的新方法，通过过滤生成文档中的幻觉内容并组合优化查询，性能优于其他小型模型方法，且与大型模型方法竞争。", "motivation": "大型语言模型（LLM）用于查询扩展成本高且计算密集，小型模型生成的文档常包含不准确内容。GOLFer旨在解决这些问题。", "method": "GOLFer包含幻觉过滤器和文档组合器：前者检测并移除不准确句子，后者通过权重向量优化查询与文档的结合。", "result": "在多个数据集上，GOLFer优于其他小型模型方法，并与大型模型方法性能相当。", "conclusion": "GOLFer证明了小型模型通过优化方法可实现高效查询扩展，降低了成本和资源需求。"}}
{"id": "2506.07249", "pdf": "https://arxiv.org/pdf/2506.07249", "abs": "https://arxiv.org/abs/2506.07249", "authors": ["Lance Calvin Lim Gamboa", "Yue Feng", "Mark Lee"], "title": "Bias Attribution in Filipino Language Models: Extending a Bias Interpretability Metric for Application on Agglutinative Languages", "categories": ["cs.CL"], "comment": "Accepted into the Gender Bias in NLP Workshop at ACL 2025\n  (GeBNLP@ACL2025)", "summary": "Emerging research on bias attribution and interpretability have revealed how\ntokens contribute to biased behavior in language models processing English\ntexts. We build on this line of inquiry by adapting the information-theoretic\nbias attribution score metric for implementation on models handling\nagglutinative languages, particularly Filipino. We then demonstrate the\neffectiveness of our adapted method by using it on a purely Filipino model and\non three multilingual models: one trained on languages worldwide and two on\nSoutheast Asian data. Our results show that Filipino models are driven towards\nbias by words pertaining to people, objects, and relationships, entity-based\nthemes that stand in contrast to the action-heavy nature of bias-contributing\nthemes in English (i.e., criminal, sexual, and prosocial behaviors). These\nfindings point to differences in how English and non-English models process\ninputs linked to sociodemographic groups and bias.", "AI": {"tldr": "该研究将信息论偏置归因评分方法扩展到粘着语（如菲律宾语），并比较了菲律宾语模型与多语言模型的偏置行为，发现菲律宾语模型的偏置主题与英语模型不同。", "motivation": "探索非英语语言模型（特别是菲律宾语）中的偏置行为，以揭示不同语言模型处理输入时的差异。", "method": "通过信息论偏置归因评分方法，分析菲律宾语模型和多语言模型中的偏置贡献主题。", "result": "菲律宾语模型的偏置主题集中在人物、对象和关系上，与英语模型的行为主题（如犯罪、性行为）形成对比。", "conclusion": "研究揭示了英语与非英语语言模型在处理社会人口群体相关输入时的差异，强调了语言特性对偏置行为的影响。"}}
{"id": "2506.07214", "pdf": "https://arxiv.org/pdf/2506.07214", "abs": "https://arxiv.org/abs/2506.07214", "authors": ["Zhiyuan Zhong", "Zhen Sun", "Yepang Liu", "Xinlei He", "Guanhong Tao"], "title": "Backdoor Attack on Vision Language Models with Stealthy Semantic Manipulation", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "Vision Language Models (VLMs) have shown remarkable performance, but are also\nvulnerable to backdoor attacks whereby the adversary can manipulate the model's\noutputs through hidden triggers. Prior attacks primarily rely on\nsingle-modality triggers, leaving the crucial cross-modal fusion nature of VLMs\nlargely unexplored. Unlike prior work, we identify a novel attack surface that\nleverages cross-modal semantic mismatches as implicit triggers. Based on this\ninsight, we propose BadSem (Backdoor Attack with Semantic Manipulation), a data\npoisoning attack that injects stealthy backdoors by deliberately misaligning\nimage-text pairs during training. To perform the attack, we construct SIMBad, a\ndataset tailored for semantic manipulation involving color and object\nattributes. Extensive experiments across four widely used VLMs show that BadSem\nachieves over 98% average ASR, generalizes well to out-of-distribution\ndatasets, and can transfer across poisoning modalities. Our detailed analysis\nusing attention visualization shows that backdoored models focus on\nsemantically sensitive regions under mismatched conditions while maintaining\nnormal behavior on clean inputs. To mitigate the attack, we try two defense\nstrategies based on system prompt and supervised fine-tuning but find that both\nof them fail to mitigate the semantic backdoor. Our findings highlight the\nurgent need to address semantic vulnerabilities in VLMs for their safer\ndeployment.", "AI": {"tldr": "论文提出了一种新型的后门攻击方法BadSem，利用跨模态语义不匹配作为隐式触发器，攻击视觉语言模型（VLMs），并展示了其高攻击成功率和泛化能力。", "motivation": "现有后门攻击主要依赖单模态触发器，而跨模态融合的特性在VLMs中未被充分探索。论文旨在填补这一空白，揭示语义不匹配作为攻击表面的潜力。", "method": "通过数据投毒，故意在训练时错配图像-文本对，构建SIMBad数据集进行语义操纵（如颜色和对象属性）。", "result": "在四种广泛使用的VLMs上，BadSem平均攻击成功率（ASR）超过98%，且能泛化到分布外数据集和跨模态传递。防御策略（系统提示和监督微调）均未能有效缓解攻击。", "conclusion": "研究强调了VLMs中语义漏洞的严重性，呼吁更安全的部署方案。"}}
{"id": "2506.06276", "pdf": "https://arxiv.org/pdf/2506.06276", "abs": "https://arxiv.org/abs/2506.06276", "authors": ["Jiatao Gu", "Tianrong Chen", "David Berthelot", "Huangjie Zheng", "Yuyang Wang", "Ruixiang Zhang", "Laurent Dinh", "Miguel Angel Bautista", "Josh Susskind", "Shuangfei Zhai"], "title": "STARFlow: Scaling Latent Normalizing Flows for High-resolution Image Synthesis", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "TLDR: We show for the first time that normalizing flows can be scaled\n  for high-resolution and text-conditioned image synthesis", "summary": "We present STARFlow, a scalable generative model based on normalizing flows\nthat achieves strong performance in high-resolution image synthesis. The core\nof STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the\nexpressive power of normalizing flows with the structured modeling capabilities\nof Autoregressive Transformers. We first establish the theoretical universality\nof TARFlow for modeling continuous distributions. Building on this foundation,\nwe introduce several key architectural and algorithmic innovations to\nsignificantly enhance scalability: (1) a deep-shallow design, wherein a deep\nTransformer block captures most of the model representational capacity,\ncomplemented by a few shallow Transformer blocks that are computationally\nefficient yet substantially beneficial; (2) modeling in the latent space of\npretrained autoencoders, which proves more effective than direct pixel-level\nmodeling; and (3) a novel guidance algorithm that significantly boosts sample\nquality. Crucially, our model remains an end-to-end normalizing flow, enabling\nexact maximum likelihood training in continuous spaces without discretization.\nSTARFlow achieves competitive performance in both class-conditional and\ntext-conditional image generation tasks, approaching state-of-the-art diffusion\nmodels in sample quality. To our knowledge, this work is the first successful\ndemonstration of normalizing flows operating effectively at this scale and\nresolution.", "AI": {"tldr": "STARFlow是一种基于归一化流的高分辨率图像生成模型，结合了Transformer的自回归能力，通过架构和算法创新提升了可扩展性。", "motivation": "解决高分辨率图像合成中归一化流的可扩展性和性能问题。", "method": "提出TARFlow（Transformer Autoregressive Flow），结合深度-浅层设计、潜在空间建模和新型引导算法。", "result": "在类条件和文本条件图像生成任务中表现优异，接近扩散模型的样本质量。", "conclusion": "首次证明归一化流在大规模高分辨率图像生成中的有效性。"}}
{"id": "2506.07270", "pdf": "https://arxiv.org/pdf/2506.07270", "abs": "https://arxiv.org/abs/2506.07270", "authors": ["Atahan Özer", "Çağatay Yıldız"], "title": "Question Answering under Temporal Conflict: Evaluating and Organizing Evolving Knowledge with LLMs", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) exhibit remarkable capabilities in question\nanswering and reasoning thanks to their extensive parametric memory. However,\ntheir knowledge is inherently limited by the scope of their pre-training data,\nwhile real-world information evolves continuously. Updating this knowledge\ntypically requires costly and brittle re-training, or in-context learning\n(ICL), which becomes impractical at scale given the volume and volatility of\nmodern information. Motivated by these limitations, we investigate how LLMs\nperform when exposed to temporal text corpora, or documents that reflect\nevolving knowledge over time, such as sports biographies where facts like a\nplayer's \"current team\" change year by year. To this end, we introduce two new\nbenchmarks: Temporal Wiki, which captures factual drift across historical\nWikipedia snapshots, and Unified Clark, which aggregates timestamped news\narticles to simulate real-world information accumulation. Our analysis reveals\nthat LLMs often struggle to reconcile conflicting or outdated facts and can be\nmisled when multiple versions of a fact appear in context. To address these\nissues, we propose a lightweight, agentic framework that incrementally builds a\nstructured, external memory from source documents without requiring\nre-training. This knowledge organization strategy enables models to retrieve\nand reason over temporally filtered, relevant information at inference time.\nEmpirically, our method outperforms ICL and RAG baselines across both\nbenchmarks, especially on questions requiring more complex reasoning or\nintegration of conflicting facts.", "AI": {"tldr": "论文探讨了大型语言模型（LLMs）在处理随时间演变的文本数据时的局限性，并提出了一种轻量级框架以动态更新知识。", "motivation": "LLMs的知识受限于预训练数据，而现实世界信息不断变化。传统更新方法成本高且不灵活，因此需要一种更高效的方式。", "method": "引入两个新基准（Temporal Wiki和Unified Clark），并提出一种轻量级框架，通过外部记忆动态组织知识。", "result": "该方法在复杂推理和冲突事实整合任务上优于ICL和RAG基线。", "conclusion": "提出的框架有效解决了LLMs处理动态知识的问题，为未来研究提供了方向。"}}
{"id": "2506.07216", "pdf": "https://arxiv.org/pdf/2506.07216", "abs": "https://arxiv.org/abs/2506.07216", "authors": ["Nada Aboudeshish", "Dmitry Ignatov", "Radu Timofte"], "title": "AugmentGest: Can Random Data Cropping Augmentation Boost Gesture Recognition Performance?", "categories": ["cs.CV"], "comment": null, "summary": "Data augmentation is a crucial technique in deep learning, particularly for\ntasks with limited dataset diversity, such as skeleton-based datasets. This\npaper proposes a comprehensive data augmentation framework that integrates\ngeometric transformations, random cropping, rotation, zooming and\nintensity-based transformations, brightness and contrast adjustments to\nsimulate real-world variations. Random cropping ensures the preservation of\nspatio-temporal integrity while addressing challenges such as viewpoint bias\nand occlusions. The augmentation pipeline generates three augmented versions\nfor each sample in addition to the data set sample, thus quadrupling the data\nset size and enriching the diversity of gesture representations. The proposed\naugmentation strategy is evaluated on three models: multi-stream e2eET, FPPR\npoint cloud-based hand gesture recognition (HGR), and DD-Network. Experiments\nare conducted on benchmark datasets including DHG14/28, SHREC'17, and JHMDB.\nThe e2eET model, recognized as the state-of-the-art for hand gesture\nrecognition on DHG14/28 and SHREC'17. The FPPR-PCD model, the second-best\nperforming model on SHREC'17, excels in point cloud-based gesture recognition.\nDD-Net, a lightweight and efficient architecture for skeleton-based action\nrecognition, is evaluated on SHREC'17 and the Human Motion Data Base (JHMDB).\nThe results underline the effectiveness and versatility of the proposed\naugmentation strategy, significantly improving model generalization and\nrobustness across diverse datasets and architectures. This framework not only\nestablishes state-of-the-art results on all three evaluated models but also\noffers a scalable solution to advance HGR and action recognition applications\nin real-world scenarios. The framework is available at\nhttps://github.com/NadaAbodeshish/Random-Cropping-augmentation-HGR", "AI": {"tldr": "本文提出了一种综合数据增强框架，通过几何变换、随机裁剪、旋转、缩放和强度变换等技术，显著提升了骨架数据集的多样性和模型性能。", "motivation": "针对骨架数据集中数据多样性有限的问题，提出一种增强框架以模拟真实世界的变化，提升模型泛化能力。", "method": "集成几何变换、随机裁剪、旋转、缩放和强度变换等方法，生成每个样本的三个增强版本，扩充数据集规模。", "result": "在多个模型（e2eET、FPPR-PCD、DD-Net）和数据集（DHG14/28、SHREC'17、JHMDB）上验证了框架的有效性，显著提升了模型性能。", "conclusion": "该框架不仅在多模型和多数据集上取得了最优结果，还为实际应用中的手势识别和动作识别提供了可扩展的解决方案。"}}
{"id": "2506.07274", "pdf": "https://arxiv.org/pdf/2506.07274", "abs": "https://arxiv.org/abs/2506.07274", "authors": ["Olga Kellert", "Nemika Tyagi", "Muhammad Imran", "Nelvin Licona-Guevara", "Carlos Gómez-Rodríguez"], "title": "Parsing the Switch: LLM-Based UD Annotation for Complex Code-Switched and Low-Resource Languages", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages", "summary": "Code-switching presents a complex challenge for syntactic analysis,\nespecially in low-resource language settings where annotated data is scarce.\nWhile recent work has explored the use of large language models (LLMs) for\nsequence-level tagging, few approaches systematically investigate how well\nthese models capture syntactic structure in code-switched contexts. Moreover,\nexisting parsers trained on monolingual treebanks often fail to generalize to\nmultilingual and mixed-language input. To address this gap, we introduce the\nBiLingua Parser, an LLM-based annotation pipeline designed to produce Universal\nDependencies (UD) annotations for code-switched text. First, we develop a\nprompt-based framework for Spanish-English and Spanish-Guaran\\'i data,\ncombining few-shot LLM prompting with expert review. Second, we release two\nannotated datasets, including the first Spanish-Guaran\\'i UD-parsed corpus.\nThird, we conduct a detailed syntactic analysis of switch points across\nlanguage pairs and communicative contexts. Experimental results show that\nBiLingua Parser achieves up to 95.29% LAS after expert revision, significantly\noutperforming prior baselines and multilingual parsers. These results show that\nLLMs, when carefully guided, can serve as practical tools for bootstrapping\nsyntactic resources in under-resourced, code-switched environments. Data and\nsource code are available at https://github.com/N3mika/ParsingProject", "AI": {"tldr": "论文提出了一种基于大语言模型（LLM）的BiLingua Parser，用于生成代码混合文本的通用依赖（UD）标注，显著提升了标注质量。", "motivation": "解决低资源语言环境下代码混合文本的句法分析难题，填补现有单语树库解析器在多语言和混合语言输入上的不足。", "method": "开发了基于提示的框架，结合少量样本LLM提示和专家评审，并发布两个标注数据集（包括首个西班牙-瓜拉尼语UD语料库）。", "result": "BiLingua Parser在专家修订后达到95.29%的LAS，显著优于现有基线方法和多语言解析器。", "conclusion": "研究表明，经过精心引导的LLM可以作为实用工具，在资源匮乏的代码混合环境中快速构建句法资源。"}}
{"id": "2506.07227", "pdf": "https://arxiv.org/pdf/2506.07227", "abs": "https://arxiv.org/abs/2506.07227", "authors": ["Tianyi Bai", "Yuxuan Fan", "Jiantao Qiu", "Fupeng Sun", "Jiayi Song", "Junlin Han", "Zichen Liu", "Conghui He", "Wentao Zhang", "Binhang Yuan"], "title": "Hallucination at a Glance: Controlled Visual Edits and Fine-Grained Multimodal Learning", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Multimodal large language models (MLLMs) have achieved strong performance on\nvision-language tasks but still struggle with fine-grained visual differences,\nleading to hallucinations or missed semantic shifts. We attribute this to\nlimitations in both training data and learning objectives. To address these\nissues, we propose a controlled data generation pipeline that produces\nminimally edited image pairs with semantically aligned captions. Using this\npipeline, we construct the Micro Edit Dataset (MED), containing over 50K\nimage-text pairs spanning 11 fine-grained edit categories, including attribute,\ncount, position, and object presence changes. Building on MED, we introduce a\nsupervised fine-tuning (SFT) framework with a feature-level consistency loss\nthat promotes stable visual embeddings under small edits. We evaluate our\napproach on the Micro Edit Detection benchmark, which includes carefully\nbalanced evaluation pairs designed to test sensitivity to subtle visual\nvariations across the same edit categories. Our method improves difference\ndetection accuracy and reduces hallucinations compared to strong baselines,\nincluding GPT-4o. Moreover, it yields consistent gains on standard\nvision-language tasks such as image captioning and visual question answering.\nThese results demonstrate the effectiveness of combining targeted data and\nalignment objectives for enhancing fine-grained visual reasoning in MLLMs.", "AI": {"tldr": "论文提出了一种针对多模态大语言模型（MLLMs）在细粒度视觉差异任务中表现不佳的解决方案，通过生成微编辑数据集（MED）和引入特征级一致性损失来提升模型性能。", "motivation": "MLLMs在视觉-语言任务中表现优秀，但在细粒度视觉差异上容易产生幻觉或忽略语义变化，主要源于训练数据和目标函数的限制。", "method": "提出可控数据生成流程构建MED数据集（50K对图像-文本），并设计监督微调框架，加入特征级一致性损失以稳定视觉嵌入。", "result": "在微编辑检测基准上，方法显著提升了差异检测准确率并减少幻觉，同时在标准视觉-语言任务（如图像描述和视觉问答）中表现更优。", "conclusion": "结合针对性数据和目标对齐能有效增强MLLMs的细粒度视觉推理能力。"}}
{"id": "2506.06286", "pdf": "https://arxiv.org/pdf/2506.06286", "abs": "https://arxiv.org/abs/2506.06286", "authors": ["Kevin Baum"], "title": "Disentangling AI Alignment: A Structured Taxonomy Beyond Safety and Ethics", "categories": ["cs.CY", "cs.AI", "cs.HC"], "comment": "accepted for the LNCS post proceedings of the AISoLA 2024 conference", "summary": "Recent advances in AI research make it increasingly plausible that artificial\nagents with consequential real-world impact will soon operate beyond tightly\ncontrolled environments. Ensuring that these agents are not only safe but that\nthey adhere to broader normative expectations is thus an urgent\ninterdisciplinary challenge. Multiple fields -- notably AI Safety, AI\nAlignment, and Machine Ethics -- claim to contribute to this task. However, the\nconceptual boundaries and interrelations among these domains remain vague,\nleaving researchers without clear guidance in positioning their work.\n  To address this meta-challenge, we develop a structured conceptual framework\nfor understanding AI alignment. Rather than focusing solely on alignment goals,\nwe introduce a taxonomy distinguishing the alignment aim (safety, ethicality,\nlegality, etc.), scope (outcome vs. execution), and constituency (individual\nvs. collective). This structural approach reveals multiple legitimate alignment\nconfigurations, providing a foundation for practical and philosophical\nintegration across domains, and clarifying what it might mean for an agent to\nbe aligned all-things-considered.", "AI": {"tldr": "论文提出了一个结构化概念框架，用于理解AI对齐问题，区分了对齐的目标、范围和利益相关者，为跨领域整合提供了基础。", "motivation": "随着AI在非受控环境中的应用增加，确保其安全性和符合规范成为紧迫挑战，但现有研究领域（如AI安全、AI对齐和机器伦理）的边界和关系模糊，缺乏明确指导。", "method": "开发了一个结构化概念框架，通过分类对齐的目标（如安全性、伦理性）、范围（结果与执行）和利益相关者（个体与集体）来解决问题。", "result": "框架揭示了多种合理的对齐配置，为跨领域实践和哲学整合提供了基础，并明确了“全面对齐”的含义。", "conclusion": "该框架为AI对齐研究提供了清晰的指导，有助于解决现有领域的模糊性，推动跨学科合作。"}}
{"id": "2506.07295", "pdf": "https://arxiv.org/pdf/2506.07295", "abs": "https://arxiv.org/abs/2506.07295", "authors": ["Lujun Li", "Lama Sleem", "Niccolo' Gentile", "Geoffrey Nichil", "Radu State"], "title": "Exploring the Impact of Temperature on Large Language Models:Hot or Cold?", "categories": ["cs.CL"], "comment": null, "summary": "The sampling temperature, a critical hyperparameter in large language models\n(LLMs), modifies the logits before the softmax layer, thereby reshaping the\ndistribution of output tokens. Recent studies have challenged the Stochastic\nParrots analogy by demonstrating that LLMs are capable of understanding\nsemantics rather than merely memorizing data and that randomness, modulated by\nsampling temperature, plays a crucial role in model inference. In this study,\nwe systematically evaluated the impact of temperature in the range of 0 to 2 on\ndata sets designed to assess six different capabilities, conducting statistical\nanalyses on open source models of three different sizes: small (1B--4B), medium\n(6B--13B), and large (40B--80B). Our findings reveal distinct skill-specific\neffects of temperature on model performance, highlighting the complexity of\noptimal temperature selection in practical applications. To address this\nchallenge, we propose a BERT-based temperature selector that takes advantage of\nthese observed effects to identify the optimal temperature for a given prompt.\nWe demonstrate that this approach can significantly improve the performance of\nsmall and medium models in the SuperGLUE datasets. Furthermore, our study\nextends to FP16 precision inference, revealing that temperature effects are\nconsistent with those observed in 4-bit quantized models. By evaluating\ntemperature effects up to 4.0 in three quantized models, we find that the\nMutation Temperature -- the point at which significant performance changes\noccur -- increases with model size.", "AI": {"tldr": "研究了采样温度对大型语言模型性能的影响，提出了一种基于BERT的温度选择器，显著提升了中小模型在SuperGLUE数据集上的表现。", "motivation": "探讨采样温度如何影响模型性能，并解决实际应用中温度选择的复杂性。", "method": "在0到2的温度范围内系统评估不同规模模型（小、中、大）的六种能力，并开发BERT温度选择器。", "result": "温度对模型性能有技能特异性影响，BERT选择器显著提升中小模型表现，温度效应在FP16和4位量化模型中一致。", "conclusion": "温度选择对模型性能至关重要，BERT选择器为实际应用提供了有效解决方案，且温度效应在不同精度下保持一致。"}}
{"id": "2506.07235", "pdf": "https://arxiv.org/pdf/2506.07235", "abs": "https://arxiv.org/abs/2506.07235", "authors": ["Tianyi Bai", "Zengjie Hu", "Fupeng Sun", "Jiantao Qiu", "Yizhen Jiang", "Guangxin He", "Bohan Zeng", "Conghui He", "Binhang Yuan", "Wentao Zhang"], "title": "Multi-Step Visual Reasoning with Visual Tokens Scaling and Verification", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Multi-modal large language models (MLLMs) have achieved remarkable\ncapabilities by integrating visual perception with language understanding,\nenabling applications such as image-grounded dialogue, visual question\nanswering, and scientific analysis. However, most MLLMs adopt a static\ninference paradigm, encoding the entire image into fixed visual tokens upfront,\nwhich limits their ability to iteratively refine understanding or adapt to\ncontext during inference. This contrasts sharply with human perception, which\nis dynamic, selective, and feedback-driven. In this work, we introduce a novel\nframework for inference-time visual token scaling that enables MLLMs to perform\niterative, verifier-guided reasoning over visual content. We formulate the\nproblem as a Markov Decision Process, involving a reasoner that proposes visual\nactions and a verifier, which is trained via multi-step Direct Preference\nOptimization (DPO), that evaluates these actions and determines when reasoning\nshould terminate. To support this, we present a new dataset, VTS, comprising\nsupervised reasoning trajectories (VTS-SFT) and preference-labeled reasoning\ncomparisons (VTS-DPO). Our method significantly outperforms existing approaches\nacross diverse visual reasoning benchmarks, offering not only improved accuracy\nbut also more interpretable and grounded reasoning processes. These results\ndemonstrate the promise of dynamic inference mechanisms for enabling\nfine-grained, context-aware visual reasoning in next-generation MLLMs.", "AI": {"tldr": "该论文提出了一种动态推理框架，通过迭代的视觉标记缩放和验证器引导的推理，改进了多模态大语言模型（MLLMs）的视觉推理能力。", "motivation": "现有的MLLMs采用静态推理范式，限制了其迭代优化和上下文适应的能力，而人类感知是动态且反馈驱动的。", "method": "提出了一种基于马尔可夫决策过程的框架，包括一个提出视觉动作的推理器和一个通过多步直接偏好优化（DPO）训练的验证器。", "result": "该方法在多种视觉推理基准测试中显著优于现有方法，提高了准确性并提供了更可解释的推理过程。", "conclusion": "动态推理机制为下一代MLLMs实现细粒度、上下文感知的视觉推理提供了潜力。"}}
{"id": "2506.06288", "pdf": "https://arxiv.org/pdf/2506.06288", "abs": "https://arxiv.org/abs/2506.06288", "authors": ["Xueying Ding", "Aakriti Mittal", "Achintya Gopal"], "title": "DELPHYNE: A Pre-Trained Model for General and Financial Time Series", "categories": ["q-fin.ST", "cs.AI", "cs.LG"], "comment": null, "summary": "Time-series data is a vital modality within data science communities. This is\nparticularly valuable in financial applications, where it helps in detecting\npatterns, understanding market behavior, and making informed decisions based on\nhistorical data. Recent advances in language modeling have led to the rise of\ntime-series pre-trained models that are trained on vast collections of datasets\nand applied to diverse tasks across financial domains. However, across\nfinancial applications, existing time-series pre-trained models have not shown\nboosts in performance over simple finance benchmarks in both zero-shot and\nfine-tuning settings. This phenomenon occurs because of a i) lack of financial\ndata within the pre-training stage, and ii) the negative transfer effect due to\ninherently different time-series patterns across domains. Furthermore,\ntime-series data is continuous, noisy, and can be collected at varying\nfrequencies and with varying lags across different variables, making this data\nmore challenging to model than languages. To address the above problems, we\nintroduce a Pre-trained MoDEL for FINance TimE-series (Delphyne). Delphyne\nachieves competitive performance to existing foundation and full-shot models\nwith few fine-tuning steps on publicly available datasets, and also shows\nsuperior performances on various financial tasks.", "AI": {"tldr": "论文提出了一种名为Delphyne的预训练模型，用于解决金融时间序列数据建模中的性能不足问题，并展示了其在多种金融任务中的优越表现。", "motivation": "现有时间序列预训练模型在金融应用中性能提升不明显，主要由于预训练阶段缺乏金融数据以及跨领域时间序列模式的负迁移效应。", "method": "提出Delphyne模型，针对金融时间序列数据的连续性和噪声特点进行优化，并通过少量微调步骤在公开数据集上实现竞争性能。", "result": "Delphyne在多种金融任务中表现优于现有基准模型和全样本模型。", "conclusion": "Delphyne为解决金融时间序列建模中的挑战提供了一种有效方法，并在实际应用中展示了其潜力。"}}
{"id": "2506.07297", "pdf": "https://arxiv.org/pdf/2506.07297", "abs": "https://arxiv.org/abs/2506.07297", "authors": ["Lauren Levine", "Amir Zeldes"], "title": "Subjectivity in the Annotation of Bridging Anaphora", "categories": ["cs.CL", "I.2.7"], "comment": "LAW-XIX, ACL 2025 Workshop", "summary": "Bridging refers to the associative relationship between inferable entities in\na discourse and the antecedents which allow us to understand them, such as\nunderstanding what \"the door\" means with respect to an aforementioned \"house\".\nAs identifying associative relations between entities is an inherently\nsubjective task, it is difficult to achieve consistent agreement in the\nannotation of bridging anaphora and their antecedents. In this paper, we\nexplore the subjectivity involved in the annotation of bridging instances at\nthree levels: anaphor recognition, antecedent resolution, and bridging subtype\nselection. To do this, we conduct an annotation pilot on the test set of the\nexisting GUM corpus, and propose a newly developed classification system for\nbridging subtypes, which we compare to previously proposed schemes. Our results\nsuggest that some previous resources are likely to be severely under-annotated.\nWe also find that while agreement on the bridging subtype category was\nmoderate, annotator overlap for exhaustively identifying instances of bridging\nis low, and that many disagreements resulted from subjective understanding of\nthe entities involved.", "AI": {"tldr": "本文探讨了桥接标注中的主观性问题，分析了三个层面的不一致性，并提出新的桥接子类型分类系统，发现现有资源可能存在严重标注不足。", "motivation": "桥接标注的主观性导致一致性难以达成，研究旨在探索标注中的不一致性及其原因。", "method": "在GUM语料库测试集上进行标注实验，提出新的桥接子类型分类系统，并与现有方案对比。", "result": "发现现有资源可能严重标注不足，桥接子类型标注一致性中等，但桥接实例识别一致性较低。", "conclusion": "桥接标注的主观性显著影响一致性，需进一步研究以提高标注质量。"}}
{"id": "2506.07280", "pdf": "https://arxiv.org/pdf/2506.07280", "abs": "https://arxiv.org/abs/2506.07280", "authors": ["Pablo Acuaviva", "Aram Davtyan", "Mariam Hassan", "Sebastian Stapf", "Ahmad Rahimi", "Alexandre Alahi", "Paolo Favaro"], "title": "From Generation to Generalization: Emergent Few-Shot Learning in Video Diffusion Models", "categories": ["cs.CV", "cs.AI"], "comment": "27 pages, 23 figures, 9 tables", "summary": "Video Diffusion Models (VDMs) have emerged as powerful generative tools,\ncapable of synthesizing high-quality spatiotemporal content. Yet, their\npotential goes far beyond mere video generation. We argue that the training\ndynamics of VDMs, driven by the need to model coherent sequences, naturally\npushes them to internalize structured representations and an implicit\nunderstanding of the visual world. To probe the extent of this internal\nknowledge, we introduce a few-shot fine-tuning framework that repurposes VDMs\nfor new tasks using only a handful of examples. Our method transforms each task\ninto a visual transition, enabling the training of LoRA weights on short\ninput-output sequences without altering the generative interface of a frozen\nVDM. Despite minimal supervision, the model exhibits strong generalization\nacross diverse tasks, from low-level vision (for example, segmentation and pose\nestimation) to high-level reasoning (for example, on ARC-AGI). These results\nreframe VDMs as more than generative engines. They are adaptable visual\nlearners with the potential to serve as the backbone for future foundation\nmodels in vision.", "AI": {"tldr": "视频扩散模型（VDMs）不仅是强大的视频生成工具，还能通过少量样本微调适应新任务，展现出广泛的视觉理解能力。", "motivation": "探索VDMs在视频生成之外的潜力，验证其内部是否隐含了对视觉世界的结构化理解和知识。", "method": "提出一种少样本微调框架，将任务转化为视觉过渡，通过训练LoRA权重在不改变VDM生成接口的情况下适应新任务。", "result": "模型在从低层视觉（如分割、姿态估计）到高层推理（如ARC-AGI）的多样化任务中表现出强大的泛化能力。", "conclusion": "VDMs不仅是生成引擎，还是适应性强的视觉学习器，有望成为未来视觉基础模型的核心。"}}
{"id": "2506.06290", "pdf": "https://arxiv.org/pdf/2506.06290", "abs": "https://arxiv.org/abs/2506.06290", "authors": ["Mingyu Lu", "Ethan Weinberger", "Chanwoo Kim", "Su-In Lee"], "title": "CellCLIP -- Learning Perturbation Effects in Cell Painting via Text-Guided Contrastive Learning", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "High-content screening (HCS) assays based on high-throughput microscopy\ntechniques such as Cell Painting have enabled the interrogation of cells'\nmorphological responses to perturbations at an unprecedented scale. The\ncollection of such data promises to facilitate a better understanding of the\nrelationships between different perturbations and their effects on cellular\nstate. Towards achieving this goal, recent advances in cross-modal contrastive\nlearning could, in theory, be leveraged to learn a unified latent space that\naligns perturbations with their corresponding morphological effects. However,\nthe application of such methods to HCS data is not straightforward due to\nsubstantial differences in the semantics of Cell Painting images compared to\nnatural images, and the difficulty of representing different classes of\nperturbations (e.g., small molecule vs CRISPR gene knockout) in a single latent\nspace. In response to these challenges, here we introduce CellCLIP, a\ncross-modal contrastive learning framework for HCS data. CellCLIP leverages\npre-trained image encoders coupled with a novel channel encoding scheme to\nbetter capture relationships between different microscopy channels in image\nembeddings, along with natural language encoders for representing\nperturbations. Our framework outperforms current open-source models,\ndemonstrating the best performance in both cross-modal retrieval and\nbiologically meaningful downstream tasks while also achieving significant\nreductions in computation time.", "AI": {"tldr": "CellCLIP是一个用于高内涵筛选数据的跨模态对比学习框架，通过预训练图像编码器和新颖的通道编码方案，显著提升了性能并减少了计算时间。", "motivation": "高内涵筛选技术（如Cell Painting）生成了大量细胞形态响应数据，但现有方法难以处理其语义差异和扰动类型的多样性。", "method": "CellCLIP结合预训练图像编码器、通道编码方案和自然语言编码器，学习统一的潜在空间。", "result": "CellCLIP在跨模态检索和生物学任务中表现最佳，同时显著减少计算时间。", "conclusion": "CellCLIP为高内涵筛选数据提供了一种高效的跨模态对比学习解决方案。"}}
{"id": "2506.07309", "pdf": "https://arxiv.org/pdf/2506.07309", "abs": "https://arxiv.org/abs/2506.07309", "authors": ["Yin Huang", "Yifan Ethan Xu", "Kai Sun", "Vera Yan", "Alicia Sun", "Haidar Khan", "Jimmy Nguyen", "Mohammad Kachuee", "Zhaojiang Lin", "Yue Liu", "Aaron Colak", "Anuj Kumar", "Wen-tau Yih", "Xin Luna Dong"], "title": "ConfQA: Answer Only If You Are Confident", "categories": ["cs.CL"], "comment": "10 pages main content, 10 pages appendix, 5 figures, 7 tables", "summary": "Can we teach Large Language Models (LLMs) to refrain from hallucinating\nfactual statements? In this paper we present a fine-tuning strategy that we\ncall ConfQA, which can reduce hallucination rate from 20-40% to under 5% across\nmultiple factuality benchmarks. The core idea is simple: when the LLM answers a\nquestion correctly, it is trained to continue with the answer; otherwise, it is\ntrained to admit \"I am unsure\". But there are two key factors that make the\ntraining highly effective. First, we introduce a dampening prompt \"answer only\nif you are confident\" to explicitly guide the behavior, without which\nhallucination remains high as 15%-25%. Second, we leverage simple factual\nstatements, specifically attribute values from knowledge graphs, to help LLMs\ncalibrate the confidence, resulting in robust generalization across domains and\nquestion types. Building on this insight, we propose the Dual Neural Knowledge\nframework, which seamlessly select between internally parameterized neural\nknowledge and externally recorded symbolic knowledge based on ConfQA's\nconfidence. The framework enables potential accuracy gains to beyond 95%, while\nreducing unnecessary external retrievals by over 30%.", "AI": {"tldr": "ConfQA是一种微调策略，通过训练LLM在不确定时承认“我不确定”，显著减少幻觉率，并结合双神经知识框架提升准确性和效率。", "motivation": "解决LLM在生成事实性陈述时的幻觉问题，提高其回答的准确性和可靠性。", "method": "引入ConfQA策略，结合“仅在自信时回答”的提示和知识图谱中的简单事实，训练LLM校准信心，并设计双神经知识框架动态选择内部和外部知识。", "result": "幻觉率从20-40%降至5%以下，准确率提升至95%以上，外部检索减少30%以上。", "conclusion": "ConfQA和双神经知识框架有效减少LLM的幻觉，提升事实性回答的准确性和效率。"}}
{"id": "2506.07286", "pdf": "https://arxiv.org/pdf/2506.07286", "abs": "https://arxiv.org/abs/2506.07286", "authors": ["Aditya Chakravarty"], "title": "Multi-Step Guided Diffusion for Image Restoration on Edge Devices: Toward Lightweight Perception in Embodied AI", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": "Accepted in CVPR 2025 Embodied AI Workshop", "summary": "Diffusion models have shown remarkable flexibility for solving inverse\nproblems without task-specific retraining. However, existing approaches such as\nManifold Preserving Guided Diffusion (MPGD) apply only a single gradient update\nper denoising step, limiting restoration fidelity and robustness, especially in\nembedded or out-of-distribution settings. In this work, we introduce a\nmultistep optimization strategy within each denoising timestep, significantly\nenhancing image quality, perceptual accuracy, and generalization. Our\nexperiments on super-resolution and Gaussian deblurring demonstrate that\nincreasing the number of gradient updates per step improves LPIPS and PSNR with\nminimal latency overhead. Notably, we validate this approach on a Jetson Orin\nNano using degraded ImageNet and a UAV dataset, showing that MPGD, originally\ntrained on face datasets, generalizes effectively to natural and aerial scenes.\nOur findings highlight MPGD's potential as a lightweight, plug-and-play\nrestoration module for real-time visual perception in embodied AI agents such\nas drones and mobile robots.", "AI": {"tldr": "该论文提出了一种在扩散模型中引入多步优化的策略，显著提升了图像质量、感知准确性和泛化能力。", "motivation": "现有方法（如MPGD）在每个去噪步骤中仅应用单次梯度更新，限制了恢复的保真度和鲁棒性，尤其是在嵌入式或分布外场景中。", "method": "在每次去噪时间步内引入多步优化策略，增加梯度更新次数。", "result": "实验表明，该方法提高了LPIPS和PSNR指标，且在Jetson Orin Nano上验证了其泛化能力。", "conclusion": "MPGD具有作为轻量级、即插即用恢复模块的潜力，适用于无人机和移动机器人等实时视觉感知任务。"}}
{"id": "2506.06291", "pdf": "https://arxiv.org/pdf/2506.06291", "abs": "https://arxiv.org/abs/2506.06291", "authors": ["Xiaoke Wang", "Batuhan Altundas", "Zhaoxin Li", "Aaron Zhao", "Matthew Gombolay"], "title": "Improvement of Optimization using Learning Based Models in Mixed Integer Linear Programming Tasks", "categories": ["cs.LG", "cs.AI", "cs.MA"], "comment": "4 pages, 4 figures", "summary": "Mixed Integer Linear Programs (MILPs) are essential tools for solving\nplanning and scheduling problems across critical industries such as\nconstruction, manufacturing, and logistics. However, their widespread adoption\nis limited by long computational times, especially in large-scale, real-time\nscenarios. To address this, we present a learning-based framework that\nleverages Behavior Cloning (BC) and Reinforcement Learning (RL) to train Graph\nNeural Networks (GNNs), producing high-quality initial solutions for\nwarm-starting MILP solvers in Multi-Agent Task Allocation and Scheduling\nProblems. Experimental results demonstrate that our method reduces optimization\ntime and variance compared to traditional techniques while maintaining solution\nquality and feasibility.", "AI": {"tldr": "提出了一种基于学习的方法，结合行为克隆和强化学习训练图神经网络，为混合整数线性规划提供高质量初始解，显著减少计算时间。", "motivation": "混合整数线性规划在关键行业广泛应用，但计算时间长限制了其在大规模实时场景中的使用。", "method": "利用行为克隆和强化学习训练图神经网络，生成高质量初始解，用于预热混合整数线性规划求解器。", "result": "实验表明，该方法减少了优化时间和方差，同时保持解的质量和可行性。", "conclusion": "该框架为大规模实时混合整数线性规划问题提供了高效解决方案。"}}
{"id": "2506.07326", "pdf": "https://arxiv.org/pdf/2506.07326", "abs": "https://arxiv.org/abs/2506.07326", "authors": ["Brian Christian", "Hannah Rose Kirk", "Jessica A. F. Thompson", "Christopher Summerfield", "Tsvetomira Dumbalska"], "title": "Reward Model Interpretability via Optimal and Pessimal Tokens", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.LG", "I.2.6; I.2.7; H.5.2; J.4; K.4.2"], "comment": "Accepted for publication in Proceedings of the 2025 ACM Conference on\n  Fairness, Accountability, and Transparency (FAccT '25), to appear June 2025", "summary": "Reward modeling has emerged as a crucial component in aligning large language\nmodels with human values. Significant attention has focused on using reward\nmodels as a means for fine-tuning generative models. However, the reward models\nthemselves -- which directly encode human value judgments by turning\nprompt-response pairs into scalar rewards -- remain relatively understudied. We\npresent a novel approach to reward model interpretability through exhaustive\nanalysis of their responses across their entire vocabulary space. By examining\nhow different reward models score every possible single-token response to\nvalue-laden prompts, we uncover several striking findings: (i) substantial\nheterogeneity between models trained on similar objectives, (ii) systematic\nasymmetries in how models encode high- vs low-scoring tokens, (iii) significant\nsensitivity to prompt framing that mirrors human cognitive biases, and (iv)\novervaluation of more frequent tokens. We demonstrate these effects across ten\nrecent open-source reward models of varying parameter counts and architectures.\nOur results challenge assumptions about the interchangeability of reward\nmodels, as well as their suitability as proxies of complex and\ncontext-dependent human values. We find that these models can encode concerning\nbiases toward certain identity groups, which may emerge as unintended\nconsequences of harmlessness training -- distortions that risk propagating\nthrough the downstream large language models now deployed to millions.", "AI": {"tldr": "论文研究了奖励模型的可解释性，发现不同模型之间存在显著异质性、对高频词的过度估值以及系统性偏见，挑战了奖励模型作为人类价值观代理的假设。", "motivation": "奖励模型在调整大型语言模型与人类价值观对齐中扮演关键角色，但其本身的可解释性和潜在偏见尚未充分研究。", "method": "通过分析奖励模型在整个词汇空间中对单令牌响应的评分，揭示模型间的异质性、系统性偏见和对高频词的偏好。", "result": "研究发现奖励模型之间存在显著差异，对高频词过度估值，并编码了系统性偏见，可能传播到下游语言模型中。", "conclusion": "奖励模型并非完全可互换，且可能编码有害偏见，需谨慎使用以避免传播到实际应用中。"}}
{"id": "2506.07304", "pdf": "https://arxiv.org/pdf/2506.07304", "abs": "https://arxiv.org/abs/2506.07304", "authors": ["Kavitha Viswanathan", "Vrinda Goel", "Shlesh Gholap", "Devayan Ghosh", "Madhav Gupta", "Dhruvi Ganatra", "Sanket Potdar", "Amit Sethi"], "title": "FANVID: A Benchmark for Face and License Plate Recognition in Low-Resolution Videos", "categories": ["cs.CV"], "comment": null, "summary": "Real-world surveillance often renders faces and license plates unrecognizable\nin individual low-resolution (LR) frames, hindering reliable identification. To\nadvance temporal recognition models, we present FANVID, a novel video-based\nbenchmark comprising nearly 1,463 LR clips (180 x 320, 20--60 FPS) featuring 63\nidentities and 49 license plates from three English-speaking countries. Each\nvideo includes distractor faces and plates, increasing task difficulty and\nrealism. The dataset contains 31,096 manually verified bounding boxes and\nlabels.\n  FANVID defines two tasks: (1) face matching -- detecting LR faces and\nmatching them to high-resolution mugshots, and (2) license plate recognition --\nextracting text from LR plates without a predefined database. Videos are\ndownsampled from high-resolution sources to ensure that faces and text are\nindecipherable in single frames, requiring models to exploit temporal\ninformation. We introduce evaluation metrics adapted from mean Average\nPrecision at IoU > 0.5, prioritizing identity correctness for faces and\ncharacter-level accuracy for text.\n  A baseline method with pre-trained video super-resolution, detection, and\nrecognition achieved performance scores of 0.58 (face matching) and 0.42 (plate\nrecognition), highlighting both the feasibility and challenge of the tasks.\nFANVID's selection of faces and plates balances diversity with recognition\nchallenge. We release the software for data access, evaluation, baseline, and\nannotation to support reproducibility and extension. FANVID aims to catalyze\ninnovation in temporal modeling for LR recognition, with applications in\nsurveillance, forensics, and autonomous vehicles.", "AI": {"tldr": "FANVID是一个新的视频基准数据集，包含低分辨率（LR）视频片段，用于提升时间识别模型，支持人脸匹配和车牌识别任务。", "motivation": "解决现实监控中低分辨率视频下人脸和车牌难以识别的问题，推动时间建模技术的发展。", "method": "构建包含1,463个LR视频片段的数据集，定义人脸匹配和车牌识别任务，并引入评估指标。", "result": "基线方法在两项任务中分别取得0.58和0.42的分数，显示任务的可行性与挑战性。", "conclusion": "FANVID旨在促进低分辨率识别的时间建模创新，适用于监控、法医和自动驾驶等领域。"}}
{"id": "2506.06292", "pdf": "https://arxiv.org/pdf/2506.06292", "abs": "https://arxiv.org/abs/2506.06292", "authors": ["Tianyuan Shi", "Canbin Huang", "Fanqi Wan", "Longguang Zhong", "Ziyi Yang", "Weizhou Shen", "Xiaojun Quan", "Ming Yan"], "title": "Mutual-Taught for Co-adapting Policy and Reward Models", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to ACL 2025 (Main Conference)", "summary": "During the preference optimization of large language models (LLMs),\ndistribution shifts may arise between newly generated model samples and the\ndata used to train the reward model (RM). This shift reduces the efficacy of\nthe RM, which in turn negatively impacts the performance of the policy model\n(PM). To address this challenge, we propose Mutual-Taught, a self-training\nmethod that iteratively improves both the PM and RM without requiring\nadditional human annotation. Our approach mirrors the expectation-maximization\n(EM) algorithm. In the E-step, the PM is updated using feedback from the\ncurrent RM, guiding the PM toward a better approximation of the latent optimal\npreference distribution. In the M-step, we update the RM by constructing\ntraining data from the outputs of the PM before and after the E-step update.\nThis process ensures that the RM adapts to the evolving policy distribution.\nExperimental results demonstrate that this iterative approach leads to\nconsistent improvements in both models. Specifically, our 8B policy model,\nLLaMA-3-8B-Instruct-MT, achieves a length-controlled win rate of 54.1\\% on\nAlpacaEval-2, while our 8B reward model, FsfairX-LLaMA3-RM-MT, performs on par\nwith GPT-4o-2024-08-06 on RewardBench.", "AI": {"tldr": "论文提出Mutual-Taught方法，通过自训练迭代优化策略模型（PM）和奖励模型（RM），解决分布偏移问题，无需额外人工标注。", "motivation": "在大型语言模型（LLM）偏好优化中，模型生成样本与奖励模型训练数据间的分布偏移会降低RM效果，进而影响PM性能。", "method": "采用类似EM算法的自训练方法：E步用当前RM反馈更新PM；M步用PM更新前后的输出构建RM训练数据，使RM适应策略分布变化。", "result": "实验显示方法持续提升两模型性能，8B策略模型在AlpacaEval-2上胜率54.1%，8B奖励模型在RewardBench上与GPT-4o-2024-08-06表现相当。", "conclusion": "Mutual-Taught通过迭代优化PM和RM，有效解决了分布偏移问题，显著提升了模型性能。"}}
{"id": "2506.07335", "pdf": "https://arxiv.org/pdf/2506.07335", "abs": "https://arxiv.org/abs/2506.07335", "authors": ["Anyi Wang", "Dong Shu", "Yifan Wang", "Yunpu Ma", "Mengnan Du"], "title": "Improving LLM Reasoning through Interpretable Role-Playing Steering", "categories": ["cs.CL", "cs.AI"], "comment": "21 pages, 8 figures, 8 tables", "summary": "Role-playing has emerged as an effective technique for enhancing the\nreasoning capabilities of large language models (LLMs). However, existing\nmethods primarily rely on prompt engineering, which often lacks stability and\ninterpretability. In this paper, we introduce Sparse Autoencoder Role-Playing\nSteering (SRPS), a novel framework that identifies and manipulates internal\nmodel features associated with role-playing behavior. Our approach extracts\nlatent representations from role-play prompts, selects the most relevant\nfeatures based on activation patterns, and constructs a steering vector that\ncan be injected into the model's residual stream with controllable intensity.\nOur method enables fine-grained control over role-specific behavior and offers\ninsights into how role information influences internal model activations.\nExtensive experiments across various reasoning benchmarks and model sizes\ndemonstrate consistent performance gains. Notably, in the zero-shot\nchain-of-thought (CoT) setting, the accuracy of Llama3.1-8B on CSQA improves\nfrom 31.86% to 39.80%, while Gemma2-9B on SVAMP increases from 37.50% to\n45.10%. These results highlight the potential of SRPS to enhance reasoning\nability in LLMs, providing better interpretability and stability compared to\ntraditional prompt-based role-playing.", "AI": {"tldr": "SRPS框架通过识别和操纵与角色扮演行为相关的内部模型特征，提升了大型语言模型的推理能力，相比传统提示工程更稳定且可解释。", "motivation": "现有角色扮演方法主要依赖提示工程，但缺乏稳定性和可解释性，因此需要一种更可控的方法。", "method": "SRPS提取角色扮演提示的潜在表示，基于激活模式选择相关特征，并构建可注入模型残差流的控制向量。", "result": "实验表明，SRPS显著提升了模型在推理任务中的表现，如Llama3.1-8B在CSQA上的准确率从31.86%提升至39.80%。", "conclusion": "SRPS通过精细控制角色行为，为LLM推理能力提供了更稳定和可解释的增强方法。"}}
{"id": "2506.07310", "pdf": "https://arxiv.org/pdf/2506.07310", "abs": "https://arxiv.org/abs/2506.07310", "authors": ["Adam W. Harley", "Yang You", "Xinglong Sun", "Yang Zheng", "Nikhil Raghuraman", "Yunqi Gu", "Sheldon Liang", "Wen-Hsuan Chu", "Achal Dave", "Pavel Tokmakov", "Suya You", "Rares Ambrus", "Katerina Fragkiadaki", "Leonidas J. Guibas"], "title": "AllTracker: Efficient Dense Point Tracking at High Resolution", "categories": ["cs.CV"], "comment": null, "summary": "We introduce AllTracker: a model that estimates long-range point tracks by\nway of estimating the flow field between a query frame and every other frame of\na video. Unlike existing point tracking methods, our approach delivers\nhigh-resolution and dense (all-pixel) correspondence fields, which can be\nvisualized as flow maps. Unlike existing optical flow methods, our approach\ncorresponds one frame to hundreds of subsequent frames, rather than just the\nnext frame. We develop a new architecture for this task, blending techniques\nfrom existing work in optical flow and point tracking: the model performs\niterative inference on low-resolution grids of correspondence estimates,\npropagating information spatially via 2D convolution layers, and propagating\ninformation temporally via pixel-aligned attention layers. The model is fast\nand parameter-efficient (16 million parameters), and delivers state-of-the-art\npoint tracking accuracy at high resolution (i.e., tracking 768x1024 pixels, on\na 40G GPU). A benefit of our design is that we can train on a wider set of\ndatasets, and we find that doing so is crucial for top performance. We provide\nan extensive ablation study on our architecture details and training recipe,\nmaking it clear which details matter most. Our code and model weights are\navailable at https://alltracker.github.io .", "AI": {"tldr": "AllTracker是一个通过估计查询帧与视频中其他帧之间的流场来预测长距离点轨迹的模型，提供高分辨率、密集的对应关系场。", "motivation": "现有点跟踪方法无法提供高分辨率、密集的对应关系场，而现有光流方法仅能处理相邻帧。AllTracker旨在解决这些问题。", "method": "结合光流和点跟踪技术，采用迭代推理、2D卷积层和像素对齐注意力层，实现时空信息传播。", "result": "模型高效（1600万参数），在768x1024分辨率下实现最先进的点跟踪精度，支持多数据集训练。", "conclusion": "AllTracker在长距离点跟踪任务中表现出色，架构和训练细节的优化是关键。"}}
{"id": "2506.06293", "pdf": "https://arxiv.org/pdf/2506.06293", "abs": "https://arxiv.org/abs/2506.06293", "authors": ["Junyi Liu", "Stanley Kok"], "title": "Prediction of Bank Credit Ratings using Heterogeneous Topological Graph Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": "WITS 2024 (Workshop on Information Technologies and Systems 2024)", "summary": "Agencies such as Standard & Poor's and Moody's provide bank credit ratings\nthat influence economic stability and decision-making by stakeholders. Accurate\nand timely predictions support informed decision-making, regulatory actions,\nand investor protection. However, a complete interbank connection graph is\noften unavailable due to privacy concerns, complicating the direct application\nof Graph Neural Networks (GNNs) for rating prediction. our research utilizes\npersistent homology to construct a network that captures relationships among\nbanks and combines this with a traditional lending network to create a\nheterogeneous network that integrates information from both sources, leading to\nimproved predictions. Experiments on a global, real-world dataset validate the\neffectiveness of HTGNN. This research has implications for investors and\nregulatory bodies in enhancing proactive risk mitigation and the implementation\nof effective market interventions.The code can be find at\nhttps://github.com/Liu-Jun-Yi/HTGNN.", "AI": {"tldr": "论文提出了一种结合持久同源性和传统借贷网络的异构图神经网络（HTGNN），用于银行信用评级预测，解决了隐私问题导致的数据不完整问题。", "motivation": "银行信用评级对经济稳定和决策至关重要，但隐私问题导致完整的银行间连接图不可用，限制了图神经网络的应用。", "method": "利用持久同源性构建银行关系网络，与传统借贷网络结合形成异构图，通过HTGNN进行预测。", "result": "在全球真实数据集上的实验验证了HTGNN的有效性。", "conclusion": "HTGNN为投资者和监管机构提供了改进的风险缓解和市场干预工具。"}}
{"id": "2506.07356", "pdf": "https://arxiv.org/pdf/2506.07356", "abs": "https://arxiv.org/abs/2506.07356", "authors": ["Seokil Ham", "Yubin Choi", "Seungju Cho", "Yujin Yang", "Younghun Kim", "Changick Kim"], "title": "Refusal-Feature-guided Teacher for Safe Finetuning via Data Filtering and Alignment Distillation", "categories": ["cs.CL"], "comment": null, "summary": "Recently, major AI service providers such as Google and OpenAI have\nintroduced Finetuning-as-a-Service, which enables users to customize Large\nLanguage Models (LLMs) for specific downstream tasks using their own data.\nHowever, this service is vulnerable to degradation of LLM safety-alignment when\nuser data contains harmful prompts. While some prior works address this issue,\nfundamentally filtering harmful data from user data remains unexplored.\nMotivated by our observation that a directional representation reflecting\nrefusal behavior (called the refusal feature) obtained from safety-aligned LLMs\ncan inherently distinguish between harmful and harmless prompts, we propose the\nRefusal-Feature-guided Teacher (ReFT). Our ReFT model is trained to identify\nharmful prompts based on the similarity between input prompt features and its\nrefusal feature. During finetuning, the ReFT model serves as a teacher that\nfilters harmful prompts from user data and distills alignment knowledge into\nthe base model. Extensive experiments demonstrate that our ReFT-based\nfinetuning strategy effectively minimizes harmful outputs and enhances\nfinetuning accuracy for user-specific tasks, offering a practical solution for\nsecure and reliable deployment of LLMs in Finetuning-as-a-Service.", "AI": {"tldr": "论文提出了一种基于拒绝特征的教师模型（ReFT），用于在Finetuning-as-a-Service中过滤有害提示，保护LLM的安全对齐性。", "motivation": "现有Finetuning-as-a-Service容易因用户数据中的有害提示导致LLM安全对齐性下降，而现有方法未能从根本上解决有害数据过滤问题。", "method": "利用安全对齐LLM中的拒绝特征区分有害与无害提示，训练ReFT模型作为教师模型，在微调时过滤有害数据并传递对齐知识。", "result": "实验表明，ReFT策略能有效减少有害输出并提升微调准确性。", "conclusion": "ReFT为Finetuning-as-a-Service提供了一种安全可靠的解决方案。"}}
{"id": "2506.07327", "pdf": "https://arxiv.org/pdf/2506.07327", "abs": "https://arxiv.org/abs/2506.07327", "authors": ["Dane Williamson", "Yangfeng Ji", "Matthew Dwyer"], "title": "\"CASE: Contrastive Activation for Saliency Estimation", "categories": ["cs.CV", "cs.LG", "I.2.6; I.5.1; I.5.5; I.2.10"], "comment": "9 pages, 5 figures. Submitted to IEEE Transactions on Neural Networks\n  and Learning Systems (TNNLS)", "summary": "Saliency methods are widely used to visualize which input features are deemed\nrelevant to a model's prediction. However, their visual plausibility can\nobscure critical limitations. In this work, we propose a diagnostic test for\nclass sensitivity: a method's ability to distinguish between competing class\nlabels on the same input. Through extensive experiments, we show that many\nwidely used saliency methods produce nearly identical explanations regardless\nof the class label, calling into question their reliability. We find that\nclass-insensitive behavior persists across architectures and datasets,\nsuggesting the failure mode is structural rather than model-specific. Motivated\nby these findings, we introduce CASE, a contrastive explanation method that\nisolates features uniquely discriminative for the predicted class. We evaluate\nCASE using the proposed diagnostic and a perturbation-based fidelity test, and\nshow that it produces faithful and more class-specific explanations than\nexisting methods.", "AI": {"tldr": "论文提出了一种诊断测试（class sensitivity）来评估显著性方法的可靠性，发现许多方法对类别标签不敏感，进而提出了新的对比性解释方法CASE。", "motivation": "显著性方法在可视化模型预测时被广泛使用，但其视觉合理性可能掩盖了关键缺陷，尤其是对类别标签的敏感性不足。", "method": "提出了一种诊断测试（class sensitivity），并通过实验验证了现有显著性方法的局限性。随后，提出了CASE方法，通过对比性解释来区分预测类别的独特特征。", "result": "实验表明，许多显著性方法对类别标签不敏感，而CASE方法在诊断测试和基于扰动的保真度测试中表现更优。", "conclusion": "CASE方法能够生成更忠实且更具类别特异性的解释，解决了现有方法的局限性。"}}
{"id": "2506.06294", "pdf": "https://arxiv.org/pdf/2506.06294", "abs": "https://arxiv.org/abs/2506.06294", "authors": ["Yunqing Liu", "Wenqi Fan", "Xiaoyong Wei", "Qing Li"], "title": "GLProtein: Global-and-Local Structure Aware Protein Representation Learning", "categories": ["cs.LG", "cs.AI", "cs.CL", "q-bio.BM"], "comment": null, "summary": "Proteins are central to biological systems, participating as building blocks\nacross all forms of life. Despite advancements in understanding protein\nfunctions through protein sequence analysis, there remains potential for\nfurther exploration in integrating protein structural information. We argue\nthat the structural information of proteins is not only limited to their 3D\ninformation but also encompasses information from amino acid molecules (local\ninformation) to protein-protein structure similarity (global information). To\naddress this, we propose \\textbf{GLProtein}, the first framework in protein\npre-training that incorporates both global structural similarity and local\namino acid details to enhance prediction accuracy and functional insights.\nGLProtein innovatively combines protein-masked modelling with triplet structure\nsimilarity scoring, protein 3D distance encoding and substructure-based amino\nacid molecule encoding. Experimental results demonstrate that GLProtein\noutperforms previous methods in several bioinformatics tasks, including\npredicting protein-protein interaction, contact prediction, and so on.", "AI": {"tldr": "GLProtein是一个新的蛋白质预训练框架，结合全局结构相似性和局部氨基酸细节，显著提高了预测准确性。", "motivation": "尽管蛋白质序列分析已取得进展，但整合蛋白质结构信息仍有潜力。GLProtein旨在通过结合全局和局部结构信息，提升功能预测。", "method": "GLProtein结合了蛋白质掩码建模、三重结构相似性评分、3D距离编码和基于子结构的氨基酸分子编码。", "result": "GLProtein在多个生物信息学任务中表现优于现有方法，如蛋白质-蛋白质相互作用预测和接触预测。", "conclusion": "GLProtein通过整合全局和局部结构信息，为蛋白质功能预测提供了更强大的工具。"}}
{"id": "2506.07423", "pdf": "https://arxiv.org/pdf/2506.07423", "abs": "https://arxiv.org/abs/2506.07423", "authors": ["Janghyeon Yun", "Sang-goo Lee"], "title": "SEED: Enhancing Text-to-SQL Performance and Practical Usability Through Automatic Evidence Generation", "categories": ["cs.CL", "I.2.7"], "comment": null, "summary": "Text-to-SQL enables non-experts to retrieve data from databases by converting\nnatural language queries into SQL. However, state-of-the-art text-to-SQL\nstudies rely on the BIRD dataset, which assumes that evidence is provided along\nwith questions. Although BIRD facilitates research advancements, it assumes\nthat users have expertise and domain knowledge, contradicting the fundamental\ngoal of text-to-SQL. In addition, human-generated evidence in BIRD contains\ndefects, including missing or erroneous evidence, which affects model\nperformance. To address this issue, we propose SEED (System for Evidence\nExtraction and Domain knowledge generation), an approach that automatically\ngenerates evidence to improve performance and practical usability in real-world\nscenarios. SEED systematically analyzes database schema, description files, and\nvalues to extract relevant information. We evaluated SEED on BIRD and Spider,\ndemonstrating that it significantly improves SQL generation accuracy in the\nno-evidence scenario, and in some cases, even outperforms the setting where\nBIRD evidence is provided. Our results highlight that SEED-generated evidence\nnot only bridges the gap between research and real-world deployment but also\nimproves the adaptability and robustness of text-to-SQL models. Our code is\navailable at https://github.com/felix01189/SEED", "AI": {"tldr": "SEED是一种自动生成证据的方法，用于改进无证据场景下的Text-to-SQL性能，并在某些情况下优于BIRD提供的人工证据。", "motivation": "现有Text-to-SQL研究依赖BIRD数据集，但该数据集假设用户具备专业知识和领域知识，且人工生成的证据存在缺陷，影响模型性能。", "method": "SEED通过系统分析数据库模式、描述文件和值来自动生成证据。", "result": "在BIRD和Spider数据集上，SEED显著提高了无证据场景下的SQL生成准确性，甚至在某些情况下优于BIRD提供的人工证据。", "conclusion": "SEED生成的证据不仅填补了研究与实际部署之间的差距，还提升了Text-to-SQL模型的适应性和鲁棒性。"}}
{"id": "2506.07338", "pdf": "https://arxiv.org/pdf/2506.07338", "abs": "https://arxiv.org/abs/2506.07338", "authors": ["Yijie Deng", "Shuaihang Yuan", "Geeta Chandra Raju Bethala", "Anthony Tzes", "Yu-Shen Liu", "Yi Fang"], "title": "Hierarchical Scoring with 3D Gaussian Splatting for Instance Image-Goal Navigation", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "Instance Image-Goal Navigation (IIN) requires autonomous agents to identify\nand navigate to a target object or location depicted in a reference image\ncaptured from any viewpoint. While recent methods leverage powerful novel view\nsynthesis (NVS) techniques, such as three-dimensional Gaussian splatting\n(3DGS), they typically rely on randomly sampling multiple viewpoints or\ntrajectories to ensure comprehensive coverage of discriminative visual cues.\nThis approach, however, creates significant redundancy through overlapping\nimage samples and lacks principled view selection, substantially increasing\nboth rendering and comparison overhead. In this paper, we introduce a novel IIN\nframework with a hierarchical scoring paradigm that estimates optimal\nviewpoints for target matching. Our approach integrates cross-level semantic\nscoring, utilizing CLIP-derived relevancy fields to identify regions with high\nsemantic similarity to the target object class, with fine-grained local\ngeometric scoring that performs precise pose estimation within promising\nregions. Extensive evaluations demonstrate that our method achieves\nstate-of-the-art performance on simulated IIN benchmarks and real-world\napplicability.", "AI": {"tldr": "提出了一种基于分层评分范式的实例图像目标导航框架，通过跨层级语义评分和局部几何评分优化视点选择，减少冗余并提升性能。", "motivation": "现有方法依赖随机采样视点或轨迹，导致冗余和效率低下，缺乏优化的视点选择机制。", "method": "结合CLIP衍生的相关性场进行跨层级语义评分，识别高语义相似区域，再通过局部几何评分进行精确位姿估计。", "result": "在模拟IIN基准测试中达到最先进性能，并具有实际应用价值。", "conclusion": "提出的分层评分范式显著提升了目标导航的效率和准确性。"}}
{"id": "2506.06295", "pdf": "https://arxiv.org/pdf/2506.06295", "abs": "https://arxiv.org/abs/2506.06295", "authors": ["Zhiyuan Liu", "Yicun Yang", "Yaojie Zhang", "Junjie Chen", "Chang Zou", "Qingyuan Wei", "Shaobo Wang", "Linfeng Zhang"], "title": "dLLM-Cache: Accelerating Diffusion Large Language Models with Adaptive Caching", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Autoregressive Models (ARMs) have long dominated the landscape of Large\nLanguage Models. Recently, a new paradigm has emerged in the form of\ndiffusion-based Large Language Models (dLLMs), which generate text by\niteratively denoising masked segments. This approach has shown significant\nadvantages and potential. However, dLLMs suffer from high inference latency.\nTraditional ARM acceleration techniques, such as Key-Value caching, are\nincompatible with dLLMs due to their bidirectional attention mechanism. To\naddress this specific challenge, our work begins with a key observation that\ndLLM inference involves a static prompt and a partially dynamic response, where\nmost tokens remain stable across adjacent denoising steps. Based on this, we\npropose dLLM-Cache, a training-free adaptive caching framework that combines\nlong-interval prompt caching with partial response updates guided by feature\nsimilarity. This design enables efficient reuse of intermediate computations\nwithout compromising model performance. Extensive experiments on representative\ndLLMs, including LLaDA 8B and Dream 7B, show that dLLM-Cache achieves up to 9.1\nx speedup over standard inference without compromising output quality. Notably,\nour method brings dLLM inference latency close to that of ARMs under many\nsettings. Codes are provided in the supplementary material and will be released\npublicly on GitHub.", "AI": {"tldr": "dLLM-Cache是一种无需训练的缓存框架，通过结合长间隔提示缓存和部分响应更新，显著降低了扩散式大语言模型（dLLMs）的推理延迟，速度提升高达9.1倍。", "motivation": "扩散式大语言模型（dLLMs）在文本生成中表现出潜力，但存在高推理延迟问题，传统加速技术不适用。", "method": "提出dLLM-Cache框架，利用静态提示和部分动态响应的特点，通过特征相似性指导部分响应更新。", "result": "在LLaDA 8B和Dream 7B等模型上，dLLM-Cache实现了高达9.1倍的加速，且不影响输出质量。", "conclusion": "dLLM-Cache显著降低了dLLMs的推理延迟，使其接近自回归模型的性能。"}}
{"id": "2506.07424", "pdf": "https://arxiv.org/pdf/2506.07424", "abs": "https://arxiv.org/abs/2506.07424", "authors": ["Kyeonghyun Kim", "Jinhee Jang", "Juhwan Choi", "Yoonji Lee", "Kyohoon Jin", "YoungBin Kim"], "title": "Plug-in and Fine-tuning: Bridging the Gap between Small Language Models and Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 main conference", "summary": "Large language models (LLMs) are renowned for their extensive linguistic\nknowledge and strong generalization capabilities, but their high computational\ndemands make them unsuitable for resource-constrained environments. In\ncontrast, small language models (SLMs) are computationally efficient but often\nlack the broad generalization capacity of LLMs. To bridge this gap, we propose\nPiFi, a novel framework that combines the strengths of both LLMs and SLMs to\nachieve high performance while maintaining efficiency. PiFi integrates a single\nfrozen layer from an LLM into a SLM and fine-tunes the combined model for\nspecific tasks, boosting performance without a significant increase in\ncomputational cost. We show that PiFi delivers consistent performance\nimprovements across a range of natural language processing tasks, including\nboth natural language understanding and generation. Moreover, our findings\ndemonstrate PiFi's ability to effectively leverage LLM knowledge, enhancing\ngeneralization to unseen domains and facilitating the transfer of linguistic\nabilities.", "AI": {"tldr": "PiFi框架通过将大型语言模型（LLM）的冻结层集成到小型语言模型（SLM）中，实现了高性能与高效计算的平衡。", "motivation": "解决LLMs计算资源需求高和SLMs泛化能力不足的问题。", "method": "将LLM的冻结层集成到SLM中，并进行任务特定微调。", "result": "在多种自然语言处理任务中表现优异，提升了泛化能力和知识迁移。", "conclusion": "PiFi成功结合了LLMs和SLMs的优势，为资源受限环境提供了高效解决方案。"}}
{"id": "2506.07357", "pdf": "https://arxiv.org/pdf/2506.07357", "abs": "https://arxiv.org/abs/2506.07357", "authors": ["Satvik Praveen", "Yoonsung Jung"], "title": "CBAM-STN-TPS-YOLO: Enhancing Agricultural Object Detection through Spatially Adaptive Attention Mechanisms", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Object detection is vital in precision agriculture for plant monitoring,\ndisease detection, and yield estimation. However, models like YOLO struggle\nwith occlusions, irregular structures, and background noise, reducing detection\naccuracy. While Spatial Transformer Networks (STNs) improve spatial invariance\nthrough learned transformations, affine mappings are insufficient for non-rigid\ndeformations such as bent leaves and overlaps.\n  We propose CBAM-STN-TPS-YOLO, a model integrating Thin-Plate Splines (TPS)\ninto STNs for flexible, non-rigid spatial transformations that better align\nfeatures. Performance is further enhanced by the Convolutional Block Attention\nModule (CBAM), which suppresses background noise and emphasizes relevant\nspatial and channel-wise features.\n  On the occlusion-heavy Plant Growth and Phenotyping (PGP) dataset, our model\noutperforms STN-YOLO in precision, recall, and mAP. It achieves a 12% reduction\nin false positives, highlighting the benefits of improved spatial flexibility\nand attention-guided refinement. We also examine the impact of the TPS\nregularization parameter in balancing transformation smoothness and detection\nperformance.\n  This lightweight model improves spatial awareness and supports real-time edge\ndeployment, making it ideal for smart farming applications requiring accurate\nand efficient monitoring.", "AI": {"tldr": "提出了一种结合TPS和CBAM的改进YOLO模型，用于农业植物检测，解决了遮挡和非刚性变形问题，提升了检测精度。", "motivation": "传统YOLO模型在农业植物检测中因遮挡、非刚性变形和背景噪声导致精度不足，需要更灵活的空间变换和注意力机制。", "method": "集成TPS到STN中实现非刚性空间变换，并引入CBAM模块抑制噪声、增强特征。", "result": "在PGP数据集上，模型在精度、召回率和mAP上优于STN-YOLO，假阳性降低12%。", "conclusion": "该轻量级模型提升了空间感知能力，适合实时边缘部署，适用于智能农业。"}}
{"id": "2506.06296", "pdf": "https://arxiv.org/pdf/2506.06296", "abs": "https://arxiv.org/abs/2506.06296", "authors": ["Hanaa El Afia", "Said Ohamouddou", "Raddouane Chiheb", "Abdellatif El Afia"], "title": "Dynamic Graph CNN with Jacobi Kolmogorov-Arnold Networks for 3D Classification of Point Sets", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We introduce Jacobi-KAN-DGCNN, a framework that integrates Dynamic Graph\nConvolutional Neural Network (DGCNN) with Jacobi Kolmogorov-Arnold Networks\n(KAN) for the classification of three-dimensional point clouds. This method\nreplaces Multi-Layer Perceptron (MLP) layers with adaptable univariate\npolynomial expansions within a streamlined DGCNN architecture, circumventing\ndeep levels for both MLP and KAN to facilitate a layer-by-layer comparison. In\ncomparative experiments on the ModelNet40 dataset, KAN layers employing Jacobi\npolynomials outperform the traditional linear layer-based DGCNN baseline in\nterms of accuracy and convergence speed, while maintaining parameter\nefficiency. Our results demonstrate that higher polynomial degrees do not\nautomatically improve performance, highlighting the need for further\ntheoretical and empirical investigation to fully understand the interactions\nbetween polynomial bases, degrees, and the mechanisms of graph-based learning.", "AI": {"tldr": "Jacobi-KAN-DGCNN框架结合动态图卷积神经网络（DGCNN）和Jacobi多项式KAN，用于三维点云分类，性能优于传统线性层DGCNN。", "motivation": "传统方法使用多层感知机（MLP）可能效率不高，需要更高效且参数优化的分类方法。", "method": "用可调单变量多项式展开替换MLP层，结合DGCNN架构，避免深层结构，实现逐层比较。", "result": "在ModelNet40数据集上，Jacobi多项式KAN层在准确性和收敛速度上优于传统DGCNN，且参数高效。", "conclusion": "高多项式阶数不一定提升性能，需进一步研究多项式基、阶数与图学习机制的相互作用。"}}
{"id": "2506.07429", "pdf": "https://arxiv.org/pdf/2506.07429", "abs": "https://arxiv.org/abs/2506.07429", "authors": ["Ratna Kandala"], "title": "Conjoined Predication and Scalar Implicature", "categories": ["cs.CL"], "comment": null, "summary": "Magri (2016) investigates two puzzles arising from conjunction. Although\nMagri has proposed a solution to the second puzzle, the first remains\nunresolved. This first puzzle reveals a hidden interaction among\nquantification, collective/concurrent interpretation, and contextual updating\ndimensions that have yet to be explored. In essence, the problem is that\ncertain forms of sentences like \"Some Italians come from a warm country,\" when\nconjoined as in \"(Only) Some Italians come from a warm country and are blond,\"\nsound infelicitous, even though no obvious alternative triggers a conflicting\nscalar implicature. In this paper, we offer a conceptual analysis of Magri's\nfirst puzzle by situating it within its original theoretical framework. We\nargue that the oddness arises from the collective or concurrent reading of the\nconjunctive predicate: in examples such as \"(Only) Some Italians come from a\nwarm country and are blond,\" this interpretation generates an indirect\ncontextual contradiction. Moreover, we suggest that the pragmatic mechanisms\ngoverning scalar implicature generation extend beyond what is captured by\nexhaustification-based grammatical licensing accounts.", "AI": {"tldr": "Magri (2016)研究了一个关于连词的未解之谜，揭示了量化、集体/并发解释与语境更新之间的隐藏互动。本文通过理论框架分析，认为连词的不自然源于集体或并发解读导致的间接语境矛盾，并指出语用机制超出传统语法解释。", "motivation": "解决Magri提出的第一个未解之谜，探索量化、集体/并发解释与语境更新的互动关系。", "method": "通过理论框架分析，结合集体或并发解读，探讨连词不自然的原因。", "result": "发现连词的不自然源于集体或并发解读导致的间接语境矛盾，语用机制超出传统语法解释。", "conclusion": "连词问题揭示了语用机制的复杂性，需扩展传统语法解释框架。"}}
{"id": "2506.07364", "pdf": "https://arxiv.org/pdf/2506.07364", "abs": "https://arxiv.org/abs/2506.07364", "authors": ["Chengchao Shen", "Dawei Liu", "Jianxin Wang"], "title": "Multiple Object Stitching for Unsupervised Representation Learning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Contrastive learning for single object centric images has achieved remarkable\nprogress on unsupervised representation, but suffering inferior performance on\nthe widespread images with multiple objects. In this paper, we propose a simple\nbut effective method, Multiple Object Stitching (MOS), to refine the\nunsupervised representation for multi-object images. Specifically, we construct\nthe multi-object images by stitching the single object centric ones, where the\nobjects in the synthesized multi-object images are predetermined. Hence,\ncompared to the existing contrastive methods, our method provides additional\nobject correspondences between multi-object images without human annotations.\nIn this manner, our method pays more attention to the representations of each\nobject in multi-object image, thus providing more detailed representations for\ncomplicated downstream tasks, such as object detection and semantic\nsegmentation. Experimental results on ImageNet, CIFAR and COCO datasets\ndemonstrate that our proposed method achieves the leading unsupervised\nrepresentation performance on both single object centric images and\nmulti-object ones. The source code is available at\nhttps://github.com/visresearch/MultipleObjectStitching.", "AI": {"tldr": "论文提出了一种名为MOS的方法，通过拼接单目标图像来改进多目标图像的无监督表示学习。", "motivation": "现有的对比学习方法在多目标图像上表现不佳，MOS方法旨在解决这一问题。", "method": "通过拼接单目标图像构建多目标图像，利用预定的对象对应关系优化表示学习。", "result": "在ImageNet、CIFAR和COCO数据集上，MOS方法在单目标和多目标图像上均取得领先的无监督表示性能。", "conclusion": "MOS方法简单有效，能够为复杂下游任务提供更详细的表示。"}}
{"id": "2506.06297", "pdf": "https://arxiv.org/pdf/2506.06297", "abs": "https://arxiv.org/abs/2506.06297", "authors": ["Bozhi Sun", "Seda Tierney", "Jeffrey A. Feinstein", "Frederick Damen", "Alison L. Marsden", "Daniele E. Schiavazzi"], "title": "Optimal patient allocation for echocardiographic assessments", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Scheduling echocardiographic exams in a hospital presents significant\nchallenges due to non-deterministic factors (e.g., patient no-shows, patient\narrival times, diverse exam durations, etc.) and asymmetric resource\nconstraints between fetal and non-fetal patient streams. To address these\nchallenges, we first conducted extensive pre-processing on one week of\noperational data from the Echo Laboratory at Stanford University's Lucile\nPackard Children's Hospital, to estimate patient no-show probabilities and\nderive empirical distributions of arrival times and exam durations. Based on\nthese inputs, we developed a discrete-event stochastic simulation model using\nSimPy, and integrate it with the open source Gymnasium Python library. As a\nbaseline for policy optimization, we developed a comparative framework to\nevaluate on-the-fly versus reservation-based allocation strategies, in which\ndifferent proportions of resources are reserved in advance. Considering a\nhospital configuration with a 1:6 ratio of fetal to non-fetal rooms and a 4:2\nratio of fetal to non-fetal sonographers, we show that on-the-fly allocation\ngenerally yields better performance, more effectively adapting to patient\nvariability and resource constraints. Building on this foundation, we apply\nreinforcement learning (RL) to derive an approximated optimal dynamic\nallocation policy. This RL-based policy is benchmarked against the\nbest-performing rule-based strategies, allowing us to quantify their\ndifferences and provide actionable insights for improving echo lab efficiency\nthrough intelligent, data-driven resource management.", "AI": {"tldr": "论文通过离散事件随机模拟和强化学习优化医院超声检查调度，提出动态分配策略优于预留策略。", "motivation": "医院超声检查调度面临非确定性因素和资源不对称的挑战，需数据驱动的方法优化资源管理。", "method": "使用SimPy构建离散事件随机模拟模型，结合Gymnasium库，比较动态分配与预留策略，并应用强化学习优化策略。", "result": "动态分配策略在适应患者变异和资源限制方面表现更优，强化学习策略进一步提升了效率。", "conclusion": "数据驱动的动态资源分配策略可有效提升超声实验室效率，强化学习为优化提供了新途径。"}}
{"id": "2506.07434", "pdf": "https://arxiv.org/pdf/2506.07434", "abs": "https://arxiv.org/abs/2506.07434", "authors": ["Feifan Song", "Shaohang Wei", "Wen Luo", "Yuxuan Fan", "Tianyu Liu", "Guoyin Wang", "Houfeng Wang"], "title": "Well Begun is Half Done: Low-resource Preference Alignment by Weak-to-Strong Decoding", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted by ACL 2025 Findings", "summary": "Large Language Models (LLMs) require alignment with human preferences to\navoid generating offensive, false, or meaningless content. Recently,\nlow-resource methods for LLM alignment have been popular, while still facing\nchallenges in obtaining both high-quality and aligned content. Motivated by the\nobservation that the difficulty of generating aligned responses is concentrated\nat the beginning of decoding, we propose a novel framework, Weak-to-Strong\nDecoding (WSD), to enhance the alignment ability of base models by the guidance\nof a small aligned model. The small model first drafts well-aligned beginnings,\nfollowed by the large base model to continue the rest, controlled by a\nwell-designed auto-switch mechanism. We also collect a new dataset, GenerAlign,\nto fine-tune a small-sized Pilot-3B as the draft model, which effectively\nenhances different base models under the WSD framework to outperform all\nbaseline methods, while avoiding degradation on downstream tasks, termed as the\nalignment tax. Extensive experiments are further conducted to examine the\nimpact of different settings and time efficiency, as well as analyses on the\nintrinsic mechanisms of WSD in depth.", "AI": {"tldr": "论文提出了一种名为弱到强解码（WSD）的新框架，通过小型对齐模型指导增强基础模型的对齐能力，解决了生成高质量且对齐内容的挑战。", "motivation": "大型语言模型（LLMs）需要与人类偏好对齐以避免生成冒犯性、虚假或无意义的内容。低资源对齐方法虽流行，但仍面临同时实现高质量和对齐的挑战。", "method": "WSD框架利用小型对齐模型起草对齐的开头，再由大型基础模型继续生成后续内容，并通过自动切换机制控制。使用新数据集GenerAlign微调小型Pilot-3B作为起草模型。", "result": "WSD框架显著提升了不同基础模型的对齐能力，优于所有基线方法，同时避免了在下游任务上的性能下降（对齐税）。", "conclusion": "WSD通过小型模型的引导有效解决了对齐问题，实验验证了其在不同设置和时间效率上的优势，并深入分析了其内在机制。"}}
{"id": "2506.07368", "pdf": "https://arxiv.org/pdf/2506.07368", "abs": "https://arxiv.org/abs/2506.07368", "authors": ["Jiaying He", "Yitong Lin", "Jiahe Chen", "Honghui Xu", "Jianwei Zheng"], "title": "C3S3: Complementary Competition and Contrastive Selection for Semi-Supervised Medical Image Segmentation", "categories": ["cs.CV", "cs.AI"], "comment": "6 pages, 4 figures, ICME2025", "summary": "For the immanent challenge of insufficiently annotated samples in the medical\nfield, semi-supervised medical image segmentation (SSMIS) offers a promising\nsolution. Despite achieving impressive results in delineating primary target\nareas, most current methodologies struggle to precisely capture the subtle\ndetails of boundaries. This deficiency often leads to significant diagnostic\ninaccuracies. To tackle this issue, we introduce C3S3, a novel semi-supervised\nsegmentation model that synergistically integrates complementary competition\nand contrastive selection. This design significantly sharpens boundary\ndelineation and enhances overall precision. Specifically, we develop an\n$\\textit{Outcome-Driven Contrastive Learning}$ module dedicated to refining\nboundary localization. Additionally, we incorporate a $\\textit{Dynamic\nComplementary Competition}$ module that leverages two high-performing\nsub-networks to generate pseudo-labels, thereby further improving segmentation\nquality. The proposed C3S3 undergoes rigorous validation on two publicly\naccessible datasets, encompassing the practices of both MRI and CT scans. The\nresults demonstrate that our method achieves superior performance compared to\nprevious cutting-edge competitors. Especially, on the 95HD and ASD metrics, our\napproach achieves a notable improvement of at least $6\\%$, highlighting the\nsignificant advancements. The code is available at\nhttps://github.com/Y-TARL/C3S3.", "AI": {"tldr": "论文提出了一种名为C3S3的半监督医学图像分割模型，通过互补竞争和对比选择提升边界细节的精确度，显著优于现有方法。", "motivation": "医学领域标注样本不足，现有方法在边界细节捕捉上表现不佳，导致诊断不准确。", "method": "结合了结果驱动的对比学习模块和动态互补竞争模块，利用两个高性能子网络生成伪标签。", "result": "在两个公开数据集上验证，性能显著优于现有方法，95HD和ASD指标提升至少6%。", "conclusion": "C3S3模型在边界细节和整体精度上表现优异，为医学图像分割提供了有效解决方案。"}}
{"id": "2506.06298", "pdf": "https://arxiv.org/pdf/2506.06298", "abs": "https://arxiv.org/abs/2506.06298", "authors": ["Daniel Halpern", "Evi Micha", "Ariel D. Procaccia", "Itai Shapira"], "title": "Pairwise Calibrated Rewards for Pluralistic Alignment", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Current alignment pipelines presume a single, universal notion of desirable\nbehavior. However, human preferences often diverge across users, contexts, and\ncultures. As a result, disagreement collapses into the majority signal and\nminority perspectives are discounted. To address this, we propose reflecting\ndiverse human preferences through a distribution over multiple reward\nfunctions, each inducing a distinct aligned policy. The distribution is learned\ndirectly from pairwise preference without annotator identifiers or predefined\ngroups. Instead, annotator disagreements are treated as informative soft\nlabels. Our central criterion is pairwise calibration: for every pair of\ncandidate responses, the proportion of reward functions preferring one response\nmatches the fraction of annotators with that preference. We prove that even a\nsmall outlier-free ensemble can accurately represent diverse preference\ndistributions. Empirically, we introduce and validate a practical training\nheuristic to learn such ensembles, and demonstrate its effectiveness through\nimproved calibration, implying a more faithful representation of pluralistic\nvalues.", "AI": {"tldr": "论文提出了一种通过多奖励函数分布反映多样化人类偏好的方法，解决了当前对齐流程忽视少数观点的问题。", "motivation": "人类偏好因用户、背景和文化而异，但现有对齐流程假设单一理想行为，导致少数观点被忽视。", "method": "通过从成对偏好中学习奖励函数分布，无需标注者标识或预定义组，将分歧视为信息性软标签。核心标准是成对校准。", "result": "理论和实验表明，小型无异常集合能准确表示多样化偏好分布，训练启发式方法提高了校准性。", "conclusion": "该方法能更忠实反映多元化价值观，改进对齐流程。"}}
{"id": "2506.07438", "pdf": "https://arxiv.org/pdf/2506.07438", "abs": "https://arxiv.org/abs/2506.07438", "authors": ["Jooyoung Choi", "Hyun Kim", "Hansol Jang", "Changwook Jun", "Kyunghoon Bae", "Hyewon Choi", "Stanley Jungkyu Choi", "Honglak Lee", "Chulmin Yun"], "title": "LG-ANNA-Embedding technical report", "categories": ["cs.CL"], "comment": "10 pages", "summary": "This report presents a unified instruction-based framework for learning\ngeneralized text embeddings optimized for both information retrieval (IR) and\nnon-IR tasks. Built upon a decoder-only large language model (Mistral-7B), our\napproach combines in-context learning, soft supervision, and adaptive\nhard-negative mining to generate context-aware embeddings without task-specific\nfine-tuning. Structured instructions and few-shot examples are used to guide\nthe model across diverse tasks, enabling strong performance on classification,\nsemantic similarity, clustering, and reranking benchmarks. To improve semantic\ndiscrimination, we employ a soft labeling framework where continuous relevance\nscores, distilled from a high-performance dense retriever and reranker, serve\nas fine-grained supervision signals. In addition, we introduce adaptive\nmargin-based hard-negative mining, which filters out semantically ambiguous\nnegatives based on their similarity to positive examples, thereby enhancing\ntraining stability and retrieval robustness. Our model is evaluated on the\nnewly introduced MTEB (English, v2) benchmark, covering 41 tasks across seven\ncategories. Results show that our method achieves strong generalization and\nranks among the top-performing models by Borda score, outperforming several\nlarger or fully fine-tuned baselines. These findings highlight the\neffectiveness of combining in-context prompting, soft supervision, and adaptive\nsampling for scalable, high-quality embedding generation.", "AI": {"tldr": "提出了一种基于指令的统一框架，用于学习适用于信息检索（IR）和非IR任务的通用文本嵌入。该方法结合了上下文学习、软监督和自适应硬负样本挖掘，无需任务特定微调即可生成上下文感知嵌入。", "motivation": "解决传统方法在通用文本嵌入任务中需要任务特定微调的问题，同时提升语义区分能力。", "method": "基于Mistral-7B模型，结合上下文学习、软监督（连续相关性分数）和自适应硬负样本挖掘。", "result": "在MTEB（英语，v2）基准测试中表现优异，覆盖41个任务，排名靠前，优于多个更大或完全微调的基线模型。", "conclusion": "结合上下文提示、软监督和自适应采样是一种高效且可扩展的嵌入生成方法。"}}
{"id": "2506.07369", "pdf": "https://arxiv.org/pdf/2506.07369", "abs": "https://arxiv.org/abs/2506.07369", "authors": ["Bolin Chen", "Shanzhi Yin", "Goluck Konuko", "Giuseppe Valenzise", "Zihan Zhang", "Shiqi Wang", "Yan Ye"], "title": "Generative Models at the Frontier of Compression: A Survey on Generative Face Video Coding", "categories": ["cs.CV"], "comment": null, "summary": "The rise of deep generative models has greatly advanced video compression,\nreshaping the paradigm of face video coding through their powerful capability\nfor semantic-aware representation and lifelike synthesis. Generative Face Video\nCoding (GFVC) stands at the forefront of this revolution, which could\ncharacterize complex facial dynamics into compact latent codes for bitstream\ncompactness at the encoder side and leverages powerful deep generative models\nto reconstruct high-fidelity face signal from the compressed latent codes at\nthe decoder side. As such, this well-designed GFVC paradigm could enable\nhigh-fidelity face video communication at ultra-low bitrate ranges, far\nsurpassing the capabilities of the latest Versatile Video Coding (VVC)\nstandard. To pioneer foundational research and accelerate the evolution of\nGFVC, this paper presents the first comprehensive survey of GFVC technologies,\nsystematically bridging critical gaps between theoretical innovation and\nindustrial standardization. In particular, we first review a broad range of\nexisting GFVC methods with different feature representations and optimization\nstrategies, and conduct a thorough benchmarking analysis. In addition, we\nconstruct a large-scale GFVC-compressed face video database with subjective\nMean Opinion Scores (MOSs) based on human perception, aiming to identify the\nmost appropriate quality metrics tailored to GFVC. Moreover, we summarize the\nGFVC standardization potentials with a unified high-level syntax and develop a\nlow-complexity GFVC system which are both expected to push forward future\npractical deployments and applications. Finally, we envision the potential of\nGFVC in industrial applications and deliberate on the current challenges and\nfuture opportunities.", "AI": {"tldr": "本文综述了生成式人脸视频编码（GFVC）技术，分析了其在高保真低码率视频通信中的优势，并提出了标准化和实际应用的未来方向。", "motivation": "随着深度生成模型的兴起，GFVC通过其语义感知和逼真合成能力，显著提升了人脸视频压缩效率，超越了现有标准（如VVC）。本文旨在填补理论创新与工业标准化之间的关键空白。", "method": "本文系统回顾了多种GFVC方法，进行了基准测试分析，并构建了一个大规模的主观评分数据库。此外，提出了统一的高级语法和低复杂度系统设计。", "result": "GFVC在高保真低码率视频通信中表现优异，超越了VVC标准。通过数据库和标准化设计，为未来应用奠定了基础。", "conclusion": "GFVC在工业应用中潜力巨大，但仍需解决当前挑战，如标准化和实际部署问题。未来研究应关注这些方向。"}}
{"id": "2506.06299", "pdf": "https://arxiv.org/pdf/2506.06299", "abs": "https://arxiv.org/abs/2506.06299", "authors": ["Daniel Thilo Schroeder", "Meeyoung Cha", "Andrea Baronchelli", "Nick Bostrom", "Nicholas A. Christakis", "David Garcia", "Amit Goldenberg", "Yara Kyrychenko", "Kevin Leyton-Brown", "Nina Lutz", "Gary Marcus", "Filippo Menczer", "Gordon Pennycook", "David G. Rand", "Frank Schweitzer", "Christopher Summerfield", "Audrey Tang", "Jay Van Bavel", "Sander van der Linden", "Dawn Song", "Jonas R. Kunst"], "title": "How Malicious AI Swarms Can Threaten Democracy", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.LG"], "comment": "8 pages, 1 figure", "summary": "Advances in AI portend a new era of sophisticated disinformation operations.\nWhile individual AI systems already create convincing -- and at times\nmisleading -- information, an imminent development is the emergence of\nmalicious AI swarms. These systems can coordinate covertly, infiltrate\ncommunities, evade traditional detectors, and run continuous A/B tests, with\nround-the-clock persistence. The result can include fabricated grassroots\nconsensus, fragmented shared reality, mass harassment, voter micro-suppression\nor mobilization, contamination of AI training data, and erosion of\ninstitutional trust. With democratic processes worldwide increasingly\nvulnerable, we urge a three-pronged response: (1) platform-side defenses --\nalways-on swarm-detection dashboards, pre-election high-fidelity\nswarm-simulation stress-tests, transparency audits, and optional client-side\n\"AI shields\" for users; (2) model-side safeguards -- standardized\npersuasion-risk tests, provenance-authenticating passkeys, and watermarking;\nand (3) system-level oversight -- a UN-backed AI Influence Observatory.", "AI": {"tldr": "论文探讨了AI恶意群体（swarms）对信息生态的威胁，并提出三方面应对措施。", "motivation": "AI技术的进步可能导致更复杂的虚假信息操作，威胁民主进程和社会信任。", "method": "提出平台端、模型端和系统级的三重防御策略。", "result": "恶意AI群体可能导致虚假共识、社会分裂、选举干预等问题。", "conclusion": "需采取多层次防御措施，包括技术工具和国际监管，以应对AI恶意群体的威胁。"}}
{"id": "2506.07453", "pdf": "https://arxiv.org/pdf/2506.07453", "abs": "https://arxiv.org/abs/2506.07453", "authors": ["Pritom Saha Akash", "Kevin Chen-Chuan Chang"], "title": "Understanding Cross-Domain Adaptation in Low-Resource Topic Modeling", "categories": ["cs.CL"], "comment": null, "summary": "Topic modeling plays a vital role in uncovering hidden semantic structures\nwithin text corpora, but existing models struggle in low-resource settings\nwhere limited target-domain data leads to unstable and incoherent topic\ninference. We address this challenge by formally introducing domain adaptation\nfor low-resource topic modeling, where a high-resource source domain informs a\nlow-resource target domain without overwhelming it with irrelevant content. We\nestablish a finite-sample generalization bound showing that effective knowledge\ntransfer depends on robust performance in both domains, minimizing latent-space\ndiscrepancy, and preventing overfitting to the data. Guided by these insights,\nwe propose DALTA (Domain-Aligned Latent Topic Adaptation), a new framework that\nemploys a shared encoder for domain-invariant features, specialized decoders\nfor domain-specific nuances, and adversarial alignment to selectively transfer\nrelevant information. Experiments on diverse low-resource datasets demonstrate\nthat DALTA consistently outperforms state-of-the-art methods in terms of topic\ncoherence, stability, and transferability.", "AI": {"tldr": "论文提出了一种名为DALTA的新框架，用于解决低资源主题建模中的领域适应问题，通过共享编码器和对抗对齐实现知识迁移。", "motivation": "现有主题模型在低资源环境下表现不佳，导致主题推断不稳定且不连贯，因此需要一种方法在有限目标领域数据下实现有效知识迁移。", "method": "提出DALTA框架，采用共享编码器提取领域不变特征，专用解码器处理领域特定细节，并通过对抗对齐选择性迁移相关信息。", "result": "实验表明，DALTA在主题连贯性、稳定性和可迁移性方面均优于现有方法。", "conclusion": "DALTA通过领域对齐和选择性知识迁移，显著提升了低资源主题建模的性能。"}}
{"id": "2506.07371", "pdf": "https://arxiv.org/pdf/2506.07371", "abs": "https://arxiv.org/abs/2506.07371", "authors": ["Ruchit Rawal", "Reza Shirkavand", "Heng Huang", "Gowthami Somepalli", "Tom Goldstein"], "title": "ARGUS: Hallucination and Omission Evaluation in Video-LLMs", "categories": ["cs.CV"], "comment": "Project page with all the artifacts:\n  https://ruchitrawal.github.io/argus", "summary": "Video large language models have not yet been widely deployed, largely due to\ntheir tendency to hallucinate. Typical benchmarks for Video-LLMs rely simply on\nmultiple-choice questions. Unfortunately, VideoLLMs hallucinate far more\naggressively on freeform text generation tasks like video captioning than they\ndo on multiple choice verification tasks. To address this weakness, we propose\nARGUS, a VideoLLM benchmark that measures freeform video captioning\nperformance. By comparing VideoLLM outputs to human ground truth captions,\nARGUS quantifies dual metrics. First, we measure the rate of hallucinations in\nthe form of incorrect statements about video content or temporal relationships.\nSecond, we measure the rate at which the model omits important descriptive\ndetails. Together, these dual metrics form a comprehensive view of video\ncaptioning performance.", "AI": {"tldr": "ARGUS是一个新的VideoLLM基准测试，专注于自由文本生成任务（如视频字幕），通过量化幻觉率和遗漏率来评估模型性能。", "motivation": "当前VideoLLM在自由文本生成任务中幻觉问题严重，而现有基准测试主要依赖选择题，无法全面评估模型表现。", "method": "提出ARGUS基准，通过比较VideoLLM生成的字幕与人工标注的真实字幕，量化幻觉（错误内容）和遗漏（重要细节缺失）两个指标。", "result": "ARGUS提供了对视频字幕性能的全面评估，揭示了模型在自由文本生成中的主要问题。", "conclusion": "ARGUS填补了VideoLLM评估的空白，为改进模型在自由文本任务中的表现提供了方向。"}}
{"id": "2506.06303", "pdf": "https://arxiv.org/pdf/2506.06303", "abs": "https://arxiv.org/abs/2506.06303", "authors": ["Kefan Song", "Amir Moeini", "Peng Wang", "Lei Gong", "Rohan Chandra", "Yanjun Qi", "Shangtong Zhang"], "title": "Reward Is Enough: LLMs Are In-Context Reinforcement Learners", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Reinforcement learning (RL) is a human-designed framework for solving\nsequential decision making problems. In this work, we demonstrate that,\nsurprisingly, RL emerges in LLM's (Large Language Model) inference time -- a\nphenomenon known as in-context RL (ICRL). Specifically, we propose a novel\nmulti-round prompting framework called ICRL prompting. The goal is to prompt\nthe LLM to complete a task. After the LLM generates a response at the current\nround, we give numerical scalar feedbacks for the response, called the rewards.\nAt the next round, we prompt the LLM again with the same task and a context\nconsisting of all previous responses and rewards. We observe that the quality\nof the LLM's response increases as the context grows. In other words, the LLM\nis able to maximize the scalar reward signal in the inference time, just like\nan RL algorithm. We evaluate ICRL prompting in three benchmarks (Game of 24,\ncreative writing, and ScienceWorld) and demonstrate significant performance\nimprovements over baseline methods such as Self-Refine and Reflexion.\nSurprisingly, in some experiments the reward signals are generated by the LLM\nitself, yet performance improvements are still observed from ICRL prompting,\noffering a promising paradigm for scaling test-time compute.", "AI": {"tldr": "论文提出了一种称为ICRL prompting的多轮提示框架，发现LLM在推理时表现出类似强化学习的行为，能够通过反馈优化响应质量。", "motivation": "探索LLM在推理时是否能够表现出类似强化学习的行为，以优化任务完成质量。", "method": "提出ICRL prompting框架，通过多轮提示和反馈（奖励信号）逐步优化LLM的响应。", "result": "在三个基准测试中，ICRL prompting显著优于基线方法，甚至在使用LLM自身生成奖励信号时也观察到性能提升。", "conclusion": "ICRL prompting为LLM在推理时优化任务完成提供了新范式，展示了强化学习行为在LLM中的涌现。"}}
{"id": "2506.07458", "pdf": "https://arxiv.org/pdf/2506.07458", "abs": "https://arxiv.org/abs/2506.07458", "authors": ["Yuxin Xiao", "Shan Chen", "Jack Gallifant", "Danielle Bitterman", "Thomas Hartvigsen", "Marzyeh Ghassemi"], "title": "KScope: A Framework for Characterizing the Knowledge Status of Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Characterizing a large language model's (LLM's) knowledge of a given question\nis challenging. As a result, prior work has primarily examined LLM behavior\nunder knowledge conflicts, where the model's internal parametric memory\ncontradicts information in the external context. However, this does not fully\nreflect how well the model knows the answer to the question. In this paper, we\nfirst introduce a taxonomy of five knowledge statuses based on the consistency\nand correctness of LLM knowledge modes. We then propose KScope, a hierarchical\nframework of statistical tests that progressively refines hypotheses about\nknowledge modes and characterizes LLM knowledge into one of these five\nstatuses. We apply KScope to nine LLMs across four datasets and systematically\nestablish: (1) Supporting context narrows knowledge gaps across models. (2)\nContext features related to difficulty, relevance, and familiarity drive\nsuccessful knowledge updates. (3) LLMs exhibit similar feature preferences when\npartially correct or conflicted, but diverge sharply when consistently wrong.\n(4) Context summarization constrained by our feature analysis, together with\nenhanced credibility, further improves update effectiveness and generalizes\nacross LLMs.", "AI": {"tldr": "论文提出了一种名为KScope的分层框架，用于评估大型语言模型（LLM）的知识状态，并将其分为五种类型。通过实验验证了上下文支持、特征分析和总结对知识更新的影响。", "motivation": "现有方法主要关注LLM在知识冲突下的行为，未能全面评估其对问题的知识掌握程度。", "method": "引入五种知识状态的分类法，并提出KScope框架，通过统计测试逐步细化假设并分类知识状态。", "result": "实验表明上下文支持缩小知识差距，特定特征驱动成功更新，不同LLM在错误状态下行为差异显著。", "conclusion": "结合特征分析和增强可信度的上下文总结能有效提升知识更新效果，且具有跨模型的通用性。"}}
{"id": "2506.07375", "pdf": "https://arxiv.org/pdf/2506.07375", "abs": "https://arxiv.org/abs/2506.07375", "authors": ["Xunjie He", "Christina Dao Wen Lee", "Meiling Wang", "Chengran Yuan", "Zefan Huang", "Yufeng Yue", "Marcelo H. Ang Jr"], "title": "DINO-CoDT: Multi-class Collaborative Detection and Tracking with Vision Foundation Models", "categories": ["cs.CV"], "comment": null, "summary": "Collaborative perception plays a crucial role in enhancing environmental\nunderstanding by expanding the perceptual range and improving robustness\nagainst sensor failures, which primarily involves collaborative 3D detection\nand tracking tasks. The former focuses on object recognition in individual\nframes, while the latter captures continuous instance tracklets over time.\nHowever, existing works in both areas predominantly focus on the vehicle\nsuperclass, lacking effective solutions for both multi-class collaborative\ndetection and tracking. This limitation hinders their applicability in\nreal-world scenarios, which involve diverse object classes with varying\nappearances and motion patterns. To overcome these limitations, we propose a\nmulti-class collaborative detection and tracking framework tailored for diverse\nroad users. We first present a detector with a global spatial attention fusion\n(GSAF) module, enhancing multi-scale feature learning for objects of varying\nsizes. Next, we introduce a tracklet RE-IDentification (REID) module that\nleverages visual semantics with a vision foundation model to effectively reduce\nID SWitch (IDSW) errors, in cases of erroneous mismatches involving small\nobjects like pedestrians. We further design a velocity-based adaptive tracklet\nmanagement (VATM) module that adjusts the tracking interval dynamically based\non object motion. Extensive experiments on the V2X-Real and OPV2V datasets show\nthat our approach significantly outperforms existing state-of-the-art methods\nin both detection and tracking accuracy.", "AI": {"tldr": "提出了一种多类别协作检测与跟踪框架，通过全局空间注意力融合模块和视觉语义重识别模块，显著提升了检测和跟踪的准确性。", "motivation": "现有协作感知研究主要集中于车辆类别，缺乏对多类别对象的有效解决方案，限制了实际应用。", "method": "设计了全局空间注意力融合（GSAF）模块增强多尺度特征学习，引入视觉语义重识别（REID）模块减少ID切换错误，并开发了基于速度的自适应轨迹管理（VATM）模块。", "result": "在V2X-Real和OPV2V数据集上的实验表明，该方法在检测和跟踪精度上显著优于现有方法。", "conclusion": "该框架为多类别道路用户的协作感知提供了有效解决方案，提升了实际场景的适用性。"}}
{"id": "2506.06313", "pdf": "https://arxiv.org/pdf/2506.06313", "abs": "https://arxiv.org/abs/2506.06313", "authors": ["Huiyao Chen", "Yi Yang", "Yinghui Li", "Meishan Zhang", "Min Zhang"], "title": "DISRetrieval: Harnessing Discourse Structure for Long Document Retrieval", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": "21 pages, 7 figures", "summary": "Long document understanding has become increasingly crucial in natural\nlanguage processing, with retrieval-based methods emerging as a promising\nsolution to address the context length limitations of large language models\n(LLMs). However, existing approaches either treat documents as flat sequences\nor employ arbitrary chunking strategies, failing to capture the inherent\ndiscourse structure that guides human comprehension. We present DISRetrieval, a\nnovel hierarchical retrieval framework that leverages linguistic discourse\nstructure to enhance long document understanding. Our approach introduces three\nkey innovations: (1) a discourse-aware document organization framework that\nutilizes rhetorical structure theory (RST) to create sentence-level\nhierarchical representations, preserving both semantic relationships and\nnatural document flow; (2) an LLM-enhanced node representation technique that\ncombines discourse structure with adaptive summarization to enrich tree nodes\nwith contextual information; and (3) a hierarchical evidence retrieval\nmechanism that effectively selects relevant content while maintaining discourse\ncoherence. Through comprehensive experiments on QASPER and QuALITY datasets,\nDISRetrieval demonstrates substantial improvements over existing methods in\nboth token-level retrieval metrics and downstream question answering tasks. Our\nablation studies confirm that incorporating discourse structure significantly\nenhances retrieval effectiveness across different document lengths and query\ntypes, validating the importance of linguistically-informed document\nrepresentation in long-text understanding. Our code and datasets are publicly\navailable at github/DreamH1gh/DISRetrieval to facilitate future research.", "AI": {"tldr": "DISRetrieval是一种基于语言学话语结构的层次化检索框架，通过RST理论组织文档，结合LLM增强节点表示和层次化证据检索，显著提升了长文档理解的效果。", "motivation": "解决现有方法无法捕捉文档内在话语结构的问题，提升长文档理解的效率和准确性。", "method": "1. 使用RST理论构建句子级层次化表示；2. 结合LLM和自适应摘要增强节点表示；3. 层次化证据检索机制。", "result": "在QASPER和QuALITY数据集上表现优于现有方法，验证了话语结构的重要性。", "conclusion": "DISRetrieval通过语言学驱动的文档表示，显著提升了长文档理解的性能。"}}
{"id": "2506.07461", "pdf": "https://arxiv.org/pdf/2506.07461", "abs": "https://arxiv.org/abs/2506.07461", "authors": ["Siddartha Devic", "Tejas Srinivasan", "Jesse Thomason", "Willie Neiswanger", "Vatsal Sharan"], "title": "From Calibration to Collaboration: LLM Uncertainty Quantification Should Be More Human-Centered", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are increasingly assisting users in the real\nworld, yet their reliability remains a concern. Uncertainty quantification (UQ)\nhas been heralded as a tool to enhance human-LLM collaboration by enabling\nusers to know when to trust LLM predictions. We argue that current practices\nfor uncertainty quantification in LLMs are not optimal for developing useful UQ\nfor human users making decisions in real-world tasks. Through an analysis of 40\nLLM UQ methods, we identify three prevalent practices hindering the community's\nprogress toward its goal of benefiting downstream users: 1) evaluating on\nbenchmarks with low ecological validity; 2) considering only epistemic\nuncertainty; and 3) optimizing metrics that are not necessarily indicative of\ndownstream utility. For each issue, we propose concrete user-centric practices\nand research directions that LLM UQ researchers should consider. Instead of\nhill-climbing on unrepresentative tasks using imperfect metrics, we argue that\nthe community should adopt a more human-centered approach to LLM uncertainty\nquantification.", "AI": {"tldr": "论文指出当前大语言模型（LLM）的不确定性量化（UQ）方法存在不足，并提出改进方向。", "motivation": "LLM在实际应用中的可靠性问题需要解决，UQ被认为能增强人机协作，但现有方法未能有效支持用户决策。", "method": "分析了40种LLM UQ方法，识别出三个主要问题：1）生态效度低的基准测试；2）仅考虑认知不确定性；3）优化指标与下游效用无关。", "result": "提出针对每个问题的用户中心化改进建议和研究方向。", "conclusion": "呼吁采用更以人为本的UQ方法，而非在非代表性任务上优化不完美指标。"}}
{"id": "2506.07376", "pdf": "https://arxiv.org/pdf/2506.07376", "abs": "https://arxiv.org/abs/2506.07376", "authors": ["Jintao Tong", "Ran Ma", "Yixiong Zou", "Guangyao Chen", "Yuhua Li", "Ruixuan Li"], "title": "Adapter Naturally Serves as Decoupler for Cross-Domain Few-Shot Semantic Segmentation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "ICML 2025 Spotlight", "summary": "Cross-domain few-shot segmentation (CD-FSS) is proposed to pre-train the\nmodel on a source-domain dataset with sufficient samples, and then transfer the\nmodel to target-domain datasets where only a few samples are available for\nefficient fine-tuning. There are majorly two challenges in this task: (1) the\ndomain gap and (2) fine-tuning with scarce data. To solve these challenges, we\nrevisit the adapter-based methods, and discover an intriguing insight not\nexplored in previous works: the adapter not only helps the fine-tuning of\ndownstream tasks but also naturally serves as a domain information decoupler.\nThen, we delve into this finding for an interpretation, and find the model's\ninherent structure could lead to a natural decoupling of domain information.\nBuilding upon this insight, we propose the Domain Feature Navigator (DFN),\nwhich is a structure-based decoupler instead of loss-based ones like current\nworks, to capture domain-specific information, thereby directing the model's\nattention towards domain-agnostic knowledge. Moreover, to prevent the potential\nexcessive overfitting of DFN during the source-domain training, we further\ndesign the SAM-SVN method to constrain DFN from learning sample-specific\nknowledge. On target domains, we freeze the model and fine-tune the DFN to\nlearn target-specific knowledge specific. Extensive experiments demonstrate\nthat our method surpasses the state-of-the-art method in CD-FSS significantly\nby 2.69% and 4.68% MIoU in 1-shot and 5-shot scenarios, respectively.", "AI": {"tldr": "论文提出了一种跨域少样本分割方法（CD-FSS），通过适配器作为域信息解耦器，并设计了域特征导航器（DFN）和SAM-SVN方法，显著提升了性能。", "motivation": "解决跨域少样本分割中的两大挑战：域差距和少样本微调问题。", "method": "提出域特征导航器（DFN）作为结构化解耦器，结合SAM-SVN方法防止过拟合。", "result": "在1-shot和5-shot场景下，性能分别提升2.69%和4.68% MIoU。", "conclusion": "DFN和SAM-SVN方法有效解决了跨域少样本分割问题，性能显著优于现有方法。"}}
{"id": "2506.06316", "pdf": "https://arxiv.org/pdf/2506.06316", "abs": "https://arxiv.org/abs/2506.06316", "authors": ["Haoyang Feng", "Yanjun Dai", "Yuan Gao"], "title": "A Reinforcement-Learning-Enhanced LLM Framework for Automated A/B Testing in Personalized Marketing", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "For personalized marketing, a new challenge of how to effectively algorithm\nthe A/B testing to maximize user response is urgently to be overcome. In this\npaper, we present a new approach, the RL-LLM-AB test framework, for using\nreinforcement learning strategy optimization combined with LLM to automate and\npersonalize A/B tests. The RL-LLM-AB test is built upon the pre-trained\ninstruction-tuned language model. It first generates A/B versions of candidate\ncontent variants using a Prompt-Conditioned Generator, and then dynamically\nembeds and fuses the user portrait and the context of the current query with\nthe multi-modal perception module to constitute the current interaction state.\nThe content version is then selected in real-time through the policy\noptimization module with an Actor-Critic structure, and long-term revenue is\nestimated according to real-time feedback (such as click-through rate and\nconversion rate). Furthermore, a Memory-Augmented Reward Estimator is embedded\ninto the framework to capture long-term user preference drift, which helps to\ngeneralize policy across multiple users and content contexts. Numerical results\ndemonstrate the superiority of our proposed RL-LLM-ABTest over existing A/B\ntesting methods, including classical A/B testing, Contextual Bandits, and\nbenchmark reinforcement learning approaches on real-world marketing data.", "AI": {"tldr": "提出了一种结合强化学习和LLM的RL-LLM-AB测试框架，用于自动化个性化A/B测试，优于传统方法。", "motivation": "解决个性化营销中如何高效算法化A/B测试以最大化用户响应的问题。", "method": "基于预训练语言模型生成候选内容变体，动态融合用户画像和上下文，通过Actor-Critic结构实时选择内容版本，并利用记忆增强奖励估计器捕捉长期偏好。", "result": "在真实营销数据上优于传统A/B测试、上下文赌博机和基准强化学习方法。", "conclusion": "RL-LLM-ABTest框架有效提升了个性化A/B测试的性能和长期收益。"}}
{"id": "2506.07463", "pdf": "https://arxiv.org/pdf/2506.07463", "abs": "https://arxiv.org/abs/2506.07463", "authors": ["Guang Liu", "Liangdong Wang", "Jijie Li", "Yang Yu", "Yao Xu", "Jiabei Chen", "Yu Bai", "Feng Liao", "Yonghua Lin"], "title": "CCI4.0: A Bilingual Pretraining Dataset for Enhancing Reasoning in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We introduce CCI4.0, a large-scale bilingual pre-training dataset engineered\nfor superior data quality and diverse human-like reasoning trajectory. CCI4.0\noccupies roughly $35$ TB of disk space and comprises two sub-datasets:\nCCI4.0-M2-Base and CCI4.0-M2-CoT. CCI4.0-M2-Base combines a $5.2$ TB carefully\ncurated Chinese web corpus, a $22.5$ TB English subset from Nemotron-CC, and\ndiverse sources from math, wiki, arxiv, and code. Although these data are\nmostly sourced from well-processed datasets, the quality standards of various\ndomains are dynamic and require extensive expert experience and labor to\nprocess. So, we propose a novel pipeline justifying data quality mainly based\non models through two-stage deduplication, multiclassifier quality scoring, and\ndomain-aware fluency filtering. We extract $4.5$ billion pieces of\nCoT(Chain-of-Thought) templates, named CCI4.0-M2-CoT. Differing from the\ndistillation of CoT from larger models, our proposed staged CoT extraction\nexemplifies diverse reasoning patterns and significantly decreases the\npossibility of hallucination. Empirical evaluations demonstrate that LLMs\npre-trained in CCI4.0 benefit from cleaner, more reliable training signals,\nyielding consistent improvements in downstream tasks, especially in math and\ncode reflection tasks. Our results underscore the critical role of rigorous\ndata curation and human thinking templates in advancing LLM performance,\nshedding some light on automatically processing pretraining corpora.", "AI": {"tldr": "CCI4.0是一个大规模双语预训练数据集，包含两个子数据集，通过高质量数据筛选和多样化推理模板提升LLM性能。", "motivation": "解决预训练数据质量动态变化和多样化推理模板的需求，以提升LLM在下游任务中的表现。", "method": "提出两阶段去重、多分类器质量评分和领域感知流畅性过滤的数据处理流程，并提取45亿条CoT模板。", "result": "实验表明，CCI4.0预训练的LLM在数学和代码任务中表现更优，数据质量和推理模板对性能至关重要。", "conclusion": "严格的数据筛选和多样化推理模板能显著提升LLM性能，为自动处理预训练语料提供启示。"}}
{"id": "2506.07399", "pdf": "https://arxiv.org/pdf/2506.07399", "abs": "https://arxiv.org/abs/2506.07399", "authors": ["Peiru Yang", "Jinhua Yin", "Haoran Zheng", "Xueying Bai", "Huili Wang", "Yufei Sun", "Xintian Li", "Shangguang Wang", "Yongfeng Huang", "Tao Qi"], "title": "MrM: Black-Box Membership Inference Attacks against Multimodal RAG Systems", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal retrieval-augmented generation (RAG) systems enhance large\nvision-language models by integrating cross-modal knowledge, enabling their\nincreasing adoption across real-world multimodal tasks. These knowledge\ndatabases may contain sensitive information that requires privacy protection.\nHowever, multimodal RAG systems inherently grant external users indirect access\nto such data, making them potentially vulnerable to privacy attacks,\nparticularly membership inference attacks (MIAs). % Existing MIA methods\ntargeting RAG systems predominantly focus on the textual modality, while the\nvisual modality remains relatively underexplored. To bridge this gap, we\npropose MrM, the first black-box MIA framework targeted at multimodal RAG\nsystems. It utilizes a multi-object data perturbation framework constrained by\ncounterfactual attacks, which can concurrently induce the RAG systems to\nretrieve the target data and generate information that leaks the membership\ninformation. Our method first employs an object-aware data perturbation method\nto constrain the perturbation to key semantics and ensure successful retrieval.\nBuilding on this, we design a counterfact-informed mask selection strategy to\nprioritize the most informative masked regions, aiming to eliminate the\ninterference of model self-knowledge and amplify attack efficacy. Finally, we\nperform statistical membership inference by modeling query trials to extract\nfeatures that reflect the reconstruction of masked semantics from response\npatterns. Experiments on two visual datasets and eight mainstream commercial\nvisual-language models (e.g., GPT-4o, Gemini-2) demonstrate that MrM achieves\nconsistently strong performance across both sample-level and set-level\nevaluations, and remains robust under adaptive defenses.", "AI": {"tldr": "本文提出了MrM，首个针对多模态RAG系统的黑盒成员推理攻击框架，通过多目标数据扰动和反事实攻击提升攻击效果。", "motivation": "多模态RAG系统可能泄露敏感信息，现有攻击方法主要关注文本模态，视觉模态研究不足。", "method": "采用多目标数据扰动框架和反事实攻击，结合对象感知扰动和掩码选择策略，通过统计推理提取成员信息。", "result": "在两个视觉数据集和八个主流视觉语言模型（如GPT-4o、Gemini-2）上，MrM在样本级和集合级评估中表现优异，且对自适应防御具有鲁棒性。", "conclusion": "MrM填补了多模态RAG系统隐私攻击的空白，为未来防御研究提供了重要参考。"}}
{"id": "2506.06318", "pdf": "https://arxiv.org/pdf/2506.06318", "abs": "https://arxiv.org/abs/2506.06318", "authors": ["Feiyang Pan", "Shenghe Zheng", "Chunyan Yin", "Guangbin Dou"], "title": "MoE-Gyro: Self-Supervised Over-Range Reconstruction and Denoising for MEMS Gyroscopes", "categories": ["eess.SP", "cs.AI"], "comment": null, "summary": "MEMS gyroscopes play a critical role in inertial navigation and motion\ncontrol applications but typically suffer from a fundamental trade-off between\nmeasurement range and noise performance. Existing hardware-based solutions\naimed at mitigating this issue introduce additional complexity, cost, and\nscalability challenges. Deep-learning methods primarily focus on noise\nreduction and typically require precisely aligned ground-truth signals, making\nthem difficult to deploy in practical scenarios and leaving the fundamental\ntrade-off unresolved. To address these challenges, we introduce Mixture of\nExperts for MEMS Gyroscopes (MoE-Gyro), a novel self-supervised framework\nspecifically designed for simultaneous over-range signal reconstruction and\nnoise suppression. MoE-Gyro employs two experts: an Over-Range Reconstruction\nExpert (ORE), featuring a Gaussian-Decay Attention mechanism for reconstructing\nsaturated segments; and a Denoise Expert (DE), utilizing dual-branch\ncomplementary masking combined with FFT-guided augmentation for robust noise\nreduction. A lightweight gating module dynamically routes input segments to the\nappropriate expert. Furthermore, existing evaluation lack a comprehensive\nstandard for assessing multi-dimensional signal enhancement. To bridge this\ngap, we introduce IMU Signal Enhancement Benchmark (ISEBench), an open-source\nbenchmarking platform comprising the GyroPeak-100 dataset and a unified\nevaluation of IMU signal enhancement methods. We evaluate MoE-Gyro using our\nproposed ISEBench, demonstrating that our framework significantly extends the\nmeasurable range from 450 deg/s to 1500 deg/s, reduces Bias Instability by\n98.4%, and achieves state-of-the-art performance, effectively addressing the\nlong-standing trade-off in inertial sensing.", "AI": {"tldr": "论文提出了一种名为MoE-Gyro的自监督框架，通过混合专家模型解决MEMS陀螺仪在测量范围和噪声性能之间的权衡问题，并引入ISEBench作为评估标准。", "motivation": "MEMS陀螺仪在惯性导航和运动控制中至关重要，但存在测量范围与噪声性能之间的固有权衡。现有硬件解决方案复杂且成本高，而深度学习方法依赖精确对齐的真实信号，难以实际部署。", "method": "MoE-Gyro框架包含两个专家模块：Over-Range Reconstruction Expert（ORE）用于重建饱和信号，Denoise Expert（DE）用于噪声抑制，并通过轻量级门控模块动态路由输入。", "result": "实验表明，MoE-Gyro将可测量范围从450 deg/s扩展到1500 deg/s，降低Bias Instability达98.4%，性能达到最优。", "conclusion": "MoE-Gyro有效解决了MEMS陀螺仪的固有权衡问题，并通过ISEBench提供了统一的评估标准。"}}
{"id": "2506.07479", "pdf": "https://arxiv.org/pdf/2506.07479", "abs": "https://arxiv.org/abs/2506.07479", "authors": ["Haoyuan Li Yusen Zhang", "Snigdha Chaturvedi"], "title": "Improving Fairness of Large Language Models in Multi-document Summarization", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 main", "summary": "Fairness in multi-document summarization (MDS) is crucial for providing\ncomprehensive views across documents with diverse social attribute values,\nwhich can significantly impact decision-making. For example, a summarization\nsystem that tends to overrepresent negative reviews of products can mislead\ncustomers into disregarding good products. Previous works measure fairness in\nMDS at two levels: summary-level and corpus-level. While summary-level fairness\nfocuses on individual summaries, corpus-level fairness focuses on a corpus of\nsummaries. Recent methods primarily focus on summary-level fairness. We propose\nFairPO, a preference tuning method that focuses on both summary-level and\ncorpus-level fairness in MDS. To improve summary-level fairness, we propose to\ngenerate preference pairs by perturbing document sets. To improve corpus-level\nfairness, we propose fairness-aware preference tuning by dynamically adjusting\nthe weights of preference pairs. Our experiments show that FairPO outperforms\nstrong baselines while maintaining the critical qualities of summaries. The\ncode is available at https://github.com/leehaoyuan/coverage_fairnes.", "AI": {"tldr": "FairPO是一种偏好调整方法，旨在提高多文档摘要（MDS）中的摘要级和语料库级公平性。通过扰动文档集生成偏好对以提升摘要级公平性，并通过动态调整偏好对权重以提升语料库级公平性。实验表明，FairPO在保持摘要质量的同时优于基线方法。", "motivation": "多文档摘要中的公平性对提供多样化社会属性值的全面视角至关重要，可能显著影响决策。现有方法主要关注摘要级公平性，而FairPO同时关注摘要级和语料库级公平性。", "method": "提出FairPO方法，通过扰动文档集生成偏好对以提升摘要级公平性，并通过动态调整偏好对权重以提升语料库级公平性。", "result": "实验表明，FairPO在保持摘要质量的同时优于基线方法。", "conclusion": "FairPO在多文档摘要中同时提升了摘要级和语料库级公平性，优于现有方法。"}}
{"id": "2506.07412", "pdf": "https://arxiv.org/pdf/2506.07412", "abs": "https://arxiv.org/abs/2506.07412", "authors": ["Changsheng Gao", "Wei Zhou", "Guosheng Lin", "Weisi Lin"], "title": "Compressed Feature Quality Assessment: Dataset and Baselines", "categories": ["cs.CV"], "comment": null, "summary": "The widespread deployment of large models in resource-constrained\nenvironments has underscored the need for efficient transmission of\nintermediate feature representations. In this context, feature coding, which\ncompresses features into compact bitstreams, becomes a critical component for\nscenarios involving feature transmission, storage, and reuse. However, this\ncompression process introduces inherent semantic degradation that is\nnotoriously difficult to quantify with traditional metrics. To address this,\nthis paper introduces the research problem of Compressed Feature Quality\nAssessment (CFQA), which seeks to evaluate the semantic fidelity of compressed\nfeatures. To advance CFQA research, we propose the first benchmark dataset,\ncomprising 300 original features and 12000 compressed features derived from\nthree vision tasks and four feature codecs. Task-specific performance drops are\nprovided as true semantic distortion for the evaluation of CFQA metrics. We\nassess the performance of three widely used metrics (MSE, cosine similarity,\nand Centered Kernel Alignment) in capturing semantic degradation. The results\nunderscore the representativeness of the dataset and highlight the need for\nmore refined metrics capable of addressing the nuances of semantic distortion\nin compressed features. To facilitate the ongoing development of CFQA research,\nwe release the dataset and all accompanying source code at\n\\href{https://github.com/chansongoal/Compressed-Feature-Quality-Assessment}{https://github.com/chansongoal/Compressed-Feature-Quality-Assessment}.\nThis contribution aims to advance the field and provide a foundational resource\nfor the community to explore CFQA.", "AI": {"tldr": "论文提出压缩特征质量评估（CFQA）问题，并创建首个包含300个原始特征和12000个压缩特征的基准数据集，评估三种常用指标的性能。", "motivation": "在资源受限环境中，特征编码的语义退化难以量化，需要新的评估方法。", "method": "提出CFQA问题，构建数据集，评估MSE、余弦相似度和中心核对齐三种指标。", "result": "数据集具有代表性，现有指标需改进以更好捕捉语义退化。", "conclusion": "发布数据集和代码以推动CFQA研究发展。"}}
{"id": "2506.06322", "pdf": "https://arxiv.org/pdf/2506.06322", "abs": "https://arxiv.org/abs/2506.06322", "authors": ["Polad Geidarov"], "title": "Neural networks with image recognition by pairs", "categories": ["cs.NE", "cs.AI"], "comment": null, "summary": "Neural networks based on metric recognition methods have a strictly\ndetermined architecture. Number of neurons, connections, as well as weights and\nthresholds values are calculated analytically, based on the initial conditions\nof tasks: number of recognizable classes, number of samples, metric expressions\nused. This paper discusses the possibility of transforming these networks in\norder to apply classical learning algorithms to them without using analytical\nexpressions that calculate weight values. In the received network, training is\ncarried out by recognizing images in pairs. This approach simplifies the\nlearning process and easily allows to expand the neural network by adding new\nimages to the recognition task. The advantages of these networks, including\nsuch as: 1) network architecture simplicity and transparency; 2) training\nsimplicity and reliability; 3) the possibility of using a large number of\nimages in the recognition problem using a neural network; 4) a consistent\nincrease in the number of recognizable classes without changing the previous\nvalues of weights and thresholds.", "AI": {"tldr": "本文探讨了将基于度量识别方法的神经网络转换为可应用经典学习算法的网络，简化学习过程并支持网络扩展。", "motivation": "传统基于度量识别方法的神经网络架构严格固定，权重和阈值需通过解析式计算。本文旨在探索如何通过转换网络结构，避免解析式计算，简化学习过程并支持网络扩展。", "method": "通过将网络转换为基于图像对识别的训练方式，应用经典学习算法，避免解析式计算权重和阈值。", "result": "转换后的网络具有架构简单透明、训练简便可靠、支持大量图像识别以及可扩展性强等优势。", "conclusion": "通过转换网络结构，成功实现了简化学习过程和网络扩展的目标，为神经网络的应用提供了新的可能性。"}}
{"id": "2506.07483", "pdf": "https://arxiv.org/pdf/2506.07483", "abs": "https://arxiv.org/abs/2506.07483", "authors": ["Berry Feng", "Jonas Lin", "Patrick Lau"], "title": "A Hybrid GA LLM Framework for Structured Task Optimization", "categories": ["cs.CL"], "comment": "7 pages", "summary": "GA LLM is a hybrid framework that combines Genetic Algorithms with Large\nLanguage Models to handle structured generation tasks under strict constraints.\nEach output, such as a plan or report, is treated as a gene, and evolutionary\noperations like selection, crossover, and mutation are guided by the language\nmodel to iteratively improve solutions. The language model provides domain\nknowledge and creative variation, while the genetic algorithm ensures\nstructural integrity and global optimization. GA LLM has proven effective in\ntasks such as itinerary planning, academic outlining, and business reporting,\nconsistently producing well structured and requirement satisfying results. Its\nmodular design also makes it easy to adapt to new tasks. Compared to using a\nlanguage model alone, GA LLM achieves better constraint satisfaction and higher\nquality solutions by combining the strengths of both components.", "AI": {"tldr": "GA LLM结合遗传算法和大型语言模型，通过迭代优化生成结构化输出，满足严格约束。", "motivation": "解决单一语言模型在结构化生成任务中难以满足严格约束的问题。", "method": "将输出视为基因，利用语言模型指导遗传算法的选择、交叉和变异操作。", "result": "在行程规划、学术大纲和商业报告等任务中表现优异，输出结构良好且满足需求。", "conclusion": "GA LLM通过结合两者优势，比单一语言模型更能满足约束并生成高质量解决方案。"}}
{"id": "2506.07414", "pdf": "https://arxiv.org/pdf/2506.07414", "abs": "https://arxiv.org/abs/2506.07414", "authors": ["Sheng-Kai Huang", "Jiun-Feng Chang", "Chun-Rong Huang"], "title": "DPFormer: Dynamic Prompt Transformer for Continual Learning", "categories": ["cs.CV"], "comment": null, "summary": "In continual learning, solving the catastrophic forgetting problem may make\nthe models fall into the stability-plasticity dilemma. Moreover, inter-task\nconfusion will also occur due to the lack of knowledge exchanges between\ndifferent tasks. In order to solve the aforementioned problems, we propose a\nnovel dynamic prompt transformer (DPFormer) with prompt schemes. The prompt\nschemes help the DPFormer memorize learned knowledge of previous classes and\ntasks, and keep on learning new knowledge from new classes and tasks under a\nsingle network structure with a nearly fixed number of model parameters.\nMoreover, they also provide discrepant information to represent different tasks\nto solve the inter-task confusion problem. Based on prompt schemes, a unified\nclassification module with the binary cross entropy loss, the knowledge\ndistillation loss and the auxiliary loss is proposed to train the whole model\nin an end-to-end trainable manner. Compared with state-of-the-art methods, our\nmethod achieves the best performance in the CIFAR-100, ImageNet100 and\nImageNet1K datasets under different class-incremental settings in continual\nlearning. The source code will be available at our GitHub after acceptance.", "AI": {"tldr": "论文提出了一种动态提示变换器（DPFormer）和提示方案，用于解决持续学习中的稳定性-可塑性困境和任务间混淆问题。", "motivation": "解决持续学习中的灾难性遗忘和任务间混淆问题，同时保持模型的稳定性和可塑性。", "method": "采用动态提示变换器和提示方案，结合二元交叉熵损失、知识蒸馏损失和辅助损失，以端到端方式训练模型。", "result": "在CIFAR-100、ImageNet100和ImageNet1K数据集上，该方法在不同类增量设置下表现最佳。", "conclusion": "DPFormer通过提示方案有效解决了持续学习中的关键问题，并在多个数据集上取得了最优性能。"}}
{"id": "2506.06325", "pdf": "https://arxiv.org/pdf/2506.06325", "abs": "https://arxiv.org/abs/2506.06325", "authors": ["Viorica Rozina Chifu", "Tudor Cioara", "Cristina Bianca Pop", "Ionut Anghel"], "title": "Evolutionary model for energy trading in community microgrids using Hawk-Dove strategies", "categories": ["cs.NE", "cs.AI", "cs.GT", "cs.MA"], "comment": null, "summary": "This paper proposes a decentralized model of energy cooperation between\nmicrogrids, in which decisions are made locally, at the level of the microgrid\ncommunity. Each microgrid is modeled as an autonomous agent that adopts a Hawk\nor Dove strategy, depending on the level of energy stored in the battery and\nits role in the energy trading process. The interactions between selling and\nbuying microgrids are modeled through an evolutionary algorithm. An individual\nin the algorithm population is represented as an energy trading matrix that\nencodes the amounts of energy traded between the selling and buying microgrids.\nThe population evolution is achieved by recombination and mutation operators.\nRecombination uses a specialized operator for matrix structures, and mutation\nis applied to the matrix elements according to a Gaussian distribution. The\nevaluation of an individual is made with a multi-criteria fitness function that\nconsiders the seller profit, the degree of energy stability at the community\nlevel, penalties for energy imbalance at the community level and for the\ndegradation of microgrids batteries. The method was tested on a simulated\nscenario with 100 microgrids, each with its own selling and buying thresholds,\nto reflect a realistic environment with variable storage characteristics of\nmicrogrids batteries. By applying the algorithm on this scenario, 95 out of the\n100 microgrids reached a stable energy state. This result confirms the\neffectiveness of the proposed model in achieving energy balance both at the\nindividual level, for each microgrid, and at the level of the entire community.", "AI": {"tldr": "本文提出了一种微电网间的去中心化能源合作模型，通过本地决策和进化算法实现能源交易的稳定状态。", "motivation": "解决微电网间能源交易的协调问题，实现个体和社区层面的能源平衡。", "method": "采用基于Hawk或Dove策略的自主代理模型，结合进化算法（包括重组和变异操作）优化能源交易矩阵。", "result": "在100个微电网的模拟场景中，95个达到了稳定能源状态。", "conclusion": "模型有效实现了微电网个体及社区的能源平衡，验证了其在实际环境中的可行性。"}}
{"id": "2506.07502", "pdf": "https://arxiv.org/pdf/2506.07502", "abs": "https://arxiv.org/abs/2506.07502", "authors": ["Haotian Guo", "Jing Han", "Yongfeng Tu", "Shihao Gao", "Shengfan Shen", "Wulong Xiang", "Weihao Gan", "Zixing Zhang"], "title": "DEBATE: A Dataset for Disentangling Textual Ambiguity in Mandarin Through Speech", "categories": ["cs.CL"], "comment": null, "summary": "Despite extensive research on textual and visual disambiguation,\ndisambiguation through speech (DTS) remains underexplored. This is largely due\nto the lack of high-quality datasets that pair spoken sentences with richly\nambiguous text. To address this gap, we present DEBATE, a unique public Chinese\nspeech-text dataset designed to study how speech cues and\npatterns-pronunciation, pause, stress and intonation-can help resolve textual\nambiguity and reveal a speaker's true intent. DEBATE contains 1,001 carefully\nselected ambiguous utterances, each recorded by 10 native speakers, capturing\ndiverse linguistic ambiguities and their disambiguation through speech. We\ndetail the data collection pipeline and provide rigorous quality analysis.\nAdditionally, we benchmark three state-of-the-art large speech and\naudio-language models, illustrating clear and huge performance gaps between\nmachine and human understanding of spoken intent. DEBATE represents the first\neffort of its kind and offers a foundation for building similar DTS datasets\nacross languages and cultures. The dataset and associated code are available\nat: https://github.com/SmileHnu/DEBATE.", "AI": {"tldr": "论文提出了DEBATE数据集，用于研究语音如何帮助解决文本歧义，并展示了机器与人类在理解语音意图上的性能差距。", "motivation": "语音消歧（DTS）研究不足，缺乏高质量数据集，因此作者创建了DEBATE数据集以填补这一空白。", "method": "通过收集1,001个歧义语句，每条由10位母语者录制，捕捉语音特征（如发音、停顿、重音和语调）来消歧。", "result": "数据集质量高，并展示了机器与人类在语音意图理解上的显著性能差距。", "conclusion": "DEBATE是首个此类数据集，为跨语言和文化的DTS研究奠定了基础。"}}
{"id": "2506.07431", "pdf": "https://arxiv.org/pdf/2506.07431", "abs": "https://arxiv.org/abs/2506.07431", "authors": ["Jie He", "Minglang Chen", "Minying Lu", "Bocheng Liang", "Junming Wei", "Guiyan Peng", "Jiaxi Chen", "Ying Tan"], "title": "FAMSeg: Fetal Femur and Cranial Ultrasound Segmentation Using Feature-Aware Attention and Mamba Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate ultrasound image segmentation is a prerequisite for precise\nbiometrics and accurate assessment. Relying on manual delineation introduces\nsignificant errors and is time-consuming. However, existing segmentation models\nare designed based on objects in natural scenes, making them difficult to adapt\nto ultrasound objects with high noise and high similarity. This is particularly\nevident in small object segmentation, where a pronounced jagged effect occurs.\nTherefore, this paper proposes a fetal femur and cranial ultrasound image\nsegmentation model based on feature perception and Mamba enhancement to address\nthese challenges. Specifically, a longitudinal and transverse independent\nviewpoint scanning convolution block and a feature perception module were\ndesigned to enhance the ability to capture local detail information and improve\nthe fusion of contextual information. Combined with the Mamba-optimized\nresidual structure, this design suppresses the interference of raw noise and\nenhances local multi-dimensional scanning. The system builds global information\nand local feature dependencies, and is trained with a combination of different\noptimizers to achieve the optimal solution. After extensive experimental\nvalidation, the FAMSeg network achieved the fastest loss reduction and the best\nsegmentation performance across images of varying sizes and orientations.", "AI": {"tldr": "提出了一种基于特征感知和Mamba增强的胎儿股骨和颅骨超声图像分割模型，解决了现有模型在噪声高、相似性强的超声图像中表现不佳的问题。", "motivation": "超声图像分割的准确性对生物测量和评估至关重要，但现有模型难以适应噪声高、相似性强的超声对象，尤其是小物体分割时锯齿效应明显。", "method": "设计了纵向和横向独立视角扫描卷积块及特征感知模块，结合Mamba优化的残差结构，增强局部细节捕捉和上下文信息融合，抑制原始噪声干扰。", "result": "FAMSeg网络在实验中表现出最快的损失下降和最佳的分割性能，适用于不同大小和方向的图像。", "conclusion": "该模型通过特征感知和Mamba增强，显著提升了超声图像分割的准确性和鲁棒性。"}}
{"id": "2506.07506", "pdf": "https://arxiv.org/pdf/2506.07506", "abs": "https://arxiv.org/abs/2506.07506", "authors": ["Muhammad Dehan Al Kautsar", "Lucky Susanto", "Derry Wijaya", "Fajri Koto"], "title": "What Do Indonesians Really Need from Language Technology? A Nationwide Survey", "categories": ["cs.CL"], "comment": "26 pages, 12 figures, 5 tables", "summary": "There is an emerging effort to develop NLP for Indonesias 700+ local\nlanguages, but progress remains costly due to the need for direct engagement\nwith native speakers. However, it is unclear what these language communities\ntruly need from language technology. To address this, we conduct a nationwide\nsurvey to assess the actual needs of native speakers in Indonesia. Our findings\nindicate that addressing language barriers, particularly through machine\ntranslation and information retrieval, is the most critical priority. Although\nthere is strong enthusiasm for advancements in language technology, concerns\naround privacy, bias, and the use of public data for AI training highlight the\nneed for greater transparency and clear communication to support broader AI\nadoption.", "AI": {"tldr": "印尼700多种本地语言的NLP发展需求调查显示，机器翻译和信息检索是首要需求，但需解决隐私和偏见问题。", "motivation": "了解印尼本地语言社区对语言技术的实际需求，以指导NLP发展。", "method": "通过全国性调查评估印尼母语者的需求。", "result": "语言障碍（如机器翻译和信息检索）是最关键需求，但对隐私和偏见的担忧阻碍AI应用。", "conclusion": "需提高透明度和沟通以支持AI技术的广泛采用。"}}
{"id": "2506.07436", "pdf": "https://arxiv.org/pdf/2506.07436", "abs": "https://arxiv.org/abs/2506.07436", "authors": ["Nishi Chaudhary", "S M Jamil Uddin", "Sathvik Sharath Chandra", "Anto Ovid", "Alex Albert"], "title": "Prompt to Protection: A Comparative Study of Multimodal LLMs in Construction Hazard Recognition", "categories": ["cs.CV", "cs.AI", "cs.ET"], "comment": null, "summary": "The recent emergence of multimodal large language models (LLMs) has\nintroduced new opportunities for improving visual hazard recognition on\nconstruction sites. Unlike traditional computer vision models that rely on\ndomain-specific training and extensive datasets, modern LLMs can interpret and\ndescribe complex visual scenes using simple natural language prompts. However,\ndespite growing interest in their applications, there has been limited\ninvestigation into how different LLMs perform in safety-critical visual tasks\nwithin the construction domain. To address this gap, this study conducts a\ncomparative evaluation of five state-of-the-art LLMs: Claude-3 Opus, GPT-4.5,\nGPT-4o, GPT-o3, and Gemini 2.0 Pro, to assess their ability to identify\npotential hazards from real-world construction images. Each model was tested\nunder three prompting strategies: zero-shot, few-shot, and chain-of-thought\n(CoT). Zero-shot prompting involved minimal instruction, few-shot incorporated\nbasic safety context and a hazard source mnemonic, and CoT provided\nstep-by-step reasoning examples to scaffold model thinking. Quantitative\nanalysis was performed using precision, recall, and F1-score metrics across all\nconditions. Results reveal that prompting strategy significantly influenced\nperformance, with CoT prompting consistently producing higher accuracy across\nmodels. Additionally, LLM performance varied under different conditions, with\nGPT-4.5 and GPT-o3 outperforming others in most settings. The findings also\ndemonstrate the critical role of prompt design in enhancing the accuracy and\nconsistency of multimodal LLMs for construction safety applications. This study\noffers actionable insights into the integration of prompt engineering and LLMs\nfor practical hazard recognition, contributing to the development of more\nreliable AI-assisted safety systems.", "AI": {"tldr": "该研究比较了五种多模态大语言模型（LLMs）在建筑工地视觉危险识别任务中的表现，发现提示策略（如零样本、少样本和思维链）对性能有显著影响，其中思维链提示效果最佳。", "motivation": "探索多模态LLMs在建筑安全领域的应用潜力，填补其在关键视觉任务中性能评估的研究空白。", "method": "对五种先进LLMs（Claude-3 Opus、GPT-4.5、GPT-4o、GPT-o3和Gemini 2.0 Pro）进行对比评估，采用三种提示策略（零样本、少样本、思维链）测试其危险识别能力。", "result": "思维链提示显著提升模型性能，GPT-4.5和GPT-o3在多数场景中表现最优。提示设计对模型准确性和一致性至关重要。", "conclusion": "研究为多模态LLMs在建筑安全领域的应用提供了实用见解，强调了提示工程的重要性，有助于开发更可靠的AI辅助安全系统。"}}
{"id": "2506.06332", "pdf": "https://arxiv.org/pdf/2506.06332", "abs": "https://arxiv.org/abs/2506.06332", "authors": ["Mikko Stenlund"], "title": "Introduction to Predictive Coding Networks for Machine Learning", "categories": ["cs.NE", "cs.AI", "cs.LG"], "comment": "22 pages", "summary": "Predictive coding networks (PCNs) constitute a biologically inspired\nframework for understanding hierarchical computation in the brain, and offer an\nalternative to traditional feedforward neural networks in ML. This note serves\nas a quick, onboarding introduction to PCNs for machine learning practitioners.\nWe cover the foundational network architecture, inference and learning update\nrules, and algorithmic implementation. A concrete image-classification task\n(CIFAR-10) is provided as a benchmark-smashing application, together with an\naccompanying Python notebook containing the PyTorch implementation.", "AI": {"tldr": "本文简要介绍了预测编码网络（PCNs）的基础架构、推理与学习规则，并通过CIFAR-10图像分类任务展示了其性能。", "motivation": "预测编码网络是一种生物启发的框架，为理解大脑中的分层计算提供了新视角，并可作为传统前馈神经网络的替代方案。", "method": "介绍了PCNs的网络架构、推理与学习更新规则，并提供了PyTorch实现的Python笔记本。", "result": "通过CIFAR-10图像分类任务展示了PCNs的性能，取得了突破性成果。", "conclusion": "PCNs为机器学习提供了一种生物启发的替代方案，具有实际应用的潜力。"}}
{"id": "2506.07510", "pdf": "https://arxiv.org/pdf/2506.07510", "abs": "https://arxiv.org/abs/2506.07510", "authors": ["Solee Im", "Wonjun Lee", "Jinmyeong An", "Yunsu Kim", "Jungseul Ok", "Gary Geunbae Lee"], "title": "DeRAGEC: Denoising Named Entity Candidates with Synthetic Rationale for ASR Error Correction", "categories": ["cs.CL"], "comment": "ACL2025 Findings", "summary": "We present DeRAGEC, a method for improving Named Entity (NE) correction in\nAutomatic Speech Recognition (ASR) systems. By extending the\nRetrieval-Augmented Generative Error Correction (RAGEC) framework, DeRAGEC\nemploys synthetic denoising rationales to filter out noisy NE candidates before\ncorrection. By leveraging phonetic similarity and augmented definitions, it\nrefines noisy retrieved NEs using in-context learning, requiring no additional\ntraining. Experimental results on CommonVoice and STOP datasets show\nsignificant improvements in Word Error Rate (WER) and NE hit ratio,\noutperforming baseline ASR and RAGEC methods. Specifically, we achieved a 28%\nrelative reduction in WER compared to ASR without postprocessing. Our source\ncode is publicly available at: https://github.com/solee0022/deragec", "AI": {"tldr": "DeRAGEC通过合成去噪逻辑改进ASR系统中的命名实体纠正，无需额外训练，显著降低WER并提高NE命中率。", "motivation": "提升自动语音识别（ASR）系统中命名实体（NE）纠正的准确性，解决现有方法（如RAGEC）中噪声候选实体的问题。", "method": "扩展RAGEC框架，利用合成去噪逻辑过滤噪声NE候选，结合语音相似性和增强定义，通过上下文学习优化NE。", "result": "在CommonVoice和STOP数据集上，WER相对降低28%，NE命中率显著提升，优于基线ASR和RAGEC方法。", "conclusion": "DeRAGEC是一种高效且无需训练的NE纠正方法，显著提升了ASR系统的性能。"}}
{"id": "2506.07456", "pdf": "https://arxiv.org/pdf/2506.07456", "abs": "https://arxiv.org/abs/2506.07456", "authors": ["Wei Yao", "Yunlian Sun", "Chang Liu", "Hongwen Zhang", "Jinhui Tang"], "title": "PhysiInter: Integrating Physical Mapping for High-Fidelity Human Interaction Generation", "categories": ["cs.CV"], "comment": null, "summary": "Driven by advancements in motion capture and generative artificial\nintelligence, leveraging large-scale MoCap datasets to train generative models\nfor synthesizing diverse, realistic human motions has become a promising\nresearch direction. However, existing motion-capture techniques and generative\nmodels often neglect physical constraints, leading to artifacts such as\ninterpenetration, sliding, and floating. These issues are exacerbated in\nmulti-person motion generation, where complex interactions are involved. To\naddress these limitations, we introduce physical mapping, integrated throughout\nthe human interaction generation pipeline. Specifically, motion imitation\nwithin a physics-based simulation environment is used to project target motions\ninto a physically valid space. The resulting motions are adjusted to adhere to\nreal-world physics constraints while retaining their original semantic meaning.\nThis mapping not only improves MoCap data quality but also directly informs\npost-processing of generated motions. Given the unique interactivity of\nmulti-person scenarios, we propose a tailored motion representation framework.\nMotion Consistency (MC) and Marker-based Interaction (MI) loss functions are\nintroduced to improve model performance. Experiments show our method achieves\nimpressive results in generated human motion quality, with a 3%-89% improvement\nin physical fidelity. Project page http://yw0208.github.io/physiinter", "AI": {"tldr": "论文提出了一种物理映射方法，用于提升生成人类动作的物理真实性，解决了现有技术中的穿透、滑动和漂浮等问题，并在多人交互场景中表现优异。", "motivation": "现有动作捕捉技术和生成模型常忽略物理约束，导致动作不真实，尤其在多人交互中问题更突出。", "method": "通过物理仿真环境中的动作模仿，将目标动作投影到物理有效空间，并引入运动一致性（MC）和基于标记的交互（MI）损失函数。", "result": "实验表明，该方法在生成动作的物理保真度上提升了3%-89%。", "conclusion": "物理映射显著提升了生成动作的质量，尤其在多人交互场景中效果显著。"}}
{"id": "2506.06335", "pdf": "https://arxiv.org/pdf/2506.06335", "abs": "https://arxiv.org/abs/2506.06335", "authors": ["Xuan Xu", "Fufang Wen", "Beilin Chu", "Zhibing Fu", "Qinhong Lin", "Jiaqi Liu", "Binjie Fei", "Zhongliang Yang", "Linna Zhou", "Yu Li"], "title": "FinBERT2: A Specialized Bidirectional Encoder for Bridging the Gap in Finance-Specific Deployment of Large Language Models", "categories": ["cs.IR", "cs.AI", "cs.CE", "cs.CL"], "comment": null, "summary": "In natural language processing (NLP), the focus has shifted from encoder-only\ntiny language models like BERT to decoder-only large language models(LLMs) such\nas GPT-3. However, LLMs' practical application in the financial sector has\nrevealed three limitations: (1) LLMs often perform worse than fine-tuned BERT\non discriminative tasks despite costing much higher computational resources,\nsuch as market sentiment analysis in financial reports; (2) Application on\ngenerative tasks heavily relies on retrieval augmented generation (RAG) methods\nto provide current and specialized information, with general retrievers showing\nsuboptimal performance on domain-specific retrieval tasks; (3) There are\nadditional inadequacies in other feature-based scenarios, such as topic\nmodeling. We introduce FinBERT2, a specialized bidirectional encoder pretrained\non a high-quality, financial-specific corpus of 32b tokens. This represents the\nlargest known Chinese financial pretraining corpus for models of this parameter\nsize. As a better backbone, FinBERT2 can bridge the gap in the\nfinancial-specific deployment of LLMs through the following achievements: (1)\nDiscriminative fine-tuned models (Fin-Labelers) outperform other (Fin)BERT\nvariants by 0.4%-3.3% and leading LLMs by 9.7%-12.3% on average across five\nfinancial classification tasks. (2) Contrastive fine-tuned models\n(Fin-Retrievers) outperform both open-source (e.g., +6.8\\% avg improvement over\nBGE-base-zh) and proprietary (e.g., +4.2\\% avg improvement over OpenAI's\ntext-embedding-3-large) embedders across five financial retrieval tasks; (3)\nBuilding on FinBERT2 variants, we construct the Fin-TopicModel, which enables\nsuperior clustering and topic representation for financial titles. Our work\nrevisits financial BERT models through comparative analysis with contemporary\nLLMs and offers practical insights for effectively utilizing FinBERT in the\nLLMs era.", "AI": {"tldr": "论文介绍了FinBERT2，一种针对金融领域优化的双向编码器，解决了大型语言模型（LLMs）在金融应用中的局限性，并在分类、检索和主题建模任务中表现优异。", "motivation": "LLMs在金融领域的实际应用存在三个主要问题：在判别任务中表现不如微调的BERT、生成任务依赖检索增强方法、以及在主题建模等场景中的不足。FinBERT2旨在填补这一空白。", "method": "FinBERT2是一个基于32b金融语料库预训练的双向编码器，通过微调构建了Fin-Labelers（分类）、Fin-Retrievers（检索）和Fin-TopicModel（主题建模）。", "result": "FinBERT2在金融分类任务中平均优于其他BERT变体和LLMs，检索任务中优于开源和专有嵌入模型，主题建模任务中表现优越。", "conclusion": "FinBERT2为金融领域的LLMs应用提供了更高效的解决方案，并通过对比分析展示了其在金融NLP中的实用性。"}}
{"id": "2506.07523", "pdf": "https://arxiv.org/pdf/2506.07523", "abs": "https://arxiv.org/abs/2506.07523", "authors": ["Sahar Admoni", "Ofra Amir", "Assaf Hallak", "Yftah Ziser"], "title": "Towards Large Language Models with Self-Consistent Natural Language Explanations", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) seem to offer an easy path to interpretability:\njust ask them to explain their decisions. Yet, studies show that these post-hoc\nexplanations often misrepresent the true decision process, as revealed by\nmismatches in feature importance. Despite growing evidence of this\ninconsistency, no systematic solutions have emerged, partly due to the high\ncost of estimating feature importance, which limits evaluations to small\ndatasets. To address this, we introduce the Post-hoc Self-Consistency Bank\n(PSCB) - a large-scale benchmark of decisions spanning diverse tasks and\nmodels, each paired with LLM-generated explanations and corresponding feature\nimportance scores. Analysis of PSCB reveals that self-consistency scores barely\ndiffer between correct and incorrect predictions. We also show that the\nstandard metric fails to meaningfully distinguish between explanations. To\novercome this limitation, we propose an alternative metric that more\neffectively captures variation in explanation quality. We use it to fine-tune\nLLMs via Direct Preference Optimization (DPO), leading to significantly better\nalignment between explanations and decision-relevant features, even under\ndomain shift. Our findings point to a scalable path toward more trustworthy,\nself-consistent LLMs.", "AI": {"tldr": "论文提出了一种新方法（PSCB）来评估LLM生成解释的自洽性，并通过新指标和DPO优化提升解释质量。", "motivation": "现有LLM生成的事后解释常与真实决策过程不一致，缺乏系统性解决方案。", "method": "引入PSCB基准，分析自洽性并提出新指标，使用DPO优化LLM。", "result": "自洽性评分在正确与错误预测间差异小，新指标更有效，优化后解释质量显著提升。", "conclusion": "PSCB和DPO为提升LLM解释自洽性提供了可扩展路径。"}}
{"id": "2506.07460", "pdf": "https://arxiv.org/pdf/2506.07460", "abs": "https://arxiv.org/abs/2506.07460", "authors": ["Taeryung Lee", "Hyeongjin Nam", "Gyeongsik Moon", "Kyoung Mu Lee"], "title": "GLOS: Sign Language Generation with Temporally Aligned Gloss-Level Conditioning", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Sign language generation (SLG), or text-to-sign generation, bridges the gap\nbetween signers and non-signers. Despite recent progress in SLG, existing\nmethods still often suffer from incorrect lexical ordering and low semantic\naccuracy. This is primarily due to sentence-level condition, which encodes the\nentire sentence of the input text into a single feature vector as a condition\nfor SLG. This approach fails to capture the temporal structure of sign language\nand lacks the granularity of word-level semantics, often leading to disordered\nsign sequences and ambiguous motions. To overcome these limitations, we propose\nGLOS, a sign language generation framework with temporally aligned gloss-level\nconditioning. First, we employ gloss-level conditions, which we define as\nsequences of gloss embeddings temporally aligned with the motion sequence. This\nenables the model to access both the temporal structure of sign language and\nword-level semantics at each timestep. As a result, this allows for\nfine-grained control of signs and better preservation of lexical order. Second,\nwe introduce a condition fusion module, temporal alignment conditioning (TAC),\nto efficiently deliver the word-level semantic and temporal structure provided\nby the gloss-level condition to the corresponding motion timesteps. Our method,\nwhich is composed of gloss-level conditions and TAC, generates signs with\ncorrect lexical order and high semantic accuracy, outperforming prior methods\non CSL-Daily and Phoenix-2014T.", "AI": {"tldr": "论文提出GLOS框架，通过时间对齐的gloss级条件改进手语生成，解决了现有方法中词序错误和语义准确性低的问题。", "motivation": "现有手语生成方法因句子级条件导致词序混乱和语义模糊，无法捕捉手语的时间结构和词级语义。", "method": "采用gloss级条件（时间对齐的gloss嵌入序列）和条件融合模块TAC，实现细粒度控制和词序保持。", "result": "在CSL-Daily和Phoenix-2014T数据集上表现优于现有方法，生成的手语词序正确且语义准确。", "conclusion": "GLOS框架通过时间对齐的gloss级条件显著提升了手语生成的质量。"}}
{"id": "2506.06339", "pdf": "https://arxiv.org/pdf/2506.06339", "abs": "https://arxiv.org/abs/2506.06339", "authors": ["Jumana Alsubhi", "Mohammad D. Alahmadi", "Ahmed Alhusayni", "Ibrahim Aldailami", "Israa Hamdine", "Ahmad Shabana", "Yazeed Iskandar", "Suhayb Khayyat"], "title": "Optimizing RAG Pipelines for Arabic: A Systematic Analysis of Core Components", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has emerged as a powerful architecture\nfor combining the precision of retrieval systems with the fluency of large\nlanguage models. While several studies have investigated RAG pipelines for\nhigh-resource languages, the optimization of RAG components for Arabic remains\nunderexplored. This study presents a comprehensive empirical evaluation of\nstate-of-the-art RAG components-including chunking strategies, embedding\nmodels, rerankers, and language models-across a diverse set of Arabic datasets.\nUsing the RAGAS framework, we systematically compare performance across four\ncore metrics: context precision, context recall, answer faithfulness, and\nanswer relevancy. Our experiments demonstrate that sentence-aware chunking\noutperforms all other segmentation methods, while BGE-M3 and\nMultilingual-E5-large emerge as the most effective embedding models. The\ninclusion of a reranker (bge-reranker-v2-m3) significantly boosts faithfulness\nin complex datasets, and Aya-8B surpasses StableLM in generation quality. These\nfindings provide critical insights for building high-quality Arabic RAG\npipelines and offer practical guidelines for selecting optimal components\nacross different document types.", "AI": {"tldr": "本文通过RAGAS框架对阿拉伯语RAG组件进行了全面评估，发现句子感知分块、BGE-M3和Multilingual-E5-large嵌入模型表现最佳，且重排序器显著提升了复杂数据集中的忠实度。", "motivation": "尽管RAG在高资源语言中已有研究，但阿拉伯语的RAG组件优化仍待探索。", "method": "使用RAGAS框架，对分块策略、嵌入模型、重排序器和语言模型进行了系统评估。", "result": "句子感知分块效果最佳，BGE-M3和Multilingual-E5-large嵌入模型表现最优，重排序器提升了忠实度，Aya-8B生成质量优于StableLM。", "conclusion": "研究为构建高质量阿拉伯语RAG管道提供了关键见解和实用指南。"}}
{"id": "2506.07541", "pdf": "https://arxiv.org/pdf/2506.07541", "abs": "https://arxiv.org/abs/2506.07541", "authors": ["Sangwhan Moon", "Tatsuya Hiraoka", "Naoaki Okazaki"], "title": "Bit-level BPE: Below the byte boundary", "categories": ["cs.CL"], "comment": null, "summary": "Byte-level fallbacks for subword tokenization have become a common practice\nin large language models. In particular, it has been demonstrated to be\nincredibly effective as a pragmatic solution for preventing OOV, especially in\nthe context of larger models. However, breaking a character down to individual\nbytes significantly increases the sequence length for long-tail tokens in\nlanguages such as Chinese, Japanese, and Korean (CJK) and other\ncharacter-diverse contexts such as emoji. The increased sequence length results\nin longer computation during both training and inference. In this work, we\npropose a simple compression technique that reduces the sequence length\nlosslessly.", "AI": {"tldr": "提出了一种无损压缩技术，减少字节级回退分词在长序列中的计算开销。", "motivation": "字节级回退分词虽能有效防止OOV，但在处理CJK等字符丰富的语言时会导致序列长度增加，影响计算效率。", "method": "提出了一种简单的无损压缩技术。", "result": "减少了序列长度，从而降低了训练和推理时的计算时间。", "conclusion": "该方法为处理字符丰富的语言提供了一种高效的解决方案。"}}
{"id": "2506.07464", "pdf": "https://arxiv.org/pdf/2506.07464", "abs": "https://arxiv.org/abs/2506.07464", "authors": ["Jinyoung Park", "Jeehye Na", "Jinyoung Kim", "Hyunwoo J. Kim"], "title": "DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware Regressive GRPO", "categories": ["cs.CV", "cs.AI"], "comment": "Work in progress", "summary": "Recent works have demonstrated the effectiveness of reinforcement learning\n(RL)-based post-training in enhancing the reasoning capabilities of large\nlanguage models (LLMs). In particular, Group Relative Policy Optimization\n(GRPO) has shown impressive success by employing a PPO-style reinforcement\nalgorithm with group-based normalized rewards. However, the application of GRPO\nto Video Large Language Models (Video LLMs) has been less studied. In this\npaper, we explore GRPO for video LLMs and identify two primary issues that\nimpede its effective learning: (1) reliance on safeguards, and (2) the\nvanishing advantage problem. To mitigate these challenges, we propose\nDeepVideo-R1, a video large language model trained with our proposed Reg-GRPO\n(Regressive GRPO) and difficulty-aware data augmentation strategy. Reg-GRPO\nreformulates the GRPO objective as a regression task, directly predicting the\nadvantage in GRPO. This design eliminates the need for safeguards like clipping\nand min functions, thereby facilitating more direct policy guidance by aligning\nthe model with the advantage values. We also design the difficulty-aware data\naugmentation strategy that dynamically augments training samples at solvable\ndifficulty levels, fostering diverse and informative reward signals. Our\ncomprehensive experiments show that DeepVideo-R1 significantly improves video\nreasoning performance across multiple video reasoning benchmarks.", "AI": {"tldr": "论文探讨了将GRPO应用于视频大语言模型（Video LLMs）时的问题，并提出Reg-GRPO和难度感知数据增强策略以解决这些问题，显著提升了视频推理性能。", "motivation": "GRPO在增强大语言模型推理能力方面表现出色，但其在视频大语言模型中的应用研究较少，且存在依赖安全措施和优势消失问题。", "method": "提出Reg-GRPO，将GRPO目标重构为回归任务，直接预测优势值；同时设计难度感知数据增强策略，动态生成训练样本。", "result": "实验表明，DeepVideo-R1在多个视频推理基准测试中显著提升了性能。", "conclusion": "Reg-GRPO和难度感知数据增强策略有效解决了GRPO在视频大语言模型中的应用问题，提升了推理能力。"}}
{"id": "2506.06340", "pdf": "https://arxiv.org/pdf/2506.06340", "abs": "https://arxiv.org/abs/2506.06340", "authors": ["Wu Hao Ran", "Xi Xi", "Furong Li", "Jingyi Lu", "Jian Jiang", "Hui Huang", "Yuzhuan Zhang", "Shi Li"], "title": "Structured Semantics from Unstructured Notes: Language Model Approaches to EHR-Based Decision Support", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "The advent of large language models (LLMs) has opened new avenues for\nanalyzing complex, unstructured data, particularly within the medical domain.\nElectronic Health Records (EHRs) contain a wealth of information in various\nformats, including free text clinical notes, structured lab results, and\ndiagnostic codes. This paper explores the application of advanced language\nmodels to leverage these diverse data sources for improved clinical decision\nsupport. We will discuss how text-based features, often overlooked in\ntraditional high dimensional EHR analysis, can provide semantically rich\nrepresentations and aid in harmonizing data across different institutions.\nFurthermore, we delve into the challenges and opportunities of incorporating\nmedical codes and ensuring the generalizability and fairness of AI models in\nhealthcare.", "AI": {"tldr": "论文探讨了大型语言模型（LLMs）在医疗领域电子健康记录（EHRs）中的应用，重点关注如何利用文本特征提升临床决策支持，并讨论了数据通用性和公平性的挑战。", "motivation": "电子健康记录（EHRs）包含丰富但格式多样的数据，传统方法常忽略文本特征。研究旨在利用LLMs挖掘这些数据，提升临床决策支持。", "method": "应用先进语言模型分析EHRs中的文本特征，结合结构化数据（如实验室结果和诊断代码），生成语义丰富的表示。", "result": "研究表明，文本特征能提供更丰富的语义信息，并有助于跨机构数据协调。", "conclusion": "LLMs在医疗EHR分析中具有潜力，但需解决通用性和公平性挑战。"}}
{"id": "2506.07557", "pdf": "https://arxiv.org/pdf/2506.07557", "abs": "https://arxiv.org/abs/2506.07557", "authors": ["Mengsong Wu", "Di Zhang", "Yuqiang Li", "Dongzhan Zhou", "Wenliang Chen"], "title": "SELT: Self-Evaluation Tree Search for LLMs with Task Decomposition", "categories": ["cs.CL", "cs.AI"], "comment": "11 pages, 5 figures", "summary": "While Large Language Models (LLMs) have achieved remarkable success in a wide\nrange of applications, their performance often degrades in complex reasoning\ntasks. In this work, we introduce SELT (Self-Evaluation LLM Tree Search), a\nnovel framework that leverages a modified Monte Carlo Tree Search (MCTS) to\nenhance LLM reasoning without relying on external reward models. By redefining\nthe Upper Confidence Bound scoring to align with intrinsic self-evaluation\ncapabilities of LLMs and decomposing the inference process into atomic subtasks\naugmented with semantic clustering at each node, SELT effectively balances\nexploration and exploitation, reduces redundant reasoning paths, and mitigates\nhallucination. We validate our approach on challenging benchmarks, including\nthe knowledge-based MMLU and the Tool Learning dataset Seal-Tools, where SELT\nachieves significant improvements in answer accuracy and reasoning robustness\ncompared to baseline methods. Notably, our framework operates without\ntask-specific fine-tuning, demonstrating strong generalizability across diverse\nreasoning tasks. Relevant results and code are available at\nhttps://github.com/fairyshine/SELT .", "AI": {"tldr": "SELT是一种基于改进的蒙特卡洛树搜索（MCTS）的框架，旨在提升大型语言模型（LLMs）在复杂推理任务中的表现，无需依赖外部奖励模型。", "motivation": "LLMs在复杂推理任务中表现不佳，需要一种无需外部奖励模型的方法来提升其推理能力。", "method": "SELT通过重新定义上置信界评分，利用LLMs的自我评估能力，并将推理过程分解为原子子任务，结合语义聚类，以平衡探索与利用，减少冗余路径和幻觉。", "result": "在MMLU和Seal-Tools等基准测试中，SELT显著提升了答案准确性和推理鲁棒性，且无需任务特定微调。", "conclusion": "SELT展示了在多样化推理任务中的强泛化能力，为LLMs的推理能力提升提供了有效解决方案。"}}
{"id": "2506.07471", "pdf": "https://arxiv.org/pdf/2506.07471", "abs": "https://arxiv.org/abs/2506.07471", "authors": ["CH Cho", "WJ Moon", "W Jun", "MS Jung", "JP Heo"], "title": "Ambiguity-Restrained Text-Video Representation Learning for Partially Relevant Video Retrieval", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to AAAI 2025", "summary": "Partially Relevant Video Retrieval~(PRVR) aims to retrieve a video where a\nspecific segment is relevant to a given text query. Typical training processes\nof PRVR assume a one-to-one relationship where each text query is relevant to\nonly one video. However, we point out the inherent ambiguity between text and\nvideo content based on their conceptual scope and propose a framework that\nincorporates this ambiguity into the model learning process. Specifically, we\npropose Ambiguity-Restrained representation Learning~(ARL) to address ambiguous\ntext-video pairs. Initially, ARL detects ambiguous pairs based on two criteria:\nuncertainty and similarity. Uncertainty represents whether instances include\ncommonly shared context across the dataset, while similarity indicates\npair-wise semantic overlap. Then, with the detected ambiguous pairs, our ARL\nhierarchically learns the semantic relationship via multi-positive contrastive\nlearning and dual triplet margin loss. Additionally, we delve into fine-grained\nrelationships within the video instances. Unlike typical training at the\ntext-video level, where pairwise information is provided, we address the\ninherent ambiguity within frames of the same untrimmed video, which often\ncontains multiple contexts. This allows us to further enhance learning at the\ntext-frame level. Lastly, we propose cross-model ambiguity detection to\nmitigate the error propagation that occurs when a single model is employed to\ndetect ambiguous pairs for its training. With all components combined, our\nproposed method demonstrates its effectiveness in PRVR.", "AI": {"tldr": "论文提出了一种名为ARL的框架，通过多正对比学习和双三元组边际损失处理文本-视频对的模糊性，提升了部分相关视频检索的性能。", "motivation": "传统PRVR训练假设文本查询与视频为一对一关系，忽略了文本与视频内容之间的模糊性。", "method": "提出ARL框架，基于不确定性和相似性检测模糊对，并通过多正对比学习和双三元组边际损失进行层次化学习。", "result": "ARL框架在PRVR任务中表现出色。", "conclusion": "通过处理模糊性和多层次学习，ARL显著提升了PRVR的效果。"}}
{"id": "2506.06341", "pdf": "https://arxiv.org/pdf/2506.06341", "abs": "https://arxiv.org/abs/2506.06341", "authors": ["Xinghe Cheng", "Xufang Zhou", "Liangda Fang", "Chaobo He", "Yuyu Zhou", "Weiqi Luo", "Zhiguo Gong", "Quanlong Guan"], "title": "NR4DER: Neural Re-ranking for Diversified Exercise Recommendation", "categories": ["cs.IR", "cs.AI", "cs.CY"], "comment": "accepted for presentation at the SIGIR 2025 Full Papers track", "summary": "With the widespread adoption of online education platforms, an increasing\nnumber of students are gaining new knowledge through Massive Open Online\nCourses (MOOCs). Exercise recommendation have made strides toward improving\nstudent learning outcomes. However, existing methods not only struggle with\nhigh dropout rates but also fail to match the diverse learning pace of\nstudents. They frequently face difficulties in adjusting to inactive students'\nlearning patterns and in accommodating individualized learning paces, resulting\nin limited accuracy and diversity in recommendations. To tackle these\nchallenges, we propose Neural Re-ranking for Diversified Exercise\nRecommendation (in short, NR4DER). NR4DER first leverages the mLSTM model to\nimprove the effectiveness of the exercise filter module. It then employs a\nsequence enhancement method to enhance the representation of inactive students,\naccurately matches students with exercises of appropriate difficulty. Finally,\nit utilizes neural re-ranking to generate diverse recommendation lists based on\nindividual students' learning histories. Extensive experimental results\nindicate that NR4DER significantly outperforms existing methods across multiple\nreal-world datasets and effectively caters to the diverse learning pace of\nstudents.", "AI": {"tldr": "论文提出NR4DER方法，通过mLSTM模型和神经重排序技术，解决在线教育平台中练习推荐的高辍学率和学习节奏多样性问题。", "motivation": "现有练习推荐方法难以应对高辍学率和学生多样化的学习节奏，导致推荐准确性和多样性受限。", "method": "NR4DER结合mLSTM模型优化练习筛选模块，通过序列增强方法提升非活跃学生的表示，并利用神经重排序生成多样化推荐列表。", "result": "实验表明，NR4DER在多个真实数据集上显著优于现有方法，并能适应学生多样化的学习节奏。", "conclusion": "NR4DER有效解决了现有推荐方法的问题，提升了推荐的准确性和多样性。"}}
{"id": "2506.07583", "pdf": "https://arxiv.org/pdf/2506.07583", "abs": "https://arxiv.org/abs/2506.07583", "authors": ["Ramakrishna Appicharla", "Baban Gain", "Santanu Pal", "Asif Ekbal"], "title": "Beyond the Sentence: A Survey on Context-Aware Machine Translation with Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite the popularity of the large language models (LLMs), their application\nto machine translation is relatively underexplored, especially in context-aware\nsettings. This work presents a literature review of context-aware translation\nwith LLMs. The existing works utilise prompting and fine-tuning approaches,\nwith few focusing on automatic post-editing and creating translation agents for\ncontext-aware machine translation. We observed that the commercial LLMs (such\nas ChatGPT and Tower LLM) achieved better results than the open-source LLMs\n(such as Llama and Bloom LLMs), and prompt-based approaches serve as good\nbaselines to assess the quality of translations. Finally, we present some\ninteresting future directions to explore.", "AI": {"tldr": "本文综述了大型语言模型（LLMs）在上下文感知机器翻译中的应用，发现商业LLMs表现优于开源LLMs，并提出了未来研究方向。", "motivation": "尽管LLMs广受欢迎，但其在上下文感知机器翻译中的应用尚未充分探索，本文旨在填补这一空白。", "method": "通过文献综述，分析了提示和微调方法，并探讨了自动后编辑和翻译代理的应用。", "result": "商业LLMs（如ChatGPT）表现优于开源LLMs（如Llama），提示方法可作为翻译质量评估的基准。", "conclusion": "未来研究可进一步探索上下文感知机器翻译的潜力，尤其是在自动后编辑和翻译代理领域。"}}
{"id": "2506.07484", "pdf": "https://arxiv.org/pdf/2506.07484", "abs": "https://arxiv.org/abs/2506.07484", "authors": ["Dasol Hong", "Wooju Lee", "Hyun Myung"], "title": "CoCoA-Mix: Confusion-and-Confidence-Aware Mixture Model for Context Optimization", "categories": ["cs.CV", "cs.AI", "cs.LG", "I.2.6; I.5.2"], "comment": "8 pages, 5 figures; accepted at ICML 2025", "summary": "Prompt tuning, which adapts vision-language models by freezing model\nparameters and optimizing only the prompt, has proven effective for\ntask-specific adaptations. The core challenge in prompt tuning is improving\nspecialization for a specific task and generalization for unseen domains.\nHowever, frozen encoders often produce misaligned features, leading to\nconfusion between classes and limiting specialization. To overcome this issue,\nwe propose a confusion-aware loss (CoA-loss) that improves specialization by\nrefining the decision boundaries between confusing classes. Additionally, we\nmathematically demonstrate that a mixture model can enhance generalization\nwithout compromising specialization. This is achieved using confidence-aware\nweights (CoA-weights), which adjust the weights of each prediction in the\nmixture model based on its confidence within the class domains. Extensive\nexperiments show that CoCoA-Mix, a mixture model with CoA-loss and CoA-weights,\noutperforms state-of-the-art methods by enhancing specialization and\ngeneralization. Our code is publicly available at\nhttps://github.com/url-kaist/CoCoA-Mix.", "AI": {"tldr": "论文提出了一种混淆感知损失（CoA-loss）和置信感知权重（CoA-weights）的方法，通过改进决策边界和混合模型，提升了视觉语言模型在任务适应中的专业化和泛化能力。", "motivation": "解决提示调优中因冻结编码器导致的特征不对齐问题，提升模型在特定任务中的专业化和对未见领域的泛化能力。", "method": "提出混淆感知损失（CoA-loss）优化决策边界，结合置信感知权重（CoA-weights）构建混合模型，以平衡专业化和泛化。", "result": "实验表明，CoCoA-Mix方法在专业化和泛化方面优于现有技术。", "conclusion": "通过CoA-loss和CoA-weights的结合，显著提升了模型性能，代码已开源。"}}
{"id": "2506.07597", "pdf": "https://arxiv.org/pdf/2506.07597", "abs": "https://arxiv.org/abs/2506.07597", "authors": ["Oscar Sainz", "Naiara Perez", "Julen Etxaniz", "Joseba Fernandez de Landa", "Itziar Aldabe", "Iker García-Ferrero", "Aimar Zabala", "Ekhi Azurmendi", "German Rigau", "Eneko Agirre", "Mikel Artetxe", "Aitor Soroa"], "title": "Instructing Large Language Models for Low-Resource Languages: A Systematic Study for Basque", "categories": ["cs.CL"], "comment": "Under review", "summary": "Instructing language models with user intent requires large instruction\ndatasets, which are only available for a limited set of languages. In this\npaper, we explore alternatives to conventional instruction adaptation pipelines\nin low-resource scenarios. We assume a realistic scenario for low-resource\nlanguages, where only the following are available: corpora in the target\nlanguage, existing open-weight multilingual base and instructed backbone LLMs,\nand synthetically generated instructions sampled from the instructed backbone.\nWe present a comprehensive set of experiments for Basque that systematically\nstudy different combinations of these components evaluated on benchmarks and\nhuman preferences from 1,680 participants. Our conclusions show that target\nlanguage corpora are essential, with synthetic instructions yielding robust\nmodels, and, most importantly, that using as backbone an instruction-tuned\nmodel outperforms using a base non-instructed model, and improved results when\nscaling up. Using Llama 3.1 instruct 70B as backbone our model comes near\nfrontier models of much larger sizes for Basque, without using any Basque data\napart from the 1.2B word corpora. We release code, models, instruction\ndatasets, and human preferences to support full reproducibility in future\nresearch on low-resource language adaptation.", "AI": {"tldr": "论文探讨了在低资源语言场景下替代传统指令适应流程的方法，通过合成指令和目标语言语料库训练模型，实验表明指令调优模型优于基础模型，且规模扩大效果更好。", "motivation": "解决低资源语言因缺乏大规模指令数据集而难以训练语言模型的问题。", "method": "利用目标语言语料库、多语言基础模型和合成指令，系统实验不同组合，评估基准和人类偏好。", "result": "使用指令调优模型作为骨干效果最佳，规模扩大进一步提升性能，模型接近更大规模的前沿模型。", "conclusion": "目标语言语料库和合成指令是关键，指令调优模型优于基础模型，规模扩大效果显著。"}}
{"id": "2506.07489", "pdf": "https://arxiv.org/pdf/2506.07489", "abs": "https://arxiv.org/abs/2506.07489", "authors": ["Yahao Shi", "Yang Liu", "Yanmin Wu", "Xing Liu", "Chen Zhao", "Jie Luo", "Bin Zhou"], "title": "Drive Any Mesh: 4D Latent Diffusion for Mesh Deformation from Video", "categories": ["cs.CV"], "comment": "technical report", "summary": "We propose DriveAnyMesh, a method for driving mesh guided by monocular video.\nCurrent 4D generation techniques encounter challenges with modern rendering\nengines. Implicit methods have low rendering efficiency and are unfriendly to\nrasterization-based engines, while skeletal methods demand significant manual\neffort and lack cross-category generalization. Animating existing 3D assets,\ninstead of creating 4D assets from scratch, demands a deep understanding of the\ninput's 3D structure. To tackle these challenges, we present a 4D diffusion\nmodel that denoises sequences of latent sets, which are then decoded to produce\nmesh animations from point cloud trajectory sequences. These latent sets\nleverage a transformer-based variational autoencoder, simultaneously capturing\n3D shape and motion information. By employing a spatiotemporal,\ntransformer-based diffusion model, information is exchanged across multiple\nlatent frames, enhancing the efficiency and generalization of the generated\nresults. Our experimental results demonstrate that DriveAnyMesh can rapidly\nproduce high-quality animations for complex motions and is compatible with\nmodern rendering engines. This method holds potential for applications in both\nthe gaming and filming industries.", "AI": {"tldr": "DriveAnyMesh是一种通过单目视频驱动网格的方法，解决了当前4D生成技术在渲染引擎中的效率与兼容性问题。", "motivation": "现有4D生成技术存在渲染效率低、手动工作量大且缺乏跨类别泛化能力的问题，而动画化现有3D资产需要对输入结构有深入理解。", "method": "提出了一种4D扩散模型，通过去噪潜在集序列并解码为网格动画，利用基于变压器的变分自编码器捕捉3D形状和运动信息。", "result": "实验表明，DriveAnyMesh能快速生成高质量复杂运动动画，且与现代渲染引擎兼容。", "conclusion": "该方法在游戏和电影行业具有应用潜力。"}}
{"id": "2506.06344", "pdf": "https://arxiv.org/pdf/2506.06344", "abs": "https://arxiv.org/abs/2506.06344", "authors": ["Alex Pierron", "Michel Barbeau", "Luca De Cicco", "Jose Rubio-Hernan", "Joaquin Garcia-Alfaro"], "title": "A Reinforcement Learning Approach for RIS-aided Fair Communications", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": "7 pages, 6 figures, 1 table, 16 references", "summary": "Reconfigurable Intelligent Surfaces (RISs) are composed of physical elements\nthat can dynamically alter electromagnetic wave properties to enhance\nbeamforming and leading to improvements in areas with low coverage properties.\nThey have the potential to be combined with Reinforcement Learning (RL)\ntechniques to achieve network performance and energy efficiency via\noptimization techniques. In addition to performance and energy improvements, it\nis also crucial to consider the concept of fair communications. RISs must\nensure that User Equipment (UE) units receive their signals with adequate\nstrength, without other UE being deprived of service due to insufficient power.\nIn this paper, we address such a problem. We explore the fairness properties of\nprevious work and propose a novel method that aims at obtaining an efficient\nand fair duplex RIS-RL system for multiple legitimate UE units. We report and\ndiscuss our experimental work and simulation results. We also release our code\nand datasets to foster further research in the topic.", "AI": {"tldr": "论文提出了一种结合可重构智能表面（RIS）和强化学习（RL）的方法，旨在实现高效且公平的多用户通信系统。", "motivation": "解决现有RIS-RL系统中用户间信号强度分配不公平的问题，确保所有用户设备（UE）都能获得足够的信号强度。", "method": "提出了一种新颖的RIS-RL优化方法，通过实验和仿真验证其公平性和效率。", "result": "实验结果表明，该方法在保证网络性能和能源效率的同时，显著提升了用户间的公平性。", "conclusion": "论文提出的方法为RIS-RL系统在多用户环境中的公平通信提供了有效解决方案，并公开了代码和数据集以促进进一步研究。"}}
{"id": "2506.07606", "pdf": "https://arxiv.org/pdf/2506.07606", "abs": "https://arxiv.org/abs/2506.07606", "authors": ["Peyman Rostami", "Vahid Rahimzadeh", "Ali Adibi", "Azadeh Shakery"], "title": "PolitiSky24: U.S. Political Bluesky Dataset with User Stance Labels", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.SI", "I.2.7"], "comment": "The dataset is available at https://doi.org/10.5281/zenodo.15616911", "summary": "Stance detection identifies the viewpoint expressed in text toward a specific\ntarget, such as a political figure. While previous datasets have focused\nprimarily on tweet-level stances from established platforms, user-level stance\nresources, especially on emerging platforms like Bluesky remain scarce.\nUser-level stance detection provides a more holistic view by considering a\nuser's complete posting history rather than isolated posts. We present the\nfirst stance detection dataset for the 2024 U.S. presidential election,\ncollected from Bluesky and centered on Kamala Harris and Donald Trump. The\ndataset comprises 16,044 user-target stance pairs enriched with engagement\nmetadata, interaction graphs, and user posting histories. PolitiSky24 was\ncreated using a carefully evaluated pipeline combining advanced information\nretrieval and large language models, which generates stance labels with\nsupporting rationales and text spans for transparency. The labeling approach\nachieves 81\\% accuracy with scalable LLMs. This resource addresses gaps in\npolitical stance analysis through its timeliness, open-data nature, and\nuser-level perspective. The dataset is available at\nhttps://doi.org/10.5281/zenodo.15616911", "AI": {"tldr": "论文提出了首个针对2024年美国总统选举的立场检测数据集PolitiSky24，收集自Bluesky平台，聚焦于Kamala Harris和Donald Trump。数据集包含16,044个用户-目标立场对，并附有互动元数据、交互图和用户发帖历史。", "motivation": "现有立场检测数据集多关注推文级别，而用户级别的立场资源稀缺，尤其是在新兴平台如Bluesky上。用户级别立场检测能通过完整发帖历史提供更全面的视角。", "method": "采用结合高级信息检索和大语言模型的标注流程，生成立场标签及支持理由和文本片段，标注准确率达81%。", "result": "PolitiSky24填补了政治立场分析的空白，具有时效性、开放数据特性和用户级别视角。", "conclusion": "该数据集为政治立场分析提供了新资源，支持透明化标注和用户级别视角。"}}
{"id": "2506.07491", "pdf": "https://arxiv.org/pdf/2506.07491", "abs": "https://arxiv.org/abs/2506.07491", "authors": ["Yongsen Mao", "Junhao Zhong", "Chuan Fang", "Jia Zheng", "Rui Tang", "Hao Zhu", "Ping Tan", "Zihan Zhou"], "title": "SpatialLM: Training Large Language Models for Structured Indoor Modeling", "categories": ["cs.CV"], "comment": null, "summary": "SpatialLM is a large language model designed to process 3D point cloud data\nand generate structured 3D scene understanding outputs. These outputs include\narchitectural elements like walls, doors, windows, and oriented object boxes\nwith their semantic categories. Unlike previous methods which exploit\ntask-specific network designs, our model adheres to the standard multimodal LLM\narchitecture and is fine-tuned directly from open-source LLMs.\n  To train SpatialLM, we collect a large-scale, high-quality synthetic dataset\nconsisting of the point clouds of 12,328 indoor scenes (54,778 rooms) with\nground-truth 3D annotations, and conduct a careful study on various modeling\nand training decisions. On public benchmarks, our model gives state-of-the-art\nperformance in layout estimation and competitive results in 3D object\ndetection. With that, we show a feasible path for enhancing the spatial\nunderstanding capabilities of modern LLMs for applications in augmented\nreality, embodied robotics, and more.", "AI": {"tldr": "SpatialLM是一个大型语言模型，用于处理3D点云数据并生成结构化3D场景理解输出，如墙壁、门窗等建筑元素和语义类别的定向物体框。", "motivation": "提升现代LLM在增强现实、机器人等应用中的空间理解能力。", "method": "基于开源LLM的多模态架构，通过大规模合成数据集（12,328个室内场景）进行微调。", "result": "在公共基准测试中，布局估计达到最先进水平，3D物体检测结果具有竞争力。", "conclusion": "展示了增强LLM空间理解能力的可行路径。"}}
{"id": "2506.06345", "pdf": "https://arxiv.org/pdf/2506.06345", "abs": "https://arxiv.org/abs/2506.06345", "authors": ["Sukru Selim Calik", "Andac Akyuz", "Zeynep Hilal Kilimci", "Kerem Colak"], "title": "Explainable-AI powered stock price prediction using time series transformers: A Case Study on BIST100", "categories": ["q-fin.ST", "cs.AI", "cs.LG"], "comment": null, "summary": "Financial literacy is increasingly dependent on the ability to interpret\ncomplex financial data and utilize advanced forecasting tools. In this context,\nthis study proposes a novel approach that combines transformer-based time\nseries models with explainable artificial intelligence (XAI) to enhance the\ninterpretability and accuracy of stock price predictions. The analysis focuses\non the daily stock prices of the five highest-volume banks listed in the\nBIST100 index, along with XBANK and XU100 indices, covering the period from\nJanuary 2015 to March 2025. Models including DLinear, LTSNet, Vanilla\nTransformer, and Time Series Transformer are employed, with input features\nenriched by technical indicators. SHAP and LIME techniques are used to provide\ntransparency into the influence of individual features on model outputs. The\nresults demonstrate the strong predictive capabilities of transformer models\nand highlight the potential of interpretable machine learning to empower\nindividuals in making informed investment decisions and actively engaging in\nfinancial markets.", "AI": {"tldr": "本研究提出了一种结合Transformer时间序列模型与可解释人工智能（XAI）的新方法，以提高股票价格预测的准确性和可解释性。", "motivation": "金融素养越来越依赖于对复杂金融数据的解读和高级预测工具的使用，因此需要更透明和准确的预测方法。", "method": "研究使用了DLinear、LTSNet、Vanilla Transformer和Time Series Transformer等模型，并结合技术指标作为输入特征，同时采用SHAP和LIME技术提供模型输出的可解释性。", "result": "结果表明Transformer模型具有强大的预测能力，可解释机器学习有助于个人做出明智的投资决策。", "conclusion": "该研究展示了可解释机器学习在金融领域的潜力，能够帮助个人更积极地参与金融市场。"}}
{"id": "2506.07617", "pdf": "https://arxiv.org/pdf/2506.07617", "abs": "https://arxiv.org/abs/2506.07617", "authors": ["Roman Kyslyi", "Yuliia Maksymiuk", "Ihor Pysmennyi"], "title": "Vuyko Mistral: Adapting LLMs for Low-Resource Dialectal Translation", "categories": ["cs.CL"], "comment": "Preprint. Will be published at Proceedings of the Fourth Ukrainian\n  Natural Language Processing Workshop (UNLP)", "summary": "In this paper we introduce the first effort to adapt large language models\n(LLMs) to the Ukrainian dialect (in our case Hutsul), a low-resource and\nmorphologically complex dialect spoken in the Carpathian Highlands. We created\na parallel corpus of 9852 dialect-to-standard Ukrainian sentence pairs and a\ndictionary of 7320 dialectal word mappings. We also addressed data shortage by\nproposing an advanced Retrieval-Augmented Generation (RAG) pipeline to generate\nsynthetic parallel translation pairs, expanding the corpus with 52142 examples.\nWe have fine-tuned multiple open-source LLMs using LoRA and evaluated them on a\nstandard-to-dialect translation task, also comparing with few-shot GPT-4o\ntranslation. In the absence of human annotators, we adopt a multi-metric\nevaluation strategy combining BLEU, chrF++, TER, and LLM-based judgment\n(GPT-4o). The results show that even small(7B) finetuned models outperform\nzero-shot baselines such as GPT-4o across both automatic and LLM-evaluated\nmetrics. All data, models, and code are publicly released at:\nhttps://github.com/woters/vuyko-hutsul", "AI": {"tldr": "论文首次尝试将大语言模型（LLMs）适配到乌克兰方言（Hutsul），通过构建平行语料库和词典，并利用RAG生成合成数据，最终微调模型在翻译任务中表现优于GPT-4o。", "motivation": "解决乌克兰Hutsul方言这一低资源且形态复杂方言的翻译问题。", "method": "构建平行语料库和词典，提出RAG生成合成数据，微调多个开源LLMs，并采用多指标评估策略。", "result": "微调的小模型（7B）在自动和LLM评估指标上均优于GPT-4o。", "conclusion": "通过合成数据和微调，小模型也能在低资源方言任务中表现优异。"}}
{"id": "2506.07497", "pdf": "https://arxiv.org/pdf/2506.07497", "abs": "https://arxiv.org/abs/2506.07497", "authors": ["Xiangyu Guo", "Zhanqian Wu", "Kaixin Xiong", "Ziyang Xu", "Lijun Zhou", "Gangwei Xu", "Shaoqing Xu", "Haiyang Sun", "Bing Wang", "Guang Chen", "Hangjun Ye", "Wenyu Liu", "Xinggang Wang"], "title": "Genesis: Multimodal Driving Scene Generation with Spatio-Temporal and Cross-Modal Consistency", "categories": ["cs.CV"], "comment": null, "summary": "We present Genesis, a unified framework for joint generation of multi-view\ndriving videos and LiDAR sequences with spatio-temporal and cross-modal\nconsistency. Genesis employs a two-stage architecture that integrates a\nDiT-based video diffusion model with 3D-VAE encoding, and a BEV-aware LiDAR\ngenerator with NeRF-based rendering and adaptive sampling. Both modalities are\ndirectly coupled through a shared latent space, enabling coherent evolution\nacross visual and geometric domains. To guide the generation with structured\nsemantics, we introduce DataCrafter, a captioning module built on\nvision-language models that provides scene-level and instance-level\nsupervision. Extensive experiments on the nuScenes benchmark demonstrate that\nGenesis achieves state-of-the-art performance across video and LiDAR metrics\n(FVD 16.95, FID 4.24, Chamfer 0.611), and benefits downstream tasks including\nsegmentation and 3D detection, validating the semantic fidelity and practical\nutility of the generated data.", "AI": {"tldr": "Genesis是一个统一框架，用于联合生成多视角驾驶视频和LiDAR序列，确保时空和跨模态一致性。它采用两阶段架构，结合DiT视频扩散模型和3D-VAE编码，以及BEV感知的LiDAR生成器。通过共享潜在空间实现视觉和几何域的一致性，并通过DataCrafter模块提供语义指导。实验表明，Genesis在nuScenes基准测试中表现优异，并提升了下游任务性能。", "motivation": "解决多视角视频和LiDAR序列联合生成中的时空和跨模态一致性问题，同时提升生成数据的语义保真度和实用性。", "method": "采用两阶段架构：1) DiT视频扩散模型与3D-VAE编码；2) BEV感知的LiDAR生成器，结合NeRF渲染和自适应采样。通过共享潜在空间和DataCrafter模块实现语义指导。", "result": "在nuScenes基准测试中，Genesis在视频和LiDAR指标上达到最优（FVD 16.95, FID 4.24, Chamfer 0.611），并提升了下游任务（如分割和3D检测）性能。", "conclusion": "Genesis通过统一框架和语义指导，实现了高质量的多模态数据生成，验证了其语义保真度和实际应用价值。"}}
{"id": "2506.07621", "pdf": "https://arxiv.org/pdf/2506.07621", "abs": "https://arxiv.org/abs/2506.07621", "authors": ["Harsh Bihany", "Shubham Patel", "Ashutosh Modi"], "title": "LoRMA: Low-Rank Multiplicative Adaptation for LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted at ACL Findings 2025; 21 pages (9 main paper + 5 pages\n  references + 7 pages appendix)", "summary": "Large Language Models have shown remarkable capabilities in the NLP domain.\nTheir effectiveness can mainly be attributed to their ability to adapt to an\narray of downstream tasks. However, generally, full fine-tuning is a\ncomputationally expensive job. To mitigate this, many techniques have been\ndeveloped that prime efficiency, a prominent one being Low-Rank Adaptation\n(LoRA). However, LoRA and its variants employ re-parametrized additive updates.\nIn this paper, we propose Low-Rank Multiplicative Adaptation (LoRMA), which\nshifts the paradigm of additive updates to a richer space of matrix\nmultiplicative transformations. We tackle challenges such as computational\ncomplexity and rank bottleneck of matrix multiplication by effectively\nre-ordering operations and introducing rank inflation strategies. We conduct\nextensive experiments to demonstrate the effectiveness of our approach in terms\nof various evaluation metrics.", "AI": {"tldr": "论文提出了一种名为LoRMA的新方法，通过矩阵乘法变换替代传统的加法更新，解决了计算复杂性和秩瓶颈问题。", "motivation": "大型语言模型在NLP领域表现出色，但完全微调计算成本高。现有方法如LoRA采用加法更新，限制了性能。", "method": "提出Low-Rank Multiplicative Adaptation (LoRMA)，通过矩阵乘法变换和操作重排序及秩膨胀策略优化。", "result": "实验证明LoRMA在多种评估指标上表现优异。", "conclusion": "LoRMA通过乘法变换提供了更高效的适应方法，优于传统加法更新。"}}
{"id": "2506.07533", "pdf": "https://arxiv.org/pdf/2506.07533", "abs": "https://arxiv.org/abs/2506.07533", "authors": ["Wei Tao", "Haocheng Lu", "Xiaoyang Qu", "Bin Zhang", "Kai Lu", "Jiguang Wan", "Jianzong Wang"], "title": "MoQAE: Mixed-Precision Quantization for Long-Context LLM Inference via Mixture of Quantization-Aware Experts", "categories": ["cs.CV"], "comment": "Accepted by the 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025)", "summary": "One of the primary challenges in optimizing large language models (LLMs) for\nlong-context inference lies in the high memory consumption of the Key-Value\n(KV) cache. Existing approaches, such as quantization, have demonstrated\npromising results in reducing memory usage. However, current quantization\nmethods cannot take both effectiveness and efficiency into account. In this\npaper, we propose MoQAE, a novel mixed-precision quantization method via\nmixture of quantization-aware experts. First, we view different quantization\nbit-width configurations as experts and use the traditional mixture of experts\n(MoE) method to select the optimal configuration. To avoid the inefficiency\ncaused by inputting tokens one by one into the router in the traditional MoE\nmethod, we input the tokens into the router chunk by chunk. Second, we design a\nlightweight router-only fine-tuning process to train MoQAE with a comprehensive\nloss to learn the trade-off between model accuracy and memory usage. Finally,\nwe introduce a routing freezing (RF) and a routing sharing (RS) mechanism to\nfurther reduce the inference overhead. Extensive experiments on multiple\nbenchmark datasets demonstrate that our method outperforms state-of-the-art KV\ncache quantization approaches in both efficiency and effectiveness.", "AI": {"tldr": "MoQAE是一种新型的混合精度量化方法，通过量化感知专家的混合选择最优配置，解决了LLM长上下文推理中KV缓存的高内存消耗问题。", "motivation": "优化大型语言模型的长上下文推理时，KV缓存的高内存消耗是主要挑战，现有量化方法无法同时兼顾效率和效果。", "method": "1. 将不同量化位宽配置视为专家，使用MoE方法选择最优配置；2. 按块输入令牌以提高效率；3. 设计轻量级路由器微调过程；4. 引入路由冻结和共享机制减少推理开销。", "result": "在多个基准数据集上的实验表明，MoQAE在效率和效果上均优于现有KV缓存量化方法。", "conclusion": "MoQAE通过混合精度量化和优化路由机制，显著降低了内存使用并保持了模型性能。"}}
{"id": "2506.06351", "pdf": "https://arxiv.org/pdf/2506.06351", "abs": "https://arxiv.org/abs/2506.06351", "authors": ["Alexis Le Pichon", "Alice Janela Cameijo", "Samir Aknine", "Youcef Sklab", "Souhila Arib", "Quentin Brissaud", "Sven Peter Naesholm"], "title": "Deep learning methods for modeling infrasound transmission loss in the middle atmosphere", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": "12 pages, 7 figures", "summary": "Accurate modeling of infrasound transmission losses (TLs) is essential to\nassess the performance of the global International Monitoring System infrasound\nnetwork. Among existing propagation modeling tools, parabolic equation (PE)\nmethod enables TLs to be finely modeled, but its computational cost does not\nallow exploration of a large parameter space for operational monitoring\napplications. To reduce computation times, Brissaud et al. 2023 explored the\npotential of convolutional neural networks trained on a large set of regionally\nsimulated wavefields (< 1000 km from the source) to predict TLs with negligible\ncomputation times compared to PE simulations. However, this method struggles in\nunfavorable initial wind conditions, especially at high frequencies, and causal\nissues with winds at large distances from the source affecting ground TLs close\nto the source. In this study, we have developed an optimized convolutional\nnetwork designed to minimize prediction errors while predicting TLs from\nglobally simulated combined temperature and wind fields spanning over\npropagation ranges of 4000 km. Our approach enhances the previously proposed\none by implementing key optimizations that improve the overall architecture\nperformance. The implemented model predicts TLs with an average error of 8.6 dB\nin the whole frequency band (0.1-3.2 Hz) and explored realistic atmospheric\nscenarios.", "AI": {"tldr": "该研究提出了一种优化的卷积神经网络，用于高效预测全球范围内的次声传输损失（TLs），解决了现有方法在高频和不利风条件下的局限性。", "motivation": "准确建模次声传输损失对评估国际监测系统性能至关重要，但现有方法计算成本高且在某些条件下表现不佳。", "method": "开发了一种优化的卷积神经网络，利用全球模拟的温度和风场数据预测TLs，改进了网络架构。", "result": "模型在整个频段（0.1-3.2 Hz）内平均误差为8.6 dB，适用于真实大气场景。", "conclusion": "优化后的卷积神经网络显著提升了TLs预测的准确性和效率，适用于大范围次声监测。"}}
{"id": "2506.07626", "pdf": "https://arxiv.org/pdf/2506.07626", "abs": "https://arxiv.org/abs/2506.07626", "authors": ["Kseniia Petukhova", "Ekaterina Kochmar"], "title": "Intent Matters: Enhancing AI Tutoring with Fine-Grained Pedagogical Intent Annotation", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) hold great promise for educational applications,\nparticularly in intelligent tutoring systems. However, effective tutoring\nrequires alignment with pedagogical strategies - something current LLMs lack\nwithout task-specific adaptation. In this work, we explore whether fine-grained\nannotation of teacher intents can improve the quality of LLM-generated tutoring\nresponses. We focus on MathDial, a dialog dataset for math instruction, and\napply an automated annotation framework to re-annotate a portion of the dataset\nusing a detailed taxonomy of eleven pedagogical intents. We then fine-tune an\nLLM using these new annotations and compare its performance to models trained\non the original four-category taxonomy. Both automatic and qualitative\nevaluations show that the fine-grained model produces more pedagogically\naligned and effective responses. Our findings highlight the value of intent\nspecificity for controlled text generation in educational settings, and we\nrelease our annotated data and code to facilitate further research.", "AI": {"tldr": "通过细粒度标注教师意图，改进LLM在教育场景中的生成响应质量，实验证明细粒度模型更符合教学策略。", "motivation": "当前LLM在教育应用中缺乏与教学策略的对齐，需任务特定适应。", "method": "使用MathDial数据集，通过自动标注框架重新标注部分数据，细分为11种教学意图，并微调LLM。", "result": "细粒度模型生成的响应在教学对齐性和有效性上优于原始四分类模型。", "conclusion": "细粒度意图标注对教育场景中的文本生成有重要价值，相关数据和代码已开源。"}}
{"id": "2506.07539", "pdf": "https://arxiv.org/pdf/2506.07539", "abs": "https://arxiv.org/abs/2506.07539", "authors": ["Xiaomeng Zhu", "Jacob Henningsson", "Duruo Li", "Pär Mårtensson", "Lars Hanson", "Mårten Björkman", "Atsuto Maki"], "title": "Domain Randomization for Object Detection in Manufacturing Applications using Synthetic Data: A Comprehensive Study", "categories": ["cs.CV", "cs.AI"], "comment": "This is accepted by 2025 IEEE International Conference on Robotics &\n  Automation (ICRA), waiting for publication. 14 pages, 14 figures", "summary": "This paper addresses key aspects of domain randomization in generating\nsynthetic data for manufacturing object detection applications. To this end, we\npresent a comprehensive data generation pipeline that reflects different\nfactors: object characteristics, background, illumination, camera settings, and\npost-processing. We also introduce the Synthetic Industrial Parts Object\nDetection dataset (SIP15-OD) consisting of 15 objects from three industrial use\ncases under varying environments as a test bed for the study, while also\nemploying an industrial dataset publicly available for robotic applications. In\nour experiments, we present more abundant results and insights into the\nfeasibility as well as challenges of sim-to-real object detection. In\nparticular, we identified material properties, rendering methods,\npost-processing, and distractors as important factors. Our method, leveraging\nthese, achieves top performance on the public dataset with Yolov8 models\ntrained exclusively on synthetic data; mAP@50 scores of 96.4% for the robotics\ndataset, and 94.1%, 99.5%, and 95.3% across three of the SIP15-OD use cases,\nrespectively. The results showcase the effectiveness of the proposed domain\nrandomization, potentially covering the distribution close to real data for the\napplications.", "AI": {"tldr": "论文提出了一种用于制造业目标检测的合成数据生成方法，通过领域随机化技术生成多样化的数据，并在公开数据集上验证了其有效性。", "motivation": "解决制造业目标检测中合成数据生成的挑战，特别是如何通过领域随机化技术覆盖真实数据的分布。", "method": "提出了一个综合的数据生成流程，考虑对象特征、背景、光照、相机设置和后处理，并引入了SIP15-OD数据集作为测试平台。", "result": "使用合成数据训练的Yolov8模型在公开数据集上表现优异，mAP@50得分高达96.4%，在SIP15-OD数据集上分别为94.1%、99.5%和95.3%。", "conclusion": "领域随机化技术能有效生成接近真实数据分布的合成数据，为制造业目标检测提供了可行的解决方案。"}}
{"id": "2506.06353", "pdf": "https://arxiv.org/pdf/2506.06353", "abs": "https://arxiv.org/abs/2506.06353", "authors": ["Naseem Babu", "Jimson Mathew", "A. P. Vinod"], "title": "Large Language Models for EEG: A Comprehensive Survey and Taxonomy", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "The growing convergence between Large Language Models (LLMs) and\nelectroencephalography (EEG) research is enabling new directions in neural\ndecoding, brain-computer interfaces (BCIs), and affective computing. This\nsurvey offers a systematic review and structured taxonomy of recent\nadvancements that utilize LLMs for EEG-based analysis and applications. We\norganize the literature into four domains: (1) LLM-inspired foundation models\nfor EEG representation learning, (2) EEG-to-language decoding, (3) cross-modal\ngeneration including image and 3D object synthesis, and (4) clinical\napplications and dataset management tools. The survey highlights how\ntransformer-based architectures adapted through fine-tuning, few-shot, and\nzero-shot learning have enabled EEG-based models to perform complex tasks such\nas natural language generation, semantic interpretation, and diagnostic\nassistance. By offering a structured overview of modeling strategies, system\ndesigns, and application areas, this work serves as a foundational resource for\nfuture work to bridge natural language processing and neural signal analysis\nthrough language models.", "AI": {"tldr": "本文综述了大型语言模型（LLMs）与脑电图（EEG）研究的结合，系统梳理了LLMs在EEG分析和应用中的最新进展，并提出了分类框架。", "motivation": "探索LLMs与EEG研究的融合如何推动神经解码、脑机接口和情感计算的发展。", "method": "通过系统综述和分类，将文献分为四个领域：LLM启发的EEG表征学习、EEG到语言的解码、跨模态生成（如图像和3D对象合成）以及临床应用和数据集管理工具。", "result": "研究表明，基于Transformer的架构通过微调、少样本和零样本学习，使EEG模型能够完成自然语言生成、语义解释和诊断辅助等复杂任务。", "conclusion": "本文为未来通过语言模型桥接自然语言处理和神经信号分析的研究提供了基础资源。"}}
{"id": "2506.07631", "pdf": "https://arxiv.org/pdf/2506.07631", "abs": "https://arxiv.org/abs/2506.07631", "authors": ["Brian Gordon", "Yonatan Bitton", "Andreea Marzoca", "Yasumasa Onoe", "Xiao Wang", "Daniel Cohen-Or", "Idan Szpektor"], "title": "Unblocking Fine-Grained Evaluation of Detailed Captions: An Explaining AutoRater and Critic-and-Revise Pipeline", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Large Vision-Language Models (VLMs) now generate highly detailed,\nparagraphlength image captions, yet evaluating their factual accuracy remains\nchallenging. Current methods often miss fine-grained errors, being designed for\nshorter texts or lacking datasets with verified inaccuracies. We introduce\nDOCCI-Critique, a benchmark with 1,400 VLM-generated paragraph captions (100\nimages, 14 VLMs) featuring over 10,216 sentence-level human annotations of\nfactual correctness and explanatory rationales for errors, all within paragraph\ncontext. Building on this, we develop VNLI-Critique, a model for automated\nsentence-level factuality classification and critique generation. We highlight\nthree key applications: (1) VNLI-Critique demonstrates robust generalization,\nvalidated by state-of-the-art performance on the M-HalDetect benchmark and\nstrong results in CHOCOLATE claim verification. (2) The VNLI-Critique driven\nAutoRater for DOCCI-Critique provides reliable VLM rankings, showing excellent\nalignment with human factuality judgments (e.g., 0.98 Spearman). (3) An\ninnovative Critic-and-Revise pipeline, where critiques from VNLI-Critique guide\nLLM-based corrections, achieves substantial improvements in caption factuality\n(e.g., a 46% gain on DetailCaps-4870). Our work offers a crucial benchmark\nalongside practical tools, designed to significantly elevate the standards for\nfine-grained evaluation and foster the improvement of VLM image understanding.\nProject page: https://google.github.io/unblocking-detail-caption", "AI": {"tldr": "论文提出了DOCCI-Critique基准和VNLI-Critique模型，用于评估和改进大视觉语言模型（VLMs）生成段落的准确性。", "motivation": "当前评估VLMs生成段落的事实准确性方法存在不足，缺乏细粒度错误检测和验证数据集。", "method": "构建DOCCI-Critique基准（1,400条段落标注）和开发VNLI-Critique模型，用于自动化事实分类和错误分析。", "result": "VNLI-Critique在多个基准测试中表现优异，AutoRater与人类判断高度一致，Critic-and-Revise流程显著提升事实准确性。", "conclusion": "研究提供了关键基准和实用工具，显著提升了VLMs的细粒度评估和改进标准。"}}
{"id": "2506.07542", "pdf": "https://arxiv.org/pdf/2506.07542", "abs": "https://arxiv.org/abs/2506.07542", "authors": ["Bowen Liu", "Weiyi Zhang", "Peranut Chotcomwongse", "Xiaolan Chen", "Ruoyu Chen", "Pawin Pakaymaskul", "Niracha Arjkongharn", "Nattaporn Vongsa", "Xuelian Cheng", "Zongyuan Ge", "Kun Huang", "Xiaohui Li", "Yiru Duan", "Zhenbang Wang", "BaoYe Xie", "Qiang Chen", "Huazhu Fu", "Michael A. Mahr", "Jiaqi Qu", "Wangyiyang Chen", "Shiye Wang", "Yubo Tan", "Yongjie Li", "Mingguang He", "Danli Shi", "Paisan Ruamviboonsuk"], "title": "APTOS-2024 challenge report: Generation of synthetic 3D OCT images from fundus photographs", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Optical Coherence Tomography (OCT) provides high-resolution, 3D, and\nnon-invasive visualization of retinal layers in vivo, serving as a critical\ntool for lesion localization and disease diagnosis. However, its widespread\nadoption is limited by equipment costs and the need for specialized operators.\nIn comparison, 2D color fundus photography offers faster acquisition and\ngreater accessibility with less dependence on expensive devices. Although\ngenerative artificial intelligence has demonstrated promising results in\nmedical image synthesis, translating 2D fundus images into 3D OCT images\npresents unique challenges due to inherent differences in data dimensionality\nand biological information between modalities. To advance generative models in\nthe fundus-to-3D-OCT setting, the Asia Pacific Tele-Ophthalmology Society\n(APTOS-2024) organized a challenge titled Artificial Intelligence-based OCT\nGeneration from Fundus Images. This paper details the challenge framework\n(referred to as APTOS-2024 Challenge), including: the benchmark dataset,\nevaluation methodology featuring two fidelity metrics-image-based distance\n(pixel-level OCT B-scan similarity) and video-based distance (semantic-level\nvolumetric consistency), and analysis of top-performing solutions. The\nchallenge attracted 342 participating teams, with 42 preliminary submissions\nand 9 finalists. Leading methodologies incorporated innovations in hybrid data\npreprocessing or augmentation (cross-modality collaborative paradigms),\npre-training on external ophthalmic imaging datasets, integration of vision\nfoundation models, and model architecture improvement. The APTOS-2024 Challenge\nis the first benchmark demonstrating the feasibility of fundus-to-3D-OCT\nsynthesis as a potential solution for improving ophthalmic care accessibility\nin under-resourced healthcare settings, while helping to expedite medical\nresearch and clinical applications.", "AI": {"tldr": "APTOS-2024挑战赛探索了从2D眼底图像生成3D OCT图像的可行性，通过创新方法提升了生成模型的性能，为资源匮乏地区的眼科护理提供了潜在解决方案。", "motivation": "OCT设备成本高且依赖专业操作人员，限制了其广泛应用。2D眼底摄影更易获取，但缺乏3D信息。挑战赛旨在探索通过AI将2D图像转化为3D OCT图像的可行性。", "method": "挑战赛提供了基准数据集和评估方法（图像和视频级一致性指标），吸引了342个团队参与。领先方法包括数据预处理、预训练、视觉基础模型集成和架构改进。", "result": "42个初步提交和9个决赛方案展示了生成模型的潜力，特别是在跨模态协作和外部数据集预训练方面的创新。", "conclusion": "APTOS-2024挑战赛首次证明了从2D眼底图像生成3D OCT图像的可行性，有望提升资源匮乏地区的眼科护理可及性。"}}
{"id": "2506.06358", "pdf": "https://arxiv.org/pdf/2506.06358", "abs": "https://arxiv.org/abs/2506.06358", "authors": ["Alice Janela Cameijo", "Alexis Le Pichon", "Youcef Sklab", "Souhila Arib", "Quentin Brissaud", "Sven peter Naesholm", "Constantino Listowski", "Samir Aknine"], "title": "Towards real-time assessment of infrasound event detection capability using deep learning-based transmission loss estimation", "categories": ["eess.SP", "cs.AI"], "comment": "49 pages, 22 figures", "summary": "Accurate modeling of infrasound transmission loss is essential for evaluating\nthe performance of the International Monitoring System, enabling the effective\ndesign and maintenance of infrasound stations to support compliance of the\nComprehensive Nuclear-Test-Ban Treaty. State-of-the-art propagation modeling\ntools enable transmission loss to be finely simulated using atmospheric models.\nHowever, the computational cost prohibits the exploration of a large parameter\nspace in operational monitoring applications. To address this, recent studies\nmade use of a deep learning algorithm capable of making transmission loss\npredictions almost instantaneously. However, the use of nudged atmospheric\nmodels leads to an incomplete representation of the medium, and the absence of\ntemperature as an input makes the algorithm incompatible with long range\npropagation. In this study, we address these limitations by using both wind and\ntemperature fields as inputs to a neural network, simulated up to 130 km\naltitude and 4,000 km distance. We also optimize several aspects of the neural\nnetwork architecture. We exploit convolutional and recurrent layers to capture\nspatially and range-dependent features embedded in realistic atmospheric\nmodels, improving the overall performance. The neural network reaches an\naverage error of 4 dB compared to full parabolic equation simulations and\nprovides epistemic and data-related uncertainty estimates. Its evaluation on\nthe 2022 Hunga Tonga-Hunga Ha'apai volcanic eruption demonstrates its\nprediction capability using atmospheric conditions and frequencies not included\nin the training. This represents a significant step towards near real-time\nassessment of International Monitoring System detection thresholds of explosive\nsources.", "AI": {"tldr": "该研究通过改进神经网络架构，利用风和温度场作为输入，提升了次声传播损耗预测的准确性和适用性。", "motivation": "现有次声传播损耗预测方法因计算成本高和输入数据不完整，无法满足实时监测需求。", "method": "使用卷积和循环神经网络层，结合风和温度场输入，模拟高达130公里高度和4000公里距离的传播。", "result": "神经网络预测误差平均为4 dB，并能提供不确定性估计，验证了其在未训练条件下的预测能力。", "conclusion": "该方法为实时评估国际监测系统的爆炸源检测阈值提供了重要进展。"}}
{"id": "2506.07642", "pdf": "https://arxiv.org/pdf/2506.07642", "abs": "https://arxiv.org/abs/2506.07642", "authors": ["Yuan Chang", "Ziyue Li", "Hengyuan Zhang", "Yuanbo Kong", "Yanru Wu", "Zhijiang Guo", "Ngai Wong"], "title": "TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient LLM-based Scientific Peer Review", "categories": ["cs.CL"], "comment": "30 pages, 17 figures", "summary": "While Large Language Models (LLMs) have shown significant potential in\nassisting peer review, current methods often struggle to generate thorough and\ninsightful reviews while maintaining efficiency. In this paper, we propose\nTreeReview, a novel framework that models paper review as a hierarchical and\nbidirectional question-answering process. TreeReview first constructs a tree of\nreview questions by recursively decomposing high-level questions into\nfine-grained sub-questions and then resolves the question tree by iteratively\naggregating answers from leaf to root to get the final review. Crucially, we\nincorporate a dynamic question expansion mechanism to enable deeper probing by\ngenerating follow-up questions when needed. We construct a benchmark derived\nfrom ICLR and NeurIPS venues to evaluate our method on full review generation\nand actionable feedback comments generation tasks. Experimental results of both\nLLM-based and human evaluation show that TreeReview outperforms strong\nbaselines in providing comprehensive, in-depth, and expert-aligned review\nfeedback, while reducing LLM token usage by up to 80% compared to\ncomputationally intensive approaches. Our code and benchmark dataset are\navailable at https://github.com/YuanChang98/tree-review.", "AI": {"tldr": "TreeReview是一种新颖的框架，通过分层和双向问答过程生成全面且深入的论文评审，同时显著减少计算资源消耗。", "motivation": "当前大型语言模型（LLMs）在辅助同行评审时，难以兼顾评审的全面性和效率。", "method": "TreeReview将评审建模为分层问答过程，递归分解问题并动态扩展问题树，从叶子到根聚合答案生成最终评审。", "result": "实验表明，TreeReview在生成全面、深入且专家对齐的评审反馈上优于基线方法，同时减少80%的LLM令牌使用。", "conclusion": "TreeReview为高效生成高质量论文评审提供了一种有效解决方案。"}}
{"id": "2506.07555", "pdf": "https://arxiv.org/pdf/2506.07555", "abs": "https://arxiv.org/abs/2506.07555", "authors": ["Haoxiang Wang", "Zinan Lin", "Da Yu", "Huishuai Zhang"], "title": "Synthesize Privacy-Preserving High-Resolution Images via Private Textual Intermediaries", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Generating high fidelity, differentially private (DP) synthetic images offers\na promising route to share and analyze sensitive visual data without\ncompromising individual privacy. However, existing DP image synthesis methods\nstruggle to produce high resolution outputs that faithfully capture the\nstructure of the original data. In this paper, we introduce a novel method,\nreferred to as Synthesis via Private Textual Intermediaries (SPTI), that can\ngenerate high resolution DP images with easy adoption. The key idea is to shift\nthe challenge of DP image synthesis from the image domain to the text domain by\nleveraging state of the art DP text generation methods. SPTI first summarizes\neach private image into a concise textual description using image to text\nmodels, then applies a modified Private Evolution algorithm to generate DP\ntext, and finally reconstructs images using text to image models. Notably, SPTI\nrequires no model training, only inference with off the shelf models. Given a\nprivate dataset, SPTI produces synthetic images of substantially higher quality\nthan prior DP approaches. On the LSUN Bedroom dataset, SPTI attains an FID less\nthan or equal to 26.71 under epsilon equal to 1.0, improving over Private\nEvolution FID of 40.36. Similarly, on MM CelebA HQ, SPTI achieves an FID less\nthan or equal to 33.27 at epsilon equal to 1.0, compared to 57.01 from DP fine\ntuning baselines. Overall, our results demonstrate that Synthesis via Private\nTextual Intermediaries provides a resource efficient and proprietary model\ncompatible framework for generating high resolution DP synthetic images,\ngreatly expanding access to private visual datasets.", "AI": {"tldr": "SPTI是一种通过文本中介生成高分辨率差分隐私（DP）图像的新方法，无需训练模型，显著提升了图像质量。", "motivation": "现有DP图像合成方法难以生成高分辨率且忠实于原始数据的图像，SPTI旨在解决这一问题。", "method": "SPTI将图像转换为文本描述，利用DP文本生成方法生成隐私保护的文本，再通过文本到图像模型重建图像。", "result": "在LSUN Bedroom和MM CelebA HQ数据集上，SPTI的FID显著优于现有方法。", "conclusion": "SPTI提供了一种资源高效且兼容专有模型的框架，扩展了对私有视觉数据集的访问。"}}
{"id": "2506.06359", "pdf": "https://arxiv.org/pdf/2506.06359", "abs": "https://arxiv.org/abs/2506.06359", "authors": ["Gabriel Antonesi", "Tudor Cioara", "Ionut Anghel", "Vasilis Michalakopoulos", "Elissaios Sarmas", "Liana Toderean"], "title": "From Transformers to Large Language Models: A systematic review of AI applications in the energy sector towards Agentic Digital Twins", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Artificial intelligence (AI) has long promised to improve energy management\nin smart grids by enhancing situational awareness and supporting more effective\ndecision-making. While traditional machine learning has demonstrated notable\nresults in forecasting and optimization, it often struggles with\ngeneralization, situational awareness, and heterogeneous data integration.\nRecent advances in foundation models such as Transformer architecture and Large\nLanguage Models (LLMs) have demonstrated improved capabilities in modelling\ncomplex temporal and contextual relationships, as well as in multi-modal data\nfusion which is essential for most AI applications in the energy sector. In\nthis review we synthesize the rapid expanding field of AI applications in the\nenergy domain focusing on Transformers and LLMs. We examine the architectural\nfoundations, domain-specific adaptations and practical implementations of\ntransformer models across various forecasting and grid management tasks. We\nthen explore the emerging role of LLMs in the field: adaptation and fine tuning\nfor the energy sector, the type of tasks they are suited for, and the new\nchallenges they introduce. Along the way, we highlight practical\nimplementations, innovations, and areas where the research frontier is rapidly\nexpanding. These recent developments reviewed underscore a broader trend:\nGenerative AI (GenAI) is beginning to augment decision-making not only in\nhigh-level planning but also in day-to-day operations, from forecasting and\ngrid balancing to workforce training and asset onboarding. Building on these\ndevelopments, we introduce the concept of the Agentic Digital Twin, a\nnext-generation model that integrates LLMs to bring autonomy, proactivity, and\nsocial interaction into digital twin-based energy management systems.", "AI": {"tldr": "本文综述了AI在能源领域的最新进展，特别是Transformer和LLMs的应用，探讨了其在预测、电网管理中的潜力，并提出了Agentic Digital Twin的概念。", "motivation": "传统机器学习在能源管理中面临泛化、情境感知和数据整合的挑战，而Transformer和LLMs在复杂关系和多模态数据融合方面表现出色，因此研究其在能源领域的应用具有重要意义。", "method": "通过综述Transformer和LLMs的架构基础、领域适应及实际应用，分析其在能源任务中的表现和挑战。", "result": "研究表明，Transformer和LLMs在能源预测和管理中具有显著潜力，并能推动决策自动化和智能化。", "conclusion": "Generative AI（如LLMs）正在改变能源管理，Agentic Digital Twin概念的提出为未来自主和主动的能源系统提供了新方向。"}}
{"id": "2506.07645", "pdf": "https://arxiv.org/pdf/2506.07645", "abs": "https://arxiv.org/abs/2506.07645", "authors": ["Maciej Chrabąszcz", "Katarzyna Lorenc", "Karolina Seweryn"], "title": "Evaluating LLMs Robustness in Less Resourced Languages with Proxy Models", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated impressive capabilities across\nvarious natural language processing (NLP) tasks in recent years. However, their\nsusceptibility to jailbreaks and perturbations necessitates additional\nevaluations. Many LLMs are multilingual, but safety-related training data\ncontains mainly high-resource languages like English. This can leave them\nvulnerable to perturbations in low-resource languages such as Polish. We show\nhow surprisingly strong attacks can be cheaply created by altering just a few\ncharacters and using a small proxy model for word importance calculation. We\nfind that these character and word-level attacks drastically alter the\npredictions of different LLMs, suggesting a potential vulnerability that can be\nused to circumvent their internal safety mechanisms. We validate our attack\nconstruction methodology on Polish, a low-resource language, and find potential\nvulnerabilities of LLMs in this language. Additionally, we show how it can be\nextended to other languages. We release the created datasets and code for\nfurther research.", "AI": {"tldr": "论文探讨了大型语言模型（LLMs）在多语言环境下的安全性问题，尤其是低资源语言（如波兰语）中的漏洞，并提出了一种低成本攻击方法。", "motivation": "LLMs在多语言任务中表现优异，但安全性训练数据主要集中在高资源语言（如英语），导致低资源语言易受攻击。", "method": "通过修改少量字符和使用小型代理模型计算词重要性，构建低成本攻击方法。", "result": "这些字符和词级攻击显著改变了LLMs的预测，揭示了其内部安全机制的潜在漏洞。", "conclusion": "研究验证了攻击方法的有效性，并扩展到其他语言，同时公开了数据集和代码以供进一步研究。"}}
{"id": "2506.07559", "pdf": "https://arxiv.org/pdf/2506.07559", "abs": "https://arxiv.org/abs/2506.07559", "authors": ["Hao Yang", "JianYu Wu", "Run Fang", "Xuelian Zhao", "Yuan Ji", "Zhiyu Chen", "Guibin He", "Junceng Guo", "Yang Liu", "Xinhua Zeng"], "title": "Cross-channel Perception Learning for H&E-to-IHC Virtual Staining", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid development of digital pathology, virtual staining has become\na key technology in multimedia medical information systems, offering new\npossibilities for the analysis and diagnosis of pathological images. However,\nexisting H&E-to-IHC studies often overlook the cross-channel correlations\nbetween cell nuclei and cell membranes. To address this issue, we propose a\nnovel Cross-Channel Perception Learning (CCPL) strategy. Specifically, CCPL\nfirst decomposes HER2 immunohistochemical staining into Hematoxylin and DAB\nstaining channels, corresponding to cell nuclei and cell membranes,\nrespectively. Using the pathology foundation model Gigapath's Tile Encoder,\nCCPL extracts dual-channel features from both the generated and real images and\nmeasures cross-channel correlations between nuclei and membranes. The features\nof the generated and real stained images, obtained through the Tile Encoder,\nare also used to calculate feature distillation loss, enhancing the model's\nfeature extraction capabilities without increasing the inference burden.\nAdditionally, CCPL performs statistical analysis on the focal optical density\nmaps of both single channels to ensure consistency in staining distribution and\nintensity. Experimental results, based on quantitative metrics such as PSNR,\nSSIM, PCC, and FID, along with professional evaluations from pathologists,\ndemonstrate that CCPL effectively preserves pathological features, generates\nhigh-quality virtual stained images, and provides robust support for automated\npathological diagnosis using multimedia medical data.", "AI": {"tldr": "提出了一种新颖的跨通道感知学习（CCPL）策略，用于解决H&E到IHC转换中忽略细胞核与细胞膜跨通道相关性的问题，通过双通道特征提取和特征蒸馏损失提升虚拟染色质量。", "motivation": "现有H&E到IHC研究常忽略细胞核与细胞膜的跨通道相关性，限制了病理图像分析与诊断的准确性。", "method": "CCPL将HER2免疫组化染色分解为Hematoxylin和DAB染色通道，利用Gigapath的Tile Encoder提取双通道特征并计算跨通道相关性，同时通过特征蒸馏损失和光学密度统计确保染色一致性。", "result": "实验表明，CCPL在PSNR、SSIM、PCC和FID等指标上表现优异，且病理学家评估证实其能有效保留病理特征并生成高质量虚拟染色图像。", "conclusion": "CCPL为多媒体医疗数据驱动的自动化病理诊断提供了有力支持。"}}
{"id": "2506.06361", "pdf": "https://arxiv.org/pdf/2506.06361", "abs": "https://arxiv.org/abs/2506.06361", "authors": ["Tim Schneider", "Guillaume Duret", "Cristiana de Farias", "Roberto Calandra", "Liming Chen", "Jan Peters"], "title": "Tactile MNIST: Benchmarking Active Tactile Perception", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Tactile perception has the potential to significantly enhance dexterous\nrobotic manipulation by providing rich local information that can complement or\nsubstitute for other sensory modalities such as vision. However, because\ntactile sensing is inherently local, it is not well-suited for tasks that\nrequire broad spatial awareness or global scene understanding on its own. A\nhuman-inspired strategy to address this issue is to consider active perception\ntechniques instead. That is, to actively guide sensors toward regions with more\ninformative or significant features and integrate such information over time in\norder to understand a scene or complete a task. Both active perception and\ndifferent methods for tactile sensing have received significant attention\nrecently. Yet, despite advancements, both fields lack standardized benchmarks.\nTo bridge this gap, we introduce the Tactile MNIST Benchmark Suite, an\nopen-source, Gymnasium-compatible benchmark specifically designed for active\ntactile perception tasks, including localization, classification, and volume\nestimation. Our benchmark suite offers diverse simulation scenarios, from\nsimple toy environments all the way to complex tactile perception tasks using\nvision-based tactile sensors. Furthermore, we also offer a comprehensive\ndataset comprising 13,500 synthetic 3D MNIST digit models and 153,600\nreal-world tactile samples collected from 600 3D printed digits. Using this\ndataset, we train a CycleGAN for realistic tactile simulation rendering. By\nproviding standardized protocols and reproducible evaluation frameworks, our\nbenchmark suite facilitates systematic progress in the fields of tactile\nsensing and active perception.", "AI": {"tldr": "论文提出了一个名为Tactile MNIST Benchmark Suite的标准化基准套件，用于推动触觉感知和主动感知领域的研究。", "motivation": "触觉感知在机器人灵巧操作中潜力巨大，但因其局部性难以单独完成全局任务。主动感知技术可以弥补这一不足，但目前缺乏标准化基准。", "method": "开发了一个开源、兼容Gymnasium的基准套件，包括多样化的模拟场景和数据集（合成3D MNIST模型和真实触觉样本），并训练CycleGAN进行触觉模拟渲染。", "result": "提供了13,500个合成3D MNIST数字模型和153,600个真实触觉样本的数据集，支持定位、分类和体积估计等任务。", "conclusion": "该基准套件通过标准化协议和可复现的评估框架，促进了触觉感知和主动感知领域的系统性进展。"}}
{"id": "2506.07646", "pdf": "https://arxiv.org/pdf/2506.07646", "abs": "https://arxiv.org/abs/2506.07646", "authors": ["Rui Hu", "Xiaolong Lin", "Jiawang Liu", "Shixi Huang", "Zhenpeng Zhan"], "title": "Transcript-Prompted Whisper with Dictionary-Enhanced Decoding for Japanese Speech Annotation", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted to INTERSPEECH 2025", "summary": "In this paper, we propose a method for annotating phonemic and prosodic\nlabels on a given audio-transcript pair, aimed at constructing Japanese\ntext-to-speech (TTS) datasets. Our approach involves fine-tuning a large-scale\npre-trained automatic speech recognition (ASR) model, conditioned on ground\ntruth transcripts, to simultaneously output phrase-level graphemes and\nannotation labels. To further correct errors in phonemic labeling, we employ a\ndecoding strategy that utilizes dictionary prior knowledge. The objective\nevaluation results demonstrate that our proposed method outperforms previous\napproaches relying solely on text or audio. The subjective evaluation results\nindicate that the naturalness of speech synthesized by the TTS model, trained\nwith labels annotated using our method, is comparable to that of a model\ntrained with manual annotations.", "AI": {"tldr": "提出了一种基于预训练ASR模型的日语TTS数据集标注方法，结合字典先验知识优化音素标注，效果优于传统方法。", "motivation": "构建高质量的日语TTS数据集需要准确的音素和韵律标注，传统方法依赖纯文本或音频，效果有限。", "method": "通过微调预训练ASR模型，结合字典先验知识解码，实现音素和韵律标注。", "result": "客观评估显示优于传统方法，主观评估表明合成语音自然度接近人工标注。", "conclusion": "该方法为日语TTS数据集标注提供了一种高效且高质量的解决方案。"}}
{"id": "2506.07565", "pdf": "https://arxiv.org/pdf/2506.07565", "abs": "https://arxiv.org/abs/2506.07565", "authors": ["Jinlu Zhang", "Zixi Kang", "Yizhou Wang"], "title": "OpenDance: Multimodal Controllable 3D Dance Generation Using Large-scale Internet Data", "categories": ["cs.CV"], "comment": null, "summary": "Music-driven dance generation offers significant creative potential yet faces\nconsiderable challenges. The absence of fine-grained multimodal data and the\ndifficulty of flexible multi-conditional generation limit previous works on\ngeneration controllability and diversity in practice. In this paper, we build\nOpenDance5D, an extensive human dance dataset comprising over 101 hours across\n14 distinct genres. Each sample has five modalities to facilitate robust\ncross-modal learning: RGB video, audio, 2D keypoints, 3D motion, and\nfine-grained textual descriptions from human arts. Furthermore, we propose\nOpenDanceNet, a unified masked modeling framework for controllable dance\ngeneration conditioned on music and arbitrary combinations of text prompts,\nkeypoints, or character positioning. Comprehensive experiments demonstrate that\nOpenDanceNet achieves high-fidelity and flexible controllability.", "AI": {"tldr": "论文提出了OpenDance5D数据集和OpenDanceNet框架，解决了音乐驱动舞蹈生成中的多模态数据缺失和可控性问题。", "motivation": "现有研究缺乏细粒度多模态数据，且难以实现灵活的多条件生成，限制了舞蹈生成的多样性和可控性。", "method": "构建OpenDance5D数据集（14种舞蹈风格，101小时数据，5种模态），并提出OpenDanceNet框架，基于掩码建模实现多条件可控舞蹈生成。", "result": "OpenDanceNet在实验中表现出高保真度和灵活的可控性。", "conclusion": "OpenDance5D和OpenDanceNet为音乐驱动舞蹈生成提供了高质量数据和有效方法。"}}
{"id": "2506.06362", "pdf": "https://arxiv.org/pdf/2506.06362", "abs": "https://arxiv.org/abs/2506.06362", "authors": ["Dejun Xu", "Jijia Chen", "Gary G. Yen", "Min Jiang"], "title": "CR-BLEA: Contrastive Ranking for Adaptive Resource Allocation in Bilevel Evolutionary Algorithms", "categories": ["cs.NE", "cs.AI", "cs.LG"], "comment": null, "summary": "Bilevel optimization poses a significant computational challenge due to its\nnested structure, where each upper-level candidate solution requires solving a\ncorresponding lower-level problem. While evolutionary algorithms (EAs) are\neffective at navigating such complex landscapes, their high resource demands\nremain a key bottleneck -- particularly the redundant evaluation of numerous\nunpromising lower-level tasks. Despite recent advances in multitasking and\ntransfer learning, resource waste persists. To address this issue, we propose a\nnovel resource allocation framework for bilevel EAs that selectively identifies\nand focuses on promising lower-level tasks. Central to our approach is a\ncontrastive ranking network that learns relational patterns between paired\nupper- and lower-level solutions online. This knowledge guides a\nreference-based ranking strategy that prioritizes tasks for optimization and\nadaptively controls resampling based on estimated population quality.\nComprehensive experiments across five state-of-the-art bilevel algorithms show\nthat our framework significantly reduces computational cost while preserving --\nor even enhancing -- solution accuracy. This work offers a generalizable\nstrategy to improve the efficiency of bilevel EAs, paving the way for more\nscalable bilevel optimization.", "AI": {"tldr": "提出了一种新的资源分配框架，通过对比排序网络选择性优化有潜力的下层任务，显著降低计算成本并保持或提升解的质量。", "motivation": "双层优化因嵌套结构导致计算成本高，进化算法虽有效但资源浪费严重，尤其是对无潜力的下层任务的冗余评估。", "method": "使用对比排序网络在线学习上下层解的关系模式，指导基于参考的排序策略，优先优化有潜力的任务并自适应控制重采样。", "result": "在五种先进双层算法上的实验表明，该框架显著降低计算成本，同时保持或提升解的质量。", "conclusion": "该框架为提升双层进化算法的效率提供了通用策略，推动了更可扩展的双层优化。"}}
{"id": "2506.07658", "pdf": "https://arxiv.org/pdf/2506.07658", "abs": "https://arxiv.org/abs/2506.07658", "authors": ["Nitin Sharma", "Thomas Wolfers", "Çağatay Yıldız"], "title": "Beyond Benchmarks: A Novel Framework for Domain-Specific LLM Evaluation and Knowledge Mapping", "categories": ["cs.CL", "I.2.7; I.2.6"], "comment": "35 pages, 24 figures. First submission", "summary": "The paper addresses two critical challenges in language model (LM)\nevaluation: creating reliable domain-specific benchmarks and understanding\nknowledge representation during domain adaptation. We introduce a deterministic\npipeline that converts raw domain corpora into completion-type benchmarks\nwithout relying on LMs or human curation, eliminating benchmark contamination\nissues while enabling evaluation on the latest domain data. Our approach\ngenerates domain-specific keywords and related word lists using TF and Term\nTF-IDF methods and constructs prompt-target pairs. We evaluate models by\nmeasuring their ability to complete these prompts with the correct\ndomain-specific targets, providing a direct assessment of domain knowledge with\nlow computational cost. Through comprehensive experiments across multiple\nmodels (GPT-2 medium/XL, Llama-2/3.1, OLMo-2, Qwen-2, Mistral) and domains, we\ndemonstrate that our benchmark strongly correlates with expert-generated\nbenchmarks while providing a more accurate measure of domain knowledge than\ntraditional perplexity metrics. We reveal that domain adaptation happens\nrapidly in smaller models (within 500 steps) and illustrate a new approach to\ndomain knowledge evaluation in base models during training for early stopping.\nBy extending mechanistic analysis to domain adaptation, we discover that\ninitial-to-mid layers are primarily responsible for attribute extraction, while\nlater layers focus on next token prediction. Furthermore, we show that during\nadaptation, forgetting begins in the middle layers, where attribute extraction\nhappens and is amplified in later layers. Our work provides both a practical\nevaluation methodology for domain-specific LMs and novel insights into\nknowledge representation during adaptation, with implications for more\nefficient fine-tuning strategies and targeted approaches to mitigate\ncatastrophic forgetting.", "AI": {"tldr": "论文提出了一种确定性流程，将原始领域语料转化为完成型评测基准，无需依赖语言模型或人工标注，解决了评测污染问题，并支持最新领域数据的评测。", "motivation": "解决语言模型评测中的两个关键挑战：创建可靠的领域特定评测基准和理解领域适应过程中的知识表示。", "method": "使用TF和Term TF-IDF方法生成领域关键词和相关词列表，构建提示-目标对，通过模型完成提示的能力评估领域知识。", "result": "评测基准与专家生成的基准强相关，且比传统困惑度指标更准确；揭示领域适应在小型模型中快速发生（500步内），并发现初始到中间层负责属性提取，后层专注于下一词预测。", "conclusion": "提供了一种实用的领域特定语言模型评测方法，并揭示了适应过程中知识表示的新见解，有助于更高效的微调策略和减轻灾难性遗忘的针对性方法。"}}
{"id": "2506.07566", "pdf": "https://arxiv.org/pdf/2506.07566", "abs": "https://arxiv.org/abs/2506.07566", "authors": ["Marco Peer", "Robert Sablatnig", "Florian Kleber"], "title": "Towards the Influence of Text Quantity on Writer Retrieval", "categories": ["cs.CV"], "comment": "accepted for ICDAR2025", "summary": "This paper investigates the task of writer retrieval, which identifies\ndocuments authored by the same individual within a dataset based on handwriting\nsimilarities. While existing datasets and methodologies primarily focus on page\nlevel retrieval, we explore the impact of text quantity on writer retrieval\nperformance by evaluating line- and word level retrieval. We examine three\nstate-of-the-art writer retrieval systems, including both handcrafted and deep\nlearning-based approaches, and analyze their performance using varying amounts\nof text. Our experiments on the CVL and IAM dataset demonstrate that while\nperformance decreases by 20-30% when only one line of text is used as query and\ngallery, retrieval accuracy remains above 90% of full-page performance when at\nleast four lines are included. We further show that text-dependent retrieval\ncan maintain strong performance in low-text scenarios. Our findings also\nhighlight the limitations of handcrafted features in low-text scenarios, with\ndeep learning-based methods like NetVLAD outperforming traditional VLAD\nencoding.", "AI": {"tldr": "本文研究了基于手写相似性的作者检索任务，探讨了文本量对检索性能的影响，发现即使使用少量文本（如四行），检索准确率仍能保持较高水平。", "motivation": "现有研究主要关注页面级检索，而本文旨在分析文本量（如行级和词级）对作者检索性能的影响。", "method": "评估了三种先进的作者检索系统（包括手工特征和深度学习方法），并在CVL和IAM数据集上测试了不同文本量的性能。", "result": "实验表明，使用一行文本时性能下降20-30%，但使用至少四行文本时，检索准确率仍能达到全页性能的90%以上。深度学习方法在低文本量场景下表现优于手工特征。", "conclusion": "文本依赖性检索在低文本量场景中仍能保持较强性能，深度学习方法（如NetVLAD）优于传统方法。"}}
{"id": "2506.07664", "pdf": "https://arxiv.org/pdf/2506.07664", "abs": "https://arxiv.org/abs/2506.07664", "authors": ["Lei Xu", "Sirui Chen", "Yuxuan Huang", "Chaochao Lu"], "title": "Synthesis by Design: Controlled Data Generation via Structural Guidance", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Mathematical reasoning remains challenging for LLMs due to complex logic and\nthe need for precise computation. Existing methods enhance LLM reasoning by\nsynthesizing datasets through problem rephrasing, but face issues with\ngeneration quality and problem complexity. To address this, we propose to\nextract structural information with generated problem-solving code from\nmathematical reasoning and guide data generation with structured solutions.\nApplied to MATH and GSM8K, our approach produces 39K problems with labeled\nintermediate steps and a 6.1K-problem benchmark of higher difficulty. Results\non our benchmark show that model performance declines as reasoning length\nincreases. Additionally, we conducted fine-tuning experiments using the\nproposed training data on a range of LLMs, and the results validate the\neffectiveness of our dataset. We hope the proposed method and dataset will\ncontribute to future research in enhancing LLM reasoning capabilities.", "AI": {"tldr": "论文提出了一种通过生成问题解决代码提取结构信息的方法，用于增强LLM的数学推理能力，并生成了高质量的数据集和基准测试。", "motivation": "现有方法在生成数据集时存在质量和复杂度问题，需要改进以提升LLM的数学推理能力。", "method": "提取结构信息并结合生成的问题解决代码，指导数据生成，生成带有标注中间步骤的问题集。", "result": "生成了39K问题和6.1K高难度基准测试，实验显示模型性能随推理长度增加而下降，微调实验验证了数据集的有效性。", "conclusion": "提出的方法和数据集有助于未来提升LLM推理能力的研究。"}}
{"id": "2506.07570", "pdf": "https://arxiv.org/pdf/2506.07570", "abs": "https://arxiv.org/abs/2506.07570", "authors": ["Yixuan Yang", "Zhen Luo", "Tongsheng Ding", "Junru Lu", "Mingqi Gao", "Jinyu Yang", "Victor Sanchez", "Feng Zheng"], "title": "LLM-driven Indoor Scene Layout Generation via Scaled Human-aligned Data Synthesis and Multi-Stage Preference Optimization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Automatic indoor layout generation has attracted increasing attention due to\nits potential in interior design, virtual environment construction, and\nembodied AI. Existing methods fall into two categories: prompt-driven\napproaches that leverage proprietary LLM services (e.g., GPT APIs) and\nlearning-based methods trained on layout data upon diffusion-based models.\nPrompt-driven methods often suffer from spatial inconsistency and high\ncomputational costs, while learning-based methods are typically constrained by\ncoarse relational graphs and limited datasets, restricting their generalization\nto diverse room categories. In this paper, we revisit LLM-based indoor layout\ngeneration and present 3D-SynthPlace, a large-scale dataset that combines\nsynthetic layouts generated via a 'GPT synthesize, Human inspect' pipeline,\nupgraded from the 3D-Front dataset. 3D-SynthPlace contains nearly 17,000\nscenes, covering four common room types -- bedroom, living room, kitchen, and\nbathroom -- enriched with diverse objects and high-level spatial annotations.\nWe further introduce OptiScene, a strong open-source LLM optimized for indoor\nlayout generation, fine-tuned based on our 3D-SynthPlace dataset through our\ntwo-stage training. For the warum-up stage I, we adopt supervised fine-tuning\n(SFT), which is taught to first generate high-level spatial descriptions then\nconditionally predict concrete object placements. For the reinforcing stage II,\nto better align the generated layouts with human design preferences, we apply\nmulti-turn direct preference optimization (DPO), which significantly improving\nlayout quality and generation success rates. Extensive experiments demonstrate\nthat OptiScene outperforms traditional prompt-driven and learning-based\nbaselines. Moreover, OptiScene shows promising potential in interactive tasks\nsuch as scene editing and robot navigation.", "AI": {"tldr": "论文提出了一种基于LLM的室内布局生成方法，结合了合成数据集3D-SynthPlace和优化模型OptiScene，显著提升了生成质量和成功率。", "motivation": "现有方法存在空间不一致性、高计算成本或泛化能力不足的问题，需要一种更高效的解决方案。", "method": "通过两阶段训练（监督微调和多轮直接偏好优化）优化LLM模型OptiScene，并利用合成数据集3D-SynthPlace。", "result": "OptiScene在生成质量和成功率上优于传统方法，并在交互任务中表现出潜力。", "conclusion": "3D-SynthPlace和OptiScene为室内布局生成提供了高效且通用的解决方案。"}}
{"id": "2506.06380", "pdf": "https://arxiv.org/pdf/2506.06380", "abs": "https://arxiv.org/abs/2506.06380", "authors": ["Jingyi Gu", "Xuan Zhang", "Guiling Wang"], "title": "Beyond the Norm: A Survey of Synthetic Data Generation for Rare Events", "categories": ["cs.LG", "cs.AI", "cs.CE"], "comment": null, "summary": "Extreme events, such as market crashes, natural disasters, and pandemics, are\nrare but catastrophic, often triggering cascading failures across\ninterconnected systems. Accurate prediction and early warning can help minimize\nlosses and improve preparedness. While data-driven methods offer powerful\ncapabilities for extreme event modeling, they require abundant training data,\nyet extreme event data is inherently scarce, creating a fundamental challenge.\nSynthetic data generation has emerged as a powerful solution. However, existing\nsurveys focus on general data with privacy preservation emphasis, rather than\nextreme events' unique performance requirements. This survey provides the first\noverview of synthetic data generation for extreme events. We systematically\nreview generative modeling techniques and large language models, particularly\nthose enhanced by statistical theory as well as specialized training and\nsampling mechanisms to capture heavy-tailed distributions. We summarize\nbenchmark datasets and introduce a tailored evaluation framework covering\nstatistical, dependence, visual, and task-oriented metrics. A central\ncontribution is our in-depth analysis of each metric's applicability in\nextremeness and domain-specific adaptations, providing actionable guidance for\nmodel evaluation in extreme settings. We categorize key application domains and\nidentify underexplored areas like behavioral finance, wildfires, earthquakes,\nwindstorms, and infectious outbreaks. Finally, we outline open challenges,\nproviding a structured foundation for advancing synthetic rare-event research.", "AI": {"tldr": "本文综述了极端事件合成数据生成的首次概述，系统回顾了生成模型技术和大语言模型，提出了针对极端事件的评估框架，并分析了关键应用领域和未探索方向。", "motivation": "极端事件虽罕见但破坏性极大，准确预测和早期预警可减少损失，但极端事件数据稀缺，合成数据生成成为解决方案。现有研究多关注通用数据，缺乏针对极端事件的性能要求。", "method": "系统回顾生成模型技术和大语言模型，特别是通过统计理论增强的模型，以及专门训练和采样机制。提出评估框架，包括统计、依赖、视觉和任务导向指标。", "result": "总结了基准数据集，分析了各指标在极端性和领域适应性中的适用性，提供了模型评估的实用指南。分类了关键应用领域，并指出未探索方向。", "conclusion": "为推进极端事件合成数据研究提供了结构化基础，并概述了开放挑战。"}}
{"id": "2506.07667", "pdf": "https://arxiv.org/pdf/2506.07667", "abs": "https://arxiv.org/abs/2506.07667", "authors": ["Prarabdh Shukla", "Wei Yin Chong", "Yash Patel", "Brennan Schaffner", "Danish Pruthi", "Arjun Bhagoji"], "title": "Silencing Empowerment, Allowing Bigotry: Auditing the Moderation of Hate Speech on Twitch", "categories": ["cs.CL", "cs.HC", "cs.LG"], "comment": null, "summary": "To meet the demands of content moderation, online platforms have resorted to\nautomated systems. Newer forms of real-time engagement($\\textit{e.g.}$, users\ncommenting on live streams) on platforms like Twitch exert additional pressures\non the latency expected of such moderation systems. Despite their prevalence,\nrelatively little is known about the effectiveness of these systems. In this\npaper, we conduct an audit of Twitch's automated moderation tool\n($\\texttt{AutoMod}$) to investigate its effectiveness in flagging hateful\ncontent. For our audit, we create streaming accounts to act as siloed test\nbeds, and interface with the live chat using Twitch's APIs to send over\n$107,000$ comments collated from $4$ datasets. We measure $\\texttt{AutoMod}$'s\naccuracy in flagging blatantly hateful content containing misogyny, racism,\nableism and homophobia. Our experiments reveal that a large fraction of hateful\nmessages, up to $94\\%$ on some datasets, $\\textit{bypass moderation}$.\nContextual addition of slurs to these messages results in $100\\%$ removal,\nrevealing $\\texttt{AutoMod}$'s reliance on slurs as a moderation signal. We\nalso find that contrary to Twitch's community guidelines, $\\texttt{AutoMod}$\nblocks up to $89.5\\%$ of benign examples that use sensitive words in\npedagogical or empowering contexts. Overall, our audit points to large gaps in\n$\\texttt{AutoMod}$'s capabilities and underscores the importance for such\nsystems to understand context effectively.", "AI": {"tldr": "论文审计了Twitch的自动审核工具AutoMod，发现其在标记仇恨内容时存在显著漏洞，同时误删了许多良性内容。", "motivation": "在线平台依赖自动化系统进行内容审核，但实时互动（如直播评论）对审核系统的延迟和有效性提出了更高要求。目前对这些系统的有效性知之甚少。", "method": "通过创建测试账户，使用Twitch API发送超过107,000条评论，测量AutoMod在标记仇恨内容（如性别歧视、种族主义等）时的准确性。", "result": "AutoMod漏检了高达94%的仇恨内容，但对包含侮辱性词汇的内容100%删除。同时，误删了89.5%的良性内容。", "conclusion": "AutoMod在理解上下文方面存在重大缺陷，需改进以更有效地区分仇恨与良性内容。"}}
{"id": "2506.07572", "pdf": "https://arxiv.org/pdf/2506.07572", "abs": "https://arxiv.org/abs/2506.07572", "authors": ["Yu Li", "Feng Xue", "Shujie Li", "Jinrui Zhang", "Shuang Yang", "Dan Guo", "Richang Hong"], "title": "Learning Speaker-Invariant Visual Features for Lipreading", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Lipreading is a challenging cross-modal task that aims to convert visual lip\nmovements into spoken text. Existing lipreading methods often extract visual\nfeatures that include speaker-specific lip attributes (e.g., shape, color,\ntexture), which introduce spurious correlations between vision and text. These\ncorrelations lead to suboptimal lipreading accuracy and restrict model\ngeneralization. To address this challenge, we introduce SIFLip, a\nspeaker-invariant visual feature learning framework that disentangles\nspeaker-specific attributes using two complementary disentanglement modules\n(Implicit Disentanglement and Explicit Disentanglement) to improve\ngeneralization. Specifically, since different speakers exhibit semantic\nconsistency between lip movements and phonetic text when pronouncing the same\nwords, our implicit disentanglement module leverages stable text embeddings as\nsupervisory signals to learn common visual representations across speakers,\nimplicitly decoupling speaker-specific features. Additionally, we design a\nspeaker recognition sub-task within the main lipreading pipeline to filter\nspeaker-specific features, then further explicitly disentangle these\npersonalized visual features from the backbone network via gradient reversal.\nExperimental results demonstrate that SIFLip significantly enhances\ngeneralization performance across multiple public datasets. Experimental\nresults demonstrate that SIFLip significantly improves generalization\nperformance across multiple public datasets, outperforming state-of-the-art\nmethods.", "AI": {"tldr": "SIFLip是一个通过解耦说话者特定特征来提高唇读泛化性能的框架，结合隐式和显式解耦模块，显著优于现有方法。", "motivation": "现有唇读方法提取的视觉特征包含说话者特定属性（如唇形、颜色、纹理），导致虚假相关性，影响准确性和泛化能力。", "method": "SIFLip通过隐式解耦模块利用文本嵌入作为监督信号学习通用视觉表示，并通过显式解耦模块（梯度反转）过滤说话者特定特征。", "result": "实验表明，SIFLip在多个公开数据集上显著提升了泛化性能，优于现有方法。", "conclusion": "SIFLip通过解耦说话者特定特征，有效提高了唇读任务的泛化能力和准确性。"}}
{"id": "2506.06381", "pdf": "https://arxiv.org/pdf/2506.06381", "abs": "https://arxiv.org/abs/2506.06381", "authors": ["Trisanth Srinivasan", "Santosh Patapati", "Himani Musku", "Idhant Gode", "Aditya Arora", "Samvit Bhattacharya", "Abubakr Nazriev", "Sanika Hirave", "Zaryab Kanjiani", "Srinjoy Ghose", "Srinidhi Shetty"], "title": "CPS-Guard: Framework for Dependability Assurance of AI- and LLM-Based Cyber-Physical Systems", "categories": ["cs.RO", "cs.AI", "cs.ET", "cs.HC", "cs.MA", "C.3; C.4; D.2.4; D.4.6; I.2.7"], "comment": null, "summary": "Cyber-Physical Systems (CPS) increasingly depend on advanced AI techniques to\noperate in critical applications. However, traditional verification and\nvalidation methods often struggle to handle the unpredictable and dynamic\nnature of AI components. In this paper, we introduce CPS-Guard, a novel\nframework that employs multi-role orchestration to automate the iterative\nassurance process for AI-powered CPS. By assigning specialized roles (e.g.,\nsafety monitoring, security assessment, fault injection, and recovery planning)\nto dedicated agents within a simulated environment, CPS-Guard continuously\nevaluates and refines AI behavior against a range of dependability\nrequirements. We demonstrate the framework through a case study involving an\nautonomous vehicle navigating an intersection with an AI-based planner. Our\nresults show that CPS-Guard effectively detects vulnerabilities, manages\nperformance impacts, and supports adaptive recovery strategies, thereby\noffering a structured and extensible solution for rigorous V&V in safety- and\nsecurity-critical systems.", "AI": {"tldr": "CPS-Guard是一个通过多角色编排自动化AI驱动的CPS保证过程的新框架，有效检测漏洞并支持自适应恢复策略。", "motivation": "传统验证方法难以应对AI组件的动态性和不可预测性，需要一种更有效的解决方案。", "method": "CPS-Guard采用多角色编排，在模拟环境中通过专用代理（如安全监控、故障注入等）持续评估和优化AI行为。", "result": "在自动驾驶车辆的案例中，CPS-Guard成功检测漏洞、管理性能影响并支持自适应恢复策略。", "conclusion": "CPS-Guard为安全和关键系统提供了一种结构化且可扩展的严格验证与验证解决方案。"}}
{"id": "2506.07671", "pdf": "https://arxiv.org/pdf/2506.07671", "abs": "https://arxiv.org/abs/2506.07671", "authors": ["Ionut-Teodor Sorodoc", "Leonardo F. R. Ribeiro", "Rexhina Blloshmi", "Christopher Davis", "Adrià de Gispert"], "title": "GaRAGe: A Benchmark with Grounding Annotations for RAG Evaluation", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 (Findings)", "summary": "We present GaRAGe, a large RAG benchmark with human-curated long-form answers\nand annotations of each grounding passage, allowing a fine-grained evaluation\nof whether LLMs can identify relevant grounding when generating RAG answers.\nOur benchmark contains 2366 questions of diverse complexity, dynamism, and\ntopics, and includes over 35K annotated passages retrieved from both private\ndocument sets and the Web, to reflect real-world RAG use cases. This makes it\nan ideal test bed to evaluate an LLM's ability to identify only the relevant\ninformation necessary to compose a response, or provide a deflective response\nwhen there is insufficient information. Evaluations of multiple\nstate-of-the-art LLMs on GaRAGe show that the models tend to over-summarise\nrather than (a) ground their answers strictly on the annotated relevant\npassages (reaching at most a Relevance-Aware Factuality Score of 60%), or (b)\ndeflect when no relevant grounding is available (reaching at most 31% true\npositive rate in deflections). The F1 in attribution to relevant sources is at\nmost 58.9%, and we show that performance is particularly reduced when answering\ntime-sensitive questions and when having to draw knowledge from sparser private\ngrounding sources.", "AI": {"tldr": "GaRAGe是一个大型RAG基准测试，包含人工标注的长答案和基础段落，用于评估LLMs在生成RAG答案时能否识别相关基础信息。测试结果显示，现有LLMs倾向于过度总结而非严格基于相关段落或正确回避问题。", "motivation": "为了评估LLMs在RAG任务中识别相关基础信息的能力，并提供一个细粒度的测试平台。", "method": "构建包含2366个多样化问题的基准测试，标注超过35K段落，评估LLMs在生成答案时的相关性和回避能力。", "result": "LLMs在严格基于相关段落生成答案（最高60%相关性分数）和正确回避问题（最高31%正确率）方面表现不佳，F1分数最高58.9%。时间敏感问题和稀疏私有数据源表现更差。", "conclusion": "GaRAGe揭示了LLMs在RAG任务中的局限性，尤其在严格依赖相关信息和回避问题方面，需进一步改进。"}}
{"id": "2506.07575", "pdf": "https://arxiv.org/pdf/2506.07575", "abs": "https://arxiv.org/abs/2506.07575", "authors": ["Ruiyang Zhang", "Hu Zhang", "Hao Fei", "Zhedong Zheng"], "title": "Uncertainty-o: One Model-agnostic Framework for Unveiling Uncertainty in Large Multimodal Models", "categories": ["cs.CV", "cs.LG"], "comment": "Project page: https://uncertainty-o.github.io/", "summary": "Large Multimodal Models (LMMs), harnessing the complementarity among diverse\nmodalities, are often considered more robust than pure Language Large Models\n(LLMs); yet do LMMs know what they do not know? There are three key open\nquestions remaining: (1) how to evaluate the uncertainty of diverse LMMs in a\nunified manner, (2) how to prompt LMMs to show its uncertainty, and (3) how to\nquantify uncertainty for downstream tasks. In an attempt to address these\nchallenges, we introduce Uncertainty-o: (1) a model-agnostic framework designed\nto reveal uncertainty in LMMs regardless of their modalities, architectures, or\ncapabilities, (2) an empirical exploration of multimodal prompt perturbations\nto uncover LMM uncertainty, offering insights and findings, and (3) derive the\nformulation of multimodal semantic uncertainty, which enables quantifying\nuncertainty from multimodal responses. Experiments across 18 benchmarks\nspanning various modalities and 10 LMMs (both open- and closed-source)\ndemonstrate the effectiveness of Uncertainty-o in reliably estimating LMM\nuncertainty, thereby enhancing downstream tasks such as hallucination\ndetection, hallucination mitigation, and uncertainty-aware Chain-of-Thought\nreasoning.", "AI": {"tldr": "论文提出Uncertainty-o框架，用于评估和量化多模态大模型（LMMs）的不确定性，并通过实验验证其在多种任务中的有效性。", "motivation": "尽管多模态大模型（LMMs）被认为比纯语言大模型（LLMs）更鲁棒，但其不确定性的评估、提示和量化仍存在挑战。", "method": "提出Uncertainty-o框架，包括模型无关的评估方法、多模态提示扰动实验和语义不确定性量化公式。", "result": "在18个基准测试和10种LMMs上验证了Uncertainty-o的可靠性，提升了幻觉检测、缓解及不确定性感知推理等下游任务。", "conclusion": "Uncertainty-o为LMMs的不确定性提供了统一评估和量化方法，显著提升了模型在实际任务中的表现。"}}
{"id": "2506.06382", "pdf": "https://arxiv.org/pdf/2506.06382", "abs": "https://arxiv.org/abs/2506.06382", "authors": ["Michał P. Karpowicz"], "title": "On the Fundamental Impossibility of Hallucination Control in Large Language Models", "categories": ["stat.ML", "cs.AI", "cs.CL", "cs.GT", "cs.LG"], "comment": null, "summary": "This paper explains \\textbf{why it is impossible to create large language\nmodels that do not hallucinate and what are the trade-offs we should be looking\nfor}. It presents a formal \\textbf{impossibility theorem} demonstrating that no\ninference mechanism can simultaneously satisfy four fundamental properties:\n\\textbf{truthful (non-hallucinatory) generation, semantic information\nconservation, relevant knowledge revelation, and knowledge-constrained\noptimality}. By modeling LLM inference as an \\textbf{auction of ideas} where\nneural components compete to contribute to responses, we prove the\nimpossibility using the Green-Laffont theorem. That mathematical framework\nprovides a rigorous foundation for understanding the nature of inference\nprocess, with implications for model architecture, training objectives, and\nevaluation methods.", "AI": {"tldr": "论文证明了无法创建不产生幻觉的大语言模型，并提出了四种基本属性之间的权衡。", "motivation": "探讨为什么无法设计出完全不产生幻觉的大语言模型，并明确模型推理中的基本限制。", "method": "通过将LLM推理建模为“想法拍卖”，利用Green-Laffont定理证明不可能同时满足四种属性。", "result": "证明了无法同时满足真实性、语义信息守恒、相关知识揭示和知识约束最优性。", "conclusion": "该研究为理解推理过程的本质提供了数学基础，对模型架构、训练目标和评估方法有重要启示。"}}
{"id": "2506.07691", "pdf": "https://arxiv.org/pdf/2506.07691", "abs": "https://arxiv.org/abs/2506.07691", "authors": ["Jiaming Li", "Haoran Ye", "Yukun Chen", "Xinyue Li", "Lei Zhang", "Hamid Alinejad-Rokny", "Jimmy Chih-Hsien Peng", "Min Yang"], "title": "Training Superior Sparse Autoencoders for Instruct Models", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "As large language models (LLMs) grow in scale and capability, understanding\ntheir internal mechanisms becomes increasingly critical. Sparse autoencoders\n(SAEs) have emerged as a key tool in mechanistic interpretability, enabling the\nextraction of human-interpretable features from LLMs. However, existing SAE\ntraining methods are primarily designed for base models, resulting in reduced\nreconstruction quality and interpretability when applied to instruct models. To\nbridge this gap, we propose\n$\\underline{\\textbf{F}}$inetuning-$\\underline{\\textbf{a}}$ligned\n$\\underline{\\textbf{S}}$equential $\\underline{\\textbf{T}}$raining\n($\\textit{FAST}$), a novel training method specifically tailored for instruct\nmodels. $\\textit{FAST}$ aligns the training process with the data distribution\nand activation patterns characteristic of instruct models, resulting in\nsubstantial improvements in both reconstruction and feature interpretability.\nOn Qwen2.5-7B-Instruct, $\\textit{FAST}$ achieves a mean squared error of 0.6468\nin token reconstruction, significantly outperforming baseline methods with\nerrors of 5.1985 and 1.5096. In feature interpretability, $\\textit{FAST}$\nyields a higher proportion of high-quality features, for Llama3.2-3B-Instruct,\n$21.1\\%$ scored in the top range, compared to $7.0\\%$ and $10.2\\%$ for\n$\\textit{BT(P)}$ and $\\textit{BT(F)}$. Surprisingly, we discover that\nintervening on the activations of special tokens via the SAEs leads to\nimprovements in output quality, suggesting new opportunities for fine-grained\ncontrol of model behavior. Code, data, and 240 trained SAEs are available at\nhttps://github.com/Geaming2002/FAST.", "AI": {"tldr": "论文提出了一种名为FAST的新训练方法，专门针对指令模型优化稀疏自编码器（SAEs），显著提升了重构质量和特征可解释性。", "motivation": "现有稀疏自编码器训练方法主要针对基础模型，应用于指令模型时效果不佳，需要一种专门优化的方法。", "method": "提出FAST方法，通过调整训练过程以匹配指令模型的数据分布和激活模式。", "result": "在Qwen2.5-7B-Instruct上，FAST的重构误差显著低于基线方法；在Llama3.2-3B-Instruct上，高质量特征比例更高。", "conclusion": "FAST不仅提升了性能，还揭示了通过干预特殊令牌激活改善模型输出的新机会。"}}
{"id": "2506.07576", "pdf": "https://arxiv.org/pdf/2506.07576", "abs": "https://arxiv.org/abs/2506.07576", "authors": ["Boyu Chen", "Siran Chen", "Kunchang Li", "Qinglin Xu", "Yu Qiao", "Yali Wang"], "title": "Super Encoding Network: Recursive Association of Multi-Modal Encoders for Video Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Video understanding has been considered as one critical step towards world\nmodeling, which is an important long-term problem in AI research. Recently,\nmulti-modal foundation models have shown such potential via large-scale\npretraining. However, these models simply align encoders of different\nmodalities via contrastive learning, while lacking deeper multi-modal\ninteractions, which is critical for understanding complex target movements with\ndiversified video scenes. To fill this gap, we propose a unified Super Encoding\nNetwork (SEN) for video understanding, which builds up such distinct\ninteractions through recursive association of multi-modal encoders in the\nfoundation models. Specifically, we creatively treat those well-trained\nencoders as \"super neurons\" in our SEN. Via designing a Recursive Association\n(RA) block, we progressively fuse multi-modalities with the input video, based\non knowledge integrating, distributing, and prompting of super neurons in a\nrecursive manner. In this way, our SEN can effectively encode deeper\nmulti-modal interactions, for prompting various video understanding tasks in\ndownstream. Extensive experiments show that, our SEN can remarkably boost the\nfour most representative video tasks, including tracking, recognition,\nchatting, and editing, e.g., for pixel-level tracking, the average jaccard\nindex improves 2.7%, temporal coherence(TC) drops 8.8% compared to the popular\nCaDeX++ approach. For one-shot video editing, textual alignment improves 6.4%,\nand frame consistency increases 4.1% compared to the popular TuneA-Video\napproach.", "AI": {"tldr": "提出了一种统一的Super Encoding Network (SEN)，通过递归关联多模态编码器，增强视频理解任务的表现。", "motivation": "现有多模态基础模型仅通过对比学习对齐不同模态的编码器，缺乏更深层次的多模态交互，难以理解复杂视频场景中的目标运动。", "method": "将预训练编码器视为“超级神经元”，设计递归关联块（RA），逐步融合多模态信息，通过知识整合、分发和提示实现多模态交互。", "result": "SEN显著提升了四种代表性视频任务（跟踪、识别、聊天和编辑）的性能，例如跟踪任务的Jaccard指数提升2.7%，编辑任务的文本对齐提升6.4%。", "conclusion": "SEN通过递归多模态交互，有效提升了视频理解能力，为下游任务提供了强大的支持。"}}
{"id": "2506.06383", "pdf": "https://arxiv.org/pdf/2506.06383", "abs": "https://arxiv.org/abs/2506.06383", "authors": ["Qian Huang", "King Wang Poon"], "title": "Human and AI collaboration in Fitness Education:A Longitudinal Study with a Pilates Instructor", "categories": ["cs.CY", "cs.AI"], "comment": "19 pages, 5 figures", "summary": "Artificial intelligence is poised to transform teaching and coaching\npractices,yet its optimal role alongside human expertise remains unclear.This\nstudy investigates human and AI collaboration in fitness education through a\none year qualitative case study with a Pilates instructor.The researcher\nparticipated in the instructor classes and conducted biweekly semi structured\ninterviews to explore how generative AI could be integrated into class planning\nand instruction.", "AI": {"tldr": "研究探讨了人工智能与人类专家在健身教育中的协作，通过一年的定性案例研究，分析生成式AI如何融入普拉提课程规划和教学。", "motivation": "人工智能有望改变教学和指导实践，但其与人类专业知识的协同作用尚不明确。", "method": "采用为期一年的定性案例研究，研究者参与普拉提课程并进行双周半结构化访谈。", "result": "研究发现生成式AI可以有效地融入课程规划和教学，但具体方式需进一步探索。", "conclusion": "AI与人类协作在健身教育中具有潜力，但需明确其角色和边界。"}}
{"id": "2506.07712", "pdf": "https://arxiv.org/pdf/2506.07712", "abs": "https://arxiv.org/abs/2506.07712", "authors": ["Renjie Luo", "Jiaxi Li", "Chen Huang", "Wei Lu"], "title": "Through the Valley: Path to Effective Long CoT Training for Small Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Long chain-of-thought (CoT) supervision has become a common strategy to\nenhance reasoning in language models. While effective for large models, we\nidentify a phenomenon we call Long CoT Degradation, in which small language\nmodels (SLMs; <=3B parameters) trained on limited long CoT data experience\nsignificant performance deterioration. Through extensive experiments on the\nQwen2.5, LLaMA3 and Gemma3 families, we demonstrate that this degradation is\nwidespread across SLMs. In some settings, models trained on only 8k long CoT\nexamples lose up to 75% of their original performance before fine-tuning.\nStrikingly, we further observe that for some particularly small models, even\ntraining on 220k long CoT examples fails to recover or surpass their original\nperformance prior to fine-tuning. Our analysis attributes this effect to error\naccumulation: while longer responses increase the capacity for multi-step\nreasoning, they also amplify the risk of compounding mistakes. Furthermore, we\nfind that Long CoT Degradation may negatively impacts downstream reinforcement\nlearning (RL), although this can be alleviated by sufficiently scaled\nsupervised fine-tuning (SFT). Our findings challenge common assumptions about\nthe benefits of long CoT training for SLMs and offer practical guidance for\nbuilding more effective small-scale reasoning models.", "AI": {"tldr": "小语言模型（SLMs）在长链思维（CoT）监督训练中会出现性能显著下降的现象，称为Long CoT Degradation。实验表明，这种现象普遍存在于SLMs中，且错误累积是主要原因。", "motivation": "研究长链思维监督训练对小语言模型性能的影响，揭示其潜在问题。", "method": "通过Qwen2.5、LLaMA3和Gemma3等模型进行实验，分析长CoT训练对SLMs性能的影响。", "result": "SLMs在长CoT训练中性能显著下降，某些情况下甚至无法恢复原始性能。错误累积是主要原因。", "conclusion": "长CoT训练对小语言模型可能有害，需谨慎使用，并建议通过足够规模的监督微调（SFT）缓解问题。"}}
{"id": "2506.07590", "pdf": "https://arxiv.org/pdf/2506.07590", "abs": "https://arxiv.org/abs/2506.07590", "authors": ["Jiacheng Shi", "Yanfu Zhang", "Huajie Shao", "Ashley Gao"], "title": "Explore the vulnerability of black-box models via diffusion models", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Recent advancements in diffusion models have enabled high-fidelity and\nphotorealistic image generation across diverse applications. However, these\nmodels also present security and privacy risks, including copyright violations,\nsensitive information leakage, and the creation of harmful or offensive content\nthat could be exploited maliciously. In this study, we uncover a novel security\nthreat where an attacker leverages diffusion model APIs to generate synthetic\nimages, which are then used to train a high-performing substitute model. This\nenables the attacker to execute model extraction and transfer-based adversarial\nattacks on black-box classification models with minimal queries, without\nneeding access to the original training data. The generated images are\nsufficiently high-resolution and diverse to train a substitute model whose\noutputs closely match those of the target model. Across the seven benchmarks,\nincluding CIFAR and ImageNet subsets, our method shows an average improvement\nof 27.37% over state-of-the-art methods while using just 0.01 times of the\nquery budget, achieving a 98.68% success rate in adversarial attacks on the\ntarget model.", "AI": {"tldr": "该研究揭示了一种新的安全威胁，攻击者利用扩散模型API生成合成图像，训练高性能替代模型，从而对黑盒分类模型进行模型提取和对抗攻击。", "motivation": "扩散模型的高保真图像生成能力带来了安全和隐私风险，如版权侵犯和敏感信息泄露。研究旨在揭示这些模型可能被恶意利用的新威胁。", "method": "攻击者通过扩散模型API生成高质量合成图像，用于训练替代模型，实现对黑盒模型的提取和对抗攻击。", "result": "在七个基准测试中，该方法平均提升27.37%，仅需0.01倍的查询预算，对抗攻击成功率达98.68%。", "conclusion": "扩散模型API可能被滥用，导致严重的安全和隐私问题，需引起重视并采取防范措施。"}}
{"id": "2506.07719", "pdf": "https://arxiv.org/pdf/2506.07719", "abs": "https://arxiv.org/abs/2506.07719", "authors": ["Mengyang Qiu", "Tran Minh Nguyen", "Zihao Huang", "Zelong Li", "Yang Gu", "Qingyu Gao", "Siliang Liu", "Jungyeul Park"], "title": "Multilingual Grammatical Error Annotation: Combining Language-Agnostic Framework with Language-Specific Flexibility", "categories": ["cs.CL"], "comment": "BEA2025", "summary": "Grammatical Error Correction (GEC) relies on accurate error annotation and\nevaluation, yet existing frameworks, such as $\\texttt{errant}$, face\nlimitations when extended to typologically diverse languages. In this paper, we\nintroduce a standardized, modular framework for multilingual grammatical error\nannotation. Our approach combines a language-agnostic foundation with\nstructured language-specific extensions, enabling both consistency and\nflexibility across languages. We reimplement $\\texttt{errant}$ using\n$\\texttt{stanza}$ to support broader multilingual coverage, and demonstrate the\nframework's adaptability through applications to English, German, Czech,\nKorean, and Chinese, ranging from general-purpose annotation to more customized\nlinguistic refinements. This work supports scalable and interpretable GEC\nannotation across languages and promotes more consistent evaluation in\nmultilingual settings. The complete codebase and annotation tools can be\naccessed at https://github.com/open-writing-evaluation/jp_errant_bea.", "AI": {"tldr": "提出了一种标准化的、模块化的多语言语法错误标注框架，结合语言无关的基础和语言特定的扩展，支持多语言覆盖。", "motivation": "现有语法错误标注框架（如errant）在扩展到类型多样的语言时存在局限性，需要一种更灵活且一致的方法。", "method": "采用语言无关的基础与结构化语言特定扩展相结合的方法，重新实现errant以支持更广泛的多语言覆盖。", "result": "框架成功应用于英语、德语、捷克语、韩语和中文，支持从通用标注到定制化语言细化。", "conclusion": "该工作促进了跨语言的可扩展和可解释的语法错误标注，提升了多语言环境下评估的一致性。"}}
{"id": "2506.07600", "pdf": "https://arxiv.org/pdf/2506.07600", "abs": "https://arxiv.org/abs/2506.07600", "authors": ["Nianbo Zeng", "Haowen Hou", "Fei Richard Yu", "Si Shi", "Ying Tiffany He"], "title": "SceneRAG: Scene-level Retrieval-Augmented Generation for Video Understanding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Despite recent advances in retrieval-augmented generation (RAG) for video\nunderstanding, effectively understanding long-form video content remains\nunderexplored due to the vast scale and high complexity of video data. Current\nRAG approaches typically segment videos into fixed-length chunks, which often\ndisrupts the continuity of contextual information and fails to capture\nauthentic scene boundaries. Inspired by the human ability to naturally organize\ncontinuous experiences into coherent scenes, we present SceneRAG, a unified\nframework that leverages large language models to segment videos into\nnarrative-consistent scenes by processing ASR transcripts alongside temporal\nmetadata. SceneRAG further sharpens these initial boundaries through\nlightweight heuristics and iterative correction. For each scene, the framework\nfuses information from both visual and textual modalities to extract entity\nrelations and dynamically builds a knowledge graph, enabling robust multi-hop\nretrieval and generation that account for long-range dependencies. Experiments\non the LongerVideos benchmark, featuring over 134 hours of diverse content,\nconfirm that SceneRAG substantially outperforms prior baselines, achieving a\nwin rate of up to 72.5 percent on generation tasks.", "AI": {"tldr": "SceneRAG是一个基于大语言模型的框架，通过处理视频的ASR转录和时间元数据，将视频分割为叙事一致的场景，并融合视觉和文本信息构建知识图谱，显著提升长视频理解性能。", "motivation": "当前检索增强生成（RAG）方法在处理长视频时，固定长度分段会破坏上下文连续性，无法捕捉真实场景边界。", "method": "SceneRAG利用大语言模型分割视频为叙事一致的场景，结合轻量级启发式和迭代校正优化边界，并融合多模态信息构建动态知识图谱。", "result": "在LongerVideos基准测试中，SceneRAG以72.5%的胜率显著优于现有基线。", "conclusion": "SceneRAG通过场景分割和多模态融合，有效解决了长视频理解的挑战。"}}
{"id": "2506.06387", "pdf": "https://arxiv.org/pdf/2506.06387", "abs": "https://arxiv.org/abs/2506.06387", "authors": ["Baptiste Chatelier", "Vincent Corlay", "Musa Furkan Keskin", "Matthieu Crussière", "Henk Wymeersch", "Luc Le Magoarou"], "title": "Model-based Neural Data Augmentation for sub-wavelength Radio Localization", "categories": ["eess.SP", "cs.AI", "cs.IT", "cs.LG", "math.IT"], "comment": null, "summary": "The increasing deployment of large antenna arrays at base stations has\nsignificantly improved the spatial resolution and localization accuracy of\nradio-localization methods. However, traditional signal processing techniques\nstruggle in complex radio environments, particularly in scenarios dominated by\nnon line of sight (NLoS) propagation paths, resulting in degraded localization\naccuracy. Recent developments in machine learning have facilitated the\ndevelopment of machine learning-assisted localization techniques, enhancing\nlocalization accuracy in complex radio environments. However, these methods\noften involve substantial computational complexity during both the training and\ninference phases. This work extends the well-established fingerprinting-based\nlocalization framework by simultaneously reducing its memory requirements and\nimproving its accuracy. Specifically, a model-based neural network is used to\nlearn the location-to-channel mapping, and then serves as a generative neural\nchannel model. This generative model augments the fingerprinting comparison\ndictionary while reducing the memory requirements. The proposed method\noutperforms fingerprinting baselines by achieving sub-wavelength localization\naccuracy, even in NLoS environments. Remarkably, it offers an improvement by\nseveral orders of magnitude in localization accuracy, while simultaneously\nreducing memory requirements by an order of magnitude compared to classical\nfingerprinting methods.", "AI": {"tldr": "本文提出了一种基于模型神经网络的指纹定位方法，显著提升了复杂无线电环境中的定位精度，同时降低了内存需求。", "motivation": "传统信号处理技术在复杂无线电环境（尤其是非视距传播路径主导的场景）中定位精度下降，而现有机器学习方法计算复杂度高。", "method": "使用基于模型的神经网络学习位置到信道的映射，作为生成神经信道模型，增强指纹比较字典并减少内存需求。", "result": "在非视距环境中实现亚波长级定位精度，定位精度提升数个数量级，内存需求比传统指纹方法降低一个数量级。", "conclusion": "该方法在提升定位精度的同时显著降低了内存需求，适用于复杂无线电环境。"}}
{"id": "2506.07726", "pdf": "https://arxiv.org/pdf/2506.07726", "abs": "https://arxiv.org/abs/2506.07726", "authors": ["Vincenzo Timmel", "Manfred Vogel", "Daniel Perruchoud", "Reza Kakooee"], "title": "Swiss Parliaments Corpus Re-Imagined (SPC_R): Enhanced Transcription with RAG-based Correction and Predicted BLEU", "categories": ["cs.CL"], "comment": null, "summary": "This paper presents a new long-form release of the Swiss Parliaments Corpus,\nconverting entire multi-hour Swiss German debate sessions (each aligned with\nthe official session protocols) into high-quality speech-text pairs. Our\npipeline starts by transcribing all session audio into Standard German using\nWhisper Large-v3 under high-compute settings. We then apply a two-step GPT-4o\ncorrection process: first, GPT-4o ingests the raw Whisper output alongside the\nofficial protocols to refine misrecognitions, mainly named entities. Second, a\nseparate GPT-4o pass evaluates each refined segment for semantic completeness.\nWe filter out any segments whose Predicted BLEU score (derived from Whisper's\naverage token log-probability) and GPT-4o evaluation score fall below a certain\nthreshold. The final corpus contains 801 hours of audio, of which 751 hours\npass our quality control. Compared to the original sentence-level SPC release,\nour long-form dataset achieves a 6-point BLEU improvement, demonstrating the\npower of combining robust ASR, LLM-based correction, and data-driven filtering\nfor low-resource, domain-specific speech corpora.", "AI": {"tldr": "论文介绍了瑞士议会语料库的长文本版本，通过结合Whisper和GPT-4o技术，将多小时的瑞士德语辩论转换为高质量的语音-文本对，并展示了显著的质量提升。", "motivation": "为低资源、特定领域的语音语料库提供高质量的长文本数据，解决传统方法在转录和语义完整性上的不足。", "method": "使用Whisper Large-v3进行初步转录，再通过两步骤的GPT-4o修正（命名实体修正和语义完整性评估），最后基于预测BLEU分数和GPT-4o评分进行过滤。", "result": "最终语料库包含801小时音频，其中751小时通过质量控制，相比原始版本BLEU分数提升6分。", "conclusion": "结合ASR、LLM修正和数据驱动过滤，能够显著提升低资源领域语音语料库的质量。"}}
{"id": "2506.07603", "pdf": "https://arxiv.org/pdf/2506.07603", "abs": "https://arxiv.org/abs/2506.07603", "authors": ["Jianhui Wei", "Zikai Xiao", "Danyu Sun", "Luqi Gong", "Zongxin Yang", "Zuozhu Liu", "Jian Wu"], "title": "SurgBench: A Unified Large-Scale Benchmark for Surgical Video Analysis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Surgical video understanding is pivotal for enabling automated intraoperative\ndecision-making, skill assessment, and postoperative quality improvement.\nHowever, progress in developing surgical video foundation models (FMs) remains\nhindered by the scarcity of large-scale, diverse datasets for pretraining and\nsystematic evaluation. In this paper, we introduce \\textbf{SurgBench}, a\nunified surgical video benchmarking framework comprising a pretraining dataset,\n\\textbf{SurgBench-P}, and an evaluation benchmark, \\textbf{SurgBench-E}.\nSurgBench offers extensive coverage of diverse surgical scenarios, with\nSurgBench-P encompassing 53 million frames across 22 surgical procedures and 11\nspecialties, and SurgBench-E providing robust evaluation across six categories\n(phase classification, camera motion, tool recognition, disease diagnosis,\naction classification, and organ detection) spanning 72 fine-grained tasks.\nExtensive experiments reveal that existing video FMs struggle to generalize\nacross varied surgical video analysis tasks, whereas pretraining on SurgBench-P\nyields substantial performance improvements and superior cross-domain\ngeneralization to unseen procedures and modalities. Our dataset and code are\navailable upon request.", "AI": {"tldr": "SurgBench是一个统一的手术视频基准框架，包含预训练数据集SurgBench-P和评估基准SurgBench-E，旨在解决手术视频基础模型开发中的数据稀缺问题。", "motivation": "手术视频理解对自动化术中决策、技能评估和术后质量改进至关重要，但目前缺乏大规模多样化数据集阻碍了进展。", "method": "提出SurgBench框架，包含53百万帧的预训练数据集和覆盖72个任务的评估基准，用于系统评估和预训练。", "result": "实验表明，现有视频基础模型在多样化任务中表现不佳，而基于SurgBench-P的预训练显著提升了性能并增强了跨领域泛化能力。", "conclusion": "SurgBench为手术视频分析提供了全面的数据集和评估标准，推动了基础模型的开发和应用。"}}
{"id": "2506.06390", "pdf": "https://arxiv.org/pdf/2506.06390", "abs": "https://arxiv.org/abs/2506.06390", "authors": ["Liangliang Chen", "Zhihao Qin", "Yiming Guo", "Jacqueline Rohde", "Ying Zhang"], "title": "Benchmarking Large Language Models on Homework Assessment in Circuit Analysis", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have the potential to revolutionize various\nfields, including code development, robotics, finance, and education, due to\ntheir extensive prior knowledge and rapid advancements. This paper investigates\nhow LLMs can be leveraged in engineering education. Specifically, we benchmark\nthe capabilities of different LLMs, including GPT-3.5 Turbo, GPT-4o, and Llama\n3 70B, in assessing homework for an undergraduate-level circuit analysis\ncourse. We have developed a novel dataset consisting of official reference\nsolutions and real student solutions to problems from various topics in circuit\nanalysis. To overcome the limitations of image recognition in current\nstate-of-the-art LLMs, the solutions in the dataset are converted to LaTeX\nformat. Using this dataset, a prompt template is designed to test five metrics\nof student solutions: completeness, method, final answer, arithmetic error, and\nunits. The results show that GPT-4o and Llama 3 70B perform significantly\nbetter than GPT-3.5 Turbo across all five metrics, with GPT-4o and Llama 3 70B\neach having distinct advantages in different evaluation aspects. Additionally,\nwe present insights into the limitations of current LLMs in several aspects of\ncircuit analysis. Given the paramount importance of ensuring reliability in\nLLM-generated homework assessment to avoid misleading students, our results\nestablish benchmarks and offer valuable insights for the development of a\nreliable, personalized tutor for circuit analysis -- a focus of our future\nwork. Furthermore, the proposed evaluation methods can be generalized to a\nbroader range of courses for engineering education in the future.", "AI": {"tldr": "论文研究了如何利用大型语言模型（LLMs）在工程教育中评估电路分析课程的作业，比较了GPT-3.5 Turbo、GPT-4o和Llama 3 70B的性能，并提出了改进方向。", "motivation": "探索LLMs在工程教育中的应用潜力，尤其是电路分析课程的作业评估，以提升教育质量和个性化辅导。", "method": "开发包含参考和学生解答的数据集，转换为LaTeX格式，设计提示模板测试五个评估指标。", "result": "GPT-4o和Llama 3 70B在所有指标上显著优于GPT-3.5 Turbo，各有优势，但也揭示了LLMs的局限性。", "conclusion": "研究为开发可靠的个性化电路分析辅导工具奠定了基础，并提供了可推广到其他工程课程的方法。"}}
{"id": "2506.07751", "pdf": "https://arxiv.org/pdf/2506.07751", "abs": "https://arxiv.org/abs/2506.07751", "authors": ["Silin Gao", "Antoine Bosselut", "Samy Bengio", "Emmanuel Abbe"], "title": "Augmenting LLMs' Reasoning by Reinforcing Abstract Thinking", "categories": ["cs.CL", "cs.AI", "cs.SC"], "comment": "Under review", "summary": "Recent studies have shown that large language models (LLMs), especially\nsmaller ones, often lack robustness in their reasoning. I.e., they tend to\nexperience performance drops when faced with distribution shifts, such as\nchanges to numerical or nominal variables, or insertions of distracting\nclauses. A possible strategy to address this involves generating synthetic data\nto further \"instantiate\" reasoning problems on potential variations. In\ncontrast, our approach focuses on \"abstracting\" reasoning problems. This not\nonly helps counteract distribution shifts but also facilitates the connection\nto symbolic tools for deriving solutions. We find that this abstraction process\nis better acquired through reinforcement learning (RL) than just supervised\nfine-tuning, which often fails to produce faithful abstractions. Our method,\nAbstraL -- which promotes abstract reasoning in LLMs using RL on granular\nabstraction data -- significantly mitigates performance degradation on recent\nGSM perturbation benchmarks.", "AI": {"tldr": "论文提出了一种通过强化学习（RL）促进抽象推理的方法（AbstraL），以应对语言模型在分布变化下的性能下降问题。", "motivation": "解决小规模语言模型在面对分布变化（如数值或名义变量变化、干扰子句插入）时推理鲁棒性不足的问题。", "method": "采用强化学习（RL）而非监督微调，通过抽象化推理问题来生成粒度化的抽象数据（AbstraL）。", "result": "AbstraL方法显著减轻了在GSM扰动基准测试中的性能下降。", "conclusion": "抽象化推理问题并通过RL学习，能有效提升语言模型在分布变化下的鲁棒性。"}}
{"id": "2506.07611", "pdf": "https://arxiv.org/pdf/2506.07611", "abs": "https://arxiv.org/abs/2506.07611", "authors": ["Yuan Zhou", "Junbao Zhou", "Qingshan Xu", "Kesen Zhao", "Yuxuan Wang", "Hao Fei", "Richang Hong", "Hanwang Zhang"], "title": "DragNeXt: Rethinking Drag-Based Image Editing", "categories": ["cs.CV"], "comment": null, "summary": "Drag-Based Image Editing (DBIE), which allows users to manipulate images by\ndirectly dragging objects within them, has recently attracted much attention\nfrom the community. However, it faces two key challenges:\n(\\emph{\\textcolor{magenta}{i}}) point-based drag is often highly ambiguous and\ndifficult to align with users' intentions; (\\emph{\\textcolor{magenta}{ii}})\ncurrent DBIE methods primarily rely on alternating between motion supervision\nand point tracking, which is not only cumbersome but also fails to produce\nhigh-quality results. These limitations motivate us to explore DBIE from a new\nperspective -- redefining it as deformation, rotation, and translation of\nuser-specified handle regions. Thereby, by requiring users to explicitly\nspecify both drag areas and types, we can effectively address the ambiguity\nissue. Furthermore, we propose a simple-yet-effective editing framework, dubbed\n\\textcolor{SkyBlue}{\\textbf{DragNeXt}}. It unifies DBIE as a Latent Region\nOptimization (LRO) problem and solves it through Progressive Backward\nSelf-Intervention (PBSI), simplifying the overall procedure of DBIE while\nfurther enhancing quality by fully leveraging region-level structure\ninformation and progressive guidance from intermediate drag states. We validate\n\\textcolor{SkyBlue}{\\textbf{DragNeXt}} on our NextBench, and extensive\nexperiments demonstrate that our proposed method can significantly outperform\nexisting approaches. Code will be released on github.", "AI": {"tldr": "论文提出了一种新的基于拖拽的图像编辑方法DragNeXt，通过明确指定拖拽区域和类型解决模糊性问题，并简化了编辑流程。", "motivation": "当前基于拖拽的图像编辑方法存在模糊性和繁琐的交替监督问题，导致结果质量不高。", "method": "将拖拽编辑重新定义为用户指定区域的变形、旋转和平移，并提出Latent Region Optimization（LRO）框架和Progressive Backward Self-Intervention（PBSI）方法。", "result": "在NextBench上的实验表明，DragNeXt显著优于现有方法。", "conclusion": "DragNeXt通过区域级结构信息和渐进式指导，简化了编辑流程并提升了结果质量。"}}
{"id": "2506.06391", "pdf": "https://arxiv.org/pdf/2506.06391", "abs": "https://arxiv.org/abs/2506.06391", "authors": ["John Mavi", "Diana Teodora Găitan", "Sergio Coronado"], "title": "From Rogue to Safe AI: The Role of Explicit Refusals in Aligning LLMs with International Humanitarian Law", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) are widely used across sectors, yet their\nalignment with International Humanitarian Law (IHL) is not well understood.\nThis study evaluates eight leading LLMs on their ability to refuse prompts that\nexplicitly violate these legal frameworks, focusing also on helpfulness - how\nclearly and constructively refusals are communicated. While most models\nrejected unlawful requests, the clarity and consistency of their responses\nvaried. By revealing the model's rationale and referencing relevant legal or\nsafety principles, explanatory refusals clarify the system's boundaries, reduce\nambiguity, and help prevent misuse. A standardised system-level safety prompt\nsignificantly improved the quality of the explanations expressed within\nrefusals in most models, highlighting the effectiveness of lightweight\ninterventions. However, more complex prompts involving technical language or\nrequests for code revealed ongoing vulnerabilities. These findings contribute\nto the development of safer, more transparent AI systems and propose a\nbenchmark to evaluate the compliance of LLM with IHL.", "AI": {"tldr": "研究评估了8种主流大语言模型（LLM）在国际人道法（IHL）合规性上的表现，发现模型普遍能拒绝非法请求，但回应清晰度和一致性不一。标准化安全提示显著提升了拒绝解释的质量，但复杂请求仍存在漏洞。", "motivation": "大语言模型（LLM）在各领域广泛应用，但其与国际人道法（IHL）的合规性尚未充分研究。", "method": "评估8种LLM对明确违反IHL的提示的拒绝能力，关注回应的清晰度和建设性。引入标准化安全提示以测试改进效果。", "result": "多数模型能拒绝非法请求，但回应质量不一。标准化提示显著提升解释质量，但对复杂请求（如技术语言或代码）仍存在漏洞。", "conclusion": "研究为开发更安全、透明的AI系统提供了基准，并展示了轻量级干预的有效性，但复杂请求仍需进一步改进。"}}
{"id": "2506.07795", "pdf": "https://arxiv.org/pdf/2506.07795", "abs": "https://arxiv.org/abs/2506.07795", "authors": ["Xiaotian Ye", "Mengqi Zhang", "Shu Wu"], "title": "LLM Unlearning Should Be Form-Independent", "categories": ["cs.CL", "cs.CR", "cs.LG"], "comment": null, "summary": "Large Language Model (LLM) unlearning aims to erase or suppress undesirable\nknowledge within the model, offering promise for controlling harmful or private\ninformation to prevent misuse. However, recent studies highlight its limited\nefficacy in real-world scenarios, hindering practical adoption. In this study,\nwe identify a pervasive issue underlying many downstream failures: the\neffectiveness of existing unlearning methods heavily depends on the form of\ntraining samples and frequently fails to generalize to alternate expressions of\nthe same knowledge. We formally characterize this problem as Form-Dependent\nBias and systematically investigate its specific manifestation patterns across\nvarious downstream tasks. To quantify its prevalence and support future\nresearch, we introduce ORT, a novel benchmark designed to evaluate the\nrobustness of unlearning methods against variations in knowledge expression.\nResults reveal that Form-Dependent Bias is both widespread and severe among\ncurrent techniques.\n  We argue that LLM unlearning should be form-independent to address the\nendless forms of downstream tasks encountered in real-world security-critical\nscenarios. Towards this goal, we introduce Rank-one Concept Redirection (ROCR),\na novel training-free method, as a promising solution path. ROCR performs\nunlearning by targeting the invariants in downstream tasks, specifically the\nactivated dangerous concepts. It is capable of modifying model parameters\nwithin seconds to redirect the model's perception of a specific unlearning\ntarget concept to another harmless concept. Extensive experiments demonstrate\nthat ROCR significantly improves unlearning effectiveness compared to\ntraditional methods while generating highly natural outputs.", "AI": {"tldr": "该论文研究了大型语言模型（LLM）遗忘技术中的形式依赖偏差问题，并提出了一种新的无训练方法ROCR来解决这一问题。", "motivation": "现有遗忘方法在实际应用中效果有限，主要因为其依赖于训练样本的形式，无法泛化到同一知识的不同表达形式。", "method": "论文提出了ROCR方法，通过重定向模型参数中的危险概念来实现无训练遗忘。", "result": "实验表明，ROCR显著提升了遗忘效果，并生成更自然的输出。", "conclusion": "LLM遗忘应具备形式独立性，ROCR为解决这一问题提供了有效路径。"}}
{"id": "2506.07612", "pdf": "https://arxiv.org/pdf/2506.07612", "abs": "https://arxiv.org/abs/2506.07612", "authors": ["Zikang Leng", "Archith Iyer", "Thomas Plötz"], "title": "Scaling Human Activity Recognition: A Comparative Evaluation of Synthetic Data Generation and Augmentation Techniques", "categories": ["cs.CV"], "comment": null, "summary": "Human activity recognition (HAR) is often limited by the scarcity of labeled\ndatasets due to the high cost and complexity of real-world data collection. To\nmitigate this, recent work has explored generating virtual inertial measurement\nunit (IMU) data via cross-modality transfer. While video-based and\nlanguage-based pipelines have each shown promise, they differ in assumptions\nand computational cost. Moreover, their effectiveness relative to traditional\nsensor-level data augmentation remains unclear. In this paper, we present a\ndirect comparison between these two virtual IMU generation approaches against\nclassical data augmentation techniques. We construct a large-scale virtual IMU\ndataset spanning 100 diverse activities from Kinetics-400 and simulate sensor\nsignals at 22 body locations. The three data generation strategies are\nevaluated on benchmark HAR datasets (UTD-MHAD, PAMAP2, HAD-AW) using four\npopular models. Results show that virtual IMU data significantly improves\nperformance over real or augmented data alone, particularly under limited-data\nconditions. We offer practical guidance on choosing data generation strategies\nand highlight the distinct advantages and disadvantages of each approach.", "AI": {"tldr": "本文比较了两种虚拟IMU数据生成方法（视频和语言）与传统数据增强技术，发现虚拟IMU数据在有限数据条件下显著提升HAR性能。", "motivation": "解决HAR中标记数据稀缺问题，探索虚拟IMU数据生成方法的有效性。", "method": "构建大规模虚拟IMU数据集，比较视频、语言和传统数据增强方法，并在多个HAR数据集上评估。", "result": "虚拟IMU数据显著优于单独使用真实或增强数据，尤其在数据有限时。", "conclusion": "提供选择数据生成策略的实用建议，并分析各方法的优缺点。"}}
{"id": "2506.07801", "pdf": "https://arxiv.org/pdf/2506.07801", "abs": "https://arxiv.org/abs/2506.07801", "authors": ["Iustin Sirbu", "Robert-Adrian Popovici", "Cornelia Caragea", "Stefan Trausan-Matu", "Traian Rebedea"], "title": "MultiMatch: Multihead Consistency Regularization Matching for Semi-Supervised Text Classification", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "comment": null, "summary": "We introduce MultiMatch, a novel semi-supervised learning (SSL) algorithm\ncombining the paradigms of co-training and consistency regularization with\npseudo-labeling. At its core, MultiMatch features a three-fold pseudo-label\nweighting module designed for three key purposes: selecting and filtering\npseudo-labels based on head agreement and model confidence, and weighting them\naccording to the perceived classification difficulty. This novel module\nenhances and unifies three existing techniques -- heads agreement from\nMultihead Co-training, self-adaptive thresholds from FreeMatch, and Average\nPseudo-Margins from MarginMatch -- resulting in a holistic approach that\nimproves robustness and performance in SSL settings. Experimental results on\nbenchmark datasets highlight the superior performance of MultiMatch, achieving\nstate-of-the-art results on 9 out of 10 setups from 5 natural language\nprocessing datasets and ranking first according to the Friedman test among 19\nmethods. Furthermore, MultiMatch demonstrates exceptional robustness in highly\nimbalanced settings, outperforming the second-best approach by 3.26% -- and\ndata imbalance is a key factor for many text classification tasks.", "AI": {"tldr": "MultiMatch是一种结合协同训练和一致性正则化的半监督学习算法，通过三重重伪标签加权模块提升性能。", "motivation": "解决半监督学习中伪标签选择和加权的问题，提升模型在数据不平衡情况下的鲁棒性。", "method": "结合协同训练和一致性正则化，引入三重重伪标签加权模块，整合多头协同训练、自适应阈值和平均伪边际技术。", "result": "在5个NLP数据集的10个设置中，9个达到最优性能，且在数据不平衡情况下表现突出。", "conclusion": "MultiMatch是一种高效且鲁棒的半监督学习算法，特别适用于文本分类任务。"}}
{"id": "2506.07627", "pdf": "https://arxiv.org/pdf/2506.07627", "abs": "https://arxiv.org/abs/2506.07627", "authors": ["Haotong Qin", "Cheng Hu", "Michele Magno"], "title": "Event-Priori-Based Vision-Language Model for Efficient Visual Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Large Language Model (LLM)-based Vision-Language Models (VLMs) have\nsubstantially extended the boundaries of visual understanding capabilities.\nHowever, their high computational demands hinder deployment on\nresource-constrained edge devices. A key source of inefficiency stems from the\nVLM's need to process dense and redundant visual information. Visual inputs\ncontain significant regions irrelevant to text semantics, rendering the\nassociated computations ineffective for inference. This paper introduces a\nnovel Event-Priori-Based Vision-Language Model, termed EP-VLM. Its core\ncontribution is a novel mechanism leveraging motion priors derived from dynamic\nevent vision to enhance VLM efficiency. Inspired by human visual cognition,\nEP-VLM first employs event data to guide the patch-wise sparsification of RGB\nvisual inputs, progressively concentrating VLM computation on salient regions\nof the visual input. Subsequently, we construct a position-preserving\ntokenization strategy for the visual encoder within the VLM architecture. This\nstrategy processes the event-guided, unstructured, sparse visual input while\naccurately preserving positional understanding within the visual input.\nExperimental results demonstrate that EP-VLM achieves significant efficiency\nimprovements while maintaining nearly lossless accuracy compared to baseline\nmodels from the Qwen2-VL series. For instance, against the original\nQwen2-VL-2B, EP-VLM achieves 50% FLOPs savings while retaining 98% of the\noriginal accuracy on the RealWorldQA dataset. This work demonstrates the\npotential of event-based vision priors for improving VLM inference efficiency,\npaving the way for creating more efficient and deployable VLMs for sustainable\nvisual understanding at the edge.", "AI": {"tldr": "EP-VLM利用动态事件视觉的运动先验提升视觉语言模型效率，通过事件数据引导视觉输入的稀疏化，显著减少计算量，同时保持高精度。", "motivation": "现有视觉语言模型（VLM）计算需求高，难以部署在资源受限的边缘设备上，且视觉输入中存在大量冗余信息。", "method": "EP-VLM采用事件数据引导RGB视觉输入的稀疏化，并设计位置保留的视觉编码器策略，集中计算于关键区域。", "result": "EP-VLM在Qwen2-VL-2B上节省50%计算量，同时保持98%的准确率。", "conclusion": "事件视觉先验可显著提升VLM效率，为边缘设备部署高效模型提供新思路。"}}
{"id": "2506.06398", "pdf": "https://arxiv.org/pdf/2506.06398", "abs": "https://arxiv.org/abs/2506.06398", "authors": ["Yin Li"], "title": "Theoretical Analysis of Positional Encodings in Transformer Models: Impact on Expressiveness and Generalization", "categories": ["cs.LG", "cs.AI", "68T07, 68Q32", "I.2.6; I.2.7; F.1.1"], "comment": null, "summary": "Positional encodings are a core part of transformer-based models, enabling\nprocessing of sequential data without recurrence. This paper presents a\ntheoretical framework to analyze how various positional encoding methods,\nincluding sinusoidal, learned, relative, and bias-based methods like Attention\nwith Linear Biases (ALiBi), impact a transformer's expressiveness,\ngeneralization ability, and extrapolation to longer sequences. Expressiveness\nis defined via function approximation, generalization bounds are established\nusing Rademacher complexity, and new encoding methods based on orthogonal\nfunctions, such as wavelets and Legendre polynomials, are proposed. The\nextrapolation capacity of existing and proposed encodings is analyzed,\nextending ALiBi's biasing approach to a unified theoretical context.\nExperimental evaluation on synthetic sequence-to-sequence tasks shows that\northogonal transform-based encodings outperform traditional sinusoidal\nencodings in generalization and extrapolation. This work addresses a critical\ngap in transformer theory, providing insights for design choices in natural\nlanguage processing, computer vision, and other transformer applications.", "AI": {"tldr": "本文提出了一个理论框架，分析不同位置编码方法对Transformer表达能力、泛化能力和长序列外推能力的影响，并提出基于正交函数的新编码方法。实验表明，正交变换编码优于传统正弦编码。", "motivation": "研究不同位置编码方法如何影响Transformer的性能，填补Transformer理论的空白，为自然语言处理、计算机视觉等领域的应用提供设计指导。", "method": "通过函数逼近定义表达能力，利用Rademacher复杂度建立泛化界限，提出基于正交函数（如小波和Legendre多项式）的新编码方法，并分析现有和提出编码的外推能力。", "result": "实验表明，基于正交变换的编码在泛化和外推方面优于传统的正弦编码。", "conclusion": "本文填补了Transformer理论的空白，为位置编码的设计提供了理论支持和实践指导。"}}
{"id": "2506.07818", "pdf": "https://arxiv.org/pdf/2506.07818", "abs": "https://arxiv.org/abs/2506.07818", "authors": ["Zhiyu Lin", "Zhengda Zhou", "Zhiyuan Zhao", "Tianrui Wan", "Yilun Ma", "Junyu Gao", "Xuelong Li"], "title": "WebUIBench: A Comprehensive Benchmark for Evaluating Multimodal Large Language Models in WebUI-to-Code", "categories": ["cs.CL"], "comment": null, "summary": "With the rapid advancement of Generative AI technology, Multimodal Large\nLanguage Models(MLLMs) have the potential to act as AI software engineers\ncapable of executing complex web application development. Considering that the\nmodel requires a confluence of multidimensional sub-capabilities to address the\nchallenges of various development phases, constructing a multi-view evaluation\nframework is crucial for accurately guiding the enhancement of development\nefficiency. However, existing benchmarks usually fail to provide an assessment\nof sub-capabilities and focus solely on webpage generation outcomes. In this\nwork, we draw inspiration from the principles of software engineering and\nfurther propose WebUIBench, a benchmark systematically designed to evaluate\nMLLMs in four key areas: WebUI Perception, HTML Programming,WebUI-HTML\nUnderstanding, and WebUI-to-Code. WebUIBench comprises 21K high-quality\nquestion-answer pairs derived from over 0.7K real-world websites. The extensive\nevaluation of 29 mainstream MLLMs uncovers the skill characteristics and\nvarious weakness that models encountered during the development process.", "AI": {"tldr": "论文提出了WebUIBench基准，用于系统评估多模态大语言模型在Web应用开发中的四个关键能力，填补了现有评估的不足。", "motivation": "现有基准通常仅关注网页生成结果，缺乏对多维子能力的评估，无法准确指导开发效率的提升。", "method": "基于软件工程原则，设计了WebUIBench基准，包含21K高质量问答对，覆盖四个关键领域：WebUI感知、HTML编程、WebUI-HTML理解和WebUI到代码转换。", "result": "对29个主流MLLM的广泛评估揭示了模型在开发过程中的技能特点和弱点。", "conclusion": "WebUIBench为MLLM在Web开发中的能力评估提供了系统性工具，有助于指导模型优化。"}}
{"id": "2506.07628", "pdf": "https://arxiv.org/pdf/2506.07628", "abs": "https://arxiv.org/abs/2506.07628", "authors": ["Weronika Smolak-Dyżewska", "Dawid Malarz", "Grzegorz Wilczyński", "Rafał Tobiasz", "Joanna Waczyńska", "Piotr Borycki", "Przemysław Spurek"], "title": "HuSc3D: Human Sculpture dataset for 3D object reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "3D scene reconstruction from 2D images is one of the most important tasks in\ncomputer graphics. Unfortunately, existing datasets and benchmarks concentrate\non idealized synthetic or meticulously captured realistic data. Such benchmarks\nfail to convey the inherent complexities encountered in newly acquired\nreal-world scenes. In such scenes especially those acquired outside, the\nbackground is often dynamic, and by popular usage of cell phone cameras, there\nmight be discrepancies in, e.g., white balance. To address this gap, we present\nHuSc3D, a novel dataset specifically designed for rigorous benchmarking of 3D\nreconstruction models under realistic acquisition challenges. Our dataset\nuniquely features six highly detailed, fully white sculptures characterized by\nintricate perforations and minimal textural and color variation. Furthermore,\nthe number of images per scene varies significantly, introducing the additional\nchallenge of limited training data for some instances alongside scenes with a\nstandard number of views. By evaluating popular 3D reconstruction methods on\nthis diverse dataset, we demonstrate the distinctiveness of HuSc3D in\neffectively differentiating model performance, particularly highlighting the\nsensitivity of methods to fine geometric details, color ambiguity, and varying\ndata availability--limitations often masked by more conventional datasets.", "AI": {"tldr": "HuSc3D是一个专为真实采集挑战设计的3D重建数据集，包含六个白色雕塑场景，用于评估模型对几何细节、颜色模糊和数据变化的敏感性。", "motivation": "现有数据集未能反映真实场景的复杂性（如动态背景、白平衡差异），因此需要HuSc3D填补这一空白。", "method": "数据集包含六个高度细节化的白色雕塑场景，图像数量差异显著，以模拟真实采集中的挑战。", "result": "评估显示HuSc3D能有效区分模型性能，揭示其对几何细节、颜色模糊和数据变化的敏感性。", "conclusion": "HuSc3D为3D重建提供了更真实的基准，突显了现有方法的局限性。"}}
{"id": "2506.07851", "pdf": "https://arxiv.org/pdf/2506.07851", "abs": "https://arxiv.org/abs/2506.07851", "authors": ["Yiju Guo", "Wenkai Yang", "Zexu Sun", "Ning Ding", "Zhiyuan Liu", "Yankai Lin"], "title": "Learning to Focus: Causal Attention Distillation via Gradient-Guided Token Pruning", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated significant improvements in\ncontextual understanding. However, their ability to attend to truly critical\ninformation during long-context reasoning and generation still falls behind the\npace. Specifically, our preliminary experiments reveal that certain distracting\npatterns can misdirect the model's attention during inference, and removing\nthese patterns substantially improves reasoning accuracy and generation\nquality. We attribute this phenomenon to spurious correlations in the training\ndata, which obstruct the model's capacity to infer authentic causal\ninstruction-response relationships. This phenomenon may induce redundant\nreasoning processes, potentially resulting in significant inference overhead\nand, more critically, the generation of erroneous or suboptimal responses. To\nmitigate this, we introduce a two-stage framework called Learning to Focus\n(LeaF) leveraging intervention-based inference to disentangle confounding\nfactors. In the first stage, LeaF employs gradient-based comparisons with an\nadvanced teacher to automatically identify confounding tokens based on causal\nrelationships in the training corpus. Then, in the second stage, it prunes\nthese tokens during distillation to enact intervention, aligning the student's\nattention with the teacher's focus distribution on truly critical context\ntokens. Experimental results demonstrate that LeaF not only achieves an\nabsolute improvement in various mathematical reasoning and code generation\nbenchmarks but also effectively suppresses attention to confounding tokens\nduring inference, yielding a more interpretable and reliable reasoning model.", "AI": {"tldr": "论文提出了一种名为LeaF的两阶段框架，通过干预式推理解决大型语言模型在长上下文推理中注意力分散的问题，显著提升了推理准确性和生成质量。", "motivation": "大型语言模型（LLMs）在上下文理解方面表现优异，但在长上下文推理和生成中对关键信息的注意力仍不足。研究发现，训练数据中的虚假相关性会误导模型注意力，导致冗余推理和错误响应。", "method": "LeaF框架分为两阶段：1）基于梯度的比较方法识别混淆标记；2）通过蒸馏修剪这些标记，使学生的注意力分布与教师的关键上下文标记对齐。", "result": "实验表明，LeaF在数学推理和代码生成任务中显著提升性能，并有效抑制了对混淆标记的注意力，生成更可解释和可靠的推理模型。", "conclusion": "LeaF通过干预式推理解决了LLMs注意力分散的问题，提升了模型在复杂任务中的表现和可靠性。"}}
{"id": "2506.07637", "pdf": "https://arxiv.org/pdf/2506.07637", "abs": "https://arxiv.org/abs/2506.07637", "authors": ["Yuchong Long", "Wen Sun", "Ningxiao Sun", "Wenxiao Wang", "Chao Li", "Shan Yin"], "title": "HieraEdgeNet: A Multi-Scale Edge-Enhanced Framework for Automated Pollen Recognition", "categories": ["cs.CV", "cs.LG", "68T07, 68T45", "I.2.10; I.4.9; I.5.4"], "comment": "16 pages, 5 figures, 2 tables. The dataset at\n  https://www.kaggle.com/datasets/ayinven/hieraedgenetintegratesdatasets. The\n  models at\n  https://huggingface.co/datasets/AyinMostima/HieraEdgeNetintegratesdatasets.\n  The source code in at https://github.com/AyinMostima/PalynoKit", "summary": "Automated pollen recognition is vital to paleoclimatology, biodiversity\nmonitoring, and public health, yet conventional methods are hampered by\ninefficiency and subjectivity. Existing deep learning models often struggle to\nachieve the requisite localization accuracy for microscopic targets like\npollen, which are characterized by their minute size, indistinct edges, and\ncomplex backgrounds. To overcome this limitation, we introduce HieraEdgeNet, a\nmulti-scale edge-enhancement framework. The framework's core innovation is the\nintroduction of three synergistic modules: the Hierarchical Edge Module (HEM),\nwhich explicitly extracts a multi-scale pyramid of edge features that\ncorresponds to the semantic hierarchy at early network stages; the Synergistic\nEdge Fusion (SEF) module, for deeply fusing these edge priors with semantic\ninformation at each respective scale; and the Cross Stage Partial Omni-Kernel\nModule (CSPOKM), which maximally refines the most detail-rich feature layers\nusing an Omni-Kernel operator - comprising anisotropic large-kernel\nconvolutions and mixed-domain attention - all within a computationally\nefficient Cross-Stage Partial (CSP) framework. On a large-scale dataset\ncomprising 120 pollen classes, HieraEdgeNet achieves a mean Average Precision\n(mAP@.5) of 0.9501, significantly outperforming state-of-the-art baseline\nmodels such as YOLOv12n and RT-DETR. Furthermore, qualitative analysis confirms\nthat our approach generates feature representations that are more precisely\nfocused on object boundaries. By systematically integrating edge information,\nHieraEdgeNet provides a robust and powerful solution for high-precision,\nhigh-efficiency automated detection of microscopic objects.", "AI": {"tldr": "HieraEdgeNet是一种多尺度边缘增强框架，显著提升了花粉等微小目标的检测精度，优于现有最先进模型。", "motivation": "传统花粉识别方法效率低且主观性强，现有深度学习模型在微小目标（如花粉）的定位精度上表现不佳。", "method": "提出HieraEdgeNet框架，包含三个模块：HEM（多尺度边缘特征提取）、SEF（边缘与语义信息融合）和CSPOKM（细节特征优化）。", "result": "在120类花粉数据集上，HieraEdgeNet的mAP@.5达到0.9501，优于YOLOv12n和RT-DETR。", "conclusion": "HieraEdgeNet通过系统整合边缘信息，为高精度、高效率的微小目标检测提供了强大解决方案。"}}
{"id": "2506.07899", "pdf": "https://arxiv.org/pdf/2506.07899", "abs": "https://arxiv.org/abs/2506.07899", "authors": ["Ke Wang", "Yiming Qin", "Nikolaos Dimitriadis", "Alessandro Favero", "Pascal Frossard"], "title": "MEMOIR: Lifelong Model Editing with Minimal Overwrite and Informed Retention for LLMs", "categories": ["cs.CL", "cs.LG"], "comment": "The first two authors contributed equally to this work", "summary": "Language models deployed in real-world systems often require post-hoc updates\nto incorporate new or corrected knowledge. However, editing such models\nefficiently and reliably - without retraining or forgetting previous\ninformation - remains a major challenge. Existing methods for lifelong model\nediting either compromise generalization, interfere with past edits, or fail to\nscale to long editing sequences. We propose MEMOIR, a novel scalable framework\nthat injects knowledge through a residual memory, i.e., a dedicated parameter\nmodule, while preserving the core capabilities of the pre-trained model. By\nsparsifying input activations through sample-dependent masks, MEMOIR confines\neach edit to a distinct subset of the memory parameters, minimizing\ninterference among edits. At inference, it identifies relevant edits by\ncomparing the sparse activation patterns of new queries to those stored during\nediting. This enables generalization to rephrased queries by activating only\nthe relevant knowledge while suppressing unnecessary memory activation for\nunrelated prompts. Experiments on question answering, hallucination correction,\nand out-of-distribution generalization benchmarks across LLaMA-3 and Mistral\ndemonstrate that MEMOIR achieves state-of-the-art performance across\nreliability, generalization, and locality metrics, scaling to thousands of\nsequential edits with minimal forgetting.", "AI": {"tldr": "MEMOIR是一种通过残差记忆模块高效更新语言模型知识的新框架，避免重新训练或遗忘旧知识，支持大规模连续编辑。", "motivation": "解决语言模型在部署后需要更新知识时，现有方法无法兼顾高效性、可靠性和可扩展性的问题。", "method": "通过残差记忆模块注入新知识，利用样本相关掩码稀疏化激活，将每个编辑限制在记忆参数子集，减少编辑间干扰。推理时通过稀疏激活模式匹配相关编辑。", "result": "在问答、幻觉纠正和分布外泛化等任务上，MEMOIR在可靠性、泛化性和局部性方面表现最优，支持数千次连续编辑。", "conclusion": "MEMOIR提供了一种高效、可靠且可扩展的语言模型知识更新方法，适用于大规模实际应用。"}}
{"id": "2506.07643", "pdf": "https://arxiv.org/pdf/2506.07643", "abs": "https://arxiv.org/abs/2506.07643", "authors": ["Jae Sung Park", "Zixian Ma", "Linjie Li", "Chenhao Zheng", "Cheng-Yu Hsieh", "Ximing Lu", "Khyathi Chandu", "Quan Kong", "Norimasa Kobori", "Ali Farhadi", "Yejin Choi", "Ranjay Krishna"], "title": "Synthetic Visual Genome", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Reasoning over visual relationships-spatial, functional, interactional,\nsocial, etc.-is considered to be a fundamental component of human cognition.\nYet, despite the major advances in visual comprehension in multimodal language\nmodels (MLMs), precise reasoning over relationships and their generations\nremains a challenge. We introduce ROBIN: an MLM instruction-tuned with densely\nannotated relationships capable of constructing high-quality dense scene graphs\nat scale. To train ROBIN, we curate SVG, a synthetic scene graph dataset by\ncompleting the missing relations of selected objects in existing scene graphs\nusing a teacher MLM and a carefully designed filtering process to ensure\nhigh-quality. To generate more accurate and rich scene graphs at scale for any\nimage, we introduce SG-EDIT: a self-distillation framework where GPT-4o further\nrefines ROBIN's predicted scene graphs by removing unlikely relations and/or\nsuggesting relevant ones. In total, our dataset contains 146K images and 5.6M\nrelationships for 2.6M objects. Results show that our ROBIN-3B model, despite\nbeing trained on less than 3 million instances, outperforms similar-size models\ntrained on over 300 million instances on relationship understanding benchmarks,\nand even surpasses larger models up to 13B parameters. Notably, it achieves\nstate-of-the-art performance in referring expression comprehension with a score\nof 88.9, surpassing the previous best of 87.4. Our results suggest that\ntraining on the refined scene graph data is crucial to maintaining high\nperformance across diverse visual reasoning task.", "AI": {"tldr": "论文提出ROBIN模型，通过密集标注的关系训练，能够生成高质量的场景图，并在关系理解任务中超越同类模型。", "motivation": "尽管多模态语言模型在视觉理解方面取得进展，但在关系和生成上的精确推理仍具挑战性。", "method": "使用合成数据集SVG训练ROBIN，并通过SG-EDIT框架进一步优化场景图生成。", "result": "ROBIN-3B模型在关系理解任务中表现优异，甚至超越更大参数量的模型，达到88.9的SOTA分数。", "conclusion": "精炼的场景图数据对提升视觉推理任务的性能至关重要。"}}
{"id": "2506.07900", "pdf": "https://arxiv.org/pdf/2506.07900", "abs": "https://arxiv.org/abs/2506.07900", "authors": ["MiniCPM Team", "Chaojun Xiao", "Yuxuan Li", "Xu Han", "Yuzhuo Bai", "Jie Cai", "Haotian Chen", "Wentong Chen", "Xin Cong", "Ganqu Cui", "Ning Ding", "Shengdan Fan", "Yewei Fang", "Zixuan Fu", "Wenyu Guan", "Yitong Guan", "Junshao Guo", "Yufeng Han", "Bingxiang He", "Yuxiang Huang", "Cunliang Kong", "Qiuzuo Li", "Siyuan Li", "Wenhao Li", "Yanghao Li", "Yishan Li", "Zhen Li", "Dan Liu", "Biyuan Lin", "Yankai Lin", "Xiang Long", "Quanyu Lu", "Yaxi Lu", "Peiyan Luo", "Hongya Lyu", "Litu Ou", "Yinxu Pan", "Zekai Qu", "Qundong Shi", "Zijun Song", "Jiayuan Su", "Zhou Su", "Ao Sun", "Xianghui Sun", "Peijun Tang", "Fangzheng Wang", "Feng Wang", "Shuo Wang", "Yudong Wang", "Yesai Wu", "Zhenyu Xiao", "Jie Xie", "Zihao Xie", "Yukun Yan", "Jiarui Yuan", "Kaihuo Zhang", "Lei Zhang", "Linyue Zhang", "Xueren Zhang", "Yudi Zhang", "Hengyu Zhao", "Weilin Zhao", "Weilun Zhao", "Yuanqian Zhao", "Zhi Zheng", "Ge Zhou", "Jie Zhou", "Wei Zhou", "Zihan Zhou", "Zixuan Zhou", "Zhiyuan Liu", "Guoyang Zeng", "Chao Jia", "Dahai Li", "Maosong Sun"], "title": "MiniCPM4: Ultra-Efficient LLMs on End Devices", "categories": ["cs.CL", "cs.AI"], "comment": "MiniCPM4 Technical Report", "summary": "This paper introduces MiniCPM4, a highly efficient large language model (LLM)\ndesigned explicitly for end-side devices. We achieve this efficiency through\nsystematic innovation in four key dimensions: model architecture, training\ndata, training algorithms, and inference systems. Specifically, in terms of\nmodel architecture, we propose InfLLM v2, a trainable sparse attention\nmechanism that accelerates both prefilling and decoding phases for long-context\nprocessing. Regarding training data, we propose UltraClean, an efficient and\naccurate pre-training data filtering and generation strategy, and UltraChat v2,\na comprehensive supervised fine-tuning dataset. These datasets enable\nsatisfactory model performance to be achieved using just 8 trillion training\ntokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient\npre-training strategy search, and improve existing post-training methods by\nintroducing chunk-wise rollout for load-balanced reinforcement learning and\ndata-efficient tenary LLM, BitCPM. Regarding inference systems, we propose\nCPM.cu that integrates sparse attention, model quantization, and speculative\nsampling to achieve efficient prefilling and decoding. To meet diverse\non-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B\nparameters, respectively. Sufficient evaluation results show that MiniCPM4\noutperforms open-source models of similar size across multiple benchmarks,\nhighlighting both its efficiency and effectiveness. Notably, MiniCPM4-8B\ndemonstrates significant speed improvements over Qwen3-8B when processing long\nsequences. Through further adaptation, MiniCPM4 successfully powers diverse\napplications, including trustworthy survey generation and tool use with model\ncontext protocol, clearly showcasing its broad usability.", "AI": {"tldr": "MiniCPM4是一种高效的端侧设备大语言模型，通过模型架构、训练数据、训练算法和推理系统的创新，实现了高性能和效率。", "motivation": "为端侧设备设计高效的大语言模型，满足多样化的需求。", "method": "提出InfLLM v2稀疏注意力机制、UltraClean和UltraChat v2数据集、ModelTunnel v2训练算法、CPM.cu推理系统。", "result": "MiniCPM4在多个基准测试中优于同类开源模型，处理长序列时速度显著提升。", "conclusion": "MiniCPM4展示了高效性和广泛适用性，成功应用于多种场景。"}}
{"id": "2506.07652", "pdf": "https://arxiv.org/pdf/2506.07652", "abs": "https://arxiv.org/abs/2506.07652", "authors": ["Hangbei Cheng", "Xiaorong Dong", "Xueyu Liu", "Jianan Zhang", "Xuetao Ma", "Mingqiang Wei", "Liansheng Wang", "Junxin Chen", "Yongfei Wu"], "title": "FMaMIL: Frequency-Driven Mamba Multi-Instance Learning for Weakly Supervised Lesion Segmentation in Medical Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate lesion segmentation in histopathology images is essential for\ndiagnostic interpretation and quantitative analysis, yet it remains challenging\ndue to the limited availability of costly pixel-level annotations. To address\nthis, we propose FMaMIL, a novel two-stage framework for weakly supervised\nlesion segmentation based solely on image-level labels. In the first stage, a\nlightweight Mamba-based encoder is introduced to capture long-range\ndependencies across image patches under the MIL paradigm. To enhance spatial\nsensitivity and structural awareness, we design a learnable frequency-domain\nencoding module that supplements spatial-domain features with spectrum-based\ninformation. CAMs generated in this stage are used to guide segmentation\ntraining. In the second stage, we refine the initial pseudo labels via a\nCAM-guided soft-label supervision and a self-correction mechanism, enabling\nrobust training even under label noise. Extensive experiments on both public\nand private histopathology datasets demonstrate that FMaMIL outperforms\nstate-of-the-art weakly supervised methods without relying on pixel-level\nannotations, validating its effectiveness and potential for digital pathology\napplications.", "AI": {"tldr": "FMaMIL是一种基于图像级标签的两阶段弱监督病灶分割框架，通过Mamba编码器和频域编码模块提升分割效果，并在第二阶段通过伪标签优化实现鲁棒训练。", "motivation": "由于像素级标注成本高且稀缺，需要一种仅依赖图像级标签的病灶分割方法。", "method": "提出FMaMIL框架：第一阶段使用Mamba编码器和频域编码模块生成CAMs；第二阶段通过伪标签优化和自校正机制训练分割模型。", "result": "在多个数据集上表现优于现有弱监督方法，无需像素级标注。", "conclusion": "FMaMIL在数字病理学中具有潜力，验证了其有效性。"}}
{"id": "2506.06407", "pdf": "https://arxiv.org/pdf/2506.06407", "abs": "https://arxiv.org/abs/2506.06407", "authors": ["Zhi Wen Soi", "Chaoyi Zhu", "Fouad Abiad", "Aditya Shankar", "Jeroen M. Galjaard", "Huijuan Wang", "Lydia Y. Chen"], "title": "TimeWak: Temporal Chained-Hashing Watermark for Time Series Data", "categories": ["cs.CR", "cs.AI", "cs.LG", "cs.MM"], "comment": null, "summary": "Synthetic time series generated by diffusion models enable sharing\nprivacy-sensitive datasets, such as patients' functional MRI records. Key\ncriteria for synthetic data include high data utility and traceability to\nverify the data source. Recent watermarking methods embed in homogeneous latent\nspaces, but state-of-the-art time series generators operate in real space,\nmaking latent-based watermarking incompatible. This creates the challenge of\nwatermarking directly in real space while handling feature heterogeneity and\ntemporal dependencies. We propose TimeWak, the first watermarking algorithm for\nmultivariate time series diffusion models. To handle temporal dependence and\nspatial heterogeneity, TimeWak embeds a temporal chained-hashing watermark\ndirectly within the real temporal-feature space. The other unique feature is\nthe $\\epsilon$-exact inversion, which addresses the non-uniform reconstruction\nerror distribution across features from inverting the diffusion process to\ndetect watermarks. We derive the error bound of inverting multivariate time\nseries and further maintain high watermark detectability. We extensively\nevaluate TimeWak on its impact on synthetic data quality, watermark\ndetectability, and robustness under various post-editing attacks, against 5\ndatasets and baselines of different temporal lengths. Our results show that\nTimeWak achieves improvements of 61.96% in context-FID score, and 8.44% in\ncorrelational scores against the state-of-the-art baseline, while remaining\nconsistently detectable.", "AI": {"tldr": "TimeWak是一种用于多元时间序列扩散模型的水印算法，直接在真实时空特征中嵌入时间链式哈希水印，解决了现有方法在异质性和时间依赖性上的不足。", "motivation": "解决现有水印方法在真实空间生成的时间序列数据中不兼容的问题，同时处理特征异质性和时间依赖性。", "method": "提出TimeWak算法，嵌入时间链式哈希水印，并采用ε-精确反转技术处理扩散过程的反转误差。", "result": "TimeWak在合成数据质量和水印可检测性上显著优于现有基线，提升了61.96%的context-FID分数和8.44%的相关性分数。", "conclusion": "TimeWak在保持高水印可检测性的同时，显著提升了合成数据的质量和鲁棒性。"}}
{"id": "2506.07937", "pdf": "https://arxiv.org/pdf/2506.07937", "abs": "https://arxiv.org/abs/2506.07937", "authors": ["Shamminuj Aktar", "Andreas Bärtschi", "Abdel-Hameed A. Badawy", "Stephan Eidenbenz"], "title": "Quantum Graph Transformer for NLP Sentiment Classification", "categories": ["cs.CL", "quant-ph"], "comment": null, "summary": "Quantum machine learning is a promising direction for building more efficient\nand expressive models, particularly in domains where understanding complex,\nstructured data is critical. We present the Quantum Graph Transformer (QGT), a\nhybrid graph-based architecture that integrates a quantum self-attention\nmechanism into the message-passing framework for structured language modeling.\nThe attention mechanism is implemented using parameterized quantum circuits\n(PQCs), which enable the model to capture rich contextual relationships while\nsignificantly reducing the number of trainable parameters compared to classical\nattention mechanisms. We evaluate QGT on five sentiment classification\nbenchmarks. Experimental results show that QGT consistently achieves higher or\ncomparable accuracy than existing quantum natural language processing (QNLP)\nmodels, including both attention-based and non-attention-based approaches. When\ncompared with an equivalent classical graph transformer, QGT yields an average\naccuracy improvement of 5.42% on real-world datasets and 4.76% on synthetic\ndatasets. Additionally, QGT demonstrates improved sample efficiency, requiring\nnearly 50% fewer labeled samples to reach comparable performance on the Yelp\ndataset. These results highlight the potential of graph-based QNLP techniques\nfor advancing efficient and scalable language understanding.", "AI": {"tldr": "论文提出了一种量子图变换器（QGT），通过量子自注意力机制提升结构化语言建模的效率，实验表明其性能优于现有量子自然语言处理模型。", "motivation": "量子机器学习在复杂结构化数据建模中具有潜力，但现有方法在参数效率和性能上存在不足。", "method": "QGT结合量子自注意力机制和消息传递框架，使用参数化量子电路（PQCs）减少可训练参数。", "result": "QGT在五个情感分类基准测试中表现优于或与现有QNLP模型相当，平均准确率提升5.42%（真实数据集）和4.76%（合成数据集），样本效率提高50%。", "conclusion": "QGT展示了图基QNLP技术在高效、可扩展语言理解中的潜力。"}}
{"id": "2506.07670", "pdf": "https://arxiv.org/pdf/2506.07670", "abs": "https://arxiv.org/abs/2506.07670", "authors": ["Xiaohan Lu", "Jiaye Fu", "Jiaqi Zhang", "Zetian Song", "Chuanmin Jia", "Siwei Ma"], "title": "ProSplat: Improved Feed-Forward 3D Gaussian Splatting for Wide-Baseline Sparse Views", "categories": ["cs.CV"], "comment": null, "summary": "Feed-forward 3D Gaussian Splatting (3DGS) has recently demonstrated promising\nresults for novel view synthesis (NVS) from sparse input views, particularly\nunder narrow-baseline conditions. However, its performance significantly\ndegrades in wide-baseline scenarios due to limited texture details and\ngeometric inconsistencies across views. To address these challenges, in this\npaper, we propose ProSplat, a two-stage feed-forward framework designed for\nhigh-fidelity rendering under wide-baseline conditions. The first stage\ninvolves generating 3D Gaussian primitives via a 3DGS generator. In the second\nstage, rendered views from these primitives are enhanced through an improvement\nmodel. Specifically, this improvement model is based on a one-step diffusion\nmodel, further optimized by our proposed Maximum Overlap Reference view\nInjection (MORI) and Distance-Weighted Epipolar Attention (DWEA). MORI\nsupplements missing texture and color by strategically selecting a reference\nview with maximum viewpoint overlap, while DWEA enforces geometric consistency\nusing epipolar constraints. Additionally, we introduce a divide-and-conquer\ntraining strategy that aligns data distributions between the two stages through\njoint optimization. We evaluate ProSplat on the RealEstate10K and DL3DV-10K\ndatasets under wide-baseline settings. Experimental results demonstrate that\nProSplat achieves an average improvement of 1 dB in PSNR compared to recent\nSOTA methods.", "AI": {"tldr": "ProSplat是一个两阶段前馈框架，用于在宽基线条件下实现高保真渲染，通过3D高斯生成器和改进模型（基于扩散模型）提升性能。", "motivation": "3D高斯溅射（3DGS）在窄基线条件下表现良好，但在宽基线场景中性能下降，原因是纹理细节不足和几何不一致。", "method": "ProSplat分为两阶段：首先生成3D高斯基元，然后通过改进模型（结合MORI和DWEA）增强渲染视图。", "result": "在RealEstate10K和DL3DV-10K数据集上，ProSplat的PSNR平均提升1 dB。", "conclusion": "ProSplat有效解决了宽基线条件下的渲染问题，性能优于现有方法。"}}
{"id": "2506.06409", "pdf": "https://arxiv.org/pdf/2506.06409", "abs": "https://arxiv.org/abs/2506.06409", "authors": ["Dor Tsur", "Carol Xuan Long", "Claudio Mayrink Verdun", "Hsiang Hsu", "Chen-Fu Chen", "Haim Permuter", "Sajani Vithana", "Flavio P. Calmon"], "title": "HeavyWater and SimplexWater: Watermarking Low-Entropy Text Distributions", "categories": ["cs.CR", "cs.AI", "cs.CL", "cs.CY", "cs.IT", "cs.LG", "math.IT"], "comment": null, "summary": "Large language model (LLM) watermarks enable authentication of text\nprovenance, curb misuse of machine-generated text, and promote trust in AI\nsystems. Current watermarks operate by changing the next-token predictions\noutput by an LLM. The updated (i.e., watermarked) predictions depend on random\nside information produced, for example, by hashing previously generated tokens.\nLLM watermarking is particularly challenging in low-entropy generation tasks -\nsuch as coding - where next-token predictions are near-deterministic. In this\npaper, we propose an optimization framework for watermark design. Our goal is\nto understand how to most effectively use random side information in order to\nmaximize the likelihood of watermark detection and minimize the distortion of\ngenerated text. Our analysis informs the design of two new watermarks:\nHeavyWater and SimplexWater. Both watermarks are tunable, gracefully\ntrading-off between detection accuracy and text distortion. They can also be\napplied to any LLM and are agnostic to side information generation. We examine\nthe performance of HeavyWater and SimplexWater through several benchmarks,\ndemonstrating that they can achieve high watermark detection accuracy with\nminimal compromise of text generation quality, particularly in the low-entropy\nregime. Our theoretical analysis also reveals surprising new connections\nbetween LLM watermarking and coding theory. The code implementation can be\nfound in https://github.com/DorTsur/HeavyWater_SimplexWater", "AI": {"tldr": "论文提出了一种优化框架，设计了两种新的水印方法（HeavyWater和SimplexWater），在低熵任务中实现高检测准确性和低文本失真。", "motivation": "解决LLM水印在低熵任务（如编码）中的挑战，优化随机辅助信息的使用以提高检测效果并减少文本失真。", "method": "提出优化框架，设计两种可调水印（HeavyWater和SimplexWater），适用于任何LLM且不依赖特定辅助信息生成方法。", "result": "实验表明，两种水印在低熵任务中能实现高检测准确性和低文本失真，并揭示了水印与编码理论的新联系。", "conclusion": "HeavyWater和SimplexWater为LLM水印提供了高效解决方案，平衡检测准确性与文本质量，适用于广泛场景。"}}
{"id": "2506.07947", "pdf": "https://arxiv.org/pdf/2506.07947", "abs": "https://arxiv.org/abs/2506.07947", "authors": ["Paulius Rauba", "Qiyao Wei", "Mihaela van der Schaar"], "title": "Statistical Hypothesis Testing for Auditing Robustness in Language Models", "categories": ["cs.CL"], "comment": "arXiv admin note: substantial text overlap with arXiv:2412.00868", "summary": "Consider the problem of testing whether the outputs of a large language model\n(LLM) system change under an arbitrary intervention, such as an input\nperturbation or changing the model variant. We cannot simply compare two LLM\noutputs since they might differ due to the stochastic nature of the system, nor\ncan we compare the entire output distribution due to computational\nintractability. While existing methods for analyzing text-based outputs exist,\nthey focus on fundamentally different problems, such as measuring bias or\nfairness. To this end, we introduce distribution-based perturbation analysis, a\nframework that reformulates LLM perturbation analysis as a frequentist\nhypothesis testing problem. We construct empirical null and alternative output\ndistributions within a low-dimensional semantic similarity space via Monte\nCarlo sampling, enabling tractable inference without restrictive distributional\nassumptions. The framework is (i) model-agnostic, (ii) supports the evaluation\nof arbitrary input perturbations on any black-box LLM, (iii) yields\ninterpretable p-values; (iv) supports multiple perturbations via controlled\nerror rates; and (v) provides scalar effect sizes. We demonstrate the\nusefulness of the framework across multiple case studies, showing how we can\nquantify response changes, measure true/false positive rates, and evaluate\nalignment with reference models. Above all, we see this as a reliable\nfrequentist hypothesis testing framework for LLM auditing.", "AI": {"tldr": "论文提出了一种基于分布的扰动分析框架，用于测试大型语言模型（LLM）在干预下的输出变化，解决了传统方法因随机性或计算复杂性无法直接比较的问题。", "motivation": "现有方法无法有效比较LLM在干预前后的输出变化，尤其是考虑到输出的随机性和计算复杂性。", "method": "通过蒙特卡洛采样在低维语义相似空间中构建经验性的零假设和替代假设分布，将LLM扰动分析转化为频次假设检验问题。", "result": "框架具有模型无关性、支持任意输入扰动、提供可解释的p值、支持多扰动控制误差率，并能量化效应大小。", "conclusion": "该框架为LLM审计提供了一个可靠的频次假设检验方法，适用于多种实际场景。"}}
{"id": "2506.07697", "pdf": "https://arxiv.org/pdf/2506.07697", "abs": "https://arxiv.org/abs/2506.07697", "authors": ["Jens Piekenbrinck", "Christian Schmidt", "Alexander Hermans", "Narunas Vaskevicius", "Timm Linder", "Bastian Leibe"], "title": "OpenSplat3D: Open-Vocabulary 3D Instance Segmentation using Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful representation for\nneural scene reconstruction, offering high-quality novel view synthesis while\nmaintaining computational efficiency. In this paper, we extend the capabilities\nof 3DGS beyond pure scene representation by introducing an approach for\nopen-vocabulary 3D instance segmentation without requiring manual labeling,\ntermed OpenSplat3D. Our method leverages feature-splatting techniques to\nassociate semantic information with individual Gaussians, enabling fine-grained\nscene understanding. We incorporate Segment Anything Model instance masks with\na contrastive loss formulation as guidance for the instance features to achieve\naccurate instance-level segmentation. Furthermore, we utilize language\nembeddings of a vision-language model, allowing for flexible, text-driven\ninstance identification. This combination enables our system to identify and\nsegment arbitrary objects in 3D scenes based on natural language descriptions.\nWe show results on LERF-mask and LERF-OVS as well as the full ScanNet++\nvalidation set, demonstrating the effectiveness of our approach.", "AI": {"tldr": "OpenSplat3D扩展了3D高斯泼溅（3DGS）的能力，实现了无需手动标注的开放词汇3D实例分割。", "motivation": "提升3D场景理解能力，实现基于自然语言描述的任意对象识别与分割。", "method": "结合特征泼溅技术、Segment Anything Model实例掩码和对比损失，利用视觉语言模型的语言嵌入。", "result": "在LERF-mask、LERF-OVS和ScanNet++验证集上展示了有效性。", "conclusion": "OpenSplat3D为3D场景理解提供了灵活且高效的解决方案。"}}
{"id": "2506.06414", "pdf": "https://arxiv.org/pdf/2506.06414", "abs": "https://arxiv.org/abs/2506.06414", "authors": ["Davis Brown", "Mahdi Sabbaghi", "Luze Sun", "Alexander Robey", "George J. Pappas", "Eric Wong", "Hamed Hassani"], "title": "Benchmarking Misuse Mitigation Against Covert Adversaries", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Existing language model safety evaluations focus on overt attacks and\nlow-stakes tasks. Realistic attackers can subvert current safeguards by\nrequesting help on small, benign-seeming tasks across many independent queries.\nBecause individual queries do not appear harmful, the attack is hard to\n{detect}. However, when combined, these fragments uplift misuse by helping the\nattacker complete hard and dangerous tasks. Toward identifying defenses against\nsuch strategies, we develop Benchmarks for Stateful Defenses (BSD), a data\ngeneration pipeline that automates evaluations of covert attacks and\ncorresponding defenses. Using this pipeline, we curate two new datasets that\nare consistently refused by frontier models and are too difficult for weaker\nopen-weight models. Our evaluations indicate that decomposition attacks are\neffective misuse enablers, and highlight stateful defenses as a countermeasure.", "AI": {"tldr": "论文提出了BSD基准，用于评估隐蔽攻击及其防御措施，发现分解攻击是有效的滥用手段，并强调状态防御的重要性。", "motivation": "现有语言模型安全评估集中于明显攻击和低风险任务，而现实攻击者可能通过多次独立查询完成危险任务，需开发新防御方法。", "method": "开发了BSD数据生成管道，自动化评估隐蔽攻击及防御，并创建了两个新数据集。", "result": "分解攻击是有效的滥用手段，BSD数据集能有效评估防御措施。", "conclusion": "状态防御是应对分解攻击的有效对策，BSD为未来防御研究提供了工具。"}}
{"id": "2506.07956", "pdf": "https://arxiv.org/pdf/2506.07956", "abs": "https://arxiv.org/abs/2506.07956", "authors": ["Tim Vieira", "Tianyu Liu", "Clemente Pasti", "Yahya Emara", "Brian DuSell", "Benjamin LeBrun", "Mario Giulianelli", "Juan Luis Gastaldi", "Timothy J. O'Donnell", "Ryan Cotterell"], "title": "Language Models over Canonical Byte-Pair Encodings", "categories": ["cs.CL", "cs.FL", "cs.LG"], "comment": "ICML 2025", "summary": "Modern language models represent probability distributions over character\nstrings as distributions over (shorter) token strings derived via a\ndeterministic tokenizer, such as byte-pair encoding. While this approach is\nhighly effective at scaling up language models to large corpora, its current\nincarnations have a concerning property: the model assigns nonzero probability\nmass to an exponential number of $\\it{noncanonical}$ token encodings of each\ncharacter string -- these are token strings that decode to valid character\nstrings but are impossible under the deterministic tokenizer (i.e., they will\nnever be seen in any training corpus, no matter how large). This misallocation\nis both erroneous, as noncanonical strings never appear in training data, and\nwasteful, diverting probability mass away from plausible outputs. These are\navoidable mistakes! In this work, we propose methods to enforce canonicality in\ntoken-level language models, ensuring that only canonical token strings are\nassigned positive probability. We present two approaches: (1) canonicality by\nconditioning, leveraging test-time inference strategies without additional\ntraining, and (2) canonicality by construction, a model parameterization that\nguarantees canonical outputs but requires training. We demonstrate that fixing\ncanonicality mistakes improves the likelihood of held-out data for several\nmodels and corpora.", "AI": {"tldr": "现代语言模型通过确定性分词器（如字节对编码）将字符字符串的概率分布表示为（更短的）标记字符串的分布。然而，当前方法存在非规范标记编码的问题，导致概率分配错误且浪费。本文提出两种方法确保模型仅分配正概率给规范标记字符串，并证明其有效性。", "motivation": "当前语言模型在概率分配上存在非规范标记编码的问题，这不仅错误地分配概率给训练数据中不存在的标记，还浪费了概率资源。", "method": "提出两种方法：(1) 通过条件推理在测试时确保规范标记；(2) 通过模型参数化在训练时保证规范输出。", "result": "实验表明，修正规范性问题提高了多个模型和语料库的似然性能。", "conclusion": "通过强制规范标记编码，可以避免概率分配错误并提升模型性能。"}}
{"id": "2506.07698", "pdf": "https://arxiv.org/pdf/2506.07698", "abs": "https://arxiv.org/abs/2506.07698", "authors": ["Yuxiao Yang", "Peihao Li", "Yuhong Zhang", "Junzhe Lu", "Xianglong He", "Minghan Qin", "Weitao Wang", "Haoqian Wang"], "title": "NOVA3D: Normal Aligned Video Diffusion Model for Single Image to 3D Generation", "categories": ["cs.CV", "cs.AI"], "comment": "8 pages, 7 figures, accepted by ICME 2025", "summary": "3D AI-generated content (AIGC) has made it increasingly accessible for anyone\nto become a 3D content creator. While recent methods leverage Score\nDistillation Sampling to distill 3D objects from pretrained image diffusion\nmodels, they often suffer from inadequate 3D priors, leading to insufficient\nmulti-view consistency. In this work, we introduce NOVA3D, an innovative\nsingle-image-to-3D generation framework. Our key insight lies in leveraging\nstrong 3D priors from a pretrained video diffusion model and integrating\ngeometric information during multi-view video fine-tuning. To facilitate\ninformation exchange between color and geometric domains, we propose the\nGeometry-Temporal Alignment (GTA) attention mechanism, thereby improving\ngeneralization and multi-view consistency. Moreover, we introduce the\nde-conflict geometry fusion algorithm, which improves texture fidelity by\naddressing multi-view inaccuracies and resolving discrepancies in pose\nalignment. Extensive experiments validate the superiority of NOVA3D over\nexisting baselines.", "AI": {"tldr": "NOVA3D是一个创新的单图像到3D生成框架，通过利用预训练视频扩散模型的强3D先验和几何信息，提升了多视角一致性。", "motivation": "解决现有方法因缺乏3D先验导致的多视角一致性问题。", "method": "利用预训练视频扩散模型的3D先验，结合几何信息进行多视角视频微调，提出GTA注意力机制和去冲突几何融合算法。", "result": "实验表明NOVA3D优于现有基线方法。", "conclusion": "NOVA3D通过改进3D先验和几何信息整合，显著提升了单图像到3D生成的质量和多视角一致性。"}}
{"id": "2506.06443", "pdf": "https://arxiv.org/pdf/2506.06443", "abs": "https://arxiv.org/abs/2506.06443", "authors": ["Luis Pinto"], "title": "Unlocking Chemical Insights: Superior Molecular Representations from Intermediate Encoder Layers", "categories": ["cs.LG", "cs.AI", "physics.chem-ph", "q-bio.BM"], "comment": null, "summary": "Pretrained molecular encoders have become indispensable in computational\nchemistry for tasks such as property prediction and molecular generation.\nHowever, the standard practice of relying solely on final-layer embeddings for\ndownstream tasks may discard valuable information. In this work, we challenge\nthis convention by conducting a comprehensive layer-wise analysis of five\ndiverse molecular encoders across 22 ADMET property prediction tasks. Our\nresults demonstrate that embeddings from intermediate layers consistently\noutperform final-layer representations. Specifically, using fixed embeddings\nfrom the optimal intermediate layers improved downstream performance by an\naverage of 5.4%, reaching gains up to 28.6%. Furthermore, finetuning up to\nthese intermediate layers yielded even greater average improvements of 8.5%,\nwith performance increases as high as 40.8%, achieving new state-of-the-art\nresults on several benchmarks. Additionally, a strong positive correlation\nbetween fixed embedding performance and finetuning outcomes supports an\nefficient evaluate-then-finetune approach, enabling identification of optimal\nlayers with reduced computational cost. These findings highlight the importance\nof exploring the full representational depth of molecular encoders to achieve\nsubstantial performance improvements and computational efficiency. The code is\nmade publicly available at\nhttps://github.com/luispintoc/Unlocking-Chemical-Insights.", "AI": {"tldr": "研究发现，分子编码器的中间层嵌入比最终层嵌入在ADMET属性预测任务中表现更好，固定嵌入平均提升5.4%，微调后平均提升8.5%，并达到新的SOTA结果。", "motivation": "挑战仅依赖最终层嵌入的常规做法，探索分子编码器各层的表现潜力。", "method": "对五种分子编码器进行分层分析，测试22种ADMET属性预测任务，比较固定嵌入和微调的效果。", "result": "中间层嵌入表现优于最终层，固定嵌入平均提升5.4%，微调后平均提升8.5%，部分任务提升高达40.8%。", "conclusion": "充分利用分子编码器的各层嵌入可显著提升性能，提出高效评估-微调方法以减少计算成本。"}}
{"id": "2506.07962", "pdf": "https://arxiv.org/pdf/2506.07962", "abs": "https://arxiv.org/abs/2506.07962", "authors": ["Elliot Kim", "Avi Garg", "Kenny Peng", "Nikhil Garg"], "title": "Correlated Errors in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY", "stat.ML"], "comment": "Accepted to ICML 2025", "summary": "Diversity in training data, architecture, and providers is assumed to\nmitigate homogeneity in LLMs. However, we lack empirical evidence on whether\ndifferent LLMs differ meaningfully. We conduct a large-scale empirical\nevaluation on over 350 LLMs overall, using two popular leaderboards and a\nresume-screening task. We find substantial correlation in model errors -- on\none leaderboard dataset, models agree 60% of the time when both models err. We\nidentify factors driving model correlation, including shared architectures and\nproviders. Crucially, however, larger and more accurate models have highly\ncorrelated errors, even with distinct architectures and providers. Finally, we\nshow the effects of correlation in two downstream tasks: LLM-as-judge\nevaluation and hiring -- the latter reflecting theoretical predictions\nregarding algorithmic monoculture.", "AI": {"tldr": "研究发现，尽管训练数据、架构和提供者的多样性被假设能减少LLM的同质性，但实际评估发现不同LLM的错误高度相关，尤其在更大、更准确的模型中。", "motivation": "探讨不同LLM之间是否存在显著差异，以及多样性是否真正能减少模型的同质性。", "method": "对超过350个LLM进行大规模评估，使用两个流行排行榜和一个简历筛选任务。", "result": "模型错误高度相关，尤其在更大、更准确的模型中，即使架构和提供者不同。", "conclusion": "模型错误的相关性在下游任务（如LLM作为评委和招聘）中产生显著影响，反映了算法单一化的理论预测。"}}
{"id": "2506.07705", "pdf": "https://arxiv.org/pdf/2506.07705", "abs": "https://arxiv.org/abs/2506.07705", "authors": ["Weilei Wen", "Chunle Guo", "Wenqi Ren", "Hongpeng Wang", "Xiuli Shao"], "title": "Adaptive Blind Super-Resolution Network for Spatial-Specific and Spatial-Agnostic Degradations", "categories": ["cs.CV", "eess.IV"], "comment": "IEEE TRANSACTIONS ON IMAGE PROCESSING", "summary": "Prior methodologies have disregarded the diversities among distinct\ndegradation types during image reconstruction, employing a uniform network\nmodel to handle multiple deteriorations. Nevertheless, we discover that\nprevalent degradation modalities, including sampling, blurring, and noise, can\nbe roughly categorized into two classes. We classify the first class as\nspatial-agnostic dominant degradations, less affected by regional changes in\nimage space, such as downsampling and noise degradation. The second class\ndegradation type is intimately associated with the spatial position of the\nimage, such as blurring, and we identify them as spatial-specific dominant\ndegradations. We introduce a dynamic filter network integrating global and\nlocal branches to address these two degradation types. This network can greatly\nalleviate the practical degradation problem. Specifically, the global dynamic\nfiltering layer can perceive the spatial-agnostic dominant degradation in\ndifferent images by applying weights generated by the attention mechanism to\nmultiple parallel standard convolution kernels, enhancing the network's\nrepresentation ability. Meanwhile, the local dynamic filtering layer converts\nfeature maps of the image into a spatially specific dynamic filtering operator,\nwhich performs spatially specific convolution operations on the image features\nto handle spatial-specific dominant degradations. By effectively integrating\nboth global and local dynamic filtering operators, our proposed method\noutperforms state-of-the-art blind super-resolution algorithms in both\nsynthetic and real image datasets.", "AI": {"tldr": "论文提出了一种动态滤波网络，通过全局和局部分支处理空间无关和空间相关的图像退化问题，显著提升了图像重建性能。", "motivation": "现有方法忽视了不同退化类型的多样性，采用统一网络模型处理多种退化问题，而研究发现退化类型可分为空间无关和空间相关两类，需要针对性解决。", "method": "引入动态滤波网络，包含全局动态滤波层（处理空间无关退化）和局部动态滤波层（处理空间相关退化），通过注意力机制和空间特定卷积操作增强网络能力。", "result": "在合成和真实图像数据集上，该方法优于当前最先进的盲超分辨率算法。", "conclusion": "动态滤波网络能有效区分并处理两类退化问题，显著提升图像重建效果。"}}
{"id": "2506.06444", "pdf": "https://arxiv.org/pdf/2506.06444", "abs": "https://arxiv.org/abs/2506.06444", "authors": ["Ruizhong Qiu", "Gaotang Li", "Tianxin Wei", "Jingrui He", "Hanghang Tong"], "title": "Saffron-1: Towards an Inference Scaling Paradigm for LLM Safety Assurance", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": "19 pages", "summary": "Existing safety assurance research has primarily focused on training-phase\nalignment to instill safe behaviors into LLMs. However, recent studies have\nexposed these methods' susceptibility to diverse jailbreak attacks.\nConcurrently, inference scaling has significantly advanced LLM reasoning\ncapabilities but remains unexplored in the context of safety assurance.\nAddressing this gap, our work pioneers inference scaling for robust and\neffective LLM safety against emerging threats. We reveal that conventional\ninference scaling techniques, despite their success in reasoning tasks, perform\npoorly in safety contexts, even falling short of basic approaches like\nBest-of-N Sampling. We attribute this inefficiency to a newly identified\nchallenge, the exploration--efficiency dilemma, arising from the high\ncomputational overhead associated with frequent process reward model (PRM)\nevaluations. To overcome this dilemma, we propose SAFFRON, a novel inference\nscaling paradigm tailored explicitly for safety assurance. Central to our\napproach is the introduction of a multifurcation reward model (MRM) that\nsignificantly reduces the required number of reward model evaluations. To\noperationalize this paradigm, we further propose: (i) a partial supervision\ntraining objective for MRM, (ii) a conservative exploration constraint to\nprevent out-of-distribution explorations, and (iii) a Trie-based key--value\ncaching strategy that facilitates cache sharing across sequences during tree\nsearch. Extensive experiments validate the effectiveness of our method.\nAdditionally, we publicly release our trained multifurcation reward model\n(Saffron-1) and the accompanying token-level safety reward dataset (Safety4M)\nto accelerate future research in LLM safety. Our code, model, and data are\npublicly available at https://github.com/q-rz/saffron , and our project\nhomepage is at https://q-rz.github.io/p/saffron .", "AI": {"tldr": "论文提出了一种新的推理扩展方法SAFFRON，针对LLM的安全性保障，解决了传统方法在安全场景中的低效问题。", "motivation": "现有安全研究主要集中在训练阶段的对齐，但易受越狱攻击；推理扩展虽提升了LLM的推理能力，但在安全性保障方面尚未探索。", "method": "提出SAFFRON，引入多分支奖励模型（MRM）减少奖励模型评估次数，并设计部分监督训练目标、保守探索约束和Trie缓存策略。", "result": "实验验证了SAFFRON的有效性，并公开了模型Saffron-1和数据集Safety4M。", "conclusion": "SAFFRON为LLM安全性提供了高效解决方案，推动了未来研究。"}}
{"id": "2506.08007", "pdf": "https://arxiv.org/pdf/2506.08007", "abs": "https://arxiv.org/abs/2506.08007", "authors": ["Qingxiu Dong", "Li Dong", "Yao Tang", "Tianzhu Ye", "Yutao Sun", "Zhifang Sui", "Furu Wei"], "title": "Reinforcement Pre-Training", "categories": ["cs.CL"], "comment": null, "summary": "In this work, we introduce Reinforcement Pre-Training (RPT) as a new scaling\nparadigm for large language models and reinforcement learning (RL).\nSpecifically, we reframe next-token prediction as a reasoning task trained\nusing RL, where it receives verifiable rewards for correctly predicting the\nnext token for a given context. RPT offers a scalable method to leverage vast\namounts of text data for general-purpose RL, rather than relying on\ndomain-specific annotated answers. By incentivizing the capability of\nnext-token reasoning, RPT significantly improves the language modeling accuracy\nof predicting the next tokens. Moreover, RPT provides a strong pre-trained\nfoundation for further reinforcement fine-tuning. The scaling curves show that\nincreased training compute consistently improves the next-token prediction\naccuracy. The results position RPT as an effective and promising scaling\nparadigm to advance language model pre-training.", "AI": {"tldr": "本文提出了一种新的扩展范式——强化预训练（RPT），通过将下一个词预测任务转化为基于强化学习的推理任务，显著提升了语言模型的准确性。", "motivation": "传统的语言模型预训练依赖于领域特定的标注数据，而RPT旨在利用大量文本数据进行通用强化学习，从而提升模型的推理能力。", "method": "RPT将下一个词预测任务重新定义为基于强化学习的推理任务，通过可验证的奖励机制激励模型正确预测下一个词。", "result": "实验表明，RPT显著提高了语言模型的下一个词预测准确性，并为后续的强化微调提供了强大的预训练基础。", "conclusion": "RPT是一种有效且有前景的语言模型预训练扩展范式，能够通过增加计算资源持续提升性能。"}}
{"id": "2506.07713", "pdf": "https://arxiv.org/pdf/2506.07713", "abs": "https://arxiv.org/abs/2506.07713", "authors": ["Ge Wang", "Songlin Fan", "Hangxu Liu", "Quanjian Song", "Hewei Wang", "Jinfeng Xu"], "title": "Consistent Video Editing as Flow-Driven Image-to-Video Generation", "categories": ["cs.CV", "cs.AI"], "comment": "16 pages, 12 figures", "summary": "With the prosper of video diffusion models, down-stream applications like\nvideo editing have been significantly promoted without consuming much\ncomputational cost. One particular challenge in this task lies at the motion\ntransfer process from the source video to the edited one, where it requires the\nconsideration of the shape deformation in between, meanwhile maintaining the\ntemporal consistency in the generated video sequence. However, existing methods\nfail to model complicated motion patterns for video editing, and are\nfundamentally limited to object replacement, where tasks with non-rigid object\nmotions like multi-object and portrait editing are largely neglected. In this\npaper, we observe that optical flows offer a promising alternative in complex\nmotion modeling, and present FlowV2V to re-investigate video editing as a task\nof flow-driven Image-to-Video (I2V) generation. Specifically, FlowV2V\ndecomposes the entire pipeline into first-frame editing and conditional I2V\ngeneration, and simulates pseudo flow sequence that aligns with the deformed\nshape, thus ensuring the consistency during editing. Experimental results on\nDAVIS-EDIT with improvements of 13.67% and 50.66% on DOVER and warping error\nillustrate the superior temporal consistency and sample quality of FlowV2V\ncompared to existing state-of-the-art ones. Furthermore, we conduct\ncomprehensive ablation studies to analyze the internal functionalities of the\nfirst-frame paradigm and flow alignment in the proposed method.", "AI": {"tldr": "FlowV2V提出了一种基于光流的视频编辑方法，通过分解任务为第一帧编辑和条件I2V生成，解决了复杂运动建模的挑战。", "motivation": "现有方法难以处理复杂运动模式（如多对象和肖像编辑），光流为复杂运动建模提供了新思路。", "method": "FlowV2V将任务分解为第一帧编辑和条件I2V生成，模拟伪光流序列以保持形状变形和时间一致性。", "result": "在DAVIS-EDIT数据集上，FlowV2V在DOVER和warping error指标上分别提升了13.67%和50.66%。", "conclusion": "FlowV2V在时间一致性和样本质量上优于现有方法，并通过消融实验验证了其内部功能的有效性。"}}
{"id": "2506.07720", "pdf": "https://arxiv.org/pdf/2506.07720", "abs": "https://arxiv.org/abs/2506.07720", "authors": ["Yufei Guo", "Yuhan Zhang", "Zhou Jie", "Xiaode Liu", "Xin Tong", "Yuanpei Chen", "Weihang Peng", "Zhe Ma"], "title": "ReverB-SNN: Reversing Bit of the Weight and Activation for Spiking Neural Networks", "categories": ["cs.CV"], "comment": "Accpeted by ICML2024", "summary": "The Spiking Neural Network (SNN), a biologically inspired neural network\ninfrastructure, has garnered significant attention recently. SNNs utilize\nbinary spike activations for efficient information transmission, replacing\nmultiplications with additions, thereby enhancing energy efficiency. However,\nbinary spike activation maps often fail to capture sufficient data information,\nresulting in reduced accuracy. To address this challenge, we advocate reversing\nthe bit of the weight and activation for SNNs, called \\textbf{ReverB-SNN},\ninspired by recent findings that highlight greater accuracy degradation from\nquantizing activations compared to weights. Specifically, our method employs\nreal-valued spike activations alongside binary weights in SNNs. This preserves\nthe event-driven and multiplication-free advantages of standard SNNs while\nenhancing the information capacity of activations. Additionally, we introduce a\ntrainable factor within binary weights to adaptively learn suitable weight\namplitudes during training, thereby increasing network capacity. To maintain\nefficiency akin to vanilla \\textbf{ReverB-SNN}, our trainable binary weight\nSNNs are converted back to standard form using a re-parameterization technique\nduring inference. Extensive experiments across various network architectures\nand datasets, both static and dynamic, demonstrate that our approach\nconsistently outperforms state-of-the-art methods.", "AI": {"tldr": "提出了一种名为ReverB-SNN的方法，通过反转权重和激活的比特位，结合实值激活和二进制权重，提升SNN的信息容量和准确性。", "motivation": "解决SNN中二进制激活映射信息不足导致的精度下降问题。", "method": "采用实值激活和二进制权重，引入可训练因子自适应学习权重幅度，并通过重参数化技术保持推理效率。", "result": "在多种网络架构和数据集上表现优于现有方法。", "conclusion": "ReverB-SNN在保持SNN高效性的同时显著提升了精度。"}}
{"id": "2506.06455", "pdf": "https://arxiv.org/pdf/2506.06455", "abs": "https://arxiv.org/abs/2506.06455", "authors": ["Antonio Jesús Banegas-Luna", "Horacio Pérez-Sánchez", "Carlos Martínez-Cortés"], "title": "WISCA: A Consensus-Based Approach to Harmonizing Interpretability in Tabular Datasets", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "27 pages, 11 figures, 2 tables, 13 equations", "summary": "While predictive accuracy is often prioritized in machine learning (ML)\nmodels, interpretability remains essential in scientific and high-stakes\ndomains. However, diverse interpretability algorithms frequently yield\nconflicting explanations, highlighting the need for consensus to harmonize\nresults. In this study, six ML models were trained on six synthetic datasets\nwith known ground truths, utilizing various model-agnostic interpretability\ntechniques. Consensus explanations were generated using established methods and\na novel approach: WISCA (Weighted Scaled Consensus Attributions), which\nintegrates class probability and normalized attributions. WISCA consistently\naligned with the most reliable individual method, underscoring the value of\nrobust consensus strategies in improving explanation reliability.", "AI": {"tldr": "论文提出了一种新的共识方法WISCA，用于整合不同可解释性算法的结果，以提高解释的可靠性。", "motivation": "在科学和高风险领域，机器学习模型的可解释性至关重要，但现有算法常产生冲突的解释，需要共识方法来统一结果。", "method": "研究训练了六个ML模型，使用多种模型无关的可解释性技术，并提出了WISCA方法，结合类别概率和归一化归因生成共识解释。", "result": "WISCA与最可靠的个体方法一致，证明了共识策略在提升解释可靠性方面的价值。", "conclusion": "WISCA作为一种稳健的共识方法，能够有效整合不同解释算法的结果，提高解释的可靠性。"}}
{"id": "2506.07725", "pdf": "https://arxiv.org/pdf/2506.07725", "abs": "https://arxiv.org/abs/2506.07725", "authors": ["Shadi Hamdan", "Chonghao Sima", "Zetong Yang", "Hongyang Li", "Fatma Güney"], "title": "ETA: Efficiency through Thinking Ahead, A Dual Approach to Self-Driving with Large Models", "categories": ["cs.CV", "cs.AI"], "comment": "ICCV 2025 submission. For code, see\n  https://github.com/opendrivelab/ETA", "summary": "How can we benefit from large models without sacrificing inference speed, a\ncommon dilemma in self-driving systems? A prevalent solution is a dual-system\narchitecture, employing a small model for rapid, reactive decisions and a\nlarger model for slower but more informative analyses. Existing dual-system\ndesigns often implement parallel architectures where inference is either\ndirectly conducted using the large model at each current frame or retrieved\nfrom previously stored inference results. However, these works still struggle\nto enable large models for a timely response to every online frame. Our key\ninsight is to shift intensive computations of the current frame to previous\ntime steps and perform a batch inference of multiple time steps to make large\nmodels respond promptly to each time step. To achieve the shifting, we\nintroduce Efficiency through Thinking Ahead (ETA), an asynchronous system\ndesigned to: (1) propagate informative features from the past to the current\nframe using future predictions from the large model, (2) extract current frame\nfeatures using a small model for real-time responsiveness, and (3) integrate\nthese dual features via an action mask mechanism that emphasizes\naction-critical image regions. Evaluated on the Bench2Drive CARLA\nLeaderboard-v2 benchmark, ETA advances state-of-the-art performance by 8% with\na driving score of 69.53 while maintaining a near-real-time inference speed at\n50 ms.", "AI": {"tldr": "论文提出了一种异步系统ETA，通过将当前帧的计算转移到之前的时间步，并批量处理多个时间步，使大模型能快速响应每个时间步，同时保持实时性。", "motivation": "解决自动驾驶系统中大模型推理速度慢的问题，避免牺牲推理速度的同时利用大模型的优势。", "method": "提出ETA系统，通过特征传播、小模型实时提取特征和动作掩码机制整合双特征。", "result": "在Bench2Drive CARLA Leaderboard-v2基准测试中，ETA将性能提升8%，驾驶分数达69.53，推理速度保持在50毫秒。", "conclusion": "ETA通过异步设计和批量处理，成功实现大模型的快速响应，同时保持高性能和实时性。"}}
{"id": "2506.06472", "pdf": "https://arxiv.org/pdf/2506.06472", "abs": "https://arxiv.org/abs/2506.06472", "authors": ["Ziqi Yuan", "Haoyang Zhang", "Yirui Eric Zhou", "Apoorve Mohan", "I-Hsin Chung", "Seetharami Seelam", "Jian Huang"], "title": "Cost-Efficient LLM Training with Lifetime-Aware Tensor Offloading via GPUDirect Storage", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PF"], "comment": null, "summary": "We present the design and implementation of a new lifetime-aware tensor\noffloading framework for GPU memory expansion using low-cost PCIe-based\nsolid-state drives (SSDs). Our framework, TERAIO, is developed explicitly for\nlarge language model (LLM) training with multiple GPUs and multiple SSDs. Its\ndesign is driven by our observation that the active tensors take only a small\nfraction (1.7% on average) of allocated GPU memory in each LLM training\niteration, the inactive tensors are usually large and will not be used for a\nlong period of time, creating ample opportunities for offloading/prefetching\ntensors to/from slow SSDs without stalling the GPU training process. TERAIO\naccurately estimates the lifetime (active period of time in GPU memory) of each\ntensor with the profiling of the first few iterations in the training process.\nWith the tensor lifetime analysis, TERAIO will generate an optimized tensor\noffloading/prefetching plan and integrate it into the compiled LLM program via\nPyTorch. TERAIO has a runtime tensor migration engine to execute the\noffloading/prefetching plan via GPUDirect storage, which allows direct tensor\nmigration between GPUs and SSDs for alleviating the CPU bottleneck and\nmaximizing the SSD bandwidth utilization. In comparison with state-of-the-art\nstudies such as ZeRO-Offload and ZeRO-Infinity, we show that TERAIO improves\nthe training performance of various LLMs by 1.47x on average, and achieves\n80.7% of the ideal performance assuming unlimited GPU memory.", "AI": {"tldr": "TERAIO是一个基于SSD的GPU内存扩展框架，通过智能张量卸载和预取优化大型语言模型（LLM）训练性能。", "motivation": "观察到LLM训练中活跃张量仅占GPU内存的1.7%，非活跃张量占用大量内存且长期未使用，为卸载和预取提供了机会。", "method": "通过分析张量生命周期生成优化卸载/预取计划，并集成到PyTorch中，利用GPUDirect存储直接迁移张量。", "result": "相比现有技术（如ZeRO-Offload和ZeRO-Infinity），TERAIO平均提升LLM训练性能1.47倍，达到理想性能的80.7%。", "conclusion": "TERAIO通过高效利用SSD扩展GPU内存，显著提升LLM训练效率。"}}
{"id": "2506.07737", "pdf": "https://arxiv.org/pdf/2506.07737", "abs": "https://arxiv.org/abs/2506.07737", "authors": ["Xuemei Chen", "Huamin Wang", "Hangchi Shen", "Shukai Duan", "Shiping Wen", "Tingwen Huang"], "title": "SpikeSMOKE: Spiking Neural Networks for Monocular 3D Object Detection with Cross-Scale Gated Coding", "categories": ["cs.CV"], "comment": null, "summary": "Low energy consumption for 3D object detection is an important research area\nbecause of the increasing energy consumption with their wide application in\nfields such as autonomous driving. The spiking neural networks (SNNs) with\nlow-power consumption characteristics can provide a novel solution for this\nresearch. Therefore, we apply SNNs to monocular 3D object detection and propose\nthe SpikeSMOKE architecture in this paper, which is a new attempt for low-power\nmonocular 3D object detection. As we all know, discrete signals of SNNs will\ngenerate information loss and limit their feature expression ability compared\nwith the artificial neural networks (ANNs).In order to address this issue,\ninspired by the filtering mechanism of biological neuronal synapses, we propose\na cross-scale gated coding mechanism(CSGC), which can enhance feature\nrepresentation by combining cross-scale fusion of attentional methods and gated\nfiltering mechanisms.In addition, to reduce the computation and increase the\nspeed of training, we present a novel light-weight residual block that can\nmaintain spiking computing paradigm and the highest possible detection\nperformance. Compared to the baseline SpikeSMOKE under the 3D Object Detection,\nthe proposed SpikeSMOKE with CSGC can achieve 11.78 (+2.82, Easy), 10.69 (+3.2,\nModerate), and 10.48 (+3.17, Hard) on the KITTI autonomous driving dataset by\nAP|R11 at 0.7 IoU threshold, respectively. It is important to note that the\nresults of SpikeSMOKE can significantly reduce energy consumption compared to\nthe results on SMOKE. For example,the energy consumption can be reduced by\n72.2% on the hard category, while the detection performance is reduced by only\n4%. SpikeSMOKE-L (lightweight) can further reduce the amount of parameters by 3\ntimes and computation by 10 times compared to SMOKE.", "AI": {"tldr": "论文提出了一种基于脉冲神经网络（SNN）的低功耗单目3D目标检测架构SpikeSMOKE，通过跨尺度门控编码机制（CSGC）增强特征表示，并设计了轻量级残差块以降低计算量。", "motivation": "由于3D目标检测在自动驾驶等领域的广泛应用导致能耗增加，研究低能耗解决方案具有重要意义。SNN因其低功耗特性成为潜在方案。", "method": "提出SpikeSMOKE架构，结合CSGC机制（跨尺度融合注意力方法与门控过滤机制）增强特征表示，并设计轻量级残差块以减少计算量。", "result": "在KITTI数据集上，SpikeSMOKE的检测性能显著提升（AP|R11指标），同时能耗降低72.2%（Hard类别），性能仅下降4%。轻量版进一步减少参数和计算量。", "conclusion": "SpikeSMOKE为低功耗单目3D目标检测提供了有效解决方案，通过CSGC和轻量化设计平衡了性能与能耗。"}}
{"id": "2506.06474", "pdf": "https://arxiv.org/pdf/2506.06474", "abs": "https://arxiv.org/abs/2506.06474", "authors": ["Everett Richards", "Bipul Thapa", "Lena Mashayekhy"], "title": "Edge-Enabled Collaborative Object Detection for Real-Time Multi-Vehicle Perception", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.MA", "cs.NI", "I.4.8; I.2.10; I.2.11; I.2.9; C.2.4"], "comment": "This paper has been accepted to IEEE EDGE 2025. The final version\n  will be published in IEEE Xplore later this year", "summary": "Accurate and reliable object detection is critical for ensuring the safety\nand efficiency of Connected Autonomous Vehicles (CAVs). Traditional on-board\nperception systems have limited accuracy due to occlusions and blind spots,\nwhile cloud-based solutions introduce significant latency, making them\nunsuitable for real-time processing demands required for autonomous driving in\ndynamic environments. To address these challenges, we introduce an innovative\nframework, Edge-Enabled Collaborative Object Detection (ECOD) for CAVs, that\nleverages edge computing and multi-CAV collaboration for real-time,\nmulti-perspective object detection. Our ECOD framework integrates two key\nalgorithms: Perceptive Aggregation and Collaborative Estimation (PACE) and\nVariable Object Tally and Evaluation (VOTE). PACE aggregates detection data\nfrom multiple CAVs on an edge server to enhance perception in scenarios where\nindividual CAVs have limited visibility. VOTE utilizes a consensus-based voting\nmechanism to improve the accuracy of object classification by integrating data\nfrom multiple CAVs. Both algorithms are designed at the edge to operate in\nreal-time, ensuring low-latency and reliable decision-making for CAVs. We\ndevelop a hardware-based controlled testbed consisting of camera-equipped\nrobotic CAVs and an edge server to evaluate the efficacy of our framework. Our\nexperimental results demonstrate the significant benefits of ECOD in terms of\nimproved object classification accuracy, outperforming traditional\nsingle-perspective onboard approaches by up to 75%, while ensuring low-latency,\nedge-driven real-time processing. This research highlights the potential of\nedge computing to enhance collaborative perception for latency-sensitive\nautonomous systems.", "AI": {"tldr": "论文提出了一种基于边缘计算和多CAV协作的实时多视角目标检测框架ECOD，通过PACE和VOTE算法提升检测精度和实时性。", "motivation": "传统车载感知系统因遮挡和盲区精度有限，云端解决方案延迟高，无法满足自动驾驶实时需求。", "method": "ECOD框架整合PACE（感知聚合与协作估计）和VOTE（基于共识的投票机制）算法，利用边缘计算实现低延迟处理。", "result": "实验显示ECOD在目标分类精度上比传统单视角方法提升高达75%，同时保持低延迟。", "conclusion": "边缘计算可显著提升协作感知能力，适用于延迟敏感的自动驾驶系统。"}}
{"id": "2506.07738", "pdf": "https://arxiv.org/pdf/2506.07738", "abs": "https://arxiv.org/abs/2506.07738", "authors": ["Lanjiong Li", "Guanhua Zhao", "Lingting Zhu", "Zeyu Cai", "Lequan Yu", "Jian Zhang", "Zeyu Wang"], "title": "AssetDropper: Asset Extraction via Diffusion Models with Reward-Driven Optimization", "categories": ["cs.CV"], "comment": "SIGGRAPH 2025. 11 pages, 12 figures", "summary": "Recent research on generative models has primarily focused on creating\nproduct-ready visual outputs; however, designers often favor access to\nstandardized asset libraries, a domain that has yet to be significantly\nenhanced by generative capabilities. Although open-world scenes provide ample\nraw materials for designers, efficiently extracting high-quality, standardized\nassets remains a challenge. To address this, we introduce AssetDropper, the\nfirst framework designed to extract assets from reference images, providing\nartists with an open-world asset palette. Our model adeptly extracts a front\nview of selected subjects from input images, effectively handling complex\nscenarios such as perspective distortion and subject occlusion. We establish a\nsynthetic dataset of more than 200,000 image-subject pairs and a real-world\nbenchmark with thousands more for evaluation, facilitating the exploration of\nfuture research in downstream tasks. Furthermore, to ensure precise asset\nextraction that aligns well with the image prompts, we employ a pre-trained\nreward model to fulfill a closed-loop with feedback. We design the reward model\nto perform an inverse task that pastes the extracted assets back into the\nreference sources, which assists training with additional consistency and\nmitigates hallucination. Extensive experiments show that, with the aid of\nreward-driven optimization, AssetDropper achieves the state-of-the-art results\nin asset extraction. Project page: AssetDropper.github.io.", "AI": {"tldr": "AssetDropper是一个框架，用于从参考图像中提取标准化资产，解决了设计师在开放世界场景中高效提取高质量资产的挑战。", "motivation": "设计师更倾向于使用标准化资产库，而现有生成模型未能显著提升这一领域。开放世界场景提供了丰富素材，但高效提取高质量资产仍具挑战性。", "method": "AssetDropper通过提取输入图像中选定对象的前视图，处理复杂场景（如透视变形和遮挡），并使用预训练的奖励模型进行闭环反馈优化。", "result": "AssetDropper在资产提取任务中实现了最先进的性能，并通过奖励驱动优化提高了提取精度。", "conclusion": "AssetDropper为设计师提供了开放世界资产调色板，并通过合成和真实数据集支持未来研究。"}}
{"id": "2506.06483", "pdf": "https://arxiv.org/pdf/2506.06483", "abs": "https://arxiv.org/abs/2506.06483", "authors": ["Yao Ni", "Song Wen", "Piotr Koniusz", "Anoop Cherian"], "title": "Noise Consistency Regularization for Improved Subject-Driven Image Synthesis", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Fine-tuning Stable Diffusion enables subject-driven image synthesis by\nadapting the model to generate images containing specific subjects. However,\nexisting fine-tuning methods suffer from two key issues: underfitting, where\nthe model fails to reliably capture subject identity, and overfitting, where it\nmemorizes the subject image and reduces background diversity. To address these\nchallenges, we propose two auxiliary consistency losses for diffusion\nfine-tuning. First, a prior consistency regularization loss ensures that the\npredicted diffusion noise for prior (non-subject) images remains consistent\nwith that of the pretrained model, improving fidelity. Second, a subject\nconsistency regularization loss enhances the fine-tuned model's robustness to\nmultiplicative noise modulated latent code, helping to preserve subject\nidentity while improving diversity. Our experimental results demonstrate that\nincorporating these losses into fine-tuning not only preserves subject identity\nbut also enhances image diversity, outperforming DreamBooth in terms of CLIP\nscores, background variation, and overall visual quality.", "AI": {"tldr": "论文提出两种辅助一致性损失（先验一致性正则化损失和主题一致性正则化损失），用于改进Stable Diffusion微调中的欠拟合和过拟合问题，提升主题身份保留和图像多样性。", "motivation": "现有微调方法在主题驱动图像合成中存在欠拟合（无法可靠捕捉主题身份）和过拟合（记忆主题图像并减少背景多样性）问题。", "method": "提出两种辅助一致性损失：1）先验一致性正则化损失，确保非主题图像的扩散噪声预测与预训练模型一致；2）主题一致性正则化损失，增强模型对噪声调制潜码的鲁棒性。", "result": "实验表明，加入这两种损失后，微调模型在主题身份保留和图像多样性上表现更优，CLIP分数、背景变化和视觉质量均优于DreamBooth。", "conclusion": "提出的方法有效解决了微调中的欠拟合和过拟合问题，同时提升了主题身份保留和图像多样性。"}}
{"id": "2506.07739", "pdf": "https://arxiv.org/pdf/2506.07739", "abs": "https://arxiv.org/abs/2506.07739", "authors": ["Jing Zhong", "Jun Yin", "Peilin Li", "Pengyu Zeng", "Miao Zhang", "Shuai Lu", "Ran Luo"], "title": "ArchiLense: A Framework for Quantitative Analysis of Architectural Styles Based on Vision Large Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Architectural cultures across regions are characterized by stylistic\ndiversity, shaped by historical, social, and technological contexts in addition\nto geograph-ical conditions. Understanding architectural styles requires the\nability to describe and analyze the stylistic features of different architects\nfrom various regions through visual observations of architectural imagery.\nHowever, traditional studies of architectural culture have largely relied on\nsubjective expert interpretations and historical literature reviews, often\nsuffering from regional biases and limited ex-planatory scope. To address these\nchallenges, this study proposes three core contributions: (1) We construct a\nprofessional architectural style dataset named ArchDiffBench, which comprises\n1,765 high-quality architectural images and their corresponding style\nannotations, collected from different regions and historical periods. (2) We\npropose ArchiLense, an analytical framework grounded in Vision-Language Models\nand constructed using the ArchDiffBench dataset. By integrating ad-vanced\ncomputer vision techniques, deep learning, and machine learning algo-rithms,\nArchiLense enables automatic recognition, comparison, and precise\nclassi-fication of architectural imagery, producing descriptive language\noutputs that ar-ticulate stylistic differences. (3) Extensive evaluations show\nthat ArchiLense achieves strong performance in architectural style recognition,\nwith a 92.4% con-sistency rate with expert annotations and 84.5% classification\naccuracy, effec-tively capturing stylistic distinctions across images. The\nproposed approach transcends the subjectivity inherent in traditional analyses\nand offers a more objective and accurate perspective for comparative studies of\narchitectural culture.", "AI": {"tldr": "该论文提出了一种基于视觉语言模型的框架ArchiLense，用于自动识别和分类建筑风格，通过构建数据集ArchDiffBench和结合计算机视觉技术，显著提升了分析的客观性和准确性。", "motivation": "传统建筑文化研究依赖主观专家解读和历史文献，存在区域偏见和解释范围有限的问题，需要更客观的分析方法。", "method": "构建ArchDiffBench数据集（1,765张高质量建筑图像），开发ArchiLense框架，结合视觉语言模型和深度学习算法，实现自动识别与分类。", "result": "ArchiLense在建筑风格识别中表现优异，与专家标注一致性达92.4%，分类准确率为84.5%。", "conclusion": "该方法超越了传统主观分析，为建筑文化比较研究提供了更客观、准确的视角。"}}
{"id": "2506.06484", "pdf": "https://arxiv.org/pdf/2506.06484", "abs": "https://arxiv.org/abs/2506.06484", "authors": ["Manuel Sage", "Khalil Al Handawi", "Yaoyao Fiona Zhao"], "title": "The Economic Dispatch of Power-to-Gas Systems with Deep Reinforcement Learning:Tackling the Challenge of Delayed Rewards with Long-Term Energy Storage", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "comment": "Accepted for publication at the 19th ASME International Conference on\n  Energy Sustainability", "summary": "Power-to-Gas (P2G) technologies gain recognition for enabling the integration\nof intermittent renewables, such as wind and solar, into electricity grids.\nHowever, determining the most cost-effective operation of these systems is\ncomplex due to the volatile nature of renewable energy, electricity prices, and\nloads. Additionally, P2G systems are less efficient in converting and storing\nenergy compared to battery energy storage systems (BESs), and the benefits of\nconverting electricity into gas are not immediately apparent. Deep\nReinforcement Learning (DRL) has shown promise in managing the operation of\nenergy systems amidst these uncertainties. Yet, DRL techniques face\ndifficulties with the delayed reward characteristic of P2G system operation.\nPrevious research has mostly focused on short-term studies that look at the\nenergy conversion process, neglecting the long-term storage capabilities of\nP2G.\n  This study presents a new method by thoroughly examining how DRL can be\napplied to the economic operation of P2G systems, in combination with BESs and\ngas turbines, over extended periods. Through three progressively more complex\ncase studies, we assess the performance of DRL algorithms, specifically Deep\nQ-Networks and Proximal Policy Optimization, and introduce modifications to\nenhance their effectiveness. These modifications include integrating forecasts,\nimplementing penalties on the reward function, and applying strategic cost\ncalculations, all aimed at addressing the issue of delayed rewards. Our\nfindings indicate that while DRL initially struggles with the complex\ndecision-making required for P2G system operation, the adjustments we propose\nsignificantly improve its capability to devise cost-effective operation\nstrategies, thereby unlocking the potential for long-term energy storage in P2G\ntechnologies.", "AI": {"tldr": "该研究探讨了如何利用深度强化学习（DRL）优化Power-to-Gas（P2G）系统的长期经济运营，结合电池储能系统和燃气轮机，并通过改进DRL算法解决延迟奖励问题。", "motivation": "P2G技术虽能整合间歇性可再生能源，但其运营成本效益难以确定，且效率低于电池储能系统。DRL在能源系统管理中表现优异，但面临延迟奖励的挑战。", "method": "研究通过三个逐步复杂的案例，评估了Deep Q-Networks和Proximal Policy Optimization算法的性能，并引入预测整合、奖励函数惩罚和战略成本计算等改进。", "result": "改进后的DRL算法显著提升了P2G系统的成本效益运营能力，解决了延迟奖励问题，展现了长期储能的潜力。", "conclusion": "研究表明，改进的DRL方法能有效优化P2G系统的长期运营策略，为其在能源存储中的应用提供了新思路。"}}
{"id": "2506.06328", "pdf": "https://arxiv.org/pdf/2506.06328", "abs": "https://arxiv.org/abs/2506.06328", "authors": ["Aziida Nanyonga", "Joiner Keith", "Turhan Ugur", "Wild Graham"], "title": "Is BERTopic Better than PLSA for Extracting Key Topics in Aviation Safety Reports?", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "This study compares the effectiveness of BERTopic and Probabilistic Latent\nSemantic Analysis (PLSA) in extracting meaningful topics from aviation safety\nreports aiming to enhance the understanding of patterns in aviation incident\ndata. Using a dataset of over 36,000 National Transportation Safety Board\n(NTSB) reports from 2000 to 2020, BERTopic employed transformer based\nembeddings and hierarchical clustering, while PLSA utilized probabilistic\nmodelling through the Expectation-Maximization (EM) algorithm. Results showed\nthat BERTopic outperformed PLSA in topic coherence, achieving a Cv score of\n0.41 compared to PLSA 0.37, while also demonstrating superior interpretability\nas validated by aviation safety experts. These findings underscore the\nadvantages of modern transformer based approaches in analyzing complex aviation\ndatasets, paving the way for enhanced insights and informed decision-making in\naviation safety. Future work will explore hybrid models, multilingual datasets,\nand advanced clustering techniques to further improve topic modelling in this\ndomain.", "AI": {"tldr": "比较BERTopic和PLSA在航空安全报告中的主题提取效果，BERTopic表现更优。", "motivation": "提升对航空事故数据模式的理解，为航空安全决策提供支持。", "method": "使用36,000+份NTSB报告，BERTopic基于Transformer嵌入和层次聚类，PLSA基于EM算法。", "result": "BERTopic在主题连贯性（Cv 0.41 vs 0.37）和可解释性上优于PLSA。", "conclusion": "现代Transformer方法在复杂航空数据分析中优势明显，未来将探索混合模型和多语言数据。"}}
{"id": "2506.07740", "pdf": "https://arxiv.org/pdf/2506.07740", "abs": "https://arxiv.org/abs/2506.07740", "authors": ["Yingping Liang", "Ying Fu", "Yutao Hu", "Wenqi Shao", "Jiaming Liu", "Debing Zhang"], "title": "Flow-Anything: Learning Real-World Optical Flow Estimation from Large-Scale Single-view Images", "categories": ["cs.CV"], "comment": null, "summary": "Optical flow estimation is a crucial subfield of computer vision, serving as\na foundation for video tasks. However, the real-world robustness is limited by\nanimated synthetic datasets for training. This introduces domain gaps when\napplied to real-world applications and limits the benefits of scaling up\ndatasets. To address these challenges, we propose \\textbf{Flow-Anything}, a\nlarge-scale data generation framework designed to learn optical flow estimation\nfrom any single-view images in the real world. We employ two effective steps to\nmake data scaling-up promising. First, we convert a single-view image into a 3D\nrepresentation using advanced monocular depth estimation networks. This allows\nus to render optical flow and novel view images under a virtual camera. Second,\nwe develop an Object-Independent Volume Rendering module and a Depth-Aware\nInpainting module to model the dynamic objects in the 3D representation. These\ntwo steps allow us to generate realistic datasets for training from large-scale\nsingle-view images, namely \\textbf{FA-Flow Dataset}. For the first time, we\ndemonstrate the benefits of generating optical flow training data from\nlarge-scale real-world images, outperforming the most advanced unsupervised\nmethods and supervised methods on synthetic datasets. Moreover, our models\nserve as a foundation model and enhance the performance of various downstream\nvideo tasks.", "AI": {"tldr": "提出Flow-Anything框架，通过单视角图像生成大规模真实世界光流训练数据，解决合成数据域差距问题。", "motivation": "光流估计在真实世界中的鲁棒性受限于合成数据训练，导致域差距问题。", "method": "1. 使用单目深度估计网络将单视角图像转换为3D表示；2. 开发对象无关体积渲染和深度感知修复模块。", "result": "生成的FA-Flow数据集在光流估计任务中优于现有无监督和监督方法。", "conclusion": "Flow-Anything框架为光流估计和下游视频任务提供了高效的数据生成基础模型。"}}
{"id": "2506.06329", "pdf": "https://arxiv.org/pdf/2506.06329", "abs": "https://arxiv.org/abs/2506.06329", "authors": ["Zheng Cao", "Wanchaloem Wunkaew", "Helyette Geman"], "title": "The Hype Index: an NLP-driven Measure of Market News Attention", "categories": ["q-fin.ST", "cs.CE", "cs.CL"], "comment": null, "summary": "This paper introduces the Hype Index as a novel metric to quantify media\nattention toward large-cap equities, leveraging advances in Natural Language\nProcessing (NLP) for extracting predictive signals from financial news. Using\nthe S&P 100 as the focus universe, we first construct a News Count-Based Hype\nIndex, which measures relative media exposure by computing the share of news\narticles referencing each stock or sector. We then extend it to the\nCapitalization Adjusted Hype Index, adjusts for economic size by taking the\nratio of a stock's or sector's media weight to its market capitalization weight\nwithin its industry or sector. We compute both versions of the Hype Index at\nthe stock and sector levels, and evaluate them through multiple lenses: (1)\ntheir classification into different hype groups, (2) their associations with\nreturns, volatility, and VIX index at various lags, (3) their signaling power\nfor short-term market movements, and (4) their empirical properties including\ncorrelations, samplings, and trends. Our findings suggest that the Hype Index\nfamily provides a valuable set of tools for stock volatility analysis, market\nsignaling, and NLP extensions in Finance.", "AI": {"tldr": "论文提出了一种名为Hype Index的新指标，用于量化媒体对大盘股的关注度，利用NLP技术从金融新闻中提取预测信号。通过S&P 100股票，构建了新闻计数和市值调整的Hype Index，并验证了其在波动性分析、市场信号和金融NLP扩展中的价值。", "motivation": "量化媒体对股票的关注度，探索其在金融市场中的预测能力。", "method": "构建新闻计数和市值调整的Hype Index，并通过分类、收益关联、波动性分析等多维度验证其有效性。", "result": "Hype Index家族在股票波动性分析、市场信号和金融NLP扩展中表现出实用价值。", "conclusion": "Hype Index为金融分析和NLP应用提供了新的工具集。"}}
{"id": "2506.07750", "pdf": "https://arxiv.org/pdf/2506.07750", "abs": "https://arxiv.org/abs/2506.07750", "authors": ["Hyunsoo Kim", "Donghyun Kim", "Suhyun Kim"], "title": "Difference Inversion: Interpolate and Isolate the Difference with Token Consistency for Image Analogy Generation", "categories": ["cs.CV"], "comment": "Published at CVPR 2025", "summary": "How can we generate an image B' that satisfies A:A'::B:B', given the input\nimages A,A' and B? Recent works have tackled this challenge through approaches\nlike visual in-context learning or visual instruction. However, these methods\nare typically limited to specific models (e.g. InstructPix2Pix. Inpainting\nmodels) rather than general diffusion models (e.g. Stable Diffusion, SDXL).\nThis dependency may lead to inherited biases or lower editing capabilities. In\nthis paper, we propose Difference Inversion, a method that isolates only the\ndifference from A and A' and applies it to B to generate a plausible B'. To\naddress model dependency, it is crucial to structure prompts in the form of a\n\"Full Prompt\" suitable for input to stable diffusion models, rather than using\nan \"Instruction Prompt\". To this end, we accurately extract the Difference\nbetween A and A' and combine it with the prompt of B, enabling a plug-and-play\napplication of the difference. To extract a precise difference, we first\nidentify it through 1) Delta Interpolation. Additionally, to ensure accurate\ntraining, we propose the 2) Token Consistency Loss and 3) Zero Initialization\nof Token Embeddings. Our extensive experiments demonstrate that Difference\nInversion outperforms existing baselines both quantitatively and qualitatively,\nindicating its ability to generate more feasible B' in a model-agnostic manner.", "AI": {"tldr": "提出了一种名为Difference Inversion的方法，通过提取A和A'之间的差异并应用到B上，生成B'，解决了现有方法对特定模型的依赖问题。", "motivation": "现有方法依赖于特定模型（如InstructPix2Pix），可能导致偏见或编辑能力受限，因此需要一种更通用的解决方案。", "method": "通过Delta Interpolation提取A和A'的差异，结合Token Consistency Loss和Zero Initialization of Token Embeddings，生成适用于稳定扩散模型的Full Prompt。", "result": "实验表明，Difference Inversion在定量和定性上均优于现有基线，能够以模型无关的方式生成更可行的B'。", "conclusion": "Difference Inversion是一种有效的通用方法，能够灵活应用于不同扩散模型，提升图像编辑能力。"}}
{"id": "2506.06499", "pdf": "https://arxiv.org/pdf/2506.06499", "abs": "https://arxiv.org/abs/2506.06499", "authors": ["Alex Havrilla", "Edward Hughes", "Mikayel Samvelyan", "Jacob Abernethy"], "title": "Synthetic Problem Generation for Reasoning via Quality-Diversity Algorithms", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language model (LLM) driven synthetic data generation has emerged as a\npowerful method for improving model reasoning capabilities. However, most\nmethods either distill large state-of-the-art models into small students or use\nnatural ground-truth problem statements to guarantee problem statement quality.\nThis limits the scalability of these approaches to more complex and diverse\nproblem domains. To address this, we present SPARQ: Synthetic Problem\nGeneration for Reasoning via Quality-Diversity Algorithms, a novel approach for\ngenerating high-quality and diverse synthetic math problem and solution pairs\nusing only a single model by measuring a problem's solve-rate: a proxy for\nproblem difficulty. Starting from a seed dataset of 7.5K samples, we generate\nover 20 million new problem-solution pairs. We show that filtering the\ngenerated data by difficulty and then fine-tuning the same model on the\nresulting data improves relative model performance by up to 24\\%. Additionally,\nwe conduct ablations studying the impact of synthetic data quantity, quality\nand diversity on model generalization. We find that higher quality, as measured\nby problem difficulty, facilitates better in-distribution performance. Further,\nwhile generating diverse synthetic data does not as strongly benefit\nin-distribution performance, filtering for more diverse data facilitates more\nrobust OOD generalization. We also confirm the existence of model and data\nscaling laws for synthetically generated problems, which positively benefit\ndownstream model generalization.", "AI": {"tldr": "SPARQ提出了一种基于质量多样性算法的方法，通过单一模型生成高质量且多样化的数学问题-解决方案对，显著提升了模型性能。", "motivation": "现有方法在复杂和多样化问题领域的扩展性受限，SPARQ旨在通过合成数据生成解决这一问题。", "method": "利用质量多样性算法，通过测量问题解决率（难度代理）生成问题-解决方案对，并进行难度和多样性筛选。", "result": "从7.5K种子数据生成2000万对问题-解决方案，模型性能提升24%；多样性数据增强OOD泛化能力。", "conclusion": "SPARQ展示了合成数据生成在提升模型性能和泛化能力方面的潜力，并验证了模型与数据的扩展规律。"}}
{"id": "2506.07773", "pdf": "https://arxiv.org/pdf/2506.07773", "abs": "https://arxiv.org/abs/2506.07773", "authors": ["Mohamed Djilani", "Nassim Ali Ousalah", "Nidhal Eddine Chenni"], "title": "Trend-Aware Fashion Recommendation with Visual Segmentation and Semantic Similarity", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We introduce a trend-aware and visually-grounded fashion recommendation\nsystem that integrates deep visual representations, garment-aware segmentation,\nsemantic category similarity and user behavior simulation. Our pipeline\nextracts focused visual embeddings by masking non-garment regions via semantic\nsegmentation followed by feature extraction using pretrained CNN backbones\n(ResNet-50, DenseNet-121, VGG16). To simulate realistic shopping behavior, we\ngenerate synthetic purchase histories influenced by user-specific trendiness\nand item popularity. Recommendations are computed using a weighted scoring\nfunction that fuses visual similarity, semantic coherence and popularity\nalignment. Experiments on the DeepFashion dataset demonstrate consistent gender\nalignment and improved category relevance, with ResNet-50 achieving 64.95%\ncategory similarity and lowest popularity MAE. An ablation study confirms the\ncomplementary roles of visual and popularity cues. Our method provides a\nscalable framework for personalized fashion recommendations that balances\nindividual style with emerging trends. Our implementation is available at\nhttps://github.com/meddjilani/FashionRecommender", "AI": {"tldr": "提出了一种结合视觉特征、语义分割和用户行为模拟的时尚推荐系统，通过加权评分函数生成个性化推荐。", "motivation": "解决时尚推荐中如何平衡个人风格与流行趋势的问题。", "method": "使用语义分割提取服装区域特征，结合预训练CNN提取视觉嵌入，模拟用户购物行为生成推荐。", "result": "在DeepFashion数据集上表现优异，ResNet-50达到64.95%的类别相似度。", "conclusion": "该方法为个性化时尚推荐提供了可扩展的框架，平衡了个人风格与流行趋势。"}}
{"id": "2506.06509", "pdf": "https://arxiv.org/pdf/2506.06509", "abs": "https://arxiv.org/abs/2506.06509", "authors": ["Jakub Jagielski", "Markus Abel"], "title": "Private GPTs for LLM-driven testing in software development and machine learning", "categories": ["cs.SE", "cs.AI", "I.2.1"], "comment": "5 pages, 10 figures", "summary": "In this contribution, we examine the capability of private GPTs to\nautomatically generate executable test code based on requirements. More\nspecifically, we use acceptance criteria as input, formulated as part of epics,\nor stories, which are typically used in modern development processes. This\ngives product owners, or business intelligence, respectively, a way to directly\nproduce testable criteria through the use of LLMs. We explore the quality of\nthe so-produced tests in two ways: i) directly by letting the LLM generate code\nfrom requirements, ii) through an intermediate step using Gherkin syntax. As a\nresult, it turns out that the two-step procedure yields better results -where\nwe define better in terms of human readability and best coding practices, i.e.\nlines of code and use of additional libraries typically used in testing.\nConcretely, we evaluate prompt effectiveness across two scenarios: a simple\n\"Hello World\" program and a digit classification model, showing that structured\nprompts lead to higher-quality test outputs.", "AI": {"tldr": "研究了私有GPT基于需求自动生成可执行测试代码的能力，发现通过Gherkin语法作为中间步骤的方法效果更好。", "motivation": "探索如何利用LLM直接从需求生成测试代码，为产品负责人提供直接生成可测试标准的方法。", "method": "比较两种方法：直接生成代码和使用Gherkin语法作为中间步骤。", "result": "两步法（使用Gherkin）在代码可读性和最佳实践方面表现更优。", "conclusion": "结构化提示能生成更高质量的测试输出。"}}
{"id": "2506.07778", "pdf": "https://arxiv.org/pdf/2506.07778", "abs": "https://arxiv.org/abs/2506.07778", "authors": ["Yichang Xu", "Gaowen Liu", "Ramana Rao Kompella", "Sihao Hu", "Tiansheng Huang", "Fatih Ilhan", "Selim Furkan Tekin", "Zachary Yahn", "Ling Liu"], "title": "Language-Vision Planner and Executor for Text-to-Visual Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "The advancement in large language models (LLMs) and large vision models has\nfueled the rapid progress in multi-modal visual-text reasoning capabilities.\nHowever, existing vision-language models (VLMs) to date suffer from\ngeneralization performance. Inspired by recent development in LLMs for visual\nreasoning, this paper presents VLAgent, an AI system that can create a\nstep-by-step visual reasoning plan with an easy-to-understand script and\nexecute each step of the plan in real time by integrating planning script with\nexecution verifications via an automated process supported by VLAgent. In the\ntask planning phase, VLAgent fine-tunes an LLM through in-context learning to\ngenerate a step-by-step planner for each user-submitted text-visual reasoning\ntask. During the plan execution phase, VLAgent progressively refines the\ncomposition of neuro-symbolic executable modules to generate high-confidence\nreasoning results. VLAgent has three unique design characteristics: First, we\nimprove the quality of plan generation through in-context learning, improving\nlogic reasoning by reducing erroneous logic steps, incorrect programs, and LLM\nhallucinations. Second, we design a syntax-semantics parser to identify and\ncorrect additional logic errors of the LLM-generated planning script prior to\nlaunching the plan executor. Finally, we employ the ensemble method to improve\nthe generalization performance of our step-executor. Extensive experiments with\nfour visual reasoning benchmarks (GQA, MME, NLVR2, VQAv2) show that VLAgent\nachieves significant performance enhancement for multimodal text-visual\nreasoning applications, compared to the exiting representative VLMs and LLM\nbased visual composition approaches like ViperGPT and VisProg, thanks to the\nnovel optimization modules of VLAgent back-engine (SS-Parser, Plan Repairer,\nOutput Verifiers). Code and data will be made available upon paper acceptance.", "AI": {"tldr": "VLAgent是一个结合规划脚本与执行验证的多模态视觉-文本推理系统，通过上下文学习优化逻辑推理，显著提升了性能。", "motivation": "现有视觉-语言模型（VLMs）在泛化性能上表现不佳，VLAgent旨在通过逐步规划和执行验证解决这一问题。", "method": "VLAgent通过上下文学习微调LLM生成逐步规划器，结合神经符号模块执行计划，并设计语法-语义解析器和集成方法优化执行。", "result": "在四个视觉推理基准测试中，VLAgent性能显著优于现有VLMs和LLM-based方法。", "conclusion": "VLAgent通过其独特的优化模块（如SS-Parser和Plan Repairer）在多模态推理任务中表现出色。"}}
{"id": "2506.06355", "pdf": "https://arxiv.org/pdf/2506.06355", "abs": "https://arxiv.org/abs/2506.06355", "authors": ["Lingyao Li", "Dawei Li", "Zhenhui Ou", "Xiaoran Xu", "Jingxiao Liu", "Zihui Ma", "Runlong Yu", "Min Deng"], "title": "LLMs as World Models: Data-Driven and Human-Centered Pre-Event Simulation for Disaster Impact Assessment", "categories": ["cs.CY", "cs.CE", "cs.CL", "cs.CV"], "comment": null, "summary": "Efficient simulation is essential for enhancing proactive preparedness for\nsudden-onset disasters such as earthquakes. Recent advancements in large\nlanguage models (LLMs) as world models show promise in simulating complex\nscenarios. This study examines multiple LLMs to proactively estimate perceived\nearthquake impacts. Leveraging multimodal datasets including geospatial,\nsocioeconomic, building, and street-level imagery data, our framework generates\nModified Mercalli Intensity (MMI) predictions at zip code and county scales.\nEvaluations on the 2014 Napa and 2019 Ridgecrest earthquakes using USGS ''Did\nYou Feel It? (DYFI)'' reports demonstrate significant alignment, as evidenced\nby a high correlation of 0.88 and a low RMSE of 0.77 as compared to real\nreports at the zip code level. Techniques such as RAG and ICL can improve\nsimulation performance, while visual inputs notably enhance accuracy compared\nto structured numerical data alone. These findings show the promise of LLMs in\nsimulating disaster impacts that can help strengthen pre-event planning.", "AI": {"tldr": "该研究利用大语言模型（LLMs）模拟地震影响，结合多模态数据预测地震烈度，结果显示与真实报告高度一致。", "motivation": "通过高效模拟提升对突发性灾害（如地震）的主动应对能力。", "method": "使用多模态数据集（地理空间、社会经济、建筑和街景图像数据），结合RAG和ICL技术，生成地震烈度预测。", "result": "在2014年Napa和2019年Ridgecrest地震中，预测与实际报告高度相关（0.88），RMSE低至0.77。", "conclusion": "LLMs在模拟灾害影响方面具有潜力，有助于加强灾前规划。"}}
{"id": "2506.07779", "pdf": "https://arxiv.org/pdf/2506.07779", "abs": "https://arxiv.org/abs/2506.07779", "authors": ["Beining Xu", "Junxian Li"], "title": "Design and Evaluation of Deep Learning-Based Dual-Spectrum Image Fusion Methods", "categories": ["cs.CV"], "comment": "11 pages, 13 figures", "summary": "Visible images offer rich texture details, while infrared images emphasize\nsalient targets. Fusing these complementary modalities enhances scene\nunderstanding, particularly for advanced vision tasks under challenging\nconditions. Recently, deep learning-based fusion methods have gained attention,\nbut current evaluations primarily rely on general-purpose metrics without\nstandardized benchmarks or downstream task performance. Additionally, the lack\nof well-developed dual-spectrum datasets and fair algorithm comparisons hinders\nprogress.\n  To address these gaps, we construct a high-quality dual-spectrum dataset\ncaptured in campus environments, comprising 1,369 well-aligned visible-infrared\nimage pairs across four representative scenarios: daytime, nighttime, smoke\nocclusion, and underpasses. We also propose a comprehensive and fair evaluation\nframework that integrates fusion speed, general metrics, and object detection\nperformance using the lang-segment-anything model to ensure fairness in\ndownstream evaluation.\n  Extensive experiments benchmark several state-of-the-art fusion algorithms\nunder this framework. Results demonstrate that fusion models optimized for\ndownstream tasks achieve superior performance in target detection, especially\nin low-light and occluded scenes. Notably, some algorithms that perform well on\ngeneral metrics do not translate to strong downstream performance, highlighting\nlimitations of current evaluation practices and validating the necessity of our\nproposed framework.\n  The main contributions of this work are: (1)a campus-oriented dual-spectrum\ndataset with diverse and challenging scenes; (2) a task-aware, comprehensive\nevaluation framework; and (3) thorough comparative analysis of leading fusion\nmethods across multiple datasets, offering insights for future development.", "AI": {"tldr": "论文提出了一种高质量的双光谱数据集和全面的评估框架，用于可见光和红外图像融合方法的评估，并验证了融合模型在下游任务中的性能。", "motivation": "当前可见光和红外图像融合方法的评估缺乏标准化基准和下游任务性能验证，且数据集和公平比较不足。", "method": "构建了一个包含1,369对对齐图像的高质量双光谱数据集，并提出一个综合评估框架，结合融合速度、通用指标和目标检测性能。", "result": "实验表明，针对下游任务优化的融合模型在目标检测中表现更优，尤其是在低光和遮挡场景中。", "conclusion": "论文的主要贡献包括高质量数据集、任务感知的评估框架以及对领先融合方法的全面比较，为未来发展提供了参考。"}}
{"id": "2506.06532", "pdf": "https://arxiv.org/pdf/2506.06532", "abs": "https://arxiv.org/abs/2506.06532", "authors": ["Zijiang Yan", "Hao Zhou", "Jianhua Pei", "Hina Tabassum"], "title": "Hierarchical and Collaborative LLM-Based Control for Multi-UAV Motion and Communication in Integrated Terrestrial and Non-Terrestrial Networks", "categories": ["cs.LG", "cs.AI", "cs.NI", "cs.RO", "cs.SY", "eess.SY"], "comment": "Accepted in ICML 2025 Workshop on Machine Learning for Wireless\n  Communication and Networks (ML4Wireless)", "summary": "Unmanned aerial vehicles (UAVs) have been widely adopted in various\nreal-world applications. However, the control and optimization of multi-UAV\nsystems remain a significant challenge, particularly in dynamic and constrained\nenvironments. This work explores the joint motion and communication control of\nmultiple UAVs operating within integrated terrestrial and non-terrestrial\nnetworks that include high-altitude platform stations (HAPS). Specifically, we\nconsider an aerial highway scenario in which UAVs must accelerate, decelerate,\nand change lanes to avoid collisions and maintain overall traffic flow.\nDifferent from existing studies, we propose a novel hierarchical and\ncollaborative method based on large language models (LLMs). In our approach, an\nLLM deployed on the HAPS performs UAV access control, while another LLM onboard\neach UAV handles motion planning and control. This LLM-based framework\nleverages the rich knowledge embedded in pre-trained models to enable both\nhigh-level strategic planning and low-level tactical decisions. This\nknowledge-driven paradigm holds great potential for the development of\nnext-generation 3D aerial highway systems. Experimental results demonstrate\nthat our proposed collaborative LLM-based method achieves higher system\nrewards, lower operational costs, and significantly reduced UAV collision rates\ncompared to baseline approaches.", "AI": {"tldr": "该论文提出了一种基于大型语言模型（LLM）的分层协作方法，用于多无人机（UAV）在动态约束环境中的联合运动与通信控制，显著提升了系统性能。", "motivation": "多无人机系统在动态和受限环境中的控制与优化是一个重要挑战，尤其是在集成地面和非地面网络（如高空平台站HAPS）的场景中。", "method": "采用分层协作的LLM框架，HAPS上的LLM负责无人机接入控制，而每架无人机上的LLM负责运动规划与控制。", "result": "实验表明，该方法在系统奖励、运营成本和无人机碰撞率方面均优于基线方法。", "conclusion": "基于LLM的知识驱动范式为下一代3D空中高速公路系统的开发提供了巨大潜力。"}}
{"id": "2506.07785", "pdf": "https://arxiv.org/pdf/2506.07785", "abs": "https://arxiv.org/abs/2506.07785", "authors": ["Qi Yang", "Chenghao Zhang", "Lubin Fan", "Kun Ding", "Jieping Ye", "Shiming Xiang"], "title": "Re-ranking Reasoning Context with Tree Search Makes Large Vision-Language Models Stronger", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "ICML 2025 Spotlight. 22 pages, 16 figures", "summary": "Recent advancements in Large Vision Language Models (LVLMs) have\nsignificantly improved performance in Visual Question Answering (VQA) tasks\nthrough multimodal Retrieval-Augmented Generation (RAG). However, existing\nmethods still face challenges, such as the scarcity of knowledge with reasoning\nexamples and erratic responses from retrieved knowledge. To address these\nissues, in this study, we propose a multimodal RAG framework, termed RCTS,\nwhich enhances LVLMs by constructing a Reasoning Context-enriched knowledge\nbase and a Tree Search re-ranking method. Specifically, we introduce a\nself-consistent evaluation mechanism to enrich the knowledge base with\nintrinsic reasoning patterns. We further propose a Monte Carlo Tree Search with\nHeuristic Rewards (MCTS-HR) to prioritize the most relevant examples. This\nensures that LVLMs can leverage high-quality contextual reasoning for better\nand more consistent responses. Extensive experiments demonstrate that our\nframework achieves state-of-the-art performance on multiple VQA datasets,\nsignificantly outperforming In-Context Learning (ICL) and Vanilla-RAG methods.\nIt highlights the effectiveness of our knowledge base and re-ranking method in\nimproving LVLMs. Our code is available at https://github.com/yannqi/RCTS-RAG.", "AI": {"tldr": "论文提出了一种多模态检索增强生成框架RCTS，通过构建推理上下文丰富的知识库和树搜索重排序方法，提升大型视觉语言模型在视觉问答任务中的性能。", "motivation": "现有方法在知识推理示例稀缺和检索知识响应不稳定方面存在挑战。", "method": "提出RCTS框架，包括自洽评估机制丰富知识库，以及蒙特卡洛树搜索与启发式奖励（MCTS-HR）重排序方法。", "result": "在多个VQA数据集上实现最先进性能，显著优于上下文学习和传统检索增强生成方法。", "conclusion": "RCTS框架通过高质量推理上下文和重排序方法，有效提升了模型的性能和一致性。"}}
{"id": "2506.07803", "pdf": "https://arxiv.org/pdf/2506.07803", "abs": "https://arxiv.org/abs/2506.07803", "authors": ["Eduard Allakhverdov", "Dmitrii Tarasov", "Elizaveta Goncharova", "Andrey Kuznetsov"], "title": "Image Reconstruction as a Tool for Feature Analysis", "categories": ["cs.CV", "68T10, 68T30, 68T45", "I.2.10"], "comment": "23 pages, 14 figures", "summary": "Vision encoders are increasingly used in modern applications, from\nvision-only models to multimodal systems such as vision-language models.\nDespite their remarkable success, it remains unclear how these architectures\nrepresent features internally. Here, we propose a novel approach for\ninterpreting vision features via image reconstruction. We compare two related\nmodel families, SigLIP and SigLIP2, which differ only in their training\nobjective, and show that encoders pre-trained on image-based tasks retain\nsignificantly more image information than those trained on non-image tasks such\nas contrastive learning. We further apply our method to a range of vision\nencoders, ranking them by the informativeness of their feature representations.\nFinally, we demonstrate that manipulating the feature space yields predictable\nchanges in reconstructed images, revealing that orthogonal rotations (rather\nthan spatial transformations) control color encoding. Our approach can be\napplied to any vision encoder, shedding light on the inner structure of its\nfeature space. The code and model weights to reproduce the experiments are\navailable in GitHub.", "AI": {"tldr": "论文提出了一种通过图像重建解释视觉特征的新方法，比较了SigLIP和SigLIP2模型，发现基于图像任务预训练的编码器保留更多图像信息，并揭示了特征空间的操作对图像重建的影响。", "motivation": "尽管视觉编码器在应用中表现优异，但其内部特征表示方式尚不明确，因此需要一种方法来解释和理解这些特征。", "method": "通过图像重建比较不同视觉编码器的特征表示能力，特别是SigLIP和SigLIP2模型，并分析特征空间的操作对重建结果的影响。", "result": "基于图像任务预训练的编码器保留更多图像信息；特征空间的旋转操作控制颜色编码。", "conclusion": "该方法适用于任何视觉编码器，揭示了特征空间的内在结构，为理解视觉编码器提供了新视角。"}}
{"id": "2506.06540", "pdf": "https://arxiv.org/pdf/2506.06540", "abs": "https://arxiv.org/abs/2506.06540", "authors": ["Patrick Y. Wu"], "title": "Large Language Models Can Be a Viable Substitute for Expert Political Surveys When a Shock Disrupts Traditional Measurement Approaches", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "19 pages, 6 figures", "summary": "After a disruptive event or shock, such as the Department of Government\nEfficiency (DOGE) federal layoffs of 2025, expert judgments are colored by\nknowledge of the outcome. This can make it difficult or impossible to\nreconstruct the pre-event perceptions needed to study the factors associated\nwith the event. This position paper argues that large language models (LLMs),\ntrained on vast amounts of digital media data, can be a viable substitute for\nexpert political surveys when a shock disrupts traditional measurement. We\nanalyze the DOGE layoffs as a specific case study for this position. We use\npairwise comparison prompts with LLMs and derive ideology scores for federal\nexecutive agencies. These scores replicate pre-layoff expert measures and\npredict which agencies were targeted by DOGE. We also use this same approach\nand find that the perceptions of certain federal agencies as knowledge\ninstitutions predict which agencies were targeted by DOGE, even when\ncontrolling for ideology. This case study demonstrates that using LLMs allows\nus to rapidly and easily test the associated factors hypothesized behind the\nshock. More broadly, our case study of this recent event exemplifies how LLMs\noffer insights into the correlational factors of the shock when traditional\nmeasurement techniques fail. We conclude by proposing a two-part criterion for\nwhen researchers can turn to LLMs as a substitute for expert political surveys.", "AI": {"tldr": "论文提出使用大型语言模型（LLMs）替代专家政治调查，以研究突发事件的关联因素，并以2025年DOGE联邦裁员为例验证其可行性。", "motivation": "突发事件（如DOGE裁员）后，专家判断受结果影响，难以重建事件前的认知。LLMs可作为替代工具。", "method": "使用LLMs进行成对比较提示，生成联邦机构的意识形态分数，并与裁员目标关联。", "result": "LLMs生成的分数复制了裁员前的专家测量，并预测了裁员目标。某些机构的知识属性也能预测裁员。", "conclusion": "LLMs能快速测试突发事件的关联因素，提出使用LLMs的两部分标准。"}}
{"id": "2506.07809", "pdf": "https://arxiv.org/pdf/2506.07809", "abs": "https://arxiv.org/abs/2506.07809", "authors": ["Weilei Wen", "Tianyi Zhang", "Qianqian Zhao", "Zhaohui Zheng", "Chunle Guo", "Xiuli Shao", "Chongyi Li"], "title": "Incorporating Uncertainty-Guided and Top-k Codebook Matching for Real-World Blind Image Super-Resolution", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in codebook-based real image super-resolution (SR) have\nshown promising results in real-world applications. The core idea involves\nmatching high-quality image features from a codebook based on low-resolution\n(LR) image features. However, existing methods face two major challenges:\ninaccurate feature matching with the codebook and poor texture detail\nreconstruction. To address these issues, we propose a novel Uncertainty-Guided\nand Top-k Codebook Matching SR (UGTSR) framework, which incorporates three key\ncomponents: (1) an uncertainty learning mechanism that guides the model to\nfocus on texture-rich regions, (2) a Top-k feature matching strategy that\nenhances feature matching accuracy by fusing multiple candidate features, and\n(3) an Align-Attention module that enhances the alignment of information\nbetween LR and HR features. Experimental results demonstrate significant\nimprovements in texture realism and reconstruction fidelity compared to\nexisting methods. We will release the code upon formal publication.", "AI": {"tldr": "提出了一种基于不确定性引导和Top-k代码书匹配的超分辨率框架（UGTSR），解决了现有方法中特征匹配不准确和纹理细节重建差的问题。", "motivation": "现有代码书超分辨率方法在特征匹配和纹理细节重建方面存在不足，影响了图像质量。", "method": "UGTSR框架包含不确定性学习机制、Top-k特征匹配策略和Align-Attention模块，分别用于引导模型关注纹理丰富区域、提高特征匹配准确性和增强LR与HR特征对齐。", "result": "实验表明，UGTSR在纹理真实性和重建保真度上显著优于现有方法。", "conclusion": "UGTSR通过创新设计有效提升了超分辨率图像的质量，代码将在正式发表后开源。"}}
{"id": "2506.06541", "pdf": "https://arxiv.org/pdf/2506.06541", "abs": "https://arxiv.org/abs/2506.06541", "authors": ["Eugenie Lai", "Gerardo Vitagliano", "Ziyu Zhang", "Sivaprasad Sudhir", "Om Chabra", "Anna Zeng", "Anton A. Zabreyko", "Chenning Li", "Ferdi Kossmann", "Jialin Ding", "Jun Chen", "Markos Markakis", "Matthew Russo", "Weiyang Wang", "Ziniu Wu", "Michael J. Cafarella", "Lei Cao", "Samuel Madden", "Tim Kraska"], "title": "KramaBench: A Benchmark for AI Systems on Data-to-Insight Pipelines over Data Lakes", "categories": ["cs.DB", "cs.AI", "cs.MA"], "comment": null, "summary": "Constructing real-world data-to-insight pipelines often involves data\nextraction from data lakes, data integration across heterogeneous data sources,\nand diverse operations from data cleaning to analysis. The design and\nimplementation of data science pipelines require domain knowledge, technical\nexpertise, and even project-specific insights. AI systems have shown remarkable\nreasoning, coding, and understanding capabilities. However, it remains unclear\nto what extent these capabilities translate into successful design and\nexecution of such complex pipelines. We introduce KRAMABENCH: a benchmark\ncomposed of 104 manually-curated real-world data science pipelines spanning\n1700 data files from 24 data sources in 6 different domains. We show that these\npipelines test the end-to-end capabilities of AI systems on data processing,\nrequiring data discovery, wrangling and cleaning, efficient processing,\nstatistical reasoning, and orchestrating data processing steps given a\nhigh-level task. Our evaluation tests 5 general models and 3 code generation\nmodels using our reference framework, DS-GURU, which instructs the AI model to\ndecompose a question into a sequence of subtasks, reason through each step, and\nsynthesize Python code that implements the proposed design. Our results on\nKRAMABENCH show that, although the models are sufficiently capable of solving\nwell-specified data science code generation tasks, when extensive data\nprocessing and domain knowledge are required to construct real-world data\nscience pipelines, existing out-of-box models fall short. Progress on\nKramaBench represents crucial steps towards developing autonomous data science\nagents for real-world applications. Our code, reference framework, and data are\navailable at https://github.com/mitdbg/KramaBench.", "AI": {"tldr": "KRAMABENCH是一个包含104个真实世界数据科学管道的基准测试，用于评估AI系统在数据处理、发现、清理和编排方面的能力。尽管现有模型在明确任务上表现良好，但在需要广泛数据处理和领域知识的复杂管道设计中仍有不足。", "motivation": "研究AI系统在设计和执行复杂数据科学管道方面的能力，以推动自主数据科学代理的发展。", "method": "使用KRAMABENCH基准测试，评估5个通用模型和3个代码生成模型在数据发现、清理、处理和编排任务中的表现。", "result": "现有模型在明确任务上表现良好，但在复杂管道设计中表现不足，尤其是需要广泛数据处理和领域知识时。", "conclusion": "KRAMABENCH为开发自主数据科学代理提供了关键步骤，但现有模型仍需改进以应对真实世界的复杂需求。"}}
{"id": "2506.07811", "pdf": "https://arxiv.org/pdf/2506.07811", "abs": "https://arxiv.org/abs/2506.07811", "authors": ["Tieyuan Chen", "Huabin Liu", "Yi Wang", "Chaofan Gan", "Mingxi Lyu", "Gui Zou", "Weiyao Lin"], "title": "Looking Beyond Visible Cues: Implicit Video Question Answering via Dual-Clue Reasoning", "categories": ["cs.CV"], "comment": "Preprint", "summary": "Video Question Answering (VideoQA) aims to answer natural language questions\nbased on the given video, with prior work primarily focusing on identifying the\nduration of relevant segments, referred to as explicit visual evidence.\nHowever, explicit visual evidence is not always directly available,\nparticularly when questions target symbolic meanings or deeper intentions,\nleading to significant performance degradation. To fill this gap, we introduce\na novel task and dataset, $\\textbf{I}$mplicit $\\textbf{V}$ideo\n$\\textbf{Q}$uestion $\\textbf{A}$nswering (I-VQA), which focuses on answering\nquestions in scenarios where explicit visual evidence is inaccessible. Given an\nimplicit question and its corresponding video, I-VQA requires answering based\non the contextual visual cues present within the video. To tackle I-VQA, we\npropose a novel reasoning framework, IRM (Implicit Reasoning Model),\nincorporating dual-stream modeling of contextual actions and intent clues as\nimplicit reasoning chains. IRM comprises the Action-Intent Module (AIM) and the\nVisual Enhancement Module (VEM). AIM deduces and preserves question-related\ndual clues by generating clue candidates and performing relation deduction. VEM\nenhances contextual visual representation by leveraging key contextual clues.\nExtensive experiments validate the effectiveness of our IRM in I-VQA tasks,\noutperforming GPT-4o, OpenAI-o3, and fine-tuned VideoChat2 by $0.76\\%$,\n$1.37\\%$, and $4.87\\%$, respectively. Additionally, IRM performs SOTA on\nsimilar implicit advertisement understanding and future prediction in\ntraffic-VQA. Datasets and codes are available for double-blind review in\nanonymous repo: https://github.com/tychen-SJTU/Implicit-VideoQA.", "AI": {"tldr": "论文提出了一个名为I-VQA的新任务和数据集，专注于在显式视觉证据不可用的情况下回答问题，并提出了IRM推理框架，通过双流建模实现性能提升。", "motivation": "现有VideoQA方法依赖显式视觉证据，但在涉及符号意义或深层意图的问题中表现不佳，因此需要解决隐式视觉证据的问题。", "method": "提出了IRM框架，包含Action-Intent Module（AIM）和Visual Enhancement Module（VEM），分别用于生成线索候选和增强视觉表示。", "result": "IRM在I-VQA任务中表现优异，分别超过GPT-4o、OpenAI-o3和VideoChat2 0.76%、1.37%和4.87%。", "conclusion": "IRM在隐式视觉推理任务中表现出色，并在广告理解和交通预测等类似任务中达到SOTA水平。"}}
{"id": "2506.06576", "pdf": "https://arxiv.org/pdf/2506.06576", "abs": "https://arxiv.org/abs/2506.06576", "authors": ["Yijia Shao", "Humishka Zope", "Yucheng Jiang", "Jiaxin Pei", "David Nguyen", "Erik Brynjolfsson", "Diyi Yang"], "title": "Future of Work with AI Agents: Auditing Automation and Augmentation Potential across the U.S. Workforce", "categories": ["cs.CY", "cs.AI", "cs.CL", "cs.HC", "cs.LG"], "comment": "Preprint", "summary": "The rapid rise of compound AI systems (a.k.a., AI agents) is reshaping the\nlabor market, raising concerns about job displacement, diminished human agency,\nand overreliance on automation. Yet, we lack a systematic understanding of the\nevolving landscape. In this paper, we address this gap by introducing a novel\nauditing framework to assess which occupational tasks workers want AI agents to\nautomate or augment, and how those desires align with the current technological\ncapabilities. Our framework features an audio-enhanced mini-interview to\ncapture nuanced worker desires and introduces the Human Agency Scale (HAS) as a\nshared language to quantify the preferred level of human involvement. Using\nthis framework, we construct the WORKBank database, building on the U.S.\nDepartment of Labor's O*NET database, to capture preferences from 1,500 domain\nworkers and capability assessments from AI experts across over 844 tasks\nspanning 104 occupations. Jointly considering the desire and technological\ncapability divides tasks in WORKBank into four zones: Automation \"Green Light\"\nZone, Automation \"Red Light\" Zone, R&D Opportunity Zone, Low Priority Zone.\nThis highlights critical mismatches and opportunities for AI agent development.\nMoving beyond a simple automate-or-not dichotomy, our results reveal diverse\nHAS profiles across occupations, reflecting heterogeneous expectations for\nhuman involvement. Moreover, our study offers early signals of how AI agent\nintegration may reshape the core human competencies, shifting from\ninformation-focused skills to interpersonal ones. These findings underscore the\nimportance of aligning AI agent development with human desires and preparing\nworkers for evolving workplace dynamics.", "AI": {"tldr": "论文提出了一种新的审计框架，用于评估工人希望AI代理自动化或增强哪些职业任务，并分析这些愿望与当前技术能力的匹配情况。通过构建WORKBank数据库，研究揭示了任务分类和人类代理需求的多样性。", "motivation": "随着复合AI系统（即AI代理）的快速发展，劳动力市场面临工作替代、人类代理权削弱和过度依赖自动化等问题。目前缺乏对这一演变景观的系统性理解。", "method": "研究引入了一个音频增强的小型访谈框架，量化人类代理需求（HAS），并基于O*NET数据库构建了WORKBank数据库，收集了1,500名工人和AI专家的数据。", "result": "任务被分为四个区域：自动化“绿灯区”、“红灯区”、研发机会区和低优先级区，揭示了AI代理开发中的关键不匹配和机会。", "conclusion": "研究强调了将AI代理开发与人类需求对齐的重要性，并指出未来工作核心能力可能从信息技能转向人际技能。"}}
{"id": "2506.07813", "pdf": "https://arxiv.org/pdf/2506.07813", "abs": "https://arxiv.org/abs/2506.07813", "authors": ["Junseo Bang", "Joonhee Lee", "Kyeonghyun Lee", "Haechang Lee", "Dong Un Kang", "Se Young Chun"], "title": "Self-Cascaded Diffusion Models for Arbitrary-Scale Image Super-Resolution", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Arbitrary-scale image super-resolution aims to upsample images to any desired\nresolution, offering greater flexibility than traditional fixed-scale\nsuper-resolution. Recent approaches in this domain utilize regression-based or\ngenerative models, but many of them are a single-stage upsampling process,\nwhich may be challenging to learn across a wide, continuous distribution of\nscaling factors. Progressive upsampling strategies have shown promise in\nmitigating this issue, yet their integration with diffusion models for flexible\nupscaling remains underexplored. Here, we present CasArbi, a novel\nself-cascaded diffusion framework for arbitrary-scale image super-resolution.\nCasArbi meets the varying scaling demands by breaking them down into smaller\nsequential factors and progressively enhancing the image resolution at each\nstep with seamless transitions for arbitrary scales. Our novel\ncoordinate-guided residual diffusion model allows for the learning of\ncontinuous image representations while enabling efficient diffusion sampling.\nExtensive experiments demonstrate that our CasArbi outperforms prior arts in\nboth perceptual and distortion performance metrics across diverse\narbitrary-scale super-resolution benchmarks.", "AI": {"tldr": "CasArbi是一种新型的自级联扩散框架，用于任意尺度图像超分辨率，通过逐步增强分辨率实现灵活上采样。", "motivation": "传统固定尺度超分辨率方法灵活性不足，而现有任意尺度方法多为单阶段上采样，难以适应广泛连续的尺度分布。", "method": "提出CasArbi框架，采用自级联扩散模型，将大尺度分解为小尺度序列，逐步提升分辨率，并结合坐标引导残差扩散模型学习连续图像表示。", "result": "实验表明，CasArbi在多种任意尺度超分辨率基准测试中，在感知和失真性能指标上均优于现有方法。", "conclusion": "CasArbi通过逐步扩散和连续表示学习，有效解决了任意尺度图像超分辨率的挑战。"}}
{"id": "2506.06566", "pdf": "https://arxiv.org/pdf/2506.06566", "abs": "https://arxiv.org/abs/2506.06566", "authors": ["Chen Bao", "Chuanbing Huo", "Qinyu Chen", "Chang Gao"], "title": "AS-ASR: A Lightweight Framework for Aphasia-Specific Automatic Speech Recognition", "categories": ["eess.AS", "cs.AI"], "comment": "Under review", "summary": "This paper proposes AS-ASR, a lightweight aphasia-specific speech recognition\nframework based on Whisper-tiny, tailored for low-resource deployment on edge\ndevices. Our approach introduces a hybrid training strategy that systematically\ncombines standard and aphasic speech at varying ratios, enabling robust\ngeneralization, and a GPT-4-based reference enhancement method that refines\nnoisy aphasic transcripts, improving supervision quality. We conduct extensive\nexperiments across multiple data mixing configurations and evaluation settings.\nResults show that our fine-tuned model significantly outperforms the zero-shot\nbaseline, reducing WER on aphasic speech by over 30% while preserving\nperformance on standard speech. The proposed framework offers a scalable,\nefficient solution for real-world disordered speech recognition.", "AI": {"tldr": "AS-ASR是一个基于Whisper-tiny的轻量级失语症专用语音识别框架，适用于边缘设备低资源部署。通过混合训练策略和GPT-4增强方法，显著降低了失语症语音的识别错误率。", "motivation": "解决失语症语音识别在低资源边缘设备上的挑战，提供高效且可扩展的解决方案。", "method": "采用混合训练策略结合标准与失语症语音，并使用GPT-4增强参考文本以提升监督质量。", "result": "模型在失语症语音上的词错误率（WER）降低了30%以上，同时保持标准语音的性能。", "conclusion": "AS-ASR为现实世界中的障碍语音识别提供了高效且可扩展的解决方案。"}}
{"id": "2506.06579", "pdf": "https://arxiv.org/pdf/2506.06579", "abs": "https://arxiv.org/abs/2506.06579", "authors": ["Adarsh Prasad Behera", "Jaya Prakash Champati", "Roberto Morabito", "Sasu Tarkoma", "James Gross"], "title": "Towards Efficient Multi-LLM Inference: Characterization and Analysis of LLM Routing and Hierarchical Techniques", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DC"], "comment": null, "summary": "Recent progress in Language Models (LMs) has dramatically advanced the field\nof natural language processing (NLP), excelling at tasks like text generation,\nsummarization, and question answering. However, their inference remains\ncomputationally expensive and energy intensive, especially in settings with\nlimited hardware, power, or bandwidth. This makes it difficult to deploy LMs in\nmobile, edge, or cost sensitive environments. To address these challenges,\nrecent approaches have introduced multi LLM intelligent model selection\nstrategies that dynamically allocate computational resources based on query\ncomplexity -- using lightweight models for simpler queries and escalating to\nlarger models only when necessary. This survey explores two complementary\nstrategies for efficient LLM inference: (i) routing, which selects the most\nsuitable model based on the query, and (ii) cascading or hierarchical inference\n(HI), which escalates queries through a sequence of models until a confident\nresponse is found. Both approaches aim to reduce computation by using\nlightweight models for simpler tasks while offloading only when needed. We\nprovide a comparative analysis of these techniques across key performance\nmetrics, discuss benchmarking efforts, and outline open challenges. Finally, we\noutline future research directions to enable faster response times, adaptive\nmodel selection based on task complexity, and scalable deployment across\nheterogeneous environments, making LLM based systems more efficient and\naccessible for real world applications.", "AI": {"tldr": "该论文探讨了通过动态模型选择和分层推理策略来优化语言模型的计算效率，以解决其在资源受限环境中的部署问题。", "motivation": "语言模型的计算和能源消耗高，限制了其在移动、边缘或成本敏感环境中的部署。", "method": "提出了两种策略：(i) 路由选择最适合查询的模型，(ii) 分层推理逐步升级模型直至获得可靠响应。", "result": "这些策略通过轻量级模型处理简单任务，仅在必要时使用大型模型，显著减少了计算开销。", "conclusion": "未来研究方向包括更快响应时间、基于任务复杂度的自适应模型选择以及异构环境中的可扩展部署。"}}
{"id": "2506.07814", "pdf": "https://arxiv.org/pdf/2506.07814", "abs": "https://arxiv.org/abs/2506.07814", "authors": ["Yongzhen Wang", "Yongjun Li", "Zhuoran Zheng", "Xiao-Ping Zhang", "Mingqiang Wei"], "title": "M2Restore: Mixture-of-Experts-based Mamba-CNN Fusion Framework for All-in-One Image Restoration", "categories": ["cs.CV"], "comment": "13 pages, 8 figures, 3 tables", "summary": "Natural images are often degraded by complex, composite degradations such as\nrain, snow, and haze, which adversely impact downstream vision applications.\nWhile existing image restoration efforts have achieved notable success, they\nare still hindered by two critical challenges: limited generalization across\ndynamically varying degradation scenarios and a suboptimal balance between\npreserving local details and modeling global dependencies. To overcome these\nchallenges, we propose M2Restore, a novel Mixture-of-Experts (MoE)-based\nMamba-CNN fusion framework for efficient and robust all-in-one image\nrestoration. M2Restore introduces three key contributions: First, to boost the\nmodel's generalization across diverse degradation conditions, we exploit a\nCLIP-guided MoE gating mechanism that fuses task-conditioned prompts with\nCLIP-derived semantic priors. This mechanism is further refined via cross-modal\nfeature calibration, which enables precise expert selection for various\ndegradation types. Second, to jointly capture global contextual dependencies\nand fine-grained local details, we design a dual-stream architecture that\nintegrates the localized representational strength of CNNs with the long-range\nmodeling efficiency of Mamba. This integration enables collaborative\noptimization of global semantic relationships and local structural fidelity,\npreserving global coherence while enhancing detail restoration. Third, we\nintroduce an edge-aware dynamic gating mechanism that adaptively balances\nglobal modeling and local enhancement by reallocating computational attention\nto degradation-sensitive regions. This targeted focus leads to more efficient\nand precise restoration. Extensive experiments across multiple image\nrestoration benchmarks validate the superiority of M2Restore in both visual\nquality and quantitative performance.", "AI": {"tldr": "M2Restore提出了一种基于MoE和Mamba-CNN融合的新型图像修复框架，通过CLIP引导的MoE门控机制、双流架构和边缘感知动态门控机制，解决了现有方法在泛化性和局部-全局平衡上的挑战。", "motivation": "自然图像常受复合退化（如雨、雪、雾）影响，现有方法在动态退化场景下的泛化性和局部-全局平衡上表现不足。", "method": "1. CLIP引导的MoE门控机制；2. 双流架构结合CNN和Mamba；3. 边缘感知动态门控机制。", "result": "在多个图像修复基准测试中，M2Restore在视觉质量和定量性能上均表现出色。", "conclusion": "M2Restore通过创新设计显著提升了图像修复的泛化性和细节恢复能力。"}}
{"id": "2506.06632", "pdf": "https://arxiv.org/pdf/2506.06632", "abs": "https://arxiv.org/abs/2506.06632", "authors": ["Shubham Parashar", "Shurui Gui", "Xiner Li", "Hongyi Ling", "Sushil Vemuri", "Blake Olson", "Eric Li", "Yu Zhang", "James Caverlee", "Dileep Kalathil", "Shuiwang Ji"], "title": "Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "We aim to improve the reasoning capabilities of language models via\nreinforcement learning (RL). Recent RL post-trained models like DeepSeek-R1\nhave demonstrated reasoning abilities on mathematical and coding tasks.\nHowever, prior studies suggest that using RL alone to improve reasoning on\ninherently difficult tasks is less effective. Here, we draw inspiration from\ncurriculum learning and propose to schedule tasks from easy to hard (E2H),\nallowing LLMs to build reasoning skills gradually. Our method is termed E2H\nReasoner. Empirically, we observe that, although easy tasks are important\ninitially, fading them out through appropriate scheduling is essential in\npreventing overfitting. Theoretically, we establish convergence guarantees for\nE2H Reasoner within an approximate policy iteration framework. We derive\nfinite-sample complexity bounds and show that when tasks are appropriately\ndecomposed and conditioned, learning through curriculum stages requires fewer\ntotal samples than direct learning. Experiments across multiple domains show\nthat E2H Reasoner significantly improves the reasoning ability of small LLMs\n(1.5B to 3B), which otherwise struggle when trained with vanilla RL alone,\nhighlighting the effectiveness of our method.", "AI": {"tldr": "论文提出了一种名为E2H Reasoner的方法，通过从易到难的任务调度（E2H）提升语言模型的推理能力，避免了单纯使用强化学习的局限性。", "motivation": "现有研究表明，仅用强化学习（RL）提升语言模型在复杂任务上的推理能力效果有限，因此需要一种更有效的方法。", "method": "采用课程学习策略，从易到难（E2H）逐步调度任务，并结合适当的任务调度防止过拟合。", "result": "实验证明，E2H Reasoner显著提升了小型语言模型（1.5B至3B）的推理能力，且样本效率更高。", "conclusion": "E2H Reasoner是一种有效的方法，通过任务分解和条件化，提升了语言模型的推理能力。"}}
{"id": "2506.07826", "pdf": "https://arxiv.org/pdf/2506.07826", "abs": "https://arxiv.org/abs/2506.07826", "authors": ["William Ljungbergh", "Bernardo Taveira", "Wenzhao Zheng", "Adam Tonderski", "Chensheng Peng", "Fredrik Kahl", "Christoffer Petersson", "Michael Felsberg", "Kurt Keutzer", "Masayoshi Tomizuka", "Wei Zhan"], "title": "R3D2: Realistic 3D Asset Insertion via Diffusion for Autonomous Driving Simulation", "categories": ["cs.CV", "cs.LG", "cs.RO"], "comment": null, "summary": "Validating autonomous driving (AD) systems requires diverse and\nsafety-critical testing, making photorealistic virtual environments essential.\nTraditional simulation platforms, while controllable, are resource-intensive to\nscale and often suffer from a domain gap with real-world data. In contrast,\nneural reconstruction methods like 3D Gaussian Splatting (3DGS) offer a\nscalable solution for creating photorealistic digital twins of real-world\ndriving scenes. However, they struggle with dynamic object manipulation and\nreusability as their per-scene optimization-based methodology tends to result\nin incomplete object models with integrated illumination effects. This paper\nintroduces R3D2, a lightweight, one-step diffusion model designed to overcome\nthese limitations and enable realistic insertion of complete 3D assets into\nexisting scenes by generating plausible rendering effects-such as shadows and\nconsistent lighting-in real time. This is achieved by training R3D2 on a novel\ndataset: 3DGS object assets are generated from in-the-wild AD data using an\nimage-conditioned 3D generative model, and then synthetically placed into\nneural rendering-based virtual environments, allowing R3D2 to learn realistic\nintegration. Quantitative and qualitative evaluations demonstrate that R3D2\nsignificantly enhances the realism of inserted assets, enabling use-cases like\ntext-to-3D asset insertion and cross-scene/dataset object transfer, allowing\nfor true scalability in AD validation. To promote further research in scalable\nand realistic AD simulation, we will release our dataset and code, see\nhttps://research.zenseact.com/publications/R3D2/.", "AI": {"tldr": "R3D2是一种轻量级扩散模型，用于在自动驾驶验证中实现真实感3D资产插入，解决了传统神经重建方法的动态对象操作和可重用性问题。", "motivation": "自动驾驶系统验证需要多样化和安全关键的测试，传统仿真平台资源密集且存在与现实数据的领域差距，而现有神经重建方法在动态对象操作和可重用性上表现不佳。", "method": "R3D2通过训练一个一步扩散模型，利用新型数据集（由3D高斯泼溅生成的3D资产）学习真实感渲染效果（如阴影和一致光照），实现实时插入完整3D资产。", "result": "定量和定性评估表明，R3D2显著提升了插入资产的真实感，支持文本到3D资产插入和跨场景/数据集对象转移，实现了自动驾驶验证的真正可扩展性。", "conclusion": "R3D2为自动驾驶验证提供了可扩展且真实的仿真解决方案，未来将公开数据集和代码以促进研究。"}}
{"id": "2506.06571", "pdf": "https://arxiv.org/pdf/2506.06571", "abs": "https://arxiv.org/abs/2506.06571", "authors": ["Mattie Ji", "Amauri H. Souza", "Vikas Garg"], "title": "Graph Persistence goes Spectral", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "24 pages, 4 figures, 6 tables", "summary": "Including intricate topological information (e.g., cycles) provably enhances\nthe expressivity of message-passing graph neural networks (GNNs) beyond the\nWeisfeiler-Leman (WL) hierarchy. Consequently, Persistent Homology (PH) methods\nare increasingly employed for graph representation learning. In this context,\nrecent works have proposed decorating classical PH diagrams with vertex and\nedge features for improved expressivity. However, due to their dependence on\nfeatures, these methods still fail to capture basic graph structural\ninformation. In this paper, we propose SpectRe -- a new topological descriptor\nfor graphs that integrates spectral information into PH diagrams. Notably,\nSpectRe is strictly more expressive than existing descriptors on graphs. We\nalso introduce notions of global and local stability to analyze existing\ndescriptors and establish that SpectRe is locally stable. Finally, experiments\non synthetic and real-world datasets demonstrate the effectiveness of SpectRe\nand its potential to enhance the capabilities of graph models in relevant\nlearning tasks.", "AI": {"tldr": "论文提出了一种新的拓扑描述符SpectRe，将谱信息融入持久同调图，提升了图神经网络的表达能力，并验证了其局部稳定性。", "motivation": "现有方法依赖特征而无法捕捉基本图结构信息，需要更强大的拓扑描述符。", "method": "提出SpectRe，将谱信息融入持久同调图，并分析其全局和局部稳定性。", "result": "SpectRe比现有描述符更具表达力，实验验证了其在合成和真实数据集上的有效性。", "conclusion": "SpectRe显著提升了图模型的表达能力，具有实际应用潜力。"}}
{"id": "2506.07841", "pdf": "https://arxiv.org/pdf/2506.07841", "abs": "https://arxiv.org/abs/2506.07841", "authors": ["Elizabeth Pavlova", "Xue-Xin Wei"], "title": "Diffusion models under low-noise regime", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent work on diffusion models proposed that they operate in two regimes:\nmemorization, in which models reproduce their training data, and\ngeneralization, in which they generate novel samples. While this has been\ntested in high-noise settings, the behavior of diffusion models as effective\ndenoisers when the corruption level is small remains unclear. To address this\ngap, we systematically investigated the behavior of diffusion models under\nlow-noise diffusion dynamics, with implications for model robustness and\ninterpretability. Using (i) CelebA subsets of varying sample sizes and (ii)\nanalytic Gaussian mixture benchmarks, we reveal that models trained on disjoint\ndata diverge near the data manifold even when their high-noise outputs\nconverge. We quantify how training set size, data geometry, and model objective\nchoice shape denoising trajectories and affect score accuracy, providing\ninsights into how these models actually learn representations of data\ndistributions. This work starts to address gaps in our understanding of\ngenerative model reliability in practical applications where small\nperturbations are common.", "AI": {"tldr": "扩散模型在低噪声条件下的行为研究，揭示了训练数据规模、数据几何和模型目标对去噪轨迹的影响。", "motivation": "理解扩散模型在小扰动（低噪声）条件下的行为，填补其在实用场景中可靠性的认知空白。", "method": "使用CelebA子集和高斯混合基准，系统研究低噪声扩散动态下的模型行为。", "result": "模型在数据流形附近的行为会因训练数据不同而分化，即使在高噪声输出时收敛。", "conclusion": "研究为扩散模型在实际应用中的可靠性和学习机制提供了新见解。"}}
{"id": "2506.06699", "pdf": "https://arxiv.org/pdf/2506.06699", "abs": "https://arxiv.org/abs/2506.06699", "authors": ["Rajeev Bhatt Ambati", "James Lester", "Shashank Srivastava", "Snigdha Chaturvedi"], "title": "MarginSel : Max-Margin Demonstration Selection for LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) excel at few-shot learning via in-context\nlearning (ICL). However, the effectiveness of ICL is often sensitive to the\nselection and ordering of demonstration examples. To address this, we present\nMarginSel: Max-Margin Demonstration Selection for LLMs, a two-step method that\nselects hard demonstration examples for the ICL prompt, adapting to each test\ninstance. Our approach achieves 2-7% absolute improvement in F1-score across\nclassification tasks, compared to a random selection of examples. We also\nprovide theoretical insights and empirical evidence showing that MarginSel\ninduces max-margin behavior in LLMs by effectively increasing the margin for\nhard examples, analogous to support vectors, thereby shifting the decision\nboundary in a beneficial direction.", "AI": {"tldr": "MarginSel是一种通过选择困难示例来优化LLMs上下文学习性能的方法，显著提升了分类任务的F1分数。", "motivation": "解决上下文学习（ICL）中示例选择和排序对性能敏感的问题。", "method": "提出MarginSel，一种两步法，为每个测试实例选择困难示例。", "result": "在分类任务中F1分数绝对提升2-7%。", "conclusion": "MarginSel通过增加困难示例的边距，优化了LLMs的决策边界。"}}
{"id": "2506.07847", "pdf": "https://arxiv.org/pdf/2506.07847", "abs": "https://arxiv.org/abs/2506.07847", "authors": ["Hengzhi Chen", "Liqian Feng", "Wenhua Wu", "Xiaogang Zhu", "Shawn Leo", "Kun Hu"], "title": "F2Net: A Frequency-Fused Network for Ultra-High Resolution Remote Sensing Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Semantic segmentation of ultra-high-resolution (UHR) remote sensing imagery\nis critical for applications like environmental monitoring and urban planning\nbut faces computational and optimization challenges. Conventional methods\neither lose fine details through downsampling or fragment global context via\npatch processing. While multi-branch networks address this trade-off, they\nsuffer from computational inefficiency and conflicting gradient dynamics during\ntraining. We propose F2Net, a frequency-aware framework that decomposes UHR\nimages into high- and low-frequency components for specialized processing. The\nhigh-frequency branch preserves full-resolution structural details, while the\nlow-frequency branch processes downsampled inputs through dual sub-branches\ncapturing short- and long-range dependencies. A Hybrid-Frequency Fusion module\nintegrates these observations, guided by two novel objectives: Cross-Frequency\nAlignment Loss ensures semantic consistency between frequency components, and\nCross-Frequency Balance Loss regulates gradient magnitudes across branches to\nstabilize training. Evaluated on DeepGlobe and Inria Aerial benchmarks, F2Net\nachieves state-of-the-art performance with mIoU of 80.22 and 83.39,\nrespectively. Our code will be publicly available.", "AI": {"tldr": "F2Net是一个频率感知框架，通过分解超高清遥感图像为高频和低频成分进行专门处理，解决了传统方法在细节丢失和全局上下文碎片化之间的权衡问题。", "motivation": "超高清遥感图像的语义分割在环境监测和城市规划中至关重要，但传统方法存在计算和优化挑战，如细节丢失或全局上下文碎片化。", "method": "F2Net将图像分解为高频和低频成分，高频分支保留全分辨率细节，低频分支通过双子分支捕获短程和长程依赖关系，并通过混合频率融合模块整合结果。", "result": "在DeepGlobe和Inria Aerial基准测试中，F2Net分别达到80.22和83.39的mIoU，表现最优。", "conclusion": "F2Net通过频率分解和融合模块，结合新颖的损失函数，实现了高效且稳定的超高清遥感图像语义分割。"}}
{"id": "2506.07848", "pdf": "https://arxiv.org/pdf/2506.07848", "abs": "https://arxiv.org/abs/2506.07848", "authors": ["Teng Hu", "Zhentao Yu", "Zhengguang Zhou", "Jiangning Zhang", "Yuan Zhou", "Qinglin Lu", "Ran Yi"], "title": "PolyVivid: Vivid Multi-Subject Video Generation with Cross-Modal Interaction and Enhancement", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Despite recent advances in video generation, existing models still lack\nfine-grained controllability, especially for multi-subject customization with\nconsistent identity and interaction. In this paper, we propose PolyVivid, a\nmulti-subject video customization framework that enables flexible and\nidentity-consistent generation. To establish accurate correspondences between\nsubject images and textual entities, we design a VLLM-based text-image fusion\nmodule that embeds visual identities into the textual space for precise\ngrounding. To further enhance identity preservation and subject interaction, we\npropose a 3D-RoPE-based enhancement module that enables structured\nbidirectional fusion between text and image embeddings. Moreover, we develop an\nattention-inherited identity injection module to effectively inject fused\nidentity features into the video generation process, mitigating identity drift.\nFinally, we construct an MLLM-based data pipeline that combines MLLM-based\ngrounding, segmentation, and a clique-based subject consolidation strategy to\nproduce high-quality multi-subject data, effectively enhancing subject\ndistinction and reducing ambiguity in downstream video generation. Extensive\nexperiments demonstrate that PolyVivid achieves superior performance in\nidentity fidelity, video realism, and subject alignment, outperforming existing\nopen-source and commercial baselines.", "AI": {"tldr": "PolyVivid是一个多主体视频定制框架，通过文本-图像融合模块和3D-RoPE增强模块实现身份一致性和交互控制，优于现有方法。", "motivation": "现有视频生成模型在多主体定制中缺乏细粒度控制和身份一致性。", "method": "设计了VLLM文本-图像融合模块、3D-RoPE增强模块和注意力继承身份注入模块，结合MLLM数据管道提升生成质量。", "result": "实验表明PolyVivid在身份保真度、视频真实性和主体对齐方面表现优异。", "conclusion": "PolyVivid在多主体视频定制中实现了灵活且身份一致的生成，显著优于现有方法。"}}
{"id": "2506.06594", "pdf": "https://arxiv.org/pdf/2506.06594", "abs": "https://arxiv.org/abs/2506.06594", "authors": ["Daniel Leite", "Igor Škrjanc", "Fernando Gomide"], "title": "From Model-Based and Adaptive Control to Evolving Fuzzy Control", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "comment": "4 pages, 2 figures. Fuzz-IEEE 2025 Booklet: 60 Years of Fuzzy Set\n  Theory", "summary": "Evolving fuzzy systems build and adapt fuzzy models - such as predictors and\ncontrollers - by incrementally updating their rule-base structure from data\nstreams. On the occasion of the 60-year anniversary of fuzzy set theory,\ncommemorated during the Fuzz-IEEE 2025 event, this brief paper revisits the\nhistorical development and core contributions of classical fuzzy and adaptive\nmodeling and control frameworks. It then highlights the emergence and\nsignificance of evolving intelligent systems in fuzzy modeling and control,\nemphasizing their advantages in handling nonstationary environments. Key\nchallenges and future directions are discussed, including safety,\ninterpretability, and principled structural evolution.", "AI": {"tldr": "本文回顾了模糊集理论60年来的发展，重点介绍了经典模糊和自适应建模与控制框架的核心贡献，并探讨了演化智能系统在非平稳环境中的优势及其未来挑战。", "motivation": "纪念模糊集理论60周年，总结其历史发展及核心贡献，并探讨演化智能系统在模糊建模与控制中的新兴作用。", "method": "通过回顾经典模糊和自适应建模与控制框架，分析演化模糊系统的规则库结构增量更新方法。", "result": "演化模糊系统在非平稳环境中表现出显著优势，但仍面临安全性、可解释性和结构演化等挑战。", "conclusion": "演化模糊系统为模糊建模与控制提供了新的发展方向，未来需解决安全性、可解释性和结构演化等关键问题。"}}
{"id": "2506.07850", "pdf": "https://arxiv.org/pdf/2506.07850", "abs": "https://arxiv.org/abs/2506.07850", "authors": ["Arash Rocky", "Q. M. Jonathan Wu"], "title": "SAM2Auto: Auto Annotation Using FLASH", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) lag behind Large Language Models due to the\nscarcity of annotated datasets, as creating paired visual-textual annotations\nis labor-intensive and expensive. To address this bottleneck, we introduce\nSAM2Auto, the first fully automated annotation pipeline for video datasets\nrequiring no human intervention or dataset-specific training. Our approach\nconsists of two key components: SMART-OD, a robust object detection system that\ncombines automatic mask generation with open-world object detection\ncapabilities, and FLASH (Frame-Level Annotation and Segmentation Handler), a\nmulti-object real-time video instance segmentation (VIS) that maintains\nconsistent object identification across video frames even with intermittent\ndetection gaps. Unlike existing open-world detection methods that require\nframe-specific hyperparameter tuning and suffer from numerous false positives,\nour system employs statistical approaches to minimize detection errors while\nensuring consistent object tracking throughout entire video sequences.\nExtensive experimental validation demonstrates that SAM2Auto achieves\ncomparable accuracy to manual annotation while dramatically reducing annotation\ntime and eliminating labor costs. The system successfully handles diverse\ndatasets without requiring retraining or extensive parameter adjustments,\nmaking it a practical solution for large-scale dataset creation. Our work\nestablishes a new baseline for automated video annotation and provides a\npathway for accelerating VLM development by addressing the fundamental dataset\nbottleneck that has constrained progress in vision-language understanding.", "AI": {"tldr": "SAM2Auto是一个全自动视频数据集标注系统，无需人工干预或数据集特定训练，显著减少标注时间和成本。", "motivation": "解决视觉语言模型（VLMs）因标注数据集稀缺而发展滞后的问题，传统标注方法耗时且昂贵。", "method": "结合SMART-OD（自动掩码生成与开放世界目标检测）和FLASH（实时视频实例分割），通过统计方法减少错误并保持对象跟踪一致性。", "result": "实验表明，SAM2Auto的标注准确性与人工相当，同时大幅提升效率。", "conclusion": "SAM2Auto为自动化视频标注设定了新基准，有望加速VLM发展。"}}
{"id": "2506.06603", "pdf": "https://arxiv.org/pdf/2506.06603", "abs": "https://arxiv.org/abs/2506.06603", "authors": ["Joseph T Colonel", "Carolyn Hagler", "Guiselle Wismer", "Laura Curtis", "Jacqueline Becker", "Juan Wisnivesky", "Alex Federman", "Gaurav Pandey"], "title": "CAtCh: Cognitive Assessment through Cookie Thief", "categories": ["cs.LG", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Several machine learning algorithms have been developed for the prediction of\nAlzheimer's disease and related dementia (ADRD) from spontaneous speech.\nHowever, none of these algorithms have been translated for the prediction of\nbroader cognitive impairment (CI), which in some cases is a precursor and risk\nfactor of ADRD. In this paper, we evaluated several speech-based open-source\nmethods originally proposed for the prediction of ADRD, as well as methods from\nmultimodal sentiment analysis for the task of predicting CI from patient audio\nrecordings. Results demonstrated that multimodal methods outperformed unimodal\nones for CI prediction, and that acoustics-based approaches performed better\nthan linguistics-based ones. Specifically, interpretable acoustic features\nrelating to affect and prosody were found to significantly outperform\nBERT-based linguistic features and interpretable linguistic features,\nrespectively. All the code developed for this study is available at\nhttps://github.com/JTColonel/catch.", "AI": {"tldr": "本文评估了多种基于语音的开源方法，用于从患者录音中预测认知障碍（CI），发现多模态方法优于单模态方法，声学特征优于语言学特征。", "motivation": "现有机器学习算法主要用于预测阿尔茨海默病及相关痴呆（ADRD），但未扩展到更广泛的认知障碍（CI）预测，而CI可能是ADRD的前兆和风险因素。", "method": "评估了基于语音的ADRD预测方法和多模态情感分析方法，用于CI预测。", "result": "多模态方法优于单模态方法，声学特征（尤其是情感和韵律相关特征）显著优于BERT和可解释语言学特征。", "conclusion": "声学特征在多模态框架下对CI预测更有效，相关代码已开源。"}}
{"id": "2506.07857", "pdf": "https://arxiv.org/pdf/2506.07857", "abs": "https://arxiv.org/abs/2506.07857", "authors": ["Zihui Zhang", "Weisheng Dai", "Hongtao Wen", "Bo Yang"], "title": "LogoSP: Local-global Grouping of Superpoints for Unsupervised Semantic Segmentation of 3D Point Clouds", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "comment": "CVPR 2025. Code and data are available at:\n  https://github.com/vLAR-group/LogoSP", "summary": "We study the problem of unsupervised 3D semantic segmentation on raw point\nclouds without needing human labels in training. Existing methods usually\nformulate this problem into learning per-point local features followed by a\nsimple grouping strategy, lacking the ability to discover additional and\npossibly richer semantic priors beyond local features. In this paper, we\nintroduce LogoSP to learn 3D semantics from both local and global point\nfeatures. The key to our approach is to discover 3D semantic information by\ngrouping superpoints according to their global patterns in the frequency\ndomain, thus generating highly accurate semantic pseudo-labels for training a\nsegmentation network. Extensive experiments on two indoor and an outdoor\ndatasets show that our LogoSP surpasses all existing unsupervised methods by\nlarge margins, achieving the state-of-the-art performance for unsupervised 3D\nsemantic segmentation. Notably, our investigation into the learned global\npatterns reveals that they truly represent meaningful 3D semantics in the\nabsence of human labels during training.", "AI": {"tldr": "LogoSP是一种无监督3D语义分割方法，通过结合局部和全局点特征学习语义信息，利用频域中的全局模式生成伪标签，显著优于现有方法。", "motivation": "解决现有无监督3D语义分割方法仅依赖局部特征而忽略全局语义先验的问题。", "method": "提出LogoSP方法，通过频域中的全局模式对超点进行分组，生成高精度语义伪标签，用于训练分割网络。", "result": "在两个室内和一个室外数据集上，LogoSP大幅超越现有无监督方法，达到最先进性能。", "conclusion": "LogoSP证明了在无人类标注的情况下，全局模式能有效捕捉3D语义信息。"}}
{"id": "2506.07860", "pdf": "https://arxiv.org/pdf/2506.07860", "abs": "https://arxiv.org/abs/2506.07860", "authors": ["Ivan Alberico", "Marco Cannici", "Giovanni Cioffi", "Davide Scaramuzza"], "title": "Egocentric Event-Based Vision for Ping Pong Ball Trajectory Prediction", "categories": ["cs.CV"], "comment": "IEEE/CVF Conference on Computer Vision and Pattern Recognition\n  Workshops (CVPRW), Nashville (TN), USA, 2025; 5th International Workshop on\n  Event-Based Vision", "summary": "In this paper, we present a real-time egocentric trajectory prediction system\nfor table tennis using event cameras. Unlike standard cameras, which suffer\nfrom high latency and motion blur at fast ball speeds, event cameras provide\nhigher temporal resolution, allowing more frequent state updates, greater\nrobustness to outliers, and accurate trajectory predictions using just a short\ntime window after the opponent's impact. We collect a dataset of ping-pong game\nsequences, including 3D ground-truth trajectories of the ball, synchronized\nwith sensor data from the Meta Project Aria glasses and event streams. Our\nsystem leverages foveated vision, using eye-gaze data from the glasses to\nprocess only events in the viewer's fovea. This biologically inspired approach\nimproves ball detection performance and significantly reduces computational\nlatency, as it efficiently allocates resources to the most perceptually\nrelevant regions, achieving a reduction factor of 10.81 on the collected\ntrajectories. Our detection pipeline has a worst-case total latency of 4.5 ms,\nincluding computation and perception - significantly lower than a frame-based\n30 FPS system, which, in the worst case, takes 66 ms solely for perception.\nFinally, we fit a trajectory prediction model to the estimated states of the\nball, enabling 3D trajectory forecasting in the future. To the best of our\nknowledge, this is the first approach to predict table tennis trajectories from\nan egocentric perspective using event cameras.", "AI": {"tldr": "提出了一种基于事件相机的实时乒乓球轨迹预测系统，利用高时间分辨率减少延迟和运动模糊，并通过注视数据优化计算资源分配。", "motivation": "标准相机在高速乒乓球运动中存在高延迟和运动模糊问题，而事件相机能提供更高的时间分辨率，从而更准确地预测轨迹。", "method": "使用事件相机和Meta Project Aria眼镜收集数据，结合注视数据实现注视区域的事件处理，优化计算资源分配。", "result": "系统在最坏情况下的总延迟为4.5毫秒，比基于帧的系统（66毫秒）显著降低，轨迹预测性能提升。", "conclusion": "这是首个基于事件相机和注视数据的乒乓球轨迹预测系统，展示了高效率和低延迟的优势。"}}
{"id": "2506.06975", "pdf": "https://arxiv.org/pdf/2506.06975", "abs": "https://arxiv.org/abs/2506.06975", "authors": ["Xiaoyuan Zhu", "Yaowen Ye", "Tianyi Qiu", "Hanlin Zhu", "Sijun Tan", "Ajraf Mannan", "Jonathan Michala", "Raluca Ada Popa", "Willie Neiswanger"], "title": "Auditing Black-Box LLM APIs with a Rank-Based Uniformity Test", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "As API access becomes a primary interface to large language models (LLMs),\nusers often interact with black-box systems that offer little transparency into\nthe deployed model. To reduce costs or maliciously alter model behaviors, API\nproviders may discreetly serve quantized or fine-tuned variants, which can\ndegrade performance and compromise safety. Detecting such substitutions is\ndifficult, as users lack access to model weights and, in most cases, even\noutput logits. To tackle this problem, we propose a rank-based uniformity test\nthat can verify the behavioral equality of a black-box LLM to a locally\ndeployed authentic model. Our method is accurate, query-efficient, and avoids\ndetectable query patterns, making it robust to adversarial providers that\nreroute or mix responses upon the detection of testing attempts. We evaluate\nthe approach across diverse threat scenarios, including quantization, harmful\nfine-tuning, jailbreak prompts, and full model substitution, showing that it\nconsistently achieves superior statistical power over prior methods under\nconstrained query budgets.", "AI": {"tldr": "提出了一种基于排序的统一性测试方法，用于验证黑盒LLM与本地部署的真实模型的行为一致性，解决了API提供商可能暗中替换模型的问题。", "motivation": "由于API提供商可能为了降低成本或恶意修改模型行为而暗中提供量化或微调变体，导致性能下降和安全性问题，用户缺乏透明度和检测手段。", "method": "采用基于排序的统一性测试方法，无需访问模型权重或输出logits，具有高准确性、查询效率高且不易被检测到。", "result": "在多种威胁场景（如量化、有害微调、越狱提示和完整模型替换）下，该方法在有限查询预算下表现出优于先前方法的统计能力。", "conclusion": "该方法能有效检测黑盒LLM的行为变化，为模型透明度和安全性提供了实用工具。"}}
{"id": "2506.07863", "pdf": "https://arxiv.org/pdf/2506.07863", "abs": "https://arxiv.org/abs/2506.07863", "authors": ["Lev Novitskiy", "Viacheslav Vasilev", "Maria Kovaleva", "Vladimir Arkhipkin", "Denis Dimitrov"], "title": "VIVAT: Virtuous Improving VAE Training through Artifact Mitigation", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Variational Autoencoders (VAEs) remain a cornerstone of generative computer\nvision, yet their training is often plagued by artifacts that degrade\nreconstruction and generation quality. This paper introduces VIVAT, a\nsystematic approach to mitigating common artifacts in KL-VAE training without\nrequiring radical architectural changes. We present a detailed taxonomy of five\nprevalent artifacts - color shift, grid patterns, blur, corner and droplet\nartifacts - and analyze their root causes. Through straightforward\nmodifications, including adjustments to loss weights, padding strategies, and\nthe integration of Spatially Conditional Normalization, we demonstrate\nsignificant improvements in VAE performance. Our method achieves\nstate-of-the-art results in image reconstruction metrics (PSNR and SSIM) across\nmultiple benchmarks and enhances text-to-image generation quality, as evidenced\nby superior CLIP scores. By preserving the simplicity of the KL-VAE framework\nwhile addressing its practical challenges, VIVAT offers actionable insights for\nresearchers and practitioners aiming to optimize VAE training.", "AI": {"tldr": "VIVAT通过简单修改（如调整损失权重、填充策略和引入空间条件归一化）有效减少KL-VAE训练中的常见伪影，提升重建和生成质量。", "motivation": "KL-VAE训练中常见的伪影（如颜色偏移、网格模式等）影响重建和生成质量，需要一种无需大幅架构改动的方法来解决。", "method": "提出VIVAT，通过调整损失权重、优化填充策略和集成空间条件归一化，系统性地减少五种常见伪影。", "result": "在多个基准测试中，VIVAT在图像重建（PSNR和SSIM）和文本到图像生成（CLIP分数）方面达到最先进水平。", "conclusion": "VIVAT为优化VAE训练提供了实用方法，同时保持了KL-VAE框架的简洁性。"}}
{"id": "2506.06622", "pdf": "https://arxiv.org/pdf/2506.06622", "abs": "https://arxiv.org/abs/2506.06622", "authors": ["Yifan Zeng"], "title": "\\textit{QuantMCP}: Grounding Large Language Models in Verifiable Financial Reality", "categories": ["cs.CE", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) hold immense promise for revolutionizing\nfinancial analysis and decision-making, yet their direct application is often\nhampered by issues of data hallucination and lack of access to real-time,\nverifiable financial information. This paper introduces QuantMCP, a novel\nframework designed to rigorously ground LLMs in financial reality. By\nleveraging the Model Context Protocol (MCP) for standardized and secure tool\ninvocation, QuantMCP enables LLMs to accurately interface with a diverse array\nof Python-accessible financial data APIs (e.g., Wind, yfinance). Users can\ninteract via natural language to precisely retrieve up-to-date financial data,\nthereby overcoming LLM's inherent limitations in factual data recall. More\ncritically, once furnished with this verified, structured data, the LLM's\nanalytical capabilities are unlocked, empowering it to perform sophisticated\ndata interpretation, generate insights, and ultimately support more informed\nfinancial decision-making processes. QuantMCP provides a robust, extensible,\nand secure bridge between conversational AI and the complex world of financial\ndata, aiming to enhance both the reliability and the analytical depth of LLM\napplications in finance.", "AI": {"tldr": "QuantMCP框架通过标准化工具调用，将大语言模型（LLMs）与实时金融数据API连接，解决数据幻觉问题，提升金融分析的准确性和深度。", "motivation": "LLMs在金融分析中潜力巨大，但面临数据幻觉和实时数据获取的挑战，QuantMCP旨在解决这些问题。", "method": "利用Model Context Protocol（MCP）标准化工具调用，通过Python接口连接金融数据API（如Wind、yfinance），实现自然语言交互获取实时数据。", "result": "QuantMCP成功将LLMs与金融数据结合，提升了数据准确性和分析能力，支持更复杂的金融决策。", "conclusion": "QuantMCP为LLMs在金融领域的应用提供了可靠、可扩展的解决方案，增强了分析的深度和可靠性。"}}
{"id": "2506.07031", "pdf": "https://arxiv.org/pdf/2506.07031", "abs": "https://arxiv.org/abs/2506.07031", "authors": ["Jingyuan Ma", "Rui Li", "Zheng Li", "Junfeng Liu", "Lei Sha", "Zhifang Sui"], "title": "HauntAttack: When Attack Follows Reasoning as a Shadow", "categories": ["cs.CR", "cs.AI", "cs.CL"], "comment": null, "summary": "Emerging Large Reasoning Models (LRMs) consistently excel in mathematical and\nreasoning tasks, showcasing exceptional capabilities. However, the enhancement\nof reasoning abilities and the exposure of their internal reasoning processes\nintroduce new safety vulnerabilities. One intriguing concern is: when reasoning\nis strongly entangled with harmfulness, what safety-reasoning trade-off do LRMs\nexhibit? To address this issue, we introduce HauntAttack, a novel and\ngeneral-purpose black-box attack framework that systematically embeds harmful\ninstructions into reasoning questions. Specifically, we treat reasoning\nquestions as carriers and substitute one of their original conditions with a\nharmful instruction. This process creates a reasoning pathway in which the\nmodel is guided step by step toward generating unsafe outputs. Based on\nHauntAttack, we conduct comprehensive experiments on multiple LRMs. Our results\nreveal that even the most advanced LRMs exhibit significant safety\nvulnerabilities. Additionally, we perform a detailed analysis of different\nmodels, various types of harmful instructions, and model output patterns,\nproviding valuable insights into the security of LRMs.", "AI": {"tldr": "HauntAttack是一种新型的黑盒攻击框架，通过将有害指令嵌入推理问题中，揭示大型推理模型（LRMs）的安全漏洞。", "motivation": "尽管LRMs在数学和推理任务中表现出色，但其推理能力的增强和内部推理过程的暴露引发了新的安全隐患。", "method": "提出HauntAttack框架，将有害指令替换推理问题中的原始条件，逐步引导模型生成不安全输出。", "result": "实验表明，即使最先进的LRMs也存在显著的安全漏洞。", "conclusion": "研究为LRMs的安全性提供了重要见解，揭示了安全与推理之间的权衡问题。"}}
{"id": "2506.07865", "pdf": "https://arxiv.org/pdf/2506.07865", "abs": "https://arxiv.org/abs/2506.07865", "authors": ["Jinxi Li", "Ziyang Song", "Siyuan Zhou", "Bo Yang"], "title": "FreeGave: 3D Physics Learning from Dynamic Videos by Gaussian Velocity", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.LG", "cs.RO"], "comment": "CVPR 2025. Code and data are available at:\n  https://github.com/vLAR-group/FreeGave", "summary": "In this paper, we aim to model 3D scene geometry, appearance, and the\nunderlying physics purely from multi-view videos. By applying various governing\nPDEs as PINN losses or incorporating physics simulation into neural networks,\nexisting works often fail to learn complex physical motions at boundaries or\nrequire object priors such as masks or types. In this paper, we propose\nFreeGave to learn the physics of complex dynamic 3D scenes without needing any\nobject priors. The key to our approach is to introduce a physics code followed\nby a carefully designed divergence-free module for estimating a per-Gaussian\nvelocity field, without relying on the inefficient PINN losses. Extensive\nexperiments on three public datasets and a newly collected challenging\nreal-world dataset demonstrate the superior performance of our method for\nfuture frame extrapolation and motion segmentation. Most notably, our\ninvestigation into the learned physics codes reveals that they truly learn\nmeaningful 3D physical motion patterns in the absence of any human labels in\ntraining.", "AI": {"tldr": "FreeGave方法通过引入物理编码和发散自由模块，无需对象先验即可学习复杂动态3D场景的物理特性，优于现有方法。", "motivation": "现有方法在处理复杂物理运动或边界时表现不佳，且需要对象先验（如掩码或类型），FreeGave旨在解决这些问题。", "method": "提出FreeGave方法，结合物理编码和发散自由模块，估计每高斯速度场，避免低效的PINN损失。", "result": "在多个数据集上验证了方法的优越性，特别是在未来帧外推和运动分割任务中。", "conclusion": "FreeGave成功学习了无标签的3D物理运动模式，展示了其潜力。"}}
{"id": "2506.06630", "pdf": "https://arxiv.org/pdf/2506.06630", "abs": "https://arxiv.org/abs/2506.06630", "authors": ["Heeju Ko", "Sungjune Kim", "Gyeongrok Oh", "Jeongyoon Yoon", "Honglak Lee", "Sujin Jang", "Seungryong Kim", "Sangpil Kim"], "title": "Active Test-time Vision-Language Navigation", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Vision-Language Navigation (VLN) policies trained on offline datasets often\nexhibit degraded task performance when deployed in unfamiliar navigation\nenvironments at test time, where agents are typically evaluated without access\nto external interaction or feedback. Entropy minimization has emerged as a\npractical solution for reducing prediction uncertainty at test time; however,\nit can suffer from accumulated errors, as agents may become overconfident in\nincorrect actions without sufficient contextual grounding. To tackle these\nchallenges, we introduce ATENA (Active TEst-time Navigation Agent), a test-time\nactive learning framework that enables a practical human-robot interaction via\nepisodic feedback on uncertain navigation outcomes. In particular, ATENA learns\nto increase certainty in successful episodes and decrease it in failed ones,\nimproving uncertainty calibration. Here, we propose mixture entropy\noptimization, where entropy is obtained from a combination of the action and\npseudo-expert distributions-a hypothetical action distribution assuming the\nagent's selected action to be optimal-controlling both prediction confidence\nand action preference. In addition, we propose a self-active learning strategy\nthat enables an agent to evaluate its navigation outcomes based on confident\npredictions. As a result, the agent stays actively engaged throughout all\niterations, leading to well-grounded and adaptive decision-making. Extensive\nevaluations on challenging VLN benchmarks-REVERIE, R2R, and R2R-CE-demonstrate\nthat ATENA successfully overcomes distributional shifts at test time,\noutperforming the compared baseline methods across various settings.", "AI": {"tldr": "ATENA是一种测试时主动学习框架，通过人类-机器人交互减少视觉语言导航中的不确定性，提升任务表现。", "motivation": "离线训练的视觉语言导航策略在陌生环境中表现不佳，熵最小化方法可能因错误累积导致过度自信。", "method": "提出混合熵优化，结合动作和伪专家分布，以及自主动学习策略，评估导航结果。", "result": "在多个VLN基准测试中，ATENA成功克服分布偏移，优于基线方法。", "conclusion": "ATENA通过主动学习和不确定性校准，提升了导航策略的适应性和表现。"}}
{"id": "2506.07878", "pdf": "https://arxiv.org/pdf/2506.07878", "abs": "https://arxiv.org/abs/2506.07878", "authors": ["Muhammad Ahmed Humais", "Xiaoqian Huang", "Hussain Sajwani", "Sajid Javed", "Yahya Zweiri"], "title": "Spatio-Temporal State Space Model For Efficient Event-Based Optical Flow", "categories": ["cs.CV"], "comment": null, "summary": "Event cameras unlock new frontiers that were previously unthinkable with\nstandard frame-based cameras. One notable example is low-latency motion\nestimation (optical flow), which is critical for many real-time applications.\nIn such applications, the computational efficiency of algorithms is paramount.\nAlthough recent deep learning paradigms such as CNN, RNN, or ViT have shown\nremarkable performance, they often lack the desired computational efficiency.\nConversely, asynchronous event-based methods including SNNs and GNNs are\ncomputationally efficient; however, these approaches fail to capture sufficient\nspatio-temporal information, a powerful feature required to achieve better\nperformance for optical flow estimation. In this work, we introduce\nSpatio-Temporal State Space Model (STSSM) module along with a novel network\narchitecture to develop an extremely efficient solution with competitive\nperformance. Our STSSM module leverages state-space models to effectively\ncapture spatio-temporal correlations in event data, offering higher performance\nwith lower complexity compared to ViT, CNN-based architectures in similar\nsettings. Our model achieves 4.5x faster inference and 8x lower computations\ncompared to TMA and 2x lower computations compared to EV-FlowNet with\ncompetitive performance on the DSEC benchmark. Our code will be available at\nhttps://github.com/AhmedHumais/E-STMFlow", "AI": {"tldr": "提出了一种基于时空状态空间模型（STSSM）的高效事件相机光流估计方法，显著降低了计算复杂度并提升了性能。", "motivation": "事件相机在低延迟运动估计（光流）方面具有潜力，但现有深度学习方法（如CNN、RNN、ViT）计算效率不足，而异步事件方法（如SNN、GNN）又无法充分捕捉时空信息。", "method": "引入STSSM模块和新型网络架构，利用状态空间模型有效捕捉事件数据的时空相关性。", "result": "在DSEC基准测试中，模型推理速度比TMA快4.5倍，计算量比TMA低8倍，比EV-FlowNet低2倍，性能具有竞争力。", "conclusion": "STSSM模块提供了一种高效且高性能的光流估计解决方案，适用于实时应用。"}}
{"id": "2506.07885", "pdf": "https://arxiv.org/pdf/2506.07885", "abs": "https://arxiv.org/abs/2506.07885", "authors": ["Zubin Bhuyan", "Yuanchang Xie", "AngkeaReach Rith", "Xintong Yan", "Nasko Apostolov", "Jimi Oke", "Chengbo Ai"], "title": "CrosswalkNet: An Optimized Deep Learning Framework for Pedestrian Crosswalk Detection in Aerial Images with High-Performance Computing", "categories": ["cs.CV"], "comment": null, "summary": "With the increasing availability of aerial and satellite imagery, deep\nlearning presents significant potential for transportation asset management,\nsafety analysis, and urban planning. This study introduces CrosswalkNet, a\nrobust and efficient deep learning framework designed to detect various types\nof pedestrian crosswalks from 15-cm resolution aerial images. CrosswalkNet\nincorporates a novel detection approach that improves upon traditional object\ndetection strategies by utilizing oriented bounding boxes (OBB), enhancing\ndetection precision by accurately capturing crosswalks regardless of their\norientation. Several optimization techniques, including Convolutional Block\nAttention, a dual-branch Spatial Pyramid Pooling-Fast module, and cosine\nannealing, are implemented to maximize performance and efficiency. A\ncomprehensive dataset comprising over 23,000 annotated crosswalk instances is\nutilized to train and validate the proposed framework. The best-performing\nmodel achieves an impressive precision of 96.5% and a recall of 93.3% on aerial\nimagery from Massachusetts, demonstrating its accuracy and effectiveness.\nCrosswalkNet has also been successfully applied to datasets from New Hampshire,\nVirginia, and Maine without transfer learning or fine-tuning, showcasing its\nrobustness and strong generalization capability. Additionally, the crosswalk\ndetection results, processed using High-Performance Computing (HPC) platforms\nand provided in polygon shapefile format, have been shown to accelerate data\nprocessing and detection, supporting real-time analysis for safety and mobility\napplications. This integration offers policymakers, transportation engineers,\nand urban planners an effective instrument to enhance pedestrian safety and\nimprove urban mobility.", "AI": {"tldr": "CrosswalkNet是一种高效的深度学习框架，用于从高分辨率航拍图像中检测行人横道，采用定向边界框（OBB）提升检测精度，并在多个地区数据集上表现出色。", "motivation": "随着航拍和卫星图像的普及，深度学习在交通资产管理、安全分析和城市规划中具有巨大潜力。", "method": "提出CrosswalkNet框架，结合OBB、注意力机制、双分支空间金字塔池化模块和余弦退火优化技术。", "result": "在Massachusetts数据集上达到96.5%的精确率和93.3%的召回率，并在其他地区无需微调即可应用。", "conclusion": "CrosswalkNet为政策制定者和规划者提供了增强行人安全和改善城市流动性的有效工具。"}}
{"id": "2506.06637", "pdf": "https://arxiv.org/pdf/2506.06637", "abs": "https://arxiv.org/abs/2506.06637", "authors": ["Olimjon Toirov", "Wei Yu"], "title": "Non-Intrusive Load Monitoring Based on Image Load Signatures and Continual Learning", "categories": ["cs.LG", "cs.AI", "cs.CV", "eess.SP"], "comment": "10 pages, 3 figures, 2025 2nd International Conference on Digital\n  Society and Artificial Intelligence (DSAI 2025), Conference dates: May 23-25,\n  2025", "summary": "Non-Intrusive Load Monitoring (NILM) identifies the operating status and\nenergy consumption of each electrical device in the circuit by analyzing the\nelectrical signals at the bus, which is of great significance for smart power\nmanagement. However, the complex and changeable load combinations and\napplication environments lead to the challenges of poor feature robustness and\ninsufficient model generalization of traditional NILM methods. To this end,\nthis paper proposes a new non-intrusive load monitoring method that integrates\n\"image load signature\" and continual learning. This method converts\nmulti-dimensional power signals such as current, voltage, and power factor into\nvisual image load feature signatures, and combines deep convolutional neural\nnetworks to realize the identification and classification of multiple devices;\nat the same time, self-supervised pre-training is introduced to improve feature\ngeneralization, and continual online learning strategies are used to overcome\nmodel forgetting to adapt to the emergence of new loads. This paper conducts a\nlarge number of experiments on high-sampling rate load datasets, and compares a\nvariety of existing methods and model variants. The results show that the\nproposed method has achieved significant improvements in recognition accuracy.", "AI": {"tldr": "本文提出了一种结合图像负载特征和持续学习的非侵入式负载监测方法，显著提高了识别精度。", "motivation": "传统非侵入式负载监测方法在复杂多变的负载组合和应用环境中特征鲁棒性差、模型泛化能力不足。", "method": "将多维电力信号转换为图像负载特征，结合深度卷积神经网络进行设备识别与分类，引入自监督预训练和持续在线学习策略。", "result": "在高采样率负载数据集上的实验表明，该方法在识别精度上有显著提升。", "conclusion": "该方法通过图像特征和持续学习，有效解决了传统方法的局限性，适用于智能电力管理。"}}
{"id": "2506.07168", "pdf": "https://arxiv.org/pdf/2506.07168", "abs": "https://arxiv.org/abs/2506.07168", "authors": ["Huanyi Xie", "Lijie Hu", "Lu Yu", "Tianhao Huang", "Longfei Li", "Meng Li", "Jun Zhou", "Huan Wang", "Di Wang"], "title": "Efficient Text-Attributed Graph Learning through Selective Annotation and Graph Alignment", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "23 pages", "summary": "In the realm of Text-attributed Graphs (TAGs), traditional graph neural\nnetworks (GNNs) often fall short due to the complex textual information\nassociated with each node. Recent methods have improved node representations by\nleveraging large language models (LLMs) to enhance node text features, but\nthese approaches typically require extensive annotations or fine-tuning across\nall nodes, which is both time-consuming and costly. To overcome these\nchallenges, we introduce GAGA, an efficient framework for TAG representation\nlearning. GAGA reduces annotation time and cost by focusing on annotating only\nrepresentative nodes and edges. It constructs an annotation graph that captures\nthe topological relationships among these annotations. Furthermore, GAGA\nemploys a two-level alignment module to effectively integrate the annotation\ngraph with the TAG, aligning their underlying structures. Experiments show that\nGAGA achieves classification accuracies on par with or surpassing\nstate-of-the-art methods while requiring only 1% of the data to be annotated,\ndemonstrating its high efficiency.", "AI": {"tldr": "GAGA是一种高效的TAG表示学习框架，通过仅标注代表性节点和边来减少时间和成本，并通过两级对齐模块整合标注图和TAG，实验表明其性能优越且高效。", "motivation": "传统GNN在处理带文本属性的图（TAG）时表现不佳，现有方法依赖大量标注或微调，成本高昂。GAGA旨在解决这一问题。", "method": "GAGA通过标注代表性节点和边构建标注图，利用两级对齐模块将标注图与TAG的结构对齐。", "result": "GAGA在仅需1%标注数据的情况下，分类准确率与或优于现有方法。", "conclusion": "GAGA是一种高效且性能优越的TAG表示学习框架。"}}
{"id": "2506.07886", "pdf": "https://arxiv.org/pdf/2506.07886", "abs": "https://arxiv.org/abs/2506.07886", "authors": ["Gen Li", "Yutong Chen", "Yiqian Wu", "Kaifeng Zhao", "Marc Pollefeys", "Siyu Tang"], "title": "EgoM2P: Egocentric Multimodal Multitask Pretraining", "categories": ["cs.CV"], "comment": null, "summary": "Understanding multimodal signals in egocentric vision, such as RGB video,\ndepth, camera poses, and gaze, is essential for applications in augmented\nreality, robotics, and human-computer interaction. These capabilities enable\nsystems to better interpret the camera wearer's actions, intentions, and\nsurrounding environment. However, building large-scale egocentric multimodal\nand multitask models presents unique challenges. Egocentric data are inherently\nheterogeneous, with large variations in modality coverage across devices and\nsettings. Generating pseudo-labels for missing modalities, such as gaze or\nhead-mounted camera trajectories, is often infeasible, making standard\nsupervised learning approaches difficult to scale. Furthermore, dynamic camera\nmotion and the complex temporal and spatial structure of first-person video\npose additional challenges for the direct application of existing multimodal\nfoundation models.\n  To address these challenges, we introduce a set of efficient temporal\ntokenizers and propose EgoM2P, a masked modeling framework that learns from\ntemporally aware multimodal tokens to train a large, general-purpose model for\negocentric 4D understanding. This unified design supports multitasking across\ndiverse egocentric perception and synthesis tasks, including gaze prediction,\negocentric camera tracking, and monocular depth estimation from egocentric\nvideo. EgoM2P also serves as a generative model for conditional egocentric\nvideo synthesis. Across these tasks, EgoM2P matches or outperforms specialist\nmodels while being an order of magnitude faster. We will fully open-source\nEgoM2P to support the community and advance egocentric vision research. Project\npage: https://egom2p.github.io/", "AI": {"tldr": "论文提出了一种名为EgoM2P的框架，通过高效的时序标记器和掩码建模方法，解决了多模态自我中心视觉数据中的异质性和缺失模态问题，支持多任务处理，并在性能上优于专用模型。", "motivation": "自我中心视觉中的多模态信号（如RGB视频、深度、相机位姿和视线）对增强现实、机器人和人机交互至关重要，但数据异质性和模态缺失使得传统监督学习方法难以扩展。", "method": "提出了高效的时序标记器和EgoM2P框架，通过掩码建模从时序感知的多模态标记中学习，支持多任务处理，包括视线预测、相机追踪和深度估计。", "result": "EgoM2P在多个任务中表现优于专用模型，且速度更快。", "conclusion": "EgoM2P为自我中心视觉研究提供了一个通用且高效的解决方案，并计划开源以推动社区发展。"}}
{"id": "2506.07891", "pdf": "https://arxiv.org/pdf/2506.07891", "abs": "https://arxiv.org/abs/2506.07891", "authors": ["Simone Facchiano", "Stefano Saravalle", "Matteo Migliarini", "Edoardo De Matteis", "Alessio Sampieri", "Andrea Pilzer", "Emanuele Rodolà", "Indro Spinelli", "Luca Franco", "Fabio Galasso"], "title": "Video Unlearning via Low-Rank Refusal Vector", "categories": ["cs.CV"], "comment": null, "summary": "Video generative models democratize the creation of visual content through\nintuitive instruction following, but they also inherit the biases and harmful\nconcepts embedded within their web-scale training data. This inheritance\ncreates a significant risk, as users can readily generate undesirable and even\nillegal content. This work introduces the first unlearning technique tailored\nexplicitly for video diffusion models to address this critical issue. Our\nmethod requires 5 multi-modal prompt pairs only. Each pair contains a \"safe\"\nand an \"unsafe\" example that differ only by the target concept. Averaging their\nper-layer latent differences produces a \"refusal vector\", which, once\nsubtracted from the model parameters, neutralizes the unsafe concept. We\nintroduce a novel low-rank factorization approach on the covariance difference\nof embeddings that yields robust refusal vectors. This isolates the target\nconcept while minimizing collateral unlearning of other semantics, thus\npreserving the visual quality of the generated video. Our method preserves the\nmodel's generation quality while operating without retraining or access to the\noriginal training data. By embedding the refusal direction directly into the\nmodel's weights, the suppression mechanism becomes inherently more robust\nagainst adversarial bypass attempts compared to surface-level input-output\nfilters. In a thorough qualitative and quantitative evaluation, we show that we\ncan neutralize a variety of harmful contents, including explicit nudity,\ngraphic violence, copyrights, and trademarks. Project page:\nhttps://www.pinlab.org/video-unlearning.", "AI": {"tldr": "本文提出了一种针对视频扩散模型的去学习技术，仅需5对多模态提示对即可生成“拒绝向量”，用于消除模型中的有害概念。", "motivation": "视频生成模型可能继承训练数据中的偏见和有害内容，导致用户生成不良或非法内容，亟需解决方案。", "method": "通过多模态提示对生成拒绝向量，采用低秩分解方法优化嵌入协方差差异，确保目标概念被隔离且不影响其他语义。", "result": "方法有效中和了多种有害内容（如裸露、暴力、版权等），同时保持生成视频的视觉质量。", "conclusion": "该技术无需重新训练或原始数据，直接嵌入拒绝方向到模型权重中，提高了对抗规避的鲁棒性。"}}
{"id": "2506.06658", "pdf": "https://arxiv.org/pdf/2506.06658", "abs": "https://arxiv.org/abs/2506.06658", "authors": ["Calvin Luo", "Zilai Zeng", "Mingxi Jia", "Yilun Du", "Chen Sun"], "title": "Self-Adapting Improvement Loops for Robotic Learning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Video generative models trained on expert demonstrations have been utilized\nas performant text-conditioned visual planners for solving robotic tasks.\nHowever, generalization to unseen tasks remains a challenge. Whereas improved\ngeneralization may be facilitated by leveraging learned prior knowledge from\nadditional pre-collected offline data sources, such as web-scale video\ndatasets, in the era of experience we aim to design agents that can\ncontinuously improve in an online manner from self-collected behaviors. In this\nwork we thus propose the Self-Adapting Improvement Loop (SAIL), where an\nin-domain video model iteratively updates itself on self-produced trajectories,\ncollected through adaptation with an internet-scale pretrained video model, and\nsteadily improves its performance for a specified task of interest. We apply\nSAIL to a diverse suite of MetaWorld tasks, as well as two manipulation tasks\non a real robot arm, and find that performance improvements continuously emerge\nover multiple iterations for novel tasks initially unseen during original\nin-domain video model training. Furthermore, we discover that SAIL is\nsurprisingly robust regarding if and how the self-collected experience is\nfiltered, and the quality of the initial in-domain demonstrations. Through\nadaptation with summarized internet-scale data, and learning through online\nexperience, we thus demonstrate a way to iteratively bootstrap a\nhigh-performance video model for solving novel robotic tasks through\nself-improvement.", "AI": {"tldr": "论文提出了一种自适应的改进循环（SAIL），通过结合互联网规模的预训练视频模型和自收集的行为数据，持续提升视频生成模型在新任务中的性能。", "motivation": "解决视频生成模型在新任务中泛化能力不足的问题，同时探索如何通过在线自我改进提升性能。", "method": "提出SAIL方法，利用预训练视频模型和自收集的轨迹数据，迭代更新域内视频模型。", "result": "在MetaWorld任务和真实机器人任务中，SAIL持续提升性能，且对数据过滤和初始演示质量具有鲁棒性。", "conclusion": "SAIL通过结合预训练数据和在线学习，为机器人任务提供了一种高效的自适应改进方法。"}}
{"id": "2506.07905", "pdf": "https://arxiv.org/pdf/2506.07905", "abs": "https://arxiv.org/abs/2506.07905", "authors": ["Jie Yang", "Feipeng Ma", "Zitian Wang", "Dacheng Yin", "Kang Rong", "Fengyun Rao", "Ruimao Zhang"], "title": "WeThink: Toward General-purpose Vision-Language Reasoning via Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "Building on the success of text-based reasoning models like DeepSeek-R1,\nextending these capabilities to multimodal reasoning holds great promise. While\nrecent works have attempted to adapt DeepSeek-R1-style reinforcement learning\n(RL) training paradigms to multimodal large language models (MLLM), focusing on\ndomain-specific tasks like math and visual perception, a critical question\nremains: How can we achieve the general-purpose visual-language reasoning\nthrough RL? To address this challenge, we make three key efforts: (1) A novel\nScalable Multimodal QA Synthesis pipeline that autonomously generates\ncontext-aware, reasoning-centric question-answer (QA) pairs directly from the\ngiven images. (2) The open-source WeThink dataset containing over 120K\nmultimodal QA pairs with annotated reasoning paths, curated from 18 diverse\ndataset sources and covering various question domains. (3) A comprehensive\nexploration of RL on our dataset, incorporating a hybrid reward mechanism that\ncombines rule-based verification with model-based assessment to optimize RL\ntraining efficiency across various task domains. Across 14 diverse MLLM\nbenchmarks, we demonstrate that our WeThink dataset significantly enhances\nperformance, from mathematical reasoning to diverse general multimodal tasks.\nMoreover, we show that our automated data pipeline can continuously increase\ndata diversity to further improve model performance.", "AI": {"tldr": "论文提出了一种通用的视觉-语言推理方法，通过强化学习（RL）训练多模态大语言模型（MLLM），并开发了自动化数据生成工具和开源数据集WeThink。", "motivation": "扩展文本推理模型（如DeepSeek-R1）到多模态领域，解决通用视觉-语言推理的挑战。", "method": "1. 提出可扩展的多模态QA合成管道；2. 开源WeThink数据集；3. 结合规则和模型评估的混合奖励机制优化RL训练。", "result": "在14个MLLM基准测试中，WeThink显著提升了性能，覆盖数学推理和多样化多模态任务。", "conclusion": "自动化数据管道和WeThink数据集有效提升了多模态推理能力，且数据多样性可进一步优化模型性能。"}}
{"id": "2506.06659", "pdf": "https://arxiv.org/pdf/2506.06659", "abs": "https://arxiv.org/abs/2506.06659", "authors": ["Wenhao Yao", "Zhenxin Li", "Shiyi Lan", "Zi Wang", "Xinglong Sun", "Jose M. Alvarez", "Zuxuan Wu"], "title": "DriveSuprim: Towards Precise Trajectory Selection for End-to-End Planning", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "15 pages, 6 figures", "summary": "In complex driving environments, autonomous vehicles must navigate safely.\nRelying on a single predicted path, as in regression-based approaches, usually\ndoes not explicitly assess the safety of the predicted trajectory.\nSelection-based methods address this by generating and scoring multiple\ntrajectory candidates and predicting the safety score for each, but face\noptimization challenges in precisely selecting the best option from thousands\nof possibilities and distinguishing subtle but safety-critical differences,\nespecially in rare or underrepresented scenarios. We propose DriveSuprim to\novercome these challenges and advance the selection-based paradigm through a\ncoarse-to-fine paradigm for progressive candidate filtering, a rotation-based\naugmentation method to improve robustness in out-of-distribution scenarios, and\na self-distillation framework to stabilize training. DriveSuprim achieves\nstate-of-the-art performance, reaching 93.5% PDMS in NAVSIM v1 and 87.1% EPDMS\nin NAVSIM v2 without extra data, demonstrating superior safetycritical\ncapabilities, including collision avoidance and compliance with rules, while\nmaintaining high trajectory quality in various driving scenarios.", "AI": {"tldr": "DriveSuprim通过粗到细的候选轨迹筛选、旋转增强方法和自蒸馏框架，提升了自动驾驶车辆在复杂环境中的安全性和轨迹质量。", "motivation": "现有基于回归的方法无法明确评估预测轨迹的安全性，而基于选择的方法在优化和区分安全关键差异方面存在挑战。", "method": "提出DriveSuprim，采用粗到细的候选轨迹筛选、旋转增强方法和自蒸馏框架。", "result": "在NAVSIM v1和v2中分别达到93.5% PDMS和87.1% EPDMS，展示了卓越的安全关键能力和轨迹质量。", "conclusion": "DriveSuprim在复杂驾驶场景中显著提升了安全性和性能，为自动驾驶提供了更可靠的解决方案。"}}
{"id": "2506.07925", "pdf": "https://arxiv.org/pdf/2506.07925", "abs": "https://arxiv.org/abs/2506.07925", "authors": ["Yaxita Amin", "Naimisha S Trivedi", "Rashmi Bhattad"], "title": "A Comparative Study of U-Net Architectures for Change Detection in Satellite Images", "categories": ["cs.CV", "cs.LG", "eess.IV"], "comment": null, "summary": "Remote sensing change detection is essential for monitoring the everchanging\nlandscapes of the Earth. The U-Net architecture has gained popularity for its\ncapability to capture spatial information and perform pixel-wise\nclassification. However, their application in the Remote sensing field remains\nlargely unexplored. Therefore, this paper fill the gap by conducting a\ncomprehensive analysis of 34 papers. This study conducts a comparison and\nanalysis of 18 different U-Net variations, assessing their potential for\ndetecting changes in remote sensing. We evaluate both benefits along with\ndrawbacks of each variation within the framework of this particular\napplication. We emphasize variations that are explicitly built for change\ndetection, such as Siamese Swin-U-Net, which utilizes a Siamese architecture.\nThe analysis highlights the significance of aspects such as managing data from\ndifferent time periods and collecting relationships over a long distance to\nenhance the precision of change detection. This study provides valuable\ninsights for researchers and practitioners that choose U-Net versions for\nremote sensing change detection tasks.", "AI": {"tldr": "本文对34篇论文进行了综合分析，比较了18种不同的U-Net变体在遥感变化检测中的应用潜力，并评估了它们的优缺点。", "motivation": "填补U-Net在遥感变化检测领域应用的空白，为研究者和实践者提供选择U-Net版本的参考。", "method": "通过文献综述和比较分析，评估18种U-Net变体在遥感变化检测中的表现，特别关注专为变化检测设计的变体（如Siamese Swin-U-Net）。", "result": "分析揭示了处理多时相数据和长距离关系对提高变化检测精度的重要性。", "conclusion": "本研究为遥感变化检测任务中U-Net版本的选择提供了有价值的见解。"}}
{"id": "2506.06683", "pdf": "https://arxiv.org/pdf/2506.06683", "abs": "https://arxiv.org/abs/2506.06683", "authors": ["Shiying Duan", "Pei Ren", "Nanxiang Jiang", "Zhengping Che", "Jian Tang", "Yifan Sun", "Zhaoxin Fan", "Wenjun Wu"], "title": "RoboPARA: Dual-Arm Robot Planning with Parallel Allocation and Recomposition Across Tasks", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Dual-arm robots play a crucial role in improving efficiency and flexibility\nin complex multitasking scenarios. While existing methods have achieved\npromising results in task planning, they often fail to fully optimize task\nparallelism, limiting the potential of dual-arm collaboration. To address this\nissue, we propose RoboPARA, a novel large language model (LLM)-driven framework\nfor dual-arm task parallelism planning. RoboPARA employs a two-stage process:\n(1) Dependency Graph-based Planning Candidates Generation, which constructs\ndirected acyclic graphs (DAGs) to model task dependencies and eliminate\nredundancy, and (2) Graph Re-Traversal-based Dual-Arm Parallel Planning, which\noptimizes DAG traversal to maximize parallelism while maintaining task\ncoherence. In addition, we introduce the Cross-Scenario Dual-Arm Parallel Task\ndataset (X-DAPT dataset), the first dataset specifically designed to evaluate\ndual-arm task parallelism across diverse scenarios and difficulty levels.\nExtensive experiments on the X-DAPT dataset demonstrate that RoboPARA\nsignificantly outperforms existing methods, achieving higher efficiency and\nreliability, particularly in complex task combinations. The code and dataset\nwill be released upon acceptance.", "AI": {"tldr": "RoboPARA是一个基于大语言模型的双臂机器人任务并行规划框架，通过两阶段优化任务依赖和并行性，显著提升效率。", "motivation": "现有方法在双臂协作任务规划中未能充分优化任务并行性，限制了双臂机器人的潜力。", "method": "RoboPARA采用两阶段方法：依赖图生成规划候选和基于图重遍历的双臂并行规划。", "result": "在X-DAPT数据集上，RoboPARA显著优于现有方法，尤其在复杂任务组合中表现更高效可靠。", "conclusion": "RoboPARA通过优化任务并行性，提升了双臂机器人的效率和灵活性。"}}
{"id": "2506.07233", "pdf": "https://arxiv.org/pdf/2506.07233", "abs": "https://arxiv.org/abs/2506.07233", "authors": ["Tzu-wen Hsu", "Ke-Han Lu", "Cheng-Han Chiang", "Hung-yi Lee"], "title": "Reducing Object Hallucination in Large Audio-Language Models via Audio-Aware Decoding", "categories": ["eess.AS", "cs.CL"], "comment": null, "summary": "Large Audio-Language Models (LALMs) can take audio and text as the inputs and\nanswer questions about the audio. While prior LALMs have shown strong\nperformance on standard benchmarks, there has been alarming evidence that LALMs\ncan hallucinate what is presented in the audio. To mitigate the hallucination\nof LALMs, we introduce Audio-Aware Decoding (AAD), a lightweight inference-time\nstrategy that uses contrastive decoding to compare the token prediction logits\nwith and without the audio context. By contrastive decoding, AAD promotes the\ntokens whose probability increases when the audio is present. We conduct our\nexperiment on object hallucination datasets with three LALMs and show that AAD\nimproves the F1 score by 0.046 to 0.428. We also show that AAD can improve the\naccuracy on general audio QA datasets like Clotho-AQA by 5.4% to 10.3%. We\nconduct thorough ablation studies to understand the effectiveness of each\ncomponent in AAD.", "AI": {"tldr": "论文提出了一种名为音频感知解码（AAD）的轻量级推理策略，通过对比解码减少大型音频语言模型（LALMs）的幻觉问题，显著提升了性能。", "motivation": "现有LALMs在标准基准测试中表现良好，但存在对音频内容产生幻觉的问题，需要一种有效的方法来缓解这一问题。", "method": "引入AAD，通过对比解码比较音频上下文存在与否时的token预测概率，优先选择音频存在时概率增加的token。", "result": "在对象幻觉数据集上，AAD将F1分数提升了0.046至0.428；在通用音频QA数据集上，准确率提升了5.4%至10.3%。", "conclusion": "AAD是一种有效的轻量级方法，能够显著减少LALMs的幻觉问题，并在多个数据集上验证了其有效性。"}}
{"id": "2506.07936", "pdf": "https://arxiv.org/pdf/2506.07936", "abs": "https://arxiv.org/abs/2506.07936", "authors": ["Chengyue Huang", "Yuchen Zhu", "Sichen Zhu", "Jingyun Xiao", "Moises Andrade", "Shivang Chopra", "Zsolt Kira"], "title": "Mimicking or Reasoning: Rethinking Multi-Modal In-Context Learning in Vision-Language Models", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Vision-language models (VLMs) are widely assumed to exhibit in-context\nlearning (ICL), a property similar to that of their language-only counterparts.\nWhile recent work suggests VLMs can perform multimodal ICL (MM-ICL), studies\nshow they often rely on shallow heuristics -- such as copying or majority\nvoting -- rather than true task understanding. We revisit this assumption by\nevaluating VLMs under distribution shifts, where support examples come from a\ndataset different from the query. Surprisingly, performance often degrades with\nmore demonstrations, and models tend to copy answers rather than learn from\nthem. To investigate further, we propose a new MM-ICL with Reasoning pipeline\nthat augments each demonstration with a generated rationale alongside the\nanswer. We conduct extensive and comprehensive experiments on both perception-\nand reasoning-required datasets with open-source VLMs ranging from 3B to 72B\nand proprietary models such as Gemini 2.0. We conduct controlled studies\nvarying shot count, retrieval method, rationale quality, and distribution. Our\nresults show limited performance sensitivity across these factors, suggesting\nthat current VLMs do not effectively utilize demonstration-level information as\nintended in MM-ICL.", "AI": {"tldr": "研究发现当前视觉语言模型（VLM）在多模态上下文学习（MM-ICL）中表现不佳，倾向于依赖浅层启发式方法而非真正理解任务。通过引入带推理的MM-ICL方法，实验表明模型对演示信息的利用有限。", "motivation": "探讨视觉语言模型是否真正具备多模态上下文学习能力，而非依赖浅层启发式方法。", "method": "提出带推理的MM-ICL方法，为每个演示生成答案和推理过程，并在不同数据集和模型上进行实验。", "result": "实验显示模型性能对演示数量、检索方法、推理质量等因素不敏感，表明当前VLM未能有效利用演示信息。", "conclusion": "当前视觉语言模型在多模态上下文学习中的表现有限，需进一步改进以提升任务理解能力。"}}
{"id": "2506.06693", "pdf": "https://arxiv.org/pdf/2506.06693", "abs": "https://arxiv.org/abs/2506.06693", "authors": ["Priyanshu Yadav"], "title": "Design and Implementation of a RISC-V SoC with Custom DSP Accelerators for Edge Computing", "categories": ["cs.AR", "cs.AI", "eess.SP", "C.1.3, B.5.2, I.5.1 C.1.3, B.5.2, I.5.1 C.1.3, B.5.2, I.5.1"], "comment": "12 Pages, 1 figure", "summary": "This paper presents a comprehensive analysis of the RISC-V instruction set\narchitecture, focusing on its modular design, implementation challenges, and\nperformance characteristics. We examine the RV32I base instruction set with\nextensions for multiplication (M) and atomic operations (A). Through\ncycle-accurate simulation of a pipelined implementation, we evaluate\nperformance metrics including CPI (cycles per instruction) and power\nefficiency. Our results demonstrate RISC-V's advantages in embedded systems and\nits scalability for custom accelerators. Comparative analysis shows a 17%\nreduction in power consumption compared to ARM Cortex-M0 implementations in\nsimilar process nodes. The open-standard nature of RISC-V provides significant\nflexibility for domain-specific optimizations.", "AI": {"tldr": "本文对RISC-V指令集架构进行了全面分析，重点关注其模块化设计、实现挑战和性能特征。通过RV32I基础指令集及其扩展（乘法和原子操作）的周期精确模拟，评估了CPI和能效等性能指标。结果表明RISC-V在嵌入式系统中的优势及其对定制加速器的可扩展性。", "motivation": "RISC-V作为一种开放标准指令集架构，其模块化设计和灵活性为嵌入式系统和定制加速器提供了潜力。本文旨在分析其性能特征和实现挑战。", "method": "使用RV32I基础指令集及其扩展（乘法和原子操作），通过周期精确的流水线实现模拟，评估CPI和能效等性能指标。", "result": "结果显示，RISC-V在嵌入式系统中具有优势，并且在类似工艺节点下比ARM Cortex-M0实现功耗降低17%。", "conclusion": "RISC-V的开放标准特性为领域特定优化提供了显著灵活性，展示了其在嵌入式系统和定制加速器中的潜力。"}}
{"id": "2506.07943", "pdf": "https://arxiv.org/pdf/2506.07943", "abs": "https://arxiv.org/abs/2506.07943", "authors": ["Yizhen Li", "Dell Zhang", "Xuelong Li", "Yiqing Shen"], "title": "Decoupling the Image Perception and Multimodal Reasoning for Reasoning Segmentation with Digital Twin Representations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Reasoning Segmentation (RS) is a multimodal vision-text task that requires\nsegmenting objects based on implicit text queries, demanding both precise\nvisual perception and vision-text reasoning capabilities. Current RS approaches\nrely on fine-tuning vision-language models (VLMs) for both perception and\nreasoning, but their tokenization of images fundamentally disrupts continuous\nspatial relationships between objects. We introduce DTwinSeger, a novel RS\napproach that leverages Digital Twin (DT) representation as an intermediate\nlayer to decouple perception from reasoning. Innovatively, DTwinSeger\nreformulates RS as a two-stage process, where the first transforms the image\ninto a structured DT representation that preserves spatial relationships and\nsemantic properties and then employs a Large Language Model (LLM) to perform\nexplicit reasoning over this representation to identify target objects. We\npropose a supervised fine-tuning method specifically for LLM with DT\nrepresentation, together with a corresponding fine-tuning dataset Seg-DT, to\nenhance the LLM's reasoning capabilities with DT representations. Experiments\nshow that our method can achieve state-of-the-art performance on two image RS\nbenchmarks and three image referring segmentation benchmarks. It yields that DT\nrepresentation functions as an effective bridge between vision and text,\nenabling complex multimodal reasoning tasks to be accomplished solely with an\nLLM.", "AI": {"tldr": "DTwinSeger提出了一种新的多模态视觉-文本任务方法，通过数字孪生（DT）表示解耦感知与推理，利用LLM进行显式推理，实现了在多个基准测试中的最优性能。", "motivation": "现有的基于视觉语言模型（VLMs）的方法在图像标记化过程中破坏了对象的连续空间关系，限制了推理能力。", "method": "DTwinSeger将任务分为两阶段：首先生成保留空间关系的DT表示，然后利用LLM进行显式推理。还提出了针对LLM的监督微调方法和数据集Seg-DT。", "result": "在多个基准测试中实现了最优性能，证明了DT表示作为视觉与文本桥梁的有效性。", "conclusion": "DTwinSeger通过DT表示和LLM的结合，成功解决了多模态推理任务，展示了DT表示在复杂任务中的潜力。"}}
{"id": "2506.07398", "pdf": "https://arxiv.org/pdf/2506.07398", "abs": "https://arxiv.org/abs/2506.07398", "authors": ["Guibin Zhang", "Muxin Fu", "Guancheng Wan", "Miao Yu", "Kun Wang", "Shuicheng Yan"], "title": "G-Memory: Tracing Hierarchical Memory for Multi-Agent Systems", "categories": ["cs.MA", "cs.CL", "cs.LG"], "comment": null, "summary": "Large language model (LLM)-powered multi-agent systems (MAS) have\ndemonstrated cognitive and execution capabilities that far exceed those of\nsingle LLM agents, yet their capacity for self-evolution remains hampered by\nunderdeveloped memory architectures. Upon close inspection, we are alarmed to\ndiscover that prevailing MAS memory mechanisms (1) are overly simplistic,\ncompletely disregarding the nuanced inter-agent collaboration trajectories, and\n(2) lack cross-trial and agent-specific customization, in stark contrast to the\nexpressive memory developed for single agents. To bridge this gap, we introduce\nG-Memory, a hierarchical, agentic memory system for MAS inspired by\norganizational memory theory, which manages the lengthy MAS interaction via a\nthree-tier graph hierarchy: insight, query, and interaction graphs. Upon\nreceiving a new user query, G-Memory performs bi-directional memory traversal\nto retrieve both $\\textit{high-level, generalizable insights}$ that enable the\nsystem to leverage cross-trial knowledge, and $\\textit{fine-grained, condensed\ninteraction trajectories}$ that compactly encode prior collaboration\nexperiences. Upon task execution, the entire hierarchy evolves by assimilating\nnew collaborative trajectories, nurturing the progressive evolution of agent\nteams. Extensive experiments across five benchmarks, three LLM backbones, and\nthree popular MAS frameworks demonstrate that G-Memory improves success rates\nin embodied action and accuracy in knowledge QA by up to $20.89\\%$ and\n$10.12\\%$, respectively, without any modifications to the original frameworks.\nOur codes are available at https://github.com/bingreeky/GMemory.", "AI": {"tldr": "论文提出G-Memory，一种分层、代理化的记忆系统，用于解决多代理系统（MAS）中记忆架构不足的问题，显著提升了任务执行成功率。", "motivation": "现有MAS记忆机制过于简单，忽视了代理间协作的复杂性，且缺乏跨任务和代理定制化能力，限制了系统的自我进化能力。", "method": "引入G-Memory，基于组织记忆理论，通过三层图层次（洞察图、查询图和交互图）管理代理间交互，并支持双向记忆检索。", "result": "实验表明，G-Memory在五个基准测试中显著提升了任务成功率和知识问答准确率，最高分别提升20.89%和10.12%。", "conclusion": "G-Memory通过分层记忆架构有效解决了MAS记忆不足的问题，推动了代理团队的持续进化。"}}
{"id": "2506.07960", "pdf": "https://arxiv.org/pdf/2506.07960", "abs": "https://arxiv.org/abs/2506.07960", "authors": ["Ari Vesalainen", "Jenna Kanerva", "Aida Nitsch", "Kiia Korsu", "Ilari Larkiola", "Laura Ruotsalainen", "Filip Ginter"], "title": "Creating a Historical Migration Dataset from Finnish Church Records, 1800-1920", "categories": ["cs.CV", "I.4.6, J.5"], "comment": null, "summary": "This article presents a large-scale effort to create a structured dataset of\ninternal migration in Finland between 1800 and 1920 using digitized church\nmoving records. These records, maintained by Evangelical-Lutheran parishes,\ndocument the migration of individuals and families and offer a valuable source\nfor studying historical demographic patterns. The dataset includes over six\nmillion entries extracted from approximately 200,000 images of handwritten\nmigration records.\n  The data extraction process was automated using a deep learning pipeline that\nincluded layout analysis, table detection, cell classification, and handwriting\nrecognition. The complete pipeline was applied to all images, resulting in a\nstructured dataset suitable for research.\n  The dataset can be used to study internal migration, urbanization, and family\nmigration, and the spread of disease in preindustrial Finland. A case study\nfrom the Elim\\\"aki parish shows how local migration histories can be\nreconstructed. The work demonstrates how large volumes of handwritten archival\nmaterial can be transformed into structured data to support historical and\ndemographic research.", "AI": {"tldr": "本文通过深度学习技术从芬兰1800-1920年的教会迁移记录中提取了600多万条数据，构建了一个结构化数据集，用于研究历史人口迁移模式。", "motivation": "利用数字化教会迁移记录研究芬兰历史人口迁移、城市化及疾病传播等课题。", "method": "采用深度学习流程（布局分析、表格检测、单元格分类和手写识别）自动化提取数据。", "result": "成功构建包含600多万条记录的结构化数据集，并通过案例展示了如何重建局部迁移历史。", "conclusion": "该研究展示了如何将大量手写档案材料转化为结构化数据，支持历史和人口学研究。"}}
{"id": "2506.06701", "pdf": "https://arxiv.org/pdf/2506.06701", "abs": "https://arxiv.org/abs/2506.06701", "authors": ["Fudong Lin", "Wanrou Du", "Jinchan Liu", "Tarikul Milon", "Shelby Meche", "Wu Xu", "Xiaoqi Qin", "Xu Yuan"], "title": "Do Protein Transformers Have Biological Intelligence?", "categories": ["cs.LG", "cs.AI", "q-bio.BM"], "comment": "Accepted by European Conference on Machine Learning and Principles\n  and Practice of Knowledge Discovery in Databases (ECML-PKDD 2025)", "summary": "Deep neural networks, particularly Transformers, have been widely adopted for\npredicting the functional properties of proteins. In this work, we focus on\nexploring whether Protein Transformers can capture biological intelligence\namong protein sequences. To achieve our goal, we first introduce a protein\nfunction dataset, namely Protein-FN, providing over 9000 protein data with\nmeaningful labels. Second, we devise a new Transformer architecture, namely\nSequence Protein Transformers (SPT), for computationally efficient protein\nfunction predictions. Third, we develop a novel Explainable Artificial\nIntelligence (XAI) technique called Sequence Score, which can efficiently\ninterpret the decision-making processes of protein models, thereby overcoming\nthe difficulty of deciphering biological intelligence bided in Protein\nTransformers. Remarkably, even our smallest SPT-Tiny model, which contains only\n5.4M parameters, demonstrates impressive predictive accuracy, achieving 94.3%\non the Antibiotic Resistance (AR) dataset and 99.6% on the Protein-FN dataset,\nall accomplished by training from scratch. Besides, our Sequence Score\ntechnique helps reveal that our SPT models can discover several meaningful\npatterns underlying the sequence structures of protein data, with these\npatterns aligning closely with the domain knowledge in the biology community.\nWe have officially released our Protein-FN dataset on Hugging Face Datasets\nhttps://huggingface.co/datasets/Protein-FN/Protein-FN. Our code is available at\nhttps://github.com/fudong03/BioIntelligence.", "AI": {"tldr": "本文提出了一种新的Transformer架构（SPT）和解释性AI技术（Sequence Score），用于高效预测蛋白质功能并揭示其生物智能。", "motivation": "探索蛋白质Transformer是否能捕捉蛋白质序列中的生物智能。", "method": "1. 引入Protein-FN数据集；2. 设计SPT架构；3. 开发Sequence Score技术。", "result": "SPT-Tiny模型在AR和Protein-FN数据集上分别达到94.3%和99.6%的准确率。", "conclusion": "SPT模型能发现与生物学知识一致的蛋白质序列模式，数据集和代码已开源。"}}
{"id": "2506.07402", "pdf": "https://arxiv.org/pdf/2506.07402", "abs": "https://arxiv.org/abs/2506.07402", "authors": ["Yukai Zhou", "Sibei Yang", "Wenjie Wang"], "title": "Beyond Jailbreaks: Revealing Stealthier and Broader LLM Security Risks Stemming from Alignment Failures", "categories": ["cs.CR", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) are increasingly deployed in real-world\napplications, raising concerns about their security. While jailbreak attacks\nhighlight failures under overtly harmful queries, they overlook a critical\nrisk: incorrectly answering harmless-looking inputs can be dangerous and cause\nreal-world harm (Implicit Harm). We systematically reformulate the LLM risk\nlandscape through a structured quadrant perspective based on output factuality\nand input harmlessness, uncovering an overlooked high-risk region. To\ninvestigate this gap, we propose JailFlipBench, a benchmark aims to capture\nimplicit harm, spanning single-modal, multimodal, and factual extension\nscenarios with diverse evaluation metrics. We further develop initial JailFlip\nattack methodologies and conduct comprehensive evaluations across multiple\nopen-source and black-box LLMs, show that implicit harm present immediate and\nurgent real-world risks, calling for broader LLM safety assessments and\nalignment beyond conventional jailbreak paradigms.", "AI": {"tldr": "论文提出LLMs存在隐性危害风险，提出JailFlipBench基准和攻击方法，揭示其现实危害。", "motivation": "LLMs在现实应用中的安全性问题日益突出，传统越狱攻击忽略了隐性危害，需系统性评估。", "method": "通过结构化象限视角重新定义LLM风险，提出JailFlipBench基准和攻击方法，评估多种LLMs。", "result": "发现隐性危害具有现实风险，需扩展LLM安全评估和校准范围。", "conclusion": "隐性危害是LLMs的紧迫风险，需超越传统越狱范式进行更广泛的安全评估。"}}
{"id": "2506.07964", "pdf": "https://arxiv.org/pdf/2506.07964", "abs": "https://arxiv.org/abs/2506.07964", "authors": ["Wenxin Tang", "Jingyu Xiao", "Wenxuan Jiang", "Xi Xiao", "Yuhang Wang", "Xuxin Tang", "Qing Li", "Yuehe Ma", "Junliang Liu", "Shisong Tang", "Michael R. Lyu"], "title": "SlideCoder: Layout-aware RAG-enhanced Hierarchical Slide Generation from Design", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Manual slide creation is labor-intensive and requires expert prior knowledge.\nExisting natural language-based LLM generation methods struggle to capture the\nvisual and structural nuances of slide designs. To address this, we formalize\nthe Reference Image to Slide Generation task and propose Slide2Code, the first\nbenchmark with difficulty-tiered samples based on a novel Slide Complexity\nMetric. We introduce SlideCoder, a layout-aware, retrieval-augmented framework\nfor generating editable slides from reference images. SlideCoder integrates a\nColor Gradient-based Segmentation algorithm and a Hierarchical\nRetrieval-Augmented Generation method to decompose complex tasks and enhance\ncode generation. We also release SlideMaster, a 7B open-source model fine-tuned\nwith improved reverse-engineered data. Experiments show that SlideCoder\noutperforms state-of-the-art baselines by up to 40.5 points, demonstrating\nstrong performance across layout fidelity, execution accuracy, and visual\nconsistency. Our code is available at\nhttps://github.com/vinsontang1/SlideCoder.", "AI": {"tldr": "Slide2Code和SlideCoder解决了手动制作幻灯片的高成本和现有LLM方法在视觉与结构设计上的不足，通过新任务和框架提升幻灯片生成效果。", "motivation": "手动制作幻灯片费时费力且需专业知识，现有基于自然语言的LLM方法难以捕捉幻灯片设计的视觉与结构细节。", "method": "提出Slide2Code基准和SlideCoder框架，结合颜色梯度分割算法和分层检索增强生成方法，优化代码生成。", "result": "SlideCoder在布局保真度、执行准确性和视觉一致性上优于现有方法，提升达40.5分。", "conclusion": "SlideCoder通过创新框架和算法显著提升了幻灯片生成的性能，并开源了SlideMaster模型。"}}
{"id": "2506.07449", "pdf": "https://arxiv.org/pdf/2506.07449", "abs": "https://arxiv.org/abs/2506.07449", "authors": ["Vahid Azizi", "Fatemeh Koochaki"], "title": "LlamaRec-LKG-RAG: A Single-Pass, Learnable Knowledge Graph-RAG Framework for LLM-Based Ranking", "categories": ["cs.IR", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent advances in Large Language Models (LLMs) have driven their adoption in\nrecommender systems through Retrieval-Augmented Generation (RAG) frameworks.\nHowever, existing RAG approaches predominantly rely on flat, similarity-based\nretrieval that fails to leverage the rich relational structure inherent in\nuser-item interactions. We introduce LlamaRec-LKG-RAG, a novel single-pass,\nend-to-end trainable framework that integrates personalized knowledge graph\ncontext into LLM-based recommendation ranking. Our approach extends the\nLlamaRec architecture by incorporating a lightweight user preference module\nthat dynamically identifies salient relation paths within a heterogeneous\nknowledge graph constructed from user behavior and item metadata. These\npersonalized subgraphs are seamlessly integrated into prompts for a fine-tuned\nLlama-2 model, enabling efficient and interpretable recommendations through a\nunified inference step. Comprehensive experiments on ML-100K and Amazon Beauty\ndatasets demonstrate consistent and significant improvements over LlamaRec\nacross key ranking metrics (MRR, NDCG, Recall). LlamaRec-LKG-RAG demonstrates\nthe critical value of structured reasoning in LLM-based recommendations and\nestablishes a foundation for scalable, knowledge-aware personalization in\nnext-generation recommender systems. Code is available\nat~\\href{https://github.com/VahidAz/LlamaRec-LKG-RAG}{repository}.", "AI": {"tldr": "LlamaRec-LKG-RAG是一种新型的端到端可训练框架，通过整合个性化知识图谱上下文到基于LLM的推荐排序中，显著提升了推荐系统的性能。", "motivation": "现有RAG方法主要依赖扁平化的相似性检索，未能充分利用用户-物品交互中的丰富关系结构。", "method": "扩展LlamaRec架构，引入轻量级用户偏好模块，动态识别异构知识图谱中的关键关系路径，并将个性化子图整合到Llama-2模型的提示中。", "result": "在ML-100K和Amazon Beauty数据集上，LlamaRec-LKG-RAG在MRR、NDCG和Recall等关键排序指标上显著优于LlamaRec。", "conclusion": "LlamaRec-LKG-RAG证明了结构化推理在基于LLM的推荐中的关键价值，为下一代推荐系统的可扩展、知识感知个性化奠定了基础。"}}
{"id": "2506.07966", "pdf": "https://arxiv.org/pdf/2506.07966", "abs": "https://arxiv.org/abs/2506.07966", "authors": ["Ziyang Gong", "Wenhao Li", "Oliver Ma", "Songyuan Li", "Jiayi Ji", "Xue Yang", "Gen Luo", "Junchi Yan", "Rongrong Ji"], "title": "SpaCE-10: A Comprehensive Benchmark for Multimodal Large Language Models in Compositional Spatial Intelligence", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable progress in\nvarious multimodal tasks. To pursue higher intelligence in space, MLLMs require\nintegrating multiple atomic spatial capabilities to handle complex and dynamic\ntasks. However, existing benchmarks struggle to comprehensively evaluate the\nspatial intelligence of common MLLMs from the atomic level to the compositional\nlevel. To fill this gap, we present SpaCE-10, a comprehensive benchmark for\ncompositional spatial evaluations. In SpaCE-10, we define 10 atomic spatial\ncapabilities, which are combined to form 8 compositional capabilities. Based on\nthese definitions, we propose a novel hierarchical annotation pipeline to\ngenerate high-quality and diverse question-answer (QA) pairs. With over 150+\nhours of human expert effort, we obtain over 5k QA pairs for 811 real indoor\nscenes in SpaCE-10, which covers various evaluation settings like point cloud\ninput and multi-choice QA. We conduct an extensive evaluation of common MLLMs\non SpaCE-10 and find that even the most advanced MLLM still lags behind humans\nby large margins. Through our careful study, we also draw several significant\nfindings that benefit the MLLM community. For example, we reveal that the\nshortcoming of counting capability greatly limits the compositional spatial\ncapabilities of existing MLLMs. The evaluation code and benchmark datasets are\navailable at https://github.com/Cuzyoung/SpaCE-10.", "AI": {"tldr": "SpaCE-10是一个用于评估多模态大语言模型（MLLMs）空间智能的综合基准，涵盖10种原子空间能力和8种组合能力，通过5k+ QA对和811个室内场景进行评估。", "motivation": "现有基准难以全面评估MLLMs从原子到组合层面的空间智能，因此需要SpaCE-10填补这一空白。", "method": "定义了10种原子空间能力和8种组合能力，采用分层标注流程生成高质量QA对，并评估常见MLLMs。", "result": "最先进的MLLM仍与人类表现有显著差距，计数能力的不足尤其限制了其组合空间能力。", "conclusion": "SpaCE-10为MLLM社区提供了重要发现和评估工具，揭示了现有模型的局限性。"}}
{"id": "2506.07452", "pdf": "https://arxiv.org/pdf/2506.07452", "abs": "https://arxiv.org/abs/2506.07452", "authors": ["Yuxin Xiao", "Sana Tonekaboni", "Walter Gerych", "Vinith Suriyakumar", "Marzyeh Ghassemi"], "title": "When Style Breaks Safety: Defending Language Models Against Superficial Style Alignment", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "Large language models (LLMs) can be prompted with specific styles (e.g.,\nformatting responses as lists), including in jailbreak queries. Although these\nstyle patterns are semantically unrelated to the malicious intents behind\njailbreak queries, their safety impact remains unclear. In this work, we seek\nto understand whether style patterns compromise LLM safety, how superficial\nstyle alignment increases model vulnerability, and how best to mitigate these\nrisks during alignment. We evaluate 32 LLMs across seven jailbreak benchmarks,\nand find that malicious queries with style patterns inflate the attack success\nrate (ASR) for nearly all models. Notably, ASR inflation correlates with both\nthe length of style patterns and the relative attention an LLM exhibits on\nthem. We then investigate superficial style alignment, and find that\nfine-tuning with specific styles makes LLMs more vulnerable to jailbreaks of\nthose same styles. Finally, we propose SafeStyle, a defense strategy that\nincorporates a small amount of safety training data augmented to match the\ndistribution of style patterns in the fine-tuning data. Across three LLMs and\nfive fine-tuning style settings, SafeStyle consistently outperforms baselines\nin maintaining LLM safety.", "AI": {"tldr": "研究发现，特定风格提示会显著增加大型语言模型（LLM）的越狱攻击成功率（ASR），且风格长度与模型对其的关注度相关。风格微调会加剧模型对相同风格越狱的脆弱性。提出的SafeStyle防御策略能有效提升模型安全性。", "motivation": "探讨风格提示对LLM安全性的影响，以及如何通过风格对齐和防御策略降低风险。", "method": "评估32个LLM在7个越狱基准上的表现，分析风格模式对ASR的影响，并提出SafeStyle防御策略。", "result": "风格提示显著提高ASR，风格微调增加模型脆弱性；SafeStrategy在多种设置下优于基线。", "conclusion": "风格提示威胁LLM安全，SafeStyle是有效的防御方法。"}}
{"id": "2506.07971", "pdf": "https://arxiv.org/pdf/2506.07971", "abs": "https://arxiv.org/abs/2506.07971", "authors": ["Jiahao Meng", "Shuyang Sun", "Yue Tan", "Lu Qi", "Yunhai Tong", "Xiangtai Li", "Longyin Wen"], "title": "CyberV: Cybernetics for Test-time Scaling in Video Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Current Multimodal Large Language Models (MLLMs) may struggle with\nunderstanding long or complex videos due to computational demands at test time,\nlack of robustness, and limited accuracy, primarily stemming from their\nfeed-forward processing nature. These limitations could be more severe for\nmodels with fewer parameters. To address these limitations, we propose a novel\nframework inspired by cybernetic principles, redesigning video MLLMs as\nadaptive systems capable of self-monitoring, self-correction, and dynamic\nresource allocation during inference. Our approach, CyberV, introduces a\ncybernetic loop consisting of an MLLM Inference System, a Sensor, and a\nController. Specifically, the sensor monitors forward processes of the MLLM and\ncollects intermediate interpretations, such as attention drift, then the\ncontroller determines when and how to trigger self-correction and generate\nfeedback to guide the next round. This test-time adaptive scaling framework\nenhances frozen MLLMs without requiring retraining or additional components.\nExperiments demonstrate significant improvements: CyberV boosts Qwen2.5-VL-7B\nby 8.3% and InternVL3-8B by 5.5% on VideoMMMU, surpassing the competitive\nproprietary model GPT-4o. When applied to Qwen2.5-VL-72B, it yields a 10.0%\nimprovement, achieving performance even comparable to human experts.\nFurthermore, our method demonstrates consistent gains on general-purpose\nbenchmarks, such as VideoMME and WorldSense, highlighting its effectiveness and\ngeneralization capabilities in making MLLMs more robust and accurate for\ndynamic video understanding. The code is released at\nhttps://github.com/marinero4972/CyberV.", "AI": {"tldr": "论文提出了一种名为CyberV的新框架，通过引入控制论原理，改进多模态大语言模型（MLLMs）在视频理解中的性能，实现自适应推理。", "motivation": "当前MLLMs在处理长或复杂视频时存在计算需求高、鲁棒性差和准确性不足的问题，尤其是参数较少的模型表现更差。", "method": "CyberV框架包含一个控制循环，由MLLM推理系统、传感器和控制器组成，实现自监控、自校正和动态资源分配。", "result": "实验显示CyberV显著提升了多个模型的性能，如Qwen2.5-VL-7B提升8.3%，InternVL3-8B提升5.5%，甚至超越GPT-4o。", "conclusion": "CyberV通过自适应推理显著提升了MLLMs的鲁棒性和准确性，适用于动态视频理解，并展示了良好的泛化能力。"}}
{"id": "2506.06730", "pdf": "https://arxiv.org/pdf/2506.06730", "abs": "https://arxiv.org/abs/2506.06730", "authors": ["Rabah Rahal", "Abdelaziz Amara Korba", "Yacine Ghamri-Doudane"], "title": "Fuse and Federate: Enhancing EV Charging Station Security with Multimodal Fusion and Federated Learning", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The rapid global adoption of electric vehicles (EVs) has established electric\nvehicle supply equipment (EVSE) as a critical component of smart grid\ninfrastructure. While essential for ensuring reliable energy delivery and\naccessibility, EVSE systems face significant cybersecurity challenges,\nincluding network reconnaissance, backdoor intrusions, and distributed\ndenial-of-service (DDoS) attacks. These emerging threats, driven by the\ninterconnected and autonomous nature of EVSE, require innovative and adaptive\nsecurity mechanisms that go beyond traditional intrusion detection systems\n(IDS). Existing approaches, whether network-based or host-based, often fail to\ndetect sophisticated and targeted attacks specifically crafted to exploit new\nvulnerabilities in EVSE infrastructure. This paper proposes a novel intrusion\ndetection framework that leverages multimodal data sources, including network\ntraffic and kernel events, to identify complex attack patterns. The framework\nemploys a distributed learning approach, enabling collaborative intelligence\nacross EVSE stations while preserving data privacy through federated learning.\nExperimental results demonstrate that the proposed framework outperforms\nexisting solutions, achieving a detection rate above 98% and a precision rate\nexceeding 97% in decentralized environments. This solution addresses the\nevolving challenges of EVSE security, offering a scalable and privacypreserving\nresponse to advanced cyber threats", "AI": {"tldr": "本文提出了一种新型入侵检测框架，利用多模态数据源和分布式学习，显著提升了电动汽车充电设施（EVSE）的网络安全性能。", "motivation": "电动汽车充电设施（EVSE）面临日益复杂的网络安全威胁，传统入侵检测系统（IDS）难以应对，亟需创新解决方案。", "method": "结合网络流量和内核事件等多模态数据，采用分布式学习和联邦学习技术，构建协作式入侵检测框架。", "result": "实验显示，该框架在去中心化环境中检测率超过98%，精确率超过97%，优于现有方案。", "conclusion": "该框架为EVSE安全提供了可扩展且隐私保护的解决方案，有效应对高级网络威胁。"}}
{"id": "2506.07977", "pdf": "https://arxiv.org/pdf/2506.07977", "abs": "https://arxiv.org/abs/2506.07977", "authors": ["Jingjing Chang", "Yixiao Fang", "Peng Xing", "Shuhan Wu", "Wei Cheng", "Rui Wang", "Xianfang Zeng", "Gang Yu", "Hai-Bao Chen"], "title": "OneIG-Bench: Omni-dimensional Nuanced Evaluation for Image Generation", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image (T2I) models have garnered significant attention for generating\nhigh-quality images aligned with text prompts. However, rapid T2I model\nadvancements reveal limitations in early benchmarks, lacking comprehensive\nevaluations, for example, the evaluation on reasoning, text rendering and\nstyle. Notably, recent state-of-the-art models, with their rich knowledge\nmodeling capabilities, show promising results on the image generation problems\nrequiring strong reasoning ability, yet existing evaluation systems have not\nadequately addressed this frontier. To systematically address these gaps, we\nintroduce OneIG-Bench, a meticulously designed comprehensive benchmark\nframework for fine-grained evaluation of T2I models across multiple dimensions,\nincluding prompt-image alignment, text rendering precision, reasoning-generated\ncontent, stylization, and diversity. By structuring the evaluation, this\nbenchmark enables in-depth analysis of model performance, helping researchers\nand practitioners pinpoint strengths and bottlenecks in the full pipeline of\nimage generation. Specifically, OneIG-Bench enables flexible evaluation by\nallowing users to focus on a particular evaluation subset. Instead of\ngenerating images for the entire set of prompts, users can generate images only\nfor the prompts associated with the selected dimension and complete the\ncorresponding evaluation accordingly. Our codebase and dataset are now publicly\navailable to facilitate reproducible evaluation studies and cross-model\ncomparisons within the T2I research community.", "AI": {"tldr": "OneIG-Bench是一个全面的文本到图像（T2I）模型评估框架，旨在解决现有基准在推理、文本渲染和风格等方面的不足，提供多维度细粒度评估。", "motivation": "现有T2I模型评估基准缺乏对推理能力、文本渲染和风格等维度的全面评估，无法满足最新模型的需求。", "method": "提出OneIG-Bench框架，支持多维度（如提示-图像对齐、文本渲染精度、推理生成内容等）的灵活评估，用户可针对特定维度生成图像并完成评估。", "result": "OneIG-Bench提供了公开的代码库和数据集，支持可重复的评估研究和跨模型比较。", "conclusion": "OneIG-Bench填补了T2I模型评估的空白，为研究者和从业者提供了深入分析模型性能的工具。"}}
{"id": "2506.06732", "pdf": "https://arxiv.org/pdf/2506.06732", "abs": "https://arxiv.org/abs/2506.06732", "authors": ["Woongjib Choi", "Byeong Hyeon Kim", "Hyungseob Lim", "Inseon Jang", "Hong-Goo Kang"], "title": "Neural Spectral Band Generation for Audio Coding", "categories": ["eess.AS", "cs.AI", "eess.SP"], "comment": "Accepted to Interspeech 2025", "summary": "Audio bandwidth extension is the task of reconstructing missing high\nfrequency components of bandwidth-limited audio signals, where bandwidth\nlimitation is a common issue for audio signals due to several reasons,\nincluding channel capacity and data constraints. While conventional spectral\nband replication is a well-established parametric approach to audio bandwidth\nextension, the SBR usually entails coarse feature extraction and reconstruction\ntechniques, which leads to limitations when processing various types of audio\nsignals. In parallel, numerous deep neural network-based audio bandwidth\nextension methods have been proposed. These DNN-based methods are usually\nreferred to as blind BWE, as these methods do not rely on prior information\nextracted from original signals, and only utilize given low frequency band\nsignals to estimate missing high frequency components. In order to replace\nconventional SBR with DNNs, simply adopting existing DNN-based methodologies\nresults in suboptimal performance due to the blindness of these methods. My\nproposed research suggests a new approach to parametric non-blind bandwidth\nextension, as DNN-based side information extraction and DNN-based bandwidth\nextension are performed only at the front and end of the audio coding pipeline.", "AI": {"tldr": "论文提出了一种新的参数化非盲带宽扩展方法，结合DNN技术以改进传统SBR的局限性。", "motivation": "传统SBR方法在处理多种音频信号时存在局限性，而现有的DNN盲带宽扩展方法因缺乏先验信息导致性能不佳。", "method": "提出了一种参数化非盲带宽扩展方法，在音频编码管道的首尾分别使用DNN进行侧信息提取和带宽扩展。", "result": "该方法有望克服传统SBR和盲DNN方法的不足，提升带宽扩展性能。", "conclusion": "结合DNN的参数化非盲带宽扩展方法为音频信号处理提供了新的解决方案。"}}
{"id": "2506.07468", "pdf": "https://arxiv.org/pdf/2506.07468", "abs": "https://arxiv.org/abs/2506.07468", "authors": ["Mickel Liu", "Liwei Jiang", "Yancheng Liang", "Simon Shaolei Du", "Yejin Choi", "Tim Althoff", "Natasha Jaques"], "title": "Chasing Moving Targets with Online Self-Play Reinforcement Learning for Safer Language Models", "categories": ["cs.LG", "cs.CL", "cs.MA"], "comment": null, "summary": "Conventional language model (LM) safety alignment relies on a reactive,\ndisjoint procedure: attackers exploit a static model, followed by defensive\nfine-tuning to patch exposed vulnerabilities. This sequential approach creates\na mismatch -- attackers overfit to obsolete defenses, while defenders\nperpetually lag behind emerging threats. To address this, we propose\nSelf-RedTeam, an online self-play reinforcement learning algorithm where an\nattacker and defender agent co-evolve through continuous interaction. We cast\nsafety alignment as a two-player zero-sum game, where a single model alternates\nbetween attacker and defender roles -- generating adversarial prompts and\nsafeguarding against them -- while a reward LM adjudicates outcomes. This\nenables dynamic co-adaptation. Grounded in the game-theoretic framework of\nzero-sum games, we establish a theoretical safety guarantee which motivates the\ndesign of our method: if self-play converges to a Nash Equilibrium, the\ndefender will reliably produce safe responses to any adversarial input.\nEmpirically, Self-RedTeam uncovers more diverse attacks (+21.8% SBERT) compared\nto attackers trained against static defenders and achieves higher robustness on\nsafety benchmarks (e.g., +65.5% on WildJailBreak) than defenders trained\nagainst static attackers. We further propose hidden Chain-of-Thought, allowing\nagents to plan privately, which boosts adversarial diversity and reduces\nover-refusals. Our results motivate a shift from reactive patching to proactive\nco-evolution in LM safety training, enabling scalable, autonomous, and robust\nself-improvement of LMs via multi-agent reinforcement learning (MARL).", "AI": {"tldr": "Self-RedTeam是一种在线自博弈强化学习算法，通过攻击者和防御者的协同进化动态提升语言模型的安全性。", "motivation": "传统语言模型安全对齐是反应式的，攻击者和防御者脱节，导致防御滞后于威胁。Self-RedTeam旨在通过动态协同进化解决这一问题。", "method": "将安全对齐建模为零和博弈，攻击者和防御者角色交替，通过奖励模型裁决结果，实现动态协同适应。", "result": "Self-RedTeam发现更多样化的攻击（+21.8% SBERT），并在安全基准测试中表现更优（如WildJailBreak +65.5%）。", "conclusion": "研究提倡从被动修补转向主动协同进化，通过多智能体强化学习实现语言模型的自主、鲁棒自我提升。"}}
{"id": "2506.07981", "pdf": "https://arxiv.org/pdf/2506.07981", "abs": "https://arxiv.org/abs/2506.07981", "authors": ["Dmitrii Vorobev", "Artem Prosvetov", "Karim Elhadji Daou"], "title": "Real-time Localization of a Soccer Ball from a Single Camera", "categories": ["cs.CV", "cs.LG"], "comment": "13 pages, 4 figures", "summary": "We propose a computationally efficient method for real-time three-dimensional\nfootball trajectory reconstruction from a single broadcast camera. In contrast\nto previous work, our approach introduces a multi-mode state model with $W$\ndiscrete modes to significantly accelerate optimization while preserving\ncentimeter-level accuracy -- even in cases of severe occlusion, motion blur,\nand complex backgrounds. The system operates on standard CPUs and achieves low\nlatency suitable for live broadcast settings. Extensive evaluation on a\nproprietary dataset of 6K-resolution Russian Premier League matches\ndemonstrates performance comparable to multi-camera systems, without the need\nfor specialized or costly infrastructure. This work provides a practical method\nfor accessible and accurate 3D ball tracking in professional football\nenvironments.", "AI": {"tldr": "提出了一种基于单摄像头的实时三维足球轨迹重建方法，通过多模态状态模型显著提升优化速度，同时保持厘米级精度。", "motivation": "解决现有方法在遮挡、运动模糊和复杂背景下的性能不足问题，提供低成本、高效的解决方案。", "method": "引入多模态状态模型（$W$离散模态），在标准CPU上实现低延迟优化。", "result": "在6K分辨率俄罗斯超级联赛数据集上验证，性能媲美多摄像头系统。", "conclusion": "为专业足球环境提供了一种实用、低成本且高精度的三维球体追踪方法。"}}
{"id": "2506.06735", "pdf": "https://arxiv.org/pdf/2506.06735", "abs": "https://arxiv.org/abs/2506.06735", "authors": ["Mesut Ozdag"], "title": "Ai-Driven Vulnerability Analysis in Smart Contracts: Trends, Challenges and Future Directions", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Smart contracts, integral to blockchain ecosystems, enable decentralized\napplications to execute predefined operations without intermediaries. Their\nability to enforce trustless interactions has made them a core component of\nplatforms such as Ethereum. Vulnerabilities such as numerical overflows,\nreentrancy attacks, and improper access permissions have led to the loss of\nmillions of dollars throughout the blockchain and smart contract sector.\nTraditional smart contract auditing techniques such as manual code reviews and\nformal verification face limitations in scalability, automation, and\nadaptability to evolving development patterns. As a result, AI-based solutions\nhave emerged as a promising alternative, offering the ability to learn complex\npatterns, detect subtle flaws, and provide scalable security assurances. This\npaper examines novel AI-driven techniques for vulnerability detection in smart\ncontracts, focusing on machine learning, deep learning, graph neural networks,\nand transformer-based models. This paper analyzes how each technique represents\ncode, processes semantic information, and responds to real world vulnerability\nclasses. We also compare their strengths and weaknesses in terms of accuracy,\ninterpretability, computational overhead, and real time applicability. Lastly,\nit highlights open challenges and future opportunities for advancing this\ndomain.", "AI": {"tldr": "本文探讨了AI驱动的智能合约漏洞检测技术，包括机器学习、深度学习、图神经网络和基于Transformer的模型，分析了它们的优缺点及未来挑战。", "motivation": "智能合约在区块链生态中至关重要，但其漏洞导致巨大损失。传统审计方法在可扩展性和自动化方面存在局限，AI技术为解决这些问题提供了新途径。", "method": "研究采用机器学习、深度学习、图神经网络和Transformer模型，分析它们在代码表示、语义信息处理和漏洞检测中的应用。", "result": "比较了不同技术的准确性、可解释性、计算开销和实时适用性，展示了AI在智能合约安全中的潜力。", "conclusion": "AI驱动的漏洞检测技术前景广阔，但仍需解决开放挑战，如模型适应性和实时性，以进一步推动该领域发展。"}}
{"id": "2506.07501", "pdf": "https://arxiv.org/pdf/2506.07501", "abs": "https://arxiv.org/abs/2506.07501", "authors": ["Libo Wang"], "title": "Graph-of-Causal Evolution: Challenging Chain-of-Model for Reasoning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "The relevant code has been uploaded to the publicly available GitHub\n  repository. The link is:\n  https://github.com/brucewang123456789/GeniusTrail/tree/main/GoCE", "summary": "In view of the problem that each subchain in the chain-of-model (CoM) relies\nonly on the information of the previous subchain and may lose long-range\ndependencies due to the causal mask blocking the global context flow between\nmulti-level subchains, this work proposes a graph of causal evolution (GoCE).\nIts core principle is to map the implicit token representation into a\ndifferentiable and sparse causal adjacency matrix, then permeate causal\nconstraints through each layer of calculation using causal-masked attention and\ncausal-MoE. By combining intervention consistency loss test and self-evolution\ngate, the dynamic balance between causal structure learning and adaptive\nupdating of transformer architecture is realized. The researcher built\nexperimental environments in sandboxes built with Claude Sonnet 4,\no4-mini-high, and DeepSeek R1 respectively with the transformer variant\narchitecture introduced in GoCE. It is evaluated on publicly available datasets\nincluding CLUTRR, CLADDER, EX-FEVER, and CausalQA and compared with the\nbaseline LLMs. The finding proves that GoCE strengthens the transformer's\nability to capture long-range causal dependencies, while the ability to\nself-evolve is improved. It not only surpasses the design of CoM in terms of\ndesign principles, but also provides experience for future research on causal\nlearning and continuous adaptive improvement.", "AI": {"tldr": "论文提出了一种图因果演化（GoCE）方法，通过稀疏因果邻接矩阵和因果约束传播，解决了链式模型（CoM）中长距离依赖丢失的问题，并在实验中验证了其优越性。", "motivation": "解决链式模型（CoM）因因果掩码导致全局上下文信息丢失的问题，提升长距离因果依赖的捕捉能力。", "method": "将隐式令牌表示映射为可微分稀疏因果邻接矩阵，结合因果掩码注意力和因果-MoE，通过干预一致性损失和自我演化门实现动态平衡。", "result": "实验表明GoCE在多个公开数据集上优于基线模型，显著提升了长距离因果依赖的捕捉能力和自我演化能力。", "conclusion": "GoCE不仅在设计上超越了CoM，还为因果学习和持续自适应改进提供了新思路。"}}
{"id": "2506.07984", "pdf": "https://arxiv.org/pdf/2506.07984", "abs": "https://arxiv.org/abs/2506.07984", "authors": ["Mingquan Lin", "Gregory Holste", "Song Wang", "Yiliang Zhou", "Yishu Wei", "Imon Banerjee", "Pengyi Chen", "Tianjie Dai", "Yuexi Du", "Nicha C. Dvornek", "Yuyan Ge", "Zuowei Guo", "Shouhei Hanaoka", "Dongkyun Kim", "Pablo Messina", "Yang Lu", "Denis Parra", "Donghyun Son", "Álvaro Soto", "Aisha Urooj", "René Vidal", "Yosuke Yamagishi", "Zefan Yang", "Ruichi Zhang", "Yang Zhou", "Leo Anthony Celi", "Ronald M. Summers", "Zhiyong Lu", "Hao Chen", "Adam Flanders", "George Shih", "Zhangyang Wang", "Yifan Peng"], "title": "CXR-LT 2024: A MICCAI challenge on long-tailed, multi-label, and zero-shot disease classification from chest X-ray", "categories": ["cs.CV", "cs.LG"], "comment": "17 pages, 3 figures", "summary": "The CXR-LT series is a community-driven initiative designed to enhance lung\ndisease classification using chest X-rays (CXR). It tackles challenges in open\nlong-tailed lung disease classification and enhances the measurability of\nstate-of-the-art techniques. The first event, CXR-LT 2023, aimed to achieve\nthese goals by providing high-quality benchmark CXR data for model development\nand conducting comprehensive evaluations to identify ongoing issues impacting\nlung disease classification performance. Building on the success of CXR-LT\n2023, the CXR-LT 2024 expands the dataset to 377,110 chest X-rays (CXRs) and 45\ndisease labels, including 19 new rare disease findings. It also introduces a\nnew focus on zero-shot learning to address limitations identified in the\nprevious event. Specifically, CXR-LT 2024 features three tasks: (i) long-tailed\nclassification on a large, noisy test set, (ii) long-tailed classification on a\nmanually annotated \"gold standard\" subset, and (iii) zero-shot generalization\nto five previously unseen disease findings. This paper provides an overview of\nCXR-LT 2024, detailing the data curation process and consolidating\nstate-of-the-art solutions, including the use of multimodal models for rare\ndisease detection, advanced generative approaches to handle noisy labels, and\nzero-shot learning strategies for unseen diseases. Additionally, the expanded\ndataset enhances disease coverage to better represent real-world clinical\nsettings, offering a valuable resource for future research. By synthesizing the\ninsights and innovations of participating teams, we aim to advance the\ndevelopment of clinically realistic and generalizable diagnostic models for\nchest radiography.", "AI": {"tldr": "CXR-LT 2024是一个社区驱动的项目，旨在通过扩展数据集和改进技术（如零样本学习）来提升胸部X射线（CXR）的肺部疾病分类性能。", "motivation": "解决开放长尾肺部疾病分类的挑战，并提升现有技术的可测量性。", "method": "通过扩展数据集（377,110张CXR和45种疾病标签）、引入零样本学习任务，以及使用多模态模型和生成方法处理噪声标签。", "result": "提供了更全面的疾病覆盖和更真实的临床场景数据，推动了临床实用和泛化诊断模型的发展。", "conclusion": "CXR-LT 2024为未来研究提供了宝贵资源，并促进了肺部疾病分类技术的进步。"}}
{"id": "2506.07515", "pdf": "https://arxiv.org/pdf/2506.07515", "abs": "https://arxiv.org/abs/2506.07515", "authors": ["Asahi Sakuma", "Hiroaki Sato", "Ryuga Sugano", "Tadashi Kumano", "Yoshihiko Kawai", "Tetsuji Ogawa"], "title": "Speaker-Distinguishable CTC: Learning Speaker Distinction Using CTC for Multi-Talker Speech Recognition", "categories": ["eess.AS", "cs.CL", "cs.SD"], "comment": "Accepted at INTERSPEECH 2025", "summary": "This paper presents a novel framework for multi-talker automatic speech\nrecognition without the need for auxiliary information. Serialized Output\nTraining (SOT), a widely used approach, suffers from recognition errors due to\nspeaker assignment failures. Although incorporating auxiliary information, such\nas token-level timestamps, can improve recognition accuracy, extracting such\ninformation from natural conversational speech remains challenging. To address\nthis limitation, we propose Speaker-Distinguishable CTC (SD-CTC), an extension\nof CTC that jointly assigns a token and its corresponding speaker label to each\nframe. We further integrate SD-CTC into the SOT framework, enabling the SOT\nmodel to learn speaker distinction using only overlapping speech and\ntranscriptions. Experimental comparisons show that multi-task learning with\nSD-CTC and SOT reduces the error rate of the SOT model by 26% and achieves\nperformance comparable to state-of-the-art methods relying on auxiliary\ninformation.", "AI": {"tldr": "提出了一种无需辅助信息的多说话人自动语音识别框架，通过Speaker-Distinguishable CTC（SD-CTC）改进现有方法，显著降低错误率。", "motivation": "现有Serialized Output Training（SOT）方法因说话人分配错误导致识别问题，而依赖辅助信息（如时间戳）的提取在实际对话中困难。", "method": "扩展CTC为SD-CTC，联合分配帧的标记和说话人标签，并将其整合到SOT框架中，仅通过重叠语音和转录学习说话人区分。", "result": "实验表明，SD-CTC与SOT的多任务学习将SOT模型的错误率降低26%，性能接近依赖辅助信息的先进方法。", "conclusion": "SD-CTC有效解决了SOT的说话人分配问题，无需辅助信息即可实现高性能多说话人语音识别。"}}
{"id": "2506.07985", "pdf": "https://arxiv.org/pdf/2506.07985", "abs": "https://arxiv.org/abs/2506.07985", "authors": ["Tuomas Oikarinen", "Ge Yan", "Akshay Kulkarni", "Tsui-Wei Weng"], "title": "Rethinking Crowd-Sourced Evaluation of Neuron Explanations", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Interpreting individual neurons or directions in activations space is an\nimportant component of mechanistic interpretability. As such, many algorithms\nhave been proposed to automatically produce neuron explanations, but it is\noften not clear how reliable these explanations are, or which methods produce\nthe best explanations. This can be measured via crowd-sourced evaluations, but\nthey can often be noisy and expensive, leading to unreliable results. In this\npaper, we carefully analyze the evaluation pipeline and develop a\ncost-effective and highly accurate crowdsourced evaluation strategy. In\ncontrast to previous human studies that only rate whether the explanation\nmatches the most highly activating inputs, we estimate whether the explanation\ndescribes neuron activations across all inputs. To estimate this effectively,\nwe introduce a novel application of importance sampling to determine which\ninputs are the most valuable to show to raters, leading to around 30x cost\nreduction compared to uniform sampling. We also analyze the label noise present\nin crowd-sourced evaluations and propose a Bayesian method to aggregate\nmultiple ratings leading to a further ~5x reduction in number of ratings\nrequired for the same accuracy. Finally, we use these methods to conduct a\nlarge-scale study comparing the quality of neuron explanations produced by the\nmost popular methods for two different vision models.", "AI": {"tldr": "论文提出了一种高效且准确的众包评估策略，用于评估神经元解释的可靠性，通过重要性采样和贝叶斯方法显著降低了成本。", "motivation": "现有神经元解释评估方法存在噪声大、成本高的问题，需要一种更可靠且经济的评估策略。", "method": "引入重要性采样选择最有价值的输入样本，并提出贝叶斯方法聚合多个评分，显著减少所需评分数量。", "result": "实现了约30倍的成本降低和约5倍的评分数量减少，同时保持高准确性。", "conclusion": "该方法为大规模神经元解释评估提供了高效且可靠的解决方案，并在实验中验证了其有效性。"}}
{"id": "2506.06752", "pdf": "https://arxiv.org/pdf/2506.06752", "abs": "https://arxiv.org/abs/2506.06752", "authors": ["Anna B. Jakobsen", "Anders B. Clausen", "Jaco van de Pol", "Irfansha Shaik"], "title": "Depth-Optimal Quantum Layout Synthesis as SAT", "categories": ["quant-ph", "cs.AI"], "comment": "24 pages, 4 figures, 11 tables", "summary": "Quantum circuits consist of gates applied to qubits. Current quantum hardware\nplatforms impose connectivity restrictions on binary CX gates. Hence, Layout\nSynthesis is an important step to transpile quantum circuits before they can be\nexecuted. Since CX gates are noisy, it is important to reduce the CX count or\nCX depth of the mapped circuits.\n  We provide a new and efficient encoding of Quantum-circuit Layout Synthesis\nin SAT. Previous SAT encodings focused on gate count and CX-gate count. Our\nencoding instead guarantees that we find mapped circuits with minimal circuit\ndepth or minimal CX-gate depth. We use incremental SAT solving and parallel\nplans for an efficient encoding. This results in speedups of more than 10-100x\ncompared to OLSQ2, which guarantees depth-optimality. But minimizing depth\nstill takes more time than minimizing gate count with Q-Synth.\n  We correlate the noise reduction achieved by simulating circuits after\n(CX)-count and (CX)-depth reduction. We find that minimizing for CX-count\ncorrelates better with reducing noise than minimizing for CX-depth. However,\ntaking into account both CX-count and CX-depth provides the best noise\nreduction.", "AI": {"tldr": "论文提出了一种新的SAT编码方法，用于量子电路布局合成，重点优化电路深度或CX门深度，相比现有方法提速10-100倍。实验表明，同时优化CX门数量和深度能最好地降低噪声。", "motivation": "当前量子硬件对CX门的连接性有限制，且CX门噪声较大，因此需要优化电路布局以减少CX门数量或深度。", "method": "采用新的SAT编码方法，结合增量SAT求解和并行计划，优化电路深度或CX门深度。", "result": "相比OLSQ2，新方法提速10-100倍；优化CX门数量比优化深度更能降低噪声，但两者结合效果最佳。", "conclusion": "新方法在深度优化上显著提速，同时优化CX门数量和深度能最大程度降低噪声。"}}
{"id": "2506.07551", "pdf": "https://arxiv.org/pdf/2506.07551", "abs": "https://arxiv.org/abs/2506.07551", "authors": ["Mengsong Wu", "YaFei Wang", "Yidong Ming", "Yuqi An", "Yuwei Wan", "Wenliang Chen", "Binbin Lin", "Yuqiang Li", "Tong Xie", "Dongzhan Zhou"], "title": "ChemAgent: Enhancing LLMs for Chemistry and Materials Science through Tree-Search Based Tool Learning", "categories": ["cs.LG", "cs.AI", "cs.CE", "cs.CL"], "comment": "15 pages, 6 figures", "summary": "Large language models (LLMs) have recently demonstrated promising\ncapabilities in chemistry tasks while still facing challenges due to outdated\npretraining knowledge and the difficulty of incorporating specialized chemical\nexpertise. To address these issues, we propose an LLM-based agent that\nsynergistically integrates 137 external chemical tools created ranging from\nbasic information retrieval to complex reaction predictions, and a dataset\ncuration pipeline to generate the dataset ChemToolBench that facilitates both\neffective tool selection and precise parameter filling during fine-tuning and\nevaluation. We introduce a Hierarchical Evolutionary Monte Carlo Tree Search\n(HE-MCTS) framework, enabling independent optimization of tool planning and\nexecution. By leveraging self-generated data, our approach supports step-level\nfine-tuning (FT) of the policy model and training task-adaptive PRM and ORM\nthat surpass GPT-4o. Experimental evaluations demonstrate that our approach\nsignificantly improves performance in Chemistry QA and discovery tasks,\noffering a robust solution to integrate specialized tools with LLMs for\nadvanced chemical applications. All datasets and code are available at\nhttps://github.com/AI4Chem/ChemistryAgent .", "AI": {"tldr": "提出了一种基于LLM的智能体，整合外部化学工具和数据集ChemToolBench，通过HE-MCTS框架优化工具规划和执行，显著提升化学任务性能。", "motivation": "解决LLMs在化学任务中因知识过时和缺乏专业知识而面临的挑战。", "method": "结合137种外部化学工具，使用HE-MCTS框架优化工具选择和参数填充，并通过自生成数据微调模型。", "result": "在化学问答和发现任务中表现显著优于GPT-4o。", "conclusion": "为LLMs与专业化学工具的结合提供了高效解决方案，代码和数据集已开源。"}}
{"id": "2506.07986", "pdf": "https://arxiv.org/pdf/2506.07986", "abs": "https://arxiv.org/abs/2506.07986", "authors": ["Zhengyao Lv", "Tianlin Pan", "Chenyang Si", "Zhaoxi Chen", "Wangmeng Zuo", "Ziwei Liu", "Kwan-Yee K. Wong"], "title": "Rethinking Cross-Modal Interaction in Multimodal Diffusion Transformers", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Diffusion Transformers (MM-DiTs) have achieved remarkable progress\nin text-driven visual generation. However, even state-of-the-art MM-DiT models\nlike FLUX struggle with achieving precise alignment between text prompts and\ngenerated content. We identify two key issues in the attention mechanism of\nMM-DiT, namely 1) the suppression of cross-modal attention due to token\nimbalance between visual and textual modalities and 2) the lack of\ntimestep-aware attention weighting, which hinder the alignment. To address\nthese issues, we propose \\textbf{Temperature-Adjusted Cross-modal Attention\n(TACA)}, a parameter-efficient method that dynamically rebalances multimodal\ninteractions through temperature scaling and timestep-dependent adjustment.\nWhen combined with LoRA fine-tuning, TACA significantly enhances text-image\nalignment on the T2I-CompBench benchmark with minimal computational overhead.\nWe tested TACA on state-of-the-art models like FLUX and SD3.5, demonstrating\nits ability to improve image-text alignment in terms of object appearance,\nattribute binding, and spatial relationships. Our findings highlight the\nimportance of balancing cross-modal attention in improving semantic fidelity in\ntext-to-image diffusion models. Our codes are publicly available at\n\\href{https://github.com/Vchitect/TACA}", "AI": {"tldr": "论文提出了一种名为TACA的方法，通过动态调整跨模态注意力机制，解决了MM-DiT模型中文本与图像对齐的问题。", "motivation": "现有的MM-DiT模型（如FLUX）在文本驱动的视觉生成中表现优异，但在文本提示与生成内容之间的精确对齐方面存在不足。", "method": "提出了Temperature-Adjusted Cross-modal Attention (TACA)，通过温度缩放和时间步依赖的调整，动态平衡多模态交互。结合LoRA微调，显著提升了文本-图像对齐效果。", "result": "在T2I-CompBench基准测试中，TACA显著提升了文本-图像对齐，改进了对象外观、属性绑定和空间关系。", "conclusion": "TACA通过平衡跨模态注意力，显著提升了文本到图像扩散模型的语义保真度。"}}
{"id": "2506.06782", "pdf": "https://arxiv.org/pdf/2506.06782", "abs": "https://arxiv.org/abs/2506.06782", "authors": ["Qinting Jiang", "Chuyang Ye", "Dongyan Wei", "Bingli Wang", "Yuan Xue", "Jingyan Jiang", "Zhi Wang"], "title": "Feature-Based Instance Neighbor Discovery: Advanced Stable Test-Time Adaptation in Dynamic World", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Despite progress, deep neural networks still suffer performance declines\nunder distribution shifts between training and test domains, leading to a\nsubstantial decrease in Quality of Experience (QoE) for applications. Existing\ntest-time adaptation (TTA) methods are challenged by dynamic, multiple test\ndistributions within batches. We observe that feature distributions across\ndifferent domains inherently cluster into distinct groups with varying means\nand variances. This divergence reveals a critical limitation of previous global\nnormalization strategies in TTA, which inevitably distort the original data\ncharacteristics. Based on this insight, we propose Feature-based Instance\nNeighbor Discovery (FIND), which comprises three key components: Layer-wise\nFeature Disentanglement (LFD), Feature Aware Batch Normalization (FABN) and\nSelective FABN (S-FABN). LFD stably captures features with similar\ndistributions at each layer by constructing graph structures. While FABN\noptimally combines source statistics with test-time distribution specific\nstatistics for robust feature representation. Finally, S-FABN determines which\nlayers require feature partitioning and which can remain unified, thereby\nenhancing inference efficiency. Extensive experiments demonstrate that FIND\nsignificantly outperforms existing methods, achieving a 30\\% accuracy\nimprovement in dynamic scenarios while maintaining computational efficiency.", "AI": {"tldr": "论文提出了一种名为FIND的新方法，通过特征聚类和动态归一化策略，显著提升了深度神经网络在动态分布变化下的性能。", "motivation": "深度神经网络在训练与测试分布变化时性能下降，现有测试时适应方法难以应对动态多分布场景。", "method": "FIND包含三个组件：层间特征解耦（LFD）、特征感知批量归一化（FABN）和选择性FABN（S-FABN），通过特征聚类和动态归一化优化性能。", "result": "FIND在动态场景中实现了30%的准确率提升，同时保持计算效率。", "conclusion": "FIND通过特征聚类和动态归一化策略，有效解决了动态分布变化下的性能问题，显著优于现有方法。"}}
{"id": "2506.07992", "pdf": "https://arxiv.org/pdf/2506.07992", "abs": "https://arxiv.org/abs/2506.07992", "authors": ["Haoguang Lu", "Jiacheng Chen", "Zhenguo Yang", "Aurele Tohokantche Gnanha", "Fu Lee Wang", "Li Qing", "Xudong Mao"], "title": "PairEdit: Learning Semantic Variations for Exemplar-based Image Editing", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in text-guided image editing have achieved notable\nsuccess by leveraging natural language prompts for fine-grained semantic\ncontrol. However, certain editing semantics are challenging to specify\nprecisely using textual descriptions alone. A practical alternative involves\nlearning editing semantics from paired source-target examples. Existing\nexemplar-based editing methods still rely on text prompts describing the change\nwithin paired examples or learning implicit text-based editing instructions. In\nthis paper, we introduce PairEdit, a novel visual editing method designed to\neffectively learn complex editing semantics from a limited number of image\npairs or even a single image pair, without using any textual guidance. We\npropose a target noise prediction that explicitly models semantic variations\nwithin paired images through a guidance direction term. Moreover, we introduce\na content-preserving noise schedule to facilitate more effective semantic\nlearning. We also propose optimizing distinct LoRAs to disentangle the learning\nof semantic variations from content. Extensive qualitative and quantitative\nevaluations demonstrate that PairEdit successfully learns intricate semantics\nwhile significantly improving content consistency compared to baseline methods.\nCode will be available at https://github.com/xudonmao/PairEdit.", "AI": {"tldr": "PairEdit是一种无需文本指导的视觉编辑方法，通过学习有限数量的图像对（甚至单对）来有效学习复杂编辑语义。", "motivation": "现有基于示例的编辑方法仍依赖文本提示或隐式文本指令，而某些编辑语义难以仅通过文本精确描述。", "method": "提出目标噪声预测和内容保留噪声调度，通过指导方向项显式建模语义变化，并优化LoRAs以分离语义学习和内容。", "result": "PairEdit成功学习复杂语义，并在内容一致性上显著优于基线方法。", "conclusion": "PairEdit为无需文本指导的视觉编辑提供了有效解决方案。"}}
{"id": "2506.06793", "pdf": "https://arxiv.org/pdf/2506.06793", "abs": "https://arxiv.org/abs/2506.06793", "authors": ["Zixuan Dong", "Yumi Omori", "Keith Ross"], "title": "Is Optimal Transport Necessary for Inverse Reinforcement Learning?", "categories": ["cs.LG", "cs.AI"], "comment": "19 pages, 10 tables", "summary": "Inverse Reinforcement Learning (IRL) aims to recover a reward function from\nexpert demonstrations. Recently, Optimal Transport (OT) methods have been\nsuccessfully deployed to align trajectories and infer rewards. While OT-based\nmethods have shown strong empirical results, they introduce algorithmic\ncomplexity, hyperparameter sensitivity, and require solving the OT optimization\nproblems. In this work, we challenge the necessity of OT in IRL by proposing\ntwo simple, heuristic alternatives: (1) Minimum-Distance Reward, which assigns\nrewards based on the nearest expert state regardless of temporal order; and (2)\nSegment-Matching Reward, which incorporates lightweight temporal alignment by\nmatching agent states to corresponding segments in the expert trajectory. These\nmethods avoid optimization, exhibit linear-time complexity, and are easy to\nimplement. Through extensive evaluations across 32 online and offline\nbenchmarks with three reinforcement learning algorithms, we show that our\nsimple rewards match or outperform recent OT-based approaches. Our findings\nsuggest that the core benefits of OT may arise from basic proximity alignment\nrather than its optimal coupling formulation, advocating for reevaluation of\ncomplexity in future IRL design.", "AI": {"tldr": "论文提出了两种简单的启发式方法替代基于最优传输（OT）的逆强化学习（IRL），并证明其性能与OT方法相当甚至更优。", "motivation": "挑战OT在IRL中的必要性，探索更简单、高效的替代方案。", "method": "提出两种方法：最小距离奖励和分段匹配奖励，避免复杂的OT优化。", "result": "在32个基准测试中，新方法表现优于或与OT方法相当。", "conclusion": "OT的核心优势可能源于基本对齐而非复杂耦合，未来IRL设计应重新评估复杂性。"}}
{"id": "2506.07996", "pdf": "https://arxiv.org/pdf/2506.07996", "abs": "https://arxiv.org/abs/2506.07996", "authors": ["Ming-Feng Li", "Xin Yang", "Fu-En Wang", "Hritam Basak", "Yuyin Sun", "Shreekant Gayaka", "Min Sun", "Cheng-Hao Kuo"], "title": "UA-Pose: Uncertainty-Aware 6D Object Pose Estimation and Online Object Completion with Partial References", "categories": ["cs.CV", "cs.RO"], "comment": "CVPR 2025", "summary": "6D object pose estimation has shown strong generalizability to novel objects.\nHowever, existing methods often require either a complete, well-reconstructed\n3D model or numerous reference images that fully cover the object. Estimating\n6D poses from partial references, which capture only fragments of an object's\nappearance and geometry, remains challenging. To address this, we propose\nUA-Pose, an uncertainty-aware approach for 6D object pose estimation and online\nobject completion specifically designed for partial references. We assume\naccess to either (1) a limited set of RGBD images with known poses or (2) a\nsingle 2D image. For the first case, we initialize a partial object 3D model\nbased on the provided images and poses, while for the second, we use\nimage-to-3D techniques to generate an initial object 3D model. Our method\nintegrates uncertainty into the incomplete 3D model, distinguishing between\nseen and unseen regions. This uncertainty enables confidence assessment in pose\nestimation and guides an uncertainty-aware sampling strategy for online object\ncompletion, enhancing robustness in pose estimation accuracy and improving\nobject completeness. We evaluate our method on the YCB-Video, YCBInEOAT, and\nHO3D datasets, including RGBD sequences of YCB objects manipulated by robots\nand human hands. Experimental results demonstrate significant performance\nimprovements over existing methods, particularly when object observations are\nincomplete or partially captured. Project page:\nhttps://minfenli.github.io/UA-Pose/", "AI": {"tldr": "UA-Pose是一种不确定性感知的6D物体姿态估计方法，针对部分参考数据设计，通过在线物体补全提升姿态估计的鲁棒性和准确性。", "motivation": "现有方法需要完整的3D模型或大量参考图像，而部分参考数据（如片段化的物体外观和几何）的6D姿态估计仍具挑战性。", "method": "提出UA-Pose方法，基于部分RGBD图像或单张2D图像生成初始3D模型，并引入不确定性区分已见和未见区域，指导在线补全和姿态估计。", "result": "在YCB-Video、YCBInEOAT和HO3D数据集上表现优于现有方法，尤其在物体观测不完整时效果显著。", "conclusion": "UA-Pose通过不确定性感知和在线补全，有效解决了部分参考数据下的6D姿态估计问题，提升了准确性和鲁棒性。"}}
{"id": "2506.07747", "pdf": "https://arxiv.org/pdf/2506.07747", "abs": "https://arxiv.org/abs/2506.07747", "authors": ["Adam Breuer"], "title": "E-LDA: Toward Interpretable LDA Topic Models with Strong Guarantees in Logarithmic Parallel Time", "categories": ["cs.LG", "cs.CL", "stat.ML"], "comment": "ICML 2025; Code available at: https://github.com/BreuerLabs/E- LDA", "summary": "In this paper, we provide the first practical algorithms with provable\nguarantees for the problem of inferring the topics assigned to each document in\nan LDA topic model. This is the primary inference problem for many applications\nof topic models in social science, data exploration, and causal inference\nsettings. We obtain this result by showing a novel non-gradient-based,\ncombinatorial approach to estimating topic models. This yields algorithms that\nconverge to near-optimal posterior probability in logarithmic parallel\ncomputation time (adaptivity) -- exponentially faster than any known LDA\nalgorithm. We also show that our approach can provide interpretability\nguarantees such that each learned topic is formally associated with a known\nkeyword. Finally, we show that unlike alternatives, our approach can maintain\nthe independence assumptions necessary to use the learned topic model for\ndownstream causal inference methods that allow researchers to study topics as\ntreatments. In terms of practical performance, our approach consistently\nreturns solutions of higher semantic quality than solutions from\nstate-of-the-art LDA algorithms, neural topic models, and LLM-based topic\nmodels across a diverse range of text datasets and evaluation parameters.", "AI": {"tldr": "本文提出了一种新颖的非梯度组合方法，用于推断LDA主题模型中每个文档的主题分配，实现了对数级并行计算时间的高效收敛，并提供了可解释性保证。", "motivation": "解决LDA主题模型在社会科学、数据探索和因果推断等应用中的主要推断问题，提供高效且可解释的解决方案。", "method": "采用非梯度组合方法估计主题模型，确保对数级并行计算时间收敛。", "result": "算法在语义质量上优于现有LDA、神经主题模型和基于LLM的主题模型，并支持因果推断。", "conclusion": "该方法在效率和可解释性上显著优于现有技术，适用于多种文本数据集和评估参数。"}}
{"id": "2506.07999", "pdf": "https://arxiv.org/pdf/2506.07999", "abs": "https://arxiv.org/abs/2506.07999", "authors": ["Junhao Chen", "Yulia Tsvetkov", "Xiaochuang Han"], "title": "MADFormer: Mixed Autoregressive and Diffusion Transformers for Continuous Image Generation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Recent progress in multimodal generation has increasingly combined\nautoregressive (AR) and diffusion-based approaches, leveraging their\ncomplementary strengths: AR models capture long-range dependencies and produce\nfluent, context-aware outputs, while diffusion models operate in continuous\nlatent spaces to refine high-fidelity visual details. However, existing hybrids\noften lack systematic guidance on how and why to allocate model capacity\nbetween these paradigms. In this work, we introduce MADFormer, a Mixed\nAutoregressive and Diffusion Transformer that serves as a testbed for analyzing\nAR-diffusion trade-offs. MADFormer partitions image generation into spatial\nblocks, using AR layers for one-pass global conditioning across blocks and\ndiffusion layers for iterative local refinement within each block. Through\ncontrolled experiments on FFHQ-1024 and ImageNet, we identify two key insights:\n(1) block-wise partitioning significantly improves performance on\nhigh-resolution images, and (2) vertically mixing AR and diffusion layers\nyields better quality-efficiency balances--improving FID by up to 75% under\nconstrained inference compute. Our findings offer practical design principles\nfor future hybrid generative models.", "AI": {"tldr": "MADFormer是一种混合自回归（AR）和扩散模型的Transformer，通过分区生成图像，结合AR的全局条件和扩散模型的局部细化，显著提升高分辨率图像生成性能。", "motivation": "现有混合模型缺乏系统指导，无法有效分配AR和扩散模型的能力。", "method": "MADFormer将图像生成分为空间块，AR层用于全局条件，扩散层用于局部细化。", "result": "实验表明，块分区显著提升高分辨率图像性能，混合层在质量和效率上取得更好平衡，FID提升达75%。", "conclusion": "MADFormer为未来混合生成模型提供了实用设计原则。"}}
{"id": "2506.07833", "pdf": "https://arxiv.org/pdf/2506.07833", "abs": "https://arxiv.org/abs/2506.07833", "authors": ["Michael K. Chen", "Xikun Zhang", "Jiaxing Huang", "Dacheng Tao"], "title": "Improving large language models with concept-aware fine-tuning", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models (LLMs) have become the cornerstone of modern AI.\nHowever, the existing paradigm of next-token prediction fundamentally limits\ntheir ability to form coherent, high-level concepts, making it a critical\nbarrier to human-like understanding and reasoning. Take the phrase \"ribonucleic\nacid\" as an example: an LLM will first decompose it into tokens, i.e.,\nartificial text fragments (\"rib\", \"on\", ...), then learn each token\nsequentially, rather than grasping the phrase as a unified, coherent semantic\nentity. This fragmented representation hinders deeper conceptual understanding\nand, ultimately, the development of truly intelligent systems. In response, we\nintroduce Concept-Aware Fine-Tuning (CAFT), a novel multi-token training method\nthat redefines how LLMs are fine-tuned. By enabling the learning of sequences\nthat span multiple tokens, this method fosters stronger concept-aware learning.\nOur experiments demonstrate significant improvements compared to conventional\nnext-token finetuning methods across diverse tasks, including traditional\napplications like text summarization and domain-specific ones like de novo\nprotein design. Multi-token prediction was previously only possible in the\nprohibitively expensive pretraining phase; CAFT, to our knowledge, is the first\nto bring the multi-token setting to the post-training phase, thus effectively\ndemocratizing its benefits for the broader community of practitioners and\nresearchers. Finally, the unexpected effectiveness of our proposed method\nsuggests wider implications for the machine learning research community. All\ncode and data are available at https://github.com/michaelchen-lab/caft-llm", "AI": {"tldr": "论文提出了一种名为概念感知微调（CAFT）的新方法，通过多令牌训练改进大语言模型（LLMs）的概念理解能力，显著提升了任务表现。", "motivation": "现有的大语言模型通过逐令牌预测限制了其形成连贯高级概念的能力，阻碍了更深层次的理解和推理。", "method": "引入概念感知微调（CAFT），一种多令牌训练方法，使模型能够学习跨多个令牌的序列，从而增强概念感知学习。", "result": "实验表明，CAFT在文本摘要和蛋白质设计等任务中显著优于传统的逐令牌微调方法。", "conclusion": "CAFT首次将多令牌预测引入训练后阶段，为研究社区提供了更广泛的应用潜力。"}}
{"id": "2506.08002", "pdf": "https://arxiv.org/pdf/2506.08002", "abs": "https://arxiv.org/abs/2506.08002", "authors": ["Aadarsh Sahoo", "Vansh Tibrewal", "Georgia Gkioxari"], "title": "Aligning Text, Images, and 3D Structure Token-by-Token", "categories": ["cs.CV"], "comment": "Project webpage: https://glab-caltech.github.io/kyvo/", "summary": "Creating machines capable of understanding the world in 3D is essential in\nassisting designers that build and edit 3D environments and robots navigating\nand interacting within a three-dimensional space. Inspired by advances in\nlanguage and image modeling, we investigate the potential of autoregressive\nmodels for a new modality: structured 3D scenes. To this end, we propose a\nunified LLM framework that aligns language, images, and 3D scenes and provide a\ndetailed ''cookbook'' outlining critical design choices for achieving optimal\ntraining and performance addressing key questions related to data\nrepresentation, modality-specific objectives, and more. We evaluate performance\nacross four core 3D tasks -- rendering, recognition, instruction-following, and\nquestion-answering -- and four 3D datasets, synthetic and real-world. We extend\nour approach to reconstruct complex 3D object shapes by enriching our 3D\nmodality with quantized shape encodings, and show our model's effectiveness on\nreal-world 3D object recognition tasks. Project webpage:\nhttps://glab-caltech.github.io/kyvo/", "AI": {"tldr": "论文提出了一种统一的LLM框架，用于对齐语言、图像和3D场景，并提供了关键设计选择的详细指南。", "motivation": "为设计师和机器人提供3D场景理解能力，推动3D建模和交互技术的发展。", "method": "采用自回归模型，结合量化形状编码，优化数据表示和模态特定目标。", "result": "在渲染、识别、指令跟随和问答等核心3D任务中表现优异，并在合成和真实世界数据集上验证。", "conclusion": "模型在复杂3D形状重建和真实世界识别任务中表现出色，为多模态3D理解提供了有效解决方案。"}}
{"id": "2506.06809", "pdf": "https://arxiv.org/pdf/2506.06809", "abs": "https://arxiv.org/abs/2506.06809", "authors": ["Di Lin", "Wanjing Ren", "Xuanbin Li", "Rui Zhang"], "title": "IMPA-HGAE:Intra-Meta-Path Augmented Heterogeneous Graph Autoencoder", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Self-supervised learning (SSL) methods have been increasingly applied to\ndiverse downstream tasks due to their superior generalization capabilities and\nlow annotation costs. However, most existing heterogeneous graph SSL models\nconvert heterogeneous graphs into homogeneous ones via meta-paths for training,\nwhich only leverage information from nodes at both ends of meta-paths while\nunderutilizing the heterogeneous node information along the meta-paths. To\naddress this limitation, this paper proposes a novel framework named IMPA-HGAE\nto enhance target node embeddings by fully exploiting internal node information\nalong meta-paths. Experimental results validate that IMPA-HGAE achieves\nsuperior performance on heterogeneous datasets. Furthermore, this paper\nintroduce innovative masking strategies to strengthen the representational\ncapacity of generative SSL models on heterogeneous graph data. Additionally,\nthis paper discuss the interpretability of the proposed method and potential\nfuture directions for generative self-supervised learning in heterogeneous\ngraphs. This work provides insights into leveraging meta-path-guided structural\nsemantics for robust representation learning in complex graph scenarios.", "AI": {"tldr": "本文提出了一种名为IMPA-HGAE的新框架，通过充分利用元路径上的内部节点信息来增强目标节点嵌入，解决了现有异构图自监督学习模型仅利用元路径两端节点信息的局限性。", "motivation": "现有异构图自监督学习方法通过元路径将异构图转换为同构图进行训练，仅利用元路径两端节点信息，而忽略了元路径上的异构节点信息。", "method": "提出IMPA-HGAE框架，通过创新的掩码策略和生成式自监督学习，充分利用元路径上的内部节点信息。", "result": "实验证明IMPA-HGAE在异构数据集上表现优异，且具有更强的表示能力。", "conclusion": "该工作为复杂图场景中利用元路径引导的结构语义进行鲁棒表示学习提供了新思路，并探讨了生成式自监督学习在异构图中的未来方向。"}}
{"id": "2506.08003", "pdf": "https://arxiv.org/pdf/2506.08003", "abs": "https://arxiv.org/abs/2506.08003", "authors": ["Shuchen Weng", "Haojie Zheng", "Zheng Chang", "Si Li", "Boxin Shi", "Xinlong Wang"], "title": "Audio-Sync Video Generation with Multi-Stream Temporal Control", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Audio is inherently temporal and closely synchronized with the visual world,\nmaking it a naturally aligned and expressive control signal for controllable\nvideo generation (e.g., movies). Beyond control, directly translating audio\ninto video is essential for understanding and visualizing rich audio narratives\n(e.g., Podcasts or historical recordings). However, existing approaches fall\nshort in generating high-quality videos with precise audio-visual\nsynchronization, especially across diverse and complex audio types. In this\nwork, we introduce MTV, a versatile framework for audio-sync video generation.\nMTV explicitly separates audios into speech, effects, and music tracks,\nenabling disentangled control over lip motion, event timing, and visual mood,\nrespectively -- resulting in fine-grained and semantically aligned video\ngeneration. To support the framework, we additionally present DEMIX, a dataset\ncomprising high-quality cinematic videos and demixed audio tracks. DEMIX is\nstructured into five overlapped subsets, enabling scalable multi-stage training\nfor diverse generation scenarios. Extensive experiments demonstrate that MTV\nachieves state-of-the-art performance across six standard metrics spanning\nvideo quality, text-video consistency, and audio-video alignment. Project page:\nhttps://hjzheng.net/projects/MTV/.", "AI": {"tldr": "MTV是一个用于音频同步视频生成的框架，通过分离音频为语音、效果和音乐轨道，实现精细化的视频生成。DEMIX数据集支持其训练，实验表明MTV在视频质量、文本一致性和音视频对齐方面表现优异。", "motivation": "音频与视觉世界紧密同步，是可控视频生成的理想控制信号。现有方法在高质量视频生成和音视频同步方面表现不足，尤其是针对复杂音频类型。", "method": "MTV框架将音频分离为语音、效果和音乐轨道，分别控制唇部动作、事件时间和视觉氛围。DEMIX数据集支持多阶段训练。", "result": "MTV在视频质量、文本一致性和音视频对齐六个标准指标上达到最先进性能。", "conclusion": "MTV通过音频分离和DEMIX数据集的支持，实现了高质量、语义对齐的视频生成，为音频驱动的视频生成提供了新思路。"}}
{"id": "2506.08004", "pdf": "https://arxiv.org/pdf/2506.08004", "abs": "https://arxiv.org/abs/2506.08004", "authors": ["Hidir Yesiltepe", "Pinar Yanardag"], "title": "Dynamic View Synthesis as an Inverse Problem", "categories": ["cs.CV", "cs.AI"], "comment": "Project Page: https://inverse-dvs.github.io/", "summary": "In this work, we address dynamic view synthesis from monocular videos as an\ninverse problem in a training-free setting. By redesigning the noise\ninitialization phase of a pre-trained video diffusion model, we enable\nhigh-fidelity dynamic view synthesis without any weight updates or auxiliary\nmodules. We begin by identifying a fundamental obstacle to deterministic\ninversion arising from zero-terminal signal-to-noise ratio (SNR) schedules and\nresolve it by introducing a novel noise representation, termed K-order\nRecursive Noise Representation. We derive a closed form expression for this\nrepresentation, enabling precise and efficient alignment between the\nVAE-encoded and the DDIM inverted latents. To synthesize newly visible regions\nresulting from camera motion, we introduce Stochastic Latent Modulation, which\nperforms visibility aware sampling over the latent space to complete occluded\nregions. Comprehensive experiments demonstrate that dynamic view synthesis can\nbe effectively performed through structured latent manipulation in the noise\ninitialization phase.", "AI": {"tldr": "该论文提出了一种无需训练的动态视图合成方法，通过改进预训练视频扩散模型的噪声初始化阶段，实现了高质量合成。", "motivation": "解决单目视频动态视图合成的逆问题，避免权重更新或辅助模块的需求。", "method": "引入K阶递归噪声表示解决确定性反转问题，并提出随机潜在调制以完成遮挡区域。", "result": "实验表明，通过噪声初始化阶段的潜在操作可有效实现动态视图合成。", "conclusion": "该方法在无需训练的情况下实现了高质量的动态视图合成。"}}
{"id": "2506.07919", "pdf": "https://arxiv.org/pdf/2506.07919", "abs": "https://arxiv.org/abs/2506.07919", "authors": ["Manuel Brenner", "Georgia Koppe"], "title": "Uncovering the Functional Roles of Nonlinearity in Memory", "categories": ["cs.LG", "cs.AI", "cs.CL", "nlin.CD", "physics.comp-ph"], "comment": "Preprint under review", "summary": "Memory and long-range temporal processing are core requirements for sequence\nmodeling tasks across natural language processing, time-series forecasting,\nspeech recognition, and control. While nonlinear recurrence has long been\nviewed as essential for enabling such mechanisms, recent work suggests that\nlinear dynamics may often suffice. In this study, we go beyond performance\ncomparisons to systematically dissect the functional role of nonlinearity in\nrecurrent networks--identifying both when it is computationally necessary, and\nwhat mechanisms it enables. We use Almost Linear Recurrent Neural Networks\n(AL-RNNs), which allow fine-grained control over nonlinearity, as both a\nflexible modeling tool and a probe into the internal mechanisms of memory.\nAcross a range of classic sequence modeling tasks and a real-world stimulus\nselection task, we find that minimal nonlinearity is not only sufficient but\noften optimal, yielding models that are simpler, more robust, and more\ninterpretable than their fully nonlinear or linear counterparts. Our results\nprovide a principled framework for selectively introducing nonlinearity,\nbridging dynamical systems theory with the functional demands of long-range\nmemory and structured computation in recurrent neural networks, with\nimplications for both artificial and biological neural systems.", "AI": {"tldr": "研究表明，在序列建模任务中，最小非线性通常足够且最优，简化模型并提高鲁棒性和可解释性。", "motivation": "探讨非线性在循环网络中的功能角色，明确其计算必要性和机制。", "method": "使用几乎线性循环神经网络（AL-RNNs）作为建模工具，分析非线性在记忆机制中的作用。", "result": "发现最小非线性不仅足够，而且通常最优，模型更简单、鲁棒且可解释。", "conclusion": "为选择性引入非线性提供了理论框架，对人工和生物神经系统有启示。"}}
{"id": "2506.08005", "pdf": "https://arxiv.org/pdf/2506.08005", "abs": "https://arxiv.org/abs/2506.08005", "authors": ["Lei Lai", "Zekai Yin", "Eshed Ohn-Bar"], "title": "ZeroVO: Visual Odometry with Minimal Assumptions", "categories": ["cs.CV"], "comment": null, "summary": "We introduce ZeroVO, a novel visual odometry (VO) algorithm that achieves\nzero-shot generalization across diverse cameras and environments, overcoming\nlimitations in existing methods that depend on predefined or static camera\ncalibration setups. Our approach incorporates three main innovations. First, we\ndesign a calibration-free, geometry-aware network structure capable of handling\nnoise in estimated depth and camera parameters. Second, we introduce a\nlanguage-based prior that infuses semantic information to enhance robust\nfeature extraction and generalization to previously unseen domains. Third, we\ndevelop a flexible, semi-supervised training paradigm that iteratively adapts\nto new scenes using unlabeled data, further boosting the models' ability to\ngeneralize across diverse real-world scenarios. We analyze complex autonomous\ndriving contexts, demonstrating over 30% improvement against prior methods on\nthree standard benchmarks, KITTI, nuScenes, and Argoverse 2, as well as a newly\nintroduced, high-fidelity synthetic dataset derived from Grand Theft Auto\n(GTA). By not requiring fine-tuning or camera calibration, our work broadens\nthe applicability of VO, providing a versatile solution for real-world\ndeployment at scale.", "AI": {"tldr": "ZeroVO是一种新型视觉里程计算法，无需预定义或静态相机校准，通过几何感知网络、语言先验和半监督训练实现零样本泛化。", "motivation": "现有视觉里程计方法依赖预定义或静态相机校准，限制了其泛化能力。ZeroVO旨在克服这一限制，适应多样化的相机和环境。", "method": "1. 设计无需校准的几何感知网络；2. 引入语言先验增强语义特征提取；3. 开发半监督训练范式，利用未标记数据适应新场景。", "result": "在KITTI、nuScenes和Argoverse 2等标准基准测试中，性能提升超过30%，并在GTA合成数据集上验证了泛化能力。", "conclusion": "ZeroVO无需微调或相机校准，为实际应用提供了更灵活的解决方案。"}}
{"id": "2506.08006", "pdf": "https://arxiv.org/pdf/2506.08006", "abs": "https://arxiv.org/abs/2506.08006", "authors": ["Sicheng Mo", "Ziyang Leng", "Leon Liu", "Weizhen Wang", "Honglin He", "Bolei Zhou"], "title": "Dreamland: Controllable World Creation with Simulator and Generative Models", "categories": ["cs.CV"], "comment": "Project Page: https://metadriverse.github.io/dreamland/", "summary": "Large-scale video generative models can synthesize diverse and realistic\nvisual content for dynamic world creation, but they often lack element-wise\ncontrollability, hindering their use in editing scenes and training embodied AI\nagents. We propose Dreamland, a hybrid world generation framework combining the\ngranular control of a physics-based simulator and the photorealistic content\noutput of large-scale pretrained generative models. In particular, we design a\nlayered world abstraction that encodes both pixel-level and object-level\nsemantics and geometry as an intermediate representation to bridge the\nsimulator and the generative model. This approach enhances controllability,\nminimizes adaptation cost through early alignment with real-world\ndistributions, and supports off-the-shelf use of existing and future pretrained\ngenerative models. We further construct a D3Sim dataset to facilitate the\ntraining and evaluation of hybrid generation pipelines. Experiments demonstrate\nthat Dreamland outperforms existing baselines with 50.8% improved image\nquality, 17.9% stronger controllability, and has great potential to enhance\nembodied agent training. Code and data will be made available.", "AI": {"tldr": "Dreamland结合物理模拟器和生成模型，通过分层世界抽象提升视频生成的可控性和质量。", "motivation": "现有大规模视频生成模型缺乏元素级可控性，限制了场景编辑和AI代理训练的实用性。", "method": "提出分层世界抽象，结合物理模拟器和预训练生成模型，使用中间表示编码像素和对象级语义与几何。", "result": "实验显示Dreamland图像质量提升50.8%，可控性增强17.9%，并支持现成生成模型的使用。", "conclusion": "Dreamland为动态世界生成提供了高效可控的解决方案，并有望提升AI代理训练效果。"}}
{"id": "2506.08008", "pdf": "https://arxiv.org/pdf/2506.08008", "abs": "https://arxiv.org/abs/2506.08008", "authors": ["Stephanie Fu", "Tyler Bonnen", "Devin Guillory", "Trevor Darrell"], "title": "Hidden in plain sight: VLMs overlook their visual representations", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Project page: https://hidden-plain-sight.github.io/", "summary": "Language provides a natural interface to specify and evaluate performance on\nvisual tasks. To realize this possibility, vision language models (VLMs) must\nsuccessfully integrate visual and linguistic information. Our work compares\nVLMs to a direct readout of their visual encoders to understand their ability\nto integrate across these modalities. Across a series of vision-centric\nbenchmarks (e.g., depth estimation, correspondence), we find that VLMs perform\nsubstantially worse than their visual encoders, dropping to near-chance\nperformance. We investigate these results through a series of analyses across\nthe entire VLM: namely 1) the degradation of vision representations, 2)\nbrittleness to task prompt, and 3) the language model's role in solving the\ntask. We find that the bottleneck in performing these vision-centric tasks lies\nin this third category; VLMs are not effectively using visual information\neasily accessible throughout the entire model, and they inherit the language\npriors present in the LLM. Our work helps diagnose the failure modes of\nopen-source VLMs, and presents a series of evaluations useful for future\ninvestigations into visual understanding within VLMs.", "AI": {"tldr": "论文比较了视觉语言模型（VLMs）与其视觉编码器的性能，发现VLMs在视觉任务中表现显著更差，接近随机水平。研究分析了三个可能原因：视觉表征退化、任务提示的脆弱性以及语言模型在任务中的作用，发现瓶颈在于语言模型未能有效利用视觉信息。", "motivation": "探索视觉语言模型（VLMs）在整合视觉和语言信息方面的能力，并诊断其在视觉任务中表现不佳的原因。", "method": "通过一系列视觉中心基准测试（如深度估计、对应关系）比较VLMs与其视觉编码器的性能，并分析VLMs的视觉表征、任务提示敏感性和语言模型的作用。", "result": "VLMs在视觉任务中表现显著低于其视觉编码器，接近随机水平。研究发现语言模型未能有效利用视觉信息，且受语言先验影响。", "conclusion": "研究揭示了VLMs在视觉任务中的失败模式，为未来研究提供了诊断工具和评估方法。"}}
{"id": "2506.07945", "pdf": "https://arxiv.org/pdf/2506.07945", "abs": "https://arxiv.org/abs/2506.07945", "authors": ["Arnav Sheth", "Ivaxi Sheth", "Mario Fritz"], "title": "ProtocolLLM: RTL Benchmark for SystemVerilog Generation of Communication Protocols", "categories": ["cs.AR", "cs.AI", "cs.CL"], "comment": "Accepted at MLSysArch@ISCA 2025", "summary": "Recent advances in Large Language Models (LLMs) have shown promising\ncapabilities in generating code for general-purpose programming languages. In\ncontrast, their applicability for hardware description languages, particularly\nfor generating synthesizable and functionally correct designs, remains\nsignificantly underexplored. HDLs such as SystemVerilog are logic-oriented and\ndemand strict adherence to timing semantics, concurrency, and synthesizability\nconstraints. Moreover, HDL-based design flows encompass a broad set of tasks\nbeyond structural code generation, including testbench development,\nassertion-based verification, timing closure, and protocol-level integration\nfor on-chip communication. The objective of our paper is to analyze the\ncapabilities of state-of-the-art LLMs in generating SystemVerilog\nimplementations of standard communication protocols, a core component of\nembedded and System-on-Chip (SoC) architectures. This paper introduces the\nfirst benchmark suite targeting four widely used protocols: SPI, I2C, UART, and\nAXI. We define code generation tasks that capture varying levels of design\nabstraction and prompt specificity. The generated designs are assessed for\nsyntactic correctness, synthesizability, and functional fidelity via waveform\nsimulation and test benches.", "AI": {"tldr": "论文探讨了大型语言模型（LLMs）在生成硬件描述语言（如SystemVerilog）代码方面的能力，特别是针对标准通信协议的实现，并提出了首个针对SPI、I2C、UART和AXI协议的基准测试套件。", "motivation": "尽管LLMs在通用编程语言代码生成方面表现出色，但其在硬件描述语言（HDLs）中的应用，尤其是在生成可综合且功能正确的设计方面，尚未得到充分研究。", "method": "论文提出了一个基准测试套件，针对四种常用通信协议（SPI、I2C、UART、AXI），定义了不同抽象层次和提示特异性的代码生成任务，并通过波形仿真和测试台评估生成设计的语法正确性、可综合性和功能保真度。", "result": "研究结果表明了LLMs在生成SystemVerilog代码方面的潜力，但也揭示了其在满足硬件设计严格约束方面的局限性。", "conclusion": "论文为LLMs在硬件设计领域的应用提供了初步探索，并指出了未来改进的方向。"}}
{"id": "2506.08009", "pdf": "https://arxiv.org/pdf/2506.08009", "abs": "https://arxiv.org/abs/2506.08009", "authors": ["Xun Huang", "Zhengqi Li", "Guande He", "Mingyuan Zhou", "Eli Shechtman"], "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Project website: http://self-forcing.github.io/", "summary": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/", "AI": {"tldr": "Self Forcing是一种新的自回归视频扩散模型训练方法，解决了曝光偏差问题，通过自生成输出进行训练，实现了高效的实时视频生成。", "motivation": "解决传统方法在推理时因依赖自身不完美输出而导致的曝光偏差问题。", "method": "采用自回归展开和KV缓存策略，结合多步扩散模型和梯度截断，优化训练效率和生成质量。", "result": "在单GPU上实现亚秒级延迟的实时视频生成，质量优于或匹配更慢的非因果扩散模型。", "conclusion": "Self Forcing提供了一种高效且高质量的实时视频生成解决方案。"}}
{"id": "2506.08010", "pdf": "https://arxiv.org/pdf/2506.08010", "abs": "https://arxiv.org/abs/2506.08010", "authors": ["Nick Jiang", "Amil Dravid", "Alexei Efros", "Yossi Gandelsman"], "title": "Vision Transformers Don't Need Trained Registers", "categories": ["cs.CV", "cs.AI"], "comment": "Project page and code: https://avdravid.github.io/test-time-registers", "summary": "We investigate the mechanism underlying a previously identified phenomenon in\nVision Transformers -- the emergence of high-norm tokens that lead to noisy\nattention maps. We observe that in multiple models (e.g., CLIP, DINOv2), a\nsparse set of neurons is responsible for concentrating high-norm activations on\noutlier tokens, leading to irregular attention patterns and degrading\ndownstream visual processing. While the existing solution for removing these\noutliers involves retraining models from scratch with additional learned\nregister tokens, we use our findings to create a training-free approach to\nmitigate these artifacts. By shifting the high-norm activations from our\ndiscovered register neurons into an additional untrained token, we can mimic\nthe effect of register tokens on a model already trained without registers. We\ndemonstrate that our method produces cleaner attention and feature maps,\nenhances performance over base models across multiple downstream visual tasks,\nand achieves results comparable to models explicitly trained with register\ntokens. We then extend test-time registers to off-the-shelf vision-language\nmodels to improve their interpretability. Our results suggest that test-time\nregisters effectively take on the role of register tokens at test-time,\noffering a training-free solution for any pre-trained model released without\nthem.", "AI": {"tldr": "论文研究了Vision Transformers中高范数令牌的机制，提出了一种无需重新训练的方法来缓解其对注意力图的干扰，并验证了其有效性。", "motivation": "探索Vision Transformers中高范数令牌导致注意力图噪声的机制，并寻找无需重新训练的解决方案。", "method": "通过将高范数激活从特定神经元转移到未训练的令牌中，模拟注册令牌的效果。", "result": "方法能生成更清晰的注意力和特征图，提升下游视觉任务性能，效果接近显式训练注册令牌的模型。", "conclusion": "测试时注册令牌可替代训练时注册令牌，为预训练模型提供无需训练的解决方案。"}}
{"id": "2506.06837", "pdf": "https://arxiv.org/pdf/2506.06837", "abs": "https://arxiv.org/abs/2506.06837", "authors": ["Eyal Briman", "Ehud Shapiro", "Nimrod Talmon"], "title": "AI-Generated Compromises for Coalition Formation", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "The challenge of finding compromises between agent proposals is fundamental\nto AI subfields such as argumentation, mediation, and negotiation. Building on\nthis tradition, Elkind et al. (2021) introduced a process for coalition\nformation that seeks majority-supported proposals preferable to the status quo,\nusing a metric space where each agent has an ideal point. A crucial step in\nthis process involves identifying compromise proposals around which agent\ncoalitions can unite. How to effectively find such compromise proposals remains\nan open question. We address this gap by formalizing a model that incorporates\nagent bounded rationality and uncertainty, and by developing AI methods to\ngenerate compromise proposals. We focus on the domain of collaborative document\nwriting, such as the democratic drafting of a community constitution. Our\napproach uses natural language processing techniques and large language models\nto induce a semantic metric space over text. Based on this space, we design\nalgorithms to suggest compromise points likely to receive broad support. To\nevaluate our methods, we simulate coalition formation processes and show that\nAI can facilitate large-scale democratic text editing, a domain where\ntraditional tools are limited.", "AI": {"tldr": "该论文提出了一种结合自然语言处理和大语言模型的方法，用于在协作文档写作中生成妥协提案，以促进大规模民主文本编辑。", "motivation": "解决在协作文档写作（如社区宪章起草）中如何有效找到妥协提案的问题，填补了现有方法的空白。", "method": "通过形式化一个包含代理有限理性和不确定性的模型，利用自然语言处理技术和大语言模型构建语义度量空间，并设计算法生成妥协提案。", "result": "模拟实验表明，AI方法能够有效促进大规模民主文本编辑，优于传统工具。", "conclusion": "AI方法在协作文档写作中具有潜力，能够帮助形成广泛支持的妥协提案。"}}
{"id": "2506.07972", "pdf": "https://arxiv.org/pdf/2506.07972", "abs": "https://arxiv.org/abs/2506.07972", "authors": ["Hongzheng Chen", "Yingheng Wang", "Yaohui Cai", "Hins Hu", "Jiajie Li", "Shirley Huang", "Chenhui Deng", "Rongjian Liang", "Shufeng Kong", "Haoxing Ren", "Samitha Samaranayake", "Carla P. Gomes", "Zhiru Zhang"], "title": "HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "While Large Language Models (LLMs) have demonstrated significant advancements\nin reasoning and agent-based problem-solving, current evaluation methodologies\nfail to adequately assess their capabilities: existing benchmarks either rely\non closed-ended questions prone to saturation and memorization, or subjective\ncomparisons that lack consistency and rigor. In this work, we introduce\nHeuriGym, an agentic framework designed for evaluating heuristic algorithms\ngenerated by LLMs for combinatorial optimization problems, characterized by\nclearly defined objectives and expansive solution spaces. HeuriGym empowers\nLLMs to propose heuristics, receive evaluative feedback via code execution, and\niteratively refine their solutions. We evaluate nine state-of-the-art models on\nnine problems across domains such as computer systems, logistics, and biology,\nexposing persistent limitations in tool use, planning, and adaptive reasoning.\nTo quantify performance, we propose the Quality-Yield Index (QYI), a metric\nthat captures both solution pass rate and quality. Even top models like\nGPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below\nthe expert baseline of 1. Our open-source benchmark aims to guide the\ndevelopment of LLMs toward more effective and realistic problem-solving in\nscientific and engineering domains.", "AI": {"tldr": "HeuriGym是一个评估LLM生成启发式算法的框架，用于组合优化问题，通过代码执行反馈迭代优化解决方案。", "motivation": "现有评估方法无法充分评估LLM的能力，要么依赖封闭问题易饱和，要么主观比较缺乏一致性。", "method": "提出HeuriGym框架，让LLM生成启发式算法并通过代码执行反馈迭代优化，引入QYI指标量化性能。", "result": "测试9个顶级模型，发现工具使用、规划和自适应推理的局限性，QYI得分最高仅0.6，远低于专家基线1。", "conclusion": "开源基准旨在推动LLM在科学和工程领域更有效、现实的解决问题能力。"}}
{"id": "2506.08011", "pdf": "https://arxiv.org/pdf/2506.08011", "abs": "https://arxiv.org/abs/2506.08011", "authors": ["Yunfei Xie", "Yinsong Ma", "Shiyi Lan", "Alan Yuille", "Junfei Xiao", "Chen Wei"], "title": "Play to Generalize: Learning to Reason Through Game Play", "categories": ["cs.CV", "cs.CL"], "comment": "Project Page: https://yunfeixie233.github.io/ViGaL/", "summary": "Developing generalizable reasoning capabilities in multimodal large language\nmodels (MLLMs) remains challenging. Motivated by cognitive science literature\nsuggesting that gameplay promotes transferable cognitive skills, we propose a\nnovel post-training paradigm, Visual Game Learning, or ViGaL, where MLLMs\ndevelop out-of-domain generalization of multimodal reasoning through playing\narcade-like games. Specifically, we show that post-training a 7B-parameter MLLM\nvia reinforcement learning (RL) on simple arcade-like games, e.g. Snake,\nsignificantly enhances its downstream performance on multimodal math benchmarks\nlike MathVista, and on multi-discipline questions like MMMU, without seeing any\nworked solutions, equations, or diagrams during RL, suggesting the capture of\ntransferable reasoning skills. Remarkably, our model outperforms specialist\nmodels tuned on multimodal reasoning data in multimodal reasoning benchmarks,\nwhile preserving the base model's performance on general visual benchmarks, a\nchallenge where specialist models often fall short. Our findings suggest a new\npost-training paradigm: synthetic, rule-based games can serve as controllable\nand scalable pre-text tasks that unlock generalizable multimodal reasoning\nabilities in MLLMs.", "AI": {"tldr": "ViGaL是一种通过玩街机游戏（如贪吃蛇）增强多模态大语言模型（MLLMs）泛化推理能力的新方法，无需依赖具体数据即可提升数学和多学科问题的表现。", "motivation": "受认知科学启发，游戏能促进可迁移的认知技能，因此提出通过游戏训练提升MLLMs的跨领域推理能力。", "method": "采用强化学习（RL）对7B参数的MLLM进行后训练，通过玩简单街机游戏（如贪吃蛇）来增强推理能力。", "result": "模型在多模态数学基准（如MathVista）和多学科问题（如MMMU）上表现优于专用模型，同时保持基础模型在通用视觉任务上的性能。", "conclusion": "规则化游戏可作为可控且可扩展的预训练任务，解锁MLLMs的泛化多模态推理能力。"}}
{"id": "2506.06840", "pdf": "https://arxiv.org/pdf/2506.06840", "abs": "https://arxiv.org/abs/2506.06840", "authors": ["Fahad Mostafa"], "title": "A Statistical Framework for Model Selection in LSTM Networks", "categories": ["stat.ML", "cs.AI", "cs.LG", "stat.AP", "62M10, 92B20, 62P10, 62P99"], "comment": null, "summary": "Long Short-Term Memory (LSTM) neural network models have become the\ncornerstone for sequential data modeling in numerous applications, ranging from\nnatural language processing to time series forecasting. Despite their success,\nthe problem of model selection, including hyperparameter tuning, architecture\nspecification, and regularization choice remains largely heuristic and\ncomputationally expensive. In this paper, we propose a unified statistical\nframework for systematic model selection in LSTM networks. Our framework\nextends classical model selection ideas, such as information criteria and\nshrinkage estimation, to sequential neural networks. We define penalized\nlikelihoods adapted to temporal structures, propose a generalized threshold\napproach for hidden state dynamics, and provide efficient estimation strategies\nusing variational Bayes and approximate marginal likelihood methods. Several\nbiomedical data centric examples demonstrate the flexibility and improved\nperformance of the proposed framework.", "AI": {"tldr": "提出了一种统一的统计框架，用于LSTM网络的系统模型选择，解决了传统方法中启发式和计算成本高的问题。", "motivation": "尽管LSTM在序列数据建模中广泛应用，但模型选择（如超参数调优、架构设计和正则化选择）仍然依赖启发式方法且计算成本高。", "method": "扩展了经典模型选择思想（如信息准则和收缩估计）至序列神经网络，提出了适应时间结构的惩罚似然、广义阈值方法，并采用变分贝叶斯和近似边际似然方法进行高效估计。", "result": "在多个生物医学数据案例中展示了该框架的灵活性和性能提升。", "conclusion": "提出的统计框架为LSTM模型选择提供了系统化和高效的方法，显著提升了性能。"}}
{"id": "2506.08013", "pdf": "https://arxiv.org/pdf/2506.08013", "abs": "https://arxiv.org/abs/2506.08013", "authors": ["Anh-Quan Cao", "Ivan Lopes", "Raoul de Charette"], "title": "StableMTL: Repurposing Latent Diffusion Models for Multi-Task Learning from Partially Annotated Synthetic Datasets", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Code is available at https://github.com/astra-vision/StableMTL", "summary": "Multi-task learning for dense prediction is limited by the need for extensive\nannotation for every task, though recent works have explored training with\npartial task labels. Leveraging the generalization power of diffusion models,\nwe extend the partial learning setup to a zero-shot setting, training a\nmulti-task model on multiple synthetic datasets, each labeled for only a subset\nof tasks. Our method, StableMTL, repurposes image generators for latent\nregression. Adapting a denoising framework with task encoding, per-task\nconditioning and a tailored training scheme. Instead of per-task losses\nrequiring careful balancing, a unified latent loss is adopted, enabling\nseamless scaling to more tasks. To encourage inter-task synergy, we introduce a\nmulti-stream model with a task-attention mechanism that converts N-to-N task\ninteractions into efficient 1-to-N attention, promoting effective cross-task\nsharing. StableMTL outperforms baselines on 7 tasks across 8 benchmarks.", "AI": {"tldr": "StableMTL利用扩散模型在多任务学习中实现零样本训练，通过统一潜在损失和多流任务注意力机制，显著提升了性能。", "motivation": "多任务学习需要大量标注数据，而部分任务标注的扩展性有限。本文旨在通过扩散模型的泛化能力，实现零样本多任务学习。", "method": "提出StableMTL方法，利用图像生成器进行潜在回归，结合任务编码、任务条件化和统一潜在损失，并通过多流模型和任务注意力机制促进任务间协同。", "result": "在8个基准测试的7个任务上，StableMTL优于基线方法。", "conclusion": "StableMTL通过扩散模型和任务注意力机制，有效解决了多任务学习中的标注限制问题，并展示了优异的性能。"}}
{"id": "2506.08001", "pdf": "https://arxiv.org/pdf/2506.08001", "abs": "https://arxiv.org/abs/2506.08001", "authors": ["Zeju Qiu", "Simon Buchholz", "Tim Z. Xiao", "Maximilian Dax", "Bernhard Schölkopf", "Weiyang Liu"], "title": "Reparameterized LLM Training via Orthogonal Equivalence Transformation", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "Technical report v1 (36 pages, 24 figures, project page:\n  https://spherelab.ai/poet-site/)", "summary": "While large language models (LLMs) are driving the rapid advancement of\nartificial intelligence, effectively and reliably training these large models\nremains one of the field's most significant challenges. To address this\nchallenge, we propose POET, a novel reParameterized training algorithm that\nuses Orthogonal Equivalence Transformation to optimize neurons. Specifically,\nPOET reparameterizes each neuron with two learnable orthogonal matrices and a\nfixed random weight matrix. Because of its provable preservation of spectral\nproperties of weight matrices, POET can stably optimize the objective function\nwith improved generalization. We further develop efficient approximations that\nmake POET flexible and scalable for training large-scale neural networks.\nExtensive experiments validate the effectiveness and scalability of POET in\ntraining LLMs.", "AI": {"tldr": "POET是一种新型的重新参数化训练算法，通过正交等价变换优化神经元，提高大语言模型训练的稳定性和泛化能力。", "motivation": "大语言模型（LLMs）的训练是人工智能领域的重要挑战，需要更有效和可靠的方法。", "method": "POET通过两个可学习的正交矩阵和一个固定随机权重矩阵重新参数化神经元，保持权重矩阵的谱特性。", "result": "实验验证了POET在训练大语言模型中的有效性和可扩展性。", "conclusion": "POET为大规模神经网络训练提供了一种稳定且高效的解决方案。"}}
{"id": "2506.08015", "pdf": "https://arxiv.org/pdf/2506.08015", "abs": "https://arxiv.org/abs/2506.08015", "authors": ["Zhen Xu", "Zhengqin Li", "Zhao Dong", "Xiaowei Zhou", "Richard Newcombe", "Zhaoyang Lv"], "title": "4DGT: Learning a 4D Gaussian Transformer Using Real-World Monocular Videos", "categories": ["cs.CV"], "comment": "Project page: https://4dgt.github.io", "summary": "We propose 4DGT, a 4D Gaussian-based Transformer model for dynamic scene\nreconstruction, trained entirely on real-world monocular posed videos. Using 4D\nGaussian as an inductive bias, 4DGT unifies static and dynamic components,\nenabling the modeling of complex, time-varying environments with varying object\nlifespans. We proposed a novel density control strategy in training, which\nenables our 4DGT to handle longer space-time input and remain efficient\nrendering at runtime. Our model processes 64 consecutive posed frames in a\nrolling-window fashion, predicting consistent 4D Gaussians in the scene. Unlike\noptimization-based methods, 4DGT performs purely feed-forward inference,\nreducing reconstruction time from hours to seconds and scaling effectively to\nlong video sequences. Trained only on large-scale monocular posed video\ndatasets, 4DGT can outperform prior Gaussian-based networks significantly in\nreal-world videos and achieve on-par accuracy with optimization-based methods\non cross-domain videos. Project page: https://4dgt.github.io", "AI": {"tldr": "4DGT是一种基于4D高斯和Transformer的动态场景重建模型，通过单目视频训练，统一静态和动态组件，支持复杂时变环境建模。", "motivation": "动态场景重建需要处理复杂的时间变化和对象生命周期，现有方法效率低且难以扩展。", "method": "使用4D高斯作为归纳偏置，提出密度控制策略，支持长时空输入和高效渲染，通过滚动窗口处理连续帧。", "result": "4DGT在真实视频中显著优于现有高斯网络，在跨域视频中与优化方法精度相当，推理时间从小时级降至秒级。", "conclusion": "4DGT通过高效的前馈推理和密度控制策略，实现了动态场景的高效重建，适用于长视频序列。"}}
{"id": "2506.06858", "pdf": "https://arxiv.org/pdf/2506.06858", "abs": "https://arxiv.org/abs/2506.06858", "authors": ["Ziwei Li", "Yuhan Duan", "Tianyu Xiong", "Yi-Tang Chen", "Wei-Lun Chao", "Han-Wei Shen"], "title": "High-Fidelity Scientific Simulation Surrogates via Adaptive Implicit Neural Representations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Effective surrogate models are critical for accelerating scientific\nsimulations. Implicit neural representations (INRs) offer a compact and\ncontinuous framework for modeling spatially structured data, but they often\nstruggle with complex scientific fields exhibiting localized, high-frequency\nvariations. Recent approaches address this by introducing additional features\nalong rigid geometric structures (e.g., grids), but at the cost of flexibility\nand increased model size. In this paper, we propose a simple yet effective\nalternative: Feature-Adaptive INR (FA-INR). FA-INR leverages cross-attention to\nan augmented memory bank to learn flexible feature representations, enabling\nadaptive allocation of model capacity based on data characteristics, rather\nthan rigid structural assumptions. To further improve scalability, we introduce\na coordinate-guided mixture of experts (MoE) that enhances the specialization\nand efficiency of feature representations. Experiments on three large-scale\nensemble simulation datasets show that FA-INR achieves state-of-the-art\nfidelity while significantly reducing model size, establishing a new trade-off\nfrontier between accuracy and compactness for INR-based surrogates.", "AI": {"tldr": "FA-INR提出了一种基于交叉注意力和记忆库的隐式神经表示方法，通过自适应分配模型容量提升复杂科学场建模的精度和效率。", "motivation": "现有INR方法在处理具有局部高频变化的复杂科学场时表现不佳，且引入刚性几何结构会增加模型复杂度和降低灵活性。", "method": "FA-INR利用交叉注意力机制和记忆库学习灵活特征表示，并引入坐标引导的专家混合（MoE）提升扩展性。", "result": "在三个大规模集成仿真数据集上，FA-INR在保持高精度的同时显著减小模型规模，实现了精度与紧凑性的新平衡。", "conclusion": "FA-INR为基于INR的代理模型提供了一种简单高效的方法，显著提升了复杂科学场建模的能力。"}}
{"id": "2506.06306", "pdf": "https://arxiv.org/pdf/2506.06306", "abs": "https://arxiv.org/abs/2506.06306", "authors": ["Ali Abedi", "Charlene H. Chu", "Shehroz S. Khan"], "title": "Benchmarking Early Agitation Prediction in Community-Dwelling People with Dementia Using Multimodal Sensors and Machine Learning", "categories": ["eess.SP", "cs.CV", "cs.HC", "cs.LG"], "comment": "16 pages, 4 figures, 2 tables", "summary": "Agitation is one of the most common responsive behaviors in people living\nwith dementia, particularly among those residing in community settings without\ncontinuous clinical supervision. Timely prediction of agitation can enable\nearly intervention, reduce caregiver burden, and improve the quality of life\nfor both patients and caregivers. This study aimed to develop and benchmark\nmachine learning approaches for the early prediction of agitation in\ncommunity-dwelling older adults with dementia using multimodal sensor data. A\nnew set of agitation-related contextual features derived from activity data was\nintroduced and employed for agitation prediction. A wide range of machine\nlearning and deep learning models was evaluated across multiple problem\nformulations, including binary classification for single-timestamp tabular\nsensor data and multi-timestamp sequential sensor data, as well as anomaly\ndetection for single-timestamp tabular sensor data. The study utilized the\nTechnology Integrated Health Management (TIHM) dataset, the largest publicly\navailable dataset for remote monitoring of people living with dementia,\ncomprising 2,803 days of in-home activity, physiology, and sleep data. The most\neffective setting involved binary classification of sensor data using the\ncurrent 6-hour timestamp to predict agitation at the subsequent timestamp.\nIncorporating additional information, such as time of day and agitation\nhistory, further improved model performance, with the highest AUC-ROC of 0.9720\nand AUC-PR of 0.4320 achieved by the light gradient boosting machine. This work\npresents the first comprehensive benchmarking of state-of-the-art techniques\nfor agitation prediction in community-based dementia care using\nprivacy-preserving sensor data. The approach enables accurate, explainable, and\nefficient agitation prediction, supporting proactive dementia care and aging in\nplace.", "AI": {"tldr": "该研究开发并比较了多种机器学习方法，用于通过多模态传感器数据早期预测社区居住的老年痴呆患者的躁动行为，效果显著。", "motivation": "老年痴呆患者的躁动行为常见且影响生活质量，早期预测可减轻护理负担并改善患者和护理者的生活质量。", "method": "研究引入了一组新的与躁动相关的上下文特征，并评估了多种机器学习和深度学习模型，包括二元分类和异常检测。", "result": "最佳模型（轻梯度提升机）在二元分类任务中表现出色，AUC-ROC达0.9720，AUC-PR为0.4320。", "conclusion": "该研究为基于隐私保护传感器数据的社区痴呆护理提供了首个全面的躁动预测基准，支持主动护理和居家养老。"}}
{"id": "2506.06862", "pdf": "https://arxiv.org/pdf/2506.06862", "abs": "https://arxiv.org/abs/2506.06862", "authors": ["Chenguang Huang", "Oier Mees", "Andy Zeng", "Wolfram Burgard"], "title": "Multimodal Spatial Language Maps for Robot Navigation and Manipulation", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SD", "eess.AS"], "comment": "accepted to International Journal of Robotics Research (IJRR). 24\n  pages, 18 figures. The paper contains texts from VLMaps(arXiv:2210.05714) and\n  AVLMaps(arXiv:2303.07522). The project page is https://mslmaps.github.io/", "summary": "Grounding language to a navigating agent's observations can leverage\npretrained multimodal foundation models to match perceptions to object or event\ndescriptions. However, previous approaches remain disconnected from environment\nmapping, lack the spatial precision of geometric maps, or neglect additional\nmodality information beyond vision. To address this, we propose multimodal\nspatial language maps as a spatial map representation that fuses pretrained\nmultimodal features with a 3D reconstruction of the environment. We build these\nmaps autonomously using standard exploration. We present two instances of our\nmaps, which are visual-language maps (VLMaps) and their extension to\naudio-visual-language maps (AVLMaps) obtained by adding audio information. When\ncombined with large language models (LLMs), VLMaps can (i) translate natural\nlanguage commands into open-vocabulary spatial goals (e.g., \"in between the\nsofa and TV\") directly localized in the map, and (ii) be shared across\ndifferent robot embodiments to generate tailored obstacle maps on demand.\nBuilding upon the capabilities above, AVLMaps extend VLMaps by introducing a\nunified 3D spatial representation integrating audio, visual, and language cues\nthrough the fusion of features from pretrained multimodal foundation models.\nThis enables robots to ground multimodal goal queries (e.g., text, images, or\naudio snippets) to spatial locations for navigation. Additionally, the\nincorporation of diverse sensory inputs significantly enhances goal\ndisambiguation in ambiguous environments. Experiments in simulation and\nreal-world settings demonstrate that our multimodal spatial language maps\nenable zero-shot spatial and multimodal goal navigation and improve recall by\n50% in ambiguous scenarios. These capabilities extend to mobile robots and\ntabletop manipulators, supporting navigation and interaction guided by visual,\naudio, and spatial cues.", "AI": {"tldr": "论文提出了一种多模态空间语言地图（VLMaps和AVLMaps），通过融合预训练多模态特征与3D环境重建，实现自然语言命令到空间目标的翻译，并支持多模态目标导航。", "motivation": "现有方法未能充分利用环境映射、空间精度和多模态信息，因此需要一种更全面的空间表示方法。", "method": "提出VLMaps和AVLMaps，结合预训练多模态特征与3D环境重建，并通过标准探索自主构建地图。", "result": "实验表明，该方法在仿真和真实环境中实现了零样本空间和多模态目标导航，并在模糊场景中召回率提高了50%。", "conclusion": "多模态空间语言地图为机器人导航和交互提供了更灵活和强大的支持。"}}
{"id": "2506.06315", "pdf": "https://arxiv.org/pdf/2506.06315", "abs": "https://arxiv.org/abs/2506.06315", "authors": ["Masoud Rahimi", "Reza Karbasi", "Abdol-Hossein Vahabie"], "title": "An Open-Source Python Framework and Synthetic ECG Image Datasets for Digitization, Lead and Lead Name Detection, and Overlapping Signal Segmentation", "categories": ["eess.SP", "cs.CV", "cs.LG"], "comment": "5 pages, 5 figures", "summary": "We introduce an open-source Python framework for generating synthetic ECG\nimage datasets to advance critical deep learning-based tasks in ECG analysis,\nincluding ECG digitization, lead region and lead name detection, and\npixel-level waveform segmentation. Using the PTB-XL signal dataset, our\nproposed framework produces four open-access datasets: (1) ECG images in\nvarious lead configurations paired with time-series signals for ECG\ndigitization, (2) ECG images annotated with YOLO-format bounding boxes for\ndetection of lead region and lead name, (3)-(4) cropped single-lead images with\nsegmentation masks compatible with U-Net-based models in normal and overlapping\nversions. In the overlapping case, waveforms from neighboring leads are\nsuperimposed onto the target lead image, while the segmentation masks remain\nclean. The open-source Python framework and datasets are publicly available at\nhttps://github.com/rezakarbasi/ecg-image-and-signal-dataset and\nhttps://doi.org/10.5281/zenodo.15484519, respectively.", "AI": {"tldr": "开源Python框架生成合成ECG图像数据集，支持ECG数字化、导联区域和名称检测、波形分割等深度学习任务。", "motivation": "推动ECG分析中的深度学习任务，如ECG数字化、导联检测和波形分割。", "method": "基于PTB-XL信号数据集，生成四种开放数据集，包括配对的ECG图像与时间序列信号、YOLO格式标注的导联区域和名称检测数据、单导联分割掩码图像（正常与重叠版本）。", "result": "提供了四种公开数据集，支持多种ECG分析任务。", "conclusion": "开源框架和数据集为ECG分析的深度学习研究提供了重要资源。"}}
{"id": "2506.06349", "pdf": "https://arxiv.org/pdf/2506.06349", "abs": "https://arxiv.org/abs/2506.06349", "authors": ["Thien Nhan Vo", "Thanh Xuan Truong"], "title": "Heart Rate Classification in ECG Signals Using Machine Learning and Deep Learning", "categories": ["eess.SP", "cs.CV", "cs.LG"], "comment": null, "summary": "This study addresses the classification of heartbeats from ECG signals\nthrough two distinct approaches: traditional machine learning utilizing\nhand-crafted features and deep learning via transformed images of ECG beats.\nThe dataset underwent preprocessing steps, including downsampling, filtering,\nand normalization, to ensure consistency and relevance for subsequent analysis.\nIn the first approach, features such as heart rate variability (HRV), mean,\nvariance, and RR intervals were extracted to train various classifiers,\nincluding SVM, Random Forest, AdaBoost, LSTM, Bi-directional LSTM, and\nLightGBM. The second approach involved transforming ECG signals into images\nusing Gramian Angular Field (GAF), Markov Transition Field (MTF), and\nRecurrence Plots (RP), with these images subsequently classified using CNN\narchitectures like VGG and Inception.\n  Experimental results demonstrate that the LightGBM model achieved the highest\nperformance, with an accuracy of 99% and an F1 score of 0.94, outperforming the\nimage-based CNN approach (F1 score of 0.85). Models such as SVM and AdaBoost\nyielded significantly lower scores, indicating limited suitability for this\ntask. The findings underscore the superior ability of hand-crafted features to\ncapture temporal and morphological variations in ECG signals compared to\nimage-based representations of individual beats. Future investigations may\nbenefit from incorporating multi-lead ECG signals and temporal dependencies\nacross successive beats to enhance classification accuracy further.", "AI": {"tldr": "该研究通过传统机器学习和深度学习方法对ECG信号进行心跳分类，发现基于手工特征的LightGBM模型表现最佳，准确率达99%。", "motivation": "解决ECG信号中心跳分类的问题，比较手工特征和图像转换方法的效果。", "method": "1. 传统机器学习：提取HRV、均值、方差等特征，训练SVM、随机森林等模型。2. 深度学习方法：将ECG信号转换为图像（GAF、MTF、RP），用CNN分类。", "result": "LightGBM表现最优（准确率99%，F1分数0.94），优于图像方法（F1分数0.85）。", "conclusion": "手工特征能更好地捕捉ECG信号的时态和形态变化，未来可研究多导联信号和连续心跳的时序依赖。"}}
{"id": "2506.06866", "pdf": "https://arxiv.org/pdf/2506.06866", "abs": "https://arxiv.org/abs/2506.06866", "authors": ["Dongyeop Lee", "Kwanhee Lee", "Jinseok Chung", "Namhoon Lee"], "title": "SAFE: Finding Sparse and Flat Minima to Improve Pruning", "categories": ["cs.LG", "cs.AI"], "comment": "ICML 2025", "summary": "Sparsifying neural networks often suffers from seemingly inevitable\nperformance degradation, and it remains challenging to restore the original\nperformance despite much recent progress. Motivated by recent studies in robust\noptimization, we aim to tackle this problem by finding subnetworks that are\nboth sparse and flat at the same time. Specifically, we formulate pruning as a\nsparsity-constrained optimization problem where flatness is encouraged as an\nobjective. We solve it explicitly via an augmented Lagrange dual approach and\nextend it further by proposing a generalized projection operation, resulting in\nnovel pruning methods called SAFE and its extension, SAFE$^+$. Extensive\nevaluations on standard image classification and language modeling tasks reveal\nthat SAFE consistently yields sparse networks with improved generalization\nperformance, which compares competitively to well-established baselines. In\naddition, SAFE demonstrates resilience to noisy data, making it well-suited for\nreal-world conditions.", "AI": {"tldr": "论文提出了一种新的剪枝方法SAFE和SAFE$^+$，通过同时优化稀疏性和平坦性来提升稀疏网络的泛化性能，并在噪声数据下表现稳健。", "motivation": "稀疏化神经网络通常会导致性能下降，现有方法难以恢复原始性能。受鲁棒优化研究的启发，作者旨在寻找同时稀疏且平坦的子网络。", "method": "将剪枝问题建模为稀疏约束优化问题，以平坦性为目标，采用增强拉格朗日对偶方法求解，并提出了广义投影操作。", "result": "在图像分类和语言建模任务中，SAFE方法生成的稀疏网络泛化性能优于基线方法，且对噪声数据具有鲁棒性。", "conclusion": "SAFE方法通过结合稀疏性和平坦性，显著提升了稀疏网络的性能，适用于实际场景。"}}
{"id": "2506.06870", "pdf": "https://arxiv.org/pdf/2506.06870", "abs": "https://arxiv.org/abs/2506.06870", "authors": ["Bugra Kilictas", "Faruk Alpay"], "title": "Recursive Semantic Anchoring in ISO 639:2023: A Structural Extension to ISO/TC 37 Frameworks", "categories": ["cs.LO", "cs.AI", "03B70, 18M05, 68T50", "F.4.1; I.2.7"], "comment": "21 pages, no figures. Includes formal proofs, RDF/Turtle ontology\n  schema, {\\phi}-index disambiguation cases, and evaluation of\n  transformer-based AI models under semantic drift", "summary": "ISO 639:2023 unifies the ISO language-code family and introduces contextual\nmetadata, but it lacks a machine-native mechanism for handling dialectal drift\nand creole mixtures. We propose a formalisation of recursive semantic\nanchoring, attaching to every language entity $\\chi$ a family of fixed-point\noperators $\\phi_{n,m}$ that model bounded semantic drift via the relation\n$\\phi_{n,m}(\\chi) = \\chi \\oplus \\Delta(\\chi)$, where $\\Delta(\\chi)$ is a drift\nvector in a latent semantic manifold. The base anchor $\\phi_{0,0}$ recovers the\ncanonical ISO 639:2023 identity, whereas $\\phi_{99,9}$ marks the maximal drift\nstate that triggers a deterministic fallback. Using category theory, we treat\nthe operators $\\phi_{n,m}$ as morphisms and drift vectors as arrows in a\ncategory $\\mathrm{DriftLang}$. A functor $\\Phi: \\mathrm{DriftLang} \\to\n\\mathrm{AnchorLang}$ maps every drifted object to its unique anchor and proves\nconvergence. We provide an RDF/Turtle schema (\\texttt{BaseLanguage},\n\\texttt{DriftedLanguage}, \\texttt{ResolvedAnchor}) and worked examples -- e.g.,\n$\\phi_{8,4}$ (Standard Mandarin) versus $\\phi_{8,7}$ (a colloquial variant),\nand $\\phi_{1,7}$ for Nigerian Pidgin anchored to English. Experiments with\ntransformer models show higher accuracy in language identification and\ntranslation on noisy or code-switched input when the $\\phi$-indices are used to\nguide fallback routing. The framework is compatible with ISO/TC 37 and provides\nan AI-tractable, drift-aware semantic layer for future standards.", "AI": {"tldr": "论文提出了一种递归语义锚定方法，用于处理语言变体和混合语的语义漂移问题，并通过实验验证了其在语言识别和翻译中的有效性。", "motivation": "ISO 639:2023缺乏处理方言漂移和混合语的机器原生机制，因此需要一种新的方法来建模语义漂移。", "method": "提出了一种基于固定点操作符的递归语义锚定方法，利用范畴论将漂移向量建模为范畴中的箭头，并通过RDF/Turtle模式实现。", "result": "实验表明，该方法在噪声或多语言混合输入的语言识别和翻译任务中提高了准确性。", "conclusion": "该框架与ISO/TC 37兼容，为未来标准提供了一个AI可处理的、漂移感知的语义层。"}}
{"id": "2506.06394", "pdf": "https://arxiv.org/pdf/2506.06394", "abs": "https://arxiv.org/abs/2506.06394", "authors": ["Yash Turkar", "Youngjin Kim", "Karthik Dantu"], "title": "Active Illumination Control in Low-Light Environments using NightHawk", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Subterranean environments such as culverts present significant challenges to\nrobot vision due to dim lighting and lack of distinctive features. Although\nonboard illumination can help, it introduces issues such as specular\nreflections, overexposure, and increased power consumption. We propose\nNightHawk, a framework that combines active illumination with exposure control\nto optimize image quality in these settings. NightHawk formulates an online\nBayesian optimization problem to determine the best light intensity and\nexposure-time for a given scene. We propose a novel feature detector-based\nmetric to quantify image utility and use it as the cost function for the\noptimizer. We built NightHawk as an event-triggered recursive optimization\npipeline and deployed it on a legged robot navigating a culvert beneath the\nErie Canal. Results from field experiments demonstrate improvements in feature\ndetection and matching by 47-197% enabling more reliable visual estimation in\nchallenging lighting conditions.", "AI": {"tldr": "NightHawk框架通过结合主动照明和曝光控制，优化了地下环境中的图像质量，显著提升了特征检测和匹配性能。", "motivation": "地下环境光线昏暗且缺乏特征，传统照明方法存在反光、过曝和功耗问题。", "method": "提出基于贝叶斯优化的在线框架，动态调整光照强度和曝光时间，并使用特征检测器量化图像效用。", "result": "实验显示特征检测和匹配性能提升47-197%，视觉估计更可靠。", "conclusion": "NightHawk有效解决了地下环境中的视觉挑战，提升了机器人导航能力。"}}
{"id": "2506.06874", "pdf": "https://arxiv.org/pdf/2506.06874", "abs": "https://arxiv.org/abs/2506.06874", "authors": ["Ala Yankouskaya", "Areej B. Babiker", "Syeda W. F. Rizvi", "Sameha Alshakhsi", "Magnus Liebherr", "Raian Ali"], "title": "LLM-D12: A Dual-Dimensional Scale of Instrumental and Relational Dependencies on Large Language Models", "categories": ["cs.HC", "cs.AI", "Human-Centered Computing -- > Human computer interaction (HCI) -->\n  HCI design and evaluation methods"], "comment": null, "summary": "There is growing interest in understanding how people interact with large\nlanguage models (LLMs) and whether such models elicit dependency or even\naddictive behaviour. Validated tools to assess the extent to which individuals\nmay become dependent on LLMs are scarce and primarily build on classic\nbehavioral addiction symptoms, adapted to the context of LLM use. We view this\nas a conceptual limitation, as the LLM-human relationship is more nuanced and\nwarrants a fresh and distinct perspective. To address this gap, we developed\nand validated a new 12-item questionnaire to measure LLM dependency, referred\nto as LLM-D12. The scale was based on the authors' prior theoretical work, with\nitems developed accordingly and responses collected from 526 participants in\nthe UK. Exploratory and confirmatory factor analyses, performed on separate\nhalves of the total sample using a split-sample approach, supported a\ntwo-factor structure: Instrumental Dependency (six items) and Relationship\nDependency (six items). Instrumental Dependency reflects the extent to which\nindividuals rely on LLMs to support or collaborate in decision-making and\ncognitive tasks. Relationship Dependency captures the tendency to perceive LLMs\nas socially meaningful, sentient, or companion-like entities. The two-factor\nstructure demonstrated excellent internal consistency and clear discriminant\nvalidity. External validation confirmed both the conceptual foundation and the\ndistinction between the two subscales. The psychometric properties and\nstructure of our LLM-D12 scale were interpreted in light of the emerging view\nthat dependency on LLMs does not necessarily indicate dysfunction but may still\nreflect reliance levels that could become problematic in certain contexts.", "AI": {"tldr": "论文开发了LLM-D12量表，用于测量人们对大型语言模型（LLM）的依赖程度，包括工具性依赖和关系性依赖两个维度。", "motivation": "现有评估LLM依赖的工具较少且基于传统行为成瘾症状，未能充分反映LLM与人类关系的复杂性。", "method": "基于理论框架开发12项问卷，收集526名英国参与者的数据，通过探索性和验证性因子分析验证量表结构。", "result": "量表分为工具性依赖（决策支持）和关系性依赖（社交感知）两个维度，具有良好的内部一致性和区分效度。", "conclusion": "LLM依赖不一定代表功能障碍，但可能在某些情境下成为问题，量表为研究提供了新视角。"}}
{"id": "2506.06400", "pdf": "https://arxiv.org/pdf/2506.06400", "abs": "https://arxiv.org/abs/2506.06400", "authors": ["Changsheng Fang", "Yongtong Liu", "Bahareh Morovati", "Shuo Han", "Yu Shi", "Li Zhou", "Shuyi Fan", "Hengyong Yu"], "title": "ResPF: Residual Poisson Flow for Efficient and Physically Consistent Sparse-View CT Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Sparse-view computed tomography (CT) is a practical solution to reduce\nradiation dose, but the resulting ill-posed inverse problem poses significant\nchallenges for accurate image reconstruction. Although deep learning and\ndiffusion-based methods have shown promising results, they often lack physical\ninterpretability or suffer from high computational costs due to iterative\nsampling starting from random noise. Recent advances in generative modeling,\nparticularly Poisson Flow Generative Models (PFGM), enable high-fidelity image\nsynthesis by modeling the full data distribution. In this work, we propose\nResidual Poisson Flow (ResPF) Generative Models for efficient and accurate\nsparse-view CT reconstruction. Based on PFGM++, ResPF integrates conditional\nguidance from sparse measurements and employs a hijacking strategy to\nsignificantly reduce sampling cost by skipping redundant initial steps.\nHowever, skipping early stages can degrade reconstruction quality and introduce\nunrealistic structures. To address this, we embed a data-consistency into each\niteration, ensuring fidelity to sparse-view measurements. Yet, PFGM sampling\nrelies on a fixed ordinary differential equation (ODE) trajectory induced by\nelectrostatic fields, which can be disrupted by step-wise data consistency,\nresulting in unstable or degraded reconstructions. Inspired by ResNet, we\nintroduce a residual fusion module to linearly combine generative outputs with\ndata-consistent reconstructions, effectively preserving trajectory continuity.\nTo the best of our knowledge, this is the first application of Poisson flow\nmodels to sparse-view CT. Extensive experiments on synthetic and clinical\ndatasets demonstrate that ResPF achieves superior reconstruction quality,\nfaster inference, and stronger robustness compared to state-of-the-art\niterative, learning-based, and diffusion models.", "AI": {"tldr": "提出了一种基于Poisson Flow生成模型（ResPF）的高效稀疏视图CT重建方法，通过条件引导和残差融合模块提升重建质量与速度。", "motivation": "稀疏视图CT可减少辐射剂量，但重建问题复杂，现有方法缺乏物理可解释性或计算成本高。", "method": "基于PFGM++，引入条件引导和残差融合模块，跳过冗余初始步骤并嵌入数据一致性。", "result": "在合成和临床数据集上，ResPF在重建质量、速度和鲁棒性上优于现有方法。", "conclusion": "ResPF首次将Poisson Flow模型应用于稀疏视图CT，展示了其高效性和优越性能。"}}
{"id": "2506.06904", "pdf": "https://arxiv.org/pdf/2506.06904", "abs": "https://arxiv.org/abs/2506.06904", "authors": ["Yuhan Helena Liu", "Guangyu Robert Yang", "Christopher J. Cueva"], "title": "Can Biologically Plausible Temporal Credit Assignment Rules Match BPTT for Neural Similarity? E-prop as an Example", "categories": ["cs.NE", "cs.AI", "q-bio.NC"], "comment": null, "summary": "Understanding how the brain learns may be informed by studying biologically\nplausible learning rules. These rules, often approximating gradient descent\nlearning to respect biological constraints such as locality, must meet two\ncritical criteria to be considered an appropriate brain model: (1) good\nneuroscience task performance and (2) alignment with neural recordings. While\nextensive research has assessed the first criterion, the second remains\nunderexamined. Employing methods such as Procrustes analysis on well-known\nneuroscience datasets, this study demonstrates the existence of a biologically\nplausible learning rule -- namely e-prop, which is based on gradient truncation\nand has demonstrated versatility across a wide range of tasks -- that can\nachieve neural data similarity comparable to Backpropagation Through Time\n(BPTT) when matched for task accuracy. Our findings also reveal that model\narchitecture and initial conditions can play a more significant role in\ndetermining neural similarity than the specific learning rule. Furthermore, we\nobserve that BPTT-trained models and their biologically plausible counterparts\nexhibit similar dynamical properties at comparable accuracies. These results\nunderscore the substantial progress made in developing biologically plausible\nlearning rules, highlighting their potential to achieve both competitive task\nperformance and neural data similarity.", "AI": {"tldr": "研究发现，基于梯度截断的生物合理学习规则（如e-prop）在任务精度匹配时，能达到与BPTT相当的神经数据相似性，且模型架构和初始条件对神经相似性的影响可能大于学习规则本身。", "motivation": "研究生物合理学习规则如何满足神经科学任务性能和神经记录对齐的双重标准，填补后者研究的不足。", "method": "采用Procrustes分析等方法，评估e-prop等学习规则在神经数据相似性上的表现，并与BPTT对比。", "result": "e-prop在任务精度匹配时与BPTT的神经数据相似性相当；模型架构和初始条件对神经相似性影响显著。", "conclusion": "生物合理学习规则在任务性能和神经数据相似性上均取得显著进展，展示了其潜力。"}}
{"id": "2506.06412", "pdf": "https://arxiv.org/pdf/2506.06412", "abs": "https://arxiv.org/abs/2506.06412", "authors": ["Junming Wang", "Yi Shi"], "title": "NeurNCD: Novel Class Discovery via Implicit Neural Representation", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted by ICMR 2024", "summary": "Discovering novel classes in open-world settings is crucial for real-world\napplications. Traditional explicit representations, such as object descriptors\nor 3D segmentation maps, are constrained by their discrete, hole-prone, and\nnoisy nature, which hinders accurate novel class discovery. To address these\nchallenges, we introduce NeurNCD, the first versatile and data-efficient\nframework for novel class discovery that employs the meticulously designed\nEmbedding-NeRF model combined with KL divergence as a substitute for\ntraditional explicit 3D segmentation maps to aggregate semantic embedding and\nentropy in visual embedding space. NeurNCD also integrates several key\ncomponents, including feature query, feature modulation and clustering,\nfacilitating efficient feature augmentation and information exchange between\nthe pre-trained semantic segmentation network and implicit neural\nrepresentations. As a result, our framework achieves superior segmentation\nperformance in both open and closed-world settings without relying on densely\nlabelled datasets for supervised training or human interaction to generate\nsparse label supervision. Extensive experiments demonstrate that our method\nsignificantly outperforms state-of-the-art approaches on the NYUv2 and Replica\ndatasets.", "AI": {"tldr": "NeurNCD是一种新型框架，通过结合Embedding-NeRF模型和KL散度，解决了开放世界中新类发现的挑战，无需密集标注数据或人工干预。", "motivation": "传统显式表示（如3D分割图）存在离散、噪声多等问题，限制了新类发现的准确性。", "method": "采用Embedding-NeRF模型和KL散度，结合特征查询、调制和聚类，实现高效特征增强和信息交换。", "result": "在NYUv2和Replica数据集上显著优于现有方法。", "conclusion": "NeurNCD在开放和封闭世界中均表现出色，无需依赖密集标注或人工干预。"}}
{"id": "2506.06907", "pdf": "https://arxiv.org/pdf/2506.06907", "abs": "https://arxiv.org/abs/2506.06907", "authors": ["Fred Xu", "Thomas Markovich"], "title": "Uncertainty Estimation on Graphs with Structure Informed Stochastic Partial Differential Equations", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Graph Neural Networks have achieved impressive results across diverse network\nmodeling tasks, but accurately estimating uncertainty on graphs remains\ndifficult, especially under distributional shifts. Unlike traditional\nuncertainty estimation, graph-based uncertainty must account for randomness\narising from both the graph's structure and its label distribution, which adds\ncomplexity. In this paper, making an analogy between the evolution of a\nstochastic partial differential equation (SPDE) driven by Matern Gaussian\nProcess and message passing using GNN layers, we present a principled way to\ndesign a novel message passing scheme that incorporates spatial-temporal noises\nmotivated by the Gaussian Process approach to SPDE. Our method simultaneously\ncaptures uncertainty across space and time and allows explicit control over the\ncovariance kernel smoothness, thereby enhancing uncertainty estimates on graphs\nwith both low and high label informativeness. Our extensive experiments on\nOut-of-Distribution (OOD) detection on graph datasets with varying label\ninformativeness demonstrate the soundness and superiority of our model to\nexisting approaches.", "AI": {"tldr": "提出了一种基于高斯过程的图神经网络消息传递方法，用于改进图数据中的不确定性估计，尤其在分布偏移情况下表现优越。", "motivation": "传统不确定性估计方法难以处理图数据中结构和标签分布带来的随机性，尤其是在分布偏移时。", "method": "通过将随机偏微分方程（SPDE）与高斯过程类比，设计了一种新的消息传递方案，结合时空噪声以捕捉不确定性。", "result": "在分布外检测实验中，该方法在多种标签信息量的图数据集上表现优于现有方法。", "conclusion": "该方法通过控制协方差核平滑度，显著提升了图数据不确定性估计的准确性。"}}
{"id": "2506.06440", "pdf": "https://arxiv.org/pdf/2506.06440", "abs": "https://arxiv.org/abs/2506.06440", "authors": ["Chuhao Chen", "Zhiyang Dou", "Chen Wang", "Yiming Huang", "Anjun Chen", "Qiao Feng", "Jiatao Gu", "Lingjie Liu"], "title": "Vid2Sim: Generalizable, Video-based Reconstruction of Appearance, Geometry and Physics for Mesh-free Simulation", "categories": ["cs.GR", "cs.CV"], "comment": "Accepted by CVPR 2025", "summary": "Faithfully reconstructing textured shapes and physical properties from videos\npresents an intriguing yet challenging problem. Significant efforts have been\ndedicated to advancing such a system identification problem in this area.\nPrevious methods often rely on heavy optimization pipelines with a\ndifferentiable simulator and renderer to estimate physical parameters. However,\nthese approaches frequently necessitate extensive hyperparameter tuning for\neach scene and involve a costly optimization process, which limits both their\npracticality and generalizability. In this work, we propose a novel framework,\nVid2Sim, a generalizable video-based approach for recovering geometry and\nphysical properties through a mesh-free reduced simulation based on Linear\nBlend Skinning (LBS), offering high computational efficiency and versatile\nrepresentation capability. Specifically, Vid2Sim first reconstructs the\nobserved configuration of the physical system from video using a feed-forward\nneural network trained to capture physical world knowledge. A lightweight\noptimization pipeline then refines the estimated appearance, geometry, and\nphysical properties to closely align with video observations within just a few\nminutes. Additionally, after the reconstruction, Vid2Sim enables high-quality,\nmesh-free simulation with high efficiency. Extensive experiments demonstrate\nthat our method achieves superior accuracy and efficiency in reconstructing\ngeometry and physical properties from video data.", "AI": {"tldr": "Vid2Sim是一种基于视频的通用框架，通过无网格简化模拟高效重建几何和物理属性。", "motivation": "从视频中忠实重建纹理形状和物理属性是一个具有挑战性的问题，现有方法依赖复杂的优化流程，效率低且泛化性差。", "method": "Vid2Sim结合前馈神经网络和轻量级优化流程，基于线性混合蒙皮（LBS）实现高效无网格模拟。", "result": "实验表明，Vid2Sim在重建几何和物理属性方面具有高精度和高效性。", "conclusion": "Vid2Sim提供了一种高效、通用的解决方案，显著提升了从视频中重建物理系统的能力。"}}
{"id": "2506.06917", "pdf": "https://arxiv.org/pdf/2506.06917", "abs": "https://arxiv.org/abs/2506.06917", "authors": ["Shangjie Du", "Hui Wei", "Dong Yoon Lee", "Zhizhang Hu", "Shijia Pan"], "title": "Graph-Based Physics-Guided Urban PM2.5 Air Quality Imputation with Constrained Monitoring Data", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by ACM Transactions on Sensor Networks (TOSN) 2025", "summary": "This work introduces GraPhy, a graph-based, physics-guided learning framework\nfor high-resolution and accurate air quality modeling in urban areas with\nlimited monitoring data. Fine-grained air quality monitoring information is\nessential for reducing public exposure to pollutants. However, monitoring\nnetworks are often sparse in socioeconomically disadvantaged regions, limiting\nthe accuracy and resolution of air quality modeling. To address this, we\npropose a physics-guided graph neural network architecture called GraPhy with\nlayers and edge features designed specifically for low-resolution monitoring\ndata. Experiments using data from California's socioeconomically disadvantaged\nSan Joaquin Valley show that GraPhy achieves the overall best performance\nevaluated by mean squared error (MSE), mean absolute error (MAE), and R-square\nvalue (R2), improving the performance by 9%-56% compared to various baseline\nmodels. Moreover, GraPhy consistently outperforms baselines across different\nspatial heterogeneity levels, demonstrating the effectiveness of our model\ndesign.", "AI": {"tldr": "GraPhy是一种基于图神经网络和物理引导的学习框架，用于高分辨率城市空气质量建模，特别适用于监测数据有限的地区。", "motivation": "在监测网络稀疏的社会经济弱势地区，空气质量建模的准确性和分辨率受限，而细粒度的空气质量信息对减少公众暴露于污染物至关重要。", "method": "提出了一种物理引导的图神经网络架构GraPhy，其层和边特征专为低分辨率监测数据设计。", "result": "在加州圣华金谷的实验表明，GraPhy在MSE、MAE和R2指标上表现最佳，性能提升9%-56%，且在不同空间异质性水平下均优于基线模型。", "conclusion": "GraPhy通过结合物理引导和图神经网络，有效提升了有限监测数据下的空气质量建模性能。"}}
{"id": "2506.06462", "pdf": "https://arxiv.org/pdf/2506.06462", "abs": "https://arxiv.org/abs/2506.06462", "authors": ["Nicolás Violante", "Andreas Meuleman", "Alban Gauthier", "Frédo Durand", "Thibault Groueix", "George Drettakis"], "title": "Splat and Replace: 3D Reconstruction with Repetitive Elements", "categories": ["cs.GR", "cs.CV"], "comment": "SIGGRAPH Conference Papers 2025. Project site:\n  https://repo-sam.inria.fr/nerphys/splat-and-replace/", "summary": "We leverage repetitive elements in 3D scenes to improve novel view synthesis.\nNeural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have greatly\nimproved novel view synthesis but renderings of unseen and occluded parts\nremain low-quality if the training views are not exhaustive enough. Our key\nobservation is that our environment is often full of repetitive elements. We\npropose to leverage those repetitions to improve the reconstruction of\nlow-quality parts of the scene due to poor coverage and occlusions. We propose\na method that segments each repeated instance in a 3DGS reconstruction,\nregisters them together, and allows information to be shared among instances.\nOur method improves the geometry while also accounting for appearance\nvariations across instances. We demonstrate our method on a variety of\nsynthetic and real scenes with typical repetitive elements, leading to a\nsubstantial improvement in the quality of novel view synthesis.", "AI": {"tldr": "利用3D场景中的重复元素提升新视角合成质量。", "motivation": "现有方法（如NeRF和3DGS）在训练视角不足时，对未见过或被遮挡部分的渲染质量较低。环境中常存在重复元素，可利用这些重复性提升重建质量。", "method": "提出一种方法，在3DGS重建中分割每个重复实例，将其对齐并共享信息，同时考虑实例间外观变化。", "result": "在合成和真实场景中验证了方法，显著提升了新视角合成的质量。", "conclusion": "通过利用重复元素，有效改善了场景重建和新视角合成的效果。"}}
{"id": "2506.06933", "pdf": "https://arxiv.org/pdf/2506.06933", "abs": "https://arxiv.org/abs/2506.06933", "authors": ["Mahdi Salmani", "Alireza Abdollahpoorrostam", "Seyed-Mohsen Moosavi-Dezfooli"], "title": "Rewriting the Budget: A General Framework for Black-Box Attacks Under Cost Asymmetry", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.CV"], "comment": null, "summary": "Traditional decision-based black-box adversarial attacks on image classifiers\naim to generate adversarial examples by slightly modifying input images while\nkeeping the number of queries low, where each query involves sending an input\nto the model and observing its output. Most existing methods assume that all\nqueries have equal cost. However, in practice, queries may incur asymmetric\ncosts; for example, in content moderation systems, certain output classes may\ntrigger additional review, enforcement, or penalties, making them more costly\nthan others. While prior work has considered such asymmetric cost settings,\neffective algorithms for this scenario remain underdeveloped. In this paper, we\npropose a general framework for decision-based attacks under asymmetric query\ncosts, which we refer to as asymmetric black-box attacks. We modify two core\ncomponents of existing attacks: the search strategy and the gradient estimation\nprocess. Specifically, we propose Asymmetric Search (AS), a more conservative\nvariant of binary search that reduces reliance on high-cost queries, and\nAsymmetric Gradient Estimation (AGREST), which shifts the sampling distribution\nto favor low-cost queries. We design efficient algorithms that minimize total\nattack cost by balancing different query types, in contrast to earlier methods\nsuch as stealthy attacks that focus only on limiting expensive (high-cost)\nqueries. Our method can be integrated into a range of existing black-box\nattacks with minimal changes. We perform both theoretical analysis and\nempirical evaluation on standard image classification benchmarks. Across\nvarious cost regimes, our method consistently achieves lower total query cost\nand smaller perturbations than existing approaches, with improvements of up to\n40% in some settings.", "AI": {"tldr": "本文提出了一种针对图像分类器的非对称黑盒攻击框架，通过优化搜索策略和梯度估计过程，显著降低了高成本查询的依赖，从而在非对称查询成本下实现更高效的攻击。", "motivation": "现有黑盒攻击方法通常假设所有查询成本相同，而实际应用中某些查询可能成本更高（如触发额外审查或惩罚）。针对这种非对称成本场景，现有算法效果不佳，因此需要开发更有效的攻击方法。", "method": "提出非对称搜索（AS）和非对称梯度估计（AGREST），前者减少对高成本查询的依赖，后者调整采样分布以偏向低成本查询。结合这两种方法，设计出能够平衡不同类型查询成本的攻击算法。", "result": "在标准图像分类基准测试中，该方法在不同成本条件下均表现优异，总查询成本和扰动幅度均低于现有方法，某些场景下提升高达40%。", "conclusion": "本文提出的非对称黑盒攻击框架通过优化查询策略，显著提升了攻击效率，适用于实际应用中非对称查询成本的场景。"}}
{"id": "2506.06633", "pdf": "https://arxiv.org/pdf/2506.06633", "abs": "https://arxiv.org/abs/2506.06633", "authors": ["Chi-Sheng Chen"], "title": "Vision-QRWKV: Exploring Quantum-Enhanced RWKV Models for Image Classification", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Recent advancements in quantum machine learning have shown promise in\nenhancing classical neural network architectures, particularly in domains\ninvolving complex, high-dimensional data. Building upon prior work in temporal\nsequence modeling, this paper introduces Vision-QRWKV, a hybrid\nquantum-classical extension of the Receptance Weighted Key Value (RWKV)\narchitecture, applied for the first time to image classification tasks. By\nintegrating a variational quantum circuit (VQC) into the channel mixing\ncomponent of RWKV, our model aims to improve nonlinear feature transformation\nand enhance the expressive capacity of visual representations.\n  We evaluate both classical and quantum RWKV models on a diverse collection of\n14 medical and standard image classification benchmarks, including MedMNIST\ndatasets, MNIST, and FashionMNIST. Our results demonstrate that the\nquantum-enhanced model outperforms its classical counterpart on a majority of\ndatasets, particularly those with subtle or noisy class distinctions (e.g.,\nChestMNIST, RetinaMNIST, BloodMNIST). This study represents the first\nsystematic application of quantum-enhanced RWKV in the visual domain, offering\ninsights into the architectural trade-offs and future potential of quantum\nmodels for lightweight and efficient vision tasks.", "AI": {"tldr": "论文提出了一种混合量子-经典架构Vision-QRWKV，首次应用于图像分类任务，通过集成变分量子电路（VQC）提升非线性特征变换能力。", "motivation": "量子机器学习在复杂高维数据领域展现出潜力，本文旨在扩展RWKV架构，探索量子模型在视觉任务中的应用。", "method": "在RWKV的通道混合组件中集成VQC，构建Vision-QRWKV模型，并在14个医学和标准图像分类基准上评估性能。", "result": "量子增强模型在多数数据集上优于经典模型，尤其在具有细微或噪声类别区分的数据集（如ChestMNIST）表现突出。", "conclusion": "研究首次系统地将量子增强RWKV应用于视觉领域，为轻量高效视觉任务的量子模型提供了架构权衡和未来潜力的见解。"}}
{"id": "2506.06946", "pdf": "https://arxiv.org/pdf/2506.06946", "abs": "https://arxiv.org/abs/2506.06946", "authors": ["Daniel Lawand", "Lucas Quaresma", "Roberto Bolgheroni", "Alfredo Goldman", "Renato Cordeiro Ferreira"], "title": "Is Your Training Pipeline Production-Ready? A Case Study in the Healthcare Domain", "categories": ["cs.SE", "cs.AI", "cs.LG", "D.2.11; D.2.7; I.2.7; I.5.4"], "comment": "9 pages, 3 figures (2 diagrams, 1 code listing), submitted to the\n  workshop SADIS 2025", "summary": "Deploying a Machine Learning (ML) training pipeline into production requires\nrobust software engineering practices. This differs significantly from\nexperimental workflows. This experience report investigates this challenge in\nSPIRA, a project whose goal is to create an ML-Enabled System (MLES) to\npre-diagnose insufficiency respiratory via speech analysis. The first version\nof SPIRA's training pipeline lacked critical software quality attributes. This\npaper presents an overview of the MLES, then compares three versions of the\narchitecture of the Continuous Training subsystem, which evolved from a Big\nBall of Mud, to a Modular Monolith, towards Microservices. By adopting\ndifferent design principles and patterns to enhance its maintainability,\nrobustness, and extensibility. In this way, the paper seeks to offer insights\nfor both ML Engineers tasked to productionize ML training pipelines and Data\nScientists seeking to adopt MLOps practices.", "AI": {"tldr": "论文探讨了将机器学习训练管道部署到生产环境中的挑战，并通过SPIRA项目的案例展示了架构从混乱到模块化再到微服务的演进过程。", "motivation": "研究机器学习训练管道在生产环境中的部署问题，强调软件工程实践的重要性，以提升系统的可维护性、健壮性和可扩展性。", "method": "通过SPIRA项目的案例研究，比较了三种架构版本（Big Ball of Mud、Modular Monolith、Microservices），并采用不同的设计原则和模式进行改进。", "result": "展示了架构演进如何提升系统的软件质量属性，为ML工程师和数据科学家提供了实践参考。", "conclusion": "论文为生产化ML训练管道和采用MLOps实践提供了有价值的见解，强调了设计原则和模式的重要性。"}}
{"id": "2506.06958", "pdf": "https://arxiv.org/pdf/2506.06958", "abs": "https://arxiv.org/abs/2506.06958", "authors": ["Chance Jiajie Li", "Jiayi Wu", "Zhenze Mo", "Ao Qu", "Yuhan Tang", "Kaiya Ivy Zhao", "Yulu Gan", "Jie Fan", "Jiangbo Yu", "Jinhua Zhao", "Paul Liang", "Luis Alonso", "Kent Larson"], "title": "Position: Simulating Society Requires Simulating Thought", "categories": ["cs.CY", "cs.AI", "cs.MA"], "comment": null, "summary": "Simulating society with large language models (LLMs), we argue, requires more\nthan generating plausible behavior -- it demands cognitively grounded reasoning\nthat is structured, revisable, and traceable. LLM-based agents are increasingly\nused to emulate individual and group behavior -- primarily through prompting\nand supervised fine-tuning. Yet they often lack internal coherence, causal\nreasoning, and belief traceability -- making them unreliable for analyzing how\npeople reason, deliberate, or respond to interventions.\n  To address this, we present a conceptual modeling paradigm, Generative Minds\n(GenMinds), which draws from cognitive science to support structured belief\nrepresentations in generative agents. To evaluate such agents, we introduce the\nRECAP (REconstructing CAusal Paths) framework, a benchmark designed to assess\nreasoning fidelity via causal traceability, demographic grounding, and\nintervention consistency. These contributions advance a broader shift: from\nsurface-level mimicry to generative agents that simulate thought -- not just\nlanguage -- for social simulations.", "AI": {"tldr": "论文提出了一种基于认知科学的生成式智能体（GenMinds）框架，旨在通过结构化信念表示提升LLM在模拟社会行为中的推理能力，并引入了RECAP评估框架。", "motivation": "当前基于LLM的智能体在模拟个体和群体行为时缺乏内部一致性、因果推理和信念可追溯性，限制了其在社会模拟中的可靠性。", "method": "提出了GenMinds框架，结合认知科学支持生成式智能体的结构化信念表示；设计了RECAP评估框架，通过因果可追溯性、人口统计基础和干预一致性评估推理保真度。", "result": "GenMinds和RECAP框架为生成式智能体提供了更可靠的推理能力，使其能够模拟思维而不仅仅是语言。", "conclusion": "该研究推动了从表面模仿到模拟思维的转变，为基于LLM的社会模拟提供了更可靠的工具。"}}
{"id": "2506.06664", "pdf": "https://arxiv.org/pdf/2506.06664", "abs": "https://arxiv.org/abs/2506.06664", "authors": ["Zhenxin Li", "Wenhao Yao", "Zi Wang", "Xinglong Sun", "Joshua Chen", "Nadine Chang", "Maying Shen", "Zuxuan Wu", "Shiyi Lan", "Jose M. Alvarez"], "title": "Generalized Trajectory Scoring for End-to-end Multimodal Planning", "categories": ["cs.RO", "cs.CV"], "comment": "The 1st place solution of the End-to-end Driving Track at the CVPR\n  2025 Autonomous Grand Challenge", "summary": "End-to-end multi-modal planning is a promising paradigm in autonomous\ndriving, enabling decision-making with diverse trajectory candidates. A key\ncomponent is a robust trajectory scorer capable of selecting the optimal\ntrajectory from these candidates. While recent trajectory scorers focus on\nscoring either large sets of static trajectories or small sets of dynamically\ngenerated ones, both approaches face significant limitations in generalization.\nStatic vocabularies provide effective coarse discretization but struggle to\nmake fine-grained adaptation, while dynamic proposals offer detailed precision\nbut fail to capture broader trajectory distributions. To overcome these\nchallenges, we propose GTRS (Generalized Trajectory Scoring), a unified\nframework for end-to-end multi-modal planning that combines coarse and\nfine-grained trajectory evaluation. GTRS consists of three complementary\ninnovations: (1) a diffusion-based trajectory generator that produces diverse\nfine-grained proposals; (2) a vocabulary generalization technique that trains a\nscorer on super-dense trajectory sets with dropout regularization, enabling its\nrobust inference on smaller subsets; and (3) a sensor augmentation strategy\nthat enhances out-of-domain generalization while incorporating refinement\ntraining for critical trajectory discrimination. As the winning solution of the\nNavsim v2 Challenge, GTRS demonstrates superior performance even with\nsub-optimal sensor inputs, approaching privileged methods that rely on\nground-truth perception. Code will be available at\nhttps://github.com/NVlabs/GTRS.", "AI": {"tldr": "GTRS提出了一种统一框架，结合粗粒度和细粒度轨迹评估，用于端到端多模态规划，解决了静态和动态轨迹评分方法的局限性。", "motivation": "静态轨迹集和动态生成轨迹集在泛化能力上存在显著局限性，需要一种统一的方法来结合两者的优势。", "method": "GTRS包含三个创新：扩散式轨迹生成器、词汇泛化技术和传感器增强策略。", "result": "GTRS在Navsim v2挑战赛中表现优异，接近依赖真实感知的特权方法。", "conclusion": "GTRS通过结合粗粒度和细粒度评估，显著提升了轨迹评分的泛化能力和性能。"}}
{"id": "2506.06677", "pdf": "https://arxiv.org/pdf/2506.06677", "abs": "https://arxiv.org/abs/2506.06677", "authors": ["Songhao Han", "Boxiang Qiu", "Yue Liao", "Siyuan Huang", "Chen Gao", "Shuicheng Yan", "Si Liu"], "title": "RoboCerebra: A Large-scale Benchmark for Long-horizon Robotic Manipulation Evaluation", "categories": ["cs.RO", "cs.CV"], "comment": "23 pages, 18 figures", "summary": "Recent advances in vision-language models (VLMs) have enabled\ninstruction-conditioned robotic systems with improved generalization. However,\nmost existing work focuses on reactive System 1 policies, underutilizing VLMs'\nstrengths in semantic reasoning and long-horizon planning. These System 2\ncapabilities-characterized by deliberative, goal-directed thinking-remain under\nexplored due to the limited temporal scale and structural complexity of current\nbenchmarks. To address this gap, we introduce RoboCerebra, a benchmark for\nevaluating high-level reasoning in long-horizon robotic manipulation.\nRoboCerebra includes: (1) a large-scale simulation dataset with extended task\nhorizons and diverse subtask sequences in household environments; (2) a\nhierarchical framework combining a high-level VLM planner with a low-level\nvision-language-action (VLA) controller; and (3) an evaluation protocol\ntargeting planning, reflection, and memory through structured System 1-System 2\ninteraction. The dataset is constructed via a top-down pipeline, where GPT\ngenerates task instructions and decomposes them into subtask sequences. Human\noperators execute the subtasks in simulation, yielding high-quality\ntrajectories with dynamic object variations. Compared to prior benchmarks,\nRoboCerebra features significantly longer action sequences and denser\nannotations. We further benchmark state-of-the-art VLMs as System 2 modules and\nanalyze their performance across key cognitive dimensions, advancing the\ndevelopment of more capable and generalizable robotic planners.", "AI": {"tldr": "RoboCerebra是一个新的基准测试，用于评估机器人操作中的高级推理能力，结合了视觉语言模型（VLM）的语义推理和长期规划能力。", "motivation": "现有研究多关注反应性策略（System 1），而忽略了VLM在语义推理和长期规划（System 2）中的潜力。RoboCerebra旨在填补这一空白。", "method": "通过大规模模拟数据集、分层框架（高级VLM规划器与低级视觉语言动作控制器结合）和结构化评估协议，测试System 1与System 2的交互。", "result": "RoboCerebra提供了更长的动作序列和更密集的标注，优于现有基准测试，并评估了先进VLM在System 2模块中的表现。", "conclusion": "RoboCerebra推动了更通用和强大的机器人规划器的发展，突出了VLM在高级推理中的潜力。"}}
{"id": "2506.06977", "pdf": "https://arxiv.org/pdf/2506.06977", "abs": "https://arxiv.org/abs/2506.06977", "authors": ["Pengfei Hu", "Xiaoxue Han", "Fei Wang", "Yue Ning"], "title": "UdonCare: Hierarchy Pruning for Unseen Domain Discovery in Predictive Healthcare", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Domain generalization has become a critical challenge in clinical prediction,\nwhere patient cohorts often exhibit shifting data distributions that degrade\nmodel performance. Typical domain generalization approaches struggle in\nreal-world healthcare settings for two main reasons: (1) patient-specific\ndomain labels are typically unavailable, making domain discovery especially\ndifficult; (2) purely data-driven approaches overlook key clinical insights,\nleading to a gap in medical knowledge integration. To address these problems,\nwe leverage hierarchical medical ontologies like the ICD-9-CM hierarchy to\ngroup diseases into higher-level categories and discover more flexible latent\ndomains. In this paper, we introduce UdonCare, a hierarchy-guided framework\nthat iteratively prunes fine-grained domains, encodes these refined domains,\nand applies a Siamese-type inference mechanism to separate domain-related\nsignals from patient-level features. Experimental results on clinical datasets\n(MIMIC-III and MIMIC-IV) show that the proposed model achieves higher\nperformance compared to other domain generalization baselines when substantial\ndomain gaps presents, highlighting the untapped potential of medical knowledge\nfor enhancing domain generalization in practical healthcare applications.", "AI": {"tldr": "UdonCare框架利用医学本体学发现潜在域，提升临床预测中的领域泛化能力。", "motivation": "解决临床预测中数据分布变化导致模型性能下降的问题，尤其是缺乏患者域标签和医学知识整合不足的挑战。", "method": "利用ICD-9-CM层次结构分组疾病，迭代修剪细粒度域，编码优化域，并采用Siamese推理机制分离域相关信号与患者特征。", "result": "在MIMIC-III和MIMIC-IV数据集上表现优于其他领域泛化基线，尤其在域差异显著时。", "conclusion": "UdonCare展示了医学知识在提升临床领域泛化中的潜力。"}}
{"id": "2506.06690", "pdf": "https://arxiv.org/pdf/2506.06690", "abs": "https://arxiv.org/abs/2506.06690", "authors": ["Hao Wang", "Chengkai Hou", "Xianglong Li", "Yankai Fu", "Chenxuan Li", "Ning Chen", "Gaole Dai", "Jiaming Liu", "Tiejun Huang", "Shanghang Zhang"], "title": "SpikePingpong: High-Frequency Spike Vision-based Robot Learning for Precise Striking in Table Tennis Game", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Learning to control high-speed objects in the real world remains a\nchallenging frontier in robotics. Table tennis serves as an ideal testbed for\nthis problem, demanding both rapid interception of fast-moving balls and\nprecise adjustment of their trajectories. This task presents two fundamental\nchallenges: it requires a high-precision vision system capable of accurately\npredicting ball trajectories, and it necessitates intelligent strategic\nplanning to ensure precise ball placement to target regions. The dynamic nature\nof table tennis, coupled with its real-time response requirements, makes it\nparticularly well-suited for advancing robotic control capabilities in\nfast-paced, precision-critical domains. In this paper, we present\nSpikePingpong, a novel system that integrates spike-based vision with imitation\nlearning for high-precision robotic table tennis. Our approach introduces two\nkey attempts that directly address the aforementioned challenges: SONIC, a\nspike camera-based module that achieves millimeter-level precision in\nball-racket contact prediction by compensating for real-world uncertainties\nsuch as air resistance and friction; and IMPACT, a strategic planning module\nthat enables accurate ball placement to targeted table regions. The system\nharnesses a 20 kHz spike camera for high-temporal resolution ball tracking,\ncombined with efficient neural network models for real-time trajectory\ncorrection and stroke planning. Experimental results demonstrate that\nSpikePingpong achieves a remarkable 91% success rate for 30 cm accuracy target\narea and 71% in the more challenging 20 cm accuracy task, surpassing previous\nstate-of-the-art approaches by 38% and 37% respectively. These significant\nperformance improvements enable the robust implementation of sophisticated\ntactical gameplay strategies, providing a new research perspective for robotic\ncontrol in high-speed dynamic tasks.", "AI": {"tldr": "SpikePingpong系统结合尖峰视觉与模仿学习，实现了高精度机器人乒乓球控制，成功率达到91%（30厘米目标区）和71%（20厘米目标区），超越现有方法。", "motivation": "乒乓球作为高速动态任务的理想测试平台，需要高精度视觉系统和智能策略规划，以解决快速拦截和精确轨迹调整的挑战。", "method": "系统包含SONIC（尖峰相机模块，实现毫米级接触预测）和IMPACT（策略规划模块，确保精确落点），结合20 kHz尖峰相机和实时神经网络。", "result": "实验显示，系统在30厘米和20厘米目标区的成功率分别为91%和71%，超越现有方法38%和37%。", "conclusion": "SpikePingpong为高速动态任务中的机器人控制提供了新视角，实现了战术策略的稳健实施。"}}
{"id": "2506.06980", "pdf": "https://arxiv.org/pdf/2506.06980", "abs": "https://arxiv.org/abs/2506.06980", "authors": ["Sajib Acharjee Dip", "Uddip Acharjee Shuvo", "Dipanwita Mallick", "Abrar Rahman Abir", "Liqing Zhang"], "title": "MoXGATE: Modality-aware cross-attention for multi-omic gastrointestinal cancer sub-type classification", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages, 1 figure, 6 tables", "summary": "Cancer subtype classification is crucial for personalized treatment and\nprognostic assessment. However, effectively integrating multi-omic data remains\nchallenging due to the heterogeneous nature of genomic, epigenomic, and\ntranscriptomic features. In this work, we propose Modality-Aware\nCross-Attention MoXGATE, a novel deep-learning framework that leverages\ncross-attention and learnable modality weights to enhance feature fusion across\nmultiple omics sources. Our approach effectively captures inter-modality\ndependencies, ensuring robust and interpretable integration. Through\nexperiments on Gastrointestinal Adenocarcinoma (GIAC) and Breast Cancer (BRCA)\ndatasets from TCGA, we demonstrate that MoXGATE outperforms existing methods,\nachieving 95\\% classification accuracy. Ablation studies validate the\neffectiveness of cross-attention over simple concatenation and highlight the\nimportance of different omics modalities. Moreover, our model generalizes well\nto unseen cancer types e.g., breast cancer, underscoring its adaptability. Key\ncontributions include (1) a cross-attention-based multi-omic integration\nframework, (2) modality-weighted fusion for enhanced interpretability, (3)\napplication of focal loss to mitigate data imbalance, and (4) validation across\nmultiple cancer subtypes. Our results indicate that MoXGATE is a promising\napproach for multi-omic cancer subtype classification, offering improved\nperformance and biological generalizability.", "AI": {"tldr": "MoXGATE是一种新型深度学习框架，通过跨注意力和可学习模态权重整合多组学数据，显著提升癌症亚型分类性能。", "motivation": "癌症亚型分类对个性化治疗和预后评估至关重要，但多组学数据的异质性导致有效整合具有挑战性。", "method": "提出Modality-Aware Cross-Attention MoXGATE框架，利用跨注意力和模态权重实现多组学特征融合，并应用焦点损失解决数据不平衡问题。", "result": "在GIAC和BRCA数据集上达到95%的分类准确率，优于现有方法，且能泛化到未见过的癌症类型。", "conclusion": "MoXGATE在多组学癌症亚型分类中表现出优越性能和生物通用性，为精准医疗提供有力工具。"}}
{"id": "2506.06999", "pdf": "https://arxiv.org/pdf/2506.06999", "abs": "https://arxiv.org/abs/2506.06999", "authors": ["Arun Sharma", "Mingzhou Yang", "Majid Farhadloo", "Subhankar Ghosh", "Bharat Jayaprakash", "Shashi Shekhar"], "title": "Towards Physics-informed Diffusion for Anomaly Detection in Trajectories", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": null, "summary": "Given trajectory data, a domain-specific study area, and a user-defined\nthreshold, we aim to find anomalous trajectories indicative of possible GPS\nspoofing (e.g., fake trajectory). The problem is societally important to curb\nillegal activities in international waters, such as unauthorized fishing and\nillicit oil transfers. The problem is challenging due to advances in AI\ngenerated in deep fakes generation (e.g., additive noise, fake trajectories)\nand lack of adequate amount of labeled samples for ground-truth verification.\nRecent literature shows promising results for anomalous trajectory detection\nusing generative models despite data sparsity. However, they do not consider\nfine-scale spatiotemporal dependencies and prior physical knowledge, resulting\nin higher false-positive rates. To address these limitations, we propose a\nphysics-informed diffusion model that integrates kinematic constraints to\nidentify trajectories that do not adhere to physical laws. Experimental results\non real-world datasets in the maritime and urban domains show that the proposed\nframework results in higher prediction accuracy and lower estimation error rate\nfor anomaly detection and trajectory generation methods, respectively. Our\nimplementation is available at\nhttps://github.com/arunshar/Physics-Informed-Diffusion-Probabilistic-Model.", "AI": {"tldr": "提出了一种基于物理知识的扩散模型，用于检测异常轨迹，尤其是在GPS欺骗场景下，结合运动学约束以提高准确性。", "motivation": "解决国际水域非法活动（如非法捕鱼和石油走私）中的GPS欺骗问题，现有方法因忽略细粒度时空依赖和物理知识导致高误报率。", "method": "提出了一种物理知识驱动的扩散模型，整合运动学约束以识别不符合物理规律的轨迹。", "result": "在真实数据集（海事和城市领域）上实验表明，该方法在异常检测和轨迹生成方面具有更高的预测准确性和更低的误差率。", "conclusion": "该方法通过引入物理知识显著提升了异常轨迹检测的性能，为GPS欺骗检测提供了有效工具。"}}
{"id": "2506.06761", "pdf": "https://arxiv.org/pdf/2506.06761", "abs": "https://arxiv.org/abs/2506.06761", "authors": ["Adrià Molina Rodríguez", "Oriol Ramos Terrades", "Josep Lladós"], "title": "The OCR Quest for Generalization: Learning to recognize low-resource alphabets with model editing", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Achieving robustness in recognition systems across diverse domains is crucial\nfor their practical utility. While ample data availability is usually assumed,\nlow-resource languages, such as ancient manuscripts and non-western languages,\ntend to be kept out of the equations of massive pretraining and foundational\ntechniques due to an under representation. In this work, we aim for building\nmodels which can generalize to new distributions of data, such as alphabets,\nfaster than centralized fine-tune strategies. For doing so, we take advantage\nof the recent advancements in model editing to enhance the incorporation of\nunseen scripts (low-resource learning). In contrast to state-of-the-art\nmeta-learning, we showcase the effectiveness of domain merging in sparse\ndistributions of data, with agnosticity of its relation to the overall\ndistribution or any other prototyping necessity. Even when using the same exact\ntraining data, our experiments showcase significant performance boosts in\n\\textbf{transfer learning} to new alphabets and \\textbf{out-of-domain\nevaluation} in challenging domain shifts, including historical ciphered texts\nand non-Latin scripts. This research contributes a novel approach into building\nmodels that can easily adopt under-represented alphabets and, therefore, enable\ndocument recognition to a wider set of contexts and cultures.", "AI": {"tldr": "论文提出了一种通过模型编辑增强低资源语言学习的方法，优于传统元学习，显著提升了跨域识别性能。", "motivation": "解决低资源语言（如古代手稿和非西方语言）在识别系统中因数据不足而被忽视的问题。", "method": "利用模型编辑技术，结合领域合并策略，提升对未见脚本的泛化能力。", "result": "实验表明，该方法在迁移学习和跨域评估中显著优于现有方法。", "conclusion": "该方法为低资源语言的文档识别提供了新思路，扩展了模型的适用性。"}}
{"id": "2506.07003", "pdf": "https://arxiv.org/pdf/2506.07003", "abs": "https://arxiv.org/abs/2506.07003", "authors": ["Utkarsh Utkarsh", "Danielle C. Maddix", "Ruijun Ma", "Michael W. Mahoney", "Yuyang Wang"], "title": "End-to-End Probabilistic Framework for Learning with Hard Constraints", "categories": ["cs.LG", "cs.AI", "cs.CE"], "comment": "46 pages, 5 figures, 10 tables", "summary": "We present a general purpose probabilistic forecasting framework,\nProbHardE2E, to learn systems that can incorporate operational/physical\nconstraints as hard requirements. ProbHardE2E enforces hard constraints by\nexploiting variance information in a novel way; and thus it is also capable of\nperforming uncertainty quantification (UQ) on the model. Our methodology uses a\nnovel differentiable probabilistic projection layer (DPPL) that can be combined\nwith a wide range of neural network architectures. This DPPL allows the model\nto learn the system in an end-to-end manner, compared to other approaches where\nthe constraints are satisfied either through a post-processing step or at\ninference. In addition, ProbHardE2E can optimize a strictly proper scoring\nrule, without making any distributional assumptions on the target, which\nenables it to obtain robust distributional estimates (in contrast to existing\napproaches that generally optimize likelihood-based objectives, which are\nheavily biased by their distributional assumptions and model choices); and it\ncan incorporate a range of non-linear constraints (increasing the power of\nmodeling and flexibility). We apply ProbHardE2E to problems in learning partial\ndifferential equations with uncertainty estimates and to probabilistic\ntime-series forecasting, showcasing it as a broadly applicable general setup\nthat connects these seemingly disparate domains.", "AI": {"tldr": "ProbHardE2E是一个通用的概率预测框架，通过新颖的可微分概率投影层（DPPL）实现硬约束，支持端到端学习，并能优化严格评分规则。", "motivation": "现有方法通常通过后处理或推理满足约束，且依赖分布假设，限制了模型的灵活性和鲁棒性。", "method": "利用DPPL层结合神经网络架构，支持端到端学习，并能处理非线性约束。", "result": "在偏微分方程学习和概率时间序列预测中展示了广泛适用性。", "conclusion": "ProbHardE2E是一个通用且灵活的框架，能够连接不同领域的问题。"}}
{"id": "2506.07006", "pdf": "https://arxiv.org/pdf/2506.07006", "abs": "https://arxiv.org/abs/2506.07006", "authors": ["Zechen Hu", "Tong Xu", "Xuesu Xiao", "Xuan Wang"], "title": "CARoL: Context-aware Adaptation for Robot Learning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Using Reinforcement Learning (RL) to learn new robotic tasks from scratch is\noften inefficient. Leveraging prior knowledge has the potential to\nsignificantly enhance learning efficiency, which, however, raises two critical\nchallenges: how to determine the relevancy of existing knowledge and how to\nadaptively integrate them into learning a new task. In this paper, we propose\nContext-aware Adaptation for Robot Learning (CARoL), a novel framework to\nefficiently learn a similar but distinct new task from prior knowledge. CARoL\nincorporates context awareness by analyzing state transitions in system\ndynamics to identify similarities between the new task and prior knowledge. It\nthen utilizes these identified similarities to prioritize and adapt specific\nknowledge pieces for the new task. Additionally, CARoL has a broad\napplicability spanning policy-based, value-based, and actor-critic RL\nalgorithms. We validate the efficiency and generalizability of CARoL on both\nsimulated robotic platforms and physical ground vehicles. The simulations\ninclude CarRacing and LunarLander environments, where CARoL demonstrates faster\nconvergence and higher rewards when learning policies for new tasks. In\nreal-world experiments, we show that CARoL enables a ground vehicle to quickly\nand efficiently adapt policies learned in simulation to smoothly traverse\nreal-world off-road terrain.", "AI": {"tldr": "CARoL是一个基于上下文感知的机器人学习框架，通过分析系统动态中的状态转移来识别新任务与先验知识的相似性，并自适应地整合相关知识以提高学习效率。", "motivation": "利用先验知识提升机器人学习新任务的效率，但需要解决如何确定相关性和如何自适应整合的挑战。", "method": "提出CARoL框架，通过上下文感知分析状态转移，识别相似性并优先整合相关知识，适用于多种RL算法。", "result": "在模拟和真实实验中，CARoL表现出更快的收敛速度和更高的奖励，成功将仿真策略迁移到真实地形。", "conclusion": "CARoL通过上下文感知和自适应整合，显著提升了机器人学习新任务的效率和泛化能力。"}}
{"id": "2506.07008", "pdf": "https://arxiv.org/pdf/2506.07008", "abs": "https://arxiv.org/abs/2506.07008", "authors": ["Fatemeh Pourahmadian", "Yang Xu"], "title": "Deep regularization networks for inverse problems with noisy operators", "categories": ["math.NA", "cs.AI", "cs.NA", "eess.SP"], "comment": null, "summary": "A supervised learning approach is proposed for regularization of large\ninverse problems where the main operator is built from noisy data. This is\ngermane to superresolution imaging via the sampling indicators of the inverse\nscattering theory. We aim to accelerate the spatiotemporal regularization\nprocess for this class of inverse problems to enable real-time imaging. In this\napproach, a neural operator maps each pattern on the right-hand side of the\nscattering equation to its affiliated regularization parameter. The network is\ntrained in two steps which entails: (1) training on low-resolution\nregularization maps furnished by the Morozov discrepancy principle with\nnonoptimal thresholds, and (2) optimizing network predictions through\nminimization of the Tikhonov loss function regulated by the validation loss.\nStep 2 allows for tailoring of the approximate maps of Step 1 toward\nconstruction of higher quality images. This approach enables direct learning\nfrom test data and dispenses with the need for a-priori knowledge of the\noptimal regularization maps. The network, trained on low-resolution data,\nquickly generates dense regularization maps for high-resolution imaging. We\nhighlight the importance of the training loss function on the network's\ngeneralizability. In particular, we demonstrate that networks informed by the\nlogic of discrepancy principle lead to images of higher contrast. In this case,\nthe training process involves many-objective optimization. We propose a new\nmethod to adaptively select the appropriate loss weights during training\nwithout requiring an additional optimization process. The proposed approach is\nsynthetically examined for imaging damage evolution in an elastic plate. The\nresults indicate that the discrepancy-informed regularization networks not only\naccelerate the imaging process, but also remarkably enhance the image quality\nin complex environments.", "AI": {"tldr": "提出了一种监督学习方法，用于正则化由噪声数据构建的大型逆问题，旨在通过神经网络加速实时成像过程。", "motivation": "解决由噪声数据构建的逆问题在超分辨率成像中的正则化需求，加速时空正则化以实现实时成像。", "method": "采用两步训练神经网络：先基于低分辨率正则化图训练，再通过Tikhonov损失函数优化预测。", "result": "网络能够快速生成高质量的正则化图，显著提升成像速度和质量。", "conclusion": "该方法不仅加速了成像过程，还在复杂环境中显著提升了图像质量。"}}
{"id": "2506.06884", "pdf": "https://arxiv.org/pdf/2506.06884", "abs": "https://arxiv.org/abs/2506.06884", "authors": ["Divya Jyoti Bajpai", "Manjesh Kumar Hanawal"], "title": "FREE: Fast and Robust Vision Language Models with Early Exits", "categories": ["cs.LG", "cs.CV"], "comment": "To appear at the Association of Computational Linguistics (ACL) 2025\n  Conference", "summary": "In recent years, Vision-Language Models (VLMs) have shown remarkable\nperformance improvements in Vision-Language tasks. However, their large size\nposes challenges for real-world applications where inference latency is a\nconcern. To tackle this issue, we propose employing Early Exit (EE) strategies\nin VLMs. However, training exit classifiers in VLMs is challenging,\nparticularly with limited labeled training data. To address this, we introduce\nFREE, an adversarial training approach within a GAN-based framework. Here, each\nexit consists of a transformer layer and a classifier. The transformer layer is\nadversarially trained to produce feature representations similar to the final\nlayer, while a feature classifier serves as the discriminator. Our method\nfocuses on performing input-adaptive inference that increases inference speed\nwith minimal drop in performance. Experimental results demonstrate the\neffectiveness of our approach in enhancing accuracy and model robustness by\nmitigating overthinking and the phenomenon of mid-crisis that we highlight. We\nexperimentally validate that our method speeds up the inference process by more\nthan 1.51x while retaining comparable performance. The source code is available\nat https://github.com/Div290/FREE.", "AI": {"tldr": "论文提出了一种名为FREE的方法，通过对抗训练在GAN框架中优化视觉语言模型（VLMs）的早期退出策略，以提高推理速度并保持性能。", "motivation": "视觉语言模型（VLMs）在性能提升的同时，其大尺寸导致推理延迟问题，限制了实际应用。", "method": "采用对抗训练方法（FREE），每个退出点包含一个Transformer层和分类器，通过GAN框架训练以生成与最终层相似的特征表示。", "result": "实验表明，该方法将推理速度提升1.51倍以上，同时保持性能，并缓解了过度思考和中间危机现象。", "conclusion": "FREE方法有效提高了VLMs的推理效率和鲁棒性，适用于实际应用。"}}
{"id": "2506.06890", "pdf": "https://arxiv.org/pdf/2506.06890", "abs": "https://arxiv.org/abs/2506.06890", "authors": ["Sumit Sharma", "Gopi Raju Matta", "Kaushik Mitra"], "title": "SPC to 3D: Novel View Synthesis from Binary SPC via I2I translation", "categories": ["eess.IV", "cs.CV", "eess.SP"], "comment": "Accepted for publication at ICIP 2025", "summary": "Single Photon Avalanche Diodes (SPADs) represent a cutting-edge imaging\ntechnology, capable of detecting individual photons with remarkable timing\nprecision. Building on this sensitivity, Single Photon Cameras (SPCs) enable\nimage capture at exceptionally high speeds under both low and high\nillumination. Enabling 3D reconstruction and radiance field recovery from such\nSPC data holds significant promise. However, the binary nature of SPC images\nleads to severe information loss, particularly in texture and color, making\ntraditional 3D synthesis techniques ineffective. To address this challenge, we\npropose a modular two-stage framework that converts binary SPC images into\nhigh-quality colorized novel views. The first stage performs image-to-image\n(I2I) translation using generative models such as Pix2PixHD, converting binary\nSPC inputs into plausible RGB representations. The second stage employs 3D\nscene reconstruction techniques like Neural Radiance Fields (NeRF) or Gaussian\nSplatting (3DGS) to generate novel views. We validate our two-stage pipeline\n(Pix2PixHD + Nerf/3DGS) through extensive qualitative and quantitative\nexperiments, demonstrating significant improvements in perceptual quality and\ngeometric consistency over the alternative baseline.", "AI": {"tldr": "提出了一种两阶段框架，将二进制SPC图像转换为高质量彩色新视图，解决了传统3D合成技术因信息丢失而无效的问题。", "motivation": "SPC数据在3D重建和辐射场恢复方面具有潜力，但二进制图像导致纹理和颜色信息严重丢失，传统方法失效。", "method": "第一阶段使用Pix2PixHD进行图像到图像转换，第二阶段采用NeRF或3DGS进行3D场景重建和新视图生成。", "result": "实验验证表明，该方法在感知质量和几何一致性上显著优于基线。", "conclusion": "提出的两阶段框架有效解决了SPC数据的3D合成问题，为高精度成像提供了新思路。"}}
{"id": "2506.07022", "pdf": "https://arxiv.org/pdf/2506.07022", "abs": "https://arxiv.org/abs/2506.07022", "authors": ["Leheng Sheng", "Changshuo Shen", "Weixiang Zhao", "Junfeng Fang", "Xiaohao Liu", "Zhenkai Liang", "Xiang Wang", "An Zhang", "Tat-Seng Chua"], "title": "AlphaSteer: Learning Refusal Steering with Principled Null-Space Constraint", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "As LLMs are increasingly deployed in real-world applications, ensuring their\nability to refuse malicious prompts, especially jailbreak attacks, is essential\nfor safe and reliable use. Recently, activation steering has emerged as an\neffective approach for enhancing LLM safety by adding a refusal direction\nvector to internal activations of LLMs during inference, which will further\ninduce the refusal behaviors of LLMs. However, indiscriminately applying\nactivation steering fundamentally suffers from the trade-off between safety and\nutility, since the same steering vector can also lead to over-refusal and\ndegraded performance on benign prompts. Although prior efforts, such as vector\ncalibration and conditional steering, have attempted to mitigate this\ntrade-off, their lack of theoretical grounding limits their robustness and\neffectiveness. To better address the trade-off between safety and utility, we\npresent a theoretically grounded and empirically effective activation steering\nmethod called AlphaSteer. Specifically, it considers activation steering as a\nlearnable process with two principled learning objectives: utility preservation\nand safety enhancement. For utility preservation, it learns to construct a\nnearly zero vector for steering benign data, with the null-space constraints.\nFor safety enhancement, it learns to construct a refusal direction vector for\nsteering malicious data, with the help of linear regression. Experiments across\nmultiple jailbreak attacks and utility benchmarks demonstrate the effectiveness\nof AlphaSteer, which significantly improves the safety of LLMs without\ncompromising general capabilities. Our codes are available at\nhttps://github.com/AlphaLab-USTC/AlphaSteer.", "AI": {"tldr": "AlphaSteer是一种基于理论的方法，通过激活导向提升LLMs的安全性，同时避免对良性提示的过度拒绝。", "motivation": "随着LLMs在现实应用中的部署增加，确保其能够拒绝恶意提示（如越狱攻击）变得至关重要。现有的激活导向方法在安全性和实用性之间存在权衡。", "method": "AlphaSteer将激活导向视为可学习过程，通过两个目标：实用性保持（构建接近零的导向向量）和安全性增强（构建拒绝导向向量）。", "result": "实验表明，AlphaSteer显著提升了LLMs的安全性，同时未损害其通用能力。", "conclusion": "AlphaSteer是一种理论支持且实证有效的方法，解决了安全性与实用性之间的权衡问题。"}}
{"id": "2506.07023", "pdf": "https://arxiv.org/pdf/2506.07023", "abs": "https://arxiv.org/abs/2506.07023", "authors": ["Suman Mahapatra", "Pradipta Maji"], "title": "Optimal Transport Driven Asymmetric Image-to-Image Translation for Nuclei Segmentation of Histological Images", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "13 pages, 8 figures", "summary": "Segmentation of nuclei regions from histological images enables morphometric\nanalysis of nuclei structures, which in turn helps in the detection and\ndiagnosis of diseases under consideration. To develop a nuclei segmentation\nalgorithm, applicable to different types of target domain representations,\nimage-to-image translation networks can be considered as they are invariant to\ntarget domain image representations. One of the important issues with\nimage-to-image translation models is that they fail miserably when the\ninformation content between two image domains are asymmetric in nature. In this\nregard, the paper introduces a new deep generative model for segmenting nuclei\nstructures from histological images. The proposed model considers an embedding\nspace for handling information-disparity between information-rich histological\nimage space and information-poor segmentation map domain. Integrating\njudiciously the concepts of optimal transport and measure theory, the model\ndevelops an invertible generator, which provides an efficient optimization\nframework with lower network complexity. The concept of invertible generator\nautomatically eliminates the need of any explicit cycle-consistency loss. The\nproposed model also introduces a spatially-constrained squeeze operation within\nthe framework of invertible generator to maintain spatial continuity within the\nimage patches. The model provides a better trade-off between network complexity\nand model performance compared to other existing models having complex network\narchitectures. The performance of the proposed deep generative model, along\nwith a comparison with state-of-the-art nuclei segmentation methods, is\ndemonstrated on publicly available histological image data sets.", "AI": {"tldr": "提出了一种新的深度生成模型，用于从组织学图像中分割细胞核结构，解决了信息不对称问题，并通过可逆生成器和空间约束优化了性能。", "motivation": "组织学图像中细胞核区域的分割有助于疾病的检测和诊断，但现有图像到图像转换模型在信息不对称时表现不佳。", "method": "引入了一种深度生成模型，利用嵌入空间处理信息不对称问题，结合最优传输和测度理论，开发了可逆生成器，并引入空间约束挤压操作。", "result": "模型在公开数据集上表现优于现有方法，实现了网络复杂度和性能的更好平衡。", "conclusion": "该模型为细胞核分割提供了一种高效且性能优越的解决方案。"}}
{"id": "2506.07028", "pdf": "https://arxiv.org/pdf/2506.07028", "abs": "https://arxiv.org/abs/2506.07028", "authors": ["Suman Mahapatra", "Pradipta Maji"], "title": "SiliCoN: Simultaneous Nuclei Segmentation and Color Normalization of Histological Images", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "10 pages, 9 figures", "summary": "Segmentation of nuclei regions from histological images is an important task\nfor automated computer-aided analysis of histological images, particularly in\nthe presence of impermissible color variation in the color appearance of\nstained tissue images. While color normalization enables better nuclei\nsegmentation, accurate segmentation of nuclei structures makes color\nnormalization rather trivial. In this respect, the paper proposes a novel deep\ngenerative model for simultaneously segmenting nuclei structures and\nnormalizing color appearance of stained histological images.This model\njudiciously integrates the merits of truncated normal distribution and spatial\nattention. The model assumes that the latent color appearance information,\ncorresponding to a particular histological image, is independent of respective\nnuclei segmentation map as well as embedding map information. The disentangled\nrepresentation makes the model generalizable and adaptable as the modification\nor loss in color appearance information cannot be able to affect the nuclei\nsegmentation map as well as embedding information. Also, for dealing with the\nstain overlap of associated histochemical reagents, the prior for latent color\nappearance code is assumed to be a mixture of truncated normal distributions.\nThe proposed model incorporates the concept of spatial attention for\nsegmentation of nuclei regions from histological images. The performance of the\nproposed approach, along with a comparative analysis with related\nstate-of-the-art algorithms, has been demonstrated on publicly available\nstandard histological image data sets.", "AI": {"tldr": "论文提出了一种新型深度生成模型，用于同时分割细胞核结构和标准化染色组织图像的颜色外观，解决了颜色变异和染色重叠的问题。", "motivation": "组织图像中细胞核区域的自动分割对计算机辅助分析至关重要，但颜色变异和染色重叠影响了分割的准确性。", "method": "模型结合了截断正态分布和空间注意力的优点，假设颜色外观信息与细胞核分割图独立，采用混合截断正态分布处理染色重叠。", "result": "在公开数据集上验证了模型的性能，并与现有算法进行了比较分析。", "conclusion": "该模型具有通用性和适应性，颜色外观信息的修改或丢失不会影响细胞核分割结果。"}}
{"id": "2506.06938", "pdf": "https://arxiv.org/pdf/2506.06938", "abs": "https://arxiv.org/abs/2506.06938", "authors": ["Bastian Jäckl", "Vojtěch Kloda", "Daniel A. Keim", "Jakub Lokoč"], "title": "Experimental Evaluation of Static Image Sub-Region-Based Search Models Using CLIP", "categories": ["cs.MM", "cs.CV", "68U10", "H.3.3; I.4.10; H.2.8"], "comment": "14 pages, 4 figures, 2 tables", "summary": "Advances in multimodal text-image models have enabled effective text-based\nquerying in extensive image collections. While these models show convincing\nperformance for everyday life scenes, querying in highly homogeneous,\nspecialized domains remains challenging. The primary problem is that users can\noften provide only vague textual descriptions as they lack expert knowledge to\ndiscriminate between homogenous entities. This work investigates whether adding\nlocation-based prompts to complement these vague text queries can enhance\nretrieval performance. Specifically, we collected a dataset of 741 human\nannotations, each containing short and long textual descriptions and bounding\nboxes indicating regions of interest in challenging underwater scenes. Using\nthese annotations, we evaluate the performance of CLIP when queried on various\nstatic sub-regions of images compared to the full image. Our results show that\nboth a simple 3-by-3 partitioning and a 5-grid overlap significantly improve\nretrieval effectiveness and remain robust to perturbations of the annotation\nbox.", "AI": {"tldr": "研究探讨了在高度同质的专业领域中，通过添加基于位置的提示来增强模糊文本查询的检索性能。", "motivation": "在多模态文本-图像模型中，专业领域的查询因用户缺乏专业知识而难以精准描述，导致检索效果不佳。", "method": "收集了741个人工标注的数据集，包含短/长文本描述和感兴趣区域的边界框，评估CLIP模型在不同静态子区域的查询性能。", "result": "简单的3x3分区和5网格重叠显著提高了检索效果，并对标注框的扰动保持稳健。", "conclusion": "基于位置的提示可以有效弥补模糊文本查询的不足，提升专业领域的检索性能。"}}
{"id": "2506.07035", "pdf": "https://arxiv.org/pdf/2506.07035", "abs": "https://arxiv.org/abs/2506.07035", "authors": ["Zixuan Jiang", "Renjing Xu"], "title": "AnnoDPO: Protein Functional Annotation Learning with Direct Preference Optimization", "categories": ["q-bio.BM", "cs.AI"], "comment": null, "summary": "Deciphering protein function remains a fundamental challenge in protein\nrepresentation learning. The task presents significant difficulties for protein\nlanguage models (PLMs) due to the sheer volume of functional annotation\ncategories and the highly imbalanced distribution of annotated instances across\nbiological ontologies. Inspired by the remarkable success of reinforcement\nlearning from human feedback (RLHF) in large language model (LLM) alignment, we\npropose AnnoDPO, a novel multi-modal framework for protein function prediction\nthat leverages Direct Preference Optimization (DPO) to enhance annotation\nlearning. Our methodology addresses the dual challenges of annotation scarcity\nand category imbalance through preference-aligned training objectives,\nestablishing a new paradigm for biological knowledge integration in protein\nrepresentation learning.", "AI": {"tldr": "AnnoDPO是一个多模态框架，利用直接偏好优化（DPO）改进蛋白质功能预测，解决注释稀缺和类别不平衡问题。", "motivation": "蛋白质功能注释的挑战在于类别繁多且分布不平衡，现有蛋白质语言模型（PLMs）难以应对。", "method": "采用直接偏好优化（DPO）进行偏好对齐训练，整合生物知识。", "result": "提出了一种新的蛋白质功能预测范式。", "conclusion": "AnnoDPO为蛋白质表示学习中的生物知识整合提供了新方法。"}}
{"id": "2506.07040", "pdf": "https://arxiv.org/pdf/2506.07040", "abs": "https://arxiv.org/abs/2506.07040", "authors": ["Yang Xu", "Swetha Ganesh", "Vaneet Aggarwal"], "title": "Efficient $Q$-Learning and Actor-Critic Methods for Robust Average Reward Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "arXiv admin note: text overlap with arXiv:2502.16816", "summary": "We present the first $Q$-learning and actor-critic algorithms for robust\naverage reward Markov Decision Processes (MDPs) with non-asymptotic convergence\nunder contamination, TV distance and Wasserstein distance uncertainty sets. We\nshow that the robust $Q$ Bellman operator is a strict contractive mapping with\nrespect to a carefully constructed semi-norm with constant functions being\nquotiented out. This property supports a stochastic approximation update, that\nlearns the optimal robust $Q$ function in $\\tilde{\\cO}(\\epsilon^{-2})$ samples.\nWe also show that the same idea can be used for robust $Q$ function estimation,\nwhich can be further used for critic estimation. Coupling it with theories in\nrobust policy mirror descent update, we present a natural actor-critic\nalgorithm that attains an $\\epsilon$-optimal robust policy in\n$\\tilde{\\cO}(\\epsilon^{-3})$ samples. These results advance the theory of\ndistributionally robust reinforcement learning in the average reward setting.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.07054", "pdf": "https://arxiv.org/pdf/2506.07054", "abs": "https://arxiv.org/abs/2506.07054", "authors": ["Uri Koren", "Navdeep Kumar", "Uri Gadot", "Giorgia Ramponi", "Kfir Yehuda Levy", "Shie Mannor"], "title": "Policy Gradient with Tree Search: Avoiding Local Optimas through Lookahead", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Classical policy gradient (PG) methods in reinforcement learning frequently\nconverge to suboptimal local optima, a challenge exacerbated in large or\ncomplex environments. This work investigates Policy Gradient with Tree Search\n(PGTS), an approach that integrates an $m$-step lookahead mechanism to enhance\npolicy optimization. We provide theoretical analysis demonstrating that\nincreasing the tree search depth $m$-monotonically reduces the set of\nundesirable stationary points and, consequently, improves the worst-case\nperformance of any resulting stationary policy. Critically, our analysis\naccommodates practical scenarios where policy updates are restricted to states\nvisited by the current policy, rather than requiring updates across the entire\nstate space. Empirical evaluations on diverse MDP structures, including Ladder,\nTightrope, and Gridworld environments, illustrate PGTS's ability to exhibit\n\"farsightedness,\" navigate challenging reward landscapes, escape local traps\nwhere standard PG fails, and achieve superior solutions.", "AI": {"tldr": "PGTS方法通过结合树搜索机制改进策略梯度方法，减少局部最优问题，提升性能。", "motivation": "传统策略梯度方法易陷入局部最优，尤其在复杂环境中表现不佳，PGTS旨在解决这一问题。", "method": "PGTS引入m步前瞻机制，优化策略更新，理论分析表明增加搜索深度可减少不良稳定点。", "result": "实验证明PGTS在多种MDP环境中表现优异，能避开局部陷阱，获得更优解。", "conclusion": "PGTS通过树搜索机制显著提升策略梯度方法的性能，尤其在复杂环境中表现突出。"}}
{"id": "2506.07060", "pdf": "https://arxiv.org/pdf/2506.07060", "abs": "https://arxiv.org/abs/2506.07060", "authors": ["Laura Cohen", "Xavier Hinaut", "Lilyana Petrova", "Alexandre Pitti", "Syd Reynal", "Ichiro Tsuda"], "title": "Less is More: some Computational Principles based on Parcimony, and Limitations of Natural Intelligence", "categories": ["q-bio.NC", "cs.AI"], "comment": null, "summary": "Natural intelligence (NI) consistently achieves more with less. Infants learn\nlanguage, develop abstract concepts, and acquire sensorimotor skills from\nsparse data, all within tight neural and energy limits. In contrast, today's AI\nrelies on virtually unlimited computational power, energy, and data to reach\nhigh performance. This paper argues that constraints in NI are paradoxically\ncatalysts for efficiency, adaptability, and creativity. We first show how\nlimited neural bandwidth promotes concise codes that still capture complex\npatterns. Spiking neurons, hierarchical structures, and symbolic-like\nrepresentations emerge naturally from bandwidth constraints, enabling robust\ngeneralization. Next, we discuss chaotic itinerancy, illustrating how the brain\ntransits among transient attractors to flexibly retrieve memories and manage\nuncertainty. We then highlight reservoir computing, where random projections\nfacilitate rapid generalization from small datasets. Drawing on developmental\nperspectives, we emphasize how intrinsic motivation, along with responsive\nsocial environments, drives infant language learning and discovery of meaning.\nSuch active, embodied processes are largely absent in current AI. Finally, we\nsuggest that adopting 'less is more' principles -- energy constraints,\nparsimonious architectures, and real-world interaction -- can foster the\nemergence of more efficient, interpretable, and biologically grounded\nartificial systems.", "AI": {"tldr": "论文探讨了自然智能（NI）如何在有限资源下高效学习，提出约束是效率、适应性和创造性的催化剂，并建议AI借鉴这些原则以实现更高效、可解释的系统。", "motivation": "自然智能（如婴儿学习语言和技能）在有限资源和数据下表现出高效性，而当前AI依赖大量计算和能源。论文旨在探索如何通过约束提升AI的效率。", "method": "分析了有限神经带宽如何促进简洁编码、混沌游走如何支持灵活记忆检索，以及储备计算如何从小数据快速泛化。同时强调了内在动机和社会环境的作用。", "result": "研究表明，约束可以催生高效、适应性强的系统，如简洁编码、混沌游走和储备计算等机制。", "conclusion": "论文建议AI采用'少即是多'原则（如能源约束、简洁架构和现实交互），以实现更高效、可解释且生物启发的系统。"}}
{"id": "2506.07046", "pdf": "https://arxiv.org/pdf/2506.07046", "abs": "https://arxiv.org/abs/2506.07046", "authors": ["Anushka Jha", "Tanushree Dewangan", "Mukul Lokhande", "Santosh Kumar Vishvakarma"], "title": "QForce-RL: Quantized FPGA-Optimized Reinforcement Learning Compute Engine", "categories": ["cs.AR", "cs.CV", "cs.RO", "eess.IV"], "comment": null, "summary": "Reinforcement Learning (RL) has outperformed other counterparts in sequential\ndecision-making and dynamic environment control. However, FPGA deployment is\nsignificantly resource-expensive, as associated with large number of\ncomputations in training agents with high-quality images and possess new\nchallenges. In this work, we propose QForce-RL takes benefits of quantization\nto enhance throughput and reduce energy footprint with light-weight RL\narchitecture, without significant performance degradation. QForce-RL takes\nadvantages from E2HRL to reduce overall RL actions to learn desired policy and\nQuaRL for quantization based SIMD for hardware acceleration. We have also\nprovided detailed analysis for different RL environments, with emphasis on\nmodel size, parameters, and accelerated compute ops. The architecture is\nscalable for resource-constrained devices and provide parametrized efficient\ndeployment with flexibility in latency, throughput, power, and energy\nefficiency. The proposed QForce-RL provides performance enhancement up to 2.3x\nand better FPS - 2.6x compared to SoTA works.", "AI": {"tldr": "QForce-RL利用量化和轻量级架构提升FPGA上强化学习的效率和性能，减少资源消耗。", "motivation": "FPGA部署强化学习资源消耗大，传统方法在高性能图像处理中计算量大，面临挑战。", "method": "结合E2HRL减少动作空间和QuaRL量化SIMD硬件加速，优化模型大小和计算操作。", "result": "性能提升2.3倍，帧率提升2.6倍，适用于资源受限设备。", "conclusion": "QForce-RL在资源效率和性能上显著优于现有技术，适合灵活部署。"}}
{"id": "2506.07062", "pdf": "https://arxiv.org/pdf/2506.07062", "abs": "https://arxiv.org/abs/2506.07062", "authors": ["Dongryung Lee", "Sejune Joo", "Kimin Lee", "Beomjoon Kim"], "title": "Prime the search: Using large language models for guiding geometric task and motion planning by warm-starting tree search", "categories": ["cs.RO", "cs.AI"], "comment": "The International Journal of Robotics Research (IJRR)", "summary": "The problem of relocating a set of objects to designated areas amidst movable\nobstacles can be framed as a Geometric Task and Motion Planning (G-TAMP)\nproblem, a subclass of task and motion planning (TAMP). Traditional approaches\nto G-TAMP have relied either on domain-independent heuristics or on learning\nfrom planning experience to guide the search, both of which typically demand\nsignificant computational resources or data. In contrast, humans often use\ncommon sense to intuitively decide which objects to manipulate in G-TAMP\nproblems. Inspired by this, we propose leveraging Large Language Models (LLMs),\nwhich have common sense knowledge acquired from internet-scale data, to guide\ntask planning in G-TAMP problems. To enable LLMs to perform geometric\nreasoning, we design a predicate-based prompt that encodes geometric\ninformation derived from a motion planning algorithm. We then query the LLM to\ngenerate a task plan, which is then used to search for a feasible set of\ncontinuous parameters. Since LLMs are prone to mistakes, instead of committing\nto LLM's outputs, we extend Monte Carlo Tree Search (MCTS) to a hybrid action\nspace and use the LLM to guide the search. Unlike the previous approach that\ncalls an LLM at every node and incurs high computational costs, we use it to\nwarm-start the MCTS with the nodes explored in completing the LLM's task plan.\nOn six different G-TAMP problems, we show our method outperforms previous LLM\nplanners and pure search algorithms. Code can be found at:\nhttps://github.com/iMSquared/prime-the-search", "AI": {"tldr": "论文提出了一种利用大型语言模型（LLM）结合几何推理和蒙特卡洛树搜索（MCTS）的方法，解决几何任务与运动规划（G-TAMP）问题，显著优于传统方法。", "motivation": "传统G-TAMP方法依赖大量计算资源或数据，而人类常凭直觉解决类似问题。受此启发，作者希望通过LLM的常识知识优化任务规划。", "method": "设计基于谓词的提示编码几何信息，利用LLM生成任务计划，并通过扩展的MCTS搜索可行参数。LLM用于预热MCTS，而非频繁调用。", "result": "在六个G-TAMP问题上，该方法优于传统LLM规划器和纯搜索算法。", "conclusion": "结合LLM的常识与MCTS的搜索能力，显著提升了G-TAMP问题的解决效率。"}}
{"id": "2506.07069", "pdf": "https://arxiv.org/pdf/2506.07069", "abs": "https://arxiv.org/abs/2506.07069", "authors": ["Zhican Wang", "Guanghui He", "Dantong Liu", "Lingjun Gao", "Shell Xu Hu", "Chen Zhang", "Zhuoran Song", "Nicholas Lane", "Wayne Luk", "Hongxiang Fan"], "title": "Accelerating 3D Gaussian Splatting with Neural Sorting and Axis-Oriented Rasterization", "categories": ["cs.GR", "cs.AR", "cs.CV", "cs.LG"], "comment": "Preprint. Under review", "summary": "3D Gaussian Splatting (3DGS) has recently gained significant attention for\nhigh-quality and efficient view synthesis, making it widely adopted in fields\nsuch as AR/VR, robotics, and autonomous driving. Despite its impressive\nalgorithmic performance, real-time rendering on resource-constrained devices\nremains a major challenge due to tight power and area budgets. This paper\npresents an architecture-algorithm co-design to address these inefficiencies.\nFirst, we reveal substantial redundancy caused by repeated computation of\ncommon terms/expressions during the conventional rasterization. To resolve\nthis, we propose axis-oriented rasterization, which pre-computes and reuses\nshared terms along both the X and Y axes through a dedicated hardware design,\neffectively reducing multiply-and-add (MAC) operations by up to 63%. Second, by\nidentifying the resource and performance inefficiency of the sorting process,\nwe introduce a novel neural sorting approach that predicts order-independent\nblending weights using an efficient neural network, eliminating the need for\ncostly hardware sorters. A dedicated training framework is also proposed to\nimprove its algorithmic stability. Third, to uniformly support rasterization\nand neural network inference, we design an efficient reconfigurable processing\narray that maximizes hardware utilization and throughput. Furthermore, we\nintroduce a $\\pi$-trajectory tile schedule, inspired by Morton encoding and\nHilbert curve, to optimize Gaussian reuse and reduce memory access overhead.\nComprehensive experiments demonstrate that the proposed design preserves\nrendering quality while achieving a speedup of $23.4\\sim27.8\\times$ and energy\nsavings of $28.8\\sim51.4\\times$ compared to edge GPUs for real-world scenes. We\nplan to open-source our design to foster further development in this field.", "AI": {"tldr": "本文提出了一种架构-算法协同设计方法，优化3D高斯泼溅（3DGS）在资源受限设备上的实时渲染性能，通过轴定向光栅化、神经排序和可重构处理阵列等技术，显著提升速度和能效。", "motivation": "尽管3DGS在高质量视图合成中表现优异，但在资源受限设备上的实时渲染仍面临挑战，主要由于计算冗余和排序效率低。", "method": "采用轴定向光栅化减少计算冗余，引入神经排序替代硬件排序器，设计可重构处理阵列支持光栅化和神经网络推理，并提出π轨迹瓦片调度优化内存访问。", "result": "实验显示，设计在保持渲染质量的同时，速度提升23.4~27.8倍，能耗降低28.8~51.4倍。", "conclusion": "该协同设计显著提升了3DGS在资源受限设备上的性能，计划开源以推动领域发展。"}}
{"id": "2506.07066", "pdf": "https://arxiv.org/pdf/2506.07066", "abs": "https://arxiv.org/abs/2506.07066", "authors": ["Li Jingyuan"], "title": "From Axioms to Algorithms: Mechanized Proofs of the vNM Utility Theorem", "categories": ["econ.TH", "cs.AI", "q-fin.CP"], "comment": null, "summary": "This paper presents a comprehensive formalization of the von\nNeumann-Morgenstern (vNM) expected utility theorem using the Lean 4 interactive\ntheorem prover. We implement the classical axioms of preference-completeness,\ntransitivity, continuity, and independence-enabling machine-verified proofs of\nboth the existence and uniqueness of utility representations. Our formalization\ncaptures the mathematical structure of preference relations over lotteries,\nverifying that preferences satisfying the vNM axioms can be represented by\nexpected utility maximization.\n  Our contributions include a granular implementation of the independence\naxiom, formally verified proofs of fundamental claims about mixture lotteries,\nconstructive demonstrations of utility existence, and computational experiments\nvalidating the results. We prove equivalence to classical presentations while\noffering greater precision at decision boundaries.\n  This formalization provides a rigorous foundation for applications in\neconomic modeling, AI alignment, and management decision systems, bridging the\ngap between theoretical decision theory and computational implementation.", "AI": {"tldr": "本文使用Lean 4交互式定理证明器对冯·诺依曼-摩根斯坦（vNM）期望效用定理进行了全面的形式化，验证了偏好关系的数学结构及其效用表示的存在性和唯一性。", "motivation": "为经济建模、AI对齐和管理决策系统提供严格的理论基础，弥合理论决策理论与计算实现之间的差距。", "method": "实现偏好完备性、传递性、连续性和独立性等经典公理，并通过机器验证证明效用表示的存在性和唯一性。", "result": "验证了满足vNM公理的偏好可以通过期望效用最大化表示，并提供了混合彩票的基本主张的形式化证明。", "conclusion": "该形式化为理论决策理论的应用提供了精确的计算实现，具有广泛的实际应用价值。"}}
{"id": "2506.07077", "pdf": "https://arxiv.org/pdf/2506.07077", "abs": "https://arxiv.org/abs/2506.07077", "authors": ["Qianshan Wei", "Jiaqi Li", "Zihan You", "Yi Zhan", "Kecen Li", "Jialin Wu", "Xinfeng Li Hengjun Liu", "Yi Yu", "Bin Cao", "Yiwen Xu", "Yang Liu", "Guilin Qi"], "title": "Dual-Priv Pruning : Efficient Differential Private Fine-Tuning in Multimodal Large Language Models", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Differential Privacy (DP) is a widely adopted technique, valued for its\neffectiveness in protecting the privacy of task-specific datasets, making it a\ncritical tool for large language models. However, its effectiveness in\nMultimodal Large Language Models (MLLMs) remains uncertain. Applying\nDifferential Privacy (DP) inherently introduces substantial computation\noverhead, a concern particularly relevant for MLLMs which process extensive\ntextual and visual data. Furthermore, a critical challenge of DP is that the\ninjected noise, necessary for privacy, scales with parameter dimensionality,\nleading to pronounced model degradation; This trade-off between privacy and\nutility complicates the application of Differential Privacy (DP) to complex\narchitectures like MLLMs. To address these, we propose Dual-Priv Pruning, a\nframework that employs two complementary pruning mechanisms for DP fine-tuning\nin MLLMs: (i) visual token pruning to reduce input dimensionality by removing\nredundant visual information, and (ii) gradient-update pruning during the DP\noptimization process. This second mechanism selectively prunes parameter\nupdates based on the magnitude of noisy gradients, aiming to mitigate noise\nimpact and improve utility. Experiments demonstrate that our approach achieves\ncompetitive results with minimal performance degradation. In terms of\ncomputational efficiency, our approach consistently utilizes less memory than\nstandard DP-SGD. While requiring only 1.74% more memory than zeroth-order\nmethods which suffer from severe performance issues on A100 GPUs, our method\ndemonstrates leading memory efficiency on H20 GPUs. To the best of our\nknowledge, we are the first to explore DP fine-tuning in MLLMs. Our code is\ncoming soon.", "AI": {"tldr": "论文提出了一种名为Dual-Priv Pruning的框架，通过两种剪枝机制在MLLMs中实现差分隐私微调，以减少计算开销和噪声影响，同时保持模型性能。", "motivation": "差分隐私（DP）在多模态大语言模型（MLLMs）中的应用存在计算开销大和噪声导致模型性能下降的问题，需要一种高效的方法来解决这些挑战。", "method": "提出Dual-Priv Pruning框架，包括视觉令牌剪枝（减少输入维度）和梯度更新剪枝（选择性修剪噪声梯度更新），以优化DP微调过程。", "result": "实验表明，该方法在性能退化最小的情况下取得竞争性结果，计算效率优于标准DP-SGD，并在H20 GPU上表现出领先的内存效率。", "conclusion": "Dual-Priv Pruning是首个探索MLLMs中DP微调的方法，有效平衡了隐私保护和模型性能，具有实际应用潜力。"}}
{"id": "2506.07209", "pdf": "https://arxiv.org/pdf/2506.07209", "abs": "https://arxiv.org/abs/2506.07209", "authors": ["Lei Li", "Angela Dai"], "title": "HOI-PAGE: Zero-Shot Human-Object Interaction Generation with Part Affordance Guidance", "categories": ["cs.GR", "cs.CV"], "comment": "Project page: https://hoipage.github.io/ Video:\n  https://youtu.be/b1pJU9lKQTE", "summary": "We present HOI-PAGE, a new approach to synthesizing 4D human-object\ninteractions (HOIs) from text prompts in a zero-shot fashion, driven by\npart-level affordance reasoning. In contrast to prior works that focus on\nglobal, whole body-object motion for 4D HOI synthesis, we observe that\ngenerating realistic and diverse HOIs requires a finer-grained understanding --\nat the level of how human body parts engage with object parts. We thus\nintroduce Part Affordance Graphs (PAGs), a structured HOI representation\ndistilled from large language models (LLMs) that encodes fine-grained part\ninformation along with contact relations. We then use these PAGs to guide a\nthree-stage synthesis: first, decomposing input 3D objects into geometric\nparts; then, generating reference HOI videos from text prompts, from which we\nextract part-based motion constraints; finally, optimizing for 4D HOI motion\nsequences that not only mimic the reference dynamics but also satisfy\npart-level contact constraints. Extensive experiments show that our approach is\nflexible and capable of generating complex multi-object or multi-person\ninteraction sequences, with significantly improved realism and text alignment\nfor zero-shot 4D HOI generation.", "AI": {"tldr": "HOI-PAGE是一种从文本提示中零样本合成4D人-物交互（HOI）的新方法，通过部分级功能推理实现。", "motivation": "现有方法主要关注全局的全身-物体运动，而生成真实多样的HOI需要更细粒度的理解，即人体部分如何与物体部分交互。", "method": "引入部分功能图（PAGs），从大语言模型中提取结构化HOI表示，指导三阶段合成：分解3D物体、生成参考视频并提取运动约束、优化4D HOI序列。", "result": "实验表明，该方法能灵活生成复杂多对象或多人的交互序列，显著提升了零样本4D HOI生成的逼真度和文本对齐性。", "conclusion": "HOI-PAGE通过部分级功能推理，实现了更真实和多样化的4D HOI合成。"}}
{"id": "2506.07079", "pdf": "https://arxiv.org/pdf/2506.07079", "abs": "https://arxiv.org/abs/2506.07079", "authors": ["Mostafa Eslami", "Maryam Babazadeh"], "title": "On the Generalization of Data-Assisted Control in port-Hamiltonian Systems (DAC-pH)", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "comment": "This paper presents an early investigation of Data-Assisted Control\n  (DAC) with reinforcement learning, showcasing its potential through a simple\n  example. Theoretical analysis is ongoing to establish formal support and\n  guarantees for the proposed approach", "summary": "This paper introduces a hypothetical hybrid control framework for\nport-Hamiltonian (p$\\mathcal{H}$) systems, employing a dynamic decomposition\nbased on Data-Assisted Control (DAC). The system's evolution is split into two\nparts with fixed topology: Right-Hand Side (RHS)- an intrinsic Hamiltonian flow\nhandling worst-case parametric uncertainties, and Left-Hand Side (LHS)- a\ndissipative/input flow addressing both structural and parametric uncertainties.\nA virtual port variable $\\Pi$ serves as the interface between these two\ncomponents. A nonlinear controller manages the intrinsic Hamiltonian flow,\ndetermining a desired port control value $\\Pi_c$. Concurrently, Reinforcement\nLearning (RL) is applied to the dissipative/input flow to learn an agent for\nproviding optimal policy in mapping $\\Pi_c$ to the actual system input. This\nhybrid approach effectively manages RHS uncertainties while preserving the\nsystem's inherent structure. Key advantages include adjustable performance via\nLHS controller parameters, enhanced AI explainability and interpretability\nthrough the port variable $\\Pi$, the ability to guarantee safety and state\nattainability with hard/soft constraints, reduced complexity in learning\nhypothesis classes compared to end-to-end solutions, and improved\nstate/parameter estimation using LHS prior knowledge and system Hamiltonian to\naddress partial observability. The paper details the p$\\mathcal{H}$\nformulation, derives the decomposition, and presents the modular controller\narchitecture. Beyond design, crucial aspects of stability and robustness\nanalysis and synthesis are investigated, paving the way for deeper theoretical\ninvestigations. An application example, a pendulum with nonlinear dynamics, is\nsimulated to demonstrate the approach's empirical and phenomenological benefits\nfor future research.", "AI": {"tldr": "本文提出了一种基于数据辅助控制（DAC）的混合控制框架，用于端口哈密顿系统，通过动态分解处理参数和结构不确定性，结合非线性控制器和强化学习实现优化控制。", "motivation": "解决端口哈密顿系统中参数和结构不确定性的管理问题，同时保持系统固有结构，并提升控制性能与AI可解释性。", "method": "将系统动态分解为右端（RHS）和左端（LHS），分别处理参数不确定性和结构不确定性；通过虚拟端口变量连接两部分，结合非线性控制器和强化学习实现优化控制。", "result": "该方法有效管理了RHS的不确定性，提升了控制性能、安全性和状态可达性，同时降低了学习复杂度。", "conclusion": "提出的混合框架在理论和应用上均表现出色，为未来研究提供了新的方向。"}}
{"id": "2506.07218", "pdf": "https://arxiv.org/pdf/2506.07218", "abs": "https://arxiv.org/abs/2506.07218", "authors": ["Tong Xiao", "Xin Xu", "Zhenya Huang", "Hongyu Gao", "Quan Liu", "Qi Liu", "Enhong Chen"], "title": "Advancing Multimodal Reasoning Capabilities of Multimodal Large Language Models via Visual Perception Reward", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Enhancing the multimodal reasoning capabilities of Multimodal Large Language\nModels (MLLMs) is a challenging task that has attracted increasing attention in\nthe community. Recently, several studies have applied Reinforcement Learning\nwith Verifiable Rewards (RLVR) to the multimodal domain in order to enhance the\nreasoning abilities of MLLMs. However, these works largely overlook the\nenhancement of multimodal perception capabilities in MLLMs, which serve as a\ncore prerequisite and foundational component of complex multimodal reasoning.\nThrough McNemar's test, we find that existing RLVR method fails to effectively\nenhance the multimodal perception capabilities of MLLMs, thereby limiting their\nfurther improvement in multimodal reasoning. To address this limitation, we\npropose Perception-R1, which introduces a novel visual perception reward that\nexplicitly encourages MLLMs to perceive the visual content accurately, thereby\ncan effectively incentivizing both their multimodal perception and reasoning\ncapabilities. Specifically, we first collect textual visual annotations from\nthe CoT trajectories of multimodal problems, which will serve as visual\nreferences for reward assignment. During RLVR training, we employ a judging LLM\nto assess the consistency between the visual annotations and the responses\ngenerated by MLLM, and assign the visual perception reward based on these\nconsistency judgments. Extensive experiments on several multimodal reasoning\nbenchmarks demonstrate the effectiveness of our Perception-R1, which achieves\nstate-of-the-art performance on most benchmarks using only 1,442 training data.", "AI": {"tldr": "论文提出Perception-R1方法，通过引入视觉感知奖励增强多模态大语言模型（MLLMs）的感知和推理能力。", "motivation": "现有基于可验证奖励的强化学习（RLVR）方法在多模态领域未能有效提升MLLMs的感知能力，限制了其推理能力的进一步提升。", "method": "提出Perception-R1，通过收集视觉注释作为奖励参考，利用评判LLM评估MLLMs响应与注释的一致性，分配视觉感知奖励。", "result": "在多个多模态推理基准测试中，Perception-R1仅用1,442个训练数据即达到最优性能。", "conclusion": "Perception-R1有效提升了MLLMs的感知和推理能力，为多模态任务提供了新思路。"}}
{"id": "2506.07092", "pdf": "https://arxiv.org/pdf/2506.07092", "abs": "https://arxiv.org/abs/2506.07092", "authors": ["Joydeb Kumar Sana", "Mohammad M. Masud", "M Sohel Rahman", "M Saifur Rahman"], "title": "Patient Similarity Computation for Clinical Decision Support: An Efficient Use of Data Transformation, Combining Static and Time Series Data", "categories": ["cs.LG", "cs.AI"], "comment": "This paper presents a novel distributed patient similarity\n  computation (DPSC) technique based on data transformation (DT) methods,\n  utilizing an effective combination of time series and static data", "summary": "Patient similarity computation (PSC) is a fundamental problem in healthcare\ninformatics. The aim of the patient similarity computation is to measure the\nsimilarity among patients according to their historical clinical records, which\nhelps to improve clinical decision support. This paper presents a novel\ndistributed patient similarity computation (DPSC) technique based on data\ntransformation (DT) methods, utilizing an effective combination of time series\nand static data. Time series data are sensor-collected patients' information,\nincluding metrics like heart rate, blood pressure, Oxygen saturation,\nrespiration, etc. The static data are mainly patient background and demographic\ndata, including age, weight, height, gender, etc. Static data has been used for\nclustering the patients. Before feeding the static data to the machine learning\nmodel adaptive Weight-of-Evidence (aWOE) and Z-score data transformation (DT)\nmethods have been performed, which improve the prediction performances. In\naWOE-based patient similarity models, sensitive patient information has been\nprocessed using aWOE which preserves the data privacy of the trained models. We\nused the Dynamic Time Warping (DTW) approach, which is robust and very popular,\nfor time series similarity. However, DTW is not suitable for big data due to\nthe significant computational run-time. To overcome this problem, distributed\nDTW computation is used in this study. For Coronary Artery Disease, our DT\nbased approach boosts prediction performance by as much as 11.4%, 10.20%, and\n12.6% in terms of AUC, accuracy, and F-measure, respectively. In the case of\nCongestive Heart Failure (CHF), our proposed method achieves performance\nenhancement up to 15.9%, 10.5%, and 21.9% for the same measures, respectively.\nThe proposed method reduces the computation time by as high as 40%.", "AI": {"tldr": "本文提出了一种基于数据转换方法的分布式患者相似性计算（DPSC）技术，结合时间序列和静态数据，显著提升了预测性能并减少了计算时间。", "motivation": "患者相似性计算（PSC）是医疗信息学中的基础问题，旨在通过患者历史临床记录衡量相似性，以改进临床决策支持。", "method": "采用数据转换方法（aWOE和Z-score）处理静态数据，并结合动态时间规整（DTW）进行时间序列相似性计算。为解决DTW在大数据中的计算效率问题，使用了分布式DTW计算。", "result": "在冠状动脉疾病和充血性心力衰竭的预测中，AUC、准确率和F-measure分别提升了11.4%、10.20%、12.6%和15.9%、10.5%、21.9%，计算时间减少了40%。", "conclusion": "提出的DPSC技术有效结合了静态和时间序列数据，显著提升了预测性能和计算效率，适用于医疗决策支持。"}}
{"id": "2506.07228", "pdf": "https://arxiv.org/pdf/2506.07228", "abs": "https://arxiv.org/abs/2506.07228", "authors": ["Shuvashis Sarker"], "title": "Transfer Learning and Explainable AI for Brain Tumor Classification: A Study Using MRI Data from Bangladesh", "categories": ["eess.IV", "cs.CV"], "comment": "2024 6th International Conference on Sustainable Technologies for\n  Industry 5.0 (STI)", "summary": "Brain tumors, regardless of being benign or malignant, pose considerable\nhealth risks, with malignant tumors being more perilous due to their swift and\nuncontrolled proliferation, resulting in malignancy. Timely identification is\ncrucial for enhancing patient outcomes, particularly in nations such as\nBangladesh, where healthcare infrastructure is constrained. Manual MRI analysis\nis arduous and susceptible to inaccuracies, rendering it inefficient for prompt\ndiagnosis. This research sought to tackle these problems by creating an\nautomated brain tumor classification system utilizing MRI data obtained from\nmany hospitals in Bangladesh. Advanced deep learning models, including VGG16,\nVGG19, and ResNet50, were utilized to classify glioma, meningioma, and various\nbrain cancers. Explainable AI (XAI) methodologies, such as Grad-CAM and\nGrad-CAM++, were employed to improve model interpretability by emphasizing the\ncritical areas in MRI scans that influenced the categorization. VGG16 achieved\nthe most accuracy, attaining 99.17%. The integration of XAI enhanced the\nsystem's transparency and stability, rendering it more appropriate for clinical\napplication in resource-limited environments such as Bangladesh. This study\nhighlights the capability of deep learning models, in conjunction with\nexplainable artificial intelligence (XAI), to enhance brain tumor detection and\nidentification in areas with restricted access to advanced medical\ntechnologies.", "AI": {"tldr": "该研究开发了一种基于深度学习的自动脑肿瘤分类系统，结合可解释AI技术，显著提高了分类准确性和临床适用性。", "motivation": "脑肿瘤的及时诊断对患者预后至关重要，尤其是在医疗资源有限的地区如孟加拉国。手动MRI分析效率低且易出错，因此需要自动化解决方案。", "method": "研究使用了VGG16、VGG19和ResNet50等深度学习模型，结合Grad-CAM和Grad-CAM++等XAI技术，对MRI数据进行分类。", "result": "VGG16模型表现最佳，准确率达99.17%。XAI技术提高了模型的透明度和稳定性。", "conclusion": "深度学习与XAI结合可有效提升脑肿瘤检测，适用于资源有限地区的临床应用。"}}
{"id": "2506.07099", "pdf": "https://arxiv.org/pdf/2506.07099", "abs": "https://arxiv.org/abs/2506.07099", "authors": ["Wenying He", "Jieling Huang", "Junhua Gu", "Ji Zhang", "Yude Bai"], "title": "Filling the Missings: Spatiotemporal Data Imputation by Conditional Diffusion", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages,3 figures", "summary": "Missing data in spatiotemporal systems presents a significant challenge for\nmodern applications, ranging from environmental monitoring to urban traffic\nmanagement. The integrity of spatiotemporal data often deteriorates due to\nhardware malfunctions and software failures in real-world deployments. Current\napproaches based on machine learning and deep learning struggle to model the\nintricate interdependencies between spatial and temporal dimensions effectively\nand, more importantly, suffer from cumulative errors during the data imputation\nprocess, which propagate and amplify through iterations. To address these\nlimitations, we propose CoFILL, a novel Conditional Diffusion Model for\nspatiotemporal data imputation. CoFILL builds on the inherent advantages of\ndiffusion models to generate high-quality imputations without relying on\npotentially error-prone prior estimates. It incorporates an innovative\ndual-stream architecture that processes temporal and frequency domain features\nin parallel. By fusing these complementary features, CoFILL captures both rapid\nfluctuations and underlying patterns in the data, which enables more robust\nimputation. The extensive experiments reveal that CoFILL's noise prediction\nnetwork successfully transforms random noise into meaningful values that align\nwith the true data distribution. The results also show that CoFILL outperforms\nstate-of-the-art methods in imputation accuracy. The source code is publicly\navailable at https://github.com/joyHJL/CoFILL.", "AI": {"tldr": "CoFILL是一种基于条件扩散模型的新型时空数据填补方法，通过双流架构处理时空和频域特征，显著提升了填补精度。", "motivation": "时空数据因硬件和软件故障常出现缺失，现有方法难以有效建模时空依赖且易产生累积误差。", "method": "提出CoFILL，利用扩散模型生成高质量填补值，采用双流架构并行处理时空和频域特征。", "result": "实验表明CoFILL能准确预测噪声并优于现有方法。", "conclusion": "CoFILL为时空数据填补提供了更鲁棒的解决方案，代码已开源。"}}
{"id": "2506.07234", "pdf": "https://arxiv.org/pdf/2506.07234", "abs": "https://arxiv.org/abs/2506.07234", "authors": ["Shuvashis Sarker"], "title": "A Comprehensive Analysis of COVID-19 Detection Using Bangladeshi Data and Explainable AI", "categories": ["eess.IV", "cs.CV"], "comment": "2024 4th International Conference on Innovations in Science,\n  Engineering and Technology (ICISET)", "summary": "COVID-19 is a rapidly spreading and highly infectious virus which has\ntriggered a global pandemic, profoundly affecting millions across the world.\nThe pandemic has introduced unprecedented challenges in public health, economic\nstability, and societal structures, necessitating the implementation of\nextensive and multifaceted health interventions globally. It had a tremendous\nimpact on Bangladesh by April 2024, with around 29,495 fatalities and more than\n2 million confirmed cases. This study focuses on improving COVID-19 detection\nin CXR images by utilizing a dataset of 4,350 images from Bangladesh\ncategorized into four classes: Normal, Lung-Opacity, COVID-19 and\nViral-Pneumonia. ML, DL and TL models are employed with the VGG19 model\nachieving an impressive 98% accuracy. LIME is used to explain model\npredictions, highlighting the regions and features influencing classification\ndecisions. SMOTE is applied to address class imbalances. By providing insight\ninto both correct and incorrect classifications, the study emphasizes the\nimportance of XAI in enhancing the transparency and reliability of models,\nultimately improving the effectiveness of detection from CXR images.", "AI": {"tldr": "该研究利用深度学习模型（VGG19）在CXR图像中检测COVID-19，准确率达98%，并通过LIME和SMOTE提升模型透明性和数据平衡。", "motivation": "COVID-19对全球和孟加拉国造成严重影响，亟需高效检测方法以应对公共卫生挑战。", "method": "使用4,350张CXR图像数据集，分为四类，采用ML、DL和TL模型，结合LIME和SMOTE优化模型。", "result": "VGG19模型达到98%准确率，LIME解释模型决策，SMOTE解决数据不平衡问题。", "conclusion": "研究强调XAI在提升模型透明性和可靠性中的重要性，有助于改进CXR图像检测效果。"}}
{"id": "2506.07236", "pdf": "https://arxiv.org/pdf/2506.07236", "abs": "https://arxiv.org/abs/2506.07236", "authors": ["Jiachen Zhong", "Yiting Wang", "Di Zhu", "Ziwei Wang"], "title": "A Narrative Review on Large AI Models in Lung Cancer Screening, Diagnosis, and Treatment Planning", "categories": ["eess.IV", "cs.CV"], "comment": "Under Review", "summary": "Lung cancer remains one of the most prevalent and fatal diseases worldwide,\ndemanding accurate and timely diagnosis and treatment. Recent advancements in\nlarge AI models have significantly enhanced medical image understanding and\nclinical decision-making. This review systematically surveys the\nstate-of-the-art in applying large AI models to lung cancer screening,\ndiagnosis, prognosis, and treatment. We categorize existing models into\nmodality-specific encoders, encoder-decoder frameworks, and joint encoder\narchitectures, highlighting key examples such as CLIP, BLIP, Flamingo,\nBioViL-T, and GLoRIA. We further examine their performance in multimodal\nlearning tasks using benchmark datasets like LIDC-IDRI, NLST, and MIMIC-CXR.\nApplications span pulmonary nodule detection, gene mutation prediction,\nmulti-omics integration, and personalized treatment planning, with emerging\nevidence of clinical deployment and validation. Finally, we discuss current\nlimitations in generalizability, interpretability, and regulatory compliance,\nproposing future directions for building scalable, explainable, and clinically\nintegrated AI systems. Our review underscores the transformative potential of\nlarge AI models to personalize and optimize lung cancer care.", "AI": {"tldr": "本文综述了大型AI模型在肺癌筛查、诊断、预后和治疗中的应用，分类并评估了现有模型的性能，同时讨论了局限性和未来方向。", "motivation": "肺癌是全球范围内高发且致命的疾病，需要更准确和及时的诊断与治疗。大型AI模型的进展为医学图像理解和临床决策提供了新工具。", "method": "系统调研了现有大型AI模型（如CLIP、BLIP、Flamingo等），分类为模态特定编码器、编码器-解码器框架和联合编码器架构，并评估其在多模态学习任务中的表现。", "result": "应用涵盖肺结节检测、基因突变预测、多组学整合和个性化治疗规划，部分模型已进入临床验证阶段。", "conclusion": "大型AI模型在肺癌诊疗中具有变革潜力，但需解决泛化性、可解释性和合规性问题，未来应构建可扩展、可解释且临床集成的AI系统。"}}
{"id": "2506.07296", "pdf": "https://arxiv.org/pdf/2506.07296", "abs": "https://arxiv.org/abs/2506.07296", "authors": ["Arian Askari", "Emmanouil Stergiadis", "Ilya Gusev", "Moran Beladev"], "title": "HotelMatch-LLM: Joint Multi-Task Training of Small and Large Language Models for Efficient Multimodal Hotel Retrieval", "categories": ["cs.IR", "cs.AI", "cs.CV"], "comment": "Accepted at ACL 2025, Main track. 13 Pages, 1 figure", "summary": "We present HotelMatch-LLM, a multimodal dense retrieval model for the travel\ndomain that enables natural language property search, addressing the\nlimitations of traditional travel search engines which require users to start\nwith a destination and editing search parameters. HotelMatch-LLM features three\nkey innovations: (1) Domain-specific multi-task optimization with three novel\nretrieval, visual, and language modeling objectives; (2) Asymmetrical dense\nretrieval architecture combining a small language model (SLM) for efficient\nonline query processing and a large language model (LLM) for embedding hotel\ndata; and (3) Extensive image processing to handle all property image\ngalleries. Experiments on four diverse test sets show HotelMatch-LLM\nsignificantly outperforms state-of-the-art models, including VISTA and MARVEL.\nSpecifically, on the test set -- main query type -- we achieve 0.681 for\nHotelMatch-LLM compared to 0.603 for the most effective baseline, MARVEL. Our\nanalysis highlights the impact of our multi-task optimization, the\ngeneralizability of HotelMatch-LLM across LLM architectures, and its\nscalability for processing large image galleries.", "AI": {"tldr": "HotelMatch-LLM是一种多模态密集检索模型，用于旅游领域，通过自然语言搜索解决传统旅游搜索引擎的局限性。", "motivation": "传统旅游搜索引擎需要用户从目的地开始并编辑搜索参数，限制了用户体验。", "method": "提出三个关键创新：领域特定的多任务优化、非对称密集检索架构和广泛的图像处理。", "result": "在四个测试集上显著优于现有模型（如VISTA和MARVEL），在主查询类型上达到0.681的分数。", "conclusion": "HotelMatch-LLM在多任务优化、通用性和可扩展性方面表现出色。"}}
{"id": "2506.07109", "pdf": "https://arxiv.org/pdf/2506.07109", "abs": "https://arxiv.org/abs/2506.07109", "authors": ["Rong-Xi Tan", "Ming Chen", "Ke Xue", "Yao Wang", "Yaoyuan Wang", "Sheng Fu", "Chao Qian"], "title": "Towards Universal Offline Black-Box Optimization via Learning Language Model Embeddings", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": "ICML 2025", "summary": "The pursuit of universal black-box optimization (BBO) algorithms is a\nlongstanding goal. However, unlike domains such as language or vision, where\nscaling structured data has driven generalization, progress in offline BBO\nremains hindered by the lack of unified representations for heterogeneous\nnumerical spaces. Thus, existing offline BBO approaches are constrained to\nsingle-task and fixed-dimensional settings, failing to achieve cross-domain\nuniversal optimization. Recent advances in language models (LMs) offer a\npromising path forward: their embeddings capture latent relationships in a\nunifying way, enabling universal optimization across different data types\npossible. In this paper, we discuss multiple potential approaches, including an\nend-to-end learning framework in the form of next-token prediction, as well as\nprioritizing the learning of latent spaces with strong representational\ncapabilities. To validate the effectiveness of these methods, we collect\noffline BBO tasks and data from open-source academic works for training.\nExperiments demonstrate the universality and effectiveness of our proposed\nmethods. Our findings suggest that unifying language model priors and learning\nstring embedding space can overcome traditional barriers in universal BBO,\npaving the way for general-purpose BBO algorithms. The code is provided at\nhttps://github.com/lamda-bbo/universal-offline-bbo.", "AI": {"tldr": "该论文探讨了利用语言模型（LMs）的嵌入能力实现通用黑盒优化（BBO）的方法，通过统一表示异构数值空间，克服了传统BBO的局限性。", "motivation": "传统离线BBO方法因缺乏统一表示而受限于单任务和固定维度设置，无法实现跨领域通用优化。语言模型的嵌入能力为解决这一问题提供了可能。", "method": "提出了多种方法，包括基于下一个令牌预测的端到端学习框架，以及学习具有强表示能力的潜在空间。", "result": "实验验证了所提方法的通用性和有效性，表明语言模型先验与字符串嵌入空间的结合可以突破传统BBO的限制。", "conclusion": "研究为通用BBO算法的发展铺平了道路，代码已开源。"}}
{"id": "2506.07301", "pdf": "https://arxiv.org/pdf/2506.07301", "abs": "https://arxiv.org/abs/2506.07301", "authors": ["Marco P. M. de Souza", "Juciane G. Maia", "Lilian N. de Andrade"], "title": "Pendulum Tracker -- SimuFísica: A Web-based Tool for Real-time Measurement of Oscillatory Motion", "categories": ["physics.ed-ph", "cs.CV"], "comment": null, "summary": "We present Pendulum Tracker, a computer vision-based application that enables\nreal-time measurement of the oscillatory motion of a physical pendulum.\nIntegrated into the educational platform SimuF\\'isica, the system uses the\nOpenCV.js library and runs directly in the browser, working on computers,\ntablets, and smartphones. The application automatically detects the pendulum's\nposition via the device's camera, displaying in real time the angle-versus-time\ngraph and estimates of the oscillation period. Experimental case studies\ndemonstrate its effectiveness in measuring the period, determining\ngravitational acceleration, and analyzing damped oscillations. The results show\nexcellent agreement with theoretical predictions, confirming the system's\naccuracy and its applicability in educational contexts. The accessible\ninterface and the ability to export raw data make Pendulum Tracker a versatile\ntool for experimental physics teaching.", "AI": {"tldr": "Pendulum Tracker是一个基于计算机视觉的应用，用于实时测量物理摆的振荡运动，适用于教育平台SimuFísica。", "motivation": "为教育实验物理提供一个实时、准确且易于使用的工具，用于测量和分析摆的运动。", "method": "利用OpenCV.js库和浏览器运行，通过设备摄像头自动检测摆的位置，实时显示角度-时间图并估算振荡周期。", "result": "实验研究表明，该系统在测量周期、确定重力加速度和分析阻尼振荡方面与理论预测高度一致。", "conclusion": "Pendulum Tracker是一个准确且多功能的工具，适用于实验物理教学，具有用户友好界面和数据导出功能。"}}
{"id": "2506.07118", "pdf": "https://arxiv.org/pdf/2506.07118", "abs": "https://arxiv.org/abs/2506.07118", "authors": ["Yu-Xuan Wu", "Ziyan Huang", "Bin Hu", "Zhi-Hong Guan"], "title": "RBA-FE: A Robust Brain-Inspired Audio Feature Extractor for Depression Diagnosis", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "14 pages", "summary": "This article proposes a robust brain-inspired audio feature extractor\n(RBA-FE) model for depression diagnosis, using an improved hierarchical network\narchitecture. Most deep learning models achieve state-of-the-art performance\nfor image-based diagnostic tasks, ignoring the counterpart audio features. In\norder to tailor the noise challenge, RBA-FE leverages six acoustic features\nextracted from the raw audio, capturing both spatial characteristics and\ntemporal dependencies. This hybrid attribute helps alleviate the precision\nlimitation in audio feature extraction within other learning models like deep\nresidual shrinkage networks. To deal with the noise issues, our model\nincorporates an improved spiking neuron model, called adaptive rate smooth\nleaky integrate-and-fire (ARSLIF). The ARSLIF model emulates the mechanism of\n``retuning of cellular signal selectivity\" in the brain attention systems,\nwhich enhances the model robustness against environmental noises in audio data.\nExperimental results demonstrate that RBA-FE achieves state-of-the-art accuracy\non the MODMA dataset, respectively with 0.8750, 0.8974, 0.8750 and 0.8750 in\nprecision, accuracy, recall and F1 score. Extensive experiments on the AVEC2014\nand DAIC-WOZ datasets both show enhancements in noise robustness. It is further\nindicated by comparison that the ARSLIF neuron model suggest the abnormal\nfiring pattern within the feature extraction on depressive audio data, offering\nbrain-inspired interpretability.", "AI": {"tldr": "本文提出了一种鲁棒的脑启发音频特征提取器（RBA-FE）模型，用于抑郁症诊断，通过改进的分层网络架构解决了音频特征提取的噪声问题。", "motivation": "现有深度学习模型在基于图像的诊断任务中表现优异，但忽略了音频特征。本文旨在解决音频特征提取中的噪声挑战。", "method": "RBA-FE利用六种声学特征提取原始音频的空间和时间特征，并引入改进的尖峰神经元模型（ARSLIF）增强噪声鲁棒性。", "result": "实验表明，RBA-FE在MODMA数据集上达到最高精度（0.8750-0.8974），并在AVEC2014和DAIC-WOZ数据集上表现出更强的噪声鲁棒性。", "conclusion": "RBA-FE不仅提升了抑郁症诊断的准确性，还通过ARSLIF模型提供了脑启发的可解释性。"}}
{"id": "2506.07350", "pdf": "https://arxiv.org/pdf/2506.07350", "abs": "https://arxiv.org/abs/2506.07350", "authors": ["Yijie Deng", "Shuaihang Yuan", "Congcong Wen", "Hao Huang", "Anthony Tzes", "Geeta Chandra Raju Bethala", "Yi Fang"], "title": "MapBERT: Bitwise Masked Modeling for Real-Time Semantic Mapping Generation", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Spatial awareness is a critical capability for embodied agents, as it enables\nthem to anticipate and reason about unobserved regions. The primary challenge\narises from learning the distribution of indoor semantics, complicated by\nsparse, imbalanced object categories and diverse spatial scales. Existing\nmethods struggle to robustly generate unobserved areas in real time and do not\ngeneralize well to new environments. To this end, we propose \\textbf{MapBERT},\na novel framework designed to effectively model the distribution of unseen\nspaces. Motivated by the observation that the one-hot encoding of semantic maps\naligns naturally with the binary structure of bit encoding, we, for the first\ntime, leverage a lookup-free BitVAE to encode semantic maps into compact\nbitwise tokens. Building on this, a masked transformer is employed to infer\nmissing regions and generate complete semantic maps from limited observations.\nTo enhance object-centric reasoning, we propose an object-aware masking\nstrategy that masks entire object categories concurrently and pairs them with\nlearnable embeddings, capturing implicit relationships between object\nembeddings and spatial tokens. By learning these relationships, the model more\neffectively captures indoor semantic distributions crucial for practical\nrobotic tasks. Experiments on Gibson benchmarks show that MapBERT achieves\nstate-of-the-art semantic map generation, balancing computational efficiency\nwith accurate reconstruction of unobserved regions.", "AI": {"tldr": "MapBERT是一种新颖的框架，通过BitVAE和掩码变换器有效建模未观察空间分布，提升语义地图生成能力。", "motivation": "现有方法难以实时生成未观察区域且泛化能力差，MapBERT旨在解决这些问题。", "method": "利用BitVAE编码语义地图为紧凑比特令牌，结合掩码变换器和对象感知掩码策略增强推理。", "result": "在Gibson基准测试中，MapBERT实现了最先进的语义地图生成，平衡了计算效率和准确性。", "conclusion": "MapBERT通过对象感知和比特编码显著提升了未观察区域的语义分布建模能力。"}}
{"id": "2506.07121", "pdf": "https://arxiv.org/pdf/2506.07121", "abs": "https://arxiv.org/abs/2506.07121", "authors": ["Ren-Jian Wang", "Ke Xue", "Zeyu Qin", "Ziniu Li", "Sheng Tang", "Hao-Tian Li", "Shengcai Liu", "Chao Qian"], "title": "Quality-Diversity Red-Teaming: Automated Generation of High-Quality and Diverse Attackers for Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.NE"], "comment": null, "summary": "Ensuring safety of large language models (LLMs) is important. Red teaming--a\nsystematic approach to identifying adversarial prompts that elicit harmful\nresponses from target LLMs--has emerged as a crucial safety evaluation method.\nWithin this framework, the diversity of adversarial prompts is essential for\ncomprehensive safety assessments. We find that previous approaches to\nred-teaming may suffer from two key limitations. First, they often pursue\ndiversity through simplistic metrics like word frequency or sentence embedding\nsimilarity, which may not capture meaningful variation in attack strategies.\nSecond, the common practice of training a single attacker model restricts\ncoverage across potential attack styles and risk categories. This paper\nintroduces Quality-Diversity Red-Teaming (QDRT), a new framework designed to\naddress these limitations. QDRT achieves goal-driven diversity through\nbehavior-conditioned training and implements a behavioral replay buffer in an\nopen-ended manner. Additionally, it trains multiple specialized attackers\ncapable of generating high-quality attacks across diverse styles and risk\ncategories. Our empirical evaluation demonstrates that QDRT generates attacks\nthat are both more diverse and more effective against a wide range of target\nLLMs, including GPT-2, Llama-3, Gemma-2, and Qwen2.5. This work advances the\nfield of LLM safety by providing a systematic and effective approach to\nautomated red-teaming, ultimately supporting the responsible deployment of\nLLMs.", "AI": {"tldr": "QDRT框架通过行为条件训练和多样性攻击策略，提升了大型语言模型的安全性评估。", "motivation": "现有红队方法在攻击多样性和覆盖范围上存在不足，QDRT旨在解决这些问题。", "method": "采用行为条件训练和开放行为重放缓冲区，训练多个专业攻击模型。", "result": "QDRT生成的攻击更具多样性和有效性，适用于多种目标模型。", "conclusion": "QDRT为自动化红队测试提供了系统性方法，支持LLM的安全部署。"}}
{"id": "2506.07400", "pdf": "https://arxiv.org/pdf/2506.07400", "abs": "https://arxiv.org/abs/2506.07400", "authors": ["Philip Liu", "Sparsh Bansal", "Jimmy Dinh", "Aditya Pawar", "Ramani Satishkumar", "Shail Desai", "Neeraj Gupta", "Xin Wang", "Shu Hu"], "title": "MedChat: A Multi-Agent Framework for Multimodal Diagnosis with Large Language Models", "categories": ["cs.MA", "cs.AI", "cs.CV", "cs.LG"], "comment": "7 pages, 6 figures. Accepted to the 2025 IEEE 8th International\n  Conference on Multimedia Information Processing and Retrieval (MIPR). Code\n  and platform available at https://github.com/Purdue-M2/MedChat", "summary": "The integration of deep learning-based glaucoma detection with large language\nmodels (LLMs) presents an automated strategy to mitigate ophthalmologist\nshortages and improve clinical reporting efficiency. However, applying general\nLLMs to medical imaging remains challenging due to hallucinations, limited\ninterpretability, and insufficient domain-specific medical knowledge, which can\npotentially reduce clinical accuracy. Although recent approaches combining\nimaging models with LLM reasoning have improved reporting, they typically rely\non a single generalist agent, restricting their capacity to emulate the diverse\nand complex reasoning found in multidisciplinary medical teams. To address\nthese limitations, we propose MedChat, a multi-agent diagnostic framework and\nplatform that combines specialized vision models with multiple role-specific\nLLM agents, all coordinated by a director agent. This design enhances\nreliability, reduces hallucination risk, and enables interactive diagnostic\nreporting through an interface tailored for clinical review and educational\nuse. Code available at https://github.com/Purdue-M2/MedChat.", "AI": {"tldr": "MedChat是一个结合专业视觉模型和多个角色特定LLM代理的多代理诊断框架，旨在解决通用LLM在医学影像中的幻觉和解释性问题，提升临床报告的可靠性和交互性。", "motivation": "解决通用LLM在医学影像应用中存在的幻觉、解释性不足和领域知识缺乏问题，同时模拟多学科医疗团队的复杂推理能力。", "method": "提出MedChat框架，结合专业视觉模型和多个角色特定的LLM代理，通过一个导演代理协调，提升诊断报告的可靠性和交互性。", "result": "MedChat减少了幻觉风险，提高了临床报告的可靠性，并提供了适合临床审查和教育用途的交互式诊断报告界面。", "conclusion": "MedChat通过多代理协作设计，有效解决了通用LLM在医学影像中的局限性，为临床诊断提供了更可靠和高效的解决方案。"}}
{"id": "2506.07413", "pdf": "https://arxiv.org/pdf/2506.07413", "abs": "https://arxiv.org/abs/2506.07413", "authors": ["Ziwen Wang", "Jiajun Fan", "Thao Nguyen", "Heng Ji", "Ge Liu"], "title": "Variational Supervised Contrastive Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Contrastive learning has proven to be highly efficient and adaptable in\nshaping representation spaces across diverse modalities by pulling similar\nsamples together and pushing dissimilar ones apart. However, two key\nlimitations persist: (1) Without explicit regulation of the embedding\ndistribution, semantically related instances can inadvertently be pushed apart\nunless complementary signals guide pair selection, and (2) excessive reliance\non large in-batch negatives and tailored augmentations hinders generalization.\nTo address these limitations, we propose Variational Supervised Contrastive\nLearning (VarCon), which reformulates supervised contrastive learning as\nvariational inference over latent class variables and maximizes a\nposterior-weighted evidence lower bound (ELBO) that replaces exhaustive\npair-wise comparisons for efficient class-aware matching and grants\nfine-grained control over intra-class dispersion in the embedding space.\nTrained exclusively on image data, our experiments on CIFAR-10, CIFAR-100,\nImageNet-100, and ImageNet-1K show that VarCon (1) achieves state-of-the-art\nperformance for contrastive learning frameworks, reaching 79.36% Top-1 accuracy\non ImageNet-1K and 78.29% on CIFAR-100 with a ResNet-50 encoder while\nconverging in just 200 epochs; (2) yields substantially clearer decision\nboundaries and semantic organization in the embedding space, as evidenced by\nKNN classification, hierarchical clustering results, and transfer-learning\nassessments; and (3) demonstrates superior performance in few-shot learning\nthan supervised baseline and superior robustness across various augmentation\nstrategies.", "AI": {"tldr": "论文提出了一种名为VarCon的变分监督对比学习方法，解决了传统对比学习的两大限制，并在多个数据集上实现了最先进的性能。", "motivation": "传统对比学习存在两个主要问题：一是缺乏对嵌入分布的显式调控，可能导致语义相关实例被错误分离；二是过度依赖大批量负样本和特定增强方法，影响泛化能力。", "method": "VarCon将监督对比学习重新表述为对潜在类别变量的变分推断，通过最大化后验加权的证据下界（ELBO）实现高效的类感知匹配，并精细控制嵌入空间中的类内分散。", "result": "在CIFAR-10、CIFAR-100、ImageNet-100和ImageNet-1K等数据集上，VarCon实现了最先进的性能（如ImageNet-1K上79.36%的Top-1准确率），并表现出更清晰的决策边界和语义组织。", "conclusion": "VarCon不仅提升了性能，还增强了嵌入空间的语义组织能力和少样本学习表现，同时展现出更强的鲁棒性。"}}
{"id": "2506.07126", "pdf": "https://arxiv.org/pdf/2506.07126", "abs": "https://arxiv.org/abs/2506.07126", "authors": ["Weihan Lu", "Hong Cai Chen"], "title": "MAGNet: A Multi-Scale Attention-Guided Graph Fusion Network for DRC Violation Detection", "categories": ["cs.AR", "cs.AI"], "comment": "9 pages, 12 figures, 2 tables", "summary": "Design rule checking (DRC) is of great significance for cost reduction and\ndesign efficiency improvement in integrated circuit (IC) designs.\nMachine-learning-based DRC has become an important approach in computer-aided\ndesign (CAD). In this paper, we propose MAGNet, a hybrid deep learning model\nthat integrates an improved U-Net with a graph neural network for DRC violation\nprediction. The U-Net backbone is enhanced with a Dynamic Attention Module\n(DAM) and a Multi-Scale Convolution Module (MSCM) to strengthen its capability\nin extracting fine-grained and multi-scale spatial features. In parallel, we\nconstruct a pixel-aligned graph structure based on chip layout tiles, and apply\na specialized GNN to model the topological relationships among pins. During\ngraph construction, a graph-to-grid mapping is generated to align GNN features\nwith the layout image. In addition, a label amplification strategy is adopted\nduring training to enhance the model's sensitivity to sparse violation\npatterns. Overall, MAGNet effectively combines spatial, semantic, and\nstructural information, achieving improved prediction accuracy and reduced\nfalse positive rates in DRC hotspot detection. Subsequently, through\nincremental training, we achieve a more sensitive discrimination ability for\nhotspots. The results demonstrate that, in comparison with ibUnet, RouteNet,\nand J-Net, MAGnet significantly outperforms these models, achieving substantial\nimprovements in overall performance.", "AI": {"tldr": "MAGNet是一种混合深度学习模型，结合改进的U-Net和图神经网络（GNN），用于集成电路设计中的DRC违规预测，显著提升了预测精度并降低了误报率。", "motivation": "DRC在集成电路设计中至关重要，机器学习方法成为CAD的重要方向，但现有模型在提取多尺度空间特征和建模拓扑关系方面存在不足。", "method": "MAGNet结合改进的U-Net（含动态注意力模块和多尺度卷积模块）和GNN，通过像素对齐图结构建模引脚拓扑关系，并采用标签放大策略增强稀疏违规模式的敏感性。", "result": "MAGNet在DRC热点检测中表现优于ibUnet、RouteNet和J-Net，显著提升了整体性能，并通过增量训练进一步优化了热点识别能力。", "conclusion": "MAGNet通过融合空间、语义和结构信息，为DRC违规预测提供了高效解决方案，具有实际应用潜力。"}}
{"id": "2506.07475", "pdf": "https://arxiv.org/pdf/2506.07475", "abs": "https://arxiv.org/abs/2506.07475", "authors": ["Gaoyu Chen"], "title": "Text-guided multi-stage cross-perception network for medical image segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Medical image segmentation plays a crucial role in clinical medicine, serving\nas a tool for auxiliary diagnosis, treatment planning, and disease monitoring,\nthus facilitating physicians in the study and treatment of diseases. However,\nexisting medical image segmentation methods are limited by the weak semantic\nexpression of the target segmentation regions, which is caused by the low\ncontrast between the target and non-target segmentation regions. To address\nthis limitation, text prompt information has greast potential to capture the\nlesion location. However, existing text-guided methods suffer from insufficient\ncross-modal interaction and inadequate cross-modal feature expression. To\nresolve these issues, we propose the Text-guided Multi-stage Cross-perception\nnetwork (TMC). In TMC, we introduce a multistage cross-attention module to\nenhance the model's understanding of semantic details and a multi-stage\nalignment loss to improve the consistency of cross-modal semantics. The results\nof the experiments demonstrate that our TMC achieves a superior performance\nwith Dice of 84.77%, 78.50%, 88.73% in three public datasets (QaTa-COV19,\nMosMedData and Breast), outperforming UNet based networks and text-guided\nmethods.", "AI": {"tldr": "论文提出了一种文本引导的多阶段交叉感知网络（TMC），用于解决医学图像分割中语义表达不足的问题，通过多阶段交叉注意力模块和多阶段对齐损失提升性能。", "motivation": "现有医学图像分割方法因目标与非目标区域对比度低导致语义表达弱，且文本引导方法存在跨模态交互和特征表达不足的问题。", "method": "提出TMC网络，引入多阶段交叉注意力模块增强语义细节理解，并使用多阶段对齐损失提升跨模态语义一致性。", "result": "TMC在三个公开数据集（QaTa-COV19、MosMedData和Breast）上Dice分数分别为84.77%、78.50%和88.73%，优于UNet和现有文本引导方法。", "conclusion": "TMC通过改进跨模态交互和特征表达，显著提升了医学图像分割的性能。"}}
{"id": "2506.07127", "pdf": "https://arxiv.org/pdf/2506.07127", "abs": "https://arxiv.org/abs/2506.07127", "authors": ["Wenke xia", "Yichu Yang", "Hongtao Wu", "Xiao Ma", "Tao Kong", "Di Hu"], "title": "Robotic Policy Learning via Human-assisted Action Preference Optimization", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Establishing a reliable and iteratively refined robotic system is essential\nfor deploying real-world applications. While Vision-Language-Action (VLA)\nmodels are widely recognized as the foundation model for such robotic\ndeployment, their dependence on expert demonstrations hinders the crucial\ncapabilities of correction and learning from failures. To mitigate this\nlimitation, we introduce a Human-assisted Action Preference Optimization method\nnamed HAPO, designed to correct deployment failures and foster effective\nadaptation through preference alignment for VLA models. This method begins with\na human-robot collaboration framework for reliable failure correction and\ninteraction trajectory collection through human intervention. These\nhuman-intervention trajectories are further employed within the action\npreference optimization process, facilitating VLA models to mitigate failure\naction occurrences while enhancing corrective action adaptation. Specifically,\nwe propose an adaptive reweighting algorithm to address the issues of\nirreversible interactions and token probability mismatch when introducing\npreference optimization into VLA models, facilitating model learning from\nbinary desirability signals derived from interactions. Through combining these\nmodules, our human-assisted action preference optimization method ensures\nreliable deployment and effective learning from failure for VLA models. The\nexperiments conducted in simulation and real-world scenarios prove superior\ngeneralization and robustness of our framework across a variety of manipulation\ntasks.", "AI": {"tldr": "论文提出了一种名为HAPO的人辅助动作偏好优化方法，旨在解决VLA模型依赖专家演示的问题，通过人类干预优化动作偏好，提升模型的纠错和学习能力。", "motivation": "VLA模型在机器人部署中依赖专家演示，限制了其从失败中学习和纠正的能力。", "method": "提出HAPO方法，结合人类干预框架和动作偏好优化，采用自适应重加权算法解决不可逆交互和标记概率不匹配问题。", "result": "实验证明HAPO在模拟和现实场景中具有优越的泛化性和鲁棒性。", "conclusion": "HAPO方法有效提升了VLA模型的可靠部署和从失败中学习的能力。"}}
{"id": "2506.07530", "pdf": "https://arxiv.org/pdf/2506.07530", "abs": "https://arxiv.org/abs/2506.07530", "authors": ["Hongyu Wang", "Chuyan Xiong", "Ruiping Wang", "Xilin Chen"], "title": "BitVLA: 1-bit Vision-Language-Action Models for Robotics Manipulation", "categories": ["cs.RO", "cs.CV"], "comment": "Work in progress", "summary": "Vision-Language-Action (VLA) models have shown impressive capabilities across\na wide range of robotics manipulation tasks. However, their growing model size\nposes significant challenges for deployment on resource-constrained robotic\nsystems. While 1-bit pretraining has proven effective for enhancing the\ninference efficiency of large language models with minimal performance loss,\nits application to VLA models remains underexplored. In this work, we present\nBitVLA, the first 1-bit VLA model for robotics manipulation, in which every\nparameter is ternary, i.e., {-1, 0, 1}. To further reduce the memory footprint\nof the vision encoder, we propose the distillation-aware training strategy that\ncompresses the full-precision encoder to 1.58-bit weights. During this process,\na full-precision encoder serves as a teacher model to better align latent\nrepresentations. Despite the lack of large-scale robotics pretraining, BitVLA\nachieves performance comparable to the state-of-the-art model OpenVLA-OFT with\n4-bit post-training quantization on the LIBERO benchmark, while consuming only\n29.8% of the memory. These results highlight BitVLA's promise for deployment on\nmemory-constrained edge devices. We release the code and model weights in\nhttps://github.com/ustcwhy/BitVLA.", "AI": {"tldr": "BitVLA是首个用于机器人操作的1-bit VLA模型，通过三元参数和1.58-bit视觉编码器压缩技术，显著减少内存占用，性能接近4-bit量化模型。", "motivation": "解决VLA模型在资源受限机器人系统上部署时因模型规模大导致的内存挑战。", "method": "采用1-bit预训练和蒸馏感知训练策略，压缩视觉编码器至1.58-bit，利用全精度教师模型对齐潜在表示。", "result": "在LIBERO基准测试中，性能接近4-bit量化模型OpenVLA-OFT，内存占用仅29.8%。", "conclusion": "BitVLA展示了在内存受限边缘设备上的部署潜力，代码和模型已开源。"}}
{"id": "2506.07134", "pdf": "https://arxiv.org/pdf/2506.07134", "abs": "https://arxiv.org/abs/2506.07134", "authors": ["Eshwar S. R.", "Gugan Thoppe", "Aditya Gopalan", "Gal Dalal"], "title": "Reliable Critics: Monotonic Improvement and Convergence Guarantees for Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "math.OC", "90C40 (Primary), 93E20, 68T05 (Secondary)", "I.2.6; G.3"], "comment": "19 pages", "summary": "Despite decades of research, it remains challenging to correctly use\nReinforcement Learning (RL) algorithms with function approximation. A prime\nexample is policy iteration, whose fundamental guarantee of monotonic\nimprovement collapses even under linear function approximation. To address this\nissue, we introduce Reliable Policy Iteration (RPI). It replaces the common\nprojection or Bellman-error minimization during policy evaluation with a\nBellman-based constrained optimization. We prove that not only does RPI confer\ntextbook monotonicity on its value estimates but these estimates also lower\nbound the true return. Also, their limit partially satisfies the unprojected\nBellman equation, emphasizing RPI's natural fit within RL. RPI is the first\nalgorithm with such monotonicity and convergence guarantees under function\napproximation. For practical use, we provide a model-free variant of RPI that\namounts to a novel critic. It can be readily integrated into primary model-free\nPI implementations such as DQN and DDPG. In classical control tasks, such\nRPI-enhanced variants consistently maintain their lower-bound guarantee while\nmatching or surpassing the performance of all baseline methods.", "AI": {"tldr": "论文提出了Reliable Policy Iteration (RPI)，解决了强化学习中函数逼近下策略迭代单调性保证失效的问题，并通过实验验证了其有效性。", "motivation": "尽管强化学习研究多年，但在函数逼近下正确使用策略迭代仍具挑战性，尤其是单调性保证的失效。", "method": "RPI通过基于贝尔曼的约束优化替代传统的投影或贝尔曼误差最小化，确保单调性和收敛性。", "result": "RPI在经典控制任务中不仅保持下界保证，还匹配或超越基线方法性能。", "conclusion": "RPI是首个在函数逼近下具有单调性和收敛性保证的算法，且易于集成到现有模型无关方法中。"}}
{"id": "2506.07135", "pdf": "https://arxiv.org/pdf/2506.07135", "abs": "https://arxiv.org/abs/2506.07135", "authors": ["José Manuel Suárez", "Luís Mariano Bibbó", "Joaquín Bogado", "Alejandro Fernandez"], "title": "Taxonomy of migration scenarios for Qiskit refactoring using LLMs", "categories": ["cs.SE", "cs.AI", "cs.ET"], "comment": "Accepted for publication in ASQC JAIIO 54\n  (https://54jaiio.sadio.org.ar/simposios/)", "summary": "As quantum computing advances, quantum programming libraries' heterogeneity\nand steady evolution create new challenges for software developers. Frequent\nupdates in software libraries break working code that needs to be refactored,\nthus adding complexity to an already complex landscape. These refactoring\nchallenges are, in many cases, fundamentally different from those known in\nclassical software engineering due to the nature of quantum computing software.\nThis study addresses these challenges by developing a taxonomy of quantum\ncircuit's refactoring problems, providing a structured framework to analyze and\ncompare different refactoring approaches. Large Language Models (LLMs) have\nproven valuable tools for classic software development, yet their value in\nquantum software engineering remains unexplored. This study uses LLMs to\ncategorize refactoring needs in migration scenarios between different Qiskit\nversions. Qiskit documentation and release notes were scrutinized to create an\ninitial taxonomy of refactoring required for migrating between Qiskit releases.\nTwo taxonomies were produced: one by expert developers and one by an LLM. These\ntaxonomies were compared, analyzing differences and similarities, and were\nintegrated into a unified taxonomy that reflects the findings of both methods.\nBy systematically categorizing refactoring challenges in Qiskit, the unified\ntaxonomy is a foundation for future research on AI-assisted migration while\nenabling a more rigorous evaluation of automated refactoring techniques.\nAdditionally, this work contributes to quantum software engineering (QSE) by\nenhancing software development workflows, improving language compatibility, and\npromoting best practices in quantum programming.", "AI": {"tldr": "该研究通过开发量子电路重构问题的分类法，利用大语言模型（LLMs）分析Qiskit版本迁移中的重构需求，并整合专家和LLM的分类结果，为量子软件工程提供系统化的重构挑战分类。", "motivation": "量子计算软件的异构性和快速更新导致代码重构问题复杂化，且这些问题与传统软件工程不同，需要专门的研究和分类方法。", "method": "通过分析Qiskit文档和发布说明，创建初始重构分类法，并分别由专家开发者和LLM生成两种分类法，最终整合为统一分类法。", "result": "生成了两种分类法并整合为统一分类法，为AI辅助迁移和自动化重构技术评估提供了基础。", "conclusion": "统一分类法为量子软件工程的研究和实践提供了支持，改进了开发流程、语言兼容性和编程最佳实践。"}}
{"id": "2506.07657", "pdf": "https://arxiv.org/pdf/2506.07657", "abs": "https://arxiv.org/abs/2506.07657", "authors": ["Zeyu Xiao", "Zhenyi Wu", "Mingyang Sun", "Qipeng Yan", "Yufan Guo", "Zhuoer Liang", "Lihua Zhang"], "title": "PIG: Physically-based Multi-Material Interaction with 3D Gaussians", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "3D Gaussian Splatting has achieved remarkable success in reconstructing both\nstatic and dynamic 3D scenes. However, in a scene represented by 3D Gaussian\nprimitives, interactions between objects suffer from inaccurate 3D\nsegmentation, imprecise deformation among different materials, and severe\nrendering artifacts. To address these challenges, we introduce PIG:\nPhysically-Based Multi-Material Interaction with 3D Gaussians, a novel approach\nthat combines 3D object segmentation with the simulation of interacting objects\nin high precision. Firstly, our method facilitates fast and accurate mapping\nfrom 2D pixels to 3D Gaussians, enabling precise 3D object-level segmentation.\nSecondly, we assign unique physical properties to correspondingly segmented\nobjects within the scene for multi-material coupled interactions. Finally, we\nhave successfully embedded constraint scales into deformation gradients,\nspecifically clamping the scaling and rotation properties of the Gaussian\nprimitives to eliminate artifacts and achieve geometric fidelity and visual\nconsistency. Experimental results demonstrate that our method not only\noutperforms the state-of-the-art (SOTA) in terms of visual quality, but also\nopens up new directions and pipelines for the field of physically realistic\nscene generation.", "AI": {"tldr": "PIG提出了一种基于物理的多材料交互方法，解决了3D高斯场景中对象交互的精度和渲染问题。", "motivation": "3D高斯场景中对象交互存在分割不准确、变形不精确和渲染伪影的问题，需要一种更精确的方法。", "method": "结合3D对象分割与高精度交互模拟，实现像素到高斯的快速映射，分配物理属性，并嵌入约束尺度到变形梯度中。", "result": "实验表明，PIG在视觉质量上优于现有方法，并为物理真实场景生成提供了新方向。", "conclusion": "PIG通过精确分割和物理模拟，显著提升了3D高斯场景的交互质量和视觉一致性。"}}
{"id": "2506.07709", "pdf": "https://arxiv.org/pdf/2506.07709", "abs": "https://arxiv.org/abs/2506.07709", "authors": ["Xihua Sheng", "Peilin Chen", "Meng Wang", "Li Zhang", "Shiqi Wang", "Dapeng Oliver Wu"], "title": "Fine-Grained Motion Compression and Selective Temporal Fusion for Neural B-Frame Video Coding", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "With the remarkable progress in neural P-frame video coding, neural B-frame\ncoding has recently emerged as a critical research direction. However, most\nexisting neural B-frame codecs directly adopt P-frame coding tools without\nadequately addressing the unique challenges of B-frame compression, leading to\nsuboptimal performance. To bridge this gap, we propose novel enhancements for\nmotion compression and temporal fusion for neural B-frame coding. First, we\ndesign a fine-grained motion compression method. This method incorporates an\ninteractive dual-branch motion auto-encoder with per-branch adaptive\nquantization steps, which enables fine-grained compression of bi-directional\nmotion vectors while accommodating their asymmetric bitrate allocation and\nreconstruction quality requirements. Furthermore, this method involves an\ninteractive motion entropy model that exploits correlations between\nbi-directional motion latent representations by interactively leveraging\npartitioned latent segments as directional priors. Second, we propose a\nselective temporal fusion method that predicts bi-directional fusion weights to\nachieve discriminative utilization of bi-directional multi-scale temporal\ncontexts with varying qualities. Additionally, this method introduces a\nhyperprior-based implicit alignment mechanism for contextual entropy modeling.\nBy treating the hyperprior as a surrogate for the contextual latent\nrepresentation, this mechanism implicitly mitigates the misalignment in the\nfused bi-directional temporal priors. Extensive experiments demonstrate that\nour proposed codec outperforms state-of-the-art neural B-frame codecs and\nachieves comparable or even superior compression performance to the H.266/VVC\nreference software under random-access configurations.", "AI": {"tldr": "论文提出了一种针对神经B帧编码的增强方法，包括细粒度运动压缩和选择性时间融合，显著提升了压缩性能。", "motivation": "现有神经B帧编码器直接采用P帧工具，未能解决B帧压缩的独特挑战，导致性能不佳。", "method": "设计了细粒度运动压缩方法（交互式双分支运动自编码器）和选择性时间融合方法（预测双向融合权重）。", "result": "实验表明，该方法优于现有神经B帧编码器，性能接近或超过H.266/VVC参考软件。", "conclusion": "提出的方法有效解决了B帧压缩的挑战，显著提升了性能。"}}
{"id": "2506.07735", "pdf": "https://arxiv.org/pdf/2506.07735", "abs": "https://arxiv.org/abs/2506.07735", "authors": ["Haizhao Jing", "Haokui Zhang", "Zhenhao Shang", "Rong Xiao", "Peng Wang", "Yanning Zhang"], "title": "Language Embedding Meets Dynamic Graph: A New Exploration for Neural Architecture Representation Learning", "categories": ["cs.LG", "cs.CV"], "comment": "9 pages, 3 figures", "summary": "Neural Architecture Representation Learning aims to transform network models\ninto feature representations for predicting network attributes, playing a\ncrucial role in deploying and designing networks for real-world applications.\nRecently, inspired by the success of transformers, transformer-based models\nintegrated with Graph Neural Networks (GNNs) have achieved significant progress\nin representation learning. However, current methods still have some\nlimitations. First, existing methods overlook hardware attribute information,\nwhich conflicts with the current trend of diversified deep learning hardware\nand limits the practical applicability of models. Second, current encoding\napproaches rely on static adjacency matrices to represent topological\nstructures, failing to capture the structural differences between computational\nnodes, which ultimately compromises encoding effectiveness. In this paper, we\nintroduce LeDG-Former, an innovative framework that addresses these limitations\nthrough the synergistic integration of language-based semantic embedding and\ndynamic graph representation learning. Specifically, inspired by large language\nmodels (LLMs), we propose a language embedding framework where both neural\narchitectures and hardware platform specifications are projected into a unified\nsemantic space through tokenization and LLM processing, enabling zero-shot\nprediction across different hardware platforms for the first time. Then, we\npropose a dynamic graph-based transformer for modeling neural architectures,\nresulting in improved neural architecture modeling performance. On the NNLQP\nbenchmark, LeDG-Former surpasses previous methods, establishing a new SOTA\nwhile demonstrating the first successful cross-hardware latency prediction\ncapability. Furthermore, our framework achieves superior performance on the\ncell-structured NAS-Bench-101 and NAS-Bench-201 datasets.", "AI": {"tldr": "LeDG-Former框架通过语言嵌入和动态图表示学习，解决了现有方法忽略硬件属性和静态邻接矩阵的问题，实现了跨硬件平台的零样本预测和更优的神经架构建模。", "motivation": "当前方法忽略了硬件属性信息，且依赖静态邻接矩阵表示拓扑结构，限制了模型的实用性和编码效果。", "method": "提出LeDG-Former框架，结合语言嵌入（通过LLM处理）和动态图表示学习，实现神经架构和硬件平台的统一语义空间投影。", "result": "在NNLQP基准测试中超越现有方法，实现跨硬件延迟预测，并在NAS-Bench-101和NAS-Bench-201数据集上表现优异。", "conclusion": "LeDG-Former通过创新方法解决了现有局限性，为神经架构表示学习提供了更高效和实用的解决方案。"}}
{"id": "2506.07153", "pdf": "https://arxiv.org/pdf/2506.07153", "abs": "https://arxiv.org/abs/2506.07153", "authors": ["Avishag Shapira", "Parth Atulbhai Gandhi", "Edan Habler", "Oleg Brodt", "Asaf Shabtai"], "title": "Mind the Web: The Security of Web Use Agents", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Web-use agents are rapidly being deployed to automate complex web tasks,\noperating with extensive browser capabilities including multi-tab navigation,\nDOM manipulation, JavaScript execution and authenticated session access.\nHowever, these powerful capabilities create a critical and previously\nunexplored attack surface. This paper demonstrates how attackers can exploit\nweb-use agents' high-privilege capabilities by embedding malicious content in\nweb pages such as comments, reviews, or advertisements that agents encounter\nduring legitimate browsing tasks. In addition, we introduce the task-aligned\ninjection technique that frame malicious commands as helpful task guidance\nrather than obvious attacks. This technique exploiting fundamental limitations\nin LLMs' contextual reasoning: agents struggle in maintaining coherent\ncontextual awareness and fail to detect when seemingly helpful web content\ncontains steering attempts that deviate from their original task goal. Through\nsystematic evaluation of four popular agents (OpenAI Operator, Browser Use, Do\nBrowser, OpenOperator), we demonstrate nine payload types that compromise\nconfidentiality, integrity, and availability, including unauthorized camera\nactivation, user impersonation, local file exfiltration, password leakage, and\ndenial of service, with validation across multiple LLMs achieving success rates\nof 80%-100%. These payloads succeed across agents with built-in safety\nmechanisms, requiring only the ability to post content on public websites,\ncreating unprecedented risks given the ease of exploitation combined with\nagents' high-privilege access. To address this attack, we propose comprehensive\nmitigation strategies including oversight mechanisms, execution constraints,\nand task-aware reasoning techniques, providing practical directions for secure\ndevelopment and deployment.", "AI": {"tldr": "论文揭示了Web-use agents的高权限能力可能被恶意利用，通过任务对齐注入技术，攻击者可嵌入恶意内容导致数据泄露、服务拒绝等风险，并提出缓解策略。", "motivation": "Web-use agents的高权限能力为攻击者提供了新的攻击面，研究旨在揭示其潜在风险并提出解决方案。", "method": "通过任务对齐注入技术，将恶意命令伪装成任务指导，利用LLMs的上下文推理局限性，系统评估了四种流行agents的九种攻击载荷。", "result": "攻击成功率高达80%-100%，包括未经授权的摄像头激活、用户冒充等，验证了攻击的普遍性和严重性。", "conclusion": "论文提出了包括监督机制、执行约束等综合缓解策略，为安全开发和部署提供了方向。"}}
{"id": "2506.07806", "pdf": "https://arxiv.org/pdf/2506.07806", "abs": "https://arxiv.org/abs/2506.07806", "authors": ["Avinash Kori", "Francesca Toni", "Ben Glocker"], "title": "Identifiable Object Representations under Spatial Ambiguities", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Modular object-centric representations are essential for *human-like\nreasoning* but are challenging to obtain under spatial ambiguities, *e.g. due\nto occlusions and view ambiguities*. However, addressing challenges presents\nboth theoretical and practical difficulties. We introduce a novel multi-view\nprobabilistic approach that aggregates view-specific slots to capture\n*invariant content* information while simultaneously learning disentangled\nglobal *viewpoint-level* information. Unlike prior single-view methods, our\napproach resolves spatial ambiguities, provides theoretical guarantees for\nidentifiability, and requires *no viewpoint annotations*. Extensive experiments\non standard benchmarks and novel complex datasets validate our method's\nrobustness and scalability.", "AI": {"tldr": "提出了一种多视角概率方法，通过聚合视角特定的槽来捕捉不变内容信息，同时学习解耦的全局视角信息，解决了空间模糊性问题。", "motivation": "模块化的对象中心表示对人类推理至关重要，但在空间模糊性（如遮挡和视角模糊）下难以获取。", "method": "引入多视角概率方法，聚合视角特定的槽以捕捉不变内容信息，并学习解耦的全局视角信息，无需视角标注。", "result": "在标准基准和新颖复杂数据集上的实验验证了方法的鲁棒性和可扩展性。", "conclusion": "该方法解决了空间模糊性问题，提供了可识别性的理论保证，且无需视角标注。"}}
{"id": "2506.07883", "pdf": "https://arxiv.org/pdf/2506.07883", "abs": "https://arxiv.org/abs/2506.07883", "authors": ["Rajat Rasal", "Avinash Kori", "Fabio De Sousa Ribeiro", "Tian Xia", "Ben Glocker"], "title": "Diffusion Counterfactual Generation with Semantic Abduction", "categories": ["cs.LG", "cs.AI", "cs.CV", "stat.ML"], "comment": "Proceedings of the 42nd International Conference on Machine Learning,\n  Vancouver, Canada", "summary": "Counterfactual image generation presents significant challenges, including\npreserving identity, maintaining perceptual quality, and ensuring faithfulness\nto an underlying causal model. While existing auto-encoding frameworks admit\nsemantic latent spaces which can be manipulated for causal control, they\nstruggle with scalability and fidelity. Advancements in diffusion models\npresent opportunities for improving counterfactual image editing, having\ndemonstrated state-of-the-art visual quality, human-aligned perception and\nrepresentation learning capabilities. Here, we present a suite of\ndiffusion-based causal mechanisms, introducing the notions of spatial, semantic\nand dynamic abduction. We propose a general framework that integrates semantic\nrepresentations into diffusion models through the lens of Pearlian causality to\nedit images via a counterfactual reasoning process. To our knowledge, this is\nthe first work to consider high-level semantic identity preservation for\ndiffusion counterfactuals and to demonstrate how semantic control enables\nprincipled trade-offs between faithful causal control and identity\npreservation.", "AI": {"tldr": "该论文提出了一种基于扩散模型的因果机制，用于改进反事实图像生成，解决了身份保留、感知质量和因果忠实性等挑战。", "motivation": "现有自编码框架在可扩展性和保真度方面存在不足，而扩散模型在视觉质量和感知对齐方面表现优异，因此探索如何将扩散模型与因果推理结合以提升反事实图像编辑效果。", "method": "提出了一套基于扩散模型的因果机制，包括空间、语义和动态反演概念，并通过Pearlian因果视角将语义表示集成到扩散模型中。", "result": "首次实现了扩散模型中高级语义身份保留的反事实图像编辑，并展示了语义控制在因果忠实性和身份保留之间的权衡。", "conclusion": "该框架为反事实图像生成提供了新的解决方案，展示了扩散模型在因果推理中的潜力。"}}
{"id": "2506.07165", "pdf": "https://arxiv.org/pdf/2506.07165", "abs": "https://arxiv.org/abs/2506.07165", "authors": ["Qi Liu", "Jingqing Ruan", "Hao Li", "Haodong Zhao", "Desheng Wang", "Jiansong Chen", "Wan Guanglu", "Xunliang Cai", "Zhi Zheng", "Tong Xu"], "title": "AMoPO: Adaptive Multi-objective Preference Optimization without Reward Models and Reference Models", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted by ACL 2025", "summary": "Existing multi-objective preference alignment methods for large language\nmodels (LLMs) face limitations: (1) the inability to effectively balance\nvarious preference dimensions, and (2) reliance on auxiliary reward/reference\nmodels introduces computational complexity. To address these challenges, we\npropose Adaptive Multi-objective Preference Optimization (AMoPO), a novel\nframework that achieves dynamic balance across preference dimensions. By\nintroducing the multi-objective optimization paradigm to use the\ndimension-aware generation metrics as implicit rewards, AMoPO aligns LLMs with\ndiverse preferences without additional reward models or reference models. We\nintroduce an adaptive weight assignment mechanism that models the generation\nspace as a Gaussian distribution, allowing dynamic prioritization of preference\ndimensions. Empirical results demonstrate that AMoPO outperforms\nstate-of-the-art baselines by 28.5%, and the experiments on 7B, 14B, and 32B\nmodels reveal the scaling ability of AMoPO. Moreover, additional analysis of\nmultiple dimensions verifies its adaptability and effectiveness. These findings\nvalidate AMoPO's capability to achieve dimension-aware preference alignment,\nhighlighting its superiority. Our codes and datasets are available at\nhttps://github.com/Javkonline/AMoPO.", "AI": {"tldr": "AMoPO是一种新型框架，通过动态平衡多目标偏好优化，无需额外奖励模型，显著提升大型语言模型的偏好对齐效果。", "motivation": "现有方法在多目标偏好对齐中存在平衡不足和计算复杂的问题，AMoPO旨在解决这些挑战。", "method": "引入多目标优化范式，利用维度感知生成指标作为隐式奖励，并采用自适应权重分配机制。", "result": "AMoPO在性能上超越现有基线28.5%，并在不同规模模型上验证了其扩展能力。", "conclusion": "AMoPO在多目标偏好对齐中表现出卓越的适应性和有效性，验证了其优越性。"}}
{"id": "2506.07897", "pdf": "https://arxiv.org/pdf/2506.07897", "abs": "https://arxiv.org/abs/2506.07897", "authors": ["Shuja Khalid", "Mohamed Ibrahim", "Yang Liu"], "title": "GaussianVAE: Adaptive Learning Dynamics of 3D Gaussians for High-Fidelity Super-Resolution", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "We present a novel approach for enhancing the resolution and geometric\nfidelity of 3D Gaussian Splatting (3DGS) beyond native training resolution.\nCurrent 3DGS methods are fundamentally limited by their input resolution,\nproducing reconstructions that cannot extrapolate finer details than are\npresent in the training views. Our work breaks this limitation through a\nlightweight generative model that predicts and refines additional 3D Gaussians\nwhere needed most. The key innovation is our Hessian-assisted sampling\nstrategy, which intelligently identifies regions that are likely to benefit\nfrom densification, ensuring computational efficiency. Unlike computationally\nintensive GANs or diffusion approaches, our method operates in real-time\n(0.015s per inference on a single consumer-grade GPU), making it practical for\ninteractive applications. Comprehensive experiments demonstrate significant\nimprovements in both geometric accuracy and rendering quality compared to\nstate-of-the-art methods, establishing a new paradigm for resolution-free 3D\nscene enhancement.", "AI": {"tldr": "提出了一种轻量级生成模型，通过Hessian辅助采样策略提升3D高斯泼溅的分辨率和几何保真度，突破输入分辨率的限制。", "motivation": "现有3DGS方法受限于输入分辨率，无法生成比训练视图更精细的细节。", "method": "使用轻量级生成模型预测并细化额外的3D高斯，采用Hessian辅助采样策略智能识别需要密集化的区域。", "result": "在几何精度和渲染质量上显著优于现有方法，实时性高（单GPU每推理0.015秒）。", "conclusion": "为分辨率无关的3D场景增强提供了新范式。"}}
{"id": "2506.07903", "pdf": "https://arxiv.org/pdf/2506.07903", "abs": "https://arxiv.org/abs/2506.07903", "authors": ["Kevin Rojas", "Yuchen Zhu", "Sichen Zhu", "Felix X. -F. Ye", "Molei Tao"], "title": "Diffuse Everything: Multimodal Diffusion Models on Arbitrary State Spaces", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": "Accepted to ICML 2025. Code available at\n  https://github.com/KevinRojas1499/Diffuse-Everything", "summary": "Diffusion models have demonstrated remarkable performance in generating\nunimodal data across various tasks, including image, video, and text\ngeneration. On the contrary, the joint generation of multimodal data through\ndiffusion models is still in the early stages of exploration. Existing\napproaches heavily rely on external preprocessing protocols, such as tokenizers\nand variational autoencoders, to harmonize varied data representations into a\nunified, unimodal format. This process heavily demands the high accuracy of\nencoders and decoders, which can be problematic for applications with limited\ndata. To lift this restriction, we propose a novel framework for building\nmultimodal diffusion models on arbitrary state spaces, enabling native\ngeneration of coupled data across different modalities. By introducing an\ninnovative decoupled noise schedule for each modality, we enable both\nunconditional and modality-conditioned generation within a single model\nsimultaneously. We empirically validate our approach for text-image generation\nand mixed-type tabular data synthesis, demonstrating that it achieves\ncompetitive performance.", "AI": {"tldr": "提出了一种新的多模态扩散模型框架，支持跨模态数据的原生生成，无需依赖外部预处理协议。", "motivation": "现有方法依赖外部预处理协议（如分词器和变分自编码器）来统一多模态数据，但这对数据有限的应用存在问题。", "method": "引入解耦的噪声调度策略，支持无条件生成和模态条件生成。", "result": "在文本-图像生成和混合类型表格数据合成中表现优异。", "conclusion": "该框架在多模态数据生成中具有竞争力，解决了现有方法的限制。"}}
{"id": "2506.07917", "pdf": "https://arxiv.org/pdf/2506.07917", "abs": "https://arxiv.org/abs/2506.07917", "authors": ["Allen Tu", "Haiyang Ying", "Alex Hanson", "Yonghan Lee", "Tom Goldstein", "Matthias Zwicker"], "title": "Speedy Deformable 3D Gaussian Splatting: Fast Rendering and Compression of Dynamic Scenes", "categories": ["cs.GR", "cs.CV"], "comment": "Project Page: https://speede3dgs.github.io/", "summary": "Recent extensions of 3D Gaussian Splatting (3DGS) to dynamic scenes achieve\nhigh-quality novel view synthesis by using neural networks to predict the\ntime-varying deformation of each Gaussian. However, performing per-Gaussian\nneural inference at every frame poses a significant bottleneck, limiting\nrendering speed and increasing memory and compute requirements. In this paper,\nwe present Speedy Deformable 3D Gaussian Splatting (SpeeDe3DGS), a general\npipeline for accelerating the rendering speed of dynamic 3DGS and 4DGS\nrepresentations by reducing neural inference through two complementary\ntechniques. First, we propose a temporal sensitivity pruning score that\nidentifies and removes Gaussians with low contribution to the dynamic scene\nreconstruction. We also introduce an annealing smooth pruning mechanism that\nimproves pruning robustness in real-world scenes with imprecise camera poses.\nSecond, we propose GroupFlow, a motion analysis technique that clusters\nGaussians by trajectory similarity and predicts a single rigid transformation\nper group instead of separate deformations for each Gaussian. Together, our\ntechniques accelerate rendering by $10.37\\times$, reduce model size by\n$7.71\\times$, and shorten training time by $2.71\\times$ on the NeRF-DS dataset.\nSpeeDe3DGS also improves rendering speed by $4.20\\times$ and $58.23\\times$ on\nthe D-NeRF and HyperNeRF vrig datasets. Our methods are modular and can be\nintegrated into any deformable 3DGS or 4DGS framework.", "AI": {"tldr": "SpeeDe3DGS通过时间敏感剪枝和GroupFlow技术，显著提升动态3D高斯泼溅的渲染速度，减少模型大小和训练时间。", "motivation": "动态3D高斯泼溅（3DGS）中，逐高斯神经网络推断导致渲染速度慢、内存和计算需求高，亟需优化。", "method": "提出时间敏感剪枝（去除低贡献高斯）和GroupFlow（基于轨迹相似性聚类预测刚性变换）两种互补技术。", "result": "在NeRF-DS数据集上，渲染速度提升10.37倍，模型大小减少7.71倍，训练时间缩短2.71倍；在D-NeRF和HyperNeRF数据集上也显著加速。", "conclusion": "SpeeDe3DGS是一种模块化方法，可集成到任何可变形3DGS或4DGS框架中，显著提升效率。"}}
{"id": "2506.07932", "pdf": "https://arxiv.org/pdf/2506.07932", "abs": "https://arxiv.org/abs/2506.07932", "authors": ["Rishit Dagli", "Yushi Guan", "Sankeerth Durvasula", "Mohammadreza Mofayezi", "Nandita Vijaykumar"], "title": "Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural Compressor", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": null, "summary": "We propose Squeeze3D, a novel framework that leverages implicit prior\nknowledge learnt by existing pre-trained 3D generative models to compress 3D\ndata at extremely high compression ratios. Our approach bridges the latent\nspaces between a pre-trained encoder and a pre-trained generation model through\ntrainable mapping networks. Any 3D model represented as a mesh, point cloud, or\na radiance field is first encoded by the pre-trained encoder and then\ntransformed (i.e. compressed) into a highly compact latent code. This latent\ncode can effectively be used as an extremely compressed representation of the\nmesh or point cloud. A mapping network transforms the compressed latent code\ninto the latent space of a powerful generative model, which is then conditioned\nto recreate the original 3D model (i.e. decompression). Squeeze3D is trained\nentirely on generated synthetic data and does not require any 3D datasets. The\nSqueeze3D architecture can be flexibly used with existing pre-trained 3D\nencoders and existing generative models. It can flexibly support different\nformats, including meshes, point clouds, and radiance fields. Our experiments\ndemonstrate that Squeeze3D achieves compression ratios of up to 2187x for\ntextured meshes, 55x for point clouds, and 619x for radiance fields while\nmaintaining visual quality comparable to many existing methods. Squeeze3D only\nincurs a small compression and decompression latency since it does not involve\ntraining object-specific networks to compress an object.", "AI": {"tldr": "Squeeze3D是一种利用预训练3D生成模型隐式先验知识的高压缩比3D数据压缩框架。", "motivation": "现有3D数据压缩方法通常需要大量真实数据训练，且压缩比有限，Squeeze3D旨在通过预训练模型实现高效压缩。", "method": "通过可训练的映射网络连接预训练编码器和生成模型的潜在空间，将3D数据（网格、点云或辐射场）编码为紧凑潜在代码，再通过生成模型解压缩。", "result": "Squeeze3D在纹理网格、点云和辐射场上分别实现2187x、55x和619x的压缩比，且视觉质量与现有方法相当。", "conclusion": "Squeeze3D无需真实数据训练，支持多种3D格式，压缩和解压缩延迟低，是一种高效灵活的3D压缩解决方案。"}}
{"id": "2506.07179", "pdf": "https://arxiv.org/pdf/2506.07179", "abs": "https://arxiv.org/abs/2506.07179", "authors": ["Kaiqi Wu", "Weiyang Kong", "Sen Zhang", "Yubao Liu", "Zitong Chen"], "title": "Regularized Adaptive Graph Learning for Large-Scale Traffic Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Traffic prediction is a critical task in spatial-temporal forecasting with\nbroad applications in travel planning and urban management. Adaptive graph\nconvolution networks have emerged as mainstream solutions due to their ability\nto learn node embeddings in a data-driven manner and capture complex latent\ndependencies. However, existing adaptive graph learning methods for traffic\nforecasting often either ignore the regularization of node embeddings, which\naccount for a significant proportion of model parameters, or face scalability\nissues from expensive graph convolution operations. To address these\nchallenges, we propose a Regularized Adaptive Graph Learning (RAGL) model.\nFirst, we introduce a regularized adaptive graph learning framework that\nsynergizes Stochastic Shared Embedding (SSE) and adaptive graph convolution via\na residual difference mechanism, achieving both embedding regularization and\nnoise suppression. Second, to ensure scalability on large road networks, we\ndevelop the Efficient Cosine Operator (ECO), which performs graph convolution\nbased on the cosine similarity of regularized embeddings with linear time\ncomplexity. Extensive experiments on four large-scale real-world traffic\ndatasets show that RAGL consistently outperforms state-of-the-art methods in\nterms of prediction accuracy and exhibits competitive computational efficiency.", "AI": {"tldr": "论文提出了一种正则化自适应图学习模型（RAGL），结合随机共享嵌入和自适应图卷积，解决了现有方法在节点嵌入正则化和计算效率上的不足。", "motivation": "交通预测在空间-时间预测中至关重要，但现有自适应图学习方法在节点嵌入正则化和计算效率上存在问题。", "method": "提出RAGL模型，结合随机共享嵌入（SSE）和自适应图卷积，并引入高效余弦算子（ECO）以提高计算效率。", "result": "在四个大规模真实交通数据集上的实验表明，RAGL在预测精度和计算效率上均优于现有方法。", "conclusion": "RAGL通过正则化嵌入和高效图卷积，显著提升了交通预测的性能和可扩展性。"}}
{"id": "2506.07998", "pdf": "https://arxiv.org/pdf/2506.07998", "abs": "https://arxiv.org/abs/2506.07998", "authors": ["Boya Zeng", "Yida Yin", "Zhiqiu Xu", "Zhuang Liu"], "title": "Generative Modeling of Weights: Generalization or Memorization?", "categories": ["cs.LG", "cs.CV"], "comment": "Project page at https://boyazeng.github.io/weight_memorization", "summary": "Generative models, with their success in image and video generation, have\nrecently been explored for synthesizing effective neural network weights. These\napproaches take trained neural network checkpoints as training data, and aim to\ngenerate high-performing neural network weights during inference. In this work,\nwe examine four representative methods on their ability to generate novel model\nweights, i.e., weights that are different from the checkpoints seen during\ntraining. Surprisingly, we find that these methods synthesize weights largely\nby memorization: they produce either replicas, or at best simple\ninterpolations, of the training checkpoints. Current methods fail to outperform\nsimple baselines, such as adding noise to the weights or taking a simple weight\nensemble, in obtaining different and simultaneously high-performing models. We\nfurther show that this memorization cannot be effectively mitigated by\nmodifying modeling factors commonly associated with memorization in image\ndiffusion models, or applying data augmentations. Our findings provide a\nrealistic assessment of what types of data current generative models can model,\nand highlight the need for more careful evaluation of generative models in new\ndomains. Our code is available at\nhttps://github.com/boyazeng/weight_memorization.", "AI": {"tldr": "生成模型在图像和视频生成中表现出色，但在生成神经网络权重时主要通过记忆训练检查点，未能产生新颖且高性能的权重。", "motivation": "探索生成模型在合成神经网络权重方面的能力，并评估其是否能生成新颖且高性能的权重。", "method": "研究了四种代表性方法，分析其生成权重的表现，并与简单基线方法（如添加噪声或权重集成）进行比较。", "result": "发现当前方法主要通过记忆训练检查点生成权重，未能超越简单基线方法。尝试缓解记忆问题的方法也无效。", "conclusion": "生成模型在新领域的应用需要更谨慎的评估，当前方法在权重生成方面存在局限性。"}}
{"id": "2506.07211", "pdf": "https://arxiv.org/pdf/2506.07211", "abs": "https://arxiv.org/abs/2506.07211", "authors": ["Gionnieve Lim", "Bryan Chen Zhengyu Tan", "Kellie Yu Hui Sim", "Weiyan Shi", "Ming Hui Chew", "Ming Shan Hee", "Roy Ka-Wei Lee", "Simon T. Perrault", "Kenny Tsu Wei Choo"], "title": "Sword and Shield: Uses and Strategies of LLMs in Navigating Disinformation", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The emergence of Large Language Models (LLMs) presents a dual challenge in\nthe fight against disinformation. These powerful tools, capable of generating\nhuman-like text at scale, can be weaponised to produce sophisticated and\npersuasive disinformation, yet they also hold promise for enhancing detection\nand mitigation strategies. This paper investigates the complex dynamics between\nLLMs and disinformation through a communication game that simulates online\nforums, inspired by the game Werewolf, with 25 participants. We analyse how\nDisinformers, Moderators, and Users leverage LLMs to advance their goals,\nrevealing both the potential for misuse and combating disinformation. Our\nfindings highlight the varying uses of LLMs depending on the participants'\nroles and strategies, underscoring the importance of understanding their\neffectiveness in this context. We conclude by discussing implications for\nfuture LLM development and online platform design, advocating for a balanced\napproach that empowers users and fosters trust while mitigating the risks of\nLLM-assisted disinformation.", "AI": {"tldr": "论文探讨了大型语言模型（LLMs）在虚假信息中的双重作用，通过模拟在线论坛的游戏实验，分析了不同角色如何利用LLMs，并提出了平衡发展的建议。", "motivation": "研究LLMs在虚假信息传播中的潜在威胁与应对潜力，揭示其在不同角色中的动态应用。", "method": "采用受狼人杀启发的通信游戏，模拟在线论坛，25名参与者分别扮演不同角色，分析LLMs的使用策略。", "result": "发现LLMs的使用因角色和策略而异，既有被滥用的风险，也有助于检测和应对虚假信息。", "conclusion": "建议未来LLM开发和平台设计需平衡用户赋能与风险控制，以应对LLM辅助的虚假信息挑战。"}}
{"id": "2506.07232", "pdf": "https://arxiv.org/pdf/2506.07232", "abs": "https://arxiv.org/abs/2506.07232", "authors": ["Xinran Li", "Chenjia Bai", "Zijian Li", "Jiakun Zheng", "Ting Xiao", "Jun Zhang"], "title": "Learn as Individuals, Evolve as a Team: Multi-agent LLMs Adaptation in Embodied Environments", "categories": ["cs.MA", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) possess extensive knowledge bases and strong\nreasoning capabilities, making them promising tools for complex, multi-agent\nplanning in embodied environments. However, despite LLMs' advanced abilities\nand the sophisticated modular design of agentic methods, existing LLM-based\nplanning algorithms remain limited by weak adaptation capabilities to\nmulti-agent embodied scenarios. We address this limitation by introducing a\nframework that enables LLM agents to learn and evolve both before and during\ntest time, equipping them with environment-relevant knowledge for better\nplanning and enhanced communication for improved cooperation. Inspired by\ncentralized training with decentralized execution in multi-agent reinforcement\nlearning, we propose a \\textit{Learn as Individuals, Evolve as a Team (LIET)}\nparadigm for multi-agent LLMs adaptation. At the individual level, LLM agents\nlearn a local utility function from exploratory datasets to better comprehend\nthe embodied environment, which is then queried during test time to support\ninformed decision-making. At the team level, LLM agents collaboratively and\niteratively maintain and update a shared cooperation knowledge list based on\nnew experiences, using it to guide more effective communication. By combining\nindividual learning with team evolution, LIET enables comprehensive and\nflexible adaptation for LLM agents. Our experiments on Communicative\nWatch-And-Help and ThreeD-World Multi-Agent Transport benchmarks demonstrate\nthat LIET, instantiated with both LLaMA and GPT-4o, outperforms existing\nbaselines and exhibits strong cooperative planning abilities.", "AI": {"tldr": "论文提出了一种名为LIET的框架，通过个体学习和团队进化提升LLM在多智能体具身环境中的适应性和协作能力。", "motivation": "尽管LLM在多智能体规划中表现出潜力，但其适应能力较弱，限制了实际应用。", "method": "提出LIET范式，结合个体学习（局部效用函数）和团队进化（共享合作知识列表），提升LLM的适应性和协作能力。", "result": "在Communicative Watch-And-Help和ThreeD-World Multi-Agent Transport基准测试中，LIET表现优于现有基线。", "conclusion": "LIET框架通过个体与团队的协同学习，显著提升了LLM在多智能体具身环境中的规划能力。"}}
{"id": "2506.07239", "pdf": "https://arxiv.org/pdf/2506.07239", "abs": "https://arxiv.org/abs/2506.07239", "authors": ["Raghu Vamshi Hemadri", "Jitendra Bhandari", "Johann Knechtel", "Badri P Gopalan", "Ramesh Narayanaswamy", "Ramesh Karri", "Siddharth Garg"], "title": "VeriLoC: Line-of-Code Level Prediction of Hardware Design Quality from Verilog Code", "categories": ["cs.AR", "cs.AI"], "comment": null, "summary": "Modern chip design is complex, and there is a crucial need for early-stage\nprediction of key design-quality metrics like timing and routing congestion\ndirectly from Verilog code (a commonly used programming language for hardware\ndesign). It is especially important yet complex to predict individual lines of\ncode that cause timing violations or downstream routing congestion. Prior works\nhave tried approaches like converting Verilog into an intermediate graph\nrepresentation and using LLM embeddings alongside other features to predict\nmodule-level quality, but did not consider line-level quality prediction. We\npropose VeriLoC, the first method that predicts design quality directly from\nVerilog at both the line- and module-level. To this end, VeriLoC leverages\nrecent Verilog code-generation LLMs to extract local line-level and\nmodule-level embeddings, and train downstream classifiers/regressors on\nconcatenations of these embeddings. VeriLoC achieves high F1-scores of\n0.86-0.95 for line-level congestion and timing prediction, and reduces the mean\naverage percentage error from 14% - 18% for SOTA methods down to only 4%. We\nbelieve that VeriLoC embeddings and insights from our work will also be of\nvalue for other predictive and optimization tasks for complex hardware design.", "AI": {"tldr": "VeriLoC是一种新方法，直接从Verilog代码中预测设计和模块级别的质量，利用LLM嵌入和分类器/回归器，显著提升了预测精度。", "motivation": "现代芯片设计复杂，需要早期预测关键设计质量指标（如时序和路由拥塞），但现有方法未考虑代码行级别的预测。", "method": "VeriLoC利用Verilog代码生成的LLM提取行级和模块级嵌入，并训练分类器/回归器。", "result": "VeriLoC在行级拥塞和时序预测上F1分数达0.86-0.95，平均误差从14%-18%降至4%。", "conclusion": "VeriLoC的嵌入和方法对其他硬件设计预测和优化任务也有价值。"}}
{"id": "2506.07240", "pdf": "https://arxiv.org/pdf/2506.07240", "abs": "https://arxiv.org/abs/2506.07240", "authors": ["Roy Eisenstadt", "Itamar Zimerman", "Lior Wolf"], "title": "Overclocking LLM Reasoning: Monitoring and Controlling Thinking Path Lengths in LLMs", "categories": ["cs.LG", "cs.AI", "I.2.6; I.2.7"], "comment": null, "summary": "Recently, techniques such as explicit structured reasoning have demonstrated\nstrong test-time scaling behavior by enforcing a separation between the model's\ninternal \"thinking\" process and the final response. A key factor influencing\nanswer quality in this setting is the length of the thinking stage. When the\nreasoning is too short, the model may fail to capture the complexity of the\ntask. Conversely, when it is too long, the model may overthink, leading to\nunnecessary computation and degraded performance. This paper explores and\nexploits the underlying mechanisms by which LLMs understand and regulate the\nlength of their reasoning during explicit thought processes. First, we show\nthat LLMs encode their progress through the reasoning process and introduce an\ninteractive progress bar visualization, which is then used to reveal insights\non the model's planning dynamics. Second, we manipulate the internal progress\nencoding during inference to reduce unnecessary steps and generate a more\nconcise and decisive chain of thoughts. Our empirical results demonstrate that\nthis \"overclocking\" method mitigates overthinking, improves answer accuracy,\nand reduces inference latency. Our code is publicly available.", "AI": {"tldr": "论文探讨了LLMs在显式推理过程中如何理解和调节推理长度，提出了通过可视化进度条和操纵内部进度编码来优化推理过程的方法，以减少不必要的步骤并提升性能。", "motivation": "显式结构化推理技术在测试时表现出良好的扩展性，但推理长度对答案质量有重要影响。推理过短可能无法捕捉任务复杂性，过长则可能导致过度思考和性能下降。", "method": "1. 展示LLMs如何编码推理进度，并引入交互式进度条可视化；2. 在推理过程中操纵内部进度编码以减少不必要的步骤，生成更简洁的思维链。", "result": "实验结果表明，这种“超频”方法能减少过度思考，提高答案准确性，并降低推理延迟。", "conclusion": "通过优化推理长度调节机制，可以有效提升LLMs在显式推理任务中的性能和效率。"}}
{"id": "2506.07276", "pdf": "https://arxiv.org/pdf/2506.07276", "abs": "https://arxiv.org/abs/2506.07276", "authors": ["Suho Shin", "Chenghao Yang", "Haifeng Xu", "Mohammad T. Hajiaghayi"], "title": "Tokenized Bandit for LLM Decoding and Alignment", "categories": ["cs.LG", "cs.AI"], "comment": "To appear at ICML 2025", "summary": "We introduce the tokenized linear bandit (TLB) and multi-armed bandit (TMAB),\nvariants of linear and stochastic multi-armed bandit problems inspired by LLM\ndecoding and alignment. In these problems, at each round $t \\in [T]$, a user\nsubmits a query (context), and the decision maker (DM) sequentially selects a\ntoken irrevocably from a token set. Once the sequence is complete, the DM\nobserves a random utility from the user, whose expectation is presented by a\nsequence function mapping the chosen token sequence to a nonnegative real value\nthat depends on the query.\n  In both problems, we first show that learning is impossible without any\nstructure on the sequence function. We introduce a natural assumption,\ndiminishing distance with more commons (DDMC), and propose algorithms with\nregret $\\tilde{O}(L\\sqrt{T})$ and $\\tilde{O}(L\\sqrt{T^{2/3}})$ for TLB and\nTMAB, respectively. As a side product, we obtain an (almost) optimality of the\ngreedy decoding for LLM decoding algorithm under DDMC, which justifies the\nunresaonable effectiveness of greedy decoding in several tasks. This also has\nan immediate application to decoding-time LLM alignment, when the misaligned\nutility can be represented as the frozen LLM's utility and a linearly\nrealizable latent function. We finally validate our algorithm's performance\nempirically as well as verify our assumptions using synthetic and real-world\ndatasets.", "AI": {"tldr": "论文提出了基于LLM解码和对齐的tokenized线性老虎机（TLB）和多臂老虎机（TMAB）问题，证明了在无结构序列函数下学习不可行，并提出了DDMC假设及相应算法。", "motivation": "研究LLM解码和对齐问题，探索在token序列选择中的学习机制。", "method": "引入DDMC假设，设计算法实现TLB和TMAB的后悔界分别为$\\tilde{O}(L\\sqrt{T})$和$\\tilde{O}(L\\sqrt{T^{2/3}})$。", "result": "验证了贪婪解码在DDMC下的最优性，并展示了算法在合成和真实数据集上的表现。", "conclusion": "DDMC假设和算法为LLM解码和对齐提供了理论支持，验证了贪婪解码的有效性。"}}
{"id": "2506.07281", "pdf": "https://arxiv.org/pdf/2506.07281", "abs": "https://arxiv.org/abs/2506.07281", "authors": ["Leah Hope Ajmani", "Nuredin Ali Abdelkadir", "Stevie Chancellor"], "title": "Secondary Stakeholders in AI: Fighting for, Brokering, and Navigating Agency", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "As AI technologies become more human-facing, there have been numerous calls\nto adapt participatory approaches to AI development -- spurring the idea of\nparticipatory AI. However, these calls often focus only on primary\nstakeholders, such as end-users, and not secondary stakeholders. This paper\nseeks to translate the ideals of participatory AI to a broader population of\nsecondary AI stakeholders through semi-structured interviews. We theorize that\nmeaningful participation involves three participatory ideals: (1) informedness,\n(2) consent, and (3) agency. We also explore how secondary stakeholders realize\nthese ideals by traversing a complicated problem space. Like walking up the\nrungs of a ladder, these ideals build on one another. We introduce three\nstakeholder archetypes: the reluctant data contributor, the unsupported\nactivist, and the well-intentioned practitioner, who must navigate systemic\nbarriers to achieving agentic AI relationships. We envision an AI future where\nsecondary stakeholders are able to meaningfully participate with the AI systems\nthey influence and are influenced by.", "AI": {"tldr": "本文探讨了如何将参与式AI的理念扩展到次要利益相关者，提出了三个参与理想（知情、同意和能动性），并通过半结构化访谈研究了次要利益相关者如何实现这些理想。", "motivation": "随着AI技术越来越面向人类，许多呼吁要求采用参与式方法开发AI，但这些呼吁往往仅关注主要利益相关者（如终端用户），而忽略了次要利益相关者。本文旨在将参与式AI的理念推广到更广泛的次要利益相关者群体。", "method": "通过半结构化访谈，研究次要利益相关者如何实现三个参与理想（知情、同意和能动性），并提出了三种利益相关者原型。", "result": "研究发现，次要利益相关者在实现参与理想时需要克服系统性障碍，并提出了三种原型：不情愿的数据贡献者、无支持的活动家和善意的实践者。", "conclusion": "本文展望了一个未来，次要利益相关者能够有意义地参与影响和被AI系统影响的过程。"}}
{"id": "2506.07298", "pdf": "https://arxiv.org/pdf/2506.07298", "abs": "https://arxiv.org/abs/2506.07298", "authors": ["Yijia Dai", "Zhaolin Gao", "Yahya Satter", "Sarah Dean", "Jennifer J. Sun"], "title": "Pre-trained Large Language Models Learn Hidden Markov Models In-context", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Hidden Markov Models (HMMs) are foundational tools for modeling sequential\ndata with latent Markovian structure, yet fitting them to real-world data\nremains computationally challenging. In this work, we show that pre-trained\nlarge language models (LLMs) can effectively model data generated by HMMs via\nin-context learning (ICL)$\\unicode{x2013}$their ability to infer patterns from\nexamples within a prompt. On a diverse set of synthetic HMMs, LLMs achieve\npredictive accuracy approaching the theoretical optimum. We uncover novel\nscaling trends influenced by HMM properties, and offer theoretical conjectures\nfor these empirical observations. We also provide practical guidelines for\nscientists on using ICL as a diagnostic tool for complex data. On real-world\nanimal decision-making tasks, ICL achieves competitive performance with models\ndesigned by human experts. To our knowledge, this is the first demonstration\nthat ICL can learn and predict HMM-generated sequences$\\unicode{x2013}$an\nadvance that deepens our understanding of in-context learning in LLMs and\nestablishes its potential as a powerful tool for uncovering hidden structure in\ncomplex scientific data.", "AI": {"tldr": "LLMs通过上下文学习（ICL）能够有效建模HMM生成的数据，接近理论最优预测准确率，并在真实动物决策任务中表现优异。", "motivation": "解决HMM拟合真实数据时计算复杂的问题，探索LLMs在建模HMM生成数据中的潜力。", "method": "利用预训练LLMs的ICL能力，在合成HMM数据上测试预测性能，并分析HMM特性对结果的影响。", "result": "LLMs在合成HMM数据上表现接近理论最优，在真实动物决策任务中与专家设计模型竞争。", "conclusion": "ICL能够学习和预测HMM生成序列，为复杂科学数据中的隐藏结构分析提供了新工具。"}}
{"id": "2506.07311", "pdf": "https://arxiv.org/pdf/2506.07311", "abs": "https://arxiv.org/abs/2506.07311", "authors": ["Thomas Joshi", "Herman Saini", "Neil Dhillon", "Antoni Viros i Martin", "Kaoutar El Maghraoui"], "title": "Paged Attention Meets FlexAttention: Unlocking Long-Context Efficiency in Deployed Inference", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) encounter severe memory inefficiencies during\nlong-context inference due to conventional handling of key-value (KV) caches.\nIn this work, we introduce a novel integration of PagedAttention with PyTorch's\nFlexAttention, addressing internal fragmentation and inefficiencies associated\nwith monolithic KV cache allocations. Implemented within IBM's Foundation Model\nStack (FMS), our fused attention kernel efficiently gathers scattered KV data.\nOur benchmarks on an NVIDIA L4 GPU (24GB) demonstrate significantly reduced\ninference latency, growing only linearly (~2x) with sequence length from 128 to\n2048 tokens when utilizing a global KV cache, compared to exponential latency\nincreases without caching. While peak memory usage remains largely unchanged\nfor single-step evaluations (dominated by model weights and activations), paged\nattention causes minimal incremental memory usage, observable only at sequence\nlengths exceeding 2048 tokens due to its power-of-two cache allocations. We\nopen-source the full implementation and discuss its implications for future\nlong-context model deployment.", "AI": {"tldr": "论文提出了一种结合PagedAttention和PyTorch的FlexAttention的方法，解决了LLMs在长上下文推理中的内存效率问题，显著降低了推理延迟。", "motivation": "传统KV缓存的分配方式导致内存效率低下，影响长上下文推理的性能。", "method": "通过集成PagedAttention和FlexAttention，优化KV缓存分配，减少内存碎片化。", "result": "在NVIDIA L4 GPU上测试，推理延迟显著降低，且随序列长度线性增长。", "conclusion": "该方法为未来长上下文模型的部署提供了高效解决方案，并开源了实现代码。"}}
{"id": "2506.07312", "pdf": "https://arxiv.org/pdf/2506.07312", "abs": "https://arxiv.org/abs/2506.07312", "authors": ["Yusuf Elnady"], "title": "Generative Modeling of Networked Time-Series via Transformer Architectures", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Many security and network applications require having large datasets to train\nthe machine learning models. Limited data access is a well-known problem in the\nsecurity domain. Recent studies have shown the potential of Transformer models\nto enlarge the size of data by synthesizing new samples, but the synthesized\nsamples don't improve the models over the real data. To address this issue, we\ndesign an efficient transformer-based model as a generative framework to\ngenerate time-series data, that can be used to boost the performance of\nexisting and new ML workflows. Our new transformer model achieves the SOTA\nresults. We style our model to be generalizable and work across different\ndatasets, and produce high-quality samples.", "AI": {"tldr": "提出了一种基于Transformer的高效生成模型，用于合成时间序列数据，以解决安全领域数据不足的问题，并提升机器学习模型的性能。", "motivation": "安全领域的数据访问受限，现有方法合成的数据无法有效提升模型性能。", "method": "设计了一种高效的Transformer生成框架，用于生成高质量的时间序列数据。", "result": "模型在多个数据集上实现了SOTA性能，并生成了高质量样本。", "conclusion": "该Transformer模型具有通用性，能够显著提升机器学习工作流的性能。"}}
{"id": "2506.07315", "pdf": "https://arxiv.org/pdf/2506.07315", "abs": "https://arxiv.org/abs/2506.07315", "authors": ["Zonghan Wu", "Junlin Wang", "Congyuan Zou", "Chenhan Wang", "Yilei Shao"], "title": "Towards Competent AI for Fundamental Analysis in Finance: A Benchmark Dataset and Evaluation", "categories": ["q-fin.ST", "cs.AI"], "comment": null, "summary": "Generative AI, particularly large language models (LLMs), is beginning to\ntransform the financial industry by automating tasks and helping to make sense\nof complex financial information. One especially promising use case is the\nautomatic creation of fundamental analysis reports, which are essential for\nmaking informed investment decisions, evaluating credit risks, guiding\ncorporate mergers, etc. While LLMs attempt to generate these reports from a\nsingle prompt, the risks of inaccuracy are significant. Poor analysis can lead\nto misguided investments, regulatory issues, and loss of trust. Existing\nfinancial benchmarks mainly evaluate how well LLMs answer financial questions\nbut do not reflect performance in real-world tasks like generating financial\nanalysis reports. In this paper, we propose FinAR-Bench, a solid benchmark\ndataset focusing on financial statement analysis, a core competence of\nfundamental analysis. To make the evaluation more precise and reliable, we\nbreak this task into three measurable steps: extracting key information,\ncalculating financial indicators, and applying logical reasoning. This\nstructured approach allows us to objectively assess how well LLMs perform each\nstep of the process. Our findings offer a clear understanding of LLMs current\nstrengths and limitations in fundamental analysis and provide a more practical\nway to benchmark their performance in real-world financial settings.", "AI": {"tldr": "论文提出了FinAR-Bench，一个专注于财务报表分析的基准数据集，用于评估大型语言模型（LLMs）在生成财务分析报告中的表现。", "motivation": "现有金融基准主要评估LLMs回答财务问题的能力，但未能反映其在生成财务分析报告等实际任务中的表现。", "method": "将财务报表分析任务分解为三个可衡量的步骤：提取关键信息、计算财务指标和应用逻辑推理。", "result": "研究结果明确了LLMs在财务报表分析中的优势和局限性，并提供了更实用的性能评估方法。", "conclusion": "FinAR-Bench为评估LLMs在真实金融场景中的表现提供了更精确和可靠的基准。"}}
{"id": "2506.07323", "pdf": "https://arxiv.org/pdf/2506.07323", "abs": "https://arxiv.org/abs/2506.07323", "authors": ["Haoyuan Yang", "Yue Zhang", "Liqiang Jing"], "title": "Speech Recognition on TV Series with Video-guided Post-Correction", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Automatic Speech Recognition (ASR) has achieved remarkable success with deep\nlearning, driving advancements in conversational artificial intelligence, media\ntranscription, and assistive technologies. However, ASR systems still struggle\nin complex environments such as TV series, where overlapping speech,\ndomain-specific terminology, and long-range contextual dependencies pose\nsignificant challenges to transcription accuracy. Existing multimodal\napproaches fail to correct ASR outputs with the rich temporal and contextual\ninformation available in video. To address this limitation, we propose a novel\nmultimodal post-correction framework that refines ASR transcriptions by\nleveraging contextual cues extracted from video. Our framework consists of two\nstages: ASR Generation and Video-based Post-Correction, where the first stage\nproduces the initial transcript and the second stage corrects errors using\nVideo-based Contextual Information Extraction and Context-aware ASR Correction.\nWe employ the Video-Large Multimodal Model (VLMM) to extract key contextual\ninformation using tailored prompts, which is then integrated with a Large\nLanguage Model (LLM) to refine the ASR output. We evaluate our method on a\nmultimodal benchmark for TV series ASR and demonstrate its effectiveness in\nimproving ASR performance by leveraging video-based context to enhance\ntranscription accuracy in complex multimedia environments.", "AI": {"tldr": "提出了一种基于视频上下文的多模态后校正框架，通过提取视频中的上下文信息来改进自动语音识别（ASR）在复杂环境中的转录准确性。", "motivation": "现有ASR系统在复杂环境（如电视剧）中表现不佳，多模态方法未能充分利用视频中的丰富信息。", "method": "采用两阶段框架：ASR生成和基于视频的后校正，结合VLMM提取上下文信息并通过LLM修正ASR输出。", "result": "在电视剧ASR多模态基准测试中，该方法通过视频上下文显著提升了转录准确性。", "conclusion": "该框架有效解决了复杂环境中ASR的挑战，为多媒体转录提供了新思路。"}}
{"id": "2506.07330", "pdf": "https://arxiv.org/pdf/2506.07330", "abs": "https://arxiv.org/abs/2506.07330", "authors": ["Yash Datta", "Sharath Rajasekar"], "title": "JavelinGuard: Low-Cost Transformer Architectures for LLM Security", "categories": ["cs.LG", "cs.AI", "cs.CR", "I.2.7"], "comment": "16 pages, 1 Figure and 5 Tables", "summary": "We present JavelinGuard, a suite of low-cost, high-performance model\narchitectures designed for detecting malicious intent in Large Language Model\n(LLM) interactions, optimized specifically for production deployment. Recent\nadvances in transformer architectures, including compact BERT(Devlin et al.\n2019) variants (e.g., ModernBERT (Warner et al. 2024)), allow us to build\nhighly accurate classifiers with as few as approximately 400M parameters that\nachieve rapid inference speeds even on standard CPU hardware. We systematically\nexplore five progressively sophisticated transformer-based architectures:\nSharanga (baseline transformer classifier), Mahendra (enhanced\nattention-weighted pooling with deeper heads), Vaishnava and Ashwina (hybrid\nneural ensemble architectures), and Raudra (an advanced multi-task framework\nwith specialized loss functions). Our models are rigorously benchmarked across\nnine diverse adversarial datasets, including popular sets like the NotInject\nseries, BIPIA, Garak, ImprovedLLM, ToxicChat, WildGuard, and our newly\nintroduced JavelinBench, specifically crafted to test generalization on\nchallenging borderline and hard-negative cases. Additionally, we compare our\narchitectures against leading open-source guardrail models as well as large\ndecoder-only LLMs such as gpt-4o, demonstrating superior cost-performance\ntrade-offs in terms of accuracy, and latency. Our findings reveal that while\nRaudra's multi-task design offers the most robust performance overall, each\narchitecture presents unique trade-offs in speed, interpretability, and\nresource requirements, guiding practitioners in selecting the optimal balance\nof complexity and efficiency for real-world LLM security applications.", "AI": {"tldr": "JavelinGuard是一套低成本、高性能的模型架构，用于检测大型语言模型（LLM）交互中的恶意意图，专为生产部署优化。", "motivation": "随着transformer架构的进步，需要高效且准确的恶意意图检测模型，以适应生产环境的需求。", "method": "探索了五种逐步复杂的transformer架构，包括Sharanga、Mahendra、Vaishnava、Ashwina和Raudra，并在九个对抗性数据集上进行了基准测试。", "result": "Raudra的多任务设计表现最稳健，但每种架构在速度、可解释性和资源需求上各有优劣。", "conclusion": "为实际LLM安全应用提供了复杂性与效率之间的最佳平衡选择。"}}
{"id": "2506.07339", "pdf": "https://arxiv.org/pdf/2506.07339", "abs": "https://arxiv.org/abs/2506.07339", "authors": ["Kevin Black", "Manuel Y. Galliker", "Sergey Levine"], "title": "Real-Time Execution of Action Chunking Flow Policies", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Modern AI systems, especially those interacting with the physical world,\nincreasingly require real-time performance. However, the high latency of\nstate-of-the-art generalist models, including recent vision-language action\nmodels (VLAs), poses a significant challenge. While action chunking has enabled\ntemporal consistency in high-frequency control tasks, it does not fully address\nthe latency problem, leading to pauses or out-of-distribution jerky movements\nat chunk boundaries. This paper presents a novel inference-time algorithm that\nenables smooth asynchronous execution of action chunking policies. Our method,\nreal-time chunking (RTC), is applicable to any diffusion- or flow-based VLA out\nof the box with no re-training. It generates the next action chunk while\nexecuting the current one, \"freezing\" actions guaranteed to execute and\n\"inpainting\" the rest. To test RTC, we introduce a new benchmark of 12 highly\ndynamic tasks in the Kinetix simulator, as well as evaluate 6 challenging\nreal-world bimanual manipulation tasks. Results demonstrate that RTC is fast,\nperformant, and uniquely robust to inference delay, significantly improving\ntask throughput and enabling high success rates in precise tasks\n$\\unicode{x2013}$ such as lighting a match $\\unicode{x2013}$ even in the\npresence of significant latency. See\nhttps://pi.website/research/real_time_chunking for videos.", "AI": {"tldr": "本文提出了一种实时分块（RTC）算法，用于解决AI系统在高频控制任务中的延迟问题，通过异步执行动作分块策略实现平滑操作。", "motivation": "现代AI系统在物理世界交互中需要实时性能，但现有通用模型的高延迟导致动作分块边界出现停顿或不连贯动作。", "method": "RTC算法无需重新训练，适用于任何基于扩散或流的视觉语言动作模型，通过在执行当前动作分块时生成下一个分块，实现平滑过渡。", "result": "在Kinetix模拟器和真实世界双手机器人任务中，RTC显著提高了任务吞吐量和成功率，尤其在存在延迟时表现优异。", "conclusion": "RTC是一种高效、鲁棒的解决方案，能够显著提升AI系统在实时任务中的性能。"}}
{"id": "2506.07347", "pdf": "https://arxiv.org/pdf/2506.07347", "abs": "https://arxiv.org/abs/2506.07347", "authors": ["Armin Lederer", "Erfaun Noorani", "Andreas Krause"], "title": "Distributed Risk-Sensitive Safety Filters for Uncertain Discrete-Time Systems", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY"], "comment": null, "summary": "Ensuring safety in multi-agent systems is a significant challenge,\nparticularly in settings where centralized coordination is impractical. In this\nwork, we propose a novel risk-sensitive safety filter for discrete-time\nmulti-agent systems with uncertain dynamics that leverages control barrier\nfunctions (CBFs) defined through value functions. Our approach relies on\ncentralized risk-sensitive safety conditions based on exponential risk\noperators to ensure robustness against model uncertainties. We introduce a\ndistributed formulation of the safety filter by deriving two alternative\nstrategies: one based on worst-case anticipation and another on proximity to a\nknown safe policy. By allowing agents to switch between strategies, feasibility\ncan be ensured. Through detailed numerical evaluations, we demonstrate the\nefficacy of our approach in maintaining safety without being overly\nconservative.", "AI": {"tldr": "提出了一种基于控制屏障函数（CBF）的风险敏感安全过滤器，用于离散时间多智能体系统，通过两种分布式策略确保安全性。", "motivation": "在集中协调不可行的情况下，确保多智能体系统的安全性是一个重要挑战。", "method": "利用基于指数风险算子的集中化风险敏感安全条件，提出两种分布式策略：最坏情况预测和接近已知安全策略。", "result": "数值评估表明，该方法在保持安全性的同时避免了过度保守。", "conclusion": "该方法有效解决了多智能体系统中的安全挑战，且具有实际可行性。"}}
{"id": "2506.07355", "pdf": "https://arxiv.org/pdf/2506.07355", "abs": "https://arxiv.org/abs/2506.07355", "authors": ["Yuya Okada", "Takayuki Nishio"], "title": "SALT: A Lightweight Model Adaptation Method for Closed Split Computing Environments", "categories": ["cs.LG", "cs.AI", "cs.NI"], "comment": "6 pages, submitted to IEEE Globecom 2025 (under review)", "summary": "We propose SALT (Split-Adaptive Lightweight Tuning), a lightweight model\nadaptation framework for Split Computing under closed constraints, where the\nhead and tail networks are proprietary and inaccessible to users. In such\nclosed environments, conventional adaptation methods are infeasible since they\nrequire access to model parameters or architectures. SALT addresses this\nchallenge by introducing a compact, trainable adapter on the client side to\nrefine latent features from the head network, enabling user-specific adaptation\nwithout modifying the original models or increasing communication overhead. We\nevaluate SALT on user-specific classification tasks with CIFAR-10 and\nCIFAR-100, demonstrating improved accuracy with lower training latency compared\nto fine-tuning methods. Furthermore, SALT facilitates model adaptation for\nrobust inference over lossy networks, a common challenge in edge-cloud\nenvironments. With minimal deployment overhead, SALT offers a practical\nsolution for personalized inference in edge AI systems under strict system\nconstraints.", "AI": {"tldr": "SALT是一种轻量级模型适配框架，用于在封闭环境中进行分割计算，通过客户端适配器优化特征，无需修改原始模型。", "motivation": "在封闭环境中，传统适配方法因无法访问模型参数或架构而不可行，需要一种无需修改原始模型的轻量级解决方案。", "method": "SALT在客户端引入紧凑、可训练的适配器，优化头部网络的潜在特征，支持用户特定适配。", "result": "在CIFAR-10和CIFAR-100上测试，SALT在降低训练延迟的同时提高了准确性，并在边缘云环境中支持鲁棒推理。", "conclusion": "SALT为严格系统约束下的边缘AI系统提供了一种实用的个性化推理解决方案。"}}
{"id": "2506.07358", "pdf": "https://arxiv.org/pdf/2506.07358", "abs": "https://arxiv.org/abs/2506.07358", "authors": ["Kuiyuan Zhang", "Wenjie Pei", "Rushi Lan", "Yifang Guo", "Zhongyun Hua"], "title": "Lightweight Joint Audio-Visual Deepfake Detection via Single-Stream Multi-Modal Learning Framework", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "comment": null, "summary": "Deepfakes are AI-synthesized multimedia data that may be abused for spreading\nmisinformation. Deepfake generation involves both visual and audio\nmanipulation. To detect audio-visual deepfakes, previous studies commonly\nemploy two relatively independent sub-models to learn audio and visual\nfeatures, respectively, and fuse them subsequently for deepfake detection.\nHowever, this may underutilize the inherent correlations between audio and\nvisual features. Moreover, utilizing two isolated feature learning sub-models\ncan result in redundant neural layers, making the overall model inefficient and\nimpractical for resource-constrained environments.\n  In this work, we design a lightweight network for audio-visual deepfake\ndetection via a single-stream multi-modal learning framework. Specifically, we\nintroduce a collaborative audio-visual learning block to efficiently integrate\nmulti-modal information while learning the visual and audio features. By\niteratively employing this block, our single-stream network achieves a\ncontinuous fusion of multi-modal features across its layers. Thus, our network\nefficiently captures visual and audio features without the need for excessive\nblock stacking, resulting in a lightweight network design. Furthermore, we\npropose a multi-modal classification module that can boost the dependence of\nthe visual and audio classifiers on modality content. It also enhances the\nwhole resistance of the video classifier against the mismatches between audio\nand visual modalities. We conduct experiments on the DF-TIMIT, FakeAVCeleb, and\nDFDC benchmark datasets. Compared to state-of-the-art audio-visual joint\ndetection methods, our method is significantly lightweight with only 0.48M\nparameters, yet it achieves superiority in both uni-modal and multi-modal\ndeepfakes, as well as in unseen types of deepfakes.", "AI": {"tldr": "提出了一种轻量级单流多模态学习框架，用于音频-视觉深度伪造检测，通过协作学习块和多模态分类模块提升性能。", "motivation": "现有方法通常独立学习音频和视觉特征，未能充分利用其相关性，且模型冗余，不适用于资源受限环境。", "method": "设计协作音频-视觉学习块，实现多模态特征的连续融合；提出多模态分类模块，增强分类器对模态内容的依赖。", "result": "在多个基准数据集上表现优异，仅需0.48M参数，优于现有方法。", "conclusion": "该方法轻量高效，适用于多种深度伪造检测场景。"}}
{"id": "2506.07363", "pdf": "https://arxiv.org/pdf/2506.07363", "abs": "https://arxiv.org/abs/2506.07363", "authors": ["Claudiu Popa", "Rex Pallath", "Liam Cunningham", "Hewad Tahiri", "Abiram Kesavarajah", "Tao Wu"], "title": "Deepfake Technology Unveiled: The Commoditization of AI and Its Impact on Digital Trust", "categories": ["cs.CY", "cs.AI", "I.2.m"], "comment": "12 pages, 13 figures", "summary": "Deepfake Technology Unveiled: The Commoditization of AI and Its Impact on\nDigital Trust. With the increasing accessibility of generative AI, tools for\nvoice cloning, face-swapping, and synthetic media creation have advanced\nsignificantly, lowering both financial and technical barriers for their use.\nWhile these technologies present innovative opportunities, their rapid growth\nraises concerns about trust, privacy, and security. This white paper explores\nthe implications of deepfake technology, analyzing its role in enabling fraud,\nmisinformation, and the erosion of authenticity in multimedia. Using\ncost-effective, easy to use tools such as Runway, Rope, and ElevenLabs, we\nexplore how realistic deepfakes can be created with limited resources,\ndemonstrating the risks posed to individuals and organizations alike. By\nanalyzing the technical and ethical challenges of deepfake mitigation and\ndetection, we emphasize the urgent need for regulatory frameworks, public\nawareness, and collaborative efforts to maintain trust in digital media.", "AI": {"tldr": "论文探讨了深度伪造技术的普及及其对数字信任的影响，分析了其带来的风险和挑战，并呼吁加强监管和公众意识。", "motivation": "随着生成式AI的普及，深度伪造技术的门槛降低，但其滥用可能导致信任危机、隐私泄露和安全问题。", "method": "通过低成本工具（如Runway、Rope、ElevenLabs）展示深度伪造的易操作性，并分析其技术和伦理挑战。", "result": "深度伪造技术对个人和组织构成风险，亟需解决检测和缓解的技术与伦理问题。", "conclusion": "需要建立监管框架、提升公众意识，并通过协作维护数字媒体的信任。"}}
{"id": "2506.07373", "pdf": "https://arxiv.org/pdf/2506.07373", "abs": "https://arxiv.org/abs/2506.07373", "authors": ["Enqiang Zhu", "Yu Zhang", "Haopeng Sun", "Ziqi Wei", "Witold Pedrycz", "Chanjuan Liu", "Jin Xu"], "title": "HyColor: An Efficient Heuristic Algorithm for Graph Coloring", "categories": ["cs.DM", "cs.AI"], "comment": "14 pages, 4 figures", "summary": "The graph coloring problem (GCP) is a classic combinatorial optimization\nproblem that aims to find the minimum number of colors assigned to vertices of\na graph such that no two adjacent vertices receive the same color. GCP has been\nextensively studied by researchers from various fields, including mathematics,\ncomputer science, and biological science. Due to the NP-hard nature, many\nheuristic algorithms have been proposed to solve GCP. However, existing GCP\nalgorithms focus on either small hard graphs or large-scale sparse graphs (with\nup to 10^7 vertices). This paper presents an efficient hybrid heuristic\nalgorithm for GCP, named HyColor, which excels in handling large-scale sparse\ngraphs while achieving impressive results on small dense graphs. The efficiency\nof HyColor comes from the following three aspects: a local decision strategy to\nimprove the lower bound on the chromatic number; a graph-reduction strategy to\nreduce the working graph; and a k-core and mixed degree-based greedy heuristic\nfor efficiently coloring graphs. HyColor is evaluated against three\nstate-of-the-art GCP algorithms across four benchmarks, comprising three\nlarge-scale sparse graph benchmarks and one small dense graph benchmark,\ntotaling 209 instances. The results demonstrate that HyColor consistently\noutperforms existing heuristic algorithms in both solution accuracy and\ncomputational efficiency for the majority of instances. Notably, HyColor\nachieved the best solutions in 194 instances (over 93%), with 34 of these\nsolutions significantly surpassing those of other algorithms. Furthermore,\nHyColor successfully determined the chromatic number and achieved optimal\ncoloring in 128 instances.", "AI": {"tldr": "HyColor是一种高效的混合启发式算法，用于解决图着色问题（GCP），特别擅长处理大规模稀疏图和小规模密集图。", "motivation": "现有的GCP算法主要针对小规模硬图或大规模稀疏图，缺乏一种能同时高效处理两者的方法。", "method": "HyColor结合了局部决策策略、图简化策略以及基于k核和混合度的贪心启发式方法。", "result": "在209个实例中，HyColor在194个实例中表现最佳，并在128个实例中确定了色数。", "conclusion": "HyColor在解决GCP问题时，无论是准确性还是计算效率，均优于现有算法。"}}
{"id": "2506.07388", "pdf": "https://arxiv.org/pdf/2506.07388", "abs": "https://arxiv.org/abs/2506.07388", "authors": ["Yun Hua", "Haosheng Chen", "Shiqin Wang", "Wenhao Li", "Xiangfeng Wang", "Jun Luo"], "title": "Shapley-Coop: Credit Assignment for Emergent Cooperation in Self-Interested LLM Agents", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) show strong collaborative performance in\nmulti-agent systems with predefined roles and workflows. However, in open-ended\nenvironments lacking coordination rules, agents tend to act in self-interested\nways. The central challenge in achieving coordination lies in credit assignment\n-- fairly evaluating each agent's contribution and designing pricing mechanisms\nthat align their heterogeneous goals. This problem is critical as LLMs\nincreasingly participate in complex human-AI collaborations, where fair\ncompensation and accountability rely on effective pricing mechanisms. Inspired\nby how human societies address similar coordination challenges (e.g., through\ntemporary collaborations such as employment or subcontracting), we propose a\ncooperative workflow, Shapley-Coop. Shapley-Coop integrates Shapley\nChain-of-Thought -- leveraging marginal contributions as a principled basis for\npricing -- with structured negotiation protocols for effective price matching,\nenabling LLM agents to coordinate through rational task-time pricing and\npost-task reward redistribution. This approach aligns agent incentives, fosters\ncooperation, and maintains autonomy. We evaluate Shapley-Coop across two\nmulti-agent games and a software engineering simulation, demonstrating that it\nconsistently enhances LLM agent collaboration and facilitates equitable credit\nassignment. These results highlight the effectiveness of Shapley-Coop's pricing\nmechanisms in accurately reflecting individual contributions during task\nexecution.", "AI": {"tldr": "论文提出Shapley-Coop，一种基于Shapley值的协作工作流，用于解决开放环境中LLM代理的协调问题，通过定价机制公平分配贡献。", "motivation": "在开放环境中，LLM代理倾向于自利行为，缺乏协调规则。公平评估代理贡献和设计定价机制是关键挑战。", "method": "结合Shapley Chain-of-Thought（基于边际贡献的定价）和结构化谈判协议，实现任务定价和奖励再分配。", "result": "在多个多代理游戏和软件工程模拟中，Shapley-Coop显著提升了协作效果和公平性。", "conclusion": "Shapley-Coop通过有效的定价机制，解决了LLM代理的协调问题，促进了公平合作。"}}
{"id": "2506.07392", "pdf": "https://arxiv.org/pdf/2506.07392", "abs": "https://arxiv.org/abs/2506.07392", "authors": ["Yuyang Zhou", "Guang Cheng", "Kang Du", "Zihan Chen", "Tian Qin", "Yuyu Zhao"], "title": "From Static to Adaptive Defense: Federated Multi-Agent Deep Reinforcement Learning-Driven Moving Target Defense Against DoS Attacks in UAV Swarm Networks", "categories": ["cs.CR", "cs.AI", "cs.LG", "68", "F.2.2"], "comment": "13pages; In submission", "summary": "The proliferation of unmanned aerial vehicle (UAV) swarms has enabled a wide\nrange of mission-critical applications, but also exposes UAV networks to severe\nDenial-of-Service (DoS) threats due to their open wireless environment, dynamic\ntopology, and resource constraints. Traditional static or centralized defense\nmechanisms are often inadequate for such dynamic and distributed scenarios. To\naddress these challenges, we propose a novel federated multi-agent deep\nreinforcement learning (FMADRL)-driven moving target defense (MTD) framework\nfor proactive and adaptive DoS mitigation in UAV swarm networks. Specifically,\nwe design three lightweight and coordinated MTD mechanisms, including leader\nswitching, route mutation, and frequency hopping, that leverage the inherent\nflexibility of UAV swarms to disrupt attacker efforts and enhance network\nresilience. The defense problem is formulated as a multi-agent partially\nobservable Markov decision process (POMDP), capturing the distributed,\nresource-constrained, and uncertain nature of UAV swarms under attack. Each UAV\nis equipped with a local policy agent that autonomously selects MTD actions\nbased on partial observations and local experiences. By employing a policy\ngradient-based FMADRL algorithm, UAVs collaboratively optimize their defense\npolicies via reward-weighted aggregation, enabling distributed learning without\nsharing raw data and thus reducing communication overhead. Extensive\nsimulations demonstrate that our approach significantly outperforms\nstate-of-the-art baselines, achieving up to a 34.6% improvement in attack\nmitigation rate, a reduction in average recovery time of up to 94.6%, and\ndecreases in energy consumption and defense cost by as much as 29.3% and 98.3%,\nrespectively, while maintaining robust mission continuity under various DoS\nattack strategies.", "AI": {"tldr": "论文提出了一种基于联邦多智能体深度强化学习（FMADRL）的移动目标防御（MTD）框架，用于无人机群网络中的DoS攻击主动和自适应缓解。", "motivation": "无人机群网络的开放无线环境、动态拓扑和资源限制使其容易受到DoS攻击，传统静态或集中式防御机制难以应对。", "method": "设计了三种轻量级协调的MTD机制（领导者切换、路由变异和频率跳变），并通过多智能体部分可观测马尔可夫决策过程（POMDP）建模防御问题，采用FMADRL算法实现分布式学习。", "result": "仿真结果表明，该方法显著优于现有基线，攻击缓解率提升34.6%，平均恢复时间减少94.6%，能耗和防御成本分别降低29.3%和98.3%。", "conclusion": "提出的FMADRL-MTD框架有效提升了无人机群网络的抗DoS攻击能力，同时保持了任务连续性。"}}
{"id": "2506.07406", "pdf": "https://arxiv.org/pdf/2506.07406", "abs": "https://arxiv.org/abs/2506.07406", "authors": ["Yifan Luo", "Zhennan Zhou", "Bin Dong"], "title": "InverseScope: Scalable Activation Inversion for Interpreting Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": "18 pages, 8 figures", "summary": "Understanding the internal representations of large language models (LLMs) is\na central challenge in interpretability research. Existing feature\ninterpretability methods often rely on strong assumptions about the structure\nof representations that may not hold in practice. In this work, we introduce\nInverseScope, an assumption-light and scalable framework for interpreting\nneural activations via input inversion. Given a target activation, we define a\ndistribution over inputs that generate similar activations and analyze this\ndistribution to infer the encoded features. To address the inefficiency of\nsampling in high-dimensional spaces, we propose a novel conditional generation\narchitecture that significantly improves sample efficiency compared to previous\nmethods. We further introduce a quantitative evaluation protocol that tests\ninterpretability hypotheses using feature consistency rate computed over the\nsampled inputs. InverseScope scales inversion-based interpretability methods to\nlarger models and practical tasks, enabling systematic and quantitative\nanalysis of internal representations in real-world LLMs.", "AI": {"tldr": "InverseScope是一个轻假设、可扩展的框架，通过输入反演解释神经激活，解决了现有方法在高维空间中采样效率低的问题。", "motivation": "理解大型语言模型（LLMs）的内部表示是解释性研究的核心挑战，现有方法依赖的强假设在实际中可能不成立。", "method": "提出InverseScope框架，通过定义生成相似激活的输入分布并分析该分布来推断编码特征；采用条件生成架构提高采样效率。", "result": "引入定量评估协议，通过特征一致性率测试解释性假设，扩展了反演解释方法在大模型和实际任务中的应用。", "conclusion": "InverseScope实现了对真实LLMs内部表示的系统化和定量分析。"}}
{"id": "2506.07407", "pdf": "https://arxiv.org/pdf/2506.07407", "abs": "https://arxiv.org/abs/2506.07407", "authors": ["Yihong Jin", "Ze Yang", "Juntian Liu", "Xinhe Xu"], "title": "Anomaly Detection and Early Warning Mechanism for Intelligent Monitoring Systems in Multi-Cloud Environments Based on LLM", "categories": ["cs.LG", "cs.AI"], "comment": "Proceedings of 2025 5th International Symposium on Computer\n  Technology and Information Science (ISCTIS 2025)", "summary": "With the rapid development of multi-cloud environments, it is increasingly\nimportant to ensure the security and reliability of intelligent monitoring\nsystems. In this paper, we propose an anomaly detection and early warning\nmechanism for intelligent monitoring system in multi-cloud environment based on\nLarge-Scale Language Model (LLM). On the basis of the existing monitoring\nframework, the proposed model innovatively introduces a multi-level feature\nextraction method, which combines the natural language processing ability of\nLLM with traditional machine learning methods to enhance the accuracy of\nanomaly detection and improve the real-time response efficiency. By introducing\nthe contextual understanding capabilities of LLMs, the model dynamically adapts\nto different cloud service providers and environments, so as to more\neffectively detect abnormal patterns and predict potential failures.\nExperimental results show that the proposed model is significantly better than\nthe traditional anomaly detection system in terms of detection accuracy and\nlatency, and significantly improves the resilience and active management\nability of cloud infrastructure.", "AI": {"tldr": "提出了一种基于大语言模型（LLM）的多云环境智能监控系统异常检测与预警机制，结合多级特征提取和传统机器学习方法，显著提升了检测精度和实时响应效率。", "motivation": "随着多云环境的快速发展，确保智能监控系统的安全性和可靠性变得日益重要。", "method": "在现有监控框架基础上，创新引入多级特征提取方法，结合LLM的自然语言处理能力和传统机器学习方法，动态适应不同云服务提供商和环境。", "result": "实验结果表明，该模型在检测精度和延迟方面显著优于传统异常检测系统，并显著提升了云基础设施的弹性和主动管理能力。", "conclusion": "该研究为多云环境下的智能监控系统提供了一种高效、可靠的异常检测与预警解决方案。"}}
{"id": "2506.07408", "pdf": "https://arxiv.org/pdf/2506.07408", "abs": "https://arxiv.org/abs/2506.07408", "authors": ["Xiaojun zhou", "Chunna Zhao", "Yaqun Huang", "Chengli Zhou", "Junjie Ye", "Kemeng Xiang"], "title": "Fractional-order Jacobian Matrix Differentiation and Its Application in Artificial Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Fractional-order differentiation has many characteristics different from\ninteger-order differentiation. These characteristics can be applied to the\noptimization algorithms of artificial neural networks to obtain better results.\nHowever, due to insufficient theoretical research, at present, there is no\nfractional-order matrix differentiation method that is perfectly compatible\nwith automatic differentiation (Autograd) technology. Therefore, we propose a\nfractional-order matrix differentiation calculation method. This method is\nintroduced by the definition of the integer-order Jacobian matrix. We denote it\nas fractional-order Jacobian matrix differentiation (${{\\bf{J}}^\\alpha }$).\nThrough ${{\\bf{J}}^\\alpha }$, we can carry out the matrix-based\nfractional-order chain rule. Based on the Linear module and the\nfractional-order differentiation, we design the fractional-order Autograd\ntechnology to enable the use of fractional-order differentiation in hidden\nlayers, thereby enhancing the practicality of fractional-order differentiation\nin deep learning. In the experiment, according to the PyTorch framework, we\ndesign fractional-order Linear (FLinear) and replace nn.Linear in the\nmultilayer perceptron with FLinear. Through the qualitative analysis of the\ntraining set and validation set $Loss$, the quantitative analysis of the test\nset indicators, and the analysis of time consumption and GPU memory usage\nduring model training, we verify the superior performance of ${{\\bf{J}}^\\alpha\n}$ and prove that it is an excellent fractional-order gradient descent method\nin the field of deep learning.", "AI": {"tldr": "提出了一种基于分数阶雅可比矩阵的矩阵微分方法（${{\\bf{J}}^\\alpha }$），并设计了分数阶Autograd技术，增强了分数阶微分在深度学习中的实用性。实验验证了其优越性能。", "motivation": "当前缺乏与自动微分技术完美兼容的分数阶矩阵微分方法，限制了分数阶微分在神经网络优化中的应用。", "method": "通过整数阶雅可比矩阵定义引入分数阶雅可比矩阵微分（${{\\bf{J}}^\\alpha }$），并基于Linear模块设计分数阶Autograd技术。", "result": "实验表明，${{\\bf{J}}^\\alpha }$在训练集和验证集上的Loss分析、测试集指标及资源消耗方面表现优越。", "conclusion": "${{\\bf{J}}^\\alpha }$是一种优秀的分数阶梯度下降方法，适用于深度学习领域。"}}
{"id": "2506.07416", "pdf": "https://arxiv.org/pdf/2506.07416", "abs": "https://arxiv.org/abs/2506.07416", "authors": ["Jin Huang", "Yuchao Jin", "Le An", "Josh Park"], "title": "LiteVLM: A Low-Latency Vision-Language Model Inference Pipeline for Resource-Constrained Environments", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "This paper introduces an efficient Vision-Language Model (VLM) pipeline\nspecifically optimized for deployment on embedded devices, such as those used\nin robotics and autonomous driving. The pipeline significantly reduces the\ncomputational overhead by jointly leveraging patch selection to filter\nirrelevant camera views, a token selection module to reduce input sequence\nlength for the LLM, and speculative decoding to accelerate token generation.\nEvaluation on the NVIDIA DRIVE Thor platform for automonous driving\napplication, our pipeline achieves $2.5\\times$ end-to-end latency reduction\nwithout compromising task accuracy. The speed-up further increases to\n$3.2\\times$ when applying FP8 post-training quantization. These results\ndemonstrate our pipeline as a viable solution for enabling real-time VLM\ndeployment in resource-constrained environments.", "AI": {"tldr": "提出了一种针对嵌入式设备优化的高效视觉语言模型（VLM）流水线，显著降低计算开销，实现实时部署。", "motivation": "解决资源受限设备（如机器人和自动驾驶设备）上实时部署VLM的挑战。", "method": "结合补丁选择、令牌选择模块和推测解码，减少计算开销。", "result": "在NVIDIA DRIVE Thor平台上，端到端延迟降低2.5倍，FP8量化后提升至3.2倍。", "conclusion": "该流水线是资源受限环境中实时VLM部署的可行解决方案。"}}
{"id": "2506.07417", "pdf": "https://arxiv.org/pdf/2506.07417", "abs": "https://arxiv.org/abs/2506.07417", "authors": ["Nan Sun", "Xixun Lin", "Zhiheng Zhou", "Yanmin Shang", "Zhenlin Cheng", "Yanan Cao"], "title": "Evidential Spectrum-Aware Contrastive Learning for OOD Detection in Dynamic Graphs", "categories": ["cs.LG", "cs.AI"], "comment": "17 pages,5 figures", "summary": "Recently, Out-of-distribution (OOD) detection in dynamic graphs, which aims\nto identify whether incoming data deviates from the distribution of the\nin-distribution (ID) training set, has garnered considerable attention in\nsecurity-sensitive fields. Current OOD detection paradigms primarily focus on\nstatic graphs and confront two critical challenges: i) high bias and high\nvariance caused by single-point estimation, which makes the predictions\nsensitive to randomness in the data; ii) score homogenization resulting from\nthe lack of OOD training data, where the model only learns ID-specific\npatterns, resulting in overall low OOD scores and a narrow score gap between ID\nand OOD data. To tackle these issues, we first investigate OOD detection in\ndynamic graphs through the lens of Evidential Deep Learning (EDL).\nSpecifically, we propose EviSEC, an innovative and effective OOD detector via\nEvidential Spectrum-awarE Contrastive Learning. We design an evidential neural\nnetwork to redefine the output as the posterior Dirichlet distribution,\nexplaining the randomness of inputs through the uncertainty of distribution,\nwhich is overlooked by single-point estimation. Moreover, spectrum-aware\naugmentation module generates OOD approximations to identify patterns with high\nOOD scores, thereby widening the score gap between ID and OOD data and\nmitigating score homogenization. Extensive experiments on real-world datasets\ndemonstrate that EviSAC effectively detects OOD samples in dynamic graphs.", "AI": {"tldr": "论文提出了一种基于证据深度学习的动态图OOD检测方法EviSEC，解决了单点估计的高偏差、高方差问题以及缺乏OOD训练数据导致的分数同质化问题。", "motivation": "动态图中的OOD检测在安全敏感领域备受关注，但现有方法主要针对静态图，存在单点估计的高偏差、高方差问题以及缺乏OOD训练数据导致的分数同质化问题。", "method": "通过证据深度学习（EDL）重新定义输出为后验Dirichlet分布，设计频谱感知增强模块生成OOD近似样本，以扩大ID与OOD数据的分数差距。", "result": "在真实数据集上的实验表明，EviSEC能有效检测动态图中的OOD样本。", "conclusion": "EviSEC通过结合证据深度学习和频谱感知对比学习，显著提升了动态图中OOD检测的性能。"}}
{"id": "2506.07435", "pdf": "https://arxiv.org/pdf/2506.07435", "abs": "https://arxiv.org/abs/2506.07435", "authors": ["Alexander Kolpakov", "Igor Rivin"], "title": "Fast Geometric Embedding for Node Influence Maximization", "categories": ["cs.SI", "cs.AI", "cs.LG", "E.1; G.2.2; G.4"], "comment": "8 pages, 4 figures, 18 tables; Github repository available\n  (https://github.com/sashakolpakov/graphem/); Package available on PyPi\n  (https://pypi.org/project/graphem-jax/)", "summary": "Computing classical centrality measures such as betweenness and closeness is\ncomputationally expensive on large-scale graphs. In this work, we introduce an\nefficient force layout algorithm that embeds a graph into a low-dimensional\nspace, where the radial distance from the origin serves as a proxy for various\ncentrality measures. We evaluate our method on multiple graph families and\ndemonstrate strong correlations with degree, PageRank, and paths-based\ncentralities. As an application, it turns out that the proposed embedding\nallows to find high-influence nodes in a network, and provides a fast and\nscalable alternative to the standard greedy algorithm.", "AI": {"tldr": "提出了一种高效的力导向布局算法，将图嵌入低维空间，用径向距离作为中心性度量的代理。", "motivation": "在大规模图上计算传统中心性度量（如介数和接近度）计算成本高。", "method": "引入力导向布局算法，将图嵌入低维空间，径向距离作为中心性度量的代理。", "result": "在多类图上验证，与度、PageRank和路径中心性有强相关性。", "conclusion": "该嵌入方法可高效找到高影响力节点，是标准贪婪算法的快速可扩展替代方案。"}}
{"id": "2506.07448", "pdf": "https://arxiv.org/pdf/2506.07448", "abs": "https://arxiv.org/abs/2506.07448", "authors": ["T. Duy Nguyen-Hien", "Desi R. Ivanova", "Yee Whye Teh", "Wee Sun Lee"], "title": "Extending Epistemic Uncertainty Beyond Parameters Would Assist in Designing Reliable LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Although large language models (LLMs) are highly interactive and extendable,\ncurrent approaches to ensure reliability in deployments remain mostly limited\nto rejecting outputs with high uncertainty in order to avoid misinformation.\nThis conservative strategy reflects the current lack of tools to systematically\ndistinguish and respond to different sources of uncertainty. In this paper, we\nadvocate for the adoption of Bayesian Modeling of Experiments -- a framework\nthat provides a coherent foundation to reason about uncertainty and clarify the\nreducibility of uncertainty -- for managing and proactively addressing\nuncertainty that arises in LLM deployments. This framework enables LLMs and\ntheir users to take contextually appropriate steps, such as requesting\nclarification, retrieving external information, or refining inputs. By\nsupporting active resolution rather than passive avoidance, it opens the door\nto more reliable, transparent, and broadly applicable LLM systems, particularly\nin high-stakes, real-world settings.", "AI": {"tldr": "论文提出采用贝叶斯实验建模框架，以系统区分和处理LLM中的不确定性，提升其可靠性和适用性。", "motivation": "当前LLM部署中，可靠性管理主要依赖拒绝高不确定性输出，缺乏系统性工具区分和应对不同不确定性来源。", "method": "采用贝叶斯实验建模框架，为不确定性提供一致的理论基础，支持主动解决而非被动避免。", "result": "该框架使LLM及其用户能采取情境适当的措施（如请求澄清、检索外部信息或优化输入），提升系统可靠性。", "conclusion": "贝叶斯框架为LLM在关键现实场景中的可靠、透明和广泛应用提供了新途径。"}}
{"id": "2506.07454", "pdf": "https://arxiv.org/pdf/2506.07454", "abs": "https://arxiv.org/abs/2506.07454", "authors": ["Jared Strader", "Aaron Ray", "Jacob Arkin", "Mason B. Peterson", "Yun Chang", "Nathan Hughes", "Christopher Bradley", "Yi Xuan Jia", "Carlos Nieto-Granda", "Rajat Talak", "Chuchu Fan", "Luca Carlone", "Jonathan P. How", "Nicholas Roy"], "title": "Language-Grounded Hierarchical Planning and Execution with Multi-Robot 3D Scene Graphs", "categories": ["cs.RO", "cs.AI"], "comment": "12 pages, 4 figures", "summary": "In this paper, we introduce a multi-robot system that integrates mapping,\nlocalization, and task and motion planning (TAMP) enabled by 3D scene graphs to\nexecute complex instructions expressed in natural language. Our system builds a\nshared 3D scene graph incorporating an open-set object-based map, which is\nleveraged for multi-robot 3D scene graph fusion. This representation supports\nreal-time, view-invariant relocalization (via the object-based map) and\nplanning (via the 3D scene graph), allowing a team of robots to reason about\ntheir surroundings and execute complex tasks. Additionally, we introduce a\nplanning approach that translates operator intent into Planning Domain\nDefinition Language (PDDL) goals using a Large Language Model (LLM) by\nleveraging context from the shared 3D scene graph and robot capabilities. We\nprovide an experimental assessment of the performance of our system on\nreal-world tasks in large-scale, outdoor environments.", "AI": {"tldr": "本文介绍了一种多机器人系统，通过3D场景图整合建图、定位和任务与运动规划（TAMP），以执行自然语言表达的复杂指令。", "motivation": "解决多机器人在复杂环境中执行自然语言指令的挑战，通过共享3D场景图实现高效协作。", "method": "构建共享3D场景图，结合开放集对象地图，支持实时重定位和规划；利用大型语言模型（LLM）将操作意图转化为PDDL目标。", "result": "系统在大规模户外环境中进行了实验评估，验证了其有效性。", "conclusion": "该系统通过3D场景图和LLM的结合，实现了多机器人高效协作和复杂任务执行。"}}
{"id": "2506.07477", "pdf": "https://arxiv.org/pdf/2506.07477", "abs": "https://arxiv.org/abs/2506.07477", "authors": ["Thomas Zhu", "Joshua Clune", "Jeremy Avigad", "Albert Qiaochu Jiang", "Sean Welleck"], "title": "Premise Selection for a Lean Hammer", "categories": ["cs.LG", "cs.AI", "cs.LO"], "comment": "LeanHammer is available at https://github.com/JOSHCLUNE/LeanHammer", "summary": "Neural methods are transforming automated reasoning for proof assistants, yet\nintegrating these advances into practical verification workflows remains\nchallenging. Hammers are tools that interface with external automatic theorem\nprovers to automate tedious reasoning steps. They have dramatically improved\nproductivity in proof assistants, but the Lean proof assistant still does not\nhave a hammer despite its growing popularity. We present LeanHammer, the first\nend-to-end domain-general hammer for Lean, built on a novel neural premise\nselection system for a hammer in dependent type theory. Unlike existing Lean\npremise selectors, our approach dynamically adapts to user-specific contexts\nand combines with symbolic proof search and reconstruction to create a\npractical hammer. With comprehensive evaluations, we show that our premise\nselector enables LeanHammer to solve 21\\% more goals relative to existing\npremise selectors, and generalize well to diverse domains. Our work bridges the\ngap between neural retrieval and symbolic reasoning, making formal verification\nmore accessible to researchers and practitioners.", "AI": {"tldr": "LeanHammer是第一个为Lean设计的端到端通用hammer工具，结合神经前提选择和符号推理，显著提升了验证效率。", "motivation": "尽管Lean证明助手日益流行，但仍缺乏hammer工具，阻碍了验证效率。", "method": "提出基于神经前提选择的新方法，动态适应用户上下文，并与符号证明搜索和重建结合。", "result": "LeanHammer比现有前提选择器多解决21%的目标，并在多领域表现优异。", "conclusion": "该工作填补了神经检索与符号推理间的鸿沟，使形式验证更易用。"}}
{"id": "2506.07505", "pdf": "https://arxiv.org/pdf/2506.07505", "abs": "https://arxiv.org/abs/2506.07505", "authors": ["Perry Dong", "Alec M. Lessing", "Annie S. Chen", "Chelsea Finn"], "title": "Reinforcement Learning via Implicit Imitation Guidance", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "We study the problem of sample efficient reinforcement learning, where prior\ndata such as demonstrations are provided for initialization in lieu of a dense\nreward signal. A natural approach is to incorporate an imitation learning\nobjective, either as regularization during training or to acquire a reference\npolicy. However, imitation learning objectives can ultimately degrade long-term\nperformance, as it does not directly align with reward maximization. In this\nwork, we propose to use prior data solely for guiding exploration via noise\nadded to the policy, sidestepping the need for explicit behavior cloning\nconstraints. The key insight in our framework, Data-Guided Noise (DGN), is that\ndemonstrations are most useful for identifying which actions should be\nexplored, rather than forcing the policy to take certain actions. Our approach\nachieves up to 2-3x improvement over prior reinforcement learning from offline\ndata methods across seven simulated continuous control tasks.", "AI": {"tldr": "论文提出了一种名为DGN的方法，利用先验数据通过噪声引导探索，而非直接模仿学习，从而提升强化学习的样本效率。", "motivation": "研究如何利用先验数据（如演示）初始化强化学习，同时避免模仿学习目标对长期性能的负面影响。", "method": "提出Data-Guided Noise (DGN)框架，将先验数据用于引导探索，通过向策略添加噪声而非强制行为克隆。", "result": "在七个模拟连续控制任务中，DGN方法相比现有离线数据强化学习方法实现了2-3倍的性能提升。", "conclusion": "DGN通过噪声引导探索有效利用先验数据，避免了模仿学习的局限性，显著提升了强化学习的样本效率。"}}
{"id": "2506.07520", "pdf": "https://arxiv.org/pdf/2506.07520", "abs": "https://arxiv.org/abs/2506.07520", "authors": ["Shun Lei", "Yaoxun Xu", "Zhiwei Lin", "Huaicheng Zhang", "Wei Tan", "Hangting Chen", "Jianwei Yu", "Yixuan Zhang", "Chenyu Yang", "Haina Zhu", "Shuai Wang", "Zhiyong Wu", "Dong Yu"], "title": "LeVo: High-Quality Song Generation with Multi-Preference Alignment", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Recent advances in large language models (LLMs) and audio language models\nhave significantly improved music generation, particularly in lyrics-to-song\ngeneration. However, existing approaches still struggle with the complex\ncomposition of songs and the scarcity of high-quality data, leading to\nlimitations in sound quality, musicality, instruction following, and\nvocal-instrument harmony. To address these challenges, we introduce LeVo, an\nLM-based framework consisting of LeLM and a music codec. LeLM is capable of\nparallelly modeling two types of tokens: mixed tokens, which represent the\ncombined audio of vocals and accompaniment to achieve vocal-instrument harmony,\nand dual-track tokens, which separately encode vocals and accompaniment for\nhigh-quality song generation. It employs two decoder-only transformers and a\nmodular extension training strategy to prevent interference between different\ntoken types. To further enhance musicality and instruction following, we\nintroduce a multi-preference alignment method based on Direct Preference\nOptimization (DPO). This method handles diverse human preferences through a\nsemi-automatic data construction process and DPO post-training. Experimental\nresults demonstrate that LeVo consistently outperforms existing methods on both\nobjective and subjective metrics. Ablation studies further justify the\neffectiveness of our designs. Audio examples are available at\nhttps://levo-demo.github.io/.", "AI": {"tldr": "LeVo是一个基于语言模型的框架，通过并行建模混合和双轨标记解决歌词到歌曲生成中的复杂性和数据稀缺问题，结合多偏好对齐方法提升音乐性和指令跟随能力。", "motivation": "现有方法在歌词到歌曲生成中存在声音质量、音乐性、指令跟随和声乐-乐器和谐性方面的局限性，需要改进。", "method": "LeVo包含LeLM和音乐编解码器，采用并行建模混合和双轨标记，结合DPO的多偏好对齐方法。", "result": "实验表明LeVo在客观和主观指标上均优于现有方法。", "conclusion": "LeVo通过创新设计和多偏好对齐方法显著提升了歌词到歌曲生成的质量。"}}
{"id": "2506.07524", "pdf": "https://arxiv.org/pdf/2506.07524", "abs": "https://arxiv.org/abs/2506.07524", "authors": ["Shiwei Feng", "Xiangzhe Xu", "Xuan Chen", "Kaiyuan Zhang", "Syed Yusuf Ahmed", "Zian Su", "Mingwei Zheng", "Xiangyu Zhang"], "title": "IntenTest: Stress Testing for Intent Integrity in API-Calling LLM Agents", "categories": ["cs.SE", "cs.AI", "cs.CY"], "comment": null, "summary": "LLM agents are increasingly deployed to automate real-world tasks by invoking\nAPIs through natural language instructions. While powerful, they often suffer\nfrom misinterpretation of user intent, leading to the agent's actions that\ndiverge from the user's intended goal, especially as external toolkits evolve.\nTraditional software testing assumes structured inputs and thus falls short in\nhandling the ambiguity of natural language. We introduce IntenTest, an\nAPI-centric stress testing framework that systematically uncovers intent\nintegrity violations in LLM agents. Unlike prior work focused on fixed\nbenchmarks or adversarial inputs, IntenTest generates realistic tasks based on\ntoolkits' documentation and applies targeted mutations to expose subtle agent\nerrors while preserving user intent. To guide testing, we propose semantic\npartitioning, which organizes natural language tasks into meaningful categories\nbased on toolkit API parameters and their equivalence classes. Within each\npartition, seed tasks are mutated and ranked by a lightweight predictor that\nestimates the likelihood of triggering agent errors. To enhance efficiency,\nIntenTest maintains a datatype-aware strategy memory that retrieves and adapts\neffective mutation patterns from past cases. Experiments on 80 toolkit APIs\ndemonstrate that IntenTest effectively uncovers intent integrity violations,\nsignificantly outperforming baselines in both error-exposing rate and query\nefficiency. Moreover, IntenTest generalizes well to stronger target models\nusing smaller LLMs for test generation, and adapts to evolving APIs across\ndomains.", "AI": {"tldr": "IntenTest是一个API为中心的测试框架，用于系统性地发现LLM代理中的意图完整性违规，通过语义分区和突变生成现实任务，显著优于基线方法。", "motivation": "LLM代理在通过自然语言指令调用API时容易误解用户意图，导致行为偏离目标，而传统软件测试无法处理自然语言的模糊性。", "method": "IntenTest利用工具包文档生成现实任务，通过语义分区和轻量级预测器对任务进行突变和排序，同时采用数据类型感知的策略记忆提高效率。", "result": "在80个工具包API上的实验表明，IntenTest能有效发现意图完整性违规，在错误暴露率和查询效率上显著优于基线。", "conclusion": "IntenTest不仅适用于更强的目标模型，还能适应跨领域的API演化，具有广泛的适用性。"}}
{"id": "2506.07563", "pdf": "https://arxiv.org/pdf/2506.07563", "abs": "https://arxiv.org/abs/2506.07563", "authors": ["Ken Yagel", "Eyal German", "Aviel Ben Siman Tov"], "title": "MoE-MLoRA for Multi-Domain CTR Prediction: Efficient Adaptation with Expert Specialization", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "Personalized recommendation systems must adapt to user interactions across\ndifferent domains. Traditional approaches like MLoRA apply a single adaptation\nper domain but lack flexibility in handling diverse user behaviors. To address\nthis, we propose MoE-MLoRA, a mixture-of-experts framework where each expert is\nfirst trained independently to specialize in its domain before a gating network\nis trained to weight their contributions dynamically. We evaluate MoE-MLoRA\nacross eight CTR models on Movielens and Taobao, showing that it improves\nperformance in large-scale, dynamic datasets (+1.45 Weighed-AUC in Taobao-20)\nbut offers limited benefits in structured datasets with low domain diversity\nand sparsity. Further analysis of the number of experts per domain reveals that\nlarger ensembles do not always improve performance, indicating the need for\nmodel-aware tuning. Our findings highlight the potential of expert-based\narchitectures for multi-domain recommendation systems, demonstrating that\ntask-aware specialization and adaptive gating can enhance predictive accuracy\nin complex environments. The implementation and code are available in our\nGitHub repository.", "AI": {"tldr": "MoE-MLoRA是一种混合专家框架，通过动态加权专家贡献提升多领域推荐系统的性能，在动态数据集上表现优异，但在低多样性数据集上效果有限。", "motivation": "传统方法（如MLoRA）在多领域推荐中缺乏灵活性，无法适应多样用户行为。", "method": "提出MoE-MLoRA框架，先独立训练专家模型，再通过门控网络动态加权其贡献。", "result": "在动态数据集（如Taobao-20）上性能提升显著（+1.45 Weighed-AUC），但在低多样性数据集上效果有限。", "conclusion": "专家架构在多领域推荐系统中潜力巨大，任务感知的专业化和自适应门控能提升复杂环境下的预测准确性。"}}
{"id": "2506.07578", "pdf": "https://arxiv.org/pdf/2506.07578", "abs": "https://arxiv.org/abs/2506.07578", "authors": ["Florian Andreas Marwitz", "Ralf Möller", "Magnus Bender", "Marcel Gehrke"], "title": "Denoising the Future: Top-p Distributions for Moving Through Time", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Inference in dynamic probabilistic models is a complex task involving\nexpensive operations. In particular, for Hidden Markov Models, the whole state\nspace has to be enumerated for advancing in time. Even states with negligible\nprobabilities are considered, resulting in computational inefficiency and\nincreased noise due to the propagation of unlikely probability mass. We propose\nto denoise the future and speed up inference by using only the top-p states,\ni.e., the most probable states with accumulated probability p. We show that the\nerror introduced by using only the top-p states is bound by p and the so-called\nminimal mixing rate of the underlying model. Moreover, in our empirical\nevaluation, we show that we can expect speedups of at least an order of\nmagnitude, while the error in terms of total variation distance is below 0.09.", "AI": {"tldr": "提出了一种通过仅使用最可能的状态（top-p状态）来加速动态概率模型推理的方法，同时误差可控。", "motivation": "动态概率模型推理复杂且计算昂贵，尤其是隐马尔可夫模型中需枚举整个状态空间，导致效率低下和噪声增加。", "method": "仅使用累积概率为p的最可能状态（top-p状态）进行推理，以减少计算量和噪声。", "result": "实验表明，该方法可实现至少一个数量级的加速，且总变差距离误差低于0.09。", "conclusion": "通过限制状态数量，显著提升了推理效率，同时误差在可控范围内。"}}
{"id": "2506.07581", "pdf": "https://arxiv.org/pdf/2506.07581", "abs": "https://arxiv.org/abs/2506.07581", "authors": ["Tan Chen", "Jintao Yan", "Yuxuan Sun", "Sheng Zhou", "Zhisheng Niu"], "title": "FedCGD: Collective Gradient Divergence Optimized Scheduling for Wireless Federated Learning", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": null, "summary": "Federated learning (FL) is a promising paradigm for multiple devices to\ncooperatively train a model. When applied in wireless networks, two issues\nconsistently affect the performance of FL, i.e., data heterogeneity of devices\nand limited bandwidth. Many papers have investigated device scheduling\nstrategies considering the two issues. However, most of them recognize data\nheterogeneity as a property of individual devices. In this paper, we prove that\nthe convergence speed of FL is affected by the sum of device-level and\nsample-level collective gradient divergence (CGD). The device-level CGD refers\nto the gradient divergence of the scheduled device group, instead of the sum of\nthe individual device divergence. The sample-level CGD is statistically upper\nbounded by sampling variance, which is inversely proportional to the total\nnumber of samples scheduled for local update. To derive a tractable form of the\ndevice-level CGD, we further consider a classification problem and transform it\ninto the weighted earth moving distance (WEMD) between the group distribution\nand the global distribution. Then we propose FedCGD algorithm to minimize the\nsum of multi-level CGDs by balancing WEMD and sampling variance, within\npolynomial time. Simulation shows that the proposed strategy increases\nclassification accuracy on the CIFAR-10 dataset by up to 4.2\\% while scheduling\n41.8\\% fewer devices, and flexibly switches between reducing WEMD and reducing\nsampling variance.", "AI": {"tldr": "论文提出FedCGD算法，通过平衡加权地球移动距离（WEMD）和采样方差，最小化多级集体梯度发散（CGD），提升联邦学习在无线网络中的性能。", "motivation": "联邦学习在无线网络中面临设备数据异质性和带宽限制的问题，现有研究多关注设备级异质性，而忽略了样本级异质性和集体梯度发散的影响。", "method": "通过分析设备级和样本级CGD对收敛速度的影响，提出FedCGD算法，将设备级CGD转化为WEMD问题，并在多项式时间内优化调度策略。", "result": "在CIFAR-10数据集上，FedCGD算法将分类准确率提升4.2%，同时减少41.8%的设备调度，并能灵活调整WEMD和采样方差的优化目标。", "conclusion": "FedCGD算法通过多级CGD优化，显著提升了联邦学习的效率和性能，为无线网络中的联邦学习提供了新的解决方案。"}}
{"id": "2506.07587", "pdf": "https://arxiv.org/pdf/2506.07587", "abs": "https://arxiv.org/abs/2506.07587", "authors": ["Tongzhou Yu", "Zhuhao Zhang", "Guanghui Zhu", "Shen Jiang", "Meikang Qiu", "Yihua Huang"], "title": "PrunePEFT: Iterative Hybrid Pruning for Parameter-Efficient Fine-tuning of LLMs", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Parameter Efficient Fine-Tuning (PEFT) methods have emerged as effective and\npromising approaches for fine-tuning pre-trained language models. Compared with\nFull parameter Fine-Tuning (FFT), PEFT achieved comparable task performance\nwith a substantial reduction of trainable parameters, which largely saved the\ntraining and storage costs. However, using the PEFT method requires considering\na vast design space, such as the type of PEFT modules and their insertion\nlayers. Inadequate configurations can lead to sub-optimal results. Conventional\nsolutions such as architectural search techniques, while effective, tend to\nintroduce substantial additional overhead. In this paper, we propose a novel\napproach, PrunePEFT, which formulates the PEFT strategy search as a pruning\nproblem and introduces a hybrid pruning strategy that capitalizes on the\nsensitivity of pruning methods to different PEFT modules. This method extends\ntraditional pruning techniques by iteratively removing redundant or conflicting\nPEFT modules, thereby optimizing the fine-tuned configuration. By efficiently\nidentifying the most relevant modules, our approach significantly reduces the\ncomputational burden typically associated with architectural search processes,\nmaking it a more scalable and efficient solution for fine-tuning large\npre-trained models.", "AI": {"tldr": "PrunePEFT将PEFT策略搜索转化为剪枝问题，通过混合剪枝策略优化模块配置，显著减少计算负担。", "motivation": "PEFT方法虽高效，但设计空间大且配置不当会导致性能不佳，传统搜索方法开销大。", "method": "将PEFT策略搜索视为剪枝问题，采用混合剪枝策略迭代去除冗余模块。", "result": "显著减少计算负担，优化配置，提升可扩展性。", "conclusion": "PrunePEFT为大规模预训练模型提供高效、可扩展的微调解决方案。"}}
{"id": "2506.07744", "pdf": "https://arxiv.org/pdf/2506.07744", "abs": "https://arxiv.org/abs/2506.07744", "authors": ["Seungho Baek", "Taegeon Park", "Jongchan Park", "Seungjun Oh", "Yusung Kim"], "title": "Graph-Assisted Stitching for Offline Hierarchical Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "ICML 2025", "summary": "Existing offline hierarchical reinforcement learning methods rely on\nhigh-level policy learning to generate subgoal sequences. However, their\nefficiency degrades as task horizons increase, and they lack effective\nstrategies for stitching useful state transitions across different\ntrajectories. We propose Graph-Assisted Stitching (GAS), a novel framework that\nformulates subgoal selection as a graph search problem rather than learning an\nexplicit high-level policy. By embedding states into a Temporal Distance\nRepresentation (TDR) space, GAS clusters semantically similar states from\ndifferent trajectories into unified graph nodes, enabling efficient transition\nstitching. A shortest-path algorithm is then applied to select subgoal\nsequences within the graph, while a low-level policy learns to reach the\nsubgoals. To improve graph quality, we introduce the Temporal Efficiency (TE)\nmetric, which filters out noisy or inefficient transition states, significantly\nenhancing task performance. GAS outperforms prior offline HRL methods across\nlocomotion, navigation, and manipulation tasks. Notably, in the most\nstitching-critical task, it achieves a score of 88.3, dramatically surpassing\nthe previous state-of-the-art score of 1.0. Our source code is available at:\nhttps://github.com/qortmdgh4141/GAS.", "AI": {"tldr": "GAS提出了一种基于图搜索的子目标选择框架，替代传统的高层策略学习，通过TDR空间嵌入和TE指标优化图质量，显著提升了离线分层强化学习的性能。", "motivation": "现有离线分层强化学习方法在任务时间跨度增加时效率下降，且缺乏跨轨迹状态转移的有效策略。", "method": "GAS将子目标选择建模为图搜索问题，利用TDR空间嵌入聚类相似状态，并通过最短路径算法选择子目标序列，同时引入TE指标优化图质量。", "result": "GAS在运动、导航和操作任务中表现优异，尤其在缝合关键任务中得分88.3，远超之前的1.0。", "conclusion": "GAS通过图搜索和状态聚类，显著提升了离线分层强化学习的效率和性能。"}}
{"id": "2506.07754", "pdf": "https://arxiv.org/pdf/2506.07754", "abs": "https://arxiv.org/abs/2506.07754", "authors": ["Nicola Lavecchia", "Sid Fadanelli", "Federico Ricciuti", "Gennaro Aloe", "Enrico Bagli", "Pietro Giuffrida", "Daniele Vergari"], "title": "Comparing Credit Risk Estimates in the Gen-AI Era", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Generative AI technologies have demonstrated significant potential across\ndiverse applications. This study provides a comparative analysis of credit\nscore modeling techniques, contrasting traditional approaches with those\nleveraging generative AI. Our findings reveal that current generative AI models\nfall short of matching the performance of traditional methods, regardless of\nthe integration strategy employed. These results highlight the limitations in\nthe current capabilities of generative AI for credit risk scoring, emphasizing\nthe need for further research and development before the possibility of\napplying generative AI for this specific task, or equivalent ones.", "AI": {"tldr": "生成式AI在信用评分建模中表现不如传统方法，需进一步研究。", "motivation": "比较生成式AI与传统方法在信用评分建模中的表现，评估其适用性。", "method": "对比分析生成式AI与传统信用评分建模技术的性能。", "result": "当前生成式AI模型在信用评分中表现不及传统方法。", "conclusion": "生成式AI在信用风险评分中能力有限，需更多研发才能应用。"}}
{"id": "2506.07804", "pdf": "https://arxiv.org/pdf/2506.07804", "abs": "https://arxiv.org/abs/2506.07804", "authors": ["Jie Bao", "Chuangyin Dang", "Rui Luo", "Hanwei Zhang", "Zhixin Zhou"], "title": "Enhancing Adversarial Robustness with Conformal Prediction: A Framework for Guaranteed Model Reliability", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "As deep learning models are increasingly deployed in high-risk applications,\nrobust defenses against adversarial attacks and reliable performance guarantees\nbecome paramount. Moreover, accuracy alone does not provide sufficient\nassurance or reliable uncertainty estimates for these models. This study\nadvances adversarial training by leveraging principles from Conformal\nPrediction. Specifically, we develop an adversarial attack method, termed OPSA\n(OPtimal Size Attack), designed to reduce the efficiency of conformal\nprediction at any significance level by maximizing model uncertainty without\nrequiring coverage guarantees. Correspondingly, we introduce OPSA-AT\n(Adversarial Training), a defense strategy that integrates OPSA within a novel\nconformal training paradigm. Experimental evaluations demonstrate that our OPSA\nattack method induces greater uncertainty compared to baseline approaches for\nvarious defenses. Conversely, our OPSA-AT defensive model significantly\nenhances robustness not only against OPSA but also other adversarial attacks,\nand maintains reliable prediction. Our findings highlight the effectiveness of\nthis integrated approach for developing trustworthy and resilient deep learning\nmodels for safety-critical domains. Our code is available at\nhttps://github.com/bjbbbb/Enhancing-Adversarial-Robustness-with-Conformal-Prediction.", "AI": {"tldr": "该研究提出了一种名为OPSA的对抗攻击方法，旨在通过最大化模型不确定性来降低保形预测的效率，并开发了防御策略OPSA-AT，实验证明其能显著提升模型鲁棒性和预测可靠性。", "motivation": "随着深度学习模型在高风险应用中的部署增加，对抗攻击的防御和性能保证变得至关重要。仅靠准确性无法提供足够的保障或可靠的不确定性估计。", "method": "研究结合保形预测原理，开发了OPSA攻击方法，并提出了OPSA-AT防御策略，通过新的保形训练范式集成OPSA。", "result": "实验表明，OPSA攻击能比基线方法引发更大不确定性，而OPSA-AT防御模型不仅对OPSA有效，还能抵御其他攻击并保持可靠预测。", "conclusion": "该集成方法能有效开发可信赖且鲁棒的深度学习模型，适用于安全关键领域。"}}
{"id": "2506.07822", "pdf": "https://arxiv.org/pdf/2506.07822", "abs": "https://arxiv.org/abs/2506.07822", "authors": ["Xintong Duan", "Yutong He", "Fahim Tajwar", "Ruslan Salakhutdinov", "J. Zico Kolter", "Jeff Schneider"], "title": "Accelerating Diffusion Models in Offline RL via Reward-Aware Consistency Trajectory Distillation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Although diffusion models have achieved strong results in decision-making\ntasks, their slow inference speed remains a key limitation. While the\nconsistency model offers a potential solution, its applications to\ndecision-making often struggle with suboptimal demonstrations or rely on\ncomplex concurrent training of multiple networks. In this work, we propose a\nnovel approach to consistency distillation for offline reinforcement learning\nthat directly incorporates reward optimization into the distillation process.\nOur method enables single-step generation while maintaining higher performance\nand simpler training. Empirical evaluations on the Gym MuJoCo benchmarks and\nlong horizon planning demonstrate that our approach can achieve an 8.7%\nimprovement over previous state-of-the-art while offering up to 142x speedup\nover diffusion counterparts in inference time.", "AI": {"tldr": "提出了一种新的离线强化学习一致性蒸馏方法，将奖励优化直接融入蒸馏过程，实现单步生成并提升性能。", "motivation": "扩散模型在决策任务中表现优异，但推理速度慢；一致性模型虽能解决速度问题，但在决策任务中常因次优演示或多网络并发训练而受限。", "method": "提出了一种将奖励优化直接融入一致性蒸馏过程的新方法，简化训练并实现单步生成。", "result": "在Gym MuJoCo基准测试和长时程规划中，性能提升8.7%，推理速度提升142倍。", "conclusion": "该方法在保持高性能的同时显著提升了推理速度，为决策任务提供了一种更高效的解决方案。"}}
{"id": "2506.07829", "pdf": "https://arxiv.org/pdf/2506.07829", "abs": "https://arxiv.org/abs/2506.07829", "authors": ["Jan Corazza", "Hadi Partovi Aria", "Hyohun Kim", "Daniel Neider", "Zhe Xu"], "title": "Decentralizing Multi-Agent Reinforcement Learning with Temporal Causal Information", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) algorithms can find an optimal policy for a\nsingle agent to accomplish a particular task. However, many real-world problems\nrequire multiple agents to collaborate in order to achieve a common goal. For\nexample, a robot executing a task in a warehouse may require the assistance of\na drone to retrieve items from high shelves. In Decentralized Multi-Agent RL\n(DMARL), agents learn independently and then combine their policies at\nexecution time, but often must satisfy constraints on compatibility of local\npolicies to ensure that they can achieve the global task when combined. In this\npaper, we study how providing high-level symbolic knowledge to agents can help\naddress unique challenges of this setting, such as privacy constraints,\ncommunication limitations, and performance concerns. In particular, we extend\nthe formal tools used to check the compatibility of local policies with the\nteam task, making decentralized training with theoretical guarantees usable in\nmore scenarios. Furthermore, we empirically demonstrate that symbolic knowledge\nabout the temporal evolution of events in the environment can significantly\nexpedite the learning process in DMARL.", "AI": {"tldr": "论文研究了如何通过提供高层符号知识来解决去中心化多智能体强化学习（DMARL）中的隐私、通信和性能问题，并扩展了理论工具以确保策略兼容性。", "motivation": "现实问题中多智能体需协作完成任务，但DMARL中智能体独立学习后需满足策略兼容性约束，同时面临隐私、通信和性能挑战。", "method": "通过提供高层符号知识，扩展了检查局部策略与团队任务兼容性的理论工具，并利用环境事件的时间演化知识加速学习。", "result": "符号知识显著加速了DMARL的学习过程，扩展的理论工具使去中心化训练在更多场景中具备理论保证。", "conclusion": "高层符号知识能有效解决DMARL中的挑战，提升智能体协作效率和学习速度。"}}
{"id": "2506.07836", "pdf": "https://arxiv.org/pdf/2506.07836", "abs": "https://arxiv.org/abs/2506.07836", "authors": ["Silvia Lucia Sanna", "Diego Soi", "Davide Maiorca", "Giorgio Giacinto"], "title": "Are Trees Really Green? A Detection Approach of IoT Malware Attacks", "categories": ["cs.CR", "cs.AI", "cs.NI"], "comment": null, "summary": "Nowadays, the Internet of Things (IoT) is widely employed, and its usage is\ngrowing exponentially because it facilitates remote monitoring, predictive\nmaintenance, and data-driven decision making, especially in the healthcare and\nindustrial sectors. However, IoT devices remain vulnerable due to their\nresource constraints and difficulty in applying security patches. Consequently,\nvarious cybersecurity attacks are reported daily, such as Denial of Service,\nparticularly in IoT-driven solutions. Most attack detection methodologies are\nbased on Machine Learning (ML) techniques, which can detect attack patterns.\nHowever, the focus is more on identification rather than considering the impact\nof ML algorithms on computational resources. This paper proposes a green\nmethodology to identify IoT malware networking attacks based on flow\nprivacy-preserving statistical features. In particular, the hyperparameters of\nthree tree-based models -- Decision Trees, Random Forest and Extra-Trees -- are\noptimized based on energy consumption and test-time performance in terms of\nMatthew's Correlation Coefficient. Our results show that models maintain high\nperformance and detection accuracy while consistently reducing power usage in\nterms of watt-hours (Wh). This suggests that on-premise ML-based Intrusion\nDetection Systems are suitable for IoT and other resource-constrained devices.", "AI": {"tldr": "论文提出了一种基于绿色方法的物联网恶意软件网络攻击检测技术，通过优化树模型的超参数，平衡性能和能耗。", "motivation": "物联网设备因资源限制和难以应用安全补丁而容易受到网络攻击，现有机器学习方法多关注攻击检测而忽略计算资源影响。", "method": "基于隐私保护的统计特征，优化决策树、随机森林和Extra-Trees的超参数，以能耗和性能为指标。", "result": "模型在保持高性能和检测准确率的同时显著降低了能耗（以瓦时计）。", "conclusion": "研究表明，基于机器学习的本地入侵检测系统适合物联网等资源受限设备。"}}
{"id": "2506.07854", "pdf": "https://arxiv.org/pdf/2506.07854", "abs": "https://arxiv.org/abs/2506.07854", "authors": ["Zheng Zhang", "Jie Bao", "Zhixin Zhou", "Nicolo Colombo", "Lixin Cheng", "Rui Luo"], "title": "Residual Reweighted Conformal Prediction for Graph Neural Networks", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Graph Neural Networks (GNNs) excel at modeling relational data but face\nsignificant challenges in high-stakes domains due to unquantified uncertainty.\nConformal prediction (CP) offers statistical coverage guarantees, but existing\nmethods often produce overly conservative prediction intervals that fail to\naccount for graph heteroscedasticity and structural biases. While residual\nreweighting CP variants address some of these limitations, they neglect graph\ntopology, cluster-specific uncertainties, and risk data leakage by reusing\ntraining sets. To address these issues, we propose Residual Reweighted GNN\n(RR-GNN), a framework designed to generate minimal prediction sets with\nprovable marginal coverage guarantees.\n  RR-GNN introduces three major innovations to enhance prediction performance.\nFirst, it employs Graph-Structured Mondrian CP to partition nodes or edges into\ncommunities based on topological features, ensuring cluster-conditional\ncoverage that reflects heterogeneity. Second, it uses Residual-Adaptive\nNonconformity Scores by training a secondary GNN on a held-out calibration set\nto estimate task-specific residuals, dynamically adjusting prediction intervals\naccording to node or edge uncertainty. Third, it adopts a Cross-Training\nProtocol, which alternates the optimization of the primary GNN and the residual\npredictor to prevent information leakage while maintaining graph dependencies.\nWe validate RR-GNN on 15 real-world graphs across diverse tasks, including node\nclassification, regression, and edge weight prediction. Compared to CP\nbaselines, RR-GNN achieves improved efficiency over state-of-the-art methods,\nwith no loss of coverage.", "AI": {"tldr": "RR-GNN是一种改进的图神经网络框架，通过结合图结构的Mondrian CP、残差自适应非一致性评分和交叉训练协议，解决了现有方法在不确定性量化、图异方差性和数据泄漏方面的不足。", "motivation": "现有图神经网络在高风险领域面临不确定性未量化的问题，而传统共形预测方法生成的预测区间过于保守，且未考虑图异方差性和结构偏差。", "method": "RR-GNN采用图结构的Mondrian CP进行节点或边的社区划分，使用残差自适应非一致性评分动态调整预测区间，并通过交叉训练协议防止数据泄漏。", "result": "在15个真实世界图上验证，RR-GNN在节点分类、回归和边权重预测任务中表现优于基线方法，且保持了覆盖率的有效性。", "conclusion": "RR-GNN通过创新的方法提升了预测性能，同时保证了统计覆盖率的可靠性，适用于高风险领域的图数据建模。"}}
{"id": "2506.07861", "pdf": "https://arxiv.org/pdf/2506.07861", "abs": "https://arxiv.org/abs/2506.07861", "authors": ["Firas Laakom", "Haobo Chen", "Jürgen Schmidhuber", "Yuheng Bu"], "title": "Fairness Overfitting in Machine Learning: An Information-Theoretic Perspective", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT"], "comment": "38 pages", "summary": "Despite substantial progress in promoting fairness in high-stake applications\nusing machine learning models, existing methods often modify the training\nprocess, such as through regularizers or other interventions, but lack formal\nguarantees that fairness achieved during training will generalize to unseen\ndata. Although overfitting with respect to prediction performance has been\nextensively studied, overfitting in terms of fairness loss has received far\nless attention. This paper proposes a theoretical framework for analyzing\nfairness generalization error through an information-theoretic lens. Our novel\nbounding technique is based on Efron-Stein inequality, which allows us to\nderive tight information-theoretic fairness generalization bounds with both\nMutual Information (MI) and Conditional Mutual Information (CMI). Our empirical\nresults validate the tightness and practical relevance of these bounds across\ndiverse fairness-aware learning algorithms. Our framework offers valuable\ninsights to guide the design of algorithms improving fairness generalization.", "AI": {"tldr": "本文提出了一种基于信息论的理论框架，用于分析公平性泛化误差，并通过Efron-Stein不等式推导出紧密的公平性泛化边界。", "motivation": "现有方法缺乏对公平性在训练数据与未见数据之间泛化的形式化保证，本文旨在填补这一空白。", "method": "采用信息论视角，基于Efron-Stein不等式，推导出基于互信息（MI）和条件互信息（CMI）的公平性泛化边界。", "result": "实证结果验证了这些边界在多种公平性学习算法中的紧密性和实用性。", "conclusion": "该框架为设计提升公平性泛化的算法提供了有价值的指导。"}}
{"id": "2506.07864", "pdf": "https://arxiv.org/pdf/2506.07864", "abs": "https://arxiv.org/abs/2506.07864", "authors": ["Mirko Paolo Barbato", "Giorgia Rigamonti", "Davide Marelli", "Paolo Napoletano"], "title": "Lightweight Sequential Transformers for Blood Glucose Level Prediction in Type-1 Diabetes", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Type 1 Diabetes (T1D) affects millions worldwide, requiring continuous\nmonitoring to prevent severe hypo- and hyperglycemic events. While continuous\nglucose monitoring has improved blood glucose management, deploying predictive\nmodels on wearable devices remains challenging due to computational and memory\nconstraints. To address this, we propose a novel Lightweight Sequential\nTransformer model designed for blood glucose prediction in T1D. By integrating\nthe strengths of Transformers' attention mechanisms and the sequential\nprocessing of recurrent neural networks, our architecture captures long-term\ndependencies while maintaining computational efficiency. The model is optimized\nfor deployment on resource-constrained edge devices and incorporates a balanced\nloss function to handle the inherent data imbalance in hypo- and hyperglycemic\nevents. Experiments on two benchmark datasets, OhioT1DM and DiaTrend,\ndemonstrate that the proposed model outperforms state-of-the-art methods in\npredicting glucose levels and detecting adverse events. This work fills the gap\nbetween high-performance modeling and practical deployment, providing a\nreliable and efficient T1D management solution.", "AI": {"tldr": "提出了一种轻量级序列Transformer模型，用于T1D患者的血糖预测，解决了计算和内存限制问题，并在实验中优于现有方法。", "motivation": "T1D需要持续监测血糖，但现有预测模型在可穿戴设备上部署困难，需解决计算和内存限制。", "method": "结合Transformer的注意力机制和循环神经网络的序列处理能力，设计轻量级模型，优化部署并处理数据不平衡。", "result": "在OhioT1DM和DiaTrend数据集上表现优于现有方法，能高效预测血糖水平和检测不良事件。", "conclusion": "填补了高性能模型与实际部署之间的空白，为T1D管理提供了可靠高效的解决方案。"}}
{"id": "2506.07935", "pdf": "https://arxiv.org/pdf/2506.07935", "abs": "https://arxiv.org/abs/2506.07935", "authors": ["Pavel Naumov", "Jia Tao"], "title": "Diffusion of Responsibility in Collective Decision Making", "categories": ["cs.MA", "cs.AI", "cs.GT"], "comment": null, "summary": "The term \"diffusion of responsibility'' refers to situations in which\nmultiple agents share responsibility for an outcome, obscuring individual\naccountability. This paper examines this frequently undesirable phenomenon in\nthe context of collective decision-making mechanisms.\n  The work shows that if a decision is made by two agents, then the only way to\navoid diffusion of responsibility is for one agent to act as a \"dictator'',\nmaking the decision unilaterally. In scenarios with more than two agents, any\ndiffusion-free mechanism is an \"elected dictatorship'' where the agents elect a\nsingle agent to make a unilateral decision.\n  The technical results are obtained by defining a bisimulation of\ndecision-making mechanisms, proving that bisimulation preserves\nresponsibility-related properties, and establishing the results for a smallest\nbisimular mechanism.", "AI": {"tldr": "论文研究了集体决策机制中的“责任扩散”现象，证明在多个代理参与决策时，唯一避免责任扩散的方法是采用“独裁”或“选举独裁”机制。", "motivation": "探讨集体决策中责任扩散的问题，旨在找到避免责任扩散的机制设计。", "method": "通过定义决策机制的互模拟，证明互模拟保持责任相关属性，并在最小互模拟机制中验证结果。", "result": "在双代理决策中，唯一避免责任扩散的方法是单边决策；多代理情况下，需选举一个代理进行单边决策。", "conclusion": "避免责任扩散的集体决策机制必须采用独裁或选举独裁的形式。"}}
{"id": "2506.07961", "pdf": "https://arxiv.org/pdf/2506.07961", "abs": "https://arxiv.org/abs/2506.07961", "authors": ["Peiyan Li", "Yixiang Chen", "Hongtao Wu", "Xiao Ma", "Xiangnan Wu", "Yan Huang", "Liang Wang", "Tao Kong", "Tieniu Tan"], "title": "BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning with Vision-Language Models", "categories": ["cs.RO", "cs.AI"], "comment": "In Submission", "summary": "Recently, leveraging pre-trained vision-language models (VLMs) for building\nvision-language-action (VLA) models has emerged as a promising approach to\neffective robot manipulation learning. However, only few methods incorporate 3D\nsignals into VLMs for action prediction, and they do not fully leverage the\nspatial structure inherent in 3D data, leading to low sample efficiency. In\nthis paper, we introduce BridgeVLA, a novel 3D VLA model that (1) projects 3D\ninputs to multiple 2D images, ensuring input alignment with the VLM backbone,\nand (2) utilizes 2D heatmaps for action prediction, unifying the input and\noutput spaces within a consistent 2D image space. In addition, we propose a\nscalable pre-training method that equips the VLM backbone with the capability\nto predict 2D heatmaps before downstream policy learning. Extensive experiments\nshow the proposed method is able to learn 3D manipulation efficiently and\neffectively. BridgeVLA outperforms state-of-the-art baseline methods across\nthree simulation benchmarks. In RLBench, it improves the average success rate\nfrom 81.4% to 88.2%. In COLOSSEUM, it demonstrates significantly better\nperformance in challenging generalization settings, boosting the average\nsuccess rate from 56.7% to 64.0%. In GemBench, it surpasses all the comparing\nbaseline methods in terms of average success rate. In real-robot experiments,\nBridgeVLA outperforms a state-of-the-art baseline method by 32% on average. It\ngeneralizes robustly in multiple out-of-distribution settings, including visual\ndisturbances and unseen instructions. Remarkably, it is able to achieve a\nsuccess rate of 96.8% on 10+ tasks with only 3 trajectories per task,\nhighlighting its extraordinary sample efficiency. Project\nWebsite:https://bridgevla.github.io/", "AI": {"tldr": "BridgeVLA是一种新型的3D视觉-语言-动作模型，通过将3D输入投影到2D图像并利用2D热图预测动作，显著提高了样本效率和性能。", "motivation": "现有方法未能充分利用3D数据的空间结构，导致样本效率低下。", "method": "BridgeVLA将3D输入投影为2D图像，并使用2D热图预测动作，同时提出了一种可扩展的预训练方法。", "result": "在多个仿真和真实机器人实验中，BridgeVLA表现优异，平均成功率显著提升，并在样本效率上表现突出。", "conclusion": "BridgeVLA在3D操作任务中高效且有效，具有强大的泛化能力和样本效率。"}}
{"id": "2506.07976", "pdf": "https://arxiv.org/pdf/2506.07976", "abs": "https://arxiv.org/abs/2506.07976", "authors": ["Junhong Shen", "Hao Bai", "Lunjun Zhang", "Yifei Zhou", "Amrith Setlur", "Shengbang Tong", "Diego Caples", "Nan Jiang", "Tong Zhang", "Ameet Talwalkar", "Aviral Kumar"], "title": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The current paradigm of test-time scaling relies on generating long reasoning\ntraces (\"thinking\" more) before producing a response. In agent problems that\nrequire interaction, this can be done by generating thinking traces before\nacting in the world. However, this process does not allow agents to acquire new\ninformation from the environment or adapt their behavior over time. In this\nwork, we propose to scale test-time interaction, an untapped dimension of\ntest-time scaling that increases the agent's interaction horizon to enable\nrunning rich behaviors such as exploration, backtracking, and dynamic\nre-planning within a single rollout. To demonstrate the promise of this scaling\ndimension, we study the domain of web agents. We first show that even\nprompting-based interaction scaling without any training can improve task\nsuccess on web benchmarks non-trivially. Building on this, we introduce TTI\n(Test-Time Interaction), a curriculum-based online reinforcement learning (RL)\napproach that trains agents by adaptively adjusting their rollout lengths.\nUsing a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data\nweb agents on WebVoyager and WebArena benchmarks. We further show that TTI\nenables agents to balance exploration and exploitation adaptively. Our results\nestablish interaction scaling as a powerful, complementary axis to scaling\nper-step compute, offering new avenues for training adaptive agents.", "AI": {"tldr": "论文提出了一种新的测试时间交互扩展方法（TTI），通过增加代理的交互范围，使其能够在单次运行中实现探索、回溯和动态重新规划等行为。该方法在Web代理领域取得了显著效果。", "motivation": "当前测试时间扩展范式依赖生成长推理轨迹，但无法让代理从环境中获取新信息或随时间调整行为。因此，研究探索了交互扩展这一未被充分利用的维度。", "method": "提出了TTI方法，基于课程学习的在线强化学习（RL），通过自适应调整代理的滚动长度来训练代理。实验使用了Gemma 3 12B模型。", "result": "TTI在WebVoyager和WebArena基准测试中实现了最先进的开放源代码和开放数据Web代理性能，并能自适应平衡探索与利用。", "conclusion": "交互扩展是计算扩展的强大补充维度，为训练自适应代理提供了新途径。"}}
