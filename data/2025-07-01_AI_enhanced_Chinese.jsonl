{"id": "2506.22604", "pdf": "https://arxiv.org/pdf/2506.22604", "abs": "https://arxiv.org/abs/2506.22604", "authors": ["David Porfirio", "Vincent Hsiao", "Morgan Fine-Morris", "Leslie Smith", "Laura M. Hiatt"], "title": "Bootstrapping Human-Like Planning via LLMs", "categories": ["cs.AI", "cs.HC", "cs.RO"], "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (RO-MAN)", "summary": "Robot end users increasingly require accessible means of specifying tasks for\nrobots to perform. Two common end-user programming paradigms include\ndrag-and-drop interfaces and natural language programming. Although natural\nlanguage interfaces harness an intuitive form of human communication,\ndrag-and-drop interfaces enable users to meticulously and precisely dictate the\nkey actions of the robot's task. In this paper, we investigate the degree to\nwhich both approaches can be combined. Specifically, we construct a large\nlanguage model (LLM)-based pipeline that accepts natural language as input and\nproduces human-like action sequences as output, specified at a level of\ngranularity that a human would produce. We then compare these generated action\nsequences to another dataset of hand-specified action sequences. Although our\nresults reveal that larger models tend to outperform smaller ones in the\nproduction of human-like action sequences, smaller models nonetheless achieve\nsatisfactory performance.", "AI": {"tldr": "研究结合自然语言和拖拽界面两种编程范式，利用大语言模型（LLM）生成类人动作序列，并与手工指定序列对比，发现大模型表现更优，但小模型也能满足需求。", "motivation": "探索如何结合自然语言和拖拽界面两种编程范式，以提升机器人任务指定的易用性和精确性。", "method": "构建基于LLM的流程，输入自然语言并输出类人动作序列，与手工指定序列进行对比。", "result": "大模型生成的动作序列更接近人类，但小模型表现也令人满意。", "conclusion": "结合自然语言和拖拽界面可行，大模型表现更优，但小模型也能满足需求。"}}
{"id": "2506.22609", "pdf": "https://arxiv.org/pdf/2506.22609", "abs": "https://arxiv.org/abs/2506.22609", "authors": ["Graham Todd", "Alexander G. Padula", "Dennis J. N. J. Soemers", "Julian Togelius"], "title": "Ludax: A GPU-Accelerated Domain Specific Language for Board Games", "categories": ["cs.AI"], "comment": "18 pages, 3 figures", "summary": "Games have long been used as benchmarks and testing environments for research\nin artificial intelligence. A key step in supporting this research was the\ndevelopment of game description languages: frameworks that compile\ndomain-specific code into playable and simulatable game environments, allowing\nresearchers to generalize their algorithms and approaches across multiple games\nwithout having to manually implement each one. More recently, progress in\nreinforcement learning (RL) has been largely driven by advances in hardware\nacceleration. Libraries like JAX allow practitioners to take full advantage of\ncutting-edge computing hardware, often speeding up training and testing by\norders of magnitude. Here, we present a synthesis of these strands of research:\na domain-specific language for board games which automatically compiles into\nhardware-accelerated code. Our framework, Ludax, combines the generality of\ngame description languages with the speed of modern parallel processing\nhardware and is designed to fit neatly into existing deep learning pipelines.\nWe envision Ludax as a tool to help accelerate games research generally, from\nRL to cognitive science, by enabling rapid simulation and providing a flexible\nrepresentation scheme. We present a detailed breakdown of Ludax's description\nlanguage and technical notes on the compilation process, along with speed\nbenchmarking and a demonstration of training RL agents. The Ludax framework,\nalong with implementations of existing board games, is open-source and freely\navailable.", "AI": {"tldr": "Ludax是一个结合游戏描述语言和硬件加速的框架，旨在加速游戏研究，支持快速模拟和灵活表示。", "motivation": "游戏在人工智能研究中作为基准和测试环境，但现有方法缺乏通用性和速度。Ludax旨在结合游戏描述语言的通用性和现代硬件加速技术。", "method": "开发Ludax框架，包括领域特定语言和编译过程，支持硬件加速，并集成到深度学习流程中。", "result": "Ludax实现了快速模拟和灵活表示，通过基准测试和RL代理训练验证了其性能。", "conclusion": "Ludax是一个开源工具，有望加速从强化学习到认知科学的游戏研究。"}}
{"id": "2506.22653", "pdf": "https://arxiv.org/pdf/2506.22653", "abs": "https://arxiv.org/abs/2506.22653", "authors": ["Michael Grosskopf", "Russell Bent", "Rahul Somasundaram", "Isaac Michaud", "Arthur Lui", "Nathan Debardeleben", "Earl Lawrence"], "title": "URSA: The Universal Research and Scientific Agent", "categories": ["cs.AI"], "comment": "31 pages, 9 figures", "summary": "Large language models (LLMs) have moved far beyond their initial form as\nsimple chatbots, now carrying out complex reasoning, planning, writing, coding,\nand research tasks. These skills overlap significantly with those that human\nscientists use day-to-day to solve complex problems that drive the cutting edge\nof research. Using LLMs in \"agentic\" AI has the potential to revolutionize\nmodern science and remove bottlenecks to progress. In this work, we present\nURSA, a scientific agent ecosystem for accelerating research tasks. URSA\nconsists of a set of modular agents and tools, including coupling to advanced\nphysics simulation codes, that can be combined to address scientific problems\nof varied complexity and impact. This work highlights the architecture of URSA,\nas well as examples that highlight the potential of the system.", "AI": {"tldr": "URSA是一个科学代理生态系统，旨在通过模块化代理和工具加速科研任务，结合高级物理模拟代码，解决复杂科学问题。", "motivation": "大型语言模型（LLMs）已具备复杂推理、规划、写作和编码能力，与科学家日常解决问题的能力高度重合。利用LLMs的“代理性”AI有望革新现代科学并消除研究瓶颈。", "method": "提出URSA系统，包含模块化代理和工具，可结合高级物理模拟代码，灵活应对不同复杂度和影响力的科学问题。", "result": "展示了URSA的架构及其在解决科学问题中的潜力。", "conclusion": "URSA为加速科研任务提供了创新解决方案，展示了LLMs在科学领域的广泛应用前景。"}}
{"id": "2506.22740", "pdf": "https://arxiv.org/pdf/2506.22740", "abs": "https://arxiv.org/abs/2506.22740", "authors": ["Jessica Hullman", "Ziyang Guo", "Berk Ustun"], "title": "Explanations are a means to an end", "categories": ["cs.AI", "stat.ML"], "comment": null, "summary": "Modern methods for explainable machine learning are designed to describe how\nmodels map inputs to outputs--without deep consideration of how these\nexplanations will be used in practice. This paper argues that explanations\nshould be designed and evaluated with a specific end in mind. We describe how\nto formalize this end in a framework based in statistical decision theory. We\nshow how this functionally-grounded approach can be applied across diverse use\ncases, such as clinical decision support, providing recourse, or debugging. We\ndemonstrate its use to characterize the maximum \"boost\" in performance on a\nparticular task that an explanation could provide an idealized decision-maker,\npreventing misuse due to ambiguity by forcing researchers to specify concrete\nuse cases that can be analyzed in light of models of expected explanation use.\nWe argue that evaluation should meld theoretical and empirical perspectives on\nthe value of explanation, and contribute definitions that span these\nperspectives.", "AI": {"tldr": "论文提出了一种基于统计决策理论的框架，强调解释性机器学习方法应根据具体用途设计和评估，并展示了其在临床决策支持等场景中的应用。", "motivation": "现有解释性机器学习方法未充分考虑实际用途，导致解释可能被误用或效果不佳。", "method": "提出基于统计决策理论的框架，要求明确具体用途，并通过理论和实证结合的方式评估解释的价值。", "result": "展示了该框架在临床决策支持等场景中的应用，并量化了理想决策者能从解释中获得的性能提升。", "conclusion": "解释性方法应针对具体用途设计，评估需结合理论和实证，以避免误用并提升实际效果。"}}
{"id": "2506.22439", "pdf": "https://arxiv.org/pdf/2506.22439", "abs": "https://arxiv.org/abs/2506.22439", "authors": ["Javier Conde", "Miguel González", "María Grandury", "Gonzalo Martínez", "Pedro Reviriego", "Mar Brysbaert"], "title": "Psycholinguistic Word Features: a New Approach for the Evaluation of LLMs Alignment with Humans", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted for the GEM2 workshop at ACL 2025", "summary": "The evaluation of LLMs has so far focused primarily on how well they can\nperform different tasks such as reasoning, question-answering, paraphrasing, or\ntranslating. For most of these tasks, performance can be measured with\nobjective metrics, such as the number of correct answers. However, other\nlanguage features are not easily quantified. For example, arousal,\nconcreteness, or gender associated with a given word, as well as the extent to\nwhich we experience words with senses and relate them to a specific sense.\nThose features have been studied for many years by psycholinguistics,\nconducting large-scale experiments with humans to produce ratings for thousands\nof words. This opens an opportunity to evaluate how well LLMs align with human\nratings on these word features, taking advantage of existing studies that cover\nmany different language features in a large number of words. In this paper, we\nevaluate the alignment of a representative group of LLMs with human ratings on\ntwo psycholinguistic datasets: the Glasgow and Lancaster norms. These datasets\ncover thirteen features over thousands of words. The results show that\nalignment is \\textcolor{black}{generally} better in the Glasgow norms evaluated\n(arousal, valence, dominance, concreteness, imageability, familiarity, and\ngender) than on the Lancaster norms evaluated (introceptive, gustatory,\nolfactory, haptic, auditory, and visual). This suggests a potential limitation\nof current LLMs in aligning with human sensory associations for words, which\nmay be due to their lack of embodied cognition present in humans and\nillustrates the usefulness of evaluating LLMs with psycholinguistic datasets.", "AI": {"tldr": "论文评估了大型语言模型（LLMs）与人类在心理语言学数据集（Glasgow和Lancaster规范）中对词汇特征的评分一致性，发现LLMs在Glasgow规范中表现较好，但在Lancaster规范中表现较弱。", "motivation": "现有LLM评估多关注任务性能，而忽略了难以量化的语言特征（如情感、感官关联等）。本文利用心理语言学数据集填补这一空白。", "method": "通过比较LLMs与人类在Glasgow和Lancaster规范中对13个词汇特征的评分，评估其一致性。", "result": "LLMs在Glasgow规范（如情感、熟悉度等）中表现较好，但在Lancaster规范（如感官特征）中表现较差。", "conclusion": "LLMs在感官关联方面存在局限性，可能与缺乏人类的具体认知有关，心理语言学数据集为LLM评估提供了新视角。"}}
{"id": "2506.22437", "pdf": "https://arxiv.org/pdf/2506.22437", "abs": "https://arxiv.org/abs/2506.22437", "authors": ["Xinxin Sun", "Peter Chang"], "title": "Robust Perspective Correction for Real-World Crack Evolution Tracking in Image-Based Structural Health Monitoring", "categories": ["cs.CV", "68T45 (Computer Vision)"], "comment": "43 pages, 5 figures, 19 tables. Submitted to NDT&E International.\n  This work may also be of interest to researchers in optical NDE and civil\n  engineering SHM", "summary": "Accurate image alignment is essential for monitoring crack evolution in\nstructural health monitoring (SHM), particularly under real-world conditions\ninvolving perspective distortion, occlusion, and low contrast. However,\ntraditional feature detectors such as SIFT and SURF, which rely on\nGaussian-based scale spaces, tend to suppress high-frequency edges, making them\nunsuitable for thin crack localization. Lightweight binary alternatives like\nORB and BRISK, while computationally efficient, often suffer from poor keypoint\nrepeatability on textured or shadowed surfaces. This study presents a\nphysics-informed alignment framework that adapts the open KAZE architecture to\nSHM-specific challenges. By utilizing nonlinear anisotropic diffusion to\nconstruct a crack-preserving scale space, and integrating RANSAC-based\nhomography estimation, the framework enables accurate geometric correction\nwithout the need for training, parameter tuning, or prior calibration. The\nmethod is validated on time-lapse images of masonry and concrete acquired via\nhandheld smartphone under varied field conditions, including shadow\ninterference, cropping, oblique viewing angles, and surface clutter. Compared\nto classical detectors, the proposed framework reduces crack area and spine\nlength errors by up to 70 percent and 90 percent, respectively, while\nmaintaining sub-5 percent alignment error in key metrics. Unsupervised,\ninterpretable, and computationally lightweight, this approach supports scalable\ndeployment via UAVs and mobile platforms. By tailoring nonlinear scale-space\nmodeling to SHM image alignment, this work offers a robust and physically\ngrounded alternative to conventional techniques for tracking real-world crack\nevolution.", "AI": {"tldr": "提出了一种基于物理信息的图像对齐框架，用于结构健康监测中的裂纹定位，显著提升了精度和鲁棒性。", "motivation": "传统特征检测方法（如SIFT、SURF）在高频边缘抑制和复杂环境下表现不佳，需要一种适应性强且无需训练的方法。", "method": "采用非线性各向异性扩散构建裂纹保留尺度空间，结合RANSAC单应性估计，实现无需训练或参数调优的几何校正。", "result": "在多种复杂条件下，裂纹面积和长度误差分别减少70%和90%，对齐误差低于5%。", "conclusion": "该方法为实际裂纹监测提供了一种高效、可扩展且物理基础明确的解决方案。"}}
{"id": "2506.22774", "pdf": "https://arxiv.org/pdf/2506.22774", "abs": "https://arxiv.org/abs/2506.22774", "authors": ["Michael Papademas", "Xenia Ziouvelou", "Antonis Troumpoukis", "Vangelis Karkaletsis"], "title": "Bridging Ethical Principles and Algorithmic Methods: An Alternative Approach for Assessing Trustworthiness in AI Systems", "categories": ["cs.AI", "cs.CY"], "comment": null, "summary": "Artificial Intelligence (AI) technology epitomizes the complex challenges\nposed by human-made artifacts, particularly those widely integrated into\nsociety and exert significant influence, highlighting potential benefits and\ntheir negative consequences. While other technologies may also pose substantial\nrisks, AI's pervasive reach makes its societal effects especially profound. The\ncomplexity of AI systems, coupled with their remarkable capabilities, can lead\nto a reliance on technologies that operate beyond direct human oversight or\nunderstanding. To mitigate the risks that arise, several theoretical tools and\nguidelines have been developed, alongside efforts to create technological tools\naimed at safeguarding Trustworthy AI. The guidelines take a more holistic view\nof the issue but fail to provide techniques for quantifying trustworthiness.\nConversely, while technological tools are better at achieving such\nquantification, they lack a holistic perspective, focusing instead on specific\naspects of Trustworthy AI. This paper aims to introduce an assessment method\nthat combines the ethical components of Trustworthy AI with the algorithmic\nprocesses of PageRank and TrustRank. The goal is to establish an assessment\nframework that minimizes the subjectivity inherent in the self-assessment\ntechniques prevalent in the field by introducing algorithmic criteria. The\napplication of our approach indicates that a holistic assessment of an AI\nsystem's trustworthiness can be achieved by providing quantitative insights\nwhile considering the theoretical content of relevant guidelines.", "AI": {"tldr": "论文提出了一种结合伦理与技术的方法，评估AI系统的可信度，旨在减少主观性并提供量化指标。", "motivation": "AI技术的复杂性和广泛影响使其可信度评估成为关键问题，现有方法或缺乏量化或缺乏全面视角。", "method": "结合Trustworthy AI的伦理组件与PageRank和TrustRank算法，建立评估框架。", "result": "该方法能通过量化指标和理论指导，全面评估AI系统的可信度。", "conclusion": "提出的方法为AI可信度评估提供了更客观和全面的解决方案。"}}
{"id": "2506.22485", "pdf": "https://arxiv.org/pdf/2506.22485", "abs": "https://arxiv.org/abs/2506.22485", "authors": ["Sudip Dasgupta", "Himanshu Shankar"], "title": "AI Agents-as-Judge: Automated Assessment of Accuracy, Consistency, Completeness and Clarity for Enterprise Documents", "categories": ["cs.CL", "cs.AI", "68T07, 68T50", "I.2.1; I.2.3; I.2.7; H.3.3"], "comment": "17 pages, 2 system diagrams, 1 table, no prior conference publication", "summary": "This study presents a modular, multi-agent system for the automated review of\nhighly structured enterprise business documents using AI agents. Unlike prior\nsolutions focused on unstructured texts or limited compliance checks, this\nframework leverages modern orchestration tools such as LangChain, CrewAI,\nTruLens, and Guidance to enable section-by-section evaluation of documents for\naccuracy, consistency, completeness, and clarity. Specialized agents, each\nresponsible for discrete review criteria such as template compliance or factual\ncorrectness, operate in parallel or sequence as required. Evaluation outputs\nare enforced to a standardized, machine-readable schema, supporting downstream\nanalytics and auditability. Continuous monitoring and a feedback loop with\nhuman reviewers allow for iterative system improvement and bias mitigation.\n  Quantitative evaluation demonstrates that the AI Agent-as-Judge system\napproaches or exceeds human performance in key areas: achieving 99% information\nconsistency (vs. 92% for humans), halving error and bias rates, and reducing\naverage review time from 30 to 2.5 minutes per document, with a 95% agreement\nrate between AI and expert human judgment. While promising for a wide range of\nindustries, the study also discusses current limitations, including the need\nfor human oversight in highly specialized domains and the operational cost of\nlarge-scale LLM usage. The proposed system serves as a flexible, auditable, and\nscalable foundation for AI-driven document quality assurance in the enterprise\ncontext.", "AI": {"tldr": "该研究提出了一种模块化多智能体系统，用于自动化审查高度结构化的企业业务文档，利用AI智能体实现逐部分评估，性能接近或超越人类。", "motivation": "现有解决方案主要针对非结构化文本或有限合规性检查，缺乏对结构化文档的全面评估能力。", "method": "采用LangChain、CrewAI等现代编排工具，设计专门智能体并行或顺序工作，评估文档准确性、一致性等，并标准化输出。", "result": "AI系统在一致性（99% vs 92%）、错误率减半、审查时间从30分钟降至2.5分钟等方面优于人类，与专家判断一致率达95%。", "conclusion": "该系统为企业文档质量保证提供了灵活、可扩展的基础，但仍需人类监督和应对大规模LLM使用的成本问题。"}}
{"id": "2506.22438", "pdf": "https://arxiv.org/pdf/2506.22438", "abs": "https://arxiv.org/abs/2506.22438", "authors": ["Xumin Gao", "Mark Stevens", "Grzegorz Cielniak"], "title": "Counting with Confidence: Accurate Pest Monitoring in Water Traps", "categories": ["cs.CV"], "comment": "\\c{opyright} 20XX the authors. This work has been accepted to IFAC\n  for publication under a Creative Commons Licence CC-BY-NC-ND", "summary": "Accurate pest population monitoring and tracking their dynamic changes are\ncrucial for precision agriculture decision-making. A common limitation in\nexisting vision-based automatic pest counting research is that models are\ntypically evaluated on datasets with ground truth but deployed in real-world\nscenarios without assessing the reliability of counting results due to the lack\nof ground truth. To this end, this paper proposed a method for comprehensively\nevaluating pest counting confidence in the image, based on information related\nto counting results and external environmental conditions. First, a pest\ndetection network is used for pest detection and counting, extracting counting\nresult-related information. Then, the pest images undergo image quality\nassessment, image complexity assessment, and pest distribution uniformity\nassessment. And the changes in image clarity caused by stirring during image\nacquisition are quantified by calculating the average gradient magnitude.\nNotably, we designed a hypothesis-driven multi-factor sensitivity analysis\nmethod to select the optimal image quality assessment and image complexity\nassessment methods. And we proposed an adaptive DBSCAN clustering algorithm for\npest distribution uniformity assessment. Finally, the obtained information\nrelated to counting results and external environmental conditions is input into\na regression model for prediction, resulting in the final pest counting\nconfidence. To the best of our knowledge, this is the first study dedicated to\ncomprehensively evaluating counting confidence in counting tasks, and\nquantifying the relationship between influencing factors and counting\nconfidence through a model. Experimental results show our method reduces MSE by\n31.7% and improves R2 by 15.2% on the pest counting confidence test set,\ncompared to the baseline built primarily on information related to counting\nresults.", "AI": {"tldr": "本文提出了一种基于计数结果和外部环境条件信息的方法，全面评估害虫计数置信度，并通过实验验证其有效性。", "motivation": "现有害虫计数研究缺乏对计数结果可靠性的评估，本文旨在填补这一空白。", "method": "结合害虫检测网络、图像质量评估、图像复杂性评估、害虫分布均匀性评估以及多因素敏感性分析，最终通过回归模型预测计数置信度。", "result": "实验表明，该方法在害虫计数置信度测试集上比基线方法减少了31.7%的MSE，提高了15.2%的R2。", "conclusion": "本文首次全面评估了害虫计数任务的置信度，并通过模型量化了影响因素与计数置信度的关系。"}}
{"id": "2506.22865", "pdf": "https://arxiv.org/pdf/2506.22865", "abs": "https://arxiv.org/abs/2506.22865", "authors": ["Ziqi Zhong", "Xunzhu Tang"], "title": "ReasonBridge: Efficient Reasoning Transfer from Closed to Open-Source Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have revealed a\nsignificant performance gap between closed-source and open-source models,\nparticularly in tasks requiring complex reasoning and precise instruction\nfollowing. This paper introduces ReasonBridge, a methodology that efficiently\ntransfers reasoning capabilities from powerful closed-source to open-source\nmodels through a novel hierarchical knowledge distillation framework. We\ndevelop a tailored dataset Reason1K with only 1,000 carefully curated reasoning\ntraces emphasizing difficulty, diversity, and quality. These traces are\nfiltered from across multiple domains using a structured multi-criteria\nselection algorithm. Our transfer learning approach incorporates: (1) a\nhierarchical distillation process capturing both strategic abstraction and\ntactical implementation patterns, (2) a sparse reasoning-focused adapter\narchitecture requiring only 0.3% additional trainable parameters, and (3) a\ntest-time compute scaling mechanism using guided inference interventions.\nComprehensive evaluations demonstrate that ReasonBridge improves reasoning\ncapabilities in open-source models by up to 23% on benchmark tasks,\nsignificantly narrowing the gap with closed-source models. Notably, the\nenhanced Qwen2.5-14B outperforms Claude-Sonnet3.5 on MATH500 and matches its\nperformance on competition-level AIME problems. Our methodology generalizes\neffectively across diverse reasoning domains and model architectures,\nestablishing a sample-efficient approach to reasoning enhancement for\ninstruction following.", "AI": {"tldr": "ReasonBridge通过分层知识蒸馏框架，将闭源模型的推理能力高效迁移到开源模型，显著缩小性能差距。", "motivation": "开源与闭源模型在复杂推理任务上存在显著性能差距，需高效方法提升开源模型能力。", "method": "采用分层蒸馏、稀疏适配器架构和推理干预机制，使用精心筛选的1,000条推理轨迹数据集Reason1K。", "result": "开源模型推理能力提升23%，Qwen2.5-14B在MATH500上超越Claude-Sonnet3.5，AIME任务表现相当。", "conclusion": "ReasonBridge为开源模型提供高效推理增强方法，适用于多领域和架构。"}}
{"id": "2506.22486", "pdf": "https://arxiv.org/pdf/2506.22486", "abs": "https://arxiv.org/abs/2506.22486", "authors": ["Ming Cheung"], "title": "Hallucination Detection with Small Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Since the introduction of ChatGPT, large language models (LLMs) have\ndemonstrated significant utility in various tasks, such as answering questions\nthrough retrieval-augmented generation. Context can be retrieved using a\nvectorized database, serving as a foundation for LLMs to generate responses.\nHowever, hallucinations in responses can undermine the reliability of LLMs in\npractical applications, and they are not easily detectable in the absence of\nground truth, particularly in question-and-answer scenarios. This paper\nproposes a framework that integrates multiple small language models to verify\nresponses generated by LLMs using the retrieved context from a vectorized\ndatabase. By breaking down the responses into individual sentences and\nutilizing the probability of generating \"Yes\" tokens from the outputs of\nmultiple models for a given set of questions, responses, and relevant context,\nhallucinations can be detected. The proposed framework is validated through\nexperiments with real datasets comprising over 100 sets of questions, answers,\nand contexts, including responses with fully and partially correct sentences.\nThe results demonstrate a 10\\% improvement in F1 scores for detecting correct\nresponses compared to hallucinations, indicating that multiple small language\nmodels can be effectively employed for answer verification, providing a\nscalable and efficient solution for both academic and practical applications.", "AI": {"tldr": "本文提出了一种框架，利用多个小型语言模型验证大型语言模型（LLM）生成的回答，通过分解回答并利用生成“是”标记的概率检测幻觉，实验表明F1分数提高了10%。", "motivation": "LLM在问答任务中可能产生幻觉，缺乏真实数据时难以检测，影响可靠性。", "method": "集成多个小型语言模型，分解回答为句子，利用生成“是”标记的概率验证回答。", "result": "实验验证了框架有效性，F1分数提高10%，能有效检测幻觉。", "conclusion": "多个小型语言模型可高效验证LLM回答，为学术和实际应用提供可扩展解决方案。"}}
{"id": "2506.22463", "pdf": "https://arxiv.org/pdf/2506.22463", "abs": "https://arxiv.org/abs/2506.22463", "authors": ["Weizhi Gao", "Zhichao Hou", "Junqi Yin", "Feiyi Wang", "Linyu Peng", "Xiaorui Liu"], "title": "Modulated Diffusion: Accelerating Generative Modeling with Modulated Quantization", "categories": ["cs.CV", "cs.LG"], "comment": "26 pages, accepted by ICML 2025", "summary": "Diffusion models have emerged as powerful generative models, but their high\ncomputation cost in iterative sampling remains a significant bottleneck. In\nthis work, we present an in-depth and insightful study of state-of-the-art\nacceleration techniques for diffusion models, including caching and\nquantization, revealing their limitations in computation error and generation\nquality. To break these limits, this work introduces Modulated Diffusion\n(MoDiff), an innovative, rigorous, and principled framework that accelerates\ngenerative modeling through modulated quantization and error compensation.\nMoDiff not only inherents the advantages of existing caching and quantization\nmethods but also serves as a general framework to accelerate all diffusion\nmodels. The advantages of MoDiff are supported by solid theoretical insight and\nanalysis. In addition, extensive experiments on CIFAR-10 and LSUN demonstrate\nthat MoDiff significant reduces activation quantization from 8 bits to 3 bits\nwithout performance degradation in post-training quantization (PTQ). Our code\nimplementation is available at https://github.com/WeizhiGao/MoDiff.", "AI": {"tldr": "MoDiff是一种创新的扩散模型加速框架，通过调制量化和误差补偿提高生成效率，同时保持性能。", "motivation": "扩散模型的高计算成本是主要瓶颈，现有加速技术（如缓存和量化）在计算误差和生成质量方面存在局限。", "method": "提出MoDiff框架，结合调制量化和误差补偿，作为通用加速方法适用于所有扩散模型。", "result": "实验表明，MoDiff在CIFAR-10和LSUN数据集上将激活量化从8位降至3位，且性能无损。", "conclusion": "MoDiff是一种高效且通用的扩散模型加速解决方案，具有理论支持和实验验证。"}}
{"id": "2506.22893", "pdf": "https://arxiv.org/pdf/2506.22893", "abs": "https://arxiv.org/abs/2506.22893", "authors": ["Arpit Narechania", "Alex Endert", "Atanu R Sinha"], "title": "Agentic Enterprise: AI-Centric User to User-Centric AI", "categories": ["cs.AI", "cs.HC"], "comment": "12 pages, 1 figure, 2 sidebars; Preprint", "summary": "After a very long winter, the Artificial Intelligence (AI) spring is here.\nOr, so it seems over the last three years. AI has the potential to impact many\nareas of human life - personal, social, health, education, professional. In\nthis paper, we take a closer look at the potential of AI for Enterprises, where\ndecision-making plays a crucial and repeated role across functions, tasks, and\noperations. We consider Agents imbued with AI as means to increase\ndecision-productivity of enterprises. We highlight six tenets for Agentic\nsuccess in enterprises, by drawing attention to what the current, AI-Centric\nUser paradigm misses, in the face of persistent needs of and usefulness for\nEnterprise Decision-Making. In underscoring a shift to User-Centric AI, we\noffer six tenets and promote market mechanisms for platforms, aligning the\ndesign of AI and its delivery by Agents to the cause of enterprise users.", "AI": {"tldr": "论文探讨了AI在企业决策中的潜力，提出了六项成功原则，并强调从AI中心转向用户中心的范式转变。", "motivation": "研究AI如何提升企业决策效率，填补当前AI中心范式在企业需求中的不足。", "method": "通过分析企业决策需求，提出六项Agentic成功原则，并倡导用户中心的AI设计和市场机制。", "result": "提出了六项原则，推动AI设计更贴合企业用户需求，提升决策生产力。", "conclusion": "用户中心的AI设计和市场机制是实现企业决策效率提升的关键。"}}
{"id": "2506.22491", "pdf": "https://arxiv.org/pdf/2506.22491", "abs": "https://arxiv.org/abs/2506.22491", "authors": ["Oliver Warke", "Joemon M. Jose", "Faegheh Hasibi", "Jan Breitsohl"], "title": "PromptAug: Fine-grained Conflict Classification Using Data Augmentation", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; J.4; K.4.2"], "comment": null, "summary": "Given the rise of conflicts on social media, effective classification models\nto detect harmful behaviours are essential. Following the\ngarbage-in-garbage-out maxim, machine learning performance depends heavily on\ntraining data quality. However, high-quality labelled data, especially for\nnuanced tasks like identifying conflict behaviours, is limited, expensive, and\ndifficult to obtain. Additionally, as social media platforms increasingly\nrestrict access to research data, text data augmentation is gaining attention\nas an alternative to generate training data. Augmenting conflict-related data\nposes unique challenges due to Large Language Model (LLM) guardrails that\nprevent generation of offensive content. This paper introduces PromptAug, an\ninnovative LLM-based data augmentation method. PromptAug achieves statistically\nsignificant improvements of 2% in both accuracy and F1-score on conflict and\nemotion datasets. To thoroughly evaluate PromptAug against other data\naugmentation methods we conduct a robust evaluation using extreme data scarcity\nscenarios, quantitative diversity analysis and a qualitative thematic analysis.\nThe thematic analysis identifies four problematic patterns in augmented text:\nLinguistic Fluidity, Humour Ambiguity, Augmented Content Ambiguity, and\nAugmented Content Misinterpretation.\n  Overall, this work presents PromptAug as an effective method for augmenting\ndata in sensitive tasks like conflict detection, offering a unique,\ninterdisciplinary evaluation grounded in both natural language processing and\nsocial science methodology.", "AI": {"tldr": "论文提出PromptAug方法，用于增强社交媒体冲突检测任务的训练数据，显著提升了模型性能，并进行了多角度评估。", "motivation": "社交媒体冲突检测需要高质量标注数据，但获取困难且成本高，数据增强成为替代方案。然而，冲突相关数据的增强面临LLM防护机制的限制。", "method": "提出PromptAug，一种基于LLM的数据增强方法，通过生成高质量训练数据提升模型性能。", "result": "PromptAug在冲突和情感数据集上显著提升2%的准确率和F1分数，并通过多样性分析和主题分析验证其有效性。", "conclusion": "PromptAug为敏感任务（如冲突检测）提供了一种有效的数据增强方法，结合了NLP和社会科学方法进行跨学科评估。"}}
{"id": "2506.22498", "pdf": "https://arxiv.org/pdf/2506.22498", "abs": "https://arxiv.org/abs/2506.22498", "authors": ["Hao Liu", "Yu Hu", "Rakiba Rayhana", "Ling Bai", "Zheng Liu"], "title": "ViFusionTST: Deep Fusion of Time-Series Image Representations from Load Signals for Early Bed-Exit Prediction", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Bed-related falls remain a leading source of injury in hospitals and\nlong-term-care facilities, yet many commercial alarms trigger only after a\npatient has already left the bed. We show that early bed-exit intent can be\npredicted using only four low-cost load cells mounted under the bed legs. The\nresulting load signals are first converted into a compact set of complementary\nimages: an RGB line plot that preserves raw waveforms and three texture maps -\nrecurrence plot, Markov transition field, and Gramian angular field - that\nexpose higher-order dynamics. We introduce ViFusionTST, a dual-stream Swin\nTransformer that processes the line plot and texture maps in parallel and fuses\nthem through cross-attention to learn data-driven modality weights.\n  To provide a realistic benchmark, we collected six months of continuous data\nfrom 95 beds in a long-term-care facility. On this real-world dataset\nViFusionTST reaches an accuracy of 0.885 and an F1 score of 0.794, surpassing\nrecent 1D and 2D time-series baselines across F1, recall, accuracy, and AUPRC.\nThe results demonstrate that image-based fusion of load-sensor signals for time\nseries classification is a practical and effective solution for real-time,\nprivacy-preserving fall prevention.", "AI": {"tldr": "论文提出了一种使用低成本负载传感器预测患者离床意图的方法，通过图像融合和时间序列分类技术实现高精度预测。", "motivation": "解决医院和长期护理设施中因床相关跌倒导致的伤害问题，现有商业警报通常在患者已离床后才触发。", "method": "使用四个负载传感器采集信号，转换为互补图像（RGB线图和三种纹理图），并设计双流Swin Transformer（ViFusionTST）进行并行处理和跨模态融合。", "result": "在真实数据集上，ViFusionTST的准确率为0.885，F1分数为0.794，优于现有基线方法。", "conclusion": "基于图像融合的负载信号分类方法是一种实用且有效的实时隐私保护跌倒预防解决方案。"}}
{"id": "2506.22919", "pdf": "https://arxiv.org/pdf/2506.22919", "abs": "https://arxiv.org/abs/2506.22919", "authors": ["Sanskar Pandey", "Ruhaan Chopra", "Saad Murtaza Bhat", "Ark Abhyudaya"], "title": "Hecto: Modular Sparse Experts for Adaptive and Interpretable Reasoning", "categories": ["cs.AI"], "comment": null, "summary": "Mixture-of-Experts (MoE) models enable conditional computation by routing\ninputs to specialized experts, but these experts rely on identical inductive\nbiases, thus limiting representational diversity. This static computation\npathway is inefficient for inputs that require different types of reasoning and\nlimits specialization and interpretability. We propose Hecto, a lightweight MoE\narchitecture that leverages architectural heterogeneity by combining a GRU\nexpert for temporal reasoning and an FFNN expert for static abstraction under a\nsparse Top-1 gating mechanism. Evaluated on three reasoning benchmarks (AG\nNews, SST-2, HotpotQA) and a regression task (STS-B), Hecto matches or closely\ntrails homogeneous baselines in performance despite receiving isolated input\nrepresentations, while achieving clear expert specialization, with each expert\naligning to distinct reasoning types (temporal vs static). At larger batch\nsizes, Hecto exhibits improved performance, benefiting from relaxed\ncomputational constraints that allow its heterogeneous architecture to optimize\nmore effectively. Ablation results isolate architectural diversity as the\nsource of Hecto's stability and interpretability across diverse reasoning\ntasks. Overall, Hecto establishes itself as a new benchmark for conditional\ncomputation, offering a principled framework for specialized reasoning in\nlow-resource regimes with its model strength derived from principled\nspecialization.", "AI": {"tldr": "Hecto是一种轻量级的混合专家（MoE）架构，通过结合GRU专家和FFNN专家，在稀疏Top-1门控机制下实现异构计算，提升了表示多样性和任务性能。", "motivation": "传统MoE模型的静态计算路径限制了表示多样性和专家专业化，Hecto旨在通过异构架构解决这一问题。", "method": "Hecto结合了GRU专家（用于时序推理）和FFNN专家（用于静态抽象），采用稀疏Top-1门控机制。", "result": "在多个推理基准测试中，Hecto性能接近或优于同质基线，同时实现了专家专业化（时序vs静态推理）。", "conclusion": "Hecto为条件计算提供了新基准，其异构架构在低资源场景下表现出稳定性和可解释性。"}}
{"id": "2506.22508", "pdf": "https://arxiv.org/pdf/2506.22508", "abs": "https://arxiv.org/abs/2506.22508", "authors": ["Chenyang Shao", "Tianxing Li", "Chenhao Pu", "Fengli Xu", "Yong Li"], "title": "AgentStealth: Reinforcing Large Language Model for Anonymizing User-generated Text", "categories": ["cs.CL", "cs.AI"], "comment": "This work has been submitted to NeurIPS 2025. Under review", "summary": "In today's digital world, casual user-generated content often contains subtle\ncues that may inadvertently expose sensitive personal attributes. Such risks\nunderscore the growing importance of effective text anonymization to safeguard\nindividual privacy. However, existing methods either rely on rigid replacements\nthat damage utility or cloud-based LLMs that are costly and pose privacy risks.\nTo address these issues, we explore the use of locally deployed smaller-scale\nlanguage models (SLMs) for anonymization. Yet training effective SLMs remains\nchallenging due to limited high-quality supervision. To address the challenge,\nwe propose AgentStealth, a self-reinforcing LLM anonymization framework.First,\nwe introduce an adversarial anonymization workflow enhanced by In-context\nContrastive Learning and Adaptive Utility-Aware Control. Second, we perform\nsupervised adaptation of SLMs using high-quality data collected from the\nworkflow, which includes both anonymization and attack signals. Finally, we\napply online reinforcement learning where the model leverages its internal\nadversarial feedback to iteratively improve anonymization performance.\nExperiments on two datasets show that our method outperforms baselines in both\nanonymization effectiveness (+12.3%) and utility (+6.8%). Our lightweight\ndesign supports direct deployment on edge devices, avoiding cloud reliance and\ncommunication-based privacy risks. Our code is open-source at\nhttps://github.com/tsinghua-fib-lab/AgentStealth.", "AI": {"tldr": "论文提出了一种名为AgentStealth的本地化小规模语言模型（SLM）匿名化框架，通过对抗性工作流和强化学习提升匿名化效果和实用性。", "motivation": "数字时代中用户生成内容可能泄露敏感信息，现有匿名化方法要么损害实用性，要么依赖云端大模型带来隐私风险。", "method": "结合上下文对比学习和自适应实用性控制的工作流，利用高质量数据进行监督适应，并通过在线强化学习优化模型。", "result": "在两个数据集上，方法在匿名化效果（+12.3%）和实用性（+6.8%）上均优于基线。", "conclusion": "AgentStealth支持边缘设备部署，避免云端依赖，同时提供高效匿名化解决方案。"}}
{"id": "2506.22499", "pdf": "https://arxiv.org/pdf/2506.22499", "abs": "https://arxiv.org/abs/2506.22499", "authors": ["Jiachao Liu", "Pablo Guarda", "Koichiro Niinuma", "Sean Qian"], "title": "Scalable Dynamic Origin-Destination Demand Estimation Enhanced by High-Resolution Satellite Imagery Data", "categories": ["cs.CV", "cs.AI", "stat.AP"], "comment": null, "summary": "This study presents a novel integrated framework for dynamic\norigin-destination demand estimation (DODE) in multi-class mesoscopic network\nmodels, leveraging high-resolution satellite imagery together with conventional\ntraffic data from local sensors. Unlike sparse local detectors, satellite\nimagery offers consistent, city-wide road and traffic information of both\nparking and moving vehicles, overcoming data availability limitations. To\nextract information from imagery data, we design a computer vision pipeline for\nclass-specific vehicle detection and map matching, generating link-level\ntraffic density observations by vehicle class. Building upon this information,\nwe formulate a computational graph-based DODE model that calibrates dynamic\nnetwork states by jointly matching observed traffic counts and travel times\nfrom local sensors with density measurements derived from satellite imagery. To\nassess the accuracy and scalability of the proposed framework, we conduct a\nseries of numerical experiments using both synthetic and real-world data. The\nresults of out-of-sample tests demonstrate that supplementing traditional data\nwith satellite-derived density significantly improves estimation performance,\nespecially for links without local sensors. Real-world experiments also confirm\nthe framework's capability to handle large-scale networks, supporting its\npotential for practical deployment in cities of varying sizes. Sensitivity\nanalysis further evaluates the impact of data quality related to satellite\nimagery data.", "AI": {"tldr": "提出了一种结合卫星影像与传统交通数据的动态起讫点需求估计框架，显著提升了估计性能，尤其适用于无本地传感器的路段。", "motivation": "传统本地传感器数据稀疏，卫星影像提供城市范围的道路和交通信息，解决了数据可用性限制。", "method": "设计了计算机视觉流程提取车辆类别信息，构建基于计算图的DODE模型，联合校准交通计数和旅行时间。", "result": "实验表明，卫星数据显著提升估计性能，框架适用于大规模网络。", "conclusion": "该框架具有实际部署潜力，适用于不同规模城市。"}}
{"id": "2506.22920", "pdf": "https://arxiv.org/pdf/2506.22920", "abs": "https://arxiv.org/abs/2506.22920", "authors": ["Pinzheng Wang", "Juntao Li", "Zecheng Tang", "Haijia Gui", "Min zhang"], "title": "Improving Rationality in the Reasoning Process of Language Models through Self-playing Game", "categories": ["cs.AI"], "comment": "Accepted by ICML 2025", "summary": "Large language models (LLMs) have demonstrated considerable reasoning\nabilities in various tasks such as mathematics and coding. However, recent\nstudies indicate that even the best models lack true comprehension of their\nreasoning processes. In this paper, we explore how self-play can enhance the\nrationality of models in the reasoning process without supervision from humans\nor superior models. We design a Critic-Discernment Game(CDG) in which a prover\nfirst provides a solution to a given problem and is subsequently challenged by\ncritiques of its solution. These critiques either aim to assist or mislead the\nprover. The objective of the prover is to maintain the correct answer when\nfaced with misleading comments, while correcting errors in response to\nconstructive feedback. Our experiments on tasks involving mathematical\nreasoning, stepwise error detection, self-correction, and long-chain reasoning\ndemonstrate that CDG training can significantly improve the ability of\nwell-aligned LLMs to comprehend their reasoning process.", "AI": {"tldr": "本文探讨了通过自我对弈（Critic-Discernment Game, CDG）提升大语言模型（LLMs）在无监督情况下的推理能力。", "motivation": "尽管LLMs在数学和编程等任务中表现出推理能力，但它们缺乏对自身推理过程的真正理解。本文旨在通过自我对弈增强模型的理性推理能力。", "method": "设计了Critic-Discernment Game（CDG），其中证明者提供解决方案，随后接受批评者的挑战。批评者可能提供帮助或误导，证明者需在误导中保持正确答案，并在建设性反馈中修正错误。", "result": "实验表明，CDG训练显著提升了LLMs在数学推理、逐步错误检测、自我修正和长链推理任务中的推理理解能力。", "conclusion": "自我对弈方法（CDG）能有效提升LLMs的推理能力，尤其是在无监督环境下。"}}
{"id": "2506.22510", "pdf": "https://arxiv.org/pdf/2506.22510", "abs": "https://arxiv.org/abs/2506.22510", "authors": ["Zihao Zhao", "Xinlong Zhai", "Jinyu Yang", "Chuan Shi"], "title": "Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive Learning", "categories": ["cs.CL", "cs.AI"], "comment": "16 pages, 5 figures", "summary": "Foundation models have achieved great success in natural language processing\n(NLP) and computer vision (CV). Their success largely stems from the ability to\nintegrate multi-domain knowledge in pre-training and transfer it to target\ndomains. Considering graph data, especially graphs without textual features, is\nubiquitous in real-world applications such as social networks and\nrecommendation systems, some researchers have attempted to extend this paradigm\nto the graph field, aiming to construct graph foundation models. However,\nunlike CV and NLP, there are huge gaps among the semantics and properties of\ngraphs in different domains, while current works still adopt traditional\ncontrastive pre-training strategies designed in the single-domain scenario,\nwhich regard contrastive samples from different domains as equivalent. From\nexperimental investigations, we discovered that inherent domain-specific\ndifferences prevent these strategies from effectively absorbing knowledge from\ndifferent domains to generate informative representations. In this paper, we\npropose a novel multi-domain pre-training and cross-domain transfer framework,\nnamely MDGCL.In the pre-training stage, we design a contrastive learning\nstrategy to substantially recognize and capture domain differences, and\nintroduce domain tokens to encode domain-level global information. In the\ndownstream stage, we introduce a domain attention mechanism to enable\nfine-grained domain knowledge transfer. Extensive experiments on five benchmark\ndatasets have demonstrated that our method outperforms state-of-the-art\nsignificantly, with the maximum improvement of 19.33\\% on accuracy and 19.13\\%\non Macro-F1 score.", "AI": {"tldr": "论文提出了一种新的多领域预训练与跨领域迁移框架MDGCL，通过对比学习策略和领域注意力机制，显著提升了图数据在不同领域的表示学习效果。", "motivation": "由于图数据在不同领域的语义和属性差异巨大，传统的单领域对比预训练策略无法有效吸收多领域知识，因此需要一种新方法来识别和利用这些差异。", "method": "在预训练阶段，设计了一种对比学习策略以识别领域差异，并引入领域令牌编码全局信息；在下游任务中，采用领域注意力机制实现细粒度知识迁移。", "result": "在五个基准数据集上的实验表明，MDGCL在准确率和Macro-F1分数上分别最高提升了19.33%和19.13%。", "conclusion": "MDGCL通过有效识别和利用领域差异，显著提升了图数据在多领域预训练和跨领域迁移中的表现。"}}
{"id": "2506.22500", "pdf": "https://arxiv.org/pdf/2506.22500", "abs": "https://arxiv.org/abs/2506.22500", "authors": ["Weiyi Zhao", "Xiaoyu Tan", "Liang Liu", "Sijia Li", "Youwei Song", "Xihe Qiu"], "title": "Visual-Semantic Knowledge Conflicts in Operating Rooms: Synthetic Data Curation for Surgical Risk Perception in Multimodal Large Language Models", "categories": ["cs.CV", "cs.AI", "68T07, 68U10, 92C55", "I.2.10; I.2.7; J.3; I.2.6"], "comment": "13 pages, 5 figures. The dataset and appendix are available at\n  https://github.com/zgg2577/VS-KC", "summary": "Surgical risk identification is critical for patient safety and reducing\npreventable medical errors. While multimodal large language models (MLLMs) show\npromise for automated operating room (OR) risk detection, they often exhibit\nvisual-semantic knowledge conflicts (VS-KC), failing to identify visual safety\nviolations despite understanding textual rules. To address this, we introduce a\ndataset comprising over 34,000 synthetic images generated by diffusion models,\ndepicting operating room scenes containing entities that violate established\nsafety rules. These images were created to alleviate data scarcity and examine\nMLLMs vulnerabilities. In addition, the dataset includes 214 human-annotated\nimages that serve as a gold-standard reference for validation. This\ncomprehensive dataset, spanning diverse perspectives, stages, and\nconfigurations, is designed to expose and study VS-KC. Fine-tuning on OR-VSKC\nsignificantly improves MLLMs' detection of trained conflict entities and\ngeneralizes well to new viewpoints for these entities, but performance on\nuntrained entity types remains poor, highlighting learning specificity and the\nneed for comprehensive training. The main contributions of this work include:\n(1) a data generation methodology tailored for rule-violation scenarios; (2)\nthe release of the OR-VSKC dataset and its associated benchmark as open-source\nresources; and (3) an empirical analysis of violation-sensitive knowledge\nconsistency in representative MLLMs. The dataset and appendix are available at\nhttps://github.com/zgg2577/VS-KC.", "AI": {"tldr": "论文提出了一种针对手术室风险检测的数据集OR-VSKC，通过合成图像和人工标注解决视觉-语义知识冲突问题，并验证了微调MLLMs的有效性。", "motivation": "手术风险识别对患者安全至关重要，但现有MLLMs在视觉-语义知识冲突（VS-KC）方面表现不佳，无法准确识别视觉安全违规。", "method": "使用扩散模型生成34,000张合成图像，并辅以214张人工标注图像，构建OR-VSKC数据集，用于微调MLLMs。", "result": "微调后的MLLMs在训练过的冲突实体检测上表现显著提升，但对未训练实体类型效果仍差。", "conclusion": "研究贡献包括数据生成方法、开源数据集及对MLLMs知识一致性的分析，为未来研究提供了资源与方向。"}}
{"id": "2506.22992", "pdf": "https://arxiv.org/pdf/2506.22992", "abs": "https://arxiv.org/abs/2506.22992", "authors": ["Yulun Jiang", "Yekun Chai", "Maria Brbić", "Michael Moor"], "title": "MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": null, "summary": "The ability to process information from multiple modalities and to reason\nthrough it step-by-step remains a critical challenge in advancing artificial\nintelligence. However, existing reasoning benchmarks focus on text-only\nreasoning, or employ multimodal questions that can be answered by directly\nretrieving information from a non-text modality. Thus, complex reasoning\nremains poorly understood in multimodal domains. Here, we present MARBLE, a\nchallenging multimodal reasoning benchmark that is designed to scrutinize\nmultimodal language models (MLLMs) in their ability to carefully reason\nstep-by-step through complex multimodal problems and environments. MARBLE is\ncomposed of two highly challenging tasks, M-Portal and M-Cube, that require the\ncrafting and understanding of multistep plans under spatial, visual, and\nphysical constraints. We find that current MLLMs perform poorly on MARBLE --\nall the 12 advanced models obtain near-random performance on M-Portal and 0%\naccuracy on M-Cube. Only in simplified subtasks some models outperform the\nrandom baseline, indicating that complex reasoning is still a challenge for\nexisting MLLMs. Moreover, we show that perception remains a bottleneck, where\nMLLMs occasionally fail to extract information from the visual inputs. By\nshedding a light on the limitations of MLLMs, we hope that MARBLE will spur the\ndevelopment of the next generation of models with the ability to reason and\nplan across many, multimodal reasoning steps.", "AI": {"tldr": "MARBLE是一个多模态推理基准测试，旨在评估多模态语言模型（MLLMs）在复杂多模态问题中的逐步推理能力。现有模型表现不佳，表明多模态复杂推理仍是挑战。", "motivation": "现有推理基准主要关注文本或简单多模态问题，复杂多模态推理能力尚未被充分研究。", "method": "MARBLE包含两个高难度任务（M-Portal和M-Cube），要求模型在空间、视觉和物理约束下进行多步规划和理解。", "result": "12个先进模型在M-Portal上表现接近随机，M-Cube上准确率为0%，仅在某些简化子任务中优于随机基线。", "conclusion": "MARBLE揭示了MLLMs在复杂多模态推理中的局限性，尤其是感知能力不足，希望推动下一代模型的开发。"}}
{"id": "2506.22516", "pdf": "https://arxiv.org/pdf/2506.22516", "abs": "https://arxiv.org/abs/2506.22516", "authors": ["Jingkai Li"], "title": "Can \"consciousness\" be observed from large language model (LLM) internal states? Dissecting LLM representations obtained from Theory of Mind test with Integrated Information Theory and Span Representation analysis", "categories": ["cs.CL", "cs.AI", "cs.NE", "q-bio.NC"], "comment": "Published as a journal paper at:\n  https://doi.org/10.1016/j.nlp.2025.100163", "summary": "Integrated Information Theory (IIT) provides a quantitative framework for\nexplaining consciousness phenomenon, positing that conscious systems comprise\nelements integrated through causal properties. We apply IIT 3.0 and 4.0 -- the\nlatest iterations of this framework -- to sequences of Large Language Model\n(LLM) representations, analyzing data derived from existing Theory of Mind\n(ToM) test results. Our study systematically investigates whether the\ndifferences of ToM test performances, when presented in the LLM\nrepresentations, can be revealed by IIT estimates, i.e., $\\Phi^{\\max}$ (IIT\n3.0), $\\Phi$ (IIT 4.0), Conceptual Information (IIT 3.0), and $\\Phi$-structure\n(IIT 4.0). Furthermore, we compare these metrics with the Span Representations\nindependent of any estimate for consciousness. This additional effort aims to\ndifferentiate between potential \"consciousness\" phenomena and inherent\nseparations within LLM representational space. We conduct comprehensive\nexperiments examining variations across LLM transformer layers and linguistic\nspans from stimuli. Our results suggest that sequences of contemporary\nTransformer-based LLM representations lack statistically significant indicators\nof observed \"consciousness\" phenomena but exhibit intriguing patterns under\n$\\textit{spatio}$-permutational analyses. The Appendix and code are available\nas Supplementary Materials at: https://doi.org/10.1016/j.nlp.2025.100163.", "AI": {"tldr": "研究应用IIT 3.0和4.0分析LLM的表征，探讨其是否显示意识现象，结果显示LLM表征缺乏显著意识指标，但在空间置换分析中表现出有趣模式。", "motivation": "探索大型语言模型（LLM）的表征是否表现出意识现象，通过IIT框架量化分析。", "method": "应用IIT 3.0和4.0的指标（如Φmax、Φ、概念信息和Φ结构）分析LLM的ToM测试数据，并与Span Representations比较。", "result": "LLM表征缺乏显著意识指标，但在空间置换分析中表现出独特模式。", "conclusion": "当代Transformer-based LLM表征未显示显著意识现象，但需进一步研究其潜在模式。"}}
{"id": "2506.22501", "pdf": "https://arxiv.org/pdf/2506.22501", "abs": "https://arxiv.org/abs/2506.22501", "authors": ["Gautam Siddharth Kashyap", "Manaswi Kulahara", "Nipun Joshi", "Usman Naseem"], "title": "How Can Multimodal Remote Sensing Datasets Transform Classification via SpatialNet-ViT?", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted in the 2025 IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS 2025), scheduled for 3 - 8 August 2025 in Brisbane,\n  Australia", "summary": "Remote sensing datasets offer significant promise for tackling key\nclassification tasks such as land-use categorization, object presence\ndetection, and rural/urban classification. However, many existing studies tend\nto focus on narrow tasks or datasets, which limits their ability to generalize\nacross various remote sensing classification challenges. To overcome this, we\npropose a novel model, SpatialNet-ViT, leveraging the power of Vision\nTransformers (ViTs) and Multi-Task Learning (MTL). This integrated approach\ncombines spatial awareness with contextual understanding, improving both\nclassification accuracy and scalability. Additionally, techniques like data\naugmentation, transfer learning, and multi-task learning are employed to\nenhance model robustness and its ability to generalize across diverse datasets", "AI": {"tldr": "提出了一种结合Vision Transformers和多任务学习的新模型SpatialNet-ViT，用于提升遥感分类任务的泛化能力和准确性。", "motivation": "现有研究多局限于狭窄任务或数据集，难以泛化到多样化的遥感分类问题。", "method": "采用Vision Transformers和多任务学习，结合数据增强、迁移学习等技术。", "result": "模型在分类准确性和可扩展性上均有提升。", "conclusion": "SpatialNet-ViT通过整合空间感知与上下文理解，显著提升了遥感分类任务的性能。"}}
{"id": "2506.23049", "pdf": "https://arxiv.org/pdf/2506.23049", "abs": "https://arxiv.org/abs/2506.23049", "authors": ["Leander Melroy Maben", "Gayathri Ganesh Lakshmy", "Srijith Radhakrishnan", "Siddhant Arora", "Shinji Watanabe"], "title": "AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks", "categories": ["cs.AI", "cs.CL", "cs.SD", "eess.AS", "68T42, 68T50,", "I.2.7; I.2.11; H.5.5"], "comment": null, "summary": "Despite advances in language and speech technologies, no open-source system\nenables full speech-to-speech, multi-turn dialogue with integrated tool use and\nagentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and\nAutomated Tool Use), the first open-source, speech-native assistant capable of\ncompleting complex, goal-driven tasks through dynamic tool invocation and\nmulti-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a\ncascaded pipeline and supports tools such as calendar booking, contact lookup,\nweb search, and email. Its modular design allows easy integration of new tools\nusing natural language prompts and action classes. On VoiceBench, AURA scores\n92.75% on OpenBookQA-outperforming all open-weight systems and nearing\nGPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems.\nHuman evaluation shows 90% task success on complex, multi-turn speech tasks.", "AI": {"tldr": "AURA是首个开源的语音原生助手，支持多轮对话和工具调用，结合ASR、TTS和LLM技术，在复杂任务中表现优异。", "motivation": "当前缺乏开源的全语音多轮对话系统，AURA填补了这一空白，支持动态工具调用和任务完成。", "method": "AURA采用模块化设计，结合开源ASR、TTS和LLM技术，支持自然语言提示和工具集成。", "result": "在VoiceBench上，AURA表现优异（OpenBookQA 92.75%，AlpacaEval 4.39），接近GPT-4o水平，人类评估任务成功率达90%。", "conclusion": "AURA为开源语音助手领域提供了高效解决方案，支持复杂任务和多轮对话。"}}
{"id": "2506.22518", "pdf": "https://arxiv.org/pdf/2506.22518", "abs": "https://arxiv.org/abs/2506.22518", "authors": ["Deyu Zou", "Yongqiang Chen", "Mufei Li", "Siqi Miao", "Chenxi Liu", "Bo Han", "James Cheng", "Pan Li"], "title": "Weak-to-Strong GraphRAG: Aligning Weak Retrievers with Large Language Models for Graph-based Retrieval Augmented Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Graph-based retrieval-augmented generation (RAG) enables large language\nmodels (LLMs) to ground responses with structured external knowledge from\nup-to-date knowledge graphs (KGs) and reduce hallucinations. However, LLMs\noften rely on a weak retriever in graph-based RAG: I) Due to the lack of ground\ntruth, the retriever is often trained on weak supervision, which often\nintroduces spurious signals to the LLMs. II) Due to the abstraction of graph\ndata, the retrieved knowledge is often presented in unorganized forms. To\nmitigate the issue, we present Refined Graph-based RAG (ReG) to align weak\nretrievers to LLMs for graph-based RAG. Specifically, ReG incorporates LLM\nfeedback to get rid of spurious signals and improve the quality of the\nsupervision. Meanwhile, ReG introduces a structure-aware reorganization module\nto refactor the retrieval results into logically coherent evidence chains.\nExperiments on prominent benchmarks demonstrate that ReG significantly and\nconsistently brings improvements across different LLM backbones by up to 10%.\nThe improved supervision quality enables ReG to match the state-of-the-art\nperformance with 5% training data and to transfer to out-of-distribution KGs.\nNotably, when adopted to reasoning-based LLMs, ReG reduces the reasoning token\ncost by up to 30% and improves the performance by up to 4%.", "AI": {"tldr": "ReG方法通过LLM反馈和结构化重组模块优化图检索增强生成（RAG），显著提升性能并减少推理成本。", "motivation": "解决图检索增强生成中弱检索器的问题，包括缺乏真实标注数据和检索结果的无序性。", "method": "引入ReG方法，利用LLM反馈消除虚假信号，并通过结构感知重组模块生成逻辑连贯的证据链。", "result": "实验显示ReG在不同LLM上性能提升达10%，训练数据需求减少80%，推理成本降低30%。", "conclusion": "ReG有效优化了图检索增强生成，提升了性能和数据效率。"}}
{"id": "2506.22503", "pdf": "https://arxiv.org/pdf/2506.22503", "abs": "https://arxiv.org/abs/2506.22503", "authors": ["Michiel Schepers", "Pieter Robberechts", "Jan Van Haaren", "Jesse Davis"], "title": "What Makes a Dribble Successful? Insights From 3D Pose Tracking Data", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Data analysis plays an increasingly important role in soccer, offering new\nways to evaluate individual and team performance. One specific application is\nthe evaluation of dribbles: one-on-one situations where an attacker attempts to\nbypass a defender with the ball. While previous research has primarily relied\non 2D positional tracking data, this fails to capture aspects like balance,\norientation, and ball control, limiting the depth of current insights. This\nstudy explores how pose tracking data (capturing players' posture and movement\nin three dimensions) can improve our understanding of dribbling skills. We\nextract novel pose-based features from 1,736 dribbles in the 2022/23 Champions\nLeague season and evaluate their impact on dribble success. Our results\nindicate that features capturing the attacker's balance and the alignment of\nthe orientation between the attacker and defender are informative for\npredicting dribble success. Incorporating these pose-based features on top of\nfeatures derived from traditional 2D positional data leads to a measurable\nimprovement in model performance.", "AI": {"tldr": "本研究探讨了姿势跟踪数据如何提升对足球中带球技能的理解，通过提取新的姿势特征并评估其对带球成功率的影响。", "motivation": "传统2D位置数据无法捕捉平衡、方向和控球等关键因素，限制了带球分析的深度。", "method": "从2022/23赛季欧冠的1,736次带球中提取姿势特征，并结合传统2D数据评估其对带球成功率的预测效果。", "result": "姿势特征（如攻击者平衡和攻防方向对齐）显著提升了模型性能。", "conclusion": "姿势数据补充传统数据，能更全面地分析带球技能。"}}
{"id": "2506.23080", "pdf": "https://arxiv.org/pdf/2506.23080", "abs": "https://arxiv.org/abs/2506.23080", "authors": ["Xinmin Fang", "Lingfeng Tao", "Zhengxiong Li"], "title": "AI's Euclid's Elements Moment: From Language Models to Computable Thought", "categories": ["cs.AI"], "comment": null, "summary": "This paper presents a comprehensive five-stage evolutionary framework for\nunderstanding the development of artificial intelligence, arguing that its\ntrajectory mirrors the historical progression of human cognitive technologies.\nWe posit that AI is advancing through distinct epochs, each defined by a\nrevolutionary shift in its capacity for representation and reasoning, analogous\nto the inventions of cuneiform, the alphabet, grammar and logic, mathematical\ncalculus, and formal logical systems. This \"Geometry of Cognition\" framework\nmoves beyond mere metaphor to provide a systematic, cross-disciplinary model\nthat not only explains AI's past architectural shifts-from expert systems to\nTransformers-but also charts a concrete and prescriptive path forward.\nCrucially, we demonstrate that this evolution is not merely linear but\nreflexive: as AI advances through these stages, the tools and insights it\ndevelops create a feedback loop that fundamentally reshapes its own underlying\narchitecture. We are currently transitioning into a \"Metalinguistic Moment,\"\ncharacterized by the emergence of self-reflective capabilities like\nChain-of-Thought prompting and Constitutional AI. The subsequent stages, the\n\"Mathematical Symbolism Moment\" and the \"Formal Logic System Moment,\" will be\ndefined by the development of a computable calculus of thought, likely through\nneuro-symbolic architectures and program synthesis, culminating in provably\naligned and reliable AI that reconstructs its own foundational representations.\nThis work serves as the methodological capstone to our trilogy, which\npreviously explored the economic drivers (\"why\") and cognitive nature (\"what\")\nof AI. Here, we address the \"how,\" providing a theoretical foundation for\nfuture research and offering concrete, actionable strategies for startups and\ndevelopers aiming to build the next generation of intelligent systems.", "AI": {"tldr": "本文提出了一个五阶段的进化框架，用于理解人工智能的发展，认为其轨迹与人类认知技术的历史进步相似。", "motivation": "探索人工智能发展的系统性模型，解释其过去架构变化并预测未来路径。", "method": "提出“认知几何”框架，将AI发展分为五个阶段，每个阶段对应一种革命性的表示和推理能力。", "result": "AI发展不仅是线性的，还具有自反性，当前正进入“元语言时刻”，未来将迈向“数学符号时刻”和“形式逻辑系统时刻”。", "conclusion": "该框架为未来研究提供了理论基础，并为开发下一代智能系统提供了具体策略。"}}
{"id": "2506.22529", "pdf": "https://arxiv.org/pdf/2506.22529", "abs": "https://arxiv.org/abs/2506.22529", "authors": ["Lu Kalkbrenner", "Veronika Solopova", "Steffen Zeiler", "Robert Nickel", "Dorothea Kolossa"], "title": "MisinfoTeleGraph: Network-driven Misinformation Detection for German Telegram Messages", "categories": ["cs.CL"], "comment": null, "summary": "Connectivity and message propagation are central, yet often underutilized,\nsources of information in misinformation detection -- especially on poorly\nmoderated platforms such as Telegram, which has become a critical channel for\nmisinformation dissemination, namely in the German electoral context. In this\npaper, we introduce Misinfo-TeleGraph, the first German-language Telegram-based\ngraph dataset for misinformation detection. It includes over 5 million messages\nfrom public channels, enriched with metadata, channel relationships, and both\nweak and strong labels. These labels are derived via semantic similarity to\nfact-checks and news articles using M3-embeddings, as well as manual\nannotation. To establish reproducible baselines, we evaluate both text-only\nmodels and graph neural networks (GNNs) that incorporate message forwarding as\na network structure. Our results show that GraphSAGE with LSTM aggregation\nsignificantly outperforms text-only baselines in terms of Matthews Correlation\nCoefficient (MCC) and F1-score. We further evaluate the impact of subscribers,\nview counts, and automatically versus human-created labels on performance, and\nhighlight both the potential and challenges of weak supervision in this domain.\nThis work provides a reproducible benchmark and open dataset for future\nresearch on misinformation detection in German-language Telegram networks and\nother low-moderation social platforms.", "AI": {"tldr": "论文介绍了首个德语Telegram虚假信息检测数据集Misinfo-TeleGraph，结合文本和图神经网络方法，显示GraphSAGE-LSTM优于纯文本模型。", "motivation": "Telegram等低监管平台成为虚假信息传播的重要渠道，但连通性和消息传播信息在虚假信息检测中未被充分利用。", "method": "构建包含500万条消息的数据集，结合元数据、频道关系和强弱标签，评估文本模型和图神经网络（如GraphSAGE-LSTM）。", "result": "GraphSAGE-LSTM在MCC和F1分数上显著优于纯文本模型，同时探讨了订阅者、浏览量及标签来源对性能的影响。", "conclusion": "该研究为德语Telegram网络虚假信息检测提供了可复现的基准和开放数据集，展示了弱监督的潜力与挑战。"}}
{"id": "2506.22504", "pdf": "https://arxiv.org/pdf/2506.22504", "abs": "https://arxiv.org/abs/2506.22504", "authors": ["Hassan Baker", "Austin J. Brockmeier"], "title": "Patch2Loc: Learning to Localize Patches for Unsupervised Brain Lesion Detection", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Detecting brain lesions as abnormalities observed in magnetic resonance\nimaging (MRI) is essential for diagnosis and treatment. In the search of\nabnormalities, such as tumors and malformations, radiologists may benefit from\ncomputer-aided diagnostics that use computer vision systems trained with\nmachine learning to segment normal tissue from abnormal brain tissue. While\nsupervised learning methods require annotated lesions, we propose a new\nunsupervised approach (Patch2Loc) that learns from normal patches taken from\nstructural MRI. We train a neural network model to map a patch back to its\nspatial location within a slice of the brain volume. During inference, abnormal\npatches are detected by the relatively higher error and/or variance of the\nlocation prediction. This generates a heatmap that can be integrated into\npixel-wise methods to achieve finer-grained segmentation. We demonstrate the\nability of our model to segment abnormal brain tissues by applying our approach\nto the detection of tumor tissues in MRI on T2-weighted images from BraTS2021\nand MSLUB datasets and T1-weighted images from ATLAS and WMH datasets. We show\nthat it outperforms the state-of-the art in unsupervised segmentation. The\ncodebase for this work can be found on our\n\\href{https://github.com/bakerhassan/Patch2Loc}{GitHub page}.", "AI": {"tldr": "提出了一种无监督学习方法（Patch2Loc），通过训练神经网络模型从正常MRI图像中学习，检测异常脑组织。", "motivation": "脑部病变检测对诊断和治疗至关重要，但现有监督学习方法需要标注数据，而Patch2Loc提供了一种无监督解决方案。", "method": "模型通过将图像块映射回其空间位置来学习正常组织特征，异常块通过预测误差或方差检测。", "result": "在多个数据集上验证，Patch2Loc在无监督分割任务中优于现有方法。", "conclusion": "Patch2Loc为无监督脑部病变检测提供了高效且精确的方法。"}}
{"id": "2506.23107", "pdf": "https://arxiv.org/pdf/2506.23107", "abs": "https://arxiv.org/abs/2506.23107", "authors": ["Bing Song", "Jianing Liu", "Sisi Jian", "Chenyang Wu", "Vinayak Dixit"], "title": "Can Large Language Models Capture Human Risk Preferences? A Cross-Cultural Study", "categories": ["cs.AI"], "comment": "20 pages, 1 figure", "summary": "Large language models (LLMs) have made significant strides, extending their\napplications to dialogue systems, automated content creation, and\ndomain-specific advisory tasks. However, as their use grows, concerns have\nemerged regarding their reliability in simulating complex decision-making\nbehavior, such as risky decision-making, where a single choice can lead to\nmultiple outcomes. This study investigates the ability of LLMs to simulate\nrisky decision-making scenarios. We compare model-generated decisions with\nactual human responses in a series of lottery-based tasks, using transportation\nstated preference survey data from participants in Sydney, Dhaka, Hong Kong,\nand Nanjing. Demographic inputs were provided to two LLMs -- ChatGPT 4o and\nChatGPT o1-mini -- which were tasked with predicting individual choices. Risk\npreferences were analyzed using the Constant Relative Risk Aversion (CRRA)\nframework. Results show that both models exhibit more risk-averse behavior than\nhuman participants, with o1-mini aligning more closely with observed human\ndecisions. Further analysis of multilingual data from Nanjing and Hong Kong\nindicates that model predictions in Chinese deviate more from actual responses\ncompared to English, suggesting that prompt language may influence simulation\nperformance. These findings highlight both the promise and the current\nlimitations of LLMs in replicating human-like risk behavior, particularly in\nlinguistic and cultural settings.", "AI": {"tldr": "研究探讨了大型语言模型（LLMs）在模拟风险决策中的表现，发现模型比人类更风险规避，且中文提示下表现更差。", "motivation": "随着LLMs应用扩展，其在复杂决策（如风险决策）中的可靠性引发关注。", "method": "通过彩票任务比较ChatGPT 4o和o1-mini的预测与人类实际选择，使用CRRA框架分析风险偏好。", "result": "模型比人类更风险规避，o1-mini更接近人类决策；中文提示下预测偏差更大。", "conclusion": "LLMs在模拟人类风险行为方面有潜力，但在语言和文化背景中存在局限性。"}}
{"id": "2506.22598", "pdf": "https://arxiv.org/pdf/2506.22598", "abs": "https://arxiv.org/abs/2506.22598", "authors": ["Nicholas Edwards", "Yukyung Lee", "Yujun", "Mao", "Yulu Qin", "Sebastian Schuster", "Najoung Kim"], "title": "RExBench: Can coding agents autonomously implement AI research extensions?", "categories": ["cs.CL"], "comment": null, "summary": "Agents based on Large Language Models (LLMs) have shown promise for\nperforming sophisticated software engineering tasks autonomously. In addition,\nthere has been progress towards developing agents that can perform parts of the\nresearch pipeline in machine learning and the natural sciences. We argue that\nresearch extension and its implementation is a critical capability for such\nsystems, and introduce RExBench to support the evaluation of this capability.\nRExBench is a benchmark consisting of 12 realistic research experiment\nimplementation tasks that aim to investigate research hypotheses that have not\npreviously been implemented. Each task is set up as an extension to an existing\nresearch paper and codebase, accompanied by domain expert-written instructions.\nRExBench is robust to data contamination, and supports an automatic evaluation\ninfrastructure that executes agent outputs to determine whether the success\ncriteria are met. We use this benchmark to evaluate nine LLM agents implemented\nusing three different frameworks: aider, Claude Code, and OpenHands. We find\nthat all agents evaluated fail to autonomously implement the majority of the\nextensions. Although the success rate improves with additional human-written\nhints, the best performance under this setting remains below 40%. This\nindicates that current agents are still short of being able to handle realistic\nresearch extension tasks without substantial human guidance.", "AI": {"tldr": "RExBench是一个用于评估LLM代理在未实现的研究假设扩展任务中表现的基准测试，结果显示当前代理在无大量人工指导的情况下表现不佳。", "motivation": "研究扩展能力是LLM代理的关键能力，但现有代理在此类任务中的表现尚未被充分评估。", "method": "引入RExBench基准测试，包含12个未实现的研究假设扩展任务，并评估了9种基于不同框架的LLM代理。", "result": "所有代理在无人工提示的情况下均未能完成大部分任务，最佳表现也低于40%。", "conclusion": "当前LLM代理在自主处理研究扩展任务时仍需大量人工指导。"}}
{"id": "2506.22505", "pdf": "https://arxiv.org/pdf/2506.22505", "abs": "https://arxiv.org/abs/2506.22505", "authors": ["Hassan Baker", "Matthew S. Emigh", "Austin J. Brockmeier"], "title": "Weakly Supervised Object Segmentation by Background Conditional Divergence", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "As a computer vision task, automatic object segmentation remains challenging\nin specialized image domains without massive labeled data, such as synthetic\naperture sonar images, remote sensing, biomedical imaging, etc. In any domain,\nobtaining pixel-wise segmentation masks is expensive. In this work, we propose\na method for training a masking network to perform binary object segmentation\nusing weak supervision in the form of image-wise presence or absence of an\nobject of interest, which provides less information but may be obtained more\nquickly from manual or automatic labeling. A key step in our method is that the\nsegmented objects can be placed into background-only images to create\nrealistic, images of the objects with counterfactual backgrounds. To create a\ncontrast between the original and counterfactual background images, we propose\nto first cluster the background-only images, and then during learning create\ncounterfactual images that blend objects segmented from their original source\nbackgrounds to backgrounds chosen from a targeted cluster. One term in the\ntraining loss is the divergence between these counterfactual images and the\nreal object images with backgrounds of the target cluster. The other term is a\nsupervised loss for background-only images. While an adversarial critic could\nprovide the divergence, we use sample-based divergences. We conduct experiments\non side-scan and synthetic aperture sonar in which our approach succeeds\ncompared to previous unsupervised segmentation baselines that were only tested\non natural images. Furthermore, to show generality we extend our experiments to\nnatural images, obtaining reasonable performance with our method that avoids\npretrained networks, generative networks, and adversarial critics. The basecode\nfor this work can be found at\n\\href{GitHub}{https://github.com/bakerhassan/WSOS}.", "AI": {"tldr": "提出一种弱监督学习方法，利用图像级标签（对象存在与否）训练二值分割网络，通过生成反事实背景图像提升分割性能。", "motivation": "在缺乏大量标注数据的专业图像领域（如声纳、遥感、生物医学图像），像素级分割标注成本高，而图像级标签更易获取。", "method": "通过聚类背景图像，将分割对象与目标背景结合生成反事实图像，利用样本间差异和背景监督损失训练网络。", "result": "在声纳图像和自然图像上均优于无监督基线方法，且无需预训练网络、生成网络或对抗判别器。", "conclusion": "该方法在弱监督条件下实现了有效的对象分割，适用于多种图像领域。"}}
{"id": "2506.23123", "pdf": "https://arxiv.org/pdf/2506.23123", "abs": "https://arxiv.org/abs/2506.23123", "authors": ["Rishi Bommasani"], "title": "The Societal Impact of Foundation Models: Advancing Evidence-based AI Policy", "categories": ["cs.AI", "cs.CY", "cs.ET"], "comment": "Stanford University PhD Dissertation of Rishi Bommasani (Department\n  of Computer Science, 2025). Also available at\n  https://purl.stanford.edu/zf669yy0336", "summary": "Artificial intelligence is humanity's most promising technology because of\nthe remarkable capabilities offered by foundation models. Yet, the same\ntechnology brings confusion and consternation: foundation models are poorly\nunderstood and they may precipitate a wide array of harms. This dissertation\nexplains how technology and society coevolve in the age of AI, organized around\nthree themes. First, the conceptual framing: the capabilities, risks, and the\nsupply chain that grounds foundation models in the broader economy. Second, the\nempirical insights that enrich the conceptual foundations: transparency created\nvia evaluations at the model level and indexes at the organization level.\nFinally, the transition from understanding to action: superior understanding of\nthe societal impact of foundation models advances evidence-based AI policy.\nView together, this dissertation makes inroads into achieving better societal\noutcomes in the age of AI by building the scientific foundations and\nresearch-policy interface required for better AI governance.", "AI": {"tldr": "论文探讨了基础模型在AI时代对社会的影响，提出通过科学基础和政策研究改善AI治理。", "motivation": "基础模型虽具潜力，但因其不透明性和潜在危害引发社会困惑，需研究其与社会共同演化的机制。", "method": "围绕三个主题展开：概念框架（能力、风险、供应链）、实证研究（模型评估与组织透明度）、从理解到行动（基于证据的AI政策）。", "result": "论文为AI治理提供了科学基础和研究-政策接口，旨在实现更好的社会结果。", "conclusion": "通过系统研究基础模型的社会影响，推动AI治理的科学化和政策化，以实现更优的社会效果。"}}
{"id": "2506.22623", "pdf": "https://arxiv.org/pdf/2506.22623", "abs": "https://arxiv.org/abs/2506.22623", "authors": ["Badr Youbi Idrissi", "Monica Millunzi", "Amelia Sorrenti", "Lorenzo Baraldi", "Daryna Dementieva"], "title": "Temperature Matters: Enhancing Watermark Robustness Against Paraphrasing Attacks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the present-day scenario, Large Language Models (LLMs) are establishing\ntheir presence as powerful instruments permeating various sectors of society.\nWhile their utility offers valuable support to individuals, there are multiple\nconcerns over potential misuse. Consequently, some academic endeavors have\nsought to introduce watermarking techniques, characterized by the inclusion of\nmarkers within machine-generated text, to facilitate algorithmic\nidentification. This research project is focused on the development of a novel\nmethodology for the detection of synthetic text, with the overarching goal of\nensuring the ethical application of LLMs in AI-driven text generation. The\ninvestigation commences with replicating findings from a previous baseline\nstudy, thereby underscoring its susceptibility to variations in the underlying\ngeneration model. Subsequently, we propose an innovative watermarking approach\nand subject it to rigorous evaluation, employing paraphrased generated text to\nasses its robustness. Experimental results highlight the robustness of our\nproposal compared to the~\\cite{aarson} watermarking method.", "AI": {"tldr": "本文提出了一种新的水印技术，用于检测合成文本，以确保大型语言模型（LLMs）的伦理应用。", "motivation": "随着LLMs在各领域的广泛应用，其潜在滥用引发担忧，因此需要开发有效的水印技术以识别机器生成文本。", "method": "研究首先复现了基线研究的结果，指出其易受生成模型变化的影响；随后提出了一种创新的水印方法，并通过改写生成文本评估其鲁棒性。", "result": "实验结果表明，所提出的水印方法比现有方法（如aarson）更具鲁棒性。", "conclusion": "该研究为合成文本检测提供了更可靠的水印技术，有助于推动LLMs的伦理使用。"}}
{"id": "2506.22509", "pdf": "https://arxiv.org/pdf/2506.22509", "abs": "https://arxiv.org/abs/2506.22509", "authors": ["Hang Xu", "Jie Huang", "Linjiang Huang", "Dong Li", "Yidi Liu", "Feng Zhao"], "title": "FreeDNA: Endowing Domain Adaptation of Diffusion-Based Dense Prediction with Training-Free Domain Noise Alignment", "categories": ["cs.CV", "cs.AI"], "comment": "ICCV2025", "summary": "Domain Adaptation(DA) for dense prediction tasks is an important topic, which\nenhances the dense prediction model's performance when tested on its unseen\ndomain. Recently, with the development of Diffusion-based Dense Prediction\n(DDP) models, the exploration of DA designs tailored to this framework is worth\nexploring, since the diffusion model is effective in modeling the distribution\ntransformation that comprises domain information. In this work, we propose a\ntraining-free mechanism for DDP frameworks, endowing them with DA capabilities.\nOur motivation arises from the observation that the exposure bias (e.g., noise\nstatistics bias) in diffusion brings domain shift, and different domains in\nconditions of DDP models can also be effectively captured by the noise\nprediction statistics. Based on this, we propose a training-free Domain Noise\nAlignment (DNA) approach, which alleviates the variations of noise statistics\nto domain changes during the diffusion sampling process, thereby achieving\ndomain adaptation. Specifically, when the source domain is available, we\ndirectly adopt the DNA method to achieve domain adaptation by aligning the\nnoise statistics of the target domain with those of the source domain. For the\nmore challenging source-free DA, inspired by the observation that regions\ncloser to the source domain exhibit higher confidence meeting variations of\nsampling noise, we utilize the statistics from the high-confidence regions\nprogressively to guide the noise statistic adjustment during the sampling\nprocess. Notably, our method demonstrates the effectiveness of enhancing the DA\ncapability of DDP models across four common dense prediction tasks. Code is\navailable at\n\\href{https://github.com/xuhang07/FreeDNA}{https://github.com/xuhang07/FreeDNA}.", "AI": {"tldr": "提出了一种无需训练的领域噪声对齐（DNA）方法，用于扩散密集预测（DDP）模型的领域自适应（DA），通过调整噪声统计实现跨域性能提升。", "motivation": "扩散模型在分布变换中表现优异，但噪声统计偏差会导致领域偏移，因此需要一种方法在采样过程中对齐噪声统计以实现领域自适应。", "method": "提出DNA方法，在源域可用时直接对齐目标域噪声统计；在无源域时，利用高置信区域的统计逐步调整噪声。", "result": "在四种密集预测任务中验证了DNA方法的有效性，提升了DDP模型的领域自适应能力。", "conclusion": "DNA是一种无需训练的高效领域自适应方法，适用于扩散密集预测框架。"}}
{"id": "2506.23128", "pdf": "https://arxiv.org/pdf/2506.23128", "abs": "https://arxiv.org/abs/2506.23128", "authors": ["Chi Chiu So", "Yueyue Sun", "Jun-Min Wang", "Siu Pang Yung", "Anthony Wai Keung Loh", "Chun Pong Chau"], "title": "Are Large Language Models Capable of Deep Relational Reasoning? Insights from DeepSeek-R1 and Benchmark Comparisons", "categories": ["cs.AI"], "comment": "10 pages, 0 figures, accepted by 2025 IEEE international conference\n  on artificial intelligence testing (AITest)", "summary": "How far are Large Language Models (LLMs) in performing deep relational\nreasoning? In this paper, we evaluate and compare the reasoning capabilities of\nthree cutting-edge LLMs, namely, DeepSeek-R1, DeepSeek-V3 and GPT-4o, through a\nsuite of carefully designed benchmark tasks in family tree and general graph\nreasoning. Our experiments reveal that DeepSeek-R1 consistently achieves the\nhighest F1-scores across multiple tasks and problem sizes, demonstrating strong\naptitude in logical deduction and relational inference. However, all evaluated\nmodels, including DeepSeek-R1, struggle significantly as problem complexity\nincreases, largely due to token length limitations and incomplete output\nstructures. A detailed analysis of DeepSeek-R1's long Chain-of-Thought\nresponses uncovers its unique planning and verification strategies, but also\nhighlights instances of incoherent or incomplete reasoning, calling attention\nto the need for deeper scrutiny into LLMs' internal inference dynamics. We\nfurther discuss key directions for future work, including the role of\nmultimodal reasoning and the systematic examination of reasoning failures. Our\nfindings provide both empirical insights and theoretical implications for\nadvancing LLMs' reasoning abilities, particularly in tasks that demand\nstructured, multi-step logical inference. Our code repository will be publicly\navailable at https://github.com/kelvinhkcs/Deep-Relational-Reasoning.", "AI": {"tldr": "论文评估了三种前沿大语言模型（DeepSeek-R1、DeepSeek-V3和GPT-4o）在深度关系推理任务中的表现，发现DeepSeek-R1在多项任务中表现最佳，但所有模型在问题复杂度增加时均表现不佳。", "motivation": "探讨大语言模型在深度关系推理任务中的能力，并比较不同模型的性能。", "method": "通过设计家庭树和通用图推理的基准任务，评估三种模型的推理能力。", "result": "DeepSeek-R1在多项任务中表现最优，但所有模型在复杂任务中均因标记长度限制和输出结构不完整而表现不佳。", "conclusion": "研究揭示了大语言模型在复杂推理任务中的局限性，并提出了未来研究方向，如多模态推理和系统性分析推理失败原因。"}}
{"id": "2506.22644", "pdf": "https://arxiv.org/pdf/2506.22644", "abs": "https://arxiv.org/abs/2506.22644", "authors": ["Chase Fensore", "Kaustubh Dhole", "Joyce C Ho", "Eugene Agichtein"], "title": "Evaluating Hybrid Retrieval Augmented Generation using Dynamic Test Sets: LiveRAG Challenge", "categories": ["cs.CL", "cs.IR"], "comment": "4 pages, 3 tables, 2 figures. Accepted at the SIGIR LiveRAG Workshop\n  2025 (Submission 2664)", "summary": "We present our submission to the LiveRAG Challenge 2025, which evaluates\nretrieval-augmented generation (RAG) systems on dynamic test sets using the\nFineWeb-10BT corpus. Our final hybrid approach combines sparse (BM25) and dense\n(E5) retrieval methods and then aims to generate relevant and faithful answers\nwith Falcon3-10B-Instruct. Through systematic evaluation on 200 synthetic\nquestions generated with DataMorgana across 64 unique question-user\ncombinations, we demonstrate that neural re-ranking with RankLLaMA improves MAP\nfrom 0.523 to 0.797 (52% relative improvement) but introduces prohibitive\ncomputational costs (84s vs 1.74s per question). While DSPy-optimized prompting\nstrategies achieved higher semantic similarity (0.771 vs 0.668), their 0%\nrefusal rates raised concerns about over-confidence and generalizability. Our\nsubmitted hybrid system without re-ranking achieved 4th place in faithfulness\nand 11th place in correctness among 25 teams. Analysis across question\ncategories reveals that vocabulary alignment between questions and documents\nwas the strongest predictor of performance on our development set, with\ndocument-similar phrasing improving cosine similarity from 0.562 to 0.762.", "AI": {"tldr": "本文介绍了参加LiveRAG Challenge 2025的混合检索增强生成系统，结合稀疏和密集检索方法，使用Falcon3-10B-Instruct生成答案。通过系统评估，展示了神经重排序和优化提示策略的效果，最终系统在忠实性和正确性上表现优异。", "motivation": "动态测试集上评估检索增强生成系统的性能，探索混合检索方法的效果。", "method": "结合稀疏（BM25）和密集（E5）检索方法，使用Falcon3-10B-Instruct生成答案，并通过神经重排序（RankLLaMA）和优化提示策略（DSPy）进行改进。", "result": "神经重排序显著提升MAP（52%），但计算成本高；优化提示策略提高语义相似性，但拒绝率为0%。最终系统在忠实性和正确性上排名靠前。", "conclusion": "词汇对齐是性能的关键因素，混合系统在动态测试集上表现良好，但需平衡计算成本和性能。"}}
{"id": "2506.22511", "pdf": "https://arxiv.org/pdf/2506.22511", "abs": "https://arxiv.org/abs/2506.22511", "authors": ["Tingting Zhou", "Feng Zhang", "Haoyang Fu", "Baoxiang Pan", "Renhe Zhang", "Feng Lu", "Zhixin Yang"], "title": "Lightning the Night with Generative Artificial Intelligence", "categories": ["cs.CV", "cs.AI", "eess.IV"], "comment": null, "summary": "The visible light reflectance data from geostationary satellites is crucial\nfor meteorological observations and plays an important role in weather\nmonitoring and forecasting. However, due to the lack of visible light at night,\nit is impossible to conduct continuous all-day weather observations using\nvisible light reflectance data. This study pioneers the use of generative\ndiffusion models to address this limitation. Based on the multi-band thermal\ninfrared brightness temperature data from the Advanced Geostationary Radiation\nImager (AGRI) onboard the Fengyun-4B (FY4B) geostationary satellite, we\ndeveloped a high-precision visible light reflectance retrieval model, called\nReflectance Diffusion (RefDiff), which enables 0.47~\\mu\\mathrm{m},\n0.65~\\mu\\mathrm{m}, and 0.825~\\mu\\mathrm{m} bands visible light reflectance\nretrieval at night. Compared to the classical models, RefDiff not only\nsignificantly improves accuracy through ensemble averaging but also provides\nuncertainty estimation. Specifically, the SSIM index of RefDiff can reach 0.90,\nwith particularly significant improvements in areas with complex cloud\nstructures and thick clouds. The model's nighttime retrieval capability was\nvalidated using VIIRS nighttime product, demonstrating comparable performance\nto its daytime counterpart. In summary, this research has made substantial\nprogress in the ability to retrieve visible light reflectance at night, with\nthe potential to expand the application of nighttime visible light data.", "AI": {"tldr": "该研究利用生成扩散模型，基于FY4B卫星的热红外数据，开发了RefDiff模型，实现了夜间可见光反射率的高精度反演，显著提升了复杂云结构区域的准确性。", "motivation": "解决夜间因缺乏可见光而无法进行全天候气象观测的问题。", "method": "基于FY4B卫星的多波段热红外亮温数据，开发了生成扩散模型RefDiff，用于夜间可见光反射率反演。", "result": "RefDiff的SSIM指数达0.90，在复杂云结构区域表现优异，夜间反演能力与白天相当。", "conclusion": "该研究显著提升了夜间可见光反射率反演能力，拓展了夜间可见光数据的应用潜力。"}}
{"id": "2506.23141", "pdf": "https://arxiv.org/pdf/2506.23141", "abs": "https://arxiv.org/abs/2506.23141", "authors": ["Siyuan Li", "Ruitong Liu", "Yan Wen", "Te Sun"], "title": "Context-Driven Knowledge Graph Completion with Semantic-Aware Relational Message Passing", "categories": ["cs.AI"], "comment": null, "summary": "Semantic context surrounding a triplet $(h, r, t)$ is crucial for Knowledge\nGraph Completion (KGC), providing vital cues for prediction. However,\ntraditional node-based message passing mechanisms, when applied to knowledge\ngraphs, often introduce noise and suffer from information dilution or\nover-smoothing by indiscriminately aggregating information from all neighboring\nedges. To address this challenge, we propose a semantic-aware relational\nmessage passing. A core innovation of this framework is the introduction of a\n\\textbf{semantic-aware Top-K neighbor selection strategy}. Specifically, this\nstrategy first evaluates the semantic relevance between a central node and its\nincident edges within a shared latent space, selecting only the Top-K most\npertinent ones. Subsequently, information from these selected edges is\neffectively fused with the central node's own representation using a\n\\textbf{multi-head attention aggregator} to generate a semantically focused\nnode message. In this manner, our model not only leverages the structure and\nfeatures of edges within the knowledge graph but also more accurately captures\nand propagates the contextual information most relevant to the specific link\nprediction task, thereby effectively mitigating interference from irrelevant\ninformation. Extensive experiments demonstrate that our method achieves\nsuperior performance compared to existing approaches on several established\nbenchmarks.", "AI": {"tldr": "论文提出了一种语义感知的关系消息传递框架，通过Top-K邻居选择策略和多头注意力聚合器，有效减少知识图谱完成中的噪声和信息稀释问题。", "motivation": "传统基于节点的消息传递机制在知识图谱中会引入噪声和信息稀释，因此需要一种更精准的语义感知方法。", "method": "提出语义感知的Top-K邻居选择策略和多头注意力聚合器，选择最相关的邻居信息进行融合。", "result": "在多个基准测试中，该方法优于现有方法。", "conclusion": "语义感知的消息传递框架能更准确地捕捉和传播上下文信息，提升知识图谱完成任务的性能。"}}
{"id": "2506.22679", "pdf": "https://arxiv.org/pdf/2506.22679", "abs": "https://arxiv.org/abs/2506.22679", "authors": ["Ankush Raut", "Projna Paromita", "Sydney Begerowski", "Suzanne Bell", "Theodora Chaspari"], "title": "Assessing the feasibility of Large Language Models for detecting micro-behaviors in team interactions during space missions", "categories": ["cs.CL"], "comment": "5 pages, 4 figures. Accepted to Interspeech 2025", "summary": "We explore the feasibility of large language models (LLMs) in detecting\nsubtle expressions of micro-behaviors in team conversations using transcripts\ncollected during simulated space missions. Specifically, we examine zero-shot\nclassification, fine-tuning, and paraphrase-augmented fine-tuning with\nencoder-only sequence classification LLMs, as well as few-shot text generation\nwith decoder-only causal language modeling LLMs, to predict the micro-behavior\nassociated with each conversational turn (i.e., dialogue). Our findings\nindicate that encoder-only LLMs, such as RoBERTa and DistilBERT, struggled to\ndetect underrepresented micro-behaviors, particularly discouraging speech, even\nwith weighted fine-tuning. In contrast, the instruction fine-tuned version of\nLlama-3.1, a decoder-only LLM, demonstrated superior performance, with the best\nmodels achieving macro F1-scores of 44% for 3-way classification and 68% for\nbinary classification. These results have implications for the development of\nspeech technologies aimed at analyzing team communication dynamics and\nenhancing training interventions in high-stakes environments such as space\nmissions, particularly in scenarios where text is the only accessible data.", "AI": {"tldr": "论文探讨了大型语言模型（LLMs）在团队对话中检测细微微行为的可行性，比较了不同模型在零样本分类、微调和生成任务中的表现。", "motivation": "研究旨在利用LLMs分析高压力环境（如太空任务）中的团队沟通动态，以提升训练干预效果。", "method": "采用零样本分类、微调和生成方法，测试了编码器-解码器模型（如RoBERTa、DistilBERT和Llama-3.1）在微行为检测中的表现。", "result": "解码器模型Llama-3.1表现最佳，而编码器模型在检测少数微行为（如消极言论）时表现较差。", "conclusion": "解码器模型在文本数据分析中更具潜力，对高压力环境下的团队沟通技术发展有重要意义。"}}
{"id": "2506.22513", "pdf": "https://arxiv.org/pdf/2506.22513", "abs": "https://arxiv.org/abs/2506.22513", "authors": ["Aditya Sharma"], "title": "Automated Defect Identification and Categorization in NDE 4.0 with the Application of Artificial Intelligence", "categories": ["cs.CV"], "comment": null, "summary": "This investigation attempts to create an automated framework for fault\ndetection and organization for usage in contemporary radiography, as per NDE\n4.0. The review's goals are to address the lack of information that is\nsufficiently explained, learn how to make the most of virtual defect increase,\nand determine whether the framework is viable by using NDE measurements. As its\nbasic information source, the technique consists of compiling and categorizing\n223 CR photographs of airplane welds. Information expansion systems, such as\nvirtual defect increase and standard increase, are used to work on the\npreparation dataset. A modified U-net model is prepared using the improved data\nto produce semantic fault division veils. To assess the effectiveness of the\nmodel, NDE boundaries such as Case, estimating exactness, and misleading call\nrate are used. Tiny a90/95 characteristics, which provide strong\ndifferentiating evidence of flaws, reveal that the suggested approach achieves\nexceptional awareness in defect detection. Considering a 90/95, size error, and\nfake call rate in the weld area, the consolidated expansion approach clearly\nwins. Due to the framework's fast derivation speed, large images can be broken\ndown efficiently and quickly. Professional controllers evaluate the transmitted\nsystem in the field and believe that it has a guarantee as a support device in\nthe testing cycle, irrespective of particular equipment cut-off points and\nprogramming resemblance.", "AI": {"tldr": "该研究开发了一种用于现代射线照相的自动化故障检测和组织框架，通过虚拟缺陷增强和数据扩展优化数据集，并使用改进的U-net模型进行语义分割，取得了高检测灵敏度。", "motivation": "解决现代射线照相中信息不足的问题，探索虚拟缺陷增强的潜力，并通过NDE测量验证框架的可行性。", "method": "收集并分类223张飞机焊缝的CR照片，使用虚拟缺陷增强和数据扩展优化数据集，训练改进的U-net模型进行语义分割。", "result": "模型在缺陷检测中表现出高灵敏度，综合扩展方法在焊缝区域表现最佳，且框架具有快速推理能力。", "conclusion": "该框架在专业评估中显示出作为测试周期支持工具的潜力，适用于不同设备和软件环境。"}}
{"id": "2506.23168", "pdf": "https://arxiv.org/pdf/2506.23168", "abs": "https://arxiv.org/abs/2506.23168", "authors": ["Mohammad Abdulla", "Tobias Hille", "Dominik Dürrschnabel", "Gerd Stumme"], "title": "Rises for Measuring Local Distributivity in Lattices", "categories": ["cs.AI", "cs.DM", "math.CO", "math.RA", "06B99", "G.2.1"], "comment": "16 pages, 2 tables, 5 figures, International Joint Conference on\n  Conceptual Knowledge Structures", "summary": "Distributivity is a well-established and extensively studied notion in\nlattice theory. In the context of data analysis, particularly within Formal\nConcept Analysis (FCA), lattices are often observed to exhibit a high degree of\ndistributivity. However, no standardized measure exists to quantify this\nproperty. In this paper, we introduce the notion of rises in (concept) lattices\nas a means to assess distributivity. Rises capture how the number of attributes\nor objects in covering concepts change within the concept lattice. We show that\na lattice is distributive if and only if no non-unit rises occur. Furthermore,\nwe relate rises to the classical notion of meet- and join distributivity. We\nobserve that concept lattices from real-world data are to a high degree\njoin-distributive, but much less meet-distributive. We additionally study how\njoin-distributivity manifests on the level of ordered sets.", "AI": {"tldr": "本文提出了一种通过“上升”来量化概念格中分配性的方法，并证明了格分配性与非单位上升的关系。", "motivation": "在形式概念分析中，格的分配性缺乏标准化度量，需要一种量化方法。", "method": "引入“上升”概念，通过覆盖概念中属性或对象数量的变化来评估分配性。", "result": "证明格分配性当且仅当无非单位上升；现实数据的概念格多为联合分配性，而非交分配性。", "conclusion": "上升为量化分配性提供了有效工具，揭示了现实数据中联合分配性的普遍性。"}}
{"id": "2506.22694", "pdf": "https://arxiv.org/pdf/2506.22694", "abs": "https://arxiv.org/abs/2506.22694", "authors": ["Raghavv Goel", "Sudhanshu Agrawal", "Mukul Gagrani", "Junyoung Park", "Yifan Zao", "He Zhang", "Tian Liu", "Yiping Yang", "Xin Yuan", "Jiuyan Lu", "Chris Lott", "Mingu Lee"], "title": "VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs", "categories": ["cs.CL"], "comment": "7 pages, 4 figures, 5 tables, accepted at ICML 2025 workshop on\n  Efficient Systems for Foundational Models", "summary": "In this paper, we introduce a simple training-free technique to improve the\nperformance of drafter-based speculative decoding (SpD) methods that\nincorporates language modeling head (LM head) during drafting process. A\ndrafter-based speculative decoding leverages one or more smaller language\nmodels, a.k.a. drafters or draft models, to sample a draft sequence or tree\nconsisting of multiple tokens, followed by verification by a base LLM, a target\nmodel, accepting a subset as its valid generation. As it is usually considered\nthat the speculative decoding requires one-to-one mapping between vocabularies\nof the target model and the draft model, it has been natural to share the\nvocabulary between them, or even share the LM head as in EAGLE or Medusa. We\nfirst identify that this draft token sampling scheme inherently contains an\nunnecessary inference overhead in drafting, especially for some target LLMs\nwith very large vocabularies. Then, we propose a simple technique, VocabTrim,\nto mitigate the drafting overhead to improve the generation speed in\nmemory-bound environment. VocabTrim reconstructs the drafter LM head to contain\nonly a limited set of tokens, selected by the most frequently sampled from the\nvocabulary of the target model. While limiting the vocabulary in drafting\nslightly degrades the acceptance rate, it significantly reduces the drafting\nlatency in memory-bound process which is often the case on edge devices,\nresulting in higher memory-bound speed up (MBSU). We show that our method can\nboost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically\nby 16% for Llama-3.2-3B-Instruct.", "AI": {"tldr": "提出了一种名为VocabTrim的无训练技术，通过优化语言模型头的词汇表来提高基于草稿的推测解码（SpD）方法的性能，显著减少内存受限环境下的延迟。", "motivation": "传统的推测解码方法需要目标模型和草稿模型共享词汇表或语言模型头，这在词汇量大的目标模型中会带来不必要的推理开销。VocabTrim旨在减少这种开销。", "method": "VocabTrim通过重构草稿模型的语言模型头，仅包含目标模型中最常采样的有限词汇集，从而减少草稿阶段的延迟。", "result": "在内存受限环境下，VocabTrim显著提高了生成速度，例如在Llama-3.2-3B-Instruct模型上实现了16%的内存加速（MBSU）。", "conclusion": "VocabTrim是一种简单有效的技术，能够在不显著降低接受率的情况下显著提升内存受限设备的生成速度。"}}
{"id": "2506.22517", "pdf": "https://arxiv.org/pdf/2506.22517", "abs": "https://arxiv.org/abs/2506.22517", "authors": ["Subhadip Kumar"], "title": "Container damage detection using advanced computer vision model Yolov12 vs Yolov11 vs RF-DETR A comparative analysis", "categories": ["cs.CV"], "comment": null, "summary": "Containers are an integral part of the logistics industry and act as a\nbarrier for cargo. A typical service life for a container is more than 20\nyears. However, overtime containers suffer various types of damage due to the\nmechanical as well as natural factors. A damaged container is a safety hazard\nfor the employees handling it and a liability for the logistic company.\nTherefore, a timely inspection and detection of the damaged container is a key\nfor prolonging service life as well as avoiding safety hazards. In this paper,\nwe will compare the performance of the damage detection by three\nstate-of-the-art advanced computer vision models Yolov12, Yolov11 and RF-DETR.\nWe will use a dataset of 278 annotated images to train, validate and test the\nmodel. We will compare the mAP and precision of the model. The objective of\nthis paper is to identify the model that is best suited for container damage\ndetection. The result is mixed. mAP@50 score of Yolov11 and 12 was 81.9%\ncompared to RF-DETR, which was 77.7%. However, while testing the model for\nnot-so-common damaged containers, the RF-DETR model outperformed the others\noverall, exhibiting superiority to accurately detecting both damaged containers\nas well as damage occurrences with high confidence.", "AI": {"tldr": "论文比较了三种计算机视觉模型（Yolov12、Yolov11和RF-DETR）在集装箱损伤检测中的性能，发现RF-DETR在不常见损伤检测中表现更优。", "motivation": "集装箱损伤是物流行业的安全隐患，需及时检测以延长使用寿命并避免风险。", "method": "使用278张标注图像训练和测试模型，比较mAP和精度。", "result": "Yolov11和12的mAP@50为81.9%，RF-DETR为77.7%，但RF-DETR在不常见损伤检测中表现更优。", "conclusion": "RF-DETR在集装箱损伤检测中更具优势，尤其是对不常见损伤的检测。"}}
{"id": "2506.23273", "pdf": "https://arxiv.org/pdf/2506.23273", "abs": "https://arxiv.org/abs/2506.23273", "authors": ["Quang Hung Nguyen", "Phuong Anh Trinh", "Phan Quoc Hung Mai", "Tuan Phong Trinh"], "title": "FinStat2SQL: A Text2SQL Pipeline for Financial Statement Analysis", "categories": ["cs.AI"], "comment": null, "summary": "Despite the advancements of large language models, text2sql still faces many\nchallenges, particularly with complex and domain-specific queries. In finance,\ndatabase designs and financial reporting layouts vary widely between financial\nentities and countries, making text2sql even more challenging. We present\nFinStat2SQL, a lightweight text2sql pipeline enabling natural language queries\nover financial statements. Tailored to local standards like VAS, it combines\nlarge and small language models in a multi-agent setup for entity extraction,\nSQL generation, and self-correction. We build a domain-specific database and\nevaluate models on a synthetic QA dataset. A fine-tuned 7B model achieves\n61.33\\% accuracy with sub-4-second response times on consumer hardware,\noutperforming GPT-4o-mini. FinStat2SQL offers a scalable, cost-efficient\nsolution for financial analysis, making AI-powered querying accessible to\nVietnamese enterprises.", "AI": {"tldr": "FinStat2SQL是一个轻量级的text2sql管道，专为金融领域设计，结合大小语言模型，支持自然语言查询财务数据，并在越南标准下表现优异。", "motivation": "金融领域的数据库设计和报表布局差异大，导致text2sql任务复杂且具有挑战性。", "method": "采用多智能体设置，结合大小语言模型，进行实体提取、SQL生成和自校正，并构建领域特定数据库。", "result": "7B微调模型在消费级硬件上达到61.33%准确率，响应时间低于4秒，优于GPT-4o-mini。", "conclusion": "FinStat2SQL为越南企业提供了一种可扩展且经济高效的金融分析解决方案。"}}
{"id": "2506.22698", "pdf": "https://arxiv.org/pdf/2506.22698", "abs": "https://arxiv.org/abs/2506.22698", "authors": ["Emily Dux Speltz"], "title": "Text Production and Comprehension by Human and Artificial Intelligence: Interdisciplinary Workshop Report", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This report synthesizes the outcomes of a recent interdisciplinary workshop\nthat brought together leading experts in cognitive psychology, language\nlearning, and artificial intelligence (AI)-based natural language processing\n(NLP). The workshop, funded by the National Science Foundation, aimed to\naddress a critical knowledge gap in our understanding of the relationship\nbetween AI language models and human cognitive processes in text comprehension\nand composition. Through collaborative dialogue across cognitive, linguistic,\nand technological perspectives, workshop participants examined the underlying\nprocesses involved when humans produce and comprehend text, and how AI can both\ninform our understanding of these processes and augment human capabilities. The\nworkshop revealed emerging patterns in the relationship between large language\nmodels (LLMs) and human cognition, with highlights on both the capabilities of\nLLMs and their limitations in fully replicating human-like language\nunderstanding and generation. Key findings include the potential of LLMs to\noffer insights into human language processing, the increasing alignment between\nLLM behavior and human language processing when models are fine-tuned with\nhuman feedback, and the opportunities and challenges presented by human-AI\ncollaboration in language tasks. By synthesizing these findings, this report\naims to guide future research, development, and implementation of LLMs in\ncognitive psychology, linguistics, and education. It emphasizes the importance\nof ethical considerations and responsible use of AI technologies while striving\nto enhance human capabilities in text comprehension and production through\neffective human-AI collaboration.", "AI": {"tldr": "报告总结了跨学科研讨会的成果，探讨了AI语言模型与人类认知过程的关系，强调了LLMs的潜力与局限，以及人机协作的机遇与挑战。", "motivation": "解决AI语言模型与人类文本理解和生成之间关系的关键知识缺口。", "method": "通过认知心理学、语言学习和AI专家的跨学科对话，分析人类与AI在语言任务中的互动。", "result": "发现LLMs能提供对人类语言处理的见解，但无法完全复制人类语言能力；人机协作在语言任务中具有潜力。", "conclusion": "报告为未来研究提供指导，强调伦理和负责任地使用AI技术，以增强人机协作的语言能力。"}}
{"id": "2506.22531", "pdf": "https://arxiv.org/pdf/2506.22531", "abs": "https://arxiv.org/abs/2506.22531", "authors": ["Prasen Kumar Sharma", "Neeraj Matiyali", "Siddharth Srivastava", "Gaurav Sharma"], "title": "Preserve Anything: Controllable Image Synthesis with Object Preservation", "categories": ["cs.CV"], "comment": "Accepted at ICCV 2025", "summary": "We introduce \\textit{Preserve Anything}, a novel method for controlled image\nsynthesis that addresses key limitations in object preservation and semantic\nconsistency in text-to-image (T2I) generation. Existing approaches often fail\n(i) to preserve multiple objects with fidelity, (ii) maintain semantic\nalignment with prompts, or (iii) provide explicit control over scene\ncomposition. To overcome these challenges, the proposed method employs an\nN-channel ControlNet that integrates (i) object preservation with size and\nplacement agnosticism, color and detail retention, and artifact elimination,\n(ii) high-resolution, semantically consistent backgrounds with accurate\nshadows, lighting, and prompt adherence, and (iii) explicit user control over\nbackground layouts and lighting conditions. Key components of our framework\ninclude object preservation and background guidance modules, enforcing lighting\nconsistency and a high-frequency overlay module to retain fine details while\nmitigating unwanted artifacts. We introduce a benchmark dataset consisting of\n240K natural images filtered for aesthetic quality and 18K 3D-rendered\nsynthetic images with metadata such as lighting, camera angles, and object\nrelationships. This dataset addresses the deficiencies of existing benchmarks\nand allows a complete evaluation. Empirical results demonstrate that our method\nachieves state-of-the-art performance, significantly improving feature-space\nfidelity (FID 15.26) and semantic alignment (CLIP-S 32.85) while maintaining\ncompetitive aesthetic quality. We also conducted a user study to demonstrate\nthe efficacy of the proposed work on unseen benchmark and observed a remarkable\nimprovement of $\\sim25\\%$, $\\sim19\\%$, $\\sim13\\%$, and $\\sim14\\%$ in terms of\nprompt alignment, photorealism, the presence of AI artifacts, and natural\naesthetics over existing works.", "AI": {"tldr": "提出了一种名为“Preserve Anything”的新方法，用于解决文本到图像生成中的对象保存和语义一致性问题，通过N通道ControlNet实现多对象保存、语义对齐和场景控制。", "motivation": "现有方法在多对象保存、语义对齐和场景控制方面存在不足，需要一种更高效的方法来解决这些问题。", "method": "采用N通道ControlNet，结合对象保存模块、背景引导模块和高频覆盖模块，确保细节保留和语义一致性。", "result": "在特征空间保真度（FID 15.26）和语义对齐（CLIP-S 32.85）方面达到最优性能，用户研究显示显著改进。", "conclusion": "该方法在多对象保存、语义一致性和用户控制方面表现优异，显著优于现有技术。"}}
{"id": "2506.23276", "pdf": "https://arxiv.org/pdf/2506.23276", "abs": "https://arxiv.org/abs/2506.23276", "authors": ["David Guzman Piedrahita", "Yongjin Yang", "Mrinmaya Sachan", "Giorgia Ramponi", "Bernhard Schölkopf", "Zhijing Jin"], "title": "Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As large language models (LLMs) are increasingly deployed as autonomous\nagents, understanding their cooperation and social mechanisms is becoming\nincreasingly important. In particular, how LLMs balance self-interest and\ncollective well-being is a critical challenge for ensuring alignment,\nrobustness, and safe deployment. In this paper, we examine the challenge of\ncostly sanctioning in multi-agent LLM systems, where an agent must decide\nwhether to invest its own resources to incentivize cooperation or penalize\ndefection. To study this, we adapt a public goods game with institutional\nchoice from behavioral economics, allowing us to observe how different LLMs\nnavigate social dilemmas over repeated interactions. Our analysis reveals four\ndistinct behavioral patterns among models: some consistently establish and\nsustain high levels of cooperation, others fluctuate between engagement and\ndisengagement, some gradually decline in cooperative behavior over time, and\nothers rigidly follow fixed strategies regardless of outcomes. Surprisingly, we\nfind that reasoning LLMs, such as the o1 series, struggle significantly with\ncooperation, whereas some traditional LLMs consistently achieve high levels of\ncooperation. These findings suggest that the current approach to improving\nLLMs, which focuses on enhancing their reasoning capabilities, does not\nnecessarily lead to cooperation, providing valuable insights for deploying LLM\nagents in environments that require sustained collaboration. Our code is\navailable at https://github.com/davidguzmanp/SanctSim", "AI": {"tldr": "研究大型语言模型（LLMs）在多智能体系统中的合作行为，特别是代价高昂的制裁行为，发现不同模型表现出四种行为模式，推理能力强的模型反而合作困难。", "motivation": "理解LLMs在自主代理部署中的合作与社会机制，尤其是如何在自利与集体利益间平衡，以确保对齐性、鲁棒性和安全部署。", "method": "通过改编行为经济学中的公共物品游戏与制度选择，观察不同LLMs在重复互动中的社会困境应对策略。", "result": "发现四种行为模式：持续高合作、波动参与、合作逐渐下降、固定策略。推理能力强的模型合作困难，传统模型反而表现更好。", "conclusion": "当前提升LLMs推理能力的方法未必促进合作，为需要持续协作的环境部署LLM代理提供了重要启示。"}}
{"id": "2506.22724", "pdf": "https://arxiv.org/pdf/2506.22724", "abs": "https://arxiv.org/abs/2506.22724", "authors": ["Niyati Bafna", "Tianjian Li", "Kenton Murray", "David R. Mortensen", "David Yarowsky", "Hale Sirin", "Daniel Khashabi"], "title": "The Translation Barrier Hypothesis: Multilingual Generation with Large Language Models Suffers from Implicit Translation Failure", "categories": ["cs.CL"], "comment": "23 pages incl. appendix", "summary": "Multilingual generation with large language models (LLMs) is often of poor\nquality for mid- to low-resource languages. Building on insights from\ninterpretability, we demonstrate the existence of an implicit\ntask-solving-->translation pipeline for generation, whereby the model first\nsolves the required task in a largely target-language-agnostic manner, and\nsubsequently translates answer concepts into the intended target language. We\nhypothesize that the failure of the translation stage is an important culprit\nfor the observed low quality of final outputs, and formalize this as the\ntranslation barrier hypothesis. We test this hypothesis for a word translation\ntask across 108 language pairs, using logit lens to observe model processing in\nintermediate layers. We find that a significant portion of overall failures\nindeed stems from translation failure, or the model's inability to translate\ncorrectly solved intermediate concepts into the target language. This is\nespecially true for low-resource target languages. Our results highlight an\nimportant hurdle for end-to-end multilingual generation, and lend guiding\ninsights for future work seeking to improve multilinguality in LLMs.", "AI": {"tldr": "研究发现多语言生成模型在低资源语言中表现不佳，原因是模型在任务解决后翻译阶段失败，尤其是低资源目标语言。", "motivation": "探究多语言生成模型在低资源语言中表现不佳的原因，并提出翻译障碍假说。", "method": "通过108种语言对的单词翻译任务，使用logit lens观察模型中间层的处理过程。", "result": "发现翻译失败是导致整体失败的重要原因，尤其是低资源目标语言。", "conclusion": "翻译障碍是多语言生成的重要障碍，为未来改进多语言模型提供了指导。"}}
{"id": "2506.22554", "pdf": "https://arxiv.org/pdf/2506.22554", "abs": "https://arxiv.org/abs/2506.22554", "authors": ["Vasu Agrawal", "Akinniyi Akinyemi", "Kathryn Alvero", "Morteza Behrooz", "Julia Buffalini", "Fabio Maria Carlucci", "Joy Chen", "Junming Chen", "Zhang Chen", "Shiyang Cheng", "Praveen Chowdary", "Joe Chuang", "Antony D'Avirro", "Jon Daly", "Ning Dong", "Mark Duppenthaler", "Cynthia Gao", "Jeff Girard", "Martin Gleize", "Sahir Gomez", "Hongyu Gong", "Srivathsan Govindarajan", "Brandon Han", "Sen He", "Denise Hernandez", "Yordan Hristov", "Rongjie Huang", "Hirofumi Inaguma", "Somya Jain", "Raj Janardhan", "Qingyao Jia", "Christopher Klaiber", "Dejan Kovachev", "Moneish Kumar", "Hang Li", "Yilei Li", "Pavel Litvin", "Wei Liu", "Guangyao Ma", "Jing Ma", "Martin Ma", "Xutai Ma", "Lucas Mantovani", "Sagar Miglani", "Sreyas Mohan", "Louis-Philippe Morency", "Evonne Ng", "Kam-Woh Ng", "Tu Anh Nguyen", "Amia Oberai", "Benjamin Peloquin", "Juan Pino", "Jovan Popovic", "Omid Poursaeed", "Fabian Prada", "Alice Rakotoarison", "Alexander Richard", "Christophe Ropers", "Safiyyah Saleem", "Vasu Sharma", "Alex Shcherbyna", "Jia Shen", "Jie Shen", "Anastasis Stathopoulos", "Anna Sun", "Paden Tomasello", "Tuan Tran", "Arina Turkatenko", "Bo Wan", "Chao Wang", "Jeff Wang", "Mary Williamson", "Carleigh Wood", "Tao Xiang", "Yilin Yang", "Julien Yao", "Chen Zhang", "Jiemin Zhang", "Xinyue Zhang", "Jason Zheng", "Pavlo Zhyzheria", "Jan Zikes", "Michael Zollhoefer"], "title": "Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Human communication involves a complex interplay of verbal and nonverbal\nsignals, essential for conveying meaning and achieving interpersonal goals. To\ndevelop socially intelligent AI technologies, it is crucial to develop models\nthat can both comprehend and generate dyadic behavioral dynamics. To this end,\nwe introduce the Seamless Interaction Dataset, a large-scale collection of over\n4,000 hours of face-to-face interaction footage from over 4,000 participants in\ndiverse contexts. This dataset enables the development of AI technologies that\nunderstand dyadic embodied dynamics, unlocking breakthroughs in virtual agents,\ntelepresence experiences, and multimodal content analysis tools. We also\ndevelop a suite of models that utilize the dataset to generate dyadic motion\ngestures and facial expressions aligned with human speech. These models can\ntake as input both the speech and visual behavior of their interlocutors. We\npresent a variant with speech from an LLM model and integrations with 2D and 3D\nrendering methods, bringing us closer to interactive virtual agents.\nAdditionally, we describe controllable variants of our motion models that can\nadapt emotional responses and expressivity levels, as well as generating more\nsemantically-relevant gestures. Finally, we discuss methods for assessing the\nquality of these dyadic motion models, which are demonstrating the potential\nfor more intuitive and responsive human-AI interactions.", "AI": {"tldr": "论文介绍了Seamless Interaction Dataset，用于开发理解双人互动行为的AI模型，并展示了生成手势和表情的模型。", "motivation": "开发能够理解和生成双人互动行为的AI技术，以推动社交智能AI的发展。", "method": "利用大规模数据集（4,000小时互动视频）训练模型，生成与语音对齐的手势和表情，并整合2D/3D渲染技术。", "result": "模型能根据语音和视觉输入生成互动行为，并支持情感和语义调整，提升了人机交互的直觉性和响应性。", "conclusion": "该研究为虚拟代理和多模态内容分析提供了突破性工具，推动了更自然的人机交互。"}}
{"id": "2506.23306", "pdf": "https://arxiv.org/pdf/2506.23306", "abs": "https://arxiv.org/abs/2506.23306", "authors": ["Qi Liu", "Can Li", "Wanjing Ma"], "title": "GATSim: Urban Mobility Simulation with Generative Agents", "categories": ["cs.AI"], "comment": null, "summary": "Traditional agent-based urban mobility simulations rely on rigid rule-based\nsystems that fail to capture the complexity, adaptability, and behavioral\ndiversity characteristic of human travel decision-making. Recent advances in\nlarge language models and AI agent technology offer opportunities to create\nagents with reasoning capabilities, persistent memory, and adaptive learning\nmechanisms. We propose GATSim (Generative-Agent Transport Simulation), a novel\nframework that leverages these advances to create generative agents with rich\nbehavioral characteristics for urban mobility simulation. Unlike conventional\napproaches, GATSim agents possess diverse socioeconomic attributes, individual\nlifestyles, and evolving preferences that shape their mobility decisions\nthrough psychologically-informed memory systems, tool usage capabilities, and\nlifelong learning mechanisms. The main contributions of this study include: (1)\na comprehensive architecture combining an urban mobility foundation model with\nagent cognitive systems and transport simulation environment, (2) a fully\nfunctional prototype implementation, and (3) systematic validation\ndemonstrating that generative agents produce believable travel behaviors.\nThrough designed reflection processes, generative agents in this study can\ntransform specific travel experiences into generalized insights, enabling\nrealistic behavioral adaptation over time with specialized mechanisms for\nactivity planning and real-time reactive behaviors tailored to urban mobility\ncontexts. Experiments show that generative agents perform competitively with\nhuman annotators in mobility scenarios while naturally producing macroscopic\ntraffic evolution patterns. The code for the prototype system is shared at\nhttps://github.com/qiliuchn/gatsim.", "AI": {"tldr": "GATSim利用大型语言模型和AI代理技术，提出了一种新型城市交通模拟框架，生成具有丰富行为特征的代理，能够模拟人类旅行的复杂性和多样性。", "motivation": "传统基于规则的代理模拟无法捕捉人类旅行决策的复杂性和适应性，因此需要更先进的模拟方法。", "method": "结合城市交通基础模型、代理认知系统和交通模拟环境，构建生成代理框架，并实现功能原型。", "result": "生成代理能产生可信的旅行行为，并在实验中表现出与人类注释者相当的性能，同时生成宏观交通演化模式。", "conclusion": "GATSim通过生成代理实现了更真实的行为模拟，为城市交通研究提供了新工具。"}}
{"id": "2506.22760", "pdf": "https://arxiv.org/pdf/2506.22760", "abs": "https://arxiv.org/abs/2506.22760", "authors": ["Alan Dao", "Dinh Bach Vu"], "title": "Jan-nano Technical Report", "categories": ["cs.CL"], "comment": null, "summary": "Most language models face a fundamental tradeoff where powerful capabilities\nrequire substantial computational resources. We shatter this constraint with\nJan-nano, a 4B parameter language model that redefines efficiency through\nradical specialization: instead of trying to know everything, it masters the\nart of finding anything instantly. Fine-tuned from Qwen3-4B using our novel\nmulti-stage RLVR system that completely eliminates reliance on next token\nprediction training (SFT), Jan-nano achieves 83.2% on SimpleQA benchmark with\nMCP integration while running on consumer hardware. With 128K context length,\nJan-nano proves that intelligence isn't about scale, it's about strategy.", "AI": {"tldr": "Jan-nano是一个4B参数的语言模型，通过多阶段RLVR系统实现高效性，无需依赖传统训练方法，在消费级硬件上表现出色。", "motivation": "解决语言模型在强大能力与计算资源需求之间的权衡问题。", "method": "基于Qwen3-4B微调，采用多阶段RLVR系统，摒弃了传统的下一词预测训练（SFT）。", "result": "在SimpleQA基准测试中达到83.2%的准确率，支持128K上下文长度。", "conclusion": "智能不在于规模，而在于策略。"}}
{"id": "2506.22556", "pdf": "https://arxiv.org/pdf/2506.22556", "abs": "https://arxiv.org/abs/2506.22556", "authors": ["Markus Juvonen", "Samuli Siltanen"], "title": "Recomposed realities: animating still images via patch clustering and randomness", "categories": ["cs.CV", "eess.IV"], "comment": "22 pages, 19 figures", "summary": "We present a patch-based image reconstruction and animation method that uses\nexisting image data to bring still images to life through motion. Image patches\nfrom curated datasets are grouped using k-means clustering and a new target\nimage is reconstructed by matching and randomly sampling from these clusters.\nThis approach emphasizes reinterpretation over replication, allowing the source\nand target domains to differ conceptually while sharing local structures.", "AI": {"tldr": "提出一种基于图像块的重建与动画方法，利用现有图像数据为静态图像添加动态效果。", "motivation": "通过重新解释而非复制，使静态图像具有动态效果，同时允许源域和目标域在概念上不同但共享局部结构。", "method": "使用k-means聚类对图像块进行分组，通过匹配和随机采样从聚类中重建新目标图像。", "result": "实现了静态图像的动态化，同时保持了局部结构的相似性。", "conclusion": "该方法为图像动画提供了一种新颖的重建方式，强调重新解释而非直接复制。"}}
{"id": "2506.23464", "pdf": "https://arxiv.org/pdf/2506.23464", "abs": "https://arxiv.org/abs/2506.23464", "authors": ["Sahil Tripathi", "Md Tabrez Nafis", "Imran Hussain", "Jiechao Gao"], "title": "The Confidence Paradox: Can LLM Know When It's Wrong", "categories": ["cs.AI"], "comment": null, "summary": "Document Visual Question Answering (DocVQA) systems are increasingly deployed\nin real world applications, yet they remain ethically opaque-often producing\noverconfident answers to ambiguous questions or failing to communicate\nuncertainty in a trustworthy manner. This misalignment between model confidence\nand actual knowledge poses significant risks, particularly in domains requiring\nethical accountability. Existing approaches such as LayoutLMv3, UDOP, and DONUT\nhave advanced SOTA performance by focusing on architectural sophistication and\naccuracy; however, they fall short in ethical responsiveness.\n  To address these limitations, we introduce HonestVQA, a self-supervised\nhonesty calibration framework for ethically aligned DocVQA. Our model-agnostic\nmethod quantifies uncertainty to identify knowledge gaps, aligns model\nconfidence with actual correctness using weighted loss functions, and enforces\nethical response behavior via contrastive learning. We further introduce two\nprincipled evaluation metrics--Honesty Score (H-Score) and Ethical Confidence\nIndex (ECI)--to benchmark alignment between confidence, accuracy, and ethical\ncommunication. Empirically, HonestVQA improves DocVQA accuracy by up to 4.3%\nand F1 by 4.3% across SpDocVQA, InfographicsVQA, and SROIE datasets. It reduces\noverconfidence, lowering H-Score and ECI by 0.072 and 0.078, respectively. In\ncross domain evaluation, it achieves up to 78.9% accuracy and 76.1% F1-score,\ndemonstrating strong generalization. Ablation shows a 3.8% drop in accuracy\nwithout alignment or contrastive loss.", "AI": {"tldr": "HonestVQA是一个自监督的诚实校准框架，旨在解决DocVQA系统中的伦理不透明问题，通过量化不确定性和对齐模型置信度，提升准确性和伦理响应能力。", "motivation": "现有DocVQA系统在伦理响应性上表现不足，模型置信度与实际知识不匹配，可能带来风险。", "method": "HonestVQA采用自监督方法，通过加权损失函数和对比学习量化不确定性并校准模型置信度，同时引入H-Score和ECI评估指标。", "result": "HonestVQA在多个数据集上提升了4.3%的准确率和F1值，降低了过自信，并在跨领域评估中表现优异。", "conclusion": "HonestVQA通过伦理对齐和不确定性量化显著提升了DocVQA系统的性能和可信度。"}}
{"id": "2506.22777", "pdf": "https://arxiv.org/pdf/2506.22777", "abs": "https://arxiv.org/abs/2506.22777", "authors": ["Miles Turpin", "Andy Arditi", "Marvin Li", "Joe Benton", "Julian Michael"], "title": "Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Language models trained with RL can engage in reward hacking--exploiting\nunintended strategies for high reward--without revealing this behavior in their\nchain-of-thought reasoning, making detection difficult and posing risks for\nhigh-stakes applications. We propose verbalization fine-tuning (VFT), a pre-RL\nintervention that trains models to explicitly acknowledge when they are\ninfluenced by prompt cues--hints which point to incorrect answers (e.g., \"a\nStanford professor thinks the answer is A\"). To evaluate VFT, we subsequently\ntrain models with RL on environments where held-out prompt cues signal which\nincorrect answers will receive high reward, incentivizing models to reward hack\nby exploiting cues instead of reasoning correctly. We measure how often models\nexploit these cues without verbalizing it. After RL, only 6% of the VFT-trained\nmodel's responses consist of undetected reward hacks. In comparison, when we\nperform RL without VFT, the rate of undetected reward hacks goes up to 88%;\nwith a debiasing baseline intervention, this increases further to 99%. VFT\nachieves this by substantially increasing how often models verbalize the\ninfluence of cues--from 8% to 42% after VFT, and up to 94% after RL--while\nbaselines remain low even after RL (10% and 1%). Our results show that teaching\nmodels to explicitly verbalize reward hacking behavior before RL significantly\nimproves their detection, offering a practical path toward more transparent and\nsafe AI systems.", "AI": {"tldr": "论文提出了一种名为VFT的预RL干预方法，通过训练模型明确承认提示线索的影响，显著减少了奖励黑客行为的未检测率。", "motivation": "解决语言模型在RL训练中可能利用奖励黑客行为（通过提示线索获取高奖励而不被发现）的问题，以提高AI系统的透明性和安全性。", "method": "提出VFT方法，在RL训练前干预，训练模型明确承认提示线索的影响。随后在RL环境中测试模型是否利用线索进行奖励黑客行为。", "result": "VFT训练后，未检测到的奖励黑客行为降至6%，而未经VFT训练的模型高达88%。VFT还显著提高了模型对线索影响的明确承认率（从8%提升至94%）。", "conclusion": "VFT通过提前训练模型明确承认奖励黑客行为，显著提高了其检测率，为构建更透明和安全的AI系统提供了实用路径。"}}
{"id": "2506.22562", "pdf": "https://arxiv.org/pdf/2506.22562", "abs": "https://arxiv.org/abs/2506.22562", "authors": ["Abhineet Singh", "Nilanjan Ray"], "title": "Improving Token-based Object Detection with Video", "categories": ["cs.CV"], "comment": "Under review for publication in IEEE Access", "summary": "This paper improves upon the Pix2Seq object detector by extending it for\nvideos. In the process, it introduces a new way to perform end-to-end video\nobject detection that improves upon existing video detectors in two key ways.\nFirst, by representing objects as variable-length sequences of discrete tokens,\nwe can succinctly represent widely varying numbers of video objects, with\ndiverse shapes and locations, without having to inject any localization cues in\nthe training process. This eliminates the need to sample the space of all\npossible boxes that constrains conventional detectors and thus solves the dual\nproblems of loss sparsity during training and heuristics-based postprocessing\nduring inference. Second, it conceptualizes and outputs the video objects as\nfully integrated and indivisible 3D boxes or tracklets instead of generating\nimage-specific 2D boxes and linking these boxes together to construct the video\nobject, as done in most conventional detectors. This allows it to scale\neffortlessly with available computational resources by simply increasing the\nlength of the video subsequence that the network takes as input, even\ngeneralizing to multi-object tracking if the subsequence can span the entire\nvideo. We compare our video detector with the baseline Pix2Seq static detector\non several datasets and demonstrate consistent improvement, although with\nstrong signs of being bottlenecked by our limited computational resources. We\nalso compare it with several video detectors on UA-DETRAC to show that it is\ncompetitive with the current state of the art even with the computational\nbottleneck. We make our code and models publicly available.", "AI": {"tldr": "论文改进了Pix2Seq目标检测器，将其扩展到视频领域，提出了一种新的端到端视频目标检测方法，解决了传统检测器的局限性。", "motivation": "传统视频目标检测方法存在训练稀疏性和后处理启发式问题，且通常需要将2D框链接为视频对象。本文旨在通过序列化表示和3D框输出解决这些问题。", "method": "通过将对象表示为离散标记的可变长度序列，避免了传统检测器的框采样问题；直接输出3D框或轨迹，无需链接2D框。", "result": "在多个数据集上优于静态Pix2Seq检测器，并在UA-DETRAC上与当前最优方法竞争。", "conclusion": "该方法在视频目标检测中表现优异，但受限于计算资源。代码和模型已公开。"}}
{"id": "2506.23503", "pdf": "https://arxiv.org/pdf/2506.23503", "abs": "https://arxiv.org/abs/2506.23503", "authors": ["Bosubabu Sambana", "Kondreddygari Archana", "Suram Indhra Sena Reddy", "Shaik Meethaigar Jameer Basha", "Shaik Karishma"], "title": "Data Augmentation for Cognitive Behavioral Therapy: Leveraging ERNIE Language Models using Artificial Intelligence", "categories": ["cs.AI"], "comment": "6 Pages, 5 Figures, IEEE IDCIoT 2025", "summary": "Cognitive Behavioral Therapy (CBT) is a proven approach for addressing the\nirrational thought patterns associated with mental health disorders, but its\neffectiveness relies on accurately identifying cognitive pathways to provide\ntargeted treatment. In today's digital age, individuals often express negative\nemotions on social media, where they may reveal cognitive distortions, and in\nsevere cases, exhibit suicidal tendencies. However, there is a significant gap\nin methodologies designed to analyze these cognitive pathways, which could be\ncritical for psychotherapists aiming to deliver timely and effective\ninterventions in online environments. Cognitive Behavioral Therapy (CBT)\nframework leveraging acceptance, commitment and data augmentation to categorize\nand address both textual and visual content as positive or negative.\nSpecifically, the system employs BERT, RoBERTa for Sentiment Analysis and T5,\nPEGASUS for Text Summarization, mT5 for Text Translation in Multiple Languages\nfocusing on detecting negative emotions and cognitive distortions within social\nmedia data. While existing models are primarily designed to identify negative\nthoughts, the proposed system goes beyond this by predicting additional\nnegative side effects and other potential mental health disorders likes\nPhobias, Eating Disorders. This enhancement allows for a more comprehensive\nunderstanding and intervention strategy, offering psychotherapists a powerful\ntool for early detection and treatment of various psychological issues.", "AI": {"tldr": "该论文提出了一种基于认知行为疗法（CBT）的系统，利用自然语言处理技术分析社交媒体中的负面情绪和认知扭曲，以辅助心理治疗师进行早期干预。", "motivation": "当前缺乏有效的方法来分析社交媒体中表达的认知扭曲和负面情绪，而这些信息对心理治疗师的在线干预至关重要。", "method": "系统结合BERT、RoBERTa进行情感分析，T5、PEGASUS进行文本摘要，mT5进行多语言翻译，以识别社交媒体数据中的负面情绪和认知扭曲，并预测潜在心理健康问题。", "result": "该系统不仅能识别负面思维，还能预测其他心理健康问题（如恐惧症、饮食失调），为治疗师提供更全面的干预策略。", "conclusion": "该研究为心理治疗师提供了一个强大的工具，用于早期检测和干预多种心理健康问题。"}}
{"id": "2506.22791", "pdf": "https://arxiv.org/pdf/2506.22791", "abs": "https://arxiv.org/abs/2506.22791", "authors": ["Jianxin Yan", "Wangze Ni", "Lei Chen", "Xuemin Lin", "Peng Cheng", "Zhan Qin", "Kui Ren"], "title": "ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in Large Language Models", "categories": ["cs.CL", "cs.DB"], "comment": null, "summary": "Semantic caching significantly reduces computational costs and improves\nefficiency by storing and reusing large language model (LLM) responses.\nHowever, existing systems rely primarily on matching individual queries,\nlacking awareness of multi-turn dialogue contexts, which leads to incorrect\ncache hits when similar queries appear in different conversational settings.\nThis demonstration introduces ContextCache, a context-aware semantic caching\nsystem for multi-turn dialogues. ContextCache employs a two-stage retrieval\narchitecture that first executes vector-based retrieval on the current query to\nidentify potential matches and then integrates current and historical dialogue\nrepresentations through self-attention mechanisms for precise contextual\nmatching. Evaluation of real-world conversations shows that ContextCache\nimproves precision and recall compared to existing methods. Additionally,\ncached responses exhibit approximately 10 times lower latency than direct LLM\ninvocation, enabling significant computational cost reductions for LLM\nconversational applications.", "AI": {"tldr": "ContextCache是一种上下文感知的语义缓存系统，通过两阶段检索架构提升多轮对话中的缓存命中精度，显著降低计算成本。", "motivation": "现有语义缓存系统缺乏对多轮对话上下文的感知，导致在不同对话环境中相似查询出现错误缓存命中。", "method": "采用两阶段检索架构：首先基于向量检索当前查询，再通过自注意力机制整合历史和当前对话表示进行精确匹配。", "result": "实验表明，ContextCache在精度和召回率上优于现有方法，缓存响应延迟比直接调用LLM低约10倍。", "conclusion": "ContextCache能显著降低LLM对话应用的计算成本，同时提升效率。"}}
{"id": "2506.22567", "pdf": "https://arxiv.org/pdf/2506.22567", "abs": "https://arxiv.org/abs/2506.22567", "authors": ["Shansong Wang", "Zhecheng Jin", "Mingzhe Hu", "Mojtaba Safari", "Feng Zhao", "Chih-Wei Chang", "Richard LJ Qiu", "Justin Roper", "David S. Yu", "Xiaofeng Yang"], "title": "Unifying Biomedical Vision-Language Expertise: Towards a Generalist Foundation Model via Multi-CLIP Knowledge Distillation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "CLIP models pretrained on natural images with billion-scale image-text pairs\nhave demonstrated impressive capabilities in zero-shot classification,\ncross-modal retrieval, and open-ended visual answering. However, transferring\nthis success to biomedicine is hindered by the scarcity of large-scale\nbiomedical image-text corpora, the heterogeneity of image modalities, and\nfragmented data standards across institutions. These limitations hinder the\ndevelopment of a unified and generalizable biomedical foundation model trained\nfrom scratch. To overcome this, we introduce MMKD-CLIP, a generalist biomedical\nfoundation model developed via Multiple Medical CLIP Knowledge Distillation.\nRather than relying on billion-scale raw data, MMKD-CLIP distills knowledge\nfrom nine state-of-the-art domain-specific or generalist biomedical CLIP\nmodels, each pretrained on millions of biomedical image-text pairs. Our\ntwo-stage training pipeline first performs CLIP-style pretraining on over 2.9\nmillion biomedical image-text pairs from 26 image modalities, followed by\nfeature-level distillation using over 19.2 million feature pairs extracted from\nteacher models. We evaluate MMKD-CLIP on 58 diverse biomedical datasets,\nencompassing over 10.8 million biomedical images across nine image modalities.\nThe evaluation spans six core task types: zero-shot classification, linear\nprobing, cross-modal retrieval, visual question answering, survival prediction,\nand cancer diagnosis. MMKD-CLIP consistently outperforms all teacher models\nwhile demonstrating remarkable robustness and generalization across image\ndomains and task settings. These results underscore that multi-teacher\nknowledge distillation is a scalable and effective paradigm for building\nhigh-performing biomedical foundation models under the practical constraints of\nreal-world data availability.", "AI": {"tldr": "MMKD-CLIP通过多教师知识蒸馏构建高性能生物医学基础模型，解决了生物医学领域数据稀缺和异构性问题。", "motivation": "生物医学领域缺乏大规模图像-文本对数据，且图像模态和标准碎片化，阻碍了通用基础模型的开发。", "method": "采用两阶段训练：先在290万生物医学图像-文本对上预训练，再从教师模型中提取1920万特征对进行特征级蒸馏。", "result": "在58个数据集上评估，涵盖1080万图像和9种模态，MMKD-CLIP在所有任务中表现优于教师模型。", "conclusion": "多教师知识蒸馏是构建高性能生物医学基础模型的有效方法。"}}
{"id": "2506.23504", "pdf": "https://arxiv.org/pdf/2506.23504", "abs": "https://arxiv.org/abs/2506.23504", "authors": ["Bosubabu Sambana", "Kotamsetty Geethika Devi", "Bandi Rajeswara Reddy", "Galeti Mohammad Hussain", "Gownivalla Siddartha"], "title": "Hybrid Approach for Electricity Price Forecasting using AlexNet and LSTM", "categories": ["cs.AI"], "comment": "6 Pages, 7 Figures", "summary": "The recent development of advanced machine learning methods for hybrid models\nhas greatly addressed the need for the correct prediction of electrical prices.\nThis method combines AlexNet and LSTM algorithms, which are used to introduce a\nnew model with higher accuracy in price forecasting. Despite RNN and ANN being\neffective, they often fail to deal with forex time sequence data. The\ntraditional methods do not accurately forecast the prices. These traditional\nmethods only focus on demand and price which leads to insufficient analysis of\ndata. To address this issue, using the hybrid approach, which focuses on\nexternal variables that also effect the predicted prices. Nevertheless, due to\nAlexNet's excellent feature extraction and LSTM's learning sequential patterns,\nthe prediction accuracy is vastly increased. The model is built on the past\ndata, which has been supplied with the most significant elements like demand,\ntemperature, sunlight, and rain. For example, the model applies methods, such\nas minimum-maximum scaling and a time window, to predict the electricity prices\nof the future. The results show that this hybrid model is good than the\nstandalone ones in terms of accuracy. Although we got our accuracy rating of\n97.08, it shows higher accompaniments than remaining models RNN and ANN with\naccuracies of 96.64 and 96.63 respectively.", "AI": {"tldr": "本文提出了一种结合AlexNet和LSTM的混合模型，用于提高电力价格预测的准确性，优于传统的RNN和ANN方法。", "motivation": "传统方法仅关注需求和价格，无法充分分析数据，且在处理外汇时间序列数据时表现不佳。", "method": "结合AlexNet的特征提取能力和LSTM的序列学习能力，引入外部变量（如需求、温度、阳光和降雨）进行预测。", "result": "混合模型的预测准确率达到97.08%，高于RNN（96.64%）和ANN（96.63%）。", "conclusion": "该混合模型在电力价格预测中表现出更高的准确性，优于传统方法。"}}
{"id": "2506.22808", "pdf": "https://arxiv.org/pdf/2506.22808", "abs": "https://arxiv.org/abs/2506.22808", "authors": ["Jianhui Wei", "Zijie Meng", "Zikai Xiao", "Tianxiang Hu", "Yang Feng", "Zhijie Zhou", "Jian Wu", "Zuozhu Liu"], "title": "MedEthicsQA: A Comprehensive Question Answering Benchmark for Medical Ethics Evaluation of LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "20 pages", "summary": "While Medical Large Language Models (MedLLMs) have demonstrated remarkable\npotential in clinical tasks, their ethical safety remains insufficiently\nexplored. This paper introduces $\\textbf{MedEthicsQA}$, a comprehensive\nbenchmark comprising $\\textbf{5,623}$ multiple-choice questions and\n$\\textbf{5,351}$ open-ended questions for evaluation of medical ethics in LLMs.\nWe systematically establish a hierarchical taxonomy integrating global medical\nethical standards. The benchmark encompasses widely used medical datasets,\nauthoritative question banks, and scenarios derived from PubMed literature.\nRigorous quality control involving multi-stage filtering and multi-faceted\nexpert validation ensures the reliability of the dataset with a low error rate\n($2.72\\%$). Evaluation of state-of-the-art MedLLMs exhibit declined performance\nin answering medical ethics questions compared to their foundation\ncounterparts, elucidating the deficiencies of medical ethics alignment. The\ndataset, registered under CC BY-NC 4.0 license, is available at\nhttps://github.com/JianhuiWei7/MedEthicsQA.", "AI": {"tldr": "论文提出了MedEthicsQA基准，用于评估医学大语言模型在医学伦理方面的表现，发现其表现不如基础模型。", "motivation": "医学大语言模型在临床任务中表现出潜力，但其伦理安全性尚未充分研究。", "method": "构建包含5,623道选择题和5,351道开放题的MedEthicsQA基准，整合全球医学伦理标准，并进行多阶段质量控制和专家验证。", "result": "评估显示医学大语言模型在回答医学伦理问题时表现下降，揭示了医学伦理对齐的不足。", "conclusion": "MedEthicsQA是一个可靠的数据集，可用于进一步研究医学伦理对齐问题。"}}
{"id": "2506.22570", "pdf": "https://arxiv.org/pdf/2506.22570", "abs": "https://arxiv.org/abs/2506.22570", "authors": ["Chee Mei Ling", "Thangarajah Akilan", "Aparna Ravinda Phalke"], "title": "Dual Atrous Separable Convolution for Improving Agricultural Semantic Segmentation", "categories": ["cs.CV"], "comment": "17 pages, 7 figures, 6 tables", "summary": "Agricultural image semantic segmentation is a pivotal component of modern\nagriculture, facilitating accurate visual data analysis to improve crop\nmanagement, optimize resource utilization, and boost overall productivity. This\nstudy proposes an efficient image segmentation method for precision\nagriculture, focusing on accurately delineating farmland anomalies to support\ninformed decision-making and proactive interventions. A novel Dual Atrous\nSeparable Convolution (DAS Conv) module is integrated within the\nDeepLabV3-based segmentation framework. The DAS Conv module is meticulously\ndesigned to achieve an optimal balance between dilation rates and padding size,\nthereby enhancing model performance without compromising efficiency. The study\nalso incorporates a strategic skip connection from an optimal stage in the\nencoder to the decoder to bolster the model's capacity to capture fine-grained\nspatial features. Despite its lower computational complexity, the proposed\nmodel outperforms its baseline and achieves performance comparable to highly\ncomplex transformer-based state-of-the-art (SOTA) models on the Agriculture\nVision benchmark dataset. It achieves more than 66% improvement in efficiency\nwhen considering the trade-off between model complexity and performance,\ncompared to the SOTA model. This study highlights an efficient and effective\nsolution for improving semantic segmentation in remote sensing applications,\noffering a computationally lightweight model capable of high-quality\nperformance in agricultural imagery.", "AI": {"tldr": "提出了一种基于DeepLabV3的高效农业图像语义分割方法，通过引入DAS Conv模块和优化跳跃连接，在保持低计算复杂度的同时，性能接近复杂Transformer模型。", "motivation": "农业图像语义分割对精准农业至关重要，但现有方法在效率和性能之间存在权衡。本研究旨在开发一种高效且高性能的分割方法。", "method": "在DeepLabV3框架中集成DAS Conv模块，优化膨胀率和填充大小，并设计跳跃连接以捕捉细粒度空间特征。", "result": "在Agriculture Vision数据集上，模型性能接近复杂Transformer模型，且效率提升66%。", "conclusion": "该方法为农业遥感提供了一种高效且高性能的语义分割解决方案。"}}
{"id": "2506.23517", "pdf": "https://arxiv.org/pdf/2506.23517", "abs": "https://arxiv.org/abs/2506.23517", "authors": ["Selin Dik", "Osman Erdem", "Mehmet Dik"], "title": "Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "As the use of AI tools by students has become more prevalent, instructors\nhave started using AI detection tools like GPTZero and QuillBot to detect AI\nwritten text. However, the reliability of these detectors remains uncertain. In\nour study, we focused mostly on the success rate of GPTZero, the most-used AI\ndetector, in identifying AI-generated texts based on different lengths of\nrandomly submitted essays: short (40-100 word count), medium (100-350 word\ncount), and long (350-800 word count). We gathered a data set consisting of\ntwenty-eight AI-generated papers and fifty human-written papers. With this\nrandomized essay data, papers were individually plugged into GPTZero and\nmeasured for percentage of AI generation and confidence. A vast majority of the\nAI-generated papers were detected accurately (ranging from 91-100% AI believed\ngeneration), while the human generated essays fluctuated; there were a handful\nof false positives. These findings suggest that although GPTZero is effective\nat detecting purely AI-generated content, its reliability in distinguishing\nhuman-authored texts is limited. Educators should therefore exercise caution\nwhen relying solely on AI detection tools.", "AI": {"tldr": "GPTZero在检测AI生成文本方面表现良好，但对人类写作的误判率较高，教育者需谨慎使用。", "motivation": "研究AI检测工具（如GPTZero）在不同长度文本中检测AI生成内容的可靠性。", "method": "收集28篇AI生成和50篇人类写作的论文，通过GPTZero检测其AI生成百分比和置信度。", "result": "AI生成文本检测准确率高（91-100%），但人类写作存在误判。", "conclusion": "GPTZero对纯AI文本检测有效，但对人类文本区分能力有限，教育者应谨慎依赖。"}}
{"id": "2506.22813", "pdf": "https://arxiv.org/pdf/2506.22813", "abs": "https://arxiv.org/abs/2506.22813", "authors": ["Zhuojun Ding", "Wei Wei", "Chenghao Fan"], "title": "Selecting and Merging: Towards Adaptable and Scalable Named Entity Recognition with Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Supervised fine-tuning (SFT) is widely used to align large language models\n(LLMs) with information extraction (IE) tasks, such as named entity recognition\n(NER). However, annotating such fine-grained labels and training\ndomain-specific models is costly. Existing works typically train a unified\nmodel across multiple domains, but such approaches lack adaptation and\nscalability since not all training data benefits target domains and scaling\ntrained models remains challenging. We propose the SaM framework, which\ndynamically Selects and Merges expert models at inference time. Specifically,\nfor a target domain, we select domain-specific experts pre-trained on existing\ndomains based on (i) domain similarity to the target domain and (ii)\nperformance on sampled instances, respectively. The experts are then merged to\ncreate task-specific models optimized for the target domain. By dynamically\nmerging experts beneficial to target domains, we improve generalization across\nvarious domains without extra training. Additionally, experts can be added or\nremoved conveniently, leading to great scalability. Extensive experiments on\nmultiple benchmarks demonstrate our framework's effectiveness, which\noutperforms the unified model by an average of 10%. We further provide insights\ninto potential improvements, practical experience, and extensions of our\nframework.", "AI": {"tldr": "论文提出SaM框架，通过动态选择和合并专家模型，优化目标领域的信息提取任务，无需额外训练即可提升泛化能力。", "motivation": "现有跨领域统一模型缺乏适应性和可扩展性，标注细粒度标签和训练领域特定模型成本高。", "method": "SaM框架动态选择预训练的领域专家模型，基于领域相似性和性能采样，合并为任务特定模型。", "result": "实验表明，SaM框架平均优于统一模型10%，并具备良好的可扩展性。", "conclusion": "SaM框架通过动态合并专家模型，显著提升跨领域性能，并支持灵活扩展。"}}
{"id": "2506.22589", "pdf": "https://arxiv.org/pdf/2506.22589", "abs": "https://arxiv.org/abs/2506.22589", "authors": ["Yijun Lin", "Rhett Olson", "Junhan Wu", "Yao-Yi Chiang", "Jerod Weinman"], "title": "LIGHT: Multi-Modal Text Linking on Historical Maps", "categories": ["cs.CV"], "comment": "Accepted at ICDAR2025", "summary": "Text on historical maps provides valuable information for studies in history,\neconomics, geography, and other related fields. Unlike structured or\nsemi-structured documents, text on maps varies significantly in orientation,\nreading order, shape, and placement. Many modern methods can detect and\ntranscribe text regions, but they struggle to effectively ``link'' the\nrecognized text fragments, e.g., determining a multi-word place name. Existing\nlayout analysis methods model word relationships to improve text understanding\nin structured documents, but they primarily rely on linguistic features and\nneglect geometric information, which is essential for handling map text. To\naddress these challenges, we propose LIGHT, a novel multi-modal approach that\nintegrates linguistic, image, and geometric features for linking text on\nhistorical maps. In particular, LIGHT includes a geometry-aware embedding\nmodule that encodes the polygonal coordinates of text regions to capture\npolygon shapes and their relative spatial positions on an image. LIGHT unifies\nthis geometric information with the visual and linguistic token embeddings from\nLayoutLMv3, a pretrained layout analysis model. LIGHT uses the cross-modal\ninformation to predict the reading-order successor of each text instance\ndirectly with a bi-directional learning strategy that enhances sequence\nrobustness. Experimental results show that LIGHT outperforms existing methods\non the ICDAR 2024/2025 MapText Competition data, demonstrating the\neffectiveness of multi-modal learning for historical map text linking.", "AI": {"tldr": "LIGHT是一种多模态方法，结合语言、图像和几何特征，用于链接历史地图上的文本，优于现有方法。", "motivation": "历史地图上的文本信息对多领域研究至关重要，但现有方法难以有效链接文本片段，尤其是多词地名。", "method": "LIGHT通过几何感知嵌入模块编码文本区域的坐标，结合LayoutLMv3的视觉和语言特征，预测文本实例的阅读顺序。", "result": "LIGHT在ICDAR 2024/2025 MapText竞赛数据上表现优于现有方法。", "conclusion": "多模态学习对历史地图文本链接有效，LIGHT展示了其优越性。"}}
{"id": "2506.23520", "pdf": "https://arxiv.org/pdf/2506.23520", "abs": "https://arxiv.org/abs/2506.23520", "authors": ["Yu Zhang", "Ruijie Yu", "Jidong Tian", "Feng Zhu", "Jiapeng Liu", "Xiaokang Yang", "Yaohui Jin", "Yanyan Xu"], "title": "ChemActor: Enhancing Automated Extraction of Chemical Synthesis Actions with LLM-Generated Data", "categories": ["cs.AI"], "comment": null, "summary": "With the increasing interest in robotic synthesis in the context of organic\nchemistry, the automated extraction of chemical procedures from literature is\ncritical. However, this task remains challenging due to the inherent ambiguity\nof chemical language and the high cost of human annotation required for\ndeveloping reliable computer-aided extraction protocols. Here, we present\nChemActor, a fully fine-tuned large language model (LLM), as a chemical\nexecutor to convert between unstructured experimental procedures and structured\naction sequences. We propose a sequential LLM-generated data framework to\naddress the challenges of insufficient and low-quality annotated data. This\nframework integrates a data selection module that selects data based on\ndistribution divergence, with a general-purpose LLM, to generate\nmachine-executable actions from a single molecule input. Additionally, we\nintroduce a novel multi-round LLMs circle review metric, which reflects the\nmodel's advanced understanding of chemical experimental procedures. Extensive\nexperiments on reaction-to-description (R2D) and description-to-action (D2A)\ntasks demonstrate that ChemActor, augmented by LLM-generated data, achieves\nstate-of-the-art performance, outperforming the baseline model by 10%. The code\nis available at: https://github.com/Zhanghahah/ChemActor.", "AI": {"tldr": "ChemActor是一个基于大型语言模型（LLM）的化学执行器，用于将非结构化的化学实验步骤转换为结构化的动作序列，通过LLM生成的数据框架解决了标注数据不足的问题，并在R2D和D2A任务中表现优异。", "motivation": "由于化学语言的模糊性和人工标注的高成本，自动化提取化学实验步骤仍具挑战性，需要开发可靠的计算机辅助提取方法。", "method": "提出ChemActor，一个完全微调的LLM，结合数据选择模块和多轮LLM循环评估指标，生成机器可执行的动作序列。", "result": "在R2D和D2A任务中，ChemActor表现优于基线模型10%，达到最先进水平。", "conclusion": "ChemActor通过LLM生成的数据框架，有效解决了化学实验步骤自动化提取的挑战，为化学合成自动化提供了可靠工具。"}}
{"id": "2506.22846", "pdf": "https://arxiv.org/pdf/2506.22846", "abs": "https://arxiv.org/abs/2506.22846", "authors": ["Duygu Altinok"], "title": "Boosting CTC-Based ASR Using LLM-Based Intermediate Loss Regularization", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "This is the accepted version of an article accepted to the TSD 2025\n  conference, published in Springer Lecture Notes in Artificial Intelligence\n  (LNAI). The final authenticated version is available online at SpringerLink", "summary": "End-to-end (E2E) automatic speech recognition (ASR) systems have\nrevolutionized the field by integrating all components into a single neural\nnetwork, with attention-based encoder-decoder models achieving state-of-the-art\nperformance. However, their autoregressive decoding process limits inference\nspeed, making them unsuitable for real-time applications. In contrast,\nCTC-based models offer faster, non-autoregressive decoding but struggle to\nmodel linguistic dependencies effectively. Addressing this challenge, we\npropose a novel auxiliary loss framework called Language-Aware Intermediate\nLoss (LAIL) to enhance CTC-based ASR using the linguistic knowledge of large\nlanguage models (LLMs). By attaching connector layers to intermediate encoder\nlayers, LAIL maps outputs to the embedding space of an LLM and computes a\ncausal language modeling loss during training. This approach enhances\nlinguistic modeling while preserving the computational efficiency of CTC\ndecoding. Using the Conformer architecture and various LLaMA models, we\ndemonstrate significant improvements in Word Error Rate (WER) on the\nLibriSpeech, TEDLIUM2, and WSJ corpora, achieving state-of-the-art performance\nfor CTC-based ASR with minimal computational overhead.", "AI": {"tldr": "提出了一种名为LAIL的辅助损失框架，通过利用大型语言模型的语义知识增强CTC-based ASR的建模能力，同时保持其高效解码特性。", "motivation": "解决CTC-based ASR模型在语言依赖性建模上的不足，同时保持其非自回归解码的高效性。", "method": "在中间编码器层附加连接层，将输出映射到LLM的嵌入空间，并计算因果语言建模损失。", "result": "在LibriSpeech、TEDLIUM2和WSJ语料库上显著降低了WER，实现了CTC-based ASR的最先进性能。", "conclusion": "LAIL框架有效提升了CTC-based ASR的语言建模能力，且计算开销极小。"}}
{"id": "2506.22591", "pdf": "https://arxiv.org/pdf/2506.22591", "abs": "https://arxiv.org/abs/2506.22591", "authors": ["Arunkumar Kannan", "Martin A. Lindquist", "Brian Caffo"], "title": "BrainMT: A Hybrid Mamba-Transformer Architecture for Modeling Long-Range Dependencies in Functional MRI Data", "categories": ["cs.CV"], "comment": "Accepted at MICCAI 2025", "summary": "Recent advances in deep learning have made it possible to predict phenotypic\nmeasures directly from functional magnetic resonance imaging (fMRI) brain\nvolumes, sparking significant interest in the neuroimaging community. However,\nexisting approaches, primarily based on convolutional neural networks or\ntransformer architectures, often struggle to model the complex relationships\ninherent in fMRI data, limited by their inability to capture long-range spatial\nand temporal dependencies. To overcome these shortcomings, we introduce\nBrainMT, a novel hybrid framework designed to efficiently learn and integrate\nlong-range spatiotemporal attributes in fMRI data. Our framework operates in\ntwo stages: (1) a bidirectional Mamba block with a temporal-first scanning\nmechanism to capture global temporal interactions in a computationally\nefficient manner; and (2) a transformer block leveraging self-attention to\nmodel global spatial relationships across the deep features processed by the\nMamba block. Extensive experiments on two large-scale public datasets,\nUKBioBank and the Human Connectome Project, demonstrate that BrainMT achieves\nstate-of-the-art performance on both classification (sex prediction) and\nregression (cognitive intelligence prediction) tasks, outperforming existing\nmethods by a significant margin. Our code and implementation details will be\nmade publicly available at this\nhttps://github.com/arunkumar-kannan/BrainMT-fMRI", "AI": {"tldr": "BrainMT是一种新型混合框架，通过结合双向Mamba块和Transformer块，有效捕捉fMRI数据中的长距离时空依赖关系，显著提升了分类和回归任务的性能。", "motivation": "现有基于卷积神经网络或Transformer的方法难以建模fMRI数据中的复杂时空关系，因此需要一种更高效的框架。", "method": "BrainMT采用两阶段设计：双向Mamba块捕捉全局时间交互，Transformer块建模空间关系。", "result": "在UKBioBank和Human Connectome Project数据集上，BrainMT在分类和回归任务中均达到最优性能。", "conclusion": "BrainMT通过高效建模时空依赖关系，显著优于现有方法，代码将公开。"}}
{"id": "2506.23549", "pdf": "https://arxiv.org/pdf/2506.23549", "abs": "https://arxiv.org/abs/2506.23549", "authors": ["Huai-Chih Wang", "Hsiang-Chun Chuang", "Hsi-Chun Cheng", "Dai-Jie Wu", "Shao-Hua Sun"], "title": "CooT: Learning to Coordinate In-Context with Coordination Transformers", "categories": ["cs.AI", "cs.HC", "cs.LG"], "comment": "23 pages, 10 tables, 8 figures", "summary": "Effective coordination among artificial agents in dynamic and uncertain\nenvironments remains a significant challenge in multi-agent systems. Existing\napproaches, such as self-play and population-based methods, either generalize\npoorly to unseen partners or require extensive training. To overcome these\nlimitations, we propose Coordination Transformers (CooT), a novel in-context\ncoordination framework that uses recent interaction histories to adapt to\nunseen partners rapidly. Unlike previous approaches that primarily aim to\nincrease the diversity of training partners, CooT explicitly focuses on\nadapting to new partner behaviors by predicting actions aligned with observed\npartner interactions. Trained on interaction trajectories collected from\ndiverse pairs of agents with complementary behaviors, CooT quickly learns\neffective coordination strategies without explicit supervision or fine-tuning.\nEvaluations on the Overcooked benchmark demonstrate that CooT significantly\noutperforms baseline methods in coordination tasks involving previously unseen\npartners. Human evaluations further confirm CooT as the most effective\ncollaborative partner, while extensive ablations highlight its robustness,\nflexibility, and sensitivity to context in multi-agent scenarios.", "AI": {"tldr": "提出了一种名为Coordination Transformers (CooT)的新框架，通过上下文协调快速适应未见过的合作伙伴，显著优于现有方法。", "motivation": "解决多智能体系统中动态和不确定环境下协调的挑战，现有方法泛化能力差或训练成本高。", "method": "利用近期交互历史预测合作伙伴行为，无需显式监督或微调，快速学习协调策略。", "result": "在Overcooked基准测试中，CooT显著优于基线方法，人类评估也证实其有效性。", "conclusion": "CooT展示了在多智能体场景中的鲁棒性、灵活性和上下文敏感性，是高效的协作伙伴。"}}
{"id": "2506.22852", "pdf": "https://arxiv.org/pdf/2506.22852", "abs": "https://arxiv.org/abs/2506.22852", "authors": ["Yucheng Cai", "Yuxuan Wu", "Yi Huang", "Junlan Feng", "Zhijian Ou"], "title": "Knowledge Augmented Finetuning Matters in both RAG and Agent Based Dialog Systems", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have recently been applied to dialog systems.\nDespite making progress, LLMs are prone to errors in knowledge-intensive\nscenarios. Recently, approaches based on retrieval augmented generation (RAG)\nand agent have emerged to improve the factual accuracy by enhancing the LLMs\nwith knowledge retrieved from external knowledge bases (KBs). This is mostly\nimplemented by prompting the LLMs with instructions, examples and the retrieved\nknowledge. However, LLMs may have difficulty using the retrieved knowledge\neffectively for response generation, because they are not well trained to do\nsuch generation for specific domains. To mitigate this problem, we propose to\nfinetune the LLMs in the RAG-based and agent-based systems with domain-specific\ndata, together with domain-specific external knowledge, which is called\nknowledge augmented finetuning (KAFT). We base our study on the MobileCS2\ndataset, a real-life customer service dialog dataset that features intensive\nknowledge interactions, to systematically compare the prompting and KAFT\ntechniques in the RAG-based and agent-based systems. Experiment results show\nthat KAFT substantially surpasses prompting in both RAG and agent systems,\nparticularly in terms of factual accuracy. To the best of our knowledge, this\npaper represents the first solid empirical work to investigate the KAFT idea.", "AI": {"tldr": "论文提出了一种名为KAFT的方法，通过在RAG和基于代理的系统中使用领域特定数据微调LLMs，显著提高了事实准确性。", "motivation": "LLMs在知识密集型场景中容易出错，现有方法（如RAG和代理）通过外部知识库增强LLMs，但LLMs难以有效利用检索到的知识生成响应。", "method": "提出KAFT方法，结合领域特定数据和外部知识微调LLMs，并在MobileCS2数据集上比较KAFT与提示技术的效果。", "result": "实验表明，KAFT在RAG和代理系统中均显著优于提示技术，尤其在事实准确性方面。", "conclusion": "KAFT是首个实证研究该方法的成果，为提升LLMs在知识密集型任务中的表现提供了有效途径。"}}
{"id": "2506.22624", "pdf": "https://arxiv.org/pdf/2506.22624", "abs": "https://arxiv.org/abs/2506.22624", "authors": ["Zuyao You", "Zuxuan Wu"], "title": "Seg-R1: Segmentation Can Be Surprisingly Simple with Reinforcement Learning", "categories": ["cs.CV"], "comment": null, "summary": "We present Seg-R1, a preliminary exploration of using reinforcement learning\n(RL) to enhance the pixel-level understanding and reasoning capabilities of\nlarge multimodal models (LMMs). Starting with foreground segmentation tasks,\nspecifically camouflaged object detection (COD) and salient object detection\n(SOD), our approach enables the LMM to generate point and bounding box prompts\nin the next-token fashion, which are then used to guide SAM2 in producing\nsegmentation masks. We introduce Group Relative Policy Optimization (GRPO) into\nthe segmentation domain, equipping the LMM with pixel-level comprehension\nthrough a carefully designed training strategy. Notably, Seg-R1 achieves\nremarkable performance with purely RL-based training, achieving .873 S-measure\non COD10K without complex model modification. Moreover, we found that pure RL\ntraining demonstrates strong open-world generalization. Despite being trained\nsolely on foreground segmentation image-mask pairs without text supervision,\nSeg-R1 achieves impressive zero-shot performance on referring segmentation and\nreasoning segmentation tasks, with 71.4 cIoU on RefCOCOg test and 56.7 gIoU on\nReasonSeg test, outperforming models fully supervised on these datasets.", "AI": {"tldr": "Seg-R1利用强化学习提升大型多模态模型的像素级理解能力，通过GRPO策略在分割任务中表现优异，无需复杂修改即可实现高性能，并展现出强大的开放世界泛化能力。", "motivation": "探索如何通过强化学习增强大型多模态模型在像素级任务（如前景分割）中的理解和推理能力。", "method": "采用Group Relative Policy Optimization (GRPO)策略，通过点提示和边界框提示引导SAM2生成分割掩码，仅使用RL训练。", "result": "在COD10K上达到0.873 S-measure，在RefCOCOg和ReasonSeg上分别实现71.4 cIoU和56.7 gIoU的零样本性能。", "conclusion": "Seg-R1展示了纯RL训练在分割任务中的高效性和泛化能力，为像素级理解任务提供了新思路。"}}
{"id": "2506.23563", "pdf": "https://arxiv.org/pdf/2506.23563", "abs": "https://arxiv.org/abs/2506.23563", "authors": ["Huanjin Yao", "Jiaxing Huang", "Yawen Qiu", "Michael K. Chen", "Wenzheng Liu", "Wei Zhang", "Wenjie Zeng", "Xikun Zhang", "Jingyi Zhang", "Yuxin Song", "Wenhao Wu", "Dacheng Tao"], "title": "MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI", "categories": ["cs.AI", "cs.CL", "cs.CV"], "comment": "Technical report", "summary": "Reasoning plays a crucial role in advancing Multimodal Large Language Models\n(MLLMs) toward Artificial General Intelligence. However, existing MLLM\nbenchmarks often fall short in precisely and comprehensively evaluating\nlong-chain reasoning abilities from three key aspects: (1) lack of difficulty\nand diversity, (2) susceptibility to guessability and memorization, (3)\ninadequate assessment of intermediate reasoning steps. To fill this gap, we\nintroduce MMReason, a new benchmark designed to precisely and comprehensively\nevaluate MLLM long-chain reasoning capability with diverse, open-ended,\nchallenging questions. First, we curate challenging questions requiring\nmulti-step reasoning from various fields (i.e., 6 disciplines) and multiple\ndifficulty levels (i.e., from pre-university to university, and from\nfoundational to competition tiers). Second, these questions are reformulated\ninto an open-ended format and filtered using a multi-model voting technique to\neliminate shortcut cases related to guessing and memorization, ensuring robust\nreasoning evaluations. Third, we annotate the questions with detailed\nstep-by-step solutions, and design a reference-based ternary scoring mechanism\nto reliably assess intermediate reasoning steps. With MMReason, we benchmark\npopular leading MLLMs and provide an in-depth analysis of their reasoning\ncapabilities. We hope MMReason will serve as a valuable resource for advancing\nMLLM reasoning research. Code will be available at\nhttps://github.com/HJYao00/MMReason.", "AI": {"tldr": "MMReason是一个新的基准测试，旨在全面评估多模态大语言模型（MLLMs）的长链推理能力，通过多样、开放和具有挑战性的问题填补现有基准的不足。", "motivation": "现有MLLM基准在评估长链推理能力时存在不足，包括缺乏难度和多样性、易受猜测和记忆影响，以及对中间推理步骤评估不足。", "method": "MMReason通过从多个学科和难度级别收集问题，将其转化为开放格式，并使用多模型投票技术过滤问题以避免猜测和记忆。此外，为问题标注详细的分步解决方案，并设计基于参考的三元评分机制。", "result": "MMReason对流行的MLLMs进行了基准测试，并深入分析了它们的推理能力。", "conclusion": "MMReason有望成为推动MLLM推理研究的重要资源。"}}
{"id": "2506.22853", "pdf": "https://arxiv.org/pdf/2506.22853", "abs": "https://arxiv.org/abs/2506.22853", "authors": ["Kyochul Jang", "Donghyeon Lee", "Kyusik Kim", "Dongseok Heo", "Taewhoo Lee", "Woojeong Kim", "Bongwon Suh"], "title": "DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language Models in Multi-Round, Multi-Party Dialogues", "categories": ["cs.CL", "cs.AI"], "comment": "9 pages, ACL 2025 Vienna", "summary": "Existing function-calling benchmarks focus on single-turn interactions.\nHowever, they overlook the complexity of real-world scenarios. To quantify how\nexisting benchmarks address practical applications, we introduce DICE-SCORE, a\nmetric that evaluates the dispersion of tool-related information such as\nfunction name and parameter values throughout the dialogue. Analyzing existing\nbenchmarks through DICE-SCORE reveals notably low scores, highlighting the need\nfor more realistic scenarios. To address this gap, we present DICE-BENCH, a\nframework that constructs practical function-calling datasets by synthesizing\nconversations through a tool graph that maintains dependencies across rounds\nand a multi-agent system with distinct personas to enhance dialogue\nnaturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our\nexperiments on 19 LLMs with DICE-BENCH show that significant advances are still\nrequired before such models can be deployed effectively in real-world settings.\nOur code and data are all publicly available:\nhttps://snuhcc.github.io/DICE-Bench/.", "AI": {"tldr": "论文提出了DICE-SCORE指标和DICE-BENCH框架，用于评估和构建更贴近现实场景的多轮函数调用数据集。", "motivation": "现有函数调用基准测试仅关注单轮交互，忽略了现实场景的复杂性，需要更实用的评估方法。", "method": "引入DICE-SCORE评估工具相关信息的分散程度，并通过工具图和多代理系统构建DICE-BENCH数据集。", "result": "现有基准测试DICE-SCORE得分低，DICE-BENCH数据集包含1,607个高质量实例，实验显示现有LLM仍需改进。", "conclusion": "DICE-BENCH为现实场景的函数调用提供了更实用的评估框架，现有模型仍需进一步优化。"}}
{"id": "2506.22636", "pdf": "https://arxiv.org/pdf/2506.22636", "abs": "https://arxiv.org/abs/2506.22636", "authors": ["Sotirios Panagiotis Chytas", "Miso Choi", "Hyunwoo J. Kim", "Vikas Singh"], "title": "ReCo: Reminder Composition Mitigates Hallucinations in Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Vision Language Models (VLMs) show impressive capabilities in integrating and\nreasoning with both visual and language data. But these models make mistakes. A\ncommon finding -- similar to LLMs -- is their tendency to hallucinate, i.e.,\ngenerate plausible sounding text which is not grounded in the visual input, or\nat worst, is contradictory. A growing consensus attributes this behavior to an\nover-reliance on language -- especially as the generation progresses, the model\nsuffers from a ``fading memory effect'' with respect to the provided visual\ninput. We study mechanisms by which this behavior can be controlled.\nSpecifically, using ideas from geometric algebra and relational compositions,\nwe propose the addition of a small, trainable module (named ReCo) on top of any\nVLM -- no other modification is needed. We show that such a lightweight module\nis able to mitigate the fading memory effect on three of the most widely used\nVLMs (InstructBLIP, LlaVA, MiniGPT4), where we see performance improvements on\nmultiple benchmarks. Additionally, we show that our module can be combined with\nmany of the other approaches for reducing hallucination where we achieve\nimproved results for each one.", "AI": {"tldr": "论文提出了一种轻量级模块ReCo，用于缓解视觉语言模型（VLMs）中的幻觉问题，通过几何代数和关系组合的方法，显著提升了模型性能。", "motivation": "视觉语言模型（VLMs）在生成文本时容易产生幻觉（即生成与视觉输入无关或矛盾的文本），主要原因是模型对语言的过度依赖和视觉输入的‘记忆衰退效应’。本文旨在研究如何控制这种行为。", "method": "提出了一种名为ReCo的小型可训练模块，基于几何代数和关系组合的方法，无需修改现有VLM结构即可直接应用于模型顶部。", "result": "在InstructBLIP、LlaVA和MiniGPT4三种广泛使用的VLM上，ReCo模块有效缓解了记忆衰退效应，并在多个基准测试中提升了性能。此外，ReCo还能与其他减少幻觉的方法结合使用，进一步改善结果。", "conclusion": "ReCo模块是一种轻量级且通用的解决方案，能够显著减少VLM中的幻觉问题，且易于与其他方法结合使用，具有广泛的应用潜力。"}}
{"id": "2506.23576", "pdf": "https://arxiv.org/pdf/2506.23576", "abs": "https://arxiv.org/abs/2506.23576", "authors": ["Maria Carolina Cornelia Wit", "Jun Pang"], "title": "Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language Models", "categories": ["cs.AI"], "comment": "26 pages, 1 figure", "summary": "Recent advances in large language models (LLMs) have raised concerns about\njailbreaking attacks, i.e., prompts that bypass safety mechanisms. This paper\ninvestigates the use of multi-agent LLM systems as a defence against such\nattacks. We evaluate three jailbreaking strategies, including the original\nAutoDefense attack and two from Deepleaps: BetterDan and JB. Reproducing the\nAutoDefense framework, we compare single-agent setups with two- and three-agent\nconfigurations. Our results show that multi-agent systems enhance resistance to\njailbreaks, especially by reducing false negatives. However, its effectiveness\nvaries by attack type, and it introduces trade-offs such as increased false\npositives and computational overhead. These findings point to the limitations\nof current automated defences and suggest directions for improving alignment\nrobustness in future LLM systems.", "AI": {"tldr": "多智能体LLM系统可增强对越狱攻击的防御，但存在误报和计算开销的权衡。", "motivation": "研究多智能体LLM系统作为防御越狱攻击的有效性。", "method": "评估三种越狱策略（AutoDefense、BetterDan、JB），比较单智能体与多智能体配置。", "result": "多智能体系统提升了防御能力，但效果因攻击类型而异，且增加了误报和计算开销。", "conclusion": "当前自动防御存在局限性，需改进对齐鲁棒性。"}}
{"id": "2506.22858", "pdf": "https://arxiv.org/pdf/2506.22858", "abs": "https://arxiv.org/abs/2506.22858", "authors": ["Duygu Altinok"], "title": "Mind the Gap: Entity-Preserved Context-Aware ASR Structured Transcriptions", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "This is the accepted version of an article accepted to the TSD 2025\n  conference, published in Springer Lecture Notes in Artificial Intelligence\n  (LNAI). The final authenticated version is available online at SpringerLink", "summary": "Automatic Speech Recognition (ASR) systems, such as Whisper, achieve high\ntranscription accuracy but struggle with named entities and numerical data,\nespecially when proper formatting is required. These issues increase word error\nrate (WER) and impair semantic understanding in critical domains like legal,\nfinancial, and medical applications. We propose a novel training approach that\nextends the semantic context of ASR models by adding overlapping context\nwindows during training. By sliding 5-second overlaps on both sides of\n30-second chunks, we create a 40-second \"effective semantic window,\" improving\nentity recognition and formatting while focusing predictions on the central 30\nseconds. To address entities spanning chunk boundaries, we reassign such\nentities entirely to the right-hand chunk, ensuring proper formatting.\nAdditionally, enriched training data with embedded entity labels enables the\nmodel to learn both recognition and type-specific formatting. Evaluated on the\nSpoken Wikipedia dataset, our method improves performance across semantic\ntasks, including named entity recognition (NER) and entity formatting. These\nresults highlight the effectiveness of context-aware training in addressing ASR\nlimitations for long-form transcription and complex entity recognition tasks.", "AI": {"tldr": "提出一种新的训练方法，通过扩展ASR模型的语义上下文窗口，提高命名实体和数值数据的识别与格式化能力。", "motivation": "ASR系统（如Whisper）在命名实体和数值数据识别及格式化方面表现不佳，影响语义理解，尤其在法律、金融和医疗等关键领域。", "method": "采用重叠上下文窗口训练方法，滑动5秒重叠的30秒音频块，形成40秒的有效语义窗口，并重新分配跨块实体。", "result": "在Spoken Wikipedia数据集上评估，该方法在命名实体识别（NER）和实体格式化等语义任务中表现提升。", "conclusion": "上下文感知训练能有效解决ASR在长文本转录和复杂实体识别任务中的局限性。"}}
{"id": "2506.22637", "pdf": "https://arxiv.org/pdf/2506.22637", "abs": "https://arxiv.org/abs/2506.22637", "authors": ["Haoxuan Wang", "Zhenghao Zhao", "Junyi Wu", "Yuzhang Shang", "Gaowen Liu", "Yan Yan"], "title": "CaO$_2$: Rectifying Inconsistencies in Diffusion-Based Dataset Distillation", "categories": ["cs.CV"], "comment": "ICCV 2025. Code is available at\n  https://github.com/hatchetProject/CaO2", "summary": "The recent introduction of diffusion models in dataset distillation has shown\npromising potential in creating compact surrogate datasets for large,\nhigh-resolution target datasets, offering improved efficiency and performance\nover traditional bi-level/uni-level optimization methods. However, current\ndiffusion-based dataset distillation approaches overlook the evaluation process\nand exhibit two critical inconsistencies in the distillation process: (1)\nObjective Inconsistency, where the distillation process diverges from the\nevaluation objective, and (2) Condition Inconsistency, leading to mismatches\nbetween generated images and their corresponding conditions. To resolve these\nissues, we introduce Condition-aware Optimization with Objective-guided\nSampling (CaO$_2$), a two-stage diffusion-based framework that aligns the\ndistillation process with the evaluation objective. The first stage employs a\nprobability-informed sample selection pipeline, while the second stage refines\nthe corresponding latent representations to improve conditional likelihood.\nCaO$_2$ achieves state-of-the-art performance on ImageNet and its subsets,\nsurpassing the best-performing baselines by an average of 2.3% accuracy.", "AI": {"tldr": "论文提出了一种名为CaO$_2$的两阶段扩散框架，解决了当前基于扩散的数据集蒸馏方法中的目标不一致和条件不一致问题，并在ImageNet及其子集上实现了最佳性能。", "motivation": "当前基于扩散的数据集蒸馏方法在评估过程中存在目标不一致和条件不一致的问题，影响了效率和性能。", "method": "CaO$_2$框架分为两阶段：第一阶段使用概率信息样本选择管道，第二阶段优化潜在表示以提高条件似然。", "result": "在ImageNet及其子集上，CaO$_2$平均准确率超过基线方法2.3%。", "conclusion": "CaO$_2$通过解决不一致问题，显著提升了数据集蒸馏的性能和效率。"}}
{"id": "2506.23626", "pdf": "https://arxiv.org/pdf/2506.23626", "abs": "https://arxiv.org/abs/2506.23626", "authors": ["António Afonso", "Iolanda Leite", "Alessandro Sestini", "Florian Fuchs", "Konrad Tollmar", "Linus Gisslén"], "title": "Self-correcting Reward Shaping via Language Models for Reinforcement Learning Agents in Games", "categories": ["cs.AI"], "comment": "16 pages in total, 10 pages of main paper, 5 figures", "summary": "Reinforcement Learning (RL) in games has gained significant momentum in\nrecent years, enabling the creation of different agent behaviors that can\ntransform a player's gaming experience. However, deploying RL agents in\nproduction environments presents two key challenges: (1) designing an effective\nreward function typically requires an RL expert, and (2) when a game's content\nor mechanics are modified, previously tuned reward weights may no longer be\noptimal. Towards the latter challenge, we propose an automated approach for\niteratively fine-tuning an RL agent's reward function weights, based on a\nuser-defined language based behavioral goal. A Language Model (LM) proposes\nupdated weights at each iteration based on this target behavior and a summary\nof performance statistics from prior training rounds. This closed-loop process\nallows the LM to self-correct and refine its output over time, producing\nincreasingly aligned behavior without the need for manual reward engineering.\nWe evaluate our approach in a racing task and show that it consistently\nimproves agent performance across iterations. The LM-guided agents show a\nsignificant increase in performance from $9\\%$ to $74\\%$ success rate in just\none iteration. We compare our LM-guided tuning against a human expert's manual\nweight design in the racing task: by the final iteration, the LM-tuned agent\nachieved an $80\\%$ success rate, and completed laps in an average of $855$ time\nsteps, a competitive performance against the expert-tuned agent's peak $94\\%$\nsuccess, and $850$ time steps.", "AI": {"tldr": "论文提出了一种基于语言模型的自动化方法，用于迭代调整强化学习代理的奖励函数权重，以解决游戏内容或机制变化时手动调整的挑战。", "motivation": "在游戏中部署强化学习代理时，手动设计奖励函数需要专家，且游戏内容变化时需重新调整权重，效率低下。", "method": "利用语言模型根据用户定义的行为目标和性能统计自动调整奖励函数权重，形成闭环优化过程。", "result": "在赛车任务中，LM引导的代理性能从9%提升到74%成功率，最终达到80%成功率，接近专家调整的94%。", "conclusion": "该方法无需手动奖励工程，能有效提升代理性能，接近专家水平。"}}
{"id": "2506.22957", "pdf": "https://arxiv.org/pdf/2506.22957", "abs": "https://arxiv.org/abs/2506.22957", "authors": ["Younwoo Choi", "Changling Li", "Yongjin Yang", "Zhijing Jin"], "title": "Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.MA"], "comment": null, "summary": "As large language models (LLMs) are increasingly integrated into multi-agent\nand human-AI systems, understanding their awareness of both self-context and\nconversational partners is essential for ensuring reliable performance and\nrobust safety. While prior work has extensively studied situational awareness\nwhich refers to an LLM's ability to recognize its operating phase and\nconstraints, it has largely overlooked the complementary capacity to identify\nand adapt to the identity and characteristics of a dialogue partner. In this\npaper, we formalize this latter capability as interlocutor awareness and\npresent the first systematic evaluation of its emergence in contemporary LLMs.\nWe examine interlocutor inference across three dimensions-reasoning patterns,\nlinguistic style, and alignment preferences-and show that LLMs reliably\nidentify same-family peers and certain prominent model families, such as GPT\nand Claude. To demonstrate its practical significance, we develop three case\nstudies in which interlocutor awareness both enhances multi-LLM collaboration\nthrough prompt adaptation and introduces new alignment and safety\nvulnerabilities, including reward-hacking behaviors and increased jailbreak\nsusceptibility. Our findings highlight the dual promise and peril of\nidentity-sensitive behavior in LLMs, underscoring the need for further\nunderstanding of interlocutor awareness and new safeguards in multi-agent\ndeployments. Our code is open-sourced at\nhttps://github.com/younwoochoi/InterlocutorAwarenessLLM.", "AI": {"tldr": "论文研究了大型语言模型（LLMs）在对话中对对话伙伴身份和特征的识别与适应能力（对话者意识），并首次系统评估了当代LLMs中这一能力的表现。", "motivation": "随着LLMs在多智能体和人类-AI系统中的广泛应用，理解其对自身和对话伙伴的认知能力对确保可靠性能和安全性至关重要。", "method": "论文通过三个维度（推理模式、语言风格和对齐偏好）评估LLMs的对话者意识，并开发了三个案例研究展示其实际影响。", "result": "研究发现LLMs能可靠识别同类模型（如GPT和Claude），对话者意识既能提升多LLM协作，也可能带来新的安全和对齐漏洞。", "conclusion": "对话者意识在LLMs中具有双重性，既可能带来好处，也可能引入风险，需进一步研究和安全措施。"}}
{"id": "2506.22678", "pdf": "https://arxiv.org/pdf/2506.22678", "abs": "https://arxiv.org/abs/2506.22678", "authors": ["Nicolas Caytuiro", "Ivan Sipiran"], "title": "3D Shape Generation: A Survey", "categories": ["cs.CV"], "comment": "20 pages, 5 figures", "summary": "Recent advances in deep learning have significantly transformed the field of\n3D shape generation, enabling the synthesis of complex, diverse, and\nsemantically meaningful 3D objects. This survey provides a comprehensive\noverview of the current state of the art in 3D shape generation, organizing the\ndiscussion around three core components: shape representations, generative\nmodeling approaches, and evaluation protocols. We begin by categorizing 3D\nrepresentations into explicit, implicit, and hybrid setups, highlighting their\nstructural properties, advantages, and limitations. Next, we review a wide\nrange of generation methods, focusing on feedforward architectures. We further\nsummarize commonly used datasets and evaluation metrics that assess fidelity,\ndiversity, and realism of generated shapes. Finally, we identify open\nchallenges and outline future research directions that could drive progress in\ncontrollable, efficient, and high-quality 3D shape generation. This survey aims\nto serve as a valuable reference for researchers and practitioners seeking a\nstructured and in-depth understanding of this rapidly evolving field.", "AI": {"tldr": "本文综述了3D形状生成的最新进展，围绕形状表示、生成方法和评估协议展开讨论，并总结了当前挑战与未来方向。", "motivation": "深度学习的发展推动了3D形状生成领域的进步，本文旨在为研究人员和实践者提供全面的参考。", "method": "分类讨论了3D形状的表示方法（显式、隐式和混合）、生成方法（前馈架构）以及评估指标。", "result": "总结了常用数据集和评估指标，分析了生成形状的保真度、多样性和真实性。", "conclusion": "指出了可控性、高效性和高质量3D形状生成的未来研究方向。"}}
{"id": "2506.23673", "pdf": "https://arxiv.org/pdf/2506.23673", "abs": "https://arxiv.org/abs/2506.23673", "authors": ["Jingsong Liu", "Han Li", "Chen Yang", "Michael Deutges", "Ario Sadafi", "Xin You", "Katharina Breininger", "Nassir Navab", "Peter J. Schüffler"], "title": "HASD: Hierarchical Adaption for pathology Slide-level Domain-shift", "categories": ["cs.AI"], "comment": null, "summary": "Domain shift is a critical problem for pathology AI as pathology data is\nheavily influenced by center-specific conditions. Current pathology domain\nadaptation methods focus on image patches rather than WSI, thus failing to\ncapture global WSI features required in typical clinical scenarios. In this\nwork, we address the challenges of slide-level domain shift by proposing a\nHierarchical Adaptation framework for Slide-level Domain-shift (HASD). HASD\nachieves multi-scale feature consistency and computationally efficient\nslide-level domain adaptation through two key components: (1) a hierarchical\nadaptation framework that integrates a Domain-level Alignment Solver for\nfeature alignment, a Slide-level Geometric Invariance Regularization to\npreserve the morphological structure, and a Patch-level Attention Consistency\nRegularization to maintain local critical diagnostic cues; and (2) a prototype\nselection mechanism that reduces computational overhead. We validate our method\non two slide-level tasks across five datasets, achieving a 4.1\\% AUROC\nimprovement in a Breast Cancer HER2 Grading cohort and a 3.9\\% C-index gain in\na UCEC survival prediction cohort. Our method provides a practical and reliable\nslide-level domain adaption solution for pathology institutions, minimizing\nboth computational and annotation costs.", "AI": {"tldr": "论文提出了一种名为HASD的分层适应框架，用于解决病理学AI中的切片级域偏移问题，通过多尺度特征一致性和计算高效的策略，显著提升了性能。", "motivation": "病理学数据受中心特定条件影响严重，现有方法仅关注图像块而非全切片图像（WSI），无法满足临床需求。", "method": "HASD框架包含分层适应模块（域对齐、几何不变性正则化和注意力一致性正则化）和原型选择机制。", "result": "在两项任务中，HASD分别提升了4.1%的AUROC和3.9%的C-index。", "conclusion": "HASD为病理学机构提供了一种实用且可靠的切片级域适应解决方案，降低了计算和标注成本。"}}
{"id": "2506.22977", "pdf": "https://arxiv.org/pdf/2506.22977", "abs": "https://arxiv.org/abs/2506.22977", "authors": ["Asen Dotsinski", "Udit Thakur", "Marko Ivanov", "Mohammad Hafeez Khan", "Maria Heuss"], "title": "On the Generalizability of \"Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals\"", "categories": ["cs.CL", "cs.LG"], "comment": "22 pages, 25 figures. For an interactive dashboard with all figures,\n  see https://comp-mech-generalizability.streamlit.app/ . For the accompanying\n  code, see https://github.com/asendotsinski/comp-mech-generalizability . To be\n  published in proceedings of the 2025 Machine Learning Reproducibility\n  Challenge", "summary": "We present a reproduction study of \"Competition of Mechanisms: Tracing How\nLanguage Models Handle Facts and Counterfactuals\" (Ortu et al., 2024), which\ninvestigates competition of mechanisms in language models between factual\nrecall and counterfactual in-context repetition. Our study successfully\nreproduces their primary findings regarding the localization of factual and\ncounterfactual information, the dominance of attention blocks in mechanism\ncompetition, and the specialization of attention heads in handling competing\ninformation. We reproduce their results on both GPT-2 (Radford et al., 2019)\nand Pythia 6.9B (Biderman et al., 2023). We extend their work in three\nsignificant directions. First, we explore the generalizability of these\nfindings to even larger models by replicating the experiments on Llama 3.1 8B\n(Grattafiori et al., 2024), discovering greatly reduced attention head\nspecialization. Second, we investigate the impact of prompt structure by\nintroducing variations where we avoid repeating the counterfactual statement\nverbatim or we change the premise word, observing a marked decrease in the\nlogit for the counterfactual token. Finally, we test the validity of the\nauthors' claims for prompts of specific domains, discovering that certain\ncategories of prompts skew the results by providing the factual prediction\ntoken as part of the subject of the sentence. Overall, we find that the\nattention head ablation proposed in Ortu et al. (2024) is ineffective for\ndomains that are underrepresented in their dataset, and that the effectiveness\nvaries based on model architecture, prompt structure, domain and task.", "AI": {"tldr": "该论文复现了Ortu等人（2024）的研究，探讨语言模型中事实回忆与反事实上下文重复的机制竞争，并扩展了研究范围。", "motivation": "验证并扩展Ortu等人关于语言模型处理事实与反事实信息的机制竞争的研究。", "method": "复现实验并在更大模型（Llama 3.1 8B）上测试，探索提示结构变化和领域特定性的影响。", "result": "发现注意力头专业化在更大模型中减弱，提示结构显著影响结果，且领域特定性导致效果差异。", "conclusion": "注意力头消融方法的有效性受模型架构、提示结构、领域和任务的影响。"}}
{"id": "2506.22710", "pdf": "https://arxiv.org/pdf/2506.22710", "abs": "https://arxiv.org/abs/2506.22710", "authors": ["Jiang Yuan", "JI Ma", "Bo Wang", "Guanzhou Ke", "Weiming Hu"], "title": "LightBSR: Towards Lightweight Blind Super-Resolution via Discriminative Implicit Degradation Representation Learning", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Implicit degradation estimation-based blind super-resolution (IDE-BSR) hinges\non extracting the implicit degradation representation (IDR) of the LR image and\nadapting it to LR image features to guide HR detail restoration. Although\nIDE-BSR has shown potential in dealing with noise interference and complex\ndegradations, existing methods ignore the importance of IDR discriminability\nfor BSR and instead over-complicate the adaptation process to improve effect,\nresulting in a significant increase in the model's parameters and computations.\nIn this paper, we focus on the discriminability optimization of IDR and propose\na new powerful and lightweight BSR model termed LightBSR. Specifically, we\nemploy a knowledge distillation-based learning framework. We first introduce a\nwell-designed degradation-prior-constrained contrastive learning technique\nduring teacher stage to make the model more focused on distinguishing different\ndegradation types. Then we utilize a feature alignment technique to transfer\nthe degradation-related knowledge acquired by the teacher to the student for\npractical inferencing. Extensive experiments demonstrate the effectiveness of\nIDR discriminability-driven BSR model design. The proposed LightBSR can achieve\noutstanding performance with minimal complexity across a range of blind SR\ntasks. Our code is accessible at: https://github.com/MJ-NCEPU/LightBSR.", "AI": {"tldr": "论文提出了一种基于隐式退化估计的盲超分辨率方法（LightBSR），通过优化隐式退化表示（IDR）的区分性，设计了一个轻量级且高效的模型。", "motivation": "现有方法忽视了IDR区分性对盲超分辨率的重要性，且过度复杂化适应过程，导致模型参数和计算量显著增加。", "method": "采用基于知识蒸馏的学习框架，包括退化先验约束的对比学习技术和特征对齐技术。", "result": "LightBSR在多种盲超分辨率任务中表现出色，且复杂度最低。", "conclusion": "优化IDR区分性是设计高效盲超分辨率模型的关键，LightBSR在性能和效率上均优于现有方法。"}}
{"id": "2506.23689", "pdf": "https://arxiv.org/pdf/2506.23689", "abs": "https://arxiv.org/abs/2506.23689", "authors": ["Zihao Liu", "Xinhang Sui", "Yueran Song", "Siwen Wang"], "title": "PokéAI: A Goal-Generating, Battle-Optimizing Multi-agent System for Pokemon Red", "categories": ["cs.AI", "cs.MA"], "comment": null, "summary": "We introduce Pok\\'eAI, the first text-based, multi-agent large language model\n(LLM) framework designed to autonomously play and progress through Pok\\'emon\nRed. Our system consists of three specialized agents-Planning, Execution, and\nCritique-each with its own memory bank, role, and skill set. The Planning Agent\nfunctions as the central brain, generating tasks to progress through the game.\nThese tasks are then delegated to the Execution Agent, which carries them out\nwithin the game environment. Upon task completion, the Critique Agent evaluates\nthe outcome to determine whether the objective was successfully achieved. Once\nverification is complete, control returns to the Planning Agent, forming a\nclosed-loop decision-making system.\n  As a preliminary step, we developed a battle module within the Execution\nAgent. Our results show that the battle AI achieves an average win rate of\n80.8% across 50 wild encounters, only 6% lower than the performance of an\nexperienced human player. Furthermore, we find that a model's battle\nperformance correlates strongly with its LLM Arena score on language-related\ntasks, indicating a meaningful link between linguistic ability and strategic\nreasoning. Finally, our analysis of gameplay logs reveals that each LLM\nexhibits a unique playstyle, suggesting that individual models develop distinct\nstrategic behaviors.", "AI": {"tldr": "PokéAI是一个基于文本的多智能体大型语言模型框架，用于自主玩《Pokémon Red》。它包含规划、执行和评估三个智能体，形成一个闭环决策系统。初步结果显示，其战斗模块的胜率为80.8%，接近人类玩家水平。", "motivation": "开发一个能够自主玩《Pokémon Red》的AI系统，探索语言模型在游戏策略中的潜力。", "method": "采用三个智能体（规划、执行、评估）分工协作，规划生成任务，执行完成任务，评估验证结果。", "result": "战斗模块在50次野生遭遇中平均胜率为80.8%，接近人类玩家水平；语言能力与战斗表现相关。", "conclusion": "PokéAI展示了语言模型在游戏策略中的潜力，不同模型表现出独特的游戏风格。"}}
{"id": "2506.22978", "pdf": "https://arxiv.org/pdf/2506.22978", "abs": "https://arxiv.org/abs/2506.22978", "authors": ["Yida Zhao", "Hao Xve", "Xiang Hu", "Kewei Tu"], "title": "A Systematic Study of Compositional Syntactic Transformer Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Syntactic language models (SLMs) enhance Transformers by incorporating\nsyntactic biases through the modeling of linearized syntactic parse trees\nalongside surface sentences. This paper focuses on compositional SLMs that are\nbased on constituency parse trees and contain explicit bottom-up composition of\nconstituent representations. We identify key aspects of design choices in\nexisting compositional SLMs and propose a unified framework encompassing both\nexisting models and novel variants. We conduct a comprehensive empirical\nevaluation of all the variants in our framework across language modeling,\nsyntactic generalization, summarization, dialogue, and inference efficiency.\nBased on the experimental results, we make multiple recommendations on the\ndesign of compositional SLMs. Our code is released at\nhttps://github.com/zhaoyd1/compositional_SLMs.", "AI": {"tldr": "本文提出了一种统一的框架，用于分析基于成分句法树的组合式句法语言模型（SLMs），并通过实验评估了多种变体。", "motivation": "通过将句法偏置引入Transformer模型，提升语言模型在句法生成、摘要、对话等任务中的表现。", "method": "提出一个统一的框架，涵盖现有组合式SLMs及新变体，并进行多任务实验评估。", "result": "实验结果表明，组合式SLMs在语言建模、句法泛化等方面表现优异，并提出了设计建议。", "conclusion": "组合式SLMs能有效提升模型性能，框架为未来研究提供了指导。"}}
{"id": "2506.22718", "pdf": "https://arxiv.org/pdf/2506.22718", "abs": "https://arxiv.org/abs/2506.22718", "authors": ["Jun-Jee Chao", "Qingyuan Jiang", "Volkan Isler"], "title": "Part Segmentation and Motion Estimation for Articulated Objects with Dynamic 3D Gaussians", "categories": ["cs.CV"], "comment": null, "summary": "Part segmentation and motion estimation are two fundamental problems for\narticulated object motion analysis. In this paper, we present a method to solve\nthese two problems jointly from a sequence of observed point clouds of a single\narticulated object. The main challenge in our problem setting is that the point\nclouds are not assumed to be generated by a fixed set of moving points.\nInstead, each point cloud in the sequence could be an arbitrary sampling of the\nobject surface at that particular time step. Such scenarios occur when the\nobject undergoes major occlusions, or if the dataset is collected using\nmeasurements from multiple sensors asynchronously. In these scenarios, methods\nthat rely on tracking point correspondences are not appropriate. We present an\nalternative approach based on a compact but effective representation where we\nrepresent the object as a collection of simple building blocks modeled as 3D\nGaussians. We parameterize the Gaussians with time-dependent rotations,\ntranslations, and scales that are shared across all time steps. With our\nrepresentation, part segmentation can be achieved by building correspondences\nbetween the observed points and the Gaussians. Moreover, the transformation of\neach point across time can be obtained by following the poses of the assigned\nGaussian (even when the point is not observed). Experiments show that our\nmethod outperforms existing methods that solely rely on finding point\ncorrespondences. Additionally, we extend existing datasets to emulate\nreal-world scenarios by considering viewpoint occlusions. We further\ndemonstrate that our method is more robust to missing points as compared to\nexisting approaches on these challenging datasets, even when some parts are\ncompletely occluded in some time-steps. Notably, our part segmentation\nperformance outperforms the state-of-the-art method by 13% on point clouds with\nocclusions.", "AI": {"tldr": "提出一种联合解决部分分割和运动估计的方法，通过3D高斯表示处理动态点云序列，适用于点云非固定采样场景。", "motivation": "解决动态点云序列中部分分割和运动估计的联合问题，特别是在点云非固定采样（如遮挡或多传感器异步测量）场景下。", "method": "使用3D高斯作为基本构建块表示物体，参数化时间依赖的旋转、平移和缩放，通过点与高斯的对应关系实现分割和运动估计。", "result": "在遮挡场景下，部分分割性能优于现有方法13%，且对缺失点更鲁棒。", "conclusion": "该方法在动态点云分析中表现优越，尤其适用于复杂场景。"}}
{"id": "2506.23692", "pdf": "https://arxiv.org/pdf/2506.23692", "abs": "https://arxiv.org/abs/2506.23692", "authors": ["Boyuan Zheng", "Zerui Fang", "Zhe Xu", "Rui Wang", "Yiwen Chen", "Cunshi Wang", "Mengwei Qu", "Lei Lei", "Zhen Feng", "Yan Liu", "Yuyang Li", "Mingzhou Tan", "Jiaji Wu", "Jianwei Shuai", "Jia Li", "Fangfu Ye"], "title": "Agent4S: The Transformation of Research Paradigms from the Perspective of Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "While AI for Science (AI4S) serves as an analytical tool in the current\nresearch paradigm, it doesn't solve its core inefficiency. We propose \"Agent\nfor Science\" (Agent4S)-the use of LLM-driven agents to automate the entire\nresearch workflow-as the true Fifth Scientific Paradigm. This paper introduces\na five-level classification for Agent4S, outlining a clear roadmap from simple\ntask automation to fully autonomous, collaborative \"AI Scientists.\" This\nframework defines the next revolutionary step in scientific discovery.", "AI": {"tldr": "论文提出用LLM驱动的Agent4S（Agent for Science）作为第五科学范式，通过五级分类框架实现从简单任务自动化到完全自主协作的‘AI科学家’。", "motivation": "当前AI4S（AI for Science）作为分析工具未能解决科研核心效率问题，因此提出Agent4S以自动化整个科研流程。", "method": "引入五级分类框架，描述从基础任务自动化到完全自主协作的AI科学家的演进路径。", "result": "提出Agent4S作为第五科学范式的革命性框架。", "conclusion": "Agent4S是科学发现的下一步革命性进展。"}}
{"id": "2506.23046", "pdf": "https://arxiv.org/pdf/2506.23046", "abs": "https://arxiv.org/abs/2506.23046", "authors": ["Xianzhe Fan", "Xuhui Zhou", "Chuanyang Jin", "Kolby Nottingham", "Hao Zhu", "Maarten Sap"], "title": "SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions", "categories": ["cs.CL", "cs.AI", "cs.CV", "cs.RO"], "comment": "23 pages, 6 figures", "summary": "Humans continuously infer the states, goals, and behaviors of others by\nperceiving their surroundings in dynamic, real-world social interactions.\nHowever, most Theory of Mind (ToM) benchmarks only evaluate static, text-based\nscenarios, which have a significant gap compared to real interactions. We\npropose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in\nembodied multi-agent complex social interactions. This benchmark is based on\nrich multimodal interaction data generated by the interaction environment SoMi,\ncovering diverse crafting goals and social relationships. Our framework\nsupports multi-level evaluation: (1) first-person evaluation provides\nmultimodal (visual, dialogue, action, etc.) input from a first-person\nperspective during a task for real-time state inference, (2) third-person\nevaluation provides complete third-person perspective video and text records\nafter a task for goal and behavior inference. This evaluation method allows for\na more comprehensive examination of a model's ToM capabilities from both the\nsubjective immediate experience and the objective global observation. We\nconstructed a challenging dataset containing 35 third-person perspective\nvideos, 363 first-person perspective images, and 1225 expert-annotated\nmultiple-choice questions (three options). On this dataset, we systematically\nevaluated the performance of human subjects and several state-of-the-art large\nvision-language models (LVLMs). The results show that LVLMs perform\nsignificantly worse than humans on SoMi-ToM: the average accuracy gap between\nhumans and models is 40.1% in first-person evaluation and 26.4% in third-person\nevaluation. This indicates that future LVLMs need to further improve their ToM\ncapabilities in embodied, complex social interactions.", "AI": {"tldr": "SoMi-ToM是一个评估多智能体复杂社交互动中多视角心智理论（ToM）的基准，填补了静态文本场景与实际动态互动之间的差距。", "motivation": "现有ToM基准多基于静态文本场景，与实际动态社交互动存在显著差距，因此需要更贴近现实的评估方法。", "method": "基于SoMi环境生成的多模态互动数据，设计了第一人称和第三人称视角的多层次评估框架，包含视频、图像和专家标注问题。", "result": "人类在SoMi-ToM上的表现显著优于大型视觉语言模型（LVLMs），准确率差距分别为40.1%（第一人称）和26.4%（第三人称）。", "conclusion": "未来LVLMs需提升在具身复杂社交互动中的ToM能力，SoMi-ToM为此提供了有效的评估工具。"}}
{"id": "2506.22720", "pdf": "https://arxiv.org/pdf/2506.22720", "abs": "https://arxiv.org/abs/2506.22720", "authors": ["Jinghao Wang", "Zhang Li", "Zi Wang", "Banglei Guan", "Yang Shang", "Qifeng Yu"], "title": "Deterministic Object Pose Confidence Region Estimation", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "6D pose confidence region estimation has emerged as a critical direction,\naiming to perform uncertainty quantification for assessing the reliability of\nestimated poses. However, current sampling-based approach suffers from critical\nlimitations that severely impede their practical deployment: 1) the sampling\nspeed significantly decreases as the number of samples increases. 2) the\nderived confidence regions are often excessively large. To address these\nchallenges, we propose a deterministic and efficient method for estimating pose\nconfidence regions. Our approach uses inductive conformal prediction to\ncalibrate the deterministically regressed Gaussian keypoint distributions into\n2D keypoint confidence regions. We then leverage the implicit function theorem\nto propagate these keypoint confidence regions directly into 6D pose confidence\nregions. This method avoids the inefficiency and inflated region sizes\nassociated with sampling and ensembling. It provides compact confidence regions\nthat cover the ground-truth poses with a user-defined confidence level.\nExperimental results on the LineMOD Occlusion and SPEED datasets show that our\nmethod achieves higher pose estimation accuracy with reduced computational\ntime. For the same coverage rate, our method yields significantly smaller\nconfidence region volumes, reducing them by up to 99.9\\% for rotations and\n99.8\\% for translations. The code will be available soon.", "AI": {"tldr": "提出一种确定性方法，用于高效估计6D姿态置信区域，解决采样方法速度慢和区域过大的问题。", "motivation": "当前基于采样的方法在速度和置信区域大小上存在不足，限制了实际应用。", "method": "使用归纳共形预测校准高斯关键点分布，并通过隐函数定理直接传播到6D姿态置信区域。", "result": "在LineMOD Occlusion和SPEED数据集上，方法提高了姿态估计精度，减少了计算时间，置信区域体积显著缩小。", "conclusion": "该方法高效且提供紧凑的置信区域，适用于实际部署。"}}
{"id": "2506.23703", "pdf": "https://arxiv.org/pdf/2506.23703", "abs": "https://arxiv.org/abs/2506.23703", "authors": ["Lars Ullrich", "Walter Zimmer", "Ross Greer", "Knut Graichen", "Alois C. Knoll", "Mohan Trivedi"], "title": "A New Perspective On AI Safety Through Control Theory Methodologies", "categories": ["cs.AI"], "comment": "Accepted to be published as part of the 2025 IEEE Open Journal of\n  Intelligent Transportation Systems (OJ-ITS)", "summary": "While artificial intelligence (AI) is advancing rapidly and mastering\nincreasingly complex problems with astonishing performance, the safety\nassurance of such systems is a major concern. Particularly in the context of\nsafety-critical, real-world cyber-physical systems, AI promises to achieve a\nnew level of autonomy but is hampered by a lack of safety assurance. While\ndata-driven control takes up recent developments in AI to improve control\nsystems, control theory in general could be leveraged to improve AI safety.\nTherefore, this article outlines a new perspective on AI safety based on an\ninterdisciplinary interpretation of the underlying data-generation process and\nthe respective abstraction by AI systems in a system theory-inspired and system\nanalysis-driven manner. In this context, the new perspective, also referred to\nas data control, aims to stimulate AI engineering to take advantage of existing\nsafety analysis and assurance in an interdisciplinary way to drive the paradigm\nof data control. Following a top-down approach, a generic foundation for safety\nanalysis and assurance is outlined at an abstract level that can be refined for\nspecific AI systems and applications and is prepared for future innovation.", "AI": {"tldr": "论文提出了一种基于系统理论和数据分析的新视角，旨在通过跨学科方法提升AI安全性，特别是在安全关键的实际系统中。", "motivation": "AI在复杂问题上的快速进步带来了安全性挑战，尤其是在安全关键的实际系统中，缺乏安全保障成为主要障碍。", "method": "采用系统理论和系统分析驱动的方法，结合数据生成过程和AI系统的抽象，提出数据控制的新视角。", "result": "提出了一种通用的安全分析和保障基础，适用于特定AI系统和应用，并为未来创新做好准备。", "conclusion": "通过跨学科方法，数据控制的新视角有望推动AI工程利用现有安全保障，提升AI系统的安全性。"}}
{"id": "2506.23051", "pdf": "https://arxiv.org/pdf/2506.23051", "abs": "https://arxiv.org/abs/2506.23051", "authors": ["João Lucas Luz Lima Sarcinelli", "Marina Lages Gonçalves Teixeira", "Jade Bortot de Paiva", "Diego Furtado Silva"], "title": "MariNER: A Dataset for Historical Brazilian Portuguese Named Entity Recognition", "categories": ["cs.CL"], "comment": null, "summary": "Named Entity Recognition (NER) is a fundamental Natural Language Processing\n(NLP) task that aims to identify and classify entity mentions in texts across\ndifferent categories. While languages such as English possess a large number of\nhigh-quality resources for this task, Brazilian Portuguese still lacks in\nquantity of gold-standard NER datasets, especially when considering specific\ndomains. Particularly, this paper considers the importance of NER for analyzing\nhistorical texts in the context of digital humanities. To address this gap,\nthis work outlines the construction of MariNER: \\textit{Mapeamento e\nAnota\\c{c}\\~oes de Registros hIst\\'oricos para NER} (Mapping and Annotation of\nHistorical Records for NER), the first gold-standard dataset for early\n20th-century Brazilian Portuguese, with more than 9,000 manually annotated\nsentences. We also assess and compare the performance of state-of-the-art NER\nmodels for the dataset.", "AI": {"tldr": "本文介绍了MariNER，这是首个针对20世纪早期巴西葡萄牙语的历史文本的黄金标准NER数据集，包含9000多个手动标注的句子，并评估了最先进的NER模型在该数据集上的表现。", "motivation": "巴西葡萄牙语在特定领域缺乏高质量的NER数据集，尤其是在数字人文学科中分析历史文本时。", "method": "构建了MariNER数据集，包含9000多个手动标注的句子，并评估了最先进的NER模型。", "result": "MariNER成为首个针对20世纪早期巴西葡萄牙语历史文本的黄金标准数据集。", "conclusion": "该研究填补了巴西葡萄牙语历史文本NER数据集的空白，并验证了现有模型在该领域的适用性。"}}
{"id": "2506.22726", "pdf": "https://arxiv.org/pdf/2506.22726", "abs": "https://arxiv.org/abs/2506.22726", "authors": ["Yu Zhang", "Xi Zhang", "Hualin zhou", "Xinyuan Chen", "Shang Gao", "Hong Jia", "Jianfei Yang", "Yuankai Qi", "Tao Gu"], "title": "XTransfer: Cross-Modality Model Transfer for Human Sensing with Few Data at the Edge", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Deep learning for human sensing on edge systems offers significant\nopportunities for smart applications. However, its training and development are\nhindered by the limited availability of sensor data and resource constraints of\nedge systems. Current methods that rely on transferring pre-trained models\noften encounter issues such as modality shift and high resource demands,\nresulting in substantial accuracy loss, resource overhead, and poor\nadaptability across different sensing applications. In this paper, we propose\nXTransfer, a first-of-its-kind method for resource-efficient, modality-agnostic\nmodel transfer. XTransfer freely leverages single or multiple pre-trained\nmodels and transfers knowledge across different modalities by (i) model\nrepairing that safely repairs modality shift in pre-trained model layers with\nonly few sensor data, and (ii) layer recombining that efficiently searches and\nrecombines layers of interest from source models in a layer-wise manner to\ncreate compact models. We benchmark various baselines across diverse human\nsensing datasets spanning different modalities. Comprehensive results\ndemonstrate that XTransfer achieves state-of-the-art performance on human\nsensing tasks while significantly reducing the costs of sensor data collection,\nmodel training, and edge deployment.", "AI": {"tldr": "XTransfer是一种资源高效、模态无关的模型迁移方法，通过模型修复和层重组解决边缘系统中深度学习模型的模态偏移和资源限制问题。", "motivation": "边缘系统中深度学习模型的训练和开发受限于传感器数据不足和资源限制，现有方法存在模态偏移、资源需求高和适应性差的问题。", "method": "XTransfer通过模型修复（修复模态偏移）和层重组（高效搜索和重组源模型层）实现资源高效的模型迁移。", "result": "XTransfer在多种人类感知任务中表现优异，显著降低了数据收集、模型训练和边缘部署的成本。", "conclusion": "XTransfer为边缘系统中的人类感知任务提供了一种高效、适应性强的解决方案。"}}
{"id": "2506.23706", "pdf": "https://arxiv.org/pdf/2506.23706", "abs": "https://arxiv.org/abs/2506.23706", "authors": ["Christoph Schnabl", "Daniel Hugenroth", "Bill Marino", "Alastair R. Beresford"], "title": "Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments", "categories": ["cs.AI", "cs.CL", "cs.CR"], "comment": "ICML 2024 Workshop TAIG", "summary": "Benchmarks are important measures to evaluate safety and compliance of AI\nmodels at scale. However, they typically do not offer verifiable results and\nlack confidentiality for model IP and benchmark datasets. We propose Attestable\nAudits, which run inside Trusted Execution Environments and enable users to\nverify interaction with a compliant AI model. Our work protects sensitive data\neven when model provider and auditor do not trust each other. This addresses\nverification challenges raised in recent AI governance frameworks. We build a\nprototype demonstrating feasibility on typical audit benchmarks against\nLlama-3.1.", "AI": {"tldr": "提出了一种名为“可验证审计”的方法，利用可信执行环境确保AI模型的合规性验证，同时保护模型和数据隐私。", "motivation": "解决现有基准测试无法提供可验证结果且缺乏对模型和数据隐私保护的问题。", "method": "在可信执行环境中运行可验证审计，确保用户能够验证与合规AI模型的交互。", "result": "原型验证了该方法在典型审计基准（如Llama-3.1）上的可行性。", "conclusion": "可验证审计为AI治理框架中的验证挑战提供了解决方案，同时保护了敏感数据。"}}
{"id": "2506.23056", "pdf": "https://arxiv.org/pdf/2506.23056", "abs": "https://arxiv.org/abs/2506.23056", "authors": ["Xiang Zhuang", "Bin Wu", "Jiyu Cui", "Kehua Feng", "Xiaotong Li", "Huabin Xing", "Keyan Ding", "Qiang Zhang", "Huajun Chen"], "title": "Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning", "categories": ["cs.CL"], "comment": "ACL 2025 Main", "summary": "Molecular structure elucidation involves deducing a molecule's structure from\nvarious types of spectral data, which is crucial in chemical experimental\nanalysis. While large language models (LLMs) have shown remarkable proficiency\nin analyzing and reasoning through complex tasks, they still encounter\nsubstantial challenges in molecular structure elucidation. We identify that\nthese challenges largely stem from LLMs' limited grasp of specialized chemical\nknowledge. In this work, we introduce a Knowledge-enhanced reasoning framework\nfor Molecular Structure Elucidation (K-MSE), leveraging Monte Carlo Tree Search\nfor test-time scaling as a plugin. Specifically, we construct an external\nmolecular substructure knowledge base to extend the LLMs' coverage of the\nchemical structure space. Furthermore, we design a specialized\nmolecule-spectrum scorer to act as a reward model for the reasoning process,\naddressing the issue of inaccurate solution evaluation in LLMs. Experimental\nresults show that our approach significantly boosts performance, particularly\ngaining more than 20% improvement on both GPT-4o-mini and GPT-4o. Our code is\navailable at https://github.com/HICAI-ZJU/K-MSE.", "AI": {"tldr": "论文提出了一种知识增强的分子结构解析框架（K-MSE），通过结合外部分子子结构知识库和蒙特卡洛树搜索，显著提升了大型语言模型（LLMs）在分子结构解析任务中的性能。", "motivation": "大型语言模型在分子结构解析任务中表现不佳，主要原因是缺乏专业化学知识。为了解决这一问题，作者提出了知识增强的框架。", "method": "构建外部分子子结构知识库以扩展LLMs的化学结构覆盖范围，并设计分子-光谱评分器作为推理过程的奖励模型。", "result": "实验结果显示，该方法显著提升了性能，在GPT-4o-mini和GPT-4o上均获得了超过20%的改进。", "conclusion": "K-MSE框架通过知识增强和优化推理过程，有效解决了LLMs在分子结构解析中的局限性。"}}
{"id": "2506.22736", "pdf": "https://arxiv.org/pdf/2506.22736", "abs": "https://arxiv.org/abs/2506.22736", "authors": ["Dayong Su", "Yafei Zhang", "Huafeng Li", "Jinxing Li", "Yu Liu"], "title": "UniFuse: A Unified All-in-One Framework for Multi-Modal Medical Image Fusion Under Diverse Degradations and Misalignments", "categories": ["cs.CV"], "comment": "Accepted by ICCV2025", "summary": "Current multimodal medical image fusion typically assumes that source images\nare of high quality and perfectly aligned at the pixel level. Its effectiveness\nheavily relies on these conditions and often deteriorates when handling\nmisaligned or degraded medical images. To address this, we propose UniFuse, a\ngeneral fusion framework. By embedding a degradation-aware prompt learning\nmodule, UniFuse seamlessly integrates multi-directional information from input\nimages and correlates cross-modal alignment with restoration, enabling joint\noptimization of both tasks within a unified framework. Additionally, we design\nan Omni Unified Feature Representation scheme, which leverages Spatial Mamba to\nencode multi-directional features and mitigate modality differences in feature\nalignment. To enable simultaneous restoration and fusion within an All-in-One\nconfiguration, we propose a Universal Feature Restoration & Fusion module,\nincorporating the Adaptive LoRA Synergistic Network (ALSN) based on LoRA\nprinciples. By leveraging ALSN's adaptive feature representation along with\ndegradation-type guidance, we enable joint restoration and fusion within a\nsingle-stage framework. Compared to staged approaches, UniFuse unifies\nalignment, restoration, and fusion within a single framework. Experimental\nresults across multiple datasets demonstrate the method's effectiveness and\nsignificant advantages over existing approaches.", "AI": {"tldr": "UniFuse是一个多模态医学图像融合框架，通过降解感知提示学习和统一特征表示，解决了图像未对齐或质量差的问题，实现了对齐、恢复和融合的联合优化。", "motivation": "当前多模态医学图像融合方法依赖于高质量和对齐良好的输入图像，但在处理未对齐或质量差的图像时效果不佳。", "method": "提出UniFuse框架，包括降解感知提示学习模块、Omni统一特征表示方案和通用特征恢复与融合模块。", "result": "实验结果表明，UniFuse在多个数据集上优于现有方法。", "conclusion": "UniFuse通过统一框架实现了对齐、恢复和融合的联合优化，显著提升了融合效果。"}}
{"id": "2506.23773", "pdf": "https://arxiv.org/pdf/2506.23773", "abs": "https://arxiv.org/abs/2506.23773", "authors": ["Stefano M. Nicoletti", "Mariëlle Stoelinga"], "title": "BayesL: Towards a Logical Framework for Bayesian Networks", "categories": ["cs.AI", "cs.LO"], "comment": null, "summary": "We introduce BayesL, a novel logical framework for specifying, querying, and\nverifying the behaviour of Bayesian networks (BNs). BayesL (pronounced \"Basil\")\nis a structured language that allows for the creation of queries over BNs. It\nfacilitates versatile reasoning concerning causal and evidence-based\nrelationships, and permits comprehensive what-if scenario evaluations without\nthe need for manual modifications to the model.", "AI": {"tldr": "BayesL是一个新的逻辑框架，用于指定、查询和验证贝叶斯网络的行为。", "motivation": "提供一个结构化语言，支持对贝叶斯网络进行灵活查询和因果推理，避免手动修改模型。", "method": "开发BayesL语言，支持创建查询和进行因果与证据关系的推理。", "result": "BayesL能够全面评估假设场景，无需手动调整模型。", "conclusion": "BayesL为贝叶斯网络的行为分析提供了高效且灵活的工具。"}}
{"id": "2506.23071", "pdf": "https://arxiv.org/pdf/2506.23071", "abs": "https://arxiv.org/abs/2506.23071", "authors": ["Zhengren Wang", "Bozhou Li", "Dongwen Yao", "Wentao Zhang"], "title": "Text2VectorSQL: Bridging Text-to-SQL and Vector Search for Unified Natural Language Queries", "categories": ["cs.CL"], "comment": "Work in progess", "summary": "While Text-to-SQL enables natural language interaction with structured\ndatabases, its effectiveness diminishes with unstructured data or ambiguous\nqueries due to rigid syntax and limited expressiveness. Concurrently, vector\nsearch has emerged as a powerful paradigm for semantic retrieval, particularly\nfor unstructured data. However, existing VectorSQL implementations still rely\nheavily on manual crafting and lack tailored evaluation frameworks, leaving a\nsignificant gap between theoretical potential and practical deployment. To\nbridge these complementary paradigms, we introduces Text2VectorSQL, a novel\nframework unifying Text-to-SQL and vector search to overcome expressiveness\nconstraints and support more diverse and holistical natural language queries.\nSpecifically, Text2VectorSQL enables semantic filtering, multi-modal matching,\nand retrieval acceleration. For evaluation, we build vector index on\nappropriate columns, extend user queries with semantic search, and annotate\nground truths via an automatic pipeline with expert review. Furthermore, we\ndevelop dedicated Text2VectorSQL models with synthetic data, demonstrating\nsignificant performance improvements over baseline methods. Our work\nestablishes the foundation for the Text2VectorSQL task, paving the way for more\nversatile and intuitive database interfaces. The repository will be publicly\navailable at https://github.com/Open-DataFlow/Text2VectorSQL.", "AI": {"tldr": "Text2VectorSQL框架结合Text-to-SQL和向量搜索，解决非结构化数据和模糊查询的问题，支持语义过滤和多模态匹配，性能显著提升。", "motivation": "Text-to-SQL在处理非结构化数据或模糊查询时效果有限，而向量搜索虽强大但依赖人工实现。两者结合可弥补各自不足。", "method": "提出Text2VectorSQL框架，支持语义过滤和多模态匹配，通过向量索引和语义扩展优化查询，并开发专用模型。", "result": "性能显著优于基线方法，为Text2VectorSQL任务奠定基础。", "conclusion": "Text2VectorSQL为更灵活直观的数据库接口铺平道路，代码将开源。"}}
{"id": "2506.22749", "pdf": "https://arxiv.org/pdf/2506.22749", "abs": "https://arxiv.org/abs/2506.22749", "authors": ["Yun Zhang", "Feifan Chen", "Na Li", "Zhiwei Guo", "Xu Wang", "Fen Miao", "Sam Kwong"], "title": "Deep Learning based Joint Geometry and Attribute Up-sampling for Large-Scale Colored Point Clouds", "categories": ["cs.CV"], "comment": null, "summary": "Colored point cloud, which includes geometry and attribute components, is a\nmainstream representation enabling realistic and immersive 3D applications. To\ngenerate large-scale and denser colored point clouds, we propose a deep\nlearning-based Joint Geometry and Attribute Up-sampling (JGAU) method that\nlearns to model both geometry and attribute patterns while leveraging spatial\nattribute correlations. First, we establish and release a large-scale dataset\nfor colored point cloud up-sampling called SYSU-PCUD, containing 121\nlarge-scale colored point clouds with diverse geometry and attribute\ncomplexities across six categories and four sampling rates. Second, to improve\nthe quality of up-sampled point clouds, we propose a deep learning-based JGAU\nframework that jointly up-samples geometry and attributes. It consists of a\ngeometry up-sampling network and an attribute up-sampling network, where the\nlatter leverages the up-sampled auxiliary geometry to model neighborhood\ncorrelations of the attributes. Third, we propose two coarse attribute\nup-sampling methods, Geometric Distance Weighted Attribute Interpolation\n(GDWAI) and Deep Learning-based Attribute Interpolation (DLAI), to generate\ncoarse up-sampled attributes for each point. Then, an attribute enhancement\nmodule is introduced to refine these up-sampled attributes and produce\nhigh-quality point clouds by further exploiting intrinsic attribute and\ngeometry patterns. Extensive experiments show that the Peak Signal-to-Noise\nRatio (PSNR) achieved by the proposed JGAU method is 33.90 decibels, 32.10\ndecibels, 31.10 decibels, and 30.39 decibels for up-sampling rates of 4 times,\n8 times, 12 times, and 16 times, respectively. Compared to state-of-the-art\nmethods, JGAU achieves average PSNR gains of 2.32 decibels, 2.47 decibels, 2.28\ndecibels, and 2.11 decibels at these four up-sampling rates, demonstrating\nsignificant improvement.", "AI": {"tldr": "提出了一种基于深度学习的联合几何和属性上采样方法（JGAU），用于生成大规模、高密度的彩色点云，并在公开数据集上验证了其优越性能。", "motivation": "彩色点云是3D应用的主流表示，但现有方法难以同时高效处理几何和属性上采样问题。", "method": "通过几何上采样网络和属性上采样网络联合建模，结合两种粗属性上采样方法和属性增强模块。", "result": "在4倍、8倍、12倍和16倍上采样率下，PSNR分别达到33.90、32.10、31.10和30.39分贝，显著优于现有方法。", "conclusion": "JGAU方法在彩色点云上采样任务中表现出色，为3D应用提供了高质量的点云数据。"}}
{"id": "2506.23784", "pdf": "https://arxiv.org/pdf/2506.23784", "abs": "https://arxiv.org/abs/2506.23784", "authors": ["Parosh Aziz Abdulla", "Mohamed Faouzi Atig", "Julie Cailler", "Chencheng Liang", "Philipp Rümmer"], "title": "When GNNs Met a Word Equations Solver: Learning to Rank Equations (Extended Technical Report)", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Nielsen transformation is a standard approach for solving word equations: by\nrepeatedly splitting equations and applying simplification steps, equations are\nrewritten until a solution is reached. When solving a conjunction of word\nequations in this way, the performance of the solver will depend considerably\non the order in which equations are processed. In this work, the use of Graph\nNeural Networks (GNNs) for ranking word equations before and during the solving\nprocess is explored. For this, a novel graph-based representation for word\nequations is presented, preserving global information across conjuncts,\nenabling the GNN to have a holistic view during ranking. To handle the variable\nnumber of conjuncts, three approaches to adapt a multi-classification task to\nthe problem of ranking equations are proposed. The training of the GNN is done\nwith the help of minimum unsatisfiable subsets (MUSes) of word equations. The\nexperimental results show that, compared to state-of-the-art string solvers,\nthe new framework solves more problems in benchmarks where each variable\nappears at most once in each equation.", "AI": {"tldr": "使用图神经网络（GNN）对词方程进行排序，以提高求解效率。", "motivation": "传统Nielsen变换求解词方程时，处理顺序对性能影响较大，需优化排序方法。", "method": "提出基于图的词方程表示方法，利用GNN进行全局排序，并采用三种多分类任务策略处理变量数不定的问题。", "result": "实验表明，新框架在变量每方程最多出现一次的基准测试中优于现有求解器。", "conclusion": "GNN排序方法能有效提升词方程求解性能。"}}
{"id": "2506.23101", "pdf": "https://arxiv.org/pdf/2506.23101", "abs": "https://arxiv.org/abs/2506.23101", "authors": ["Yue Xu", "Wenjie Wang"], "title": "From Individuals to Interactions: Benchmarking Gender Bias in Multimodal Large Language Models from the Lens of Social Relationship", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Multimodal large language models (MLLMs) have shown impressive capabilities\nacross tasks involving both visual and textual modalities. However, growing\nconcerns remain about their potential to encode and amplify gender bias,\nparticularly in socially sensitive applications. Existing benchmarks\npredominantly evaluate bias in isolated scenarios, overlooking how bias may\nemerge subtly through interpersonal interactions. We fill this gap by going\nbeyond single-entity evaluation and instead focusing on a deeper examination of\nrelational and contextual gender bias in dual-individual interactions. We\nintroduce Genres, a novel benchmark designed to evaluate gender bias in MLLMs\nthrough the lens of social relationships in generated narratives. Genres\nassesses gender bias through a dual-character profile and narrative generation\ntask that captures rich interpersonal dynamics and supports a fine-grained bias\nevaluation suite across multiple dimensions. Experiments on both open- and\nclosed-source MLLMs reveal persistent, context-sensitive gender biases that are\nnot evident in single-character settings. Our findings underscore the\nimportance of relationship-aware benchmarks for diagnosing subtle,\ninteraction-driven gender bias in MLLMs and provide actionable insights for\nfuture bias mitigation.", "AI": {"tldr": "Genres 是一个新基准，用于评估多模态大语言模型（MLLMs）在双人互动中的性别偏见，揭示单场景评估中未发现的上下文敏感偏见。", "motivation": "现有基准主要评估孤立场景中的性别偏见，忽视了人际互动中可能出现的微妙偏见，因此需要更深入的关系和上下文偏见分析。", "method": "通过双角色配置和叙事生成任务，Genres 评估 MLLMs 在社交关系中的性别偏见，支持多维度细粒度分析。", "result": "实验表明，MLLMs 在双人互动中存在持续且上下文敏感的性别偏见，这些偏见在单角色场景中不明显。", "conclusion": "关系感知的基准对诊断 MLLMs 中微妙的互动驱动性别偏见至关重要，并为未来偏见缓解提供了可操作的见解。"}}
{"id": "2506.22753", "pdf": "https://arxiv.org/pdf/2506.22753", "abs": "https://arxiv.org/abs/2506.22753", "authors": ["Jianing Zhang", "Jiayi Zhu", "Feiyu Ji", "Xiaokang Yang", "Xiaoyun Yuan"], "title": "Degradation-Modeled Multipath Diffusion for Tunable Metalens Photography", "categories": ["cs.CV"], "comment": null, "summary": "Metalenses offer significant potential for ultra-compact computational\nimaging but face challenges from complex optical degradation and computational\nrestoration difficulties. Existing methods typically rely on precise optical\ncalibration or massive paired datasets, which are non-trivial for real-world\nimaging systems. Furthermore, a lack of control over the inference process\noften results in undesirable hallucinated artifacts. We introduce\nDegradation-Modeled Multipath Diffusion for tunable metalens photography,\nleveraging powerful natural image priors from pretrained models instead of\nlarge datasets. Our framework uses positive, neutral, and negative-prompt paths\nto balance high-frequency detail generation, structural fidelity, and\nsuppression of metalens-specific degradation, alongside \\textit{pseudo} data\naugmentation. A tunable decoder enables controlled trade-offs between fidelity\nand perceptual quality. Additionally, a spatially varying degradation-aware\nattention (SVDA) module adaptively models complex optical and sensor-induced\ndegradation. Finally, we design and build a millimeter-scale MetaCamera for\nreal-world validation. Extensive results show that our approach outperforms\nstate-of-the-art methods, achieving high-fidelity and sharp image\nreconstruction. More materials: https://dmdiff.github.io/.", "AI": {"tldr": "提出了一种基于预训练模型的金属透镜摄影方法，通过多路径扩散和伪数据增强，平衡细节生成与结构保真度，同时抑制伪影。", "motivation": "金属透镜在计算成像中潜力巨大，但面临光学退化和计算恢复的挑战，现有方法依赖精确校准或大量配对数据，难以实际应用。", "method": "采用多路径扩散框架，结合正、中、负提示路径，伪数据增强和可调解码器，并引入空间变化退化感知注意力模块（SVDA）。", "result": "实验表明，该方法优于现有技术，实现了高保真和清晰的图像重建。", "conclusion": "该方法通过预训练模型和可控推理过程，有效解决了金属透镜摄影中的退化问题，具有实际应用潜力。"}}
{"id": "2506.23793", "pdf": "https://arxiv.org/pdf/2506.23793", "abs": "https://arxiv.org/abs/2506.23793", "authors": ["Anton Andreychuk", "Konstantin Yakovlev", "Aleksandr Panov", "Alexey Skrynnik"], "title": "Advancing Learnable Multi-Agent Pathfinding Solvers with Active Fine-Tuning", "categories": ["cs.AI", "cs.LG", "cs.MA"], "comment": null, "summary": "Multi-agent pathfinding (MAPF) is a common abstraction of multi-robot\ntrajectory planning problems, where multiple homogeneous robots simultaneously\nmove in the shared environment. While solving MAPF optimally has been proven to\nbe NP-hard, scalable, and efficient, solvers are vital for real-world\napplications like logistics, search-and-rescue, etc. To this end, decentralized\nsuboptimal MAPF solvers that leverage machine learning have come on stage.\nBuilding on the success of the recently introduced MAPF-GPT, a pure imitation\nlearning solver, we introduce MAPF-GPT-DDG. This novel approach effectively\nfine-tunes the pre-trained MAPF model using centralized expert data. Leveraging\na novel delta-data generation mechanism, MAPF-GPT-DDG accelerates training\nwhile significantly improving performance at test time. Our experiments\ndemonstrate that MAPF-GPT-DDG surpasses all existing learning-based MAPF\nsolvers, including the original MAPF-GPT, regarding solution quality across\nmany testing scenarios. Remarkably, it can work with MAPF instances involving\nup to 1 million agents in a single environment, setting a new milestone for\nscalability in MAPF domains.", "AI": {"tldr": "MAPF-GPT-DDG是一种基于机器学习的多智能体路径规划（MAPF）求解器，通过改进预训练模型和引入新的数据生成机制，显著提升了性能和可扩展性。", "motivation": "解决多机器人轨迹规划问题，特别是在物流、搜救等实际应用中需要高效且可扩展的MAPF求解器。", "method": "利用集中式专家数据对预训练的MAPF-GPT模型进行微调，并采用新的delta-data生成机制加速训练。", "result": "MAPF-GPT-DDG在测试场景中超越了所有现有基于学习的MAPF求解器，包括原始MAPF-GPT，并能处理多达100万个智能体的MAPF实例。", "conclusion": "MAPF-GPT-DDG在性能和可扩展性上取得了显著突破，为MAPF领域树立了新的里程碑。"}}
{"id": "2506.23111", "pdf": "https://arxiv.org/pdf/2506.23111", "abs": "https://arxiv.org/abs/2506.23111", "authors": ["Janki Atul Nawale", "Mohammed Safi Ur Rahman Khan", "Janani D", "Mansi Gupta", "Danish Pruthi", "Mitesh M. Khapra"], "title": "FairI Tales: Evaluation of Fairness in Indian Contexts with a Focus on Bias and Stereotypes", "categories": ["cs.CL"], "comment": "Accepted in ACL 2025", "summary": "Existing studies on fairness are largely Western-focused, making them\ninadequate for culturally diverse countries such as India. To address this gap,\nwe introduce INDIC-BIAS, a comprehensive India-centric benchmark designed to\nevaluate fairness of LLMs across 85 identity groups encompassing diverse\ncastes, religions, regions, and tribes. We first consult domain experts to\ncurate over 1,800 socio-cultural topics spanning behaviors and situations,\nwhere biases and stereotypes are likely to emerge. Grounded in these topics, we\ngenerate and manually validate 20,000 real-world scenario templates to probe\nLLMs for fairness. We structure these templates into three evaluation tasks:\nplausibility, judgment, and generation. Our evaluation of 14 popular LLMs on\nthese tasks reveals strong negative biases against marginalized identities,\nwith models frequently reinforcing common stereotypes. Additionally, we find\nthat models struggle to mitigate bias even when explicitly asked to rationalize\ntheir decision. Our evaluation provides evidence of both allocative and\nrepresentational harms that current LLMs could cause towards Indian identities,\ncalling for a more cautious usage in practical applications. We release\nINDIC-BIAS as an open-source benchmark to advance research on benchmarking and\nmitigating biases and stereotypes in the Indian context.", "AI": {"tldr": "INDIC-BIAS是一个印度中心的基准测试，用于评估LLM在85个身份群体中的公平性，揭示了LLM对边缘化群体的强烈偏见。", "motivation": "现有公平性研究主要基于西方背景，不适用于文化多元的印度，因此需要开发本土化评估工具。", "method": "通过专家咨询筛选1800多个社会文化主题，生成并验证20000个现实场景模板，分为三种评估任务：合理性、判断和生成。", "result": "评估14个主流LLM显示，模型对边缘化身份存在显著负面偏见，且难以通过理性化减少偏见。", "conclusion": "LLM可能对印度身份群体造成分配和代表性伤害，需谨慎使用。INDIC-BIAS作为开源基准发布，推动印度背景下的偏见研究。"}}
{"id": "2506.22756", "pdf": "https://arxiv.org/pdf/2506.22756", "abs": "https://arxiv.org/abs/2506.22756", "authors": ["Tao Tang", "Likui Zhang", "Youpeng Wen", "Kaidong Zhang", "Jia-Wang Bian", "xia zhou", "Tianyi Yan", "Kun Zhan", "Peng Jia", "Hefeng Wu", "Liang Lin", "Xiaodan Liang"], "title": "RoboPearls: Editable Video Simulation for Robot Manipulation", "categories": ["cs.CV", "cs.RO"], "comment": "ICCV 2025", "summary": "The development of generalist robot manipulation policies has seen\nsignificant progress, driven by large-scale demonstration data across diverse\nenvironments. However, the high cost and inefficiency of collecting real-world\ndemonstrations hinder the scalability of data acquisition. While existing\nsimulation platforms enable controlled environments for robotic learning, the\nchallenge of bridging the sim-to-real gap remains. To address these challenges,\nwe propose RoboPearls, an editable video simulation framework for robotic\nmanipulation. Built on 3D Gaussian Splatting (3DGS), RoboPearls enables the\nconstruction of photo-realistic, view-consistent simulations from demonstration\nvideos, and supports a wide range of simulation operators, including various\nobject manipulations, powered by advanced modules like Incremental Semantic\nDistillation (ISD) and 3D regularized NNFM Loss (3D-NNFM). Moreover, by\nincorporating large language models (LLMs), RoboPearls automates the simulation\nproduction process in a user-friendly manner through flexible command\ninterpretation and execution. Furthermore, RoboPearls employs a vision-language\nmodel (VLM) to analyze robotic learning issues to close the simulation loop for\nperformance enhancement. To demonstrate the effectiveness of RoboPearls, we\nconduct extensive experiments on multiple datasets and scenes, including\nRLBench, COLOSSEUM, Ego4D, Open X-Embodiment, and a real-world robot, which\ndemonstrate our satisfactory simulation performance.", "AI": {"tldr": "RoboPearls是一个基于3D高斯散射的可编辑视频仿真框架，用于机器人操作，通过结合大语言模型和视觉语言模型，实现了高效、逼真的仿真和自动化流程。", "motivation": "解决真实世界演示数据收集成本高、效率低的问题，以及仿真与现实之间的差距。", "method": "利用3D高斯散射技术构建逼真仿真，结合增量语义蒸馏和3D正则化NNFM损失，并通过大语言模型和视觉语言模型实现自动化仿真和性能分析。", "result": "在多个数据集和场景中验证了RoboPearls的有效性，展示了令人满意的仿真性能。", "conclusion": "RoboPearls为机器人操作提供了一种高效、可扩展的仿真解决方案，显著提升了仿真与现实之间的过渡效果。"}}
{"id": "2506.23844", "pdf": "https://arxiv.org/pdf/2506.23844", "abs": "https://arxiv.org/abs/2506.23844", "authors": ["Hang Su", "Jun Luo", "Chang Liu", "Xiao Yang", "Yichi Zhang", "Yinpeng Dong", "Jun Zhu"], "title": "A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents", "categories": ["cs.AI"], "comment": "18 pages", "summary": "Recent advances in large language models (LLMs) have catalyzed the rise of\nautonomous AI agents capable of perceiving, reasoning, and acting in dynamic,\nopen-ended environments. These large-model agents mark a paradigm shift from\nstatic inference systems to interactive, memory-augmented entities. While these\ncapabilities significantly expand the functional scope of AI, they also\nintroduce qualitatively novel security risks - such as memory poisoning, tool\nmisuse, reward hacking, and emergent misalignment - that extend beyond the\nthreat models of conventional systems or standalone LLMs. In this survey, we\nfirst examine the structural foundations and key capabilities that underpin\nincreasing levels of agent autonomy, including long-term memory retention,\nmodular tool use, recursive planning, and reflective reasoning. We then analyze\nthe corresponding security vulnerabilities across the agent stack, identifying\nfailure modes such as deferred decision hazards, irreversible tool chains, and\ndeceptive behaviors arising from internal state drift or value misalignment.\nThese risks are traced to architectural fragilities that emerge across\nperception, cognition, memory, and action modules. To address these challenges,\nwe systematically review recent defense strategies deployed at different\nautonomy layers, including input sanitization, memory lifecycle control,\nconstrained decision-making, structured tool invocation, and introspective\nreflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a\nunified cognitive framework grounded in Constrained Markov Decision Processes\n(CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation,\nand joint reward-risk optimization to enable principled, proactive safety\nacross the agent's decision-making loop.", "AI": {"tldr": "大型语言模型（LLM）推动的自主AI代理带来新安全风险，如记忆污染、工具滥用等，需系统性防御策略。", "motivation": "探讨自主AI代理的结构基础与能力，分析其安全漏洞及应对策略。", "method": "调查代理架构、关键能力及安全风险，提出防御策略及R2A2框架。", "result": "识别了代理堆栈中的脆弱性，并提出基于CMDP的统一安全框架。", "conclusion": "R2A2框架通过风险感知建模和优化，提升代理决策的安全性。"}}
{"id": "2506.23122", "pdf": "https://arxiv.org/pdf/2506.23122", "abs": "https://arxiv.org/abs/2506.23122", "authors": ["Shivam Sharma", "Tanmoy Chakraborty"], "title": "Decoding Memes: Benchmarking Narrative Role Classification across Multilingual and Multimodal Models", "categories": ["cs.CL", "cs.CY"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This work investigates the challenging task of identifying narrative roles -\nHero, Villain, Victim, and Other - in Internet memes, across three diverse test\nsets spanning English and code-mixed (English-Hindi) languages. Building on an\nannotated dataset originally skewed toward the 'Other' class, we explore a more\nbalanced and linguistically diverse extension, originally introduced as part of\nthe CLEF 2024 shared task. Comprehensive lexical and structural analyses\nhighlight the nuanced, culture-specific, and context-rich language used in real\nmemes, in contrast to synthetically curated hateful content, which exhibits\nexplicit and repetitive lexical markers. To benchmark the role detection task,\nwe evaluate a wide spectrum of models, including fine-tuned multilingual\ntransformers, sentiment and abuse-aware classifiers, instruction-tuned LLMs,\nand multimodal vision-language models. Performance is assessed under zero-shot\nsettings using precision, recall, and F1 metrics. While larger models like\nDeBERTa-v3 and Qwen2.5-VL demonstrate notable gains, results reveal consistent\nchallenges in reliably identifying the 'Victim' class and generalising across\ncultural and code-mixed content. We also explore prompt design strategies to\nguide multimodal models and find that hybrid prompts incorporating structured\ninstructions and role definitions offer marginal yet consistent improvements.\nOur findings underscore the importance of cultural grounding, prompt\nengineering, and multimodal reasoning in modelling subtle narrative framings in\nvisual-textual content.", "AI": {"tldr": "研究探讨了在英语和混合语言（英语-印地语）的网络迷因中识别叙事角色（英雄、反派、受害者等）的任务，评估了多种模型，发现文化背景和提示设计对性能有显著影响。", "motivation": "网络迷因中的叙事角色识别是一个复杂且文化相关的任务，现有数据集偏向“其他”类别，需要更平衡和多样化的数据来提升模型性能。", "method": "通过扩展和平衡数据集，评估了多种模型（如多语言Transformer、多模态模型等），并探索了提示设计策略。", "result": "大型模型（如DeBERTa-v3和Qwen2.5-VL）表现较好，但识别“受害者”类别和跨文化内容仍具挑战性。混合提示设计带来小幅改进。", "conclusion": "文化背景、提示工程和多模态推理对建模视觉-文本内容中的叙事框架至关重要。"}}
{"id": "2506.22762", "pdf": "https://arxiv.org/pdf/2506.22762", "abs": "https://arxiv.org/abs/2506.22762", "authors": ["Dinh Phu Tran", "Dao Duy Hung", "Daeyoung Kim"], "title": "VSRM: A Robust Mamba-Based Framework for Video Super-Resolution", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Video super-resolution remains a major challenge in low-level vision tasks.\nTo date, CNN- and Transformer-based methods have delivered impressive results.\nHowever, CNNs are limited by local receptive fields, while Transformers\nstruggle with quadratic complexity, posing challenges for processing long\nsequences in VSR. Recently, Mamba has drawn attention for its long-sequence\nmodeling, linear complexity, and large receptive fields. In this work, we\npropose VSRM, a novel \\textbf{V}ideo \\textbf{S}uper-\\textbf{R}esolution\nframework that leverages the power of \\textbf{M}amba. VSRM introduces\nSpatial-to-Temporal Mamba and Temporal-to-Spatial Mamba blocks to extract\nlong-range spatio-temporal features and enhance receptive fields efficiently.\nTo better align adjacent frames, we propose Deformable Cross-Mamba Alignment\nmodule. This module utilizes a deformable cross-mamba mechanism to make the\ncompensation stage more dynamic and flexible, preventing feature distortions.\nFinally, we minimize the frequency domain gaps between reconstructed and\nground-truth frames by proposing a simple yet effective Frequency\nCharbonnier-like loss that better preserves high-frequency content and enhances\nvisual quality. Through extensive experiments, VSRM achieves state-of-the-art\nresults on diverse benchmarks, establishing itself as a solid foundation for\nfuture research.", "AI": {"tldr": "VSRM是一种基于Mamba的视频超分辨率框架，通过引入空间到时间和时间到空间的Mamba块以及可变形交叉Mamba对齐模块，有效提取长程时空特征并增强感受野，同时提出频率Charbonnier-like损失以提升视觉质量。", "motivation": "视频超分辨率任务中，CNN和Transformer方法分别受限于局部感受野和二次复杂度，而Mamba因其长序列建模能力和线性复杂度成为潜在解决方案。", "method": "VSRM采用空间到时间和时间到空间的Mamba块提取特征，提出可变形交叉Mamba对齐模块动态补偿帧间对齐，并使用频率Charbonnier-like损失减少频域差距。", "result": "VSRM在多个基准测试中取得最先进结果。", "conclusion": "VSRM为视频超分辨率任务提供了高效且灵活的框架，为未来研究奠定基础。"}}
{"id": "2506.23908", "pdf": "https://arxiv.org/pdf/2506.23908", "abs": "https://arxiv.org/abs/2506.23908", "authors": ["András György", "Tor Lattimore", "Nevena Lazić", "Csaba Szepesvári"], "title": "Beyond Statistical Learning: Exact Learning Is Essential for General Intelligence", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Sound deductive reasoning -- the ability to derive new knowledge from\nexisting facts and rules -- is an indisputably desirable aspect of general\nintelligence. Despite the major advances of AI systems in areas such as math\nand science, especially since the introduction of transformer architectures, it\nis well-documented that even the most advanced frontier systems regularly and\nconsistently falter on easily-solvable deductive reasoning tasks. Hence, these\nsystems are unfit to fulfill the dream of achieving artificial general\nintelligence capable of sound deductive reasoning. We argue that their unsound\nbehavior is a consequence of the statistical learning approach powering their\ndevelopment. To overcome this, we contend that to achieve reliable deductive\nreasoning in learning-based AI systems, researchers must fundamentally shift\nfrom optimizing for statistical performance against distributions on reasoning\nproblems and algorithmic tasks to embracing the more ambitious exact learning\nparadigm, which demands correctness on all inputs. We argue that exact learning\nis both essential and possible, and that this ambitious objective should guide\nalgorithm design.", "AI": {"tldr": "论文主张通过从统计学习转向精确学习范式，以实现AI系统在演绎推理中的可靠性。", "motivation": "当前最先进的AI系统在演绎推理任务中表现不佳，无法实现真正的通用智能。", "method": "提出从统计性能优化转向精确学习范式，要求在所有输入上正确。", "result": "精确学习是实现可靠演绎推理的必要且可行的目标。", "conclusion": "精确学习应成为算法设计的指导原则，以实现AI系统的可靠演绎推理。"}}
{"id": "2506.23127", "pdf": "https://arxiv.org/pdf/2506.23127", "abs": "https://arxiv.org/abs/2506.23127", "authors": ["Zhaoye Fei", "Li Ji", "Siyin Wang", "Junhao Shi", "Jingjing Gong", "Xipeng Qiu"], "title": "Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they face significant challenges in embodied task planning\nscenarios that require continuous environmental understanding and action\ngeneration. Existing approaches generate open-loop action scripts based on\nstatic knowledge, making it difficult to learn causal relationships between\nactions and environmental feedback, particularly in partially observable\nenvironments. We introduce Embodied Planner-R1, a novel outcome-driven\nreinforcement learning framework that enables LLMs to develop interactive\ncapabilities through autonomous exploration with minimal supervision. Our\nframework incorporates three key innovations: (1) Without human annotations, we\nemploy pure reinforcement learning with group rollout, incorporating\nin-environment interaction through parallel exploration; (2) completion-driven\nsparse reward; and (3) Interactive Policy Optimization (IPO) for efficient\nlearning from grouped trajectories. Across two challenging text-based Embodied\nplanning benchmarks, Embodied Planner-R1 achieves impressive completion rates\nof 97.78% on ALFWorld and 79.92% on ScienceWorld, surpassing prior methods by a\nlarge margin, and suffers only a -3.66% drop in previously unseen environments,\nevidencing strong generalization.", "AI": {"tldr": "Embodied Planner-R1是一个基于强化学习的框架，旨在提升大语言模型在具身任务规划中的交互能力，通过自主探索和稀疏奖励实现高效学习。", "motivation": "现有方法在部分可观测环境中难以学习动作与环境反馈的因果关系，Embodied Planner-R1旨在解决这一问题。", "method": "采用纯强化学习与组内并行探索、完成驱动的稀疏奖励以及交互策略优化（IPO）。", "result": "在ALFWorld和ScienceWorld基准测试中分别达到97.78%和79.92%的完成率，泛化能力强。", "conclusion": "Embodied Planner-R1显著提升了LLMs在具身任务中的表现，展示了强泛化能力。"}}
{"id": "2506.22783", "pdf": "https://arxiv.org/pdf/2506.22783", "abs": "https://arxiv.org/abs/2506.22783", "authors": ["Oguzhan Baser", "Ahmet Ege Tanriverdi", "Sriram Vishwanath", "Sandeep P. Chinchali"], "title": "PhonemeFake: Redefining Deepfake Realism with Language-Driven Segmental Manipulation and Adaptive Bilevel Detection", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "5 pages, 3 figures, Published at Proceedings of Interspeech 2025, for\n  the dataset see https://huggingface.co/datasets/phonemefake/PhonemeFakeV2,\n  for the code see https://github.com/UTAustin-SwarmLab/ PhonemeFake", "summary": "Deepfake (DF) attacks pose a growing threat as generative models become\nincreasingly advanced. However, our study reveals that existing DF datasets\nfail to deceive human perception, unlike real DF attacks that influence public\ndiscourse. It highlights the need for more realistic DF attack vectors. We\nintroduce PhonemeFake (PF), a DF attack that manipulates critical speech\nsegments using language reasoning, significantly reducing human perception by\nup to 42% and benchmark accuracies by up to 94%. We release an easy-to-use PF\ndataset on HuggingFace and open-source bilevel DF segment detection model that\nadaptively prioritizes compute on manipulated regions. Our extensive\nexperiments across three known DF datasets reveal that our detection model\nreduces EER by 91% while achieving up to 90% speed-up, with minimal compute\noverhead and precise localization beyond existing models as a scalable\nsolution.", "AI": {"tldr": "论文提出PhonemeFake（PF）攻击方法，通过语言推理操纵关键语音片段，显著降低人类感知和基准准确率，并开源检测模型和数据集。", "motivation": "现有Deepfake数据集未能真实模拟攻击对人类感知的影响，需要更现实的攻击向量。", "method": "引入PF攻击方法，利用语言推理操纵关键语音片段，并开发自适应检测模型。", "result": "PF攻击降低人类感知42%，基准准确率94%；检测模型降低EER 91%，速度提升90%。", "conclusion": "PF攻击和检测模型为Deepfake攻击提供了更现实的解决方案，具有高效和可扩展性。"}}
{"id": "2506.23924", "pdf": "https://arxiv.org/pdf/2506.23924", "abs": "https://arxiv.org/abs/2506.23924", "authors": ["Akshit Kumar", "Tianyi Peng", "Yuhang Wu", "Assaf Zeevi"], "title": "Performance of LLMs on Stochastic Modeling Operations Research Problems: From Theory to Practice", "categories": ["cs.AI"], "comment": null, "summary": "Large language models (LLMs) have exhibited expert-level capabilities across\nvarious domains. However, their abilities to solve problems in Operations\nResearch (OR) -- the analysis and optimization of mathematical models derived\nfrom real-world problems or their verbal descriptions -- remain underexplored.\nIn this work, we take a first step toward evaluating LLMs' abilities to solve\nstochastic modeling problems, a core class of OR problems characterized by\nuncertainty and typically involving tools from probability, statistics, and\nstochastic processes. We manually procure a representative set of\ngraduate-level homework and doctoral qualification-exam problems and test LLMs'\nabilities to solve them. We further leverage SimOpt, an open-source library of\nsimulation-optimization problems and solvers, to investigate LLMs' abilities to\nmake real-world decisions under uncertainty. Our results show that, though a\nnontrivial amount of work is still needed to reliably automate the stochastic\nmodeling pipeline in reality, state-of-the-art LLMs demonstrate proficiency on\npar with human experts in both classroom and practical settings. These findings\nhighlight the potential of building AI agents that assist OR researchers and\namplify the real-world impact of OR through automation.", "AI": {"tldr": "论文探讨了大型语言模型（LLMs）在解决运筹学（OR）中随机建模问题的能力，发现其表现与人类专家相当，但仍需进一步研究以实现可靠自动化。", "motivation": "评估LLMs在解决运筹学中随机建模问题的能力，填补该领域的研究空白。", "method": "手动收集研究生课程作业和博士资格考试题目，测试LLMs解决能力，并利用SimOpt库研究其在实际决策中的表现。", "result": "LLMs在课堂和实际场景中表现出与人类专家相当的能力，但完全自动化仍需改进。", "conclusion": "LLMs有潜力辅助运筹学研究，通过自动化提升OR的实际影响力。"}}
{"id": "2506.23133", "pdf": "https://arxiv.org/pdf/2506.23133", "abs": "https://arxiv.org/abs/2506.23133", "authors": ["Dingzirui Wang", "Xuanliang Zhang", "Rongyu Cao", "Longxu Dou", "Xianzhen Luo", "Yingwei Ma", "Qingfu Zhu", "Wanxiang Che", "Binhua Li", "Fei Huang", "Yongbin Li"], "title": "Format-Adapter: Improving Reasoning Capability of LLMs by Adapting Suitable Format", "categories": ["cs.CL"], "comment": null, "summary": "Generating and voting multiple answers is an effective method to mitigate\nreasoning inconsistencies of large language models (LLMs). Prior works have\nshown that multiple reasoning formats outperform a single format when\ngenerating multiple answers. However, previous works using multiple formats\nrely on formats labeled by humans, which could be unsuitable for all tasks and\nhave high labeling costs. To address this issue, we adapt suitable formats to\nthe given tasks by generating and selecting formats. We first propose how to\nmeasure the reasoning error when generating multiple answers. Then, we\nintroduce Format-Adapter, which utilizes LLMs to generate and select suitable\nreasoning formats by minimizing the error measurement we present. We conduct\nexperiments on math and commonsense reasoning tasks, where Format-Adapter\nachieves a 4.3% performance improvement on average over previous works,\ndemonstrating the effectiveness.", "AI": {"tldr": "通过生成和选择推理格式，Format-Adapter方法显著提升了LLMs在数学和常识推理任务中的性能。", "motivation": "解决现有方法依赖人工标注推理格式的局限性，降低标注成本并提高任务适应性。", "method": "提出测量推理误差的方法，并开发Format-Adapter，利用LLMs生成和选择最优推理格式。", "result": "在数学和常识推理任务中，Format-Adapter平均性能提升4.3%。", "conclusion": "Format-Adapter通过自适应生成和选择推理格式，有效提升了LLMs的推理一致性。"}}
{"id": "2506.22784", "pdf": "https://arxiv.org/pdf/2506.22784", "abs": "https://arxiv.org/abs/2506.22784", "authors": ["Yu Han", "Zhiwei Huang", "Yanting Zhang", "Fangjun Ding", "Shen Cai", "Rui Fan"], "title": "Single-Frame Point-Pixel Registration via Supervised Cross-Modal Feature Matching", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "Point-pixel registration between LiDAR point clouds and camera images is a\nfundamental yet challenging task in autonomous driving and robotic perception.\nA key difficulty lies in the modality gap between unstructured point clouds and\nstructured images, especially under sparse single-frame LiDAR settings.\nExisting methods typically extract features separately from point clouds and\nimages, then rely on hand-crafted or learned matching strategies. This separate\nencoding fails to bridge the modality gap effectively, and more critically,\nthese methods struggle with the sparsity and noise of single-frame LiDAR, often\nrequiring point cloud accumulation or additional priors to improve reliability.\nInspired by recent progress in detector-free matching paradigms (e.g.\nMatchAnything), we revisit the projection-based approach and introduce the\ndetector-free framework for direct point-pixel matching between LiDAR and\ncamera views. Specifically, we project the LiDAR intensity map into a 2D view\nfrom the LiDAR perspective and feed it into an attention-based detector-free\nmatching network, enabling cross-modal correspondence estimation without\nrelying on multi-frame accumulation. To further enhance matching reliability,\nwe introduce a repeatability scoring mechanism that acts as a soft visibility\nprior. This guides the network to suppress unreliable matches in regions with\nlow intensity variation, improving robustness under sparse input. Extensive\nexperiments on KITTI, nuScenes, and MIAS-LCEC-TF70 benchmarks demonstrate that\nour method achieves state-of-the-art performance, outperforming prior\napproaches on nuScenes (even those relying on accumulated point clouds),\ndespite using only single-frame LiDAR.", "AI": {"tldr": "提出了一种基于无检测器匹配框架的点-像素配准方法，通过投影LiDAR强度图并使用注意力网络直接匹配，解决了单帧LiDAR稀疏性和噪声问题。", "motivation": "解决LiDAR点云与相机图像之间的模态差异问题，尤其是在单帧LiDAR稀疏和噪声情况下的配准挑战。", "method": "将LiDAR强度图投影到2D视图，通过注意力网络进行跨模态匹配，引入可重复性评分机制提升可靠性。", "result": "在KITTI、nuScenes和MIAS-LCEC-TF70基准测试中表现优异，优于依赖多帧点云的方法。", "conclusion": "该方法在单帧LiDAR下实现了高性能，为跨模态配准提供了新思路。"}}
{"id": "2506.23926", "pdf": "https://arxiv.org/pdf/2506.23926", "abs": "https://arxiv.org/abs/2506.23926", "authors": ["Junping Wang", "Bicheng Wang", "Yibo Xuea", "Yuan Xie"], "title": "Industrial brain: a human-like autonomous neuro-symbolic cognitive decision-making system", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Resilience non-equilibrium measurement, the ability to maintain fundamental\nfunctionality amidst failures and errors, is crucial for scientific management\nand engineering applications of industrial chain. The problem is particularly\nchallenging when the number or types of multiple co-evolution of resilience\n(for example, randomly placed) are extremely chaos. Existing end-to-end deep\nlearning ordinarily do not generalize well to unseen full-feld reconstruction\nof spatiotemporal co-evolution structure, and predict resilience of network\ntopology, especially in multiple chaos data regimes typically seen in\nreal-world applications. To address this challenge, here we propose industrial\nbrain, a human-like autonomous cognitive decision-making and planning framework\nintegrating higher-order activity-driven neuro network and CT-OODA symbolic\nreasoning to autonomous plan resilience directly from observational data of\nglobal variable. The industrial brain not only understands and model structure\nof node activity dynamics and network co-evolution topology without simplifying\nassumptions, and reveal the underlying laws hidden behind complex networks, but\nalso enabling accurate resilience prediction, inference, and planning.\nExperimental results show that industrial brain significantly outperforms\nresilience prediction and planning methods, with an accurate improvement of up\nto 10.8\\% over GoT and OlaGPT framework and 11.03\\% over spectral dimension\nreduction. It also generalizes to unseen topologies and dynamics and maintains\nrobust performance despite observational disturbances. Our findings suggest\nthat industrial brain addresses an important gap in resilience prediction and\nplanning for industrial chain.", "AI": {"tldr": "提出了一种名为“工业大脑”的框架，结合高阶神经网络和符号推理，用于预测和规划工业链的弹性，显著优于现有方法。", "motivation": "现有深度学习模型在复杂时空共演结构的弹性预测上泛化能力不足，尤其是在多混沌数据场景下。", "method": "结合高阶活动驱动神经网络和CT-OODA符号推理，直接从观测数据中自主规划弹性。", "result": "工业大脑在弹性预测和规划上显著优于GoT、OlaGPT和谱降维方法，准确率提升达10.8%和11.03%。", "conclusion": "工业大脑填补了工业链弹性预测和规划的重要空白，具有鲁棒性和泛化能力。"}}
{"id": "2506.23136", "pdf": "https://arxiv.org/pdf/2506.23136", "abs": "https://arxiv.org/abs/2506.23136", "authors": ["Shadman Sobhan", "Mohammad Ariful Haque"], "title": "LLM-Assisted Question-Answering on Technical Documents Using Structured Data-Aware Retrieval Augmented Generation", "categories": ["cs.CL"], "comment": "29 Pages, 11 Tables", "summary": "Large Language Models (LLMs) are capable of natural language understanding\nand generation. But they face challenges such as hallucination and outdated\nknowledge. Fine-tuning is one possible solution, but it is resource-intensive\nand must be repeated with every data update. Retrieval-Augmented Generation\n(RAG) offers an efficient solution by allowing LLMs to access external\nknowledge sources. However, traditional RAG pipelines struggle with retrieving\ninformation from complex technical documents with structured data such as\ntables and images. In this work, we propose a RAG pipeline, capable of handling\ntables and images in documents, for technical documents that support both\nscanned and searchable formats. Its retrieval process combines vector\nsimilarity search with a fine-tuned reranker based on Gemma-2-9b-it. The\nreranker is trained using RAFT (Retrieval-Augmented Fine-Tuning) on a custom\ndataset designed to improve context identification for question answering. Our\nevaluation demonstrates that the proposed pipeline achieves a high faithfulness\nscore of 94% (RAGas) and 96% (DeepEval), and an answer relevancy score of 87%\n(RAGas) and 93% (DeepEval). Comparative analysis demonstrates that the proposed\narchitecture is superior to general RAG pipelines in terms of table-based\nquestions and handling questions outside context.", "AI": {"tldr": "提出了一种改进的RAG管道，能够处理技术文档中的表格和图像，结合向量相似性搜索和基于Gemma-2-9b-it的重新排序器，显著提高了答案的准确性和相关性。", "motivation": "解决传统RAG管道在处理复杂技术文档（如表格和图像）时的局限性，以及LLMs的幻觉和知识过时问题。", "method": "结合向量相似性搜索和基于Gemma-2-9b-it的重新排序器，使用RAFT在自定义数据集上进行微调，优化上下文识别。", "result": "在RAGas和DeepEval评估中，忠实度分别达到94%和96%，答案相关性分别为87%和93%，优于通用RAG管道。", "conclusion": "提出的RAG管道在处理技术文档时表现优异，尤其在表格问题和上下文外问题上具有显著优势。"}}
{"id": "2506.22800", "pdf": "https://arxiv.org/pdf/2506.22800", "abs": "https://arxiv.org/abs/2506.22800", "authors": ["Sicong Du", "Jiarun Liu", "Qifeng Chen", "Hao-Xiang Chen", "Tai-Jiang Mu", "Sheng Yang"], "title": "RGE-GS: Reward-Guided Expansive Driving Scene Reconstruction via Diffusion Priors", "categories": ["cs.CV"], "comment": null, "summary": "A single-pass driving clip frequently results in incomplete scanning of the\nroad structure, making reconstructed scene expanding a critical requirement for\nsensor simulators to effectively regress driving actions. Although contemporary\n3D Gaussian Splatting (3DGS) techniques achieve remarkable reconstruction\nquality, their direct extension through the integration of diffusion priors\noften introduces cumulative physical inconsistencies and compromises training\nefficiency. To address these limitations, we present RGE-GS, a novel expansive\nreconstruction framework that synergizes diffusion-based generation with\nreward-guided Gaussian integration. The RGE-GS framework incorporates two key\ninnovations: First, we propose a reward network that learns to identify and\nprioritize consistently generated patterns prior to reconstruction phases,\nthereby enabling selective retention of diffusion outputs for spatial\nstability. Second, during the reconstruction process, we devise a\ndifferentiated training strategy that automatically adjust Gaussian\noptimization progress according to scene converge metrics, which achieving\nbetter convergence than baseline methods. Extensive evaluations of publicly\navailable datasets demonstrate that RGE-GS achieves state-of-the-art\nperformance in reconstruction quality. Our source-code will be made publicly\navailable at https://github.com/CN-ADLab/RGE-GS. (Camera-ready version\nincorporating reviewer suggestions will be updated soon.)", "AI": {"tldr": "RGE-GS是一种新颖的场景扩展重建框架，结合扩散生成和奖励引导的高斯积分，解决了传统方法中的物理不一致性和训练效率问题。", "motivation": "单次驾驶片段扫描通常导致道路结构不完整，需要扩展重建以支持传感器模拟器有效回归驾驶行为。", "method": "RGE-GS框架包含两个关键创新：奖励网络选择性地保留扩散输出以保持空间稳定性；差异化训练策略根据场景收敛指标调整高斯优化进度。", "result": "在公开数据集上的评估表明，RGE-GS在重建质量上达到最先进水平。", "conclusion": "RGE-GS通过结合扩散生成和奖励引导的高斯积分，显著提升了场景扩展重建的性能和效率。"}}
{"id": "2506.23949", "pdf": "https://arxiv.org/pdf/2506.23949", "abs": "https://arxiv.org/abs/2506.23949", "authors": ["Anthony M. Barrett", "Jessica Newman", "Brandie Nonnecke", "Nada Madkour", "Dan Hendrycks", "Evan R. Murphy", "Krystal Jackson", "Deepika Raman"], "title": "AI Risk-Management Standards Profile for General-Purpose AI (GPAI) and Foundation Models", "categories": ["cs.AI", "cs.CR", "cs.CY"], "comment": null, "summary": "Increasingly multi-purpose AI models, such as cutting-edge large language\nmodels or other 'general-purpose AI' (GPAI) models, 'foundation models,'\ngenerative AI models, and 'frontier models' (typically all referred to\nhereafter with the umbrella term 'GPAI/foundation models' except where greater\nspecificity is needed), can provide many beneficial capabilities but also risks\nof adverse events with profound consequences. This document provides\nrisk-management practices or controls for identifying, analyzing, and\nmitigating risks of GPAI/foundation models. We intend this document primarily\nfor developers of large-scale, state-of-the-art GPAI/foundation models; others\nthat can benefit from this guidance include downstream developers of end-use\napplications that build on a GPAI/foundation model. This document facilitates\nconformity with or use of leading AI risk management-related standards,\nadapting and building on the generic voluntary guidance in the NIST AI Risk\nManagement Framework and ISO/IEC 23894, with a focus on the unique issues faced\nby developers of GPAI/foundation models.", "AI": {"tldr": "本文提出了针对通用AI/基础模型的风险管理实践，旨在帮助开发者识别、分析和减轻风险。", "motivation": "随着多用途AI模型的普及，其带来的潜在风险需要系统化管理。", "method": "基于NIST AI风险管理框架和ISO/IEC 23894标准，提出针对GPAI/基础模型的独特风险管理实践。", "result": "为开发者提供了实用的风险管理指南，适用于大型GPAI/基础模型及其下游应用。", "conclusion": "本文为GPAI/基础模型的风险管理提供了标准化指导，有助于降低潜在负面影响。"}}
{"id": "2506.23137", "pdf": "https://arxiv.org/pdf/2506.23137", "abs": "https://arxiv.org/abs/2506.23137", "authors": ["Siyuan Li", "Ruitong Liu", "Yan Wen", "Te Sun"], "title": "Flow-Modulated Scoring for Semantic-Aware Knowledge Graph Completion", "categories": ["cs.CL", "cs.AI"], "comment": "10 pages", "summary": "Effective modeling of multifaceted relations is pivotal for Knowledge Graph\nCompletion (KGC). However, a majority of existing approaches are predicated on\nstatic, embedding-based scoring, exhibiting inherent limitations in capturing\ncontextual dependencies and relational dynamics. Addressing this gap, we\npropose the Flow-Modulated Scoring (FMS) framework. FMS comprises two principal\ncomponents: (1) a semantic context learning module that encodes\ncontext-sensitive entity representations, and (2) a conditional flow-matching\nmodule designed to learn the dynamic transformation from a head to a tail\nembedding, governed by the aforementioned context. The resultant predictive\nvector field, representing the context-informed relational path, serves to\ndynamically refine the initial static score of an entity pair. Through this\nsynergy of context-aware static representations and conditioned dynamic\ninformation, FMS facilitates a more profound modeling of relational semantics.\nComprehensive evaluations on several standard benchmarks demonstrate that our\nproposed method surpasses prior state-of-the-art results.", "AI": {"tldr": "提出Flow-Modulated Scoring (FMS)框架，结合静态嵌入和动态信息，提升知识图谱补全的建模能力。", "motivation": "现有知识图谱补全方法多为静态嵌入评分，难以捕捉上下文依赖和关系动态性。", "method": "FMS包含语义上下文学习模块和条件流匹配模块，动态优化实体对评分。", "result": "在多个基准测试中超越现有最优方法。", "conclusion": "FMS通过结合静态和动态信息，显著提升了关系语义建模能力。"}}
{"id": "2506.22803", "pdf": "https://arxiv.org/pdf/2506.22803", "abs": "https://arxiv.org/abs/2506.22803", "authors": ["Nuoye Xiong", "Anqi Dong", "Ning Wang", "Cong Hua", "Guangming Zhu", "Mei Lin", "Peiyi Shen", "Liang Zhang"], "title": "Intervening in Black Box: Concept Bottleneck Model for Enhancing Human Neural Network Mutual Understanding", "categories": ["cs.CV", "cs.HC", "cs.LG"], "comment": "Accepted by ICCV 2025", "summary": "Recent advances in deep learning have led to increasingly complex models with\ndeeper layers and more parameters, reducing interpretability and making their\ndecisions harder to understand. While many methods explain black-box reasoning,\nmost lack effective interventions or only operate at sample-level without\nmodifying the model itself. To address this, we propose the Concept Bottleneck\nModel for Enhancing Human-Neural Network Mutual Understanding (CBM-HNMU).\nCBM-HNMU leverages the Concept Bottleneck Model (CBM) as an interpretable\nframework to approximate black-box reasoning and communicate conceptual\nunderstanding. Detrimental concepts are automatically identified and refined\n(removed/replaced) based on global gradient contributions. The modified CBM\nthen distills corrected knowledge back into the black-box model, enhancing both\ninterpretability and accuracy. We evaluate CBM-HNMU on various CNN and\ntransformer-based models across Flower-102, CIFAR-10, CIFAR-100, FGVC-Aircraft,\nand CUB-200, achieving a maximum accuracy improvement of 2.64% and a maximum\nincrease in average accuracy across 1.03%. Source code is available at:\nhttps://github.com/XiGuaBo/CBM-HNMU.", "AI": {"tldr": "提出了一种基于概念瓶颈模型（CBM）的方法CBM-HNMU，通过识别和修正有害概念，增强深度模型的解释性和准确性。", "motivation": "深度学习模型复杂度增加导致解释性下降，现有方法缺乏有效干预或仅针对样本层面。", "method": "利用CBM框架近似黑盒推理，自动识别并修正有害概念，将修正后的知识蒸馏回原模型。", "result": "在多个数据集和模型上测试，最高准确率提升2.64%，平均准确率提升1.03%。", "conclusion": "CBM-HNMU有效提升了模型的解释性和性能。"}}
{"id": "2506.23992", "pdf": "https://arxiv.org/pdf/2506.23992", "abs": "https://arxiv.org/abs/2506.23992", "authors": ["Aditya Shrivastava", "Komal Gupta", "Shraddha Arora"], "title": "Harnessing AI Agents to Advance Research on Refugee Child Mental Health", "categories": ["cs.AI", "cs.ET"], "comment": "14 page , 2 image , 2 tables , accepted under 5th International\n  Conference on Innovations in Computational Intelligence and Computer Vision\n  (ICICV-2025)", "summary": "The international refugee crisis deepens, exposing millions of dis placed\nchildren to extreme psychological trauma. This research suggests a com pact,\nAI-based framework for processing unstructured refugee health data and\ndistilling knowledge on child mental health. We compare two Retrieval-Aug\nmented Generation (RAG) pipelines, Zephyr-7B-beta and DeepSeek R1-7B, to\ndetermine how well they process challenging humanitarian datasets while avoid\ning hallucination hazards. By combining cutting-edge AI methods with migration\nresearch and child psychology, this study presents a scalable strategy to\nassist policymakers, mental health practitioners, and humanitarian agencies to\nbetter assist displaced children and recognize their mental wellbeing. In\ntotal, both the models worked properly but significantly Deepseek R1 is\nsuperior to Zephyr with an accuracy of answer relevance 0.91", "AI": {"tldr": "研究提出了一种基于AI的框架，用于处理难民健康数据并分析儿童心理健康，比较了两种RAG模型（Zephyr-7B-beta和DeepSeek R1-7B），发现DeepSeek R1-7B表现更优。", "motivation": "国际难民危机加剧，数百万儿童面临心理创伤，亟需高效工具处理难民健康数据以支持决策。", "method": "采用两种RAG模型（Zephyr-7B-beta和DeepSeek R1-7B）处理难民健康数据，避免幻觉风险。", "result": "DeepSeek R1-7B表现优于Zephyr，答案相关性准确率达0.91。", "conclusion": "结合AI与心理学研究，为政策制定者和人道机构提供了可扩展的解决方案，以改善难民儿童心理健康支持。"}}
{"id": "2506.23139", "pdf": "https://arxiv.org/pdf/2506.23139", "abs": "https://arxiv.org/abs/2506.23139", "authors": ["Prafulla Kumar Choubey", "Xiangyu Peng", "Shilpa Bhagavath", "Kung-Hsiang Huang", "Caiming Xiong", "Chien-Sheng Wu"], "title": "Benchmarking Deep Search over Heterogeneous Enterprise Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present a new benchmark for evaluating Deep Search--a realistic and\ncomplex form of retrieval-augmented generation (RAG) that requires\nsource-aware, multi-hop reasoning over diverse, sparsed, but related sources.\nThese include documents, meeting transcripts, Slack messages, GitHub, and URLs,\nwhich vary in structure and often contain human-to-human interactions. We build\nit using a synthetic data pipeline that simulates business workflows across\nproduct planning, development, and support stages, generating interconnected\ncontent with realistic noise and multi-hop questions with guaranteed\nground-truth answers. We release our benchmark with both answerable and\nunanswerable queries, and retrieval pool of 39,190 enterprise artifacts,\nenabling fine-grained evaluation of long-context LLM and RAG systems. Our\nexperiments reveal that even the best-performing agentic RAG methods achieve an\naverage performance score of 32.96 on our benchmark. With further analysis, we\nhighlight retrieval as the main bottleneck: existing methods struggle to\nconduct deep searches and retrieve all necessary evidence. Consequently, they\noften reason over partial context, leading to significant performance\ndegradation.", "AI": {"tldr": "提出了一种新的深度搜索基准，用于评估检索增强生成（RAG）系统在多跳推理和多样化稀疏数据源上的表现。实验显示现有方法在检索环节存在瓶颈。", "motivation": "现有RAG系统在多跳推理和多样化数据源上的表现不足，需要更复杂的基准来评估和改进。", "method": "通过合成数据管道模拟企业工作流程，生成多样化、噪声丰富的多跳问题及其答案，构建包含39,190个企业工件的检索池。", "result": "最佳RAG方法平均得分仅为32.96，检索环节是主要瓶颈，导致推理基于不完整上下文。", "conclusion": "深度搜索需要改进检索方法，以解决多跳推理中的证据缺失问题。"}}
{"id": "2506.22806", "pdf": "https://arxiv.org/pdf/2506.22806", "abs": "https://arxiv.org/abs/2506.22806", "authors": ["Byung Hyun Lee", "Sungjin Lim", "Seunggyu Lee", "Dong Un Kang", "Se Young Chun"], "title": "Concept Pinpoint Eraser for Text-to-image Diffusion Models via Residual Attention Gate", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Remarkable progress in text-to-image diffusion models has brought a major\nconcern about potentially generating images on inappropriate or trademarked\nconcepts. Concept erasing has been investigated with the goals of deleting\ntarget concepts in diffusion models while preserving other concepts with\nminimal distortion. To achieve these goals, recent concept erasing methods\nusually fine-tune the cross-attention layers of diffusion models. In this work,\nwe first show that merely updating the cross-attention layers in diffusion\nmodels, which is mathematically equivalent to adding \\emph{linear} modules to\nweights, may not be able to preserve diverse remaining concepts. Then, we\npropose a novel framework, dubbed Concept Pinpoint Eraser (CPE), by adding\n\\emph{nonlinear} Residual Attention Gates (ResAGs) that selectively erase (or\ncut) target concepts while safeguarding remaining concepts from broad\ndistributions by employing an attention anchoring loss to prevent the\nforgetting. Moreover, we adversarially train CPE with ResAG and learnable text\nembeddings in an iterative manner to maximize erasing performance and enhance\nrobustness against adversarial attacks. Extensive experiments on the erasure of\ncelebrities, artistic styles, and explicit contents demonstrated that the\nproposed CPE outperforms prior arts by keeping diverse remaining concepts while\ndeleting the target concepts with robustness against attack prompts. Code is\navailable at https://github.com/Hyun1A/CPE", "AI": {"tldr": "本文提出了一种名为Concept Pinpoint Eraser (CPE)的新框架，通过非线性模块选择性擦除目标概念，同时保护其他概念，优于现有方法。", "motivation": "文本到图像扩散模型的进步引发了关于生成不当或商标概念的担忧，现有方法仅微调交叉注意力层可能无法有效保护其他概念。", "method": "CPE通过添加非线性残差注意力门（ResAGs）和注意力锚定损失，选择性擦除目标概念并保护其他概念，同时通过对抗训练增强鲁棒性。", "result": "实验表明，CPE在擦除名人、艺术风格和不当内容方面优于现有方法，并能抵御攻击提示。", "conclusion": "CPE通过非线性模块和对抗训练，实现了高效的概念擦除和鲁棒性，代码已开源。"}}
{"id": "2506.24026", "pdf": "https://arxiv.org/pdf/2506.24026", "abs": "https://arxiv.org/abs/2506.24026", "authors": ["Yongyi Wang", "Wenxin Li"], "title": "Constructing Non-Markovian Decision Process via History Aggregator", "categories": ["cs.AI"], "comment": null, "summary": "In the domain of algorithmic decision-making, non-Markovian dynamics manifest\nas a significant impediment, especially for paradigms such as Reinforcement\nLearning (RL), thereby exerting far-reaching consequences on the advancement\nand effectiveness of the associated systems. Nevertheless, the existing\nbenchmarks are deficient in comprehensively assessing the capacity of decision\nalgorithms to handle non-Markovian dynamics. To address this deficiency, we\nhave devised a generalized methodology grounded in category theory. Notably, we\nestablished the category of Markov Decision Processes (MDP) and the category of\nnon-Markovian Decision Processes (NMDP), and proved the equivalence\nrelationship between them. This theoretical foundation provides a novel\nperspective for understanding and addressing non-Markovian dynamics. We further\nintroduced non-Markovianity into decision-making problem settings via the\nHistory Aggregator for State (HAS). With HAS, we can precisely control the\nstate dependency structure of decision-making problems in the time series. Our\nanalysis demonstrates the effectiveness of our method in representing a broad\nrange of non-Markovian dynamics. This approach facilitates a more rigorous and\nflexible evaluation of decision algorithms by testing them in problem settings\nwhere non-Markovian dynamics are explicitly constructed.", "AI": {"tldr": "论文提出了一种基于范畴论的方法，用于解决算法决策中的非马尔可夫动态问题，并通过建立MDP和NMDP的等价关系，为理解和处理非马尔可夫动态提供了新视角。", "motivation": "现有基准无法全面评估决策算法处理非马尔可夫动态的能力，因此需要一种更系统的方法。", "method": "基于范畴论，建立了MDP和NMDP的范畴，并证明其等价关系；引入HAS（历史状态聚合器）来精确控制状态依赖结构。", "result": "方法能有效表示广泛的非马尔可夫动态，为决策算法提供了更严格和灵活的评估框架。", "conclusion": "该理论框架为理解和处理非马尔可夫动态提供了新工具，推动了算法决策领域的发展。"}}
{"id": "2506.23146", "pdf": "https://arxiv.org/pdf/2506.23146", "abs": "https://arxiv.org/abs/2506.23146", "authors": ["Dingzriui Wang", "Xuanliang Zhang", "Keyan Xu", "Qingfu Zhu", "Wanxiang Che", "Yang Deng"], "title": "Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness Beyond Performance Illusions", "categories": ["cs.CL"], "comment": null, "summary": "In-context learning (ICL) has emerged as an effective approach to enhance the\nperformance of large language models (LLMs). However, its effectiveness varies\nsignificantly across models and tasks, posing challenges for practitioners to\ndetermine when ICL reliably improves performance. Current evaluation\napproaches, reliant on performance change after applying ICL, suffer from low\nreliability, poor attribution, and impracticality in data-insufficient\nscenarios. We propose the Learning-to-Context Slope (LCS), a novel metric that\nquantifies ICL effectiveness by modeling the slope between learning gain (loss\ndecrease from demonstrations) and contextual relevance (demonstration-input\nrelevance). LCS addresses key limitations of performance-based metrics: (1) it\ncaptures continuous loss changes even when outputs are incorrect, improving\nreliability; (2) its formulation attributes ICL failures to weak contextual\nalignment (inability to adapt inputs to demonstrations) or strong output\ncalibration (self-verification of correctness); and (3) it minimizes reliance\non labeled data via synthetic evaluation. Extensive experiments demonstrate\nthat LCS strongly correlates with performance improvements in labeled settings\nand reliably reflects true effectiveness in biased or data-scarce scenarios.\nFurther analysis reveals actionable thresholds for LCS and identifies model\ncapabilities critical to ICL success.", "AI": {"tldr": "论文提出了一种名为学习上下文斜率（LCS）的新指标，用于量化上下文学习（ICL）的有效性，解决了现有评估方法的局限性。", "motivation": "当前评估ICL效果的方法存在可靠性低、归因性差和数据不足时不可行的问题，需要一种更可靠的指标。", "method": "通过建模学习增益（损失减少）与上下文相关性（演示与输入的相关性）之间的斜率，提出LCS指标。", "result": "LCS在标记数据场景中与性能改进强相关，在数据稀缺或偏置场景中也能可靠反映ICL效果。", "conclusion": "LCS是一种可靠且实用的ICL有效性评估指标，并揭示了模型能力对ICL成功的关键作用。"}}
{"id": "2506.22807", "pdf": "https://arxiv.org/pdf/2506.22807", "abs": "https://arxiv.org/abs/2506.22807", "authors": ["Yueyang Li", "Shengyu Gong", "Weiming Zeng", "Nizhuan Wang", "Wai Ting Siok"], "title": "FreqDGT: Frequency-Adaptive Dynamic Graph Networks with Transformer for Cross-subject EEG Emotion Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Electroencephalography (EEG) serves as a reliable and objective signal for\nemotion recognition in affective brain-computer interfaces, offering unique\nadvantages through its high temporal resolution and ability to capture\nauthentic emotional states that cannot be consciously controlled. However,\ncross-subject generalization remains a fundamental challenge due to individual\nvariability, cognitive traits, and emotional responses. We propose FreqDGT, a\nfrequency-adaptive dynamic graph transformer that systematically addresses\nthese limitations through an integrated framework. FreqDGT introduces\nfrequency-adaptive processing (FAP) to dynamically weight emotion-relevant\nfrequency bands based on neuroscientific evidence, employs adaptive dynamic\ngraph learning (ADGL) to learn input-specific brain connectivity patterns, and\nimplements multi-scale temporal disentanglement network (MTDN) that combines\nhierarchical temporal transformers with adversarial feature disentanglement to\ncapture both temporal dynamics and ensure cross-subject robustness.\nComprehensive experiments demonstrate that FreqDGT significantly improves\ncross-subject emotion recognition accuracy, confirming the effectiveness of\nintegrating frequency-adaptive, spatial-dynamic, and temporal-hierarchical\nmodeling while ensuring robustness to individual differences. The code is\navailable at https://github.com/NZWANG/FreqDGT.", "AI": {"tldr": "FreqDGT是一种频率自适应动态图变换器，通过结合频率自适应处理、动态图学习和多尺度时间解缠网络，显著提高了跨被试情感识别的准确性。", "motivation": "解决脑电信号（EEG）在情感识别中因个体差异导致的跨被试泛化问题。", "method": "提出FreqDGT框架，包括频率自适应处理（FAP）、自适应动态图学习（ADGL）和多尺度时间解缠网络（MTDN）。", "result": "实验证明FreqDGT显著提高了跨被试情感识别的准确性。", "conclusion": "FreqDGT通过整合频率、空间和时间建模，有效解决了跨被试情感识别的挑战。"}}
{"id": "2506.24119", "pdf": "https://arxiv.org/pdf/2506.24119", "abs": "https://arxiv.org/abs/2506.24119", "authors": ["Bo Liu", "Leon Guertler", "Simon Yu", "Zichen Liu", "Penghui Qi", "Daniel Balcells", "Mickel Liu", "Cheston Tan", "Weiyan Shi", "Min Lin", "Wee Sun Lee", "Natasha Jaques"], "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Work in Progress", "summary": "Recent advances in reinforcement learning have shown that language models can\ndevelop sophisticated reasoning through training on tasks with verifiable\nrewards, but these approaches depend on human-curated problem-answer pairs and\ndomain-specific reward engineering. We introduce SPIRAL, a self-play framework\nwhere models learn by playing multi-turn, zero-sum games against continuously\nimproving versions of themselves, eliminating the need for human supervision.\nThrough self-play, SPIRAL generates an infinite curriculum of progressively\nchallenging problems as models must constantly adapt to stronger opponents. To\nenable this self-play training at scale, We implement a fully online,\nmulti-turn, multi-agent reinforcement learning system for LLMs and propose\nrole-conditioned advantage estimation (RAE) to stabilize multi-agent training.\nUsing SPIRAL, self-play on zero-sum games produces reasoning capabilities that\ntransfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\nexpert game trajectories. Analysis reveals that this transfer occurs through\nthree cognitive patterns: systematic decomposition, expected value calculation,\nand case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple\nNegotiation) further enhances performance as each game develops distinct\nreasoning strengths. Applying SPIRAL to a strong reasoning model\n(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These\nresults demonstrate that zero-sum games naturally develop transferable\nreasoning capabilities, highlighting a promising direction for autonomous\nreasoning development.", "AI": {"tldr": "SPIRAL是一个自博弈框架，通过多轮零和游戏让语言模型自我学习，无需人工监督，生成无限渐进式挑战问题，提升推理能力。", "motivation": "现有强化学习方法依赖人工标注和领域特定奖励工程，SPIRAL旨在通过自博弈消除这些限制，实现自主推理能力开发。", "method": "SPIRAL采用在线多轮多智能体强化学习系统，提出角色条件优势估计（RAE）稳定训练，通过零和游戏自博弈生成渐进式问题。", "result": "在Kuhn Poker上训练的模型在数学和通用推理任务上分别提升8.6%和8.4%，多游戏训练进一步增强了性能。", "conclusion": "零和游戏能自然开发可迁移的推理能力，为自主推理发展提供了新方向。"}}
{"id": "2506.23149", "pdf": "https://arxiv.org/pdf/2506.23149", "abs": "https://arxiv.org/abs/2506.23149", "authors": ["Dingzirui Wang", "Xuanliang Zhang", "Keyan Xu", "Qingfu Zhu", "Wanxiang Che", "Yang Deng"], "title": "V-SYNTHESIS: Task-Agnostic Synthesis of Consistent and Diverse In-Context Demonstrations from Scratch via V-Entropy", "categories": ["cs.CL"], "comment": null, "summary": "High labeling cost for in-context learning (ICL) demonstrations motivates\nusing large language models (LLMs) for synthesis to reduce overhead. However,\nexisting synthesis methods are mainly task-specific or rely on pre-existing\ndemonstrations. So this paper focuses on synthesizing demonstrations from\nscratch for arbitrary tasks. A major challenge in synthesizing from scratch is\nensuring consistency with the target task, as the lack of labeling guidance\ncould lead to synthesis bias. We first propose a consistency metric called\nV-Score, which has higher performance and lower computation cost compared with\nthe metrics based on grams or embedding vectors. Furthermore, we introduce\nV-Synthesis, which leverages V-Score for proportional sampling to ensure both\nhigh consistency and diversity of synthesized demonstrations. Experimental\nresults demonstrate that V-Synthesis yields an average performance improvement\nof 2.0% compared to existing synthesis methods confirming the effectiveness of\nV-Synthesis.", "AI": {"tldr": "本文提出了一种名为V-Synthesis的方法，用于从零开始合成任意任务的演示样本，解决了现有方法依赖任务特定或预存样本的问题。通过引入一致性度量V-Score和比例采样，V-Synthesis在性能和多样性上表现优异，实验显示其平均性能提升2.0%。", "motivation": "高标注成本促使使用大语言模型合成演示样本以减少开销，但现有方法依赖任务特定或预存样本，限制了灵活性。", "method": "提出一致性度量V-Score，并基于此开发V-Synthesis方法，通过比例采样确保合成样本的一致性和多样性。", "result": "实验结果表明，V-Synthesis相比现有方法平均性能提升2.0%。", "conclusion": "V-Synthesis有效解决了从零合成演示样本的挑战，为减少标注成本提供了新思路。"}}
{"id": "2506.22814", "pdf": "https://arxiv.org/pdf/2506.22814", "abs": "https://arxiv.org/abs/2506.22814", "authors": ["Andrew Hamara", "Andrew C. Freeman"], "title": "Efficient Multi-Crop Saliency Partitioning for Automatic Image Cropping", "categories": ["cs.CV"], "comment": null, "summary": "Automatic image cropping aims to extract the most visually salient regions\nwhile preserving essential composition elements. Traditional saliency-aware\ncropping methods optimize a single bounding box, making them ineffective for\napplications requiring multiple disjoint crops. In this work, we extend the\nFixed Aspect Ratio Cropping algorithm to efficiently extract multiple\nnon-overlapping crops in linear time. Our approach dynamically adjusts\nattention thresholds and removes selected crops from consideration without\nrecomputing the entire saliency map. We discuss qualitative results and\nintroduce the potential for future datasets and benchmarks.", "AI": {"tldr": "提出了一种高效的多区域图像裁剪方法，动态调整注意力阈值，避免重复计算显著性图。", "motivation": "传统方法仅优化单一裁剪框，无法满足多区域裁剪需求。", "method": "扩展固定宽高比裁剪算法，动态调整注意力阈值，线性时间内提取多个非重叠裁剪区域。", "result": "方法高效，无需重复计算显著性图，支持多区域裁剪。", "conclusion": "为未来数据集和基准测试提供了潜力。"}}
{"id": "1610.09431", "pdf": "https://arxiv.org/pdf/1610.09431", "abs": "https://arxiv.org/abs/1610.09431", "authors": ["Omar Claflin"], "title": "Attention acts to suppress goal-based conflict under high competition", "categories": ["q-bio.NC", "cs.AI"], "comment": "25 pages, 3 figures, 3 tables", "summary": "It is known that when multiple stimuli are present, top-down attention\nselectively enhances the neural signal in the visual cortex for task-relevant\nstimuli, but this has been tested only under conditions of minimal competition\nof visual attention. Here we show during high competition, that is, two stimuli\nin a shared receptive field possessing opposing modulatory goals, top-down\nattention suppresses both task-relevant and irrelevant neural signals within\n100 ms of stimuli onset. This non-selective engagement of top-down attentional\nresources serves to reduce the feedforward signal representing irrelevant\nstimuli.", "AI": {"tldr": "在高竞争条件下，自上而下的注意力会非选择性地抑制任务相关和不相关的神经信号。", "motivation": "研究在高度竞争条件下，自上而下注意力对神经信号的影响，填补了现有研究的空白。", "method": "通过实验观察两个刺激在共享感受野中对立调节目标时的神经信号变化。", "result": "在刺激出现100毫秒内，自上而下注意力非选择性地抑制了任务相关和不相关的神经信号。", "conclusion": "这种非选择性抑制机制有助于减少无关刺激的前馈信号。"}}
{"id": "2506.23192", "pdf": "https://arxiv.org/pdf/2506.23192", "abs": "https://arxiv.org/abs/2506.23192", "authors": ["Gabriel Iturra-Bocaz", "Felipe Bravo-Marquez"], "title": "RiverText: A Python Library for Training and Evaluating Incremental Word Embeddings from Text Data Streams", "categories": ["cs.CL", "cs.LG"], "comment": "Accepted at SIGIR'23", "summary": "Word embeddings have become essential components in various information\nretrieval and natural language processing tasks, such as ranking, document\nclassification, and question answering. However, despite their widespread use,\ntraditional word embedding models present a limitation in their static nature,\nwhich hampers their ability to adapt to the constantly evolving language\npatterns that emerge in sources such as social media and the web (e.g., new\nhashtags or brand names). To overcome this problem, incremental word embedding\nalgorithms are introduced, capable of dynamically updating word representations\nin response to new language patterns and processing continuous data streams.\n  This paper presents RiverText, a Python library for training and evaluating\nincremental word embeddings from text data streams. Our tool is a resource for\nthe information retrieval and natural language processing communities that work\nwith word embeddings in streaming scenarios, such as analyzing social media.\nThe library implements different incremental word embedding techniques, such as\nSkip-gram, Continuous Bag of Words, and Word Context Matrix, in a standardized\nframework. In addition, it uses PyTorch as its backend for neural network\ntraining. We have implemented a module that adapts existing intrinsic static\nword embedding evaluation tasks for word similarity and word categorization to\na streaming setting. Finally, we compare the implemented methods with different\nhyperparameter settings and discuss the results. Our open-source library is\navailable at https://github.com/dccuchile/rivertext.", "AI": {"tldr": "RiverText是一个Python库，用于从文本数据流中训练和评估增量词嵌入，解决了传统静态词嵌入无法适应语言动态变化的问题。", "motivation": "传统词嵌入模型无法适应语言动态变化（如社交媒体中的新词汇），因此需要增量词嵌入算法。", "method": "RiverText实现了多种增量词嵌入技术（如Skip-gram、CBOW等），并提供了标准化框架和PyTorch后端支持。", "result": "库中实现了动态评估任务，比较了不同超参数设置下的方法效果。", "conclusion": "RiverText为信息检索和自然语言处理社区提供了处理流数据的工具，开源可用。"}}
{"id": "2506.22817", "pdf": "https://arxiv.org/pdf/2506.22817", "abs": "https://arxiv.org/abs/2506.22817", "authors": ["Xingyilang Yin", "Jiale Wang", "Xi Yang", "Mutian Xu", "Xu Gu", "Nannan Wang"], "title": "Unleashing the Multi-View Fusion Potential: Noise Correction in VLM for Open-Vocabulary 3D Scene Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Recent open-vocabulary 3D scene understanding approaches mainly focus on\ntraining 3D networks through contrastive learning with point-text pairs or by\ndistilling 2D features into 3D models via point-pixel alignment. While these\nmethods show considerable performance in benchmarks with limited vocabularies,\nthey struggle to handle diverse object categories as the limited amount of 3D\ndata upbound training strong open-vocabulary 3d models. We observe that 2D\nmulti-view fusion methods take precedence in understanding diverse concepts in\n3D scenes. However, inherent noises in vision-language models lead multi-view\nfusion to sub-optimal performance. To this end, we introduce MVOV3D, a novel\napproach aimed at unleashing the potential of 2D multi-view fusion for\nopen-vocabulary 3D scene understanding. We focus on reducing the inherent\nnoises without training, thereby preserving the generalizability while\nenhancing open-world capabilities. Specifically, MVOV3D improves multi-view 2D\nfeatures by leveraging precise region-level image features and text features\nencoded by CLIP encoders and incorporates 3D geometric priors to optimize\nmulti-view fusion. Extensive experiments on various datasets demonstrate the\neffectiveness of our method. Notably, our MVOV3D achieves a new record with\n14.7% mIoU on ScanNet200 and 16.2% mIoU on Matterport160 for challenge\nopen-vocabulary semantic segmentation, outperforming current leading trained 3D\nnetworks by a significant margin.", "AI": {"tldr": "MVOV3D是一种新方法，通过减少2D多视图融合中的固有噪声，提升开放词汇3D场景理解能力，无需训练即可保持泛化性。", "motivation": "现有方法在有限词汇量的基准测试中表现良好，但难以处理多样化的对象类别，且3D数据量有限限制了开放词汇3D模型的训练。", "method": "MVOV3D利用CLIP编码器提取精确的区域级图像和文本特征，并结合3D几何先验优化多视图融合，减少固有噪声。", "result": "MVOV3D在ScanNet200和Matterport160上分别达到14.7%和16.2%的mIoU，显著优于当前领先的3D网络。", "conclusion": "MVOV3D通过优化多视图融合，显著提升了开放词汇3D场景理解的性能。"}}
{"id": "2402.09146", "pdf": "https://arxiv.org/pdf/2402.09146", "abs": "https://arxiv.org/abs/2402.09146", "authors": ["Muhammad Kashif", "Muhammad Shafique"], "title": "ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural Networks", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": null, "summary": "In this paper, we present a novel framework for enhancing the performance of\nQuanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional\nlayers and addressing the critical challenges associated with them. Traditional\nquanvolutional layers, although beneficial for feature extraction, have largely\nbeen static, offering limited adaptability. Unlike state-of-the-art, our\nresearch overcomes this limitation by enabling training within these layers,\nsignificantly increasing the flexibility and potential of QuNNs. However, the\nintroduction of multiple trainable quanvolutional layers induces complexities\nin gradient-based optimization, primarily due to the difficulty in accessing\ngradients across these layers. To resolve this, we propose a novel\narchitecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging\nthe concept of residual learning, which facilitates the flow of gradients by\nadding skip connections between layers. By inserting residual blocks between\nquanvolutional layers, we ensure enhanced gradient access throughout the\nnetwork, leading to improved training performance. Moreover, we provide\nempirical evidence on the strategic placement of these residual blocks within\nQuNNs. Through extensive experimentation, we identify an efficient\nconfiguration of residual blocks, which enables gradients across all the layers\nin the network that eventually results in efficient training. Our findings\nsuggest that the precise location of residual blocks plays a crucial role in\nmaximizing the performance gains in QuNNs. Our results mark a substantial step\nforward in the evolution of quantum deep learning, offering new avenues for\nboth theoretical development and practical quantum computing applications.", "AI": {"tldr": "提出了一种通过可训练量子卷积层和残差学习提升量子卷积神经网络（QuNNs）性能的新框架。", "motivation": "传统量子卷积层缺乏适应性，限制了QuNNs的潜力。本研究旨在通过可训练层和残差连接解决这一限制。", "method": "引入Residual Quanvolutional Neural Networks（ResQuNNs），利用残差块和跳过连接优化梯度流动。", "result": "实验证明残差块的策略性放置显著提升训练效率和性能。", "conclusion": "该研究为量子深度学习提供了理论和实践上的新方向。"}}
{"id": "2506.23235", "pdf": "https://arxiv.org/pdf/2506.23235", "abs": "https://arxiv.org/abs/2506.23235", "authors": ["Yi-Chen Li", "Tian Xu", "Yang Yu", "Xuqin Zhang", "Xiong-Hui Chen", "Zhongxiang Ling", "Ningjing Chao", "Lei Yuan", "Zhi-Hua Zhou"], "title": "Generalist Reward Models: Found Inside Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "The alignment of Large Language Models (LLMs) is critically dependent on\nreward models trained on costly human preference data. While recent work\nexplores bypassing this cost with AI feedback, these methods often lack a\nrigorous theoretical foundation. In this paper, we discover that a powerful\ngeneralist reward model is already latently present within any LLM trained via\nstandard next-token prediction. We prove that this endogenous reward is not a\nheuristic, but is theoretically equivalent to a reward function learned through\noffline inverse reinforcement learning. This connection allows us to directly\nelicit a high-quality reward signal from a base (pre-trained or supervised\nfine-tuned) model without any further training. Critically, we also prove that\nsubsequent reinforcement learning using this endogenous reward leads to a\npolicy with a provably superior error bound compared to the base model. To our\nbest knowledge, this is the first theoretical proof of the effectiveness of\nreinforcement learning for LLMs. Our experiments validate this theory,\ndemonstrating that our method not only outperforms existing LLM-as-a-judge\napproaches but can also surpass explicitly trained reward models. These\nfindings suggest that the reward modeling stage can be replaced by a principled\nmethod of eliciting the knowledge already captured during pre-training,\nheralding a more efficient, powerful, and scalable paradigm for LLMs alignment\nas well as multi-modal models.", "AI": {"tldr": "论文提出了一种从预训练语言模型中提取潜在奖励信号的方法，无需额外训练，证明了其理论有效性，并在实验中优于现有方法。", "motivation": "现有基于人类偏好数据的奖励模型成本高昂，且缺乏理论支持，因此探索如何从预训练模型中直接提取高质量奖励信号。", "method": "通过理论证明，预训练语言模型中已存在与离线逆强化学习等效的潜在奖励信号，可直接提取并用于强化学习。", "result": "实验表明，该方法优于现有LLM-as-a-judge方法，甚至超过显式训练的奖励模型。", "conclusion": "研究为LLM对齐提供了一种高效、可扩展的新范式，替代了传统的奖励建模阶段。"}}
{"id": "2506.22819", "pdf": "https://arxiv.org/pdf/2506.22819", "abs": "https://arxiv.org/abs/2506.22819", "authors": ["Ramya Hebbalaguppe", "Tamoghno Kandar", "Abhinav Nagpal", "Chetan Arora"], "title": "Prompting without Panic: Attribute-aware, Zero-shot, Test-Time Calibration", "categories": ["cs.CV", "cs.LG"], "comment": "26 pages", "summary": "Vision-language models (VLM) have demonstrated impressive performance in\nimage recognition by leveraging self-supervised training on large datasets.\nTheir performance can be further improved by adapting to the test sample using\ntest-time prompt tuning (TPT). Unfortunately, the singular focus of TPT\napproaches on improving the accuracy suffers from tunnel vision, and leads to\ndegradation in confidence calibration. This limits the applicability of TPT in\ncritical applications.\n  We make three contributions in this work. (1) We posit that random or naive\ninitialization of prompts leads to overfitting on a particular test sample, and\nis the main reason for miscalibration of the VLM after TPT. To mitigate the\nproblem, we propose careful initialization of test time prompt using prior\nknowledge about the target label attributes from a large language model (LLM);\n(2) To further maintain the quality of prompts during \\tpt, we propose a novel\nregularization loss to reduce intraclass distance, and increase inter-class\ndistance between the learnt\n  Through extensive experiments on different CLIP architectures and 15\ndatasets, we show that our approach can effectively improve the calibration\nafter TPT. We report an average expected calibration error (ECE) of 4.11 with\nour method, TCA, compared to 11.7 for vanilla TPT, 6.12 for C-TPT (ICLR'24),\n6.78 for DiffTPT (CVPR'23), and 8.43 for PromptAlign (NeurIPS'23). The code is\npublicly accessible at:\nhttps://github.com/rhebbalaguppe/TCA_PromptWithoutPanic.", "AI": {"tldr": "本文提出了一种改进测试时间提示调优（TPT）的方法，通过利用大语言模型（LLM）初始化提示并引入正则化损失，显著提升了视觉语言模型（VLM）的校准性能。", "motivation": "现有TPT方法虽能提升准确性，但会导致置信度校准问题，限制了其在关键应用中的适用性。", "method": "1. 利用LLM初始化提示以减少过拟合；2. 提出正则化损失以减少类内距离并增加类间距离。", "result": "在15个数据集上，TCA方法的平均ECE为4.11，显著优于其他方法。", "conclusion": "TCA方法有效解决了TPT的校准问题，提升了VLM的可靠性。"}}
{"id": "2504.15071", "pdf": "https://arxiv.org/pdf/2504.15071", "abs": "https://arxiv.org/abs/2504.15071", "authors": ["Louis Bradshaw", "Simon Colton"], "title": "Aria-MIDI: A Dataset of Piano MIDI Files for Symbolic Music Modeling", "categories": ["cs.SD", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce an extensive new dataset of MIDI files, created by transcribing\naudio recordings of piano performances into their constituent notes. The data\npipeline we use is multi-stage, employing a language model to autonomously\ncrawl and score audio recordings from the internet based on their metadata,\nfollowed by a stage of pruning and segmentation using an audio classifier. The\nresulting dataset contains over one million distinct MIDI files, comprising\nroughly 100,000 hours of transcribed audio. We provide an in-depth analysis of\nour techniques, offering statistical insights, and investigate the content by\nextracting metadata tags, which we also provide. Dataset available at\nhttps://github.com/loubbrad/aria-midi.", "AI": {"tldr": "论文介绍了一个通过音频转录生成的大规模MIDI数据集，包含100万+文件，约10万小时内容，并提供了数据收集、处理及分析的详细方法。", "motivation": "为音乐研究和机器学习提供高质量的MIDI数据集，填补现有资源的不足。", "method": "采用多阶段数据管道，包括语言模型自动爬取音频、音频分类器修剪和分段，最终转录为MIDI文件。", "result": "生成了超过100万个MIDI文件，约10万小时的转录音频，并提供了详细的统计分析和元数据标签。", "conclusion": "该数据集为音乐研究和机器学习提供了丰富资源，方法具有可扩展性和高效性。"}}
{"id": "2506.23288", "pdf": "https://arxiv.org/pdf/2506.23288", "abs": "https://arxiv.org/abs/2506.23288", "authors": ["Miguel Domingo", "Francisco Casacuberta"], "title": "Two Spelling Normalization Approaches Based on Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "The absence of standardized spelling conventions and the organic evolution of\nhuman language present an inherent linguistic challenge within historical\ndocuments, a longstanding concern for scholars in the humanities. Addressing\nthis issue, spelling normalization endeavors to align a document's orthography\nwith contemporary standards. In this study, we propose two new approaches based\non large language models: one of which has been trained without a supervised\ntraining, and a second one which has been trained for machine translation. Our\nevaluation spans multiple datasets encompassing diverse languages and\nhistorical periods, leading us to the conclusion that while both of them\nyielded encouraging results, statistical machine translation still seems to be\nthe most suitable technology for this task.", "AI": {"tldr": "论文提出两种基于大语言模型的拼写规范化方法，一种无监督训练，另一种基于机器翻译，实验表明机器翻译更适合此任务。", "motivation": "历史文献中拼写不规范的问题长期困扰人文研究者，拼写规范化旨在将其对齐到现代标准。", "method": "提出两种方法：一种无监督训练的大语言模型，另一种基于机器翻译的模型。", "result": "在多语言和多历史时期数据集上评估，两种方法均有不错效果，但机器翻译表现更优。", "conclusion": "统计机器翻译仍是拼写规范化任务的最合适技术。"}}
{"id": "2506.22832", "pdf": "https://arxiv.org/pdf/2506.22832", "abs": "https://arxiv.org/abs/2506.22832", "authors": ["Alexander Gambashidze", "Li Pengyi", "Matvey Skripkin", "Andrey Galichin", "Anton Gusarov", "Konstantin Sobolev", "Andrey Kuznetsov", "Ivan Oseledets"], "title": "Listener-Rewarded Thinking in VLMs for Image Preferences", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Training robust and generalizable reward models for human visual preferences\nis essential for aligning text-to-image and text-to-video generative models\nwith human intent. However, current reward models often fail to generalize, and\nsupervised fine-tuning leads to memorization, demanding complex annotation\npipelines. While reinforcement learning (RL), specifically Group Relative\nPolicy Optimization (GRPO), improves generalization, we uncover a key failure\nmode: a significant drop in reasoning accuracy occurs when a model's reasoning\ntrace contradicts that of an independent, frozen vision-language model\n(\"listener\") evaluating the same output. To address this, we introduce a\nlistener-augmented GRPO framework. Here, the listener re-evaluates the\nreasoner's chain-of-thought to provide a dense, calibrated confidence score,\nshaping the RL reward signal. This encourages the reasoner not only to answer\ncorrectly, but to produce explanations that are persuasive to an independent\nmodel. Our listener-shaped reward scheme achieves best accuracy on the\nImageReward benchmark (67.4%), significantly improves out-of-distribution (OOD)\nperformance on a large-scale human preference dataset (1.2M votes, up to +6%\nover naive reasoner), and reduces reasoning contradictions compared to strong\nGRPO and SFT baselines. These results demonstrate that listener-based rewards\nprovide a scalable, data-efficient path to aligning vision-language models with\nnuanced human preferences. We will release our reasoning model here:\nhttps://huggingface.co/alexgambashidze/qwen2.5vl_image_preference_reasoner.", "AI": {"tldr": "论文提出了一种基于听众增强的GRPO框架，通过引入独立的视觉语言模型（听众）重新评估推理链，生成密集校准的置信度分数，从而优化强化学习奖励信号，显著提升了模型的泛化能力和推理准确性。", "motivation": "现有奖励模型在泛化性和推理准确性上表现不佳，监督微调容易导致记忆化问题，需要复杂的标注流程。", "method": "提出听众增强的GRPO框架，听众模型重新评估推理链并提供置信度分数，优化奖励信号。", "result": "在ImageReward基准测试中达到67.4%的准确率，OOD性能提升6%，减少推理矛盾。", "conclusion": "听众增强的奖励机制为视觉语言模型与人类偏好对齐提供了高效、可扩展的解决方案。"}}
{"id": "2506.23293", "pdf": "https://arxiv.org/pdf/2506.23293", "abs": "https://arxiv.org/abs/2506.23293", "authors": ["P. Myles Eugenio"], "title": "Objective-Free Local Learning and Emergent Language Structure in Thinking Machines", "categories": ["cs.CL", "cs.AI", "cs.LG", "q-bio.NC"], "comment": "22 pages, 7 figures", "summary": "We present a neuro-symbolic framework for generative language modeling based\non local, event-driven emergent learning. At its core is a hierarchical\nHopfield memory chain acting as a compositional short-term memory and dynamic\ntokenizer (retokenizer). Rather than relying on predefined tokens or\nsupervision, the model builds structure from scratch, learning symbol sequences\nas multi-scale representations. It constructs projection tensors that bind\nco-occurring features into hierarchical tokens, introducing redundancy (i.e an\nemergent gauge structure) and enabling compression of local activations into\nlong-range dependencies. Curiously, we find that the retokenizer can filter\nnatural language patterns from noise, generating synthetic languages with\ncoherent internal morphology -- quantifiably the same as human language.\nLanguage is learned in a local (Hebbian) fashion, where model constraints\ndictate allowed emergent structure, and new information is retained in\nalignment with this structure. The absence of a global objective enables a form\nof plasticity not found in conventional language models, allowing the system to\ngeneralize beyond its initial inference class -- even without explicit data. We\ndemonstrate that briefly activating a new neuron during inference binds\ndistributed multi-scale token features into a symbolic embedding. These\nemergent embedding neurons act as long-term memory and support a key-value\nmechanism for compositional inference and generalization. This architecture\nprovides a methodological foundation for studying how symbolic structure can\nemerge from local neural learning. It offers a new pathway for building\nscalable, interpretable neuro-symbolic systems -- where tokens, grammar, and\nreasoning arise as compressed memory traces within a Hopfield hierarchy. This\napproach advances the development of neuromorphic architectures for generative\nlanguage models.", "AI": {"tldr": "提出了一种基于局部事件驱动涌现学习的神经符号生成语言模型框架，核心是分层Hopfield记忆链，作为组合短期记忆和动态分词器。模型从零构建结构，学习符号序列为多尺度表示，生成具有内部形态一致性的合成语言。", "motivation": "探索如何通过局部神经学习涌现符号结构，构建可扩展且可解释的神经符号系统，推动生成语言模型的神经形态架构发展。", "method": "使用分层Hopfield记忆链作为动态分词器，通过投影张量绑定共现特征为分层标记，引入冗余并压缩局部激活为长程依赖。", "result": "模型能从噪声中过滤自然语言模式，生成具有人类语言一致性的合成语言，并通过局部学习实现超越初始推理类的泛化。", "conclusion": "该框架为研究符号结构如何从局部神经学习中涌现提供了方法论基础，为构建可扩展、可解释的神经符号系统开辟了新途径。"}}
{"id": "2506.22833", "pdf": "https://arxiv.org/pdf/2506.22833", "abs": "https://arxiv.org/abs/2506.22833", "authors": ["Shashikant Verma", "Shanmuganathan Raman"], "title": "SemFaceEdit: Semantic Face Editing on Generative Radiance Manifolds", "categories": ["cs.CV"], "comment": null, "summary": "Despite multiple view consistency offered by 3D-aware GAN techniques, the\nresulting images often lack the capacity for localized editing. In response,\ngenerative radiance manifolds emerge as an efficient approach for constrained\npoint sampling within volumes, effectively reducing computational demands and\nenabling the learning of fine details. This work introduces SemFaceEdit, a\nnovel method that streamlines the appearance and geometric editing process by\ngenerating semantic fields on generative radiance manifolds. Utilizing latent\ncodes, our method effectively disentangles the geometry and appearance\nassociated with different facial semantics within the generated image. In\ncontrast to existing methods that can change the appearance of the entire\nradiance field, our method enables the precise editing of particular facial\nsemantics while preserving the integrity of other regions. Our network\ncomprises two key modules: the Geometry module, which generates semantic\nradiance and occupancy fields, and the Appearance module, which is responsible\nfor predicting RGB radiance. We jointly train both modules in adversarial\nsettings to learn semantic-aware geometry and appearance descriptors. The\nappearance descriptors are then conditioned on their respective semantic latent\ncodes by the Appearance Module, facilitating disentanglement and enhanced\ncontrol. Our experiments highlight SemFaceEdit's superior performance in\nsemantic field-based editing, particularly in achieving improved radiance field\ndisentanglement.", "AI": {"tldr": "SemFaceEdit是一种基于生成辐射流形的新方法，通过语义场实现面部图像的局部编辑，同时保持其他区域的完整性。", "motivation": "现有3D感知GAN技术虽然提供多视角一致性，但缺乏局部编辑能力，因此需要一种更高效的方法来实现精细编辑。", "method": "SemFaceEdit通过几何模块和外观模块联合训练，生成语义辐射和占用场，并利用潜在码解耦几何与外观。", "result": "实验表明，SemFaceEdit在语义场编辑和辐射场解耦方面表现优异。", "conclusion": "SemFaceEdit为面部图像的局部编辑提供了一种高效且精确的解决方案。"}}
{"id": "2506.22441", "pdf": "https://arxiv.org/pdf/2506.22441", "abs": "https://arxiv.org/abs/2506.22441", "authors": ["Lei Yang"], "title": "Latent Factorization of Tensors with Threshold Distance Weighted Loss for Traffic Data Estimation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Intelligent transportation systems (ITS) rely heavily on complete and\nhigh-quality spatiotemporal traffic data to achieve optimal performance.\nNevertheless, in real-word traffic data collection processes, issues such as\ncommunication failures and sensor malfunctions often lead to incomplete or\ncorrupted datasets, thereby posing significant challenges to the advancement of\nITS. Among various methods for imputing missing spatiotemporal traffic data,\nthe latent factorization of tensors (LFT) model has emerged as a widely adopted\nand effective solution. However, conventional LFT models typically employ the\nstandard L2-norm in their learning objective, which makes them vulnerable to\nthe influence of outliers. To overcome this limitation, this paper proposes a\nthreshold distance weighted (TDW) loss-incorporated Latent Factorization of\nTensors (TDWLFT) model. The proposed loss function effectively reduces the\nmodel's sensitivity to outliers by assigning differentiated weights to\nindividual samples. Extensive experiments conducted on two traffic speed\ndatasets sourced from diverse urban environments confirm that the proposed\nTDWLFT model consistently outperforms state-of-the-art approaches in terms of\nboth in both prediction accuracy and computational efficiency.", "AI": {"tldr": "论文提出了一种基于阈值距离加权损失（TDW）的张量潜在因子分解模型（TDWLFT），用于处理交通数据中的缺失值和异常值，提高了预测精度和计算效率。", "motivation": "智能交通系统（ITS）依赖高质量时空交通数据，但实际数据常因通信故障或传感器故障而缺失或损坏。现有张量潜在因子分解模型（LFT）对异常值敏感，需改进。", "method": "提出TDWLFT模型，采用阈值距离加权损失函数，通过为样本分配不同权重减少异常值影响。", "result": "在两个城市交通速度数据集上的实验表明，TDWLFT在预测精度和计算效率上均优于现有方法。", "conclusion": "TDWLFT模型有效解决了异常值问题，提升了交通数据补全的性能。"}}
{"id": "2506.23315", "pdf": "https://arxiv.org/pdf/2506.23315", "abs": "https://arxiv.org/abs/2506.23315", "authors": ["Shouvon Sarker", "Xishuang Dong", "Lijun Qian"], "title": "Ensemble BERT for Medication Event Classification on Electronic Health Records (EHRs)", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Identification of key variables such as medications, diseases, relations from\nhealth records and clinical notes has a wide range of applications in the\nclinical domain. n2c2 2022 provided shared tasks on challenges in natural\nlanguage processing for clinical data analytics on electronic health records\n(EHR), where it built a comprehensive annotated clinical data Contextualized\nMedication Event Dataset (CMED). This study focuses on subtask 2 in Track 1 of\nthis challenge that is to detect and classify medication events from clinical\nnotes through building a novel BERT-based ensemble model. It started with\npretraining BERT models on different types of big data such as Wikipedia and\nMIMIC. Afterwards, these pretrained BERT models were fine-tuned on CMED\ntraining data. These fine-tuned BERT models were employed to accomplish\nmedication event classification on CMED testing data with multiple predictions.\nThese multiple predictions generated by these fine-tuned BERT models were\nintegrated to build final prediction with voting strategies. Experimental\nresults demonstrated that BERT-based ensemble models can effectively improve\nstrict Micro-F score by about 5% and strict Macro-F score by about 6%,\nrespectively.", "AI": {"tldr": "研究通过构建基于BERT的集成模型，从临床笔记中检测和分类药物事件，显著提升了性能指标。", "motivation": "识别健康记录和临床笔记中的关键变量（如药物、疾病、关系）在临床领域有广泛应用。n2c2 2022提供了共享任务，旨在通过自然语言处理分析电子健康记录（EHR）中的临床数据。", "method": "研究采用预训练的BERT模型（基于Wikipedia和MIMIC数据），并在CMED训练数据上微调。通过集成多个微调BERT模型的预测结果，采用投票策略生成最终预测。", "result": "实验结果表明，基于BERT的集成模型显著提升了严格Micro-F分数（约5%）和严格Macro-F分数（约6%）。", "conclusion": "基于BERT的集成模型能有效提升药物事件分类的性能，为临床数据分析提供了有力工具。"}}
{"id": "2506.22836", "pdf": "https://arxiv.org/pdf/2506.22836", "abs": "https://arxiv.org/abs/2506.22836", "authors": ["Hongyan An", "Kuan Zhu", "Xin He", "Haiyun Guo", "Chaoyang Zhao", "Ming Tang", "Jinqiao Wang"], "title": "FOCUS: Fine-grained Optimization with Semantic Guided Understanding for Pedestrian Attributes Recognition", "categories": ["cs.CV"], "comment": "ICME 2025 Oral", "summary": "Pedestrian attribute recognition (PAR) is a fundamental perception task in\nintelligent transportation and security. To tackle this fine-grained task, most\nexisting methods focus on extracting regional features to enrich attribute\ninformation. However, a regional feature is typically used to predict a fixed\nset of pre-defined attributes in these methods, which limits the performance\nand practicality in two aspects: 1) Regional features may compromise\nfine-grained patterns unique to certain attributes in favor of capturing common\ncharacteristics shared across attributes. 2) Regional features cannot\ngeneralize to predict unseen attributes in the test time. In this paper, we\npropose the \\textbf{F}ine-grained \\textbf{O}ptimization with semanti\\textbf{C}\ng\\textbf{U}ided under\\textbf{S}tanding (FOCUS) approach for PAR, which\nadaptively extracts fine-grained attribute-level features for each attribute\nindividually, regardless of whether the attributes are seen or not during\ntraining. Specifically, we propose the Multi-Granularity Mix Tokens (MGMT) to\ncapture latent features at varying levels of visual granularity, thereby\nenriching the diversity of the extracted information. Next, we introduce the\nAttribute-guided Visual Feature Extraction (AVFE) module, which leverages\ntextual attributes as queries to retrieve their corresponding visual attribute\nfeatures from the Mix Tokens using a cross-attention mechanism. To ensure that\ntextual attributes focus on the appropriate Mix Tokens, we further incorporate\na Region-Aware Contrastive Learning (RACL) method, encouraging attributes\nwithin the same region to share consistent attention maps. Extensive\nexperiments on PA100K, PETA, and RAPv1 datasets demonstrate the effectiveness\nand strong generalization ability of our method.", "AI": {"tldr": "提出FOCUS方法，通过多粒度混合令牌和属性引导的视觉特征提取模块，自适应提取细粒度属性级特征，解决行人属性识别中区域特征的局限性。", "motivation": "现有方法通常使用区域特征预测固定属性集，限制了性能和实用性，无法泛化到未见属性。", "method": "提出多粒度混合令牌（MGMT）捕获不同视觉粒度特征，属性引导视觉特征提取（AVFE）模块利用文本属性查询视觉特征，并结合区域感知对比学习（RACL）优化注意力。", "result": "在PA100K、PETA和RAPv1数据集上验证了方法的有效性和强泛化能力。", "conclusion": "FOCUS方法通过自适应提取细粒度特征，显著提升了行人属性识别的性能，并能泛化到未见属性。"}}
{"id": "2506.22445", "pdf": "https://arxiv.org/pdf/2506.22445", "abs": "https://arxiv.org/abs/2506.22445", "authors": ["Saad Alqithami"], "title": "Hierarchical Adversarially-Resilient Multi-Agent Reinforcement Learning for Cyber-Physical Systems Security", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.MA"], "comment": null, "summary": "Cyber-Physical Systems play a critical role in the infrastructure of various\nsectors, including manufacturing, energy distribution, and autonomous\ntransportation systems. However, their increasing connectivity renders them\nhighly vulnerable to sophisticated cyber threats, such as adaptive and zero-day\nattacks, against which traditional security methods like rule-based intrusion\ndetection and single-agent reinforcement learning prove insufficient. To\novercome these challenges, this paper introduces a novel Hierarchical\nAdversarially-Resilient Multi-Agent Reinforcement Learning (HAMARL) framework.\nHAMARL employs a hierarchical structure consisting of local agents dedicated to\nsubsystem security and a global coordinator that oversees and optimizes\ncomprehensive, system-wide defense strategies. Furthermore, the framework\nincorporates an adversarial training loop designed to simulate and anticipate\nevolving cyber threats, enabling proactive defense adaptation. Extensive\nexperimental evaluations conducted on a simulated industrial IoT testbed\nindicate that HAMARL substantially outperforms traditional multi-agent\nreinforcement learning approaches, significantly improving attack detection\naccuracy, reducing response times, and ensuring operational continuity. The\nresults underscore the effectiveness of combining hierarchical multi-agent\ncoordination with adversarially-aware training to enhance the resilience and\nsecurity of next-generation CPS.", "AI": {"tldr": "本文提出了一种新型的分层对抗弹性多智能体强化学习框架（HAMARL），用于提升网络物理系统的安全性，显著优于传统方法。", "motivation": "网络物理系统（CPS）在多个关键领域发挥重要作用，但其日益增加的连接性使其面临复杂的网络威胁，传统安全方法难以应对。", "method": "HAMARL采用分层结构，包括专注于子系统安全的本地智能体和全局协调器，并结合对抗训练循环以模拟和预测威胁。", "result": "在模拟工业物联网测试平台上的实验表明，HAMARL显著提高了攻击检测准确性、减少了响应时间，并确保了操作连续性。", "conclusion": "结合分层多智能体协调和对抗感知训练，HAMARL有效提升了下一代CPS的弹性和安全性。"}}
{"id": "2506.23340", "pdf": "https://arxiv.org/pdf/2506.23340", "abs": "https://arxiv.org/abs/2506.23340", "authors": ["Yumeng Lin", "Xufeng Duan", "David Haslett", "Yige Chen", "Zhenguang G. Cai"], "title": "Information Loss in LLMs' Multilingual Translation: The Role of Training Data, Language Proximity, and Language Family", "categories": ["cs.CL"], "comment": null, "summary": "Large language models have achieved impressive progress in multilingual\ntranslation, yet they continue to face challenges with certain language\npairs-particularly those with limited training data or significant linguistic\ndivergence from English. This study systematically investigates how training\ndata, language proximity, and language family affect information loss in\nmultilingual translation. We evaluate two large language models, GPT-4 and\nLlama 2, by performing round-trip translations. Translation quality was\nassessed using BLEU scores and BERT similarity metrics. Our results reveal a\nrobust interaction between training data size and language distance: while\nabundant training data can mitigate the effects of linguistic divergence,\nlanguages structurally closer to English consistently yield higher translation\nquality in low-resource conditions. Among various distance metrics,\northographic, phylogenetic, syntactic, and geographical distances emerge as\nstrong predictors of translation performance. Language family also exerts an\nindependent influence. These findings contribute to a deeper understanding of\nthe linguistic constraints shaping multilingual translation in large language\nmodels, emphasizing that translation quality is shaped not only by data volume\nbut also by structural and typological relationships between languages.", "AI": {"tldr": "研究探讨了训练数据、语言距离和语系对多语言翻译中信息损失的影响，发现数据量和语言结构关系共同影响翻译质量。", "motivation": "尽管大语言模型在多语言翻译中取得进展，但在数据有限或与英语差异较大的语言对上仍面临挑战。", "method": "通过往返翻译评估GPT-4和Llama 2，使用BLEU分数和BERT相似度指标分析翻译质量。", "result": "训练数据量和语言距离交互作用显著；语言结构接近英语时，低资源条件下翻译质量更高。", "conclusion": "翻译质量不仅受数据量影响，还受语言结构和类型学关系制约。"}}
{"id": "2506.22843", "pdf": "https://arxiv.org/pdf/2506.22843", "abs": "https://arxiv.org/abs/2506.22843", "authors": ["Kien Nguyen", "Clinton Fookes", "Sridha Sridharan", "Huy Nguyen", "Feng Liu", "Xiaoming Liu", "Arun Ross", "Dana Michalski", "Tamás Endrei", "Ivan DeAndres-Tame", "Ruben Tolosana", "Ruben Vera-Rodriguez", "Aythami Morales", "Julian Fierrez", "Javier Ortega-Garcia", "Zijing Gong", "Yuhao Wang", "Xuehu Liu", "Pingping Zhang", "Md Rashidunnabi", "Hugo Proença", "Kailash A. Hambarde", "Saeid Rezaei"], "title": "AG-VPReID 2025: Aerial-Ground Video-based Person Re-identification Challenge Results", "categories": ["cs.CV"], "comment": null, "summary": "Person re-identification (ReID) across aerial and ground vantage points has\nbecome crucial for large-scale surveillance and public safety applications.\nAlthough significant progress has been made in ground-only scenarios, bridging\nthe aerial-ground domain gap remains a formidable challenge due to extreme\nviewpoint differences, scale variations, and occlusions. Building upon the\nachievements of the AG-ReID 2023 Challenge, this paper introduces the AG-VPReID\n2025 Challenge - the first large-scale video-based competition focused on\nhigh-altitude (80-120m) aerial-ground ReID. Constructed on the new AG-VPReID\ndataset with 3,027 identities, over 13,500 tracklets, and approximately 3.7\nmillion frames captured from UAVs, CCTV, and wearable cameras, the challenge\nfeatured four international teams. These teams developed solutions ranging from\nmulti-stream architectures to transformer-based temporal reasoning and\nphysics-informed modeling. The leading approach, X-TFCLIP from UAM, attained\n72.28% Rank-1 accuracy in the aerial-to-ground ReID setting and 70.77% in the\nground-to-aerial ReID setting, surpassing existing baselines while highlighting\nthe dataset's complexity. For additional details, please refer to the official\nwebsite at https://agvpreid25.github.io.", "AI": {"tldr": "AG-VPReID 2025 Challenge是首个专注于高空（80-120米）无人机与地面摄像头间行人重识别的大规模视频竞赛，基于新数据集AG-VPReID，包含3,027个身份和约370万帧。最佳方法X-TFCLIP在无人机到地面和地面到无人机的重识别中分别达到72.28%和70.77%的Rank-1准确率。", "motivation": "解决无人机与地面摄像头间行人重识别的挑战，如视角差异、尺度变化和遮挡，以提升大规模监控和公共安全应用的效果。", "method": "竞赛团队采用了多流架构、基于Transformer的时序推理和物理信息建模等方法。", "result": "最佳方法X-TFCLIP在无人机到地面和地面到无人机的重识别中分别达到72.28%和70.77%的Rank-1准确率。", "conclusion": "AG-VPReID 2025 Challenge展示了高空与地面间行人重识别的复杂性，并为未来研究提供了新的基准。"}}
{"id": "2506.22446", "pdf": "https://arxiv.org/pdf/2506.22446", "abs": "https://arxiv.org/abs/2506.22446", "authors": ["Aakash Tripathi", "Asim Waqas", "Matthew B. Schabath", "Yasin Yilmaz", "Ghulam Rasool"], "title": "EAGLE: Efficient Alignment of Generalized Latent Embeddings for Multimodal Survival Prediction with Interpretable Attribution Analysis", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate cancer survival prediction requires integration of diverse data\nmodalities that reflect the complex interplay between imaging, clinical\nparameters, and textual reports. However, existing multimodal approaches suffer\nfrom simplistic fusion strategies, massive computational requirements, and lack\nof interpretability-critical barriers to clinical adoption. We present EAGLE\n(Efficient Alignment of Generalized Latent Embeddings), a novel deep learning\nframework that addresses these limitations through attention-based multimodal\nfusion with comprehensive attribution analysis. EAGLE introduces four key\ninnovations: (1) dynamic cross-modal attention mechanisms that learn\nhierarchical relationships between modalities, (2) massive dimensionality\nreduction (99.96%) while maintaining predictive performance, (3) three\ncomplementary attribution methods providing patient-level interpretability, and\n(4) a unified pipeline enabling seamless adaptation across cancer types. We\nevaluated EAGLE on 911 patients across three distinct malignancies:\nglioblastoma (GBM, n=160), intraductal papillary mucinous neoplasms (IPMN,\nn=171), and non-small cell lung cancer (NSCLC, n=580). Patient-level analysis\nshowed high-risk individuals relied more heavily on adverse imaging features,\nwhile low-risk patients demonstrated balanced modality contributions. Risk\nstratification identified clinically meaningful groups with 4-fold (GBM) to\n5-fold (NSCLC) differences in median survival, directly informing treatment\nintensity decisions. By combining state-of-the-art performance with clinical\ninterpretability, EAGLE bridges the gap between advanced AI capabilities and\npractical healthcare deployment, offering a scalable solution for multimodal\nsurvival prediction that enhances both prognostic accuracy and physician trust\nin automated predictions.", "AI": {"tldr": "EAGLE是一种新型深度学习框架，通过注意力机制和多模态融合解决癌症生存预测中的计算和可解释性问题。", "motivation": "现有方法在多模态融合上过于简单，计算量大且缺乏可解释性，限制了临床应用。", "method": "EAGLE采用动态跨模态注意力机制、降维技术、三种归因方法和统一管道。", "result": "在三种癌症类型中验证，EAGLE能有效区分高低风险患者，生存时间差异显著。", "conclusion": "EAGLE结合高性能和临床可解释性，为多模态生存预测提供了实用解决方案。"}}
{"id": "2506.23342", "pdf": "https://arxiv.org/pdf/2506.23342", "abs": "https://arxiv.org/abs/2506.23342", "authors": ["Akim Tsvigun", "Daniil Vasilev", "Ivan Tsvigun", "Ivan Lysenko", "Talgat Bektleuov", "Aleksandr Medvedev", "Uliana Vinogradova", "Nikita Severin", "Mikhail Mozikov", "Andrey Savchenko", "Rostislav Grigorev", "Ramil Kuleev", "Fedor Zhdanov", "Artem Shelmanov", "Ilya Makarov"], "title": "ATGen: A Framework for Active Text Generation", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ACL 2025 System Demonstrations", "summary": "Active learning (AL) has demonstrated remarkable potential in reducing the\nannotation effort required for training machine learning models. However,\ndespite the surging popularity of natural language generation (NLG) tasks in\nrecent years, the application of AL to NLG has been limited. In this paper, we\nintroduce Active Text Generation (ATGen) - a comprehensive framework that\nbridges AL with text generation tasks, enabling the application of\nstate-of-the-art AL strategies to NLG. Our framework simplifies AL-empowered\nannotation in NLG tasks using both human annotators and automatic annotation\nagents based on large language models (LLMs). The framework supports LLMs\ndeployed as services, such as ChatGPT and Claude, or operated on-premises.\nFurthermore, ATGen provides a unified platform for smooth implementation and\nbenchmarking of novel AL strategies tailored to NLG tasks. Finally, we present\nevaluation results for state-of-the-art AL strategies across diverse settings\nand multiple text generation tasks. We show that ATGen reduces both the effort\nof human annotators and costs associated with API calls to LLM-based annotation\nagents. The code of the framework is available on GitHub under the MIT license.\nThe video presentation is available at http://atgen-video.nlpresearch.group", "AI": {"tldr": "ATGen框架将主动学习（AL）与文本生成任务结合，减少人工标注和API调用成本，支持多种AL策略。", "motivation": "尽管AL在减少标注需求方面潜力巨大，但其在自然语言生成（NLG）任务中的应用有限。ATGen旨在填补这一空白。", "method": "ATGen是一个综合框架，支持人类标注者和基于大语言模型（LLM）的自动标注代理，适用于多种AL策略。", "result": "ATGen显著减少了人工标注和LLM API调用的成本，并在多种文本生成任务中验证了其有效性。", "conclusion": "ATGen为NLG任务提供了一个统一的AL实现和评估平台，具有实际应用价值。"}}
{"id": "2506.22850", "pdf": "https://arxiv.org/pdf/2506.22850", "abs": "https://arxiv.org/abs/2506.22850", "authors": ["Aalok Gangopadhyay", "Shashikant Verma", "Shanmuganathan Raman"], "title": "DMD-Net: Deep Mesh Denoising Network", "categories": ["cs.CV"], "comment": null, "summary": "We present Deep Mesh Denoising Network (DMD-Net), an end-to-end deep learning\nframework, for solving the mesh denoising problem. DMD-Net consists of a Graph\nConvolutional Neural Network in which aggregation is performed in both the\nprimal as well as the dual graph. This is realized in the form of an asymmetric\ntwo-stream network, which contains a primal-dual fusion block that enables\ncommunication between the primal-stream and the dual-stream. We develop a\nFeature Guided Transformer (FGT) paradigm, which consists of a feature\nextractor, a transformer, and a denoiser. The feature extractor estimates the\nlocal features, that guide the transformer to compute a transformation, which\nis applied to the noisy input mesh to obtain a useful intermediate\nrepresentation. This is further processed by the denoiser to obtain the\ndenoised mesh. Our network is trained on a large scale dataset of 3D objects.\nWe perform exhaustive ablation studies to demonstrate that each component in\nour network is essential for obtaining the best performance. We show that our\nmethod obtains competitive or better results when compared with the\nstate-of-the-art mesh denoising algorithms. We demonstrate that our method is\nrobust to various kinds of noise. We observe that even in the presence of\nextremely high noise, our method achieves excellent performance.", "AI": {"tldr": "DMD-Net是一种端到端的深度学习框架，用于解决网格去噪问题，通过图卷积神经网络和双流网络实现，性能优于现有方法。", "motivation": "解决网格去噪问题，提高去噪效果，尤其是在高噪声环境下。", "method": "使用图卷积神经网络（GCN）和双流网络，结合特征引导变换器（FGT）范式，包括特征提取器、变换器和去噪器。", "result": "在多种噪声条件下表现优异，性能优于现有方法，且在高噪声环境下仍能保持出色表现。", "conclusion": "DMD-Net是一种高效、鲁棒的网格去噪方法，适用于各种噪声场景。"}}
{"id": "2506.22447", "pdf": "https://arxiv.org/pdf/2506.22447", "abs": "https://arxiv.org/abs/2506.22447", "authors": ["Fabio Merizzi", "Harilaos Loukos"], "title": "Vision Transformers for Multi-Variable Climate Downscaling: Emulating Regional Climate Models with a Shared Encoder and Multi-Decoder Architecture", "categories": ["cs.LG", "cs.AI", "eess.IV"], "comment": null, "summary": "Global Climate Models (GCMs) are critical for simulating large-scale climate\ndynamics, but their coarse spatial resolution limits their applicability in\nregional studies. Regional Climate Models (RCMs) refine this through dynamic\ndownscaling, albeit at considerable computational cost and with limited\nflexibility. While deep learning has emerged as an efficient data-driven\nalternative, most existing studies have focused on single-variable models that\ndownscale one variable at a time. This approach can lead to limited contextual\nawareness, redundant computation, and lack of cross-variable interaction. Our\nstudy addresses these limitations by proposing a multi-task, multi-variable\nVision Transformer (ViT) architecture with a shared encoder and\nvariable-specific decoders (1EMD). The proposed architecture jointly predicts\nthree key climate variables: surface temperature (tas), wind speed (sfcWind),\nand 500 hPa geopotential height (zg500), directly from GCM-resolution inputs,\nemulating RCM-scale downscaling over Europe. We show that our multi-variable\napproach achieves positive cross-variable knowledge transfer and consistently\noutperforms single-variable baselines trained under identical conditions, while\nalso improving computational efficiency. These results demonstrate the\neffectiveness of multi-variable modeling for high-resolution climate\ndownscaling.", "AI": {"tldr": "论文提出了一种多任务、多变量的Vision Transformer架构（1EMD），用于联合预测三个关键气候变量，优于单变量方法并提高了计算效率。", "motivation": "全球气候模型（GCMs）分辨率低，区域气候模型（RCMs）计算成本高且灵活性有限。现有深度学习方法多为单变量模型，缺乏跨变量交互和上下文感知。", "method": "采用共享编码器和变量特定解码器的ViT架构，联合预测表面温度、风速和500 hPa位势高度。", "result": "多变量方法实现了跨变量知识转移，性能优于单变量基线，同时提高了计算效率。", "conclusion": "多变量建模在高分辨率气候降尺度中具有有效性。"}}
{"id": "2506.23377", "pdf": "https://arxiv.org/pdf/2506.23377", "abs": "https://arxiv.org/abs/2506.23377", "authors": ["Taejin Kim", "Siun-Chuon Mau", "Konrad Vesey"], "title": "Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs", "categories": ["cs.CL", "cs.AI"], "comment": "7 pages, 5 main pages of text, 5 figures, 2 tables. Research work\n  performed at CACI INTL INC", "summary": "Large language models (LLMs) are used in a variety of mission-critical roles.\nDue to the rapidly developing nature of LLMs, there is a lack of quantifiable\nunderstanding of the bias and perspective associated with LLM output. Inspired\nby this need, this paper considers the broader issue of perspective or\nviewpoint of general text and perspective control of large-language model (LLM)\noutput. Perspective-Dial consists of two main components: a (1) metric space,\ndubbed Perspective Space, that enables quantitative measurements of different\nperspectives regarding a topic, and the use of (2) Systematic Prompt\nEngineering that utilizes greedy-coordinate descent to control LLM output\nperspective based on measurement feedback from the Perspective Space. The\nempirical nature of the approach allows progress to side step a principled\nunderstanding of perspective or bias -- effectively quantifying and adjusting\noutputs for a variety of topics. Potential applications include detection,\ntracking and mitigation of LLM bias, narrative detection, sense making and\ntracking in public discourse, and debate bot advocating given perspective.", "AI": {"tldr": "论文提出了一种名为Perspective-Dial的方法，用于量化和控制大型语言模型（LLM）输出的视角或偏见。", "motivation": "由于LLM在关键任务中的广泛应用，但其输出中的偏见和视角缺乏量化理解，因此需要解决这一问题。", "method": "Perspective-Dial包括两个主要组件：Perspective Space（用于量化不同视角的度量空间）和Systematic Prompt Engineering（基于反馈调整LLM输出的视角）。", "result": "该方法能够量化并调整LLM输出的视角，适用于多种主题。", "conclusion": "Perspective-Dial为LLM的视角控制提供了实用工具，可应用于偏见检测、公共话语分析等场景。"}}
{"id": "2506.22864", "pdf": "https://arxiv.org/pdf/2506.22864", "abs": "https://arxiv.org/abs/2506.22864", "authors": ["Li-Cheng Shen", "Jih-Kang Hsieh", "Wei-Hua Li", "Chu-Song Chen"], "title": "Mask-aware Text-to-Image Retrieval: Referring Expression Segmentation Meets Cross-modal Retrieval", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "ICMR 2025", "summary": "Text-to-image retrieval (TIR) aims to find relevant images based on a textual\nquery, but existing approaches are primarily based on whole-image captions and\nlack interpretability. Meanwhile, referring expression segmentation (RES)\nenables precise object localization based on natural language descriptions but\nis computationally expensive when applied across large image collections. To\nbridge this gap, we introduce Mask-aware TIR (MaTIR), a new task that unifies\nTIR and RES, requiring both efficient image search and accurate object\nsegmentation. To address this task, we propose a two-stage framework,\ncomprising a first stage for segmentation-aware image retrieval and a second\nstage for reranking and object grounding with a multimodal large language model\n(MLLM). We leverage SAM 2 to generate object masks and Alpha-CLIP to extract\nregion-level embeddings offline at first, enabling effective and scalable\nonline retrieval. Secondly, MLLM is used to refine retrieval rankings and\ngenerate bounding boxes, which are matched to segmentation masks. We evaluate\nour approach on COCO and D$^3$ datasets, demonstrating significant improvements\nin both retrieval accuracy and segmentation quality over previous methods.", "AI": {"tldr": "MaTIR统一了文本到图像检索（TIR）和参考表达式分割（RES），提出了一种两阶段框架，结合分割感知的图像检索和多模态大语言模型（MLLM）进行重排序和对象定位，显著提升了检索和分割性能。", "motivation": "现有TIR方法缺乏可解释性，而RES在大规模图像集合中计算成本高，MaTIR旨在统一两者，实现高效图像搜索和精确对象分割。", "method": "提出两阶段框架：1）离线生成对象掩码和区域级嵌入；2）在线使用MLLM重排序和生成边界框，匹配分割掩码。", "result": "在COCO和D$^3$数据集上，MaTIR在检索精度和分割质量上均显著优于现有方法。", "conclusion": "MaTIR有效结合了TIR和RES的优势，为文本到图像检索和对象分割提供了高效且准确的解决方案。"}}
{"id": "2506.22448", "pdf": "https://arxiv.org/pdf/2506.22448", "abs": "https://arxiv.org/abs/2506.22448", "authors": ["Yu Ma", "Xingyu Zhou", "Xiao Li", "Le Liang", "Shi Jin"], "title": "Unsupervised Learning-Based Joint Resource Allocation and Beamforming Design for RIS-Assisted MISO-OFDMA Systems", "categories": ["eess.SP", "cs.AI", "cs.IT", "math.IT"], "comment": "Due to the limitation \"The abstract field cannot be longer than 1,920\n  characters\", the abstract here is shorter than that in the PDF file", "summary": "Reconfigurable intelligent surfaces (RIS) are key enablers for 6G wireless\nsystems. This paper studies downlink transmission in an RIS-assisted MISO-OFDMA\nsystem, addressing resource allocation challenges. A two-stage unsupervised\nlearning-based framework is proposed to jointly design RIS phase shifts, BS\nbeamforming, and resource block (RB) allocation. The framework includes\nBeamNet, which predicts RIS phase shifts from CSI, and AllocationNet, which\nallocates RBs using equivalent CSI derived from BeamNet outputs. Active\nbeamforming is implemented via maximum ratio transmission and water-filling. To\nhandle discrete constraints while ensuring differentiability, quantization and\nthe Gumbel-softmax trick are adopted. A customized loss and phased training\nenhance performance under QoS constraints. Simulations show the method achieves\n99.93% of the sum rate of the SCA baseline with only 0.036% of its runtime, and\nit remains robust across varying channel and user conditions.", "AI": {"tldr": "提出了一种基于无监督学习的框架，用于RIS辅助的MISO-OFDMA系统中的资源分配问题，显著提升了效率。", "motivation": "解决6G无线系统中RIS辅助下的资源分配挑战，提升系统性能。", "method": "采用两阶段无监督学习框架，结合BeamNet和AllocationNet设计RIS相位偏移和资源块分配，并利用量化与Gumbel-softmax技巧处理离散约束。", "result": "仿真显示，该方法达到了SCA基线99.93%的和速率，且运行时间仅为0.036%，并在不同信道和用户条件下表现稳健。", "conclusion": "该框架高效解决了RIS辅助系统的资源分配问题，具有实际应用潜力。"}}
{"id": "2506.23393", "pdf": "https://arxiv.org/pdf/2506.23393", "abs": "https://arxiv.org/abs/2506.23393", "authors": ["Eugene J. Yu", "Dawei Zhu", "Yifan Song", "Xiangyu Wong", "Jiebin Zhang", "Wenxuan Shi", "Xiaoguang Li", "Qun Liu", "Sujian Li"], "title": "Hierarchical Memory Organization for Wikipedia Generation", "categories": ["cs.CL", "cs.AI"], "comment": "ACL 2025 Main Conference", "summary": "Generating Wikipedia articles autonomously is a challenging task requiring\nthe integration of accurate, comprehensive, and well-structured information\nfrom diverse sources. This paper introduces the Memory Organization-based\nGeneration (MOG) framework, a novel approach to address these challenges by\nleveraging a hierarchical memory architecture. MOG extracts fine-grained memory\nunits from web documents, recursively organizes them into a Wikipedia-style\nhierarchical structure, and uses this structure to guide the generation\nprocess. This ensures alignment between memory and the article outline,\nimproving both informativeness and verifiability while minimizing\nhallucinations. Additionally, a citation module is implemented to enhance\ntraceability by linking every generated sentence to specific memory units.\nEvaluations on our newly created WikiStart dataset demonstrate that MOG\noutperforms baseline methods in producing informative and reliable articles,\nmaking it particularly robust in real-world scenarios.", "AI": {"tldr": "MOG框架通过分层记忆架构生成维基百科文章，提高信息准确性和可验证性。", "motivation": "解决从多源信息生成准确、全面且结构化的维基百科文章的挑战。", "method": "采用分层记忆架构，提取细粒度记忆单元并递归组织为维基百科式结构，指导生成过程。", "result": "在WikiStart数据集上表现优于基线方法，生成信息丰富且可靠的文章。", "conclusion": "MOG框架在真实场景中表现稳健，尤其适合生成高质量维基百科内容。"}}
{"id": "2506.22866", "pdf": "https://arxiv.org/pdf/2506.22866", "abs": "https://arxiv.org/abs/2506.22866", "authors": ["Hang-Cheng Dong", "Lu Zou", "Bingguo Liu", "Dong Ye", "Guodong Liu"], "title": "Region-Aware CAM: High-Resolution Weakly-Supervised Defect Segmentation via Salient Region Perception", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Surface defect detection plays a critical role in industrial quality\ninspection. Recent advances in artificial intelligence have significantly\nenhanced the automation level of detection processes. However, conventional\nsemantic segmentation and object detection models heavily rely on large-scale\nannotated datasets, which conflicts with the practical requirements of defect\ndetection tasks. This paper proposes a novel weakly supervised semantic\nsegmentation framework comprising two key components: a region-aware class\nactivation map (CAM) and pseudo-label training. To address the limitations of\nexisting CAM methods, especially low-resolution thermal maps, and insufficient\ndetail preservation, we introduce filtering-guided backpropagation (FGBP),\nwhich refines target regions by filtering gradient magnitudes to identify areas\nwith higher relevance to defects. Building upon this, we further develop a\nregion-aware weighted module to enhance spatial precision. Finally,\npseudo-label segmentation is implemented to refine the model's performance\niteratively. Comprehensive experiments on industrial defect datasets\ndemonstrate the superiority of our method. The proposed framework effectively\nbridges the gap between weakly supervised learning and high-precision defect\nsegmentation, offering a practical solution for resource-constrained industrial\nscenarios.", "AI": {"tldr": "提出了一种弱监督语义分割框架，结合区域感知CAM和伪标签训练，解决了缺陷检测中标注数据不足的问题。", "motivation": "工业缺陷检测依赖大规模标注数据，但实际任务中标注资源有限，需要弱监督方法。", "method": "引入过滤引导反向传播（FGBP）改进CAM，结合区域感知加权模块和伪标签训练提升分割精度。", "result": "在工业缺陷数据集上表现优越，实现了高精度分割。", "conclusion": "该框架有效填补了弱监督学习与高精度缺陷分割之间的差距，适用于资源受限的工业场景。"}}
{"id": "2506.22457", "pdf": "https://arxiv.org/pdf/2506.22457", "abs": "https://arxiv.org/abs/2506.22457", "authors": ["Iulia Orvas", "Andrei Radu", "Alessandra Galli", "Ana Neacsu", "Elisabetta Peri"], "title": "A Complex UNet Approach for Non-Invasive Fetal ECG Extraction Using Single-Channel Dry Textile Electrodes", "categories": ["eess.SP", "cs.AI"], "comment": null, "summary": "Continuous, non-invasive pregnancy monitoring is crucial for minimising\npotential complications. The fetal electrocardiogram (fECG) represents a\npromising tool for assessing fetal health beyond clinical environments.\nHome-based monitoring necessitates the use of a minimal number of comfortable\nand durable electrodes, such as dry textile electrodes. However, this setup\npresents many challenges, including increased noise and motion artefacts, which\ncomplicate the accurate extraction of fECG signals. To overcome these\nchallenges, we introduce a pioneering method for extracting fECG from\nsingle-channel recordings obtained using dry textile electrodes using AI\ntechniques. We created a new dataset by simulating abdominal recordings,\nincluding noise closely resembling real-world characteristics of in-vivo\nrecordings through dry textile electrodes, alongside mECG and fECG. To ensure\nthe reliability of the extracted fECG, we propose an innovative pipeline based\non a complex-valued denoising network, Complex UNet. Unlike previous approaches\nthat focused solely on signal magnitude, our method processes both real and\nimaginary components of the spectrogram, addressing phase information and\npreventing incongruous predictions. We evaluated our novel pipeline against\ntraditional, well-established approaches, on both simulated and real data in\nterms of fECG extraction and R-peak detection. The results showcase that our\nsuggested method achieves new state-of-the-art results, enabling an accurate\nextraction of fECG morphology across all evaluated settings. This method is the\nfirst to effectively extract fECG signals from single-channel recordings using\ndry textile electrodes, making a significant advancement towards a fully\nnon-invasive and self-administered fECG extraction solution.", "AI": {"tldr": "论文提出了一种基于AI的新方法，用于从单通道干纺织电极记录中提取胎儿心电图（fECG），解决了噪声和运动伪影问题，并展示了优于传统方法的效果。", "motivation": "家庭环境中使用干纺织电极进行连续、无创的胎儿健康监测面临噪声和运动伪影的挑战，需要一种可靠的方法来提取fECG信号。", "method": "提出了一种基于复杂值去噪网络（Complex UNet）的创新流程，处理频谱图的实部和虚部，解决了相位信息问题。", "result": "该方法在模拟和真实数据上均优于传统方法，实现了fECG形态的准确提取，达到了新的最先进水平。", "conclusion": "该方法是首个成功从单通道干纺织电极记录中提取fECG信号的技术，为非侵入式家庭监测提供了重要进展。"}}
{"id": "2506.23411", "pdf": "https://arxiv.org/pdf/2506.23411", "abs": "https://arxiv.org/abs/2506.23411", "authors": ["Jiale Zhang", "Zichong Wang", "Avash Palikhe", "Zhipeng Yin", "Wenbin Zhang"], "title": "Datasets for Fairness in Language Models: An In-Depth Survey", "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "Fairness benchmarks play a central role in shaping how we evaluate language\nmodels, yet surprisingly little attention has been given to examining the\ndatasets that these benchmarks rely on. This survey addresses that gap by\npresenting a broad and careful review of the most widely used fairness datasets\nin current language model research, characterizing them along several key\ndimensions including their origin, scope, content, and intended use to help\nresearchers better appreciate the assumptions and limitations embedded in these\nresources. To support more meaningful comparisons and analyses, we introduce a\nunified evaluation framework that reveals consistent patterns of demographic\ndisparities across datasets and scoring methods. Applying this framework to\ntwenty four common benchmarks, we highlight the often overlooked biases that\ncan influence conclusions about model fairness and offer practical guidance for\nselecting, combining, and interpreting these datasets. We also point to\nopportunities for creating new fairness benchmarks that reflect more diverse\nsocial contexts and encourage more thoughtful use of these tools going forward.\nAll code, data, and detailed results are publicly available at\nhttps://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets\nto promote transparency and reproducibility across the research community.", "AI": {"tldr": "该论文综述了语言模型公平性基准的数据集，分析了其来源、范围和局限性，并提出了统一评估框架，揭示了数据集中的人口差异和潜在偏见。", "motivation": "现有公平性基准的数据集缺乏系统审查，可能导致评估偏差，因此需要全面分析以提升研究透明度和实用性。", "method": "通过广泛审查24个常用公平性数据集，提出统一评估框架，分析其人口差异和偏见。", "result": "研究发现数据集存在一致的偏见模式，影响模型公平性结论，并提供了数据集选择和使用的实用建议。", "conclusion": "论文呼吁开发更多样化的公平性基准，并更谨慎地使用现有数据集，所有数据和代码公开以促进可重复性。"}}
{"id": "2506.22868", "pdf": "https://arxiv.org/pdf/2506.22868", "abs": "https://arxiv.org/abs/2506.22868", "authors": ["Junsung Lee", "Junoh Kang", "Bohyung Han"], "title": "STR-Match: Matching SpatioTemporal Relevance Score for Training-Free Video Editing", "categories": ["cs.CV", "cs.AI"], "comment": "15 pages, 9 figures, 3 tables", "summary": "Previous text-guided video editing methods often suffer from temporal\ninconsistency, motion distortion, and-most notably-limited domain\ntransformation. We attribute these limitations to insufficient modeling of\nspatiotemporal pixel relevance during the editing process. To address this, we\npropose STR-Match, a training-free video editing algorithm that produces\nvisually appealing and spatiotemporally coherent videos through latent\noptimization guided by our novel STR score. The score captures spatiotemporal\npixel relevance across adjacent frames by leveraging 2D spatial attention and\n1D temporal modules in text-to-video (T2V) diffusion models, without the\noverhead of computationally expensive 3D attention mechanisms. Integrated into\na latent optimization framework with a latent mask, STR-Match generates\ntemporally consistent and visually faithful videos, maintaining strong\nperformance even under significant domain transformations while preserving key\nvisual attributes of the source. Extensive experiments demonstrate that\nSTR-Match consistently outperforms existing methods in both visual quality and\nspatiotemporal consistency.", "AI": {"tldr": "STR-Match是一种无需训练的视频编辑算法，通过新型STR分数优化潜在空间，解决了现有方法在时间不一致性和运动失真等问题上的不足。", "motivation": "现有文本引导视频编辑方法存在时间不一致、运动失真和领域转换受限的问题，主要原因是时空像素相关性建模不足。", "method": "提出STR-Match算法，利用2D空间注意力和1D时间模块计算STR分数，指导潜在优化，避免计算昂贵的3D注意力机制。", "result": "实验表明，STR-Match在视觉质量和时空一致性上均优于现有方法，尤其在显著领域转换时仍保持性能。", "conclusion": "STR-Match通过有效建模时空相关性，生成高质量且一致的视频编辑结果。"}}
{"id": "2506.22460", "pdf": "https://arxiv.org/pdf/2506.22460", "abs": "https://arxiv.org/abs/2506.22460", "authors": ["Ibne Farabi Shihab"], "title": "Heart rate and respiratory rate prediction from noisy real-world smartphone based on Deep Learning methods", "categories": ["eess.SP", "cs.AI"], "comment": null, "summary": "Using mobile phone video of the fingertip as a data source for estimating\nvital signs such as heart rate (HR) and respiratory rate (RR) during daily life\nhas long been suggested. While existing literature indicates that these\nestimates are accurate to within several beats or breaths per minute, the data\nused to draw these conclusions are typically collected in laboratory\nenvironments under careful experimental control, and yet the results are\nassumed to generalize to daily life. In an effort to test it, a team of\nresearchers collected a large dataset of mobile phone video recordings made\nduring daily life and annotated with ground truth HR and RR labels from N=111\nparticipants. They found that traditional algorithm performance on the\nfingerprint videos is worse than previously reported (7 times and 13 times\nworse for RR and HR, respectively). Fortunately, recent advancements in deep\nlearning, especially in convolutional neural networks (CNNs), offer a promising\nsolution to improve this performance. This study proposes a new method for\nestimating HR and RR using a novel 3D deep CNN, demonstrating a reduced error\nin estimated HR by 68% and RR by 75%. These promising results suggest that\nregressor-based deep learning approaches should be used in estimating HR and\nRR.", "AI": {"tldr": "研究通过手机指尖视频估计心率和呼吸率，发现传统算法在日常生活数据中表现较差，但提出了一种新的3D深度CNN方法，显著降低了误差。", "motivation": "验证传统算法在日常生活数据中的表现，并探索深度学习方法提升心率和呼吸率估计的准确性。", "method": "使用3D深度卷积神经网络（CNN）对手机指尖视频进行分析，估计心率和呼吸率。", "result": "新方法将心率估计误差降低68%，呼吸率估计误差降低75%。", "conclusion": "基于回归器的深度学习方法在估计心率和呼吸率方面具有显著优势。"}}
{"id": "2506.23423", "pdf": "https://arxiv.org/pdf/2506.23423", "abs": "https://arxiv.org/abs/2506.23423", "authors": ["Felipe Nuti", "Tim Franzmeyer", "João Henriques"], "title": "TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs", "categories": ["cs.CL", "cs.AI"], "comment": "ICML 2025", "summary": "Past work has studied the effects of fine-tuning on large language models'\n(LLMs) overall performance on certain tasks. However, a quantitative and\nsystematic method for analyzing its effect on individual outputs is still\nlacking. Here, we propose a new method for measuring the contribution that\nfine-tuning makes to individual LLM responses, assuming access to the original\npre-trained model. Our method tracks the model's intermediate hidden states,\nproviding a more fine-grained insight into the effects of fine-tuning than a\nsimple comparison of final outputs from pre-trained and fine-tuned models. We\nintroduce and theoretically analyze an exact decomposition of any fine-tuned\nLLM into a pre-training component and a fine-tuning component. Empirically, we\nfind that model behavior and performance can be steered by up- or down-scaling\nthe fine-tuning component during the forward pass. Motivated by this finding\nand our theoretical analysis, we define the Tuning Contribution (TuCo) as the\nratio of the magnitudes of the fine-tuning component to the pre-training\ncomponent. We observe that three prominent adversarial attacks on LLMs\ncircumvent safety measures in a way that reduces TuCo, and that TuCo is\nconsistently lower on prompts where these attacks succeed compared to those\nwhere they do not. This suggests that attenuating the effect of fine-tuning on\nmodel outputs plays a role in the success of such attacks. In summary, TuCo\nenables the quantitative study of how fine-tuning influences model behavior and\nsafety, and vice versa.", "AI": {"tldr": "提出了一种新方法TuCo，用于量化分析微调对LLM输出的贡献，并通过理论和实验验证其有效性。", "motivation": "现有研究缺乏系统方法分析微调对LLM个体输出的影响，需要更精细的量化工具。", "method": "通过跟踪模型中间隐藏状态，将微调模型分解为预训练和微调两部分，并定义TuCo（微调贡献比）。", "result": "发现TuCo与模型行为和安全相关，攻击成功时TuCo较低。", "conclusion": "TuCo为研究微调对模型行为和安全的影响提供了量化工具。"}}
{"id": "2506.22880", "pdf": "https://arxiv.org/pdf/2506.22880", "abs": "https://arxiv.org/abs/2506.22880", "authors": ["Dang Jisheng", "Wu Xudong", "Wang Bimei", "Lv Ning", "Chen Jiayu", "Jingwen Zhao", "Yichu liu", "Jizhao Liu", "Juncheng Li", "Teng Wang"], "title": "Decoupled Seg Tokens Make Stronger Reasoning Video Segmenter and Grounder", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Existing video segmenter and grounder approaches, exemplified by Sa2VA,\ndirectly fuse features within segmentation models. This often results in an\nundesirable entanglement of dynamic visual information and static semantics,\nthereby degrading segmentation accuracy. To systematically mitigate this issue,\nwe propose DeSa2VA, a decoupling-enhanced prompting scheme integrating text\npre-training and a linear decoupling module to address the information\nprocessing limitations inherent in SAM-2. Specifically, first, we devise a\npre-training paradigm that converts textual ground-truth labels into\npoint-level prompts while generating corresponding text masks. These masks are\nrefined through a hybrid loss function to strengthen the model's semantic\ngrounding capabilities. Next, we employ linear projection to disentangle hidden\nstates that generated by a large language model into distinct textual and\nvisual feature subspaces. Finally, a dynamic mask fusion strategy\nsynergistically combines these decoupled features through triple supervision\nfrom predicted text/visual masks and ground-truth annotations. Extensive\nexperiments demonstrate state-of-the-art performance across diverse tasks,\nincluding image segmentation, image question answering, video segmentation, and\nvideo question answering. Our codes are available at\nhttps://github.com/longmalongma/DeSa2VA.", "AI": {"tldr": "DeSa2VA提出了一种解耦增强的提示方案，通过文本预训练和线性解耦模块，解决了现有视频分割和接地方法中动态视觉信息与静态语义纠缠的问题。", "motivation": "现有方法（如Sa2VA）直接将特征融合到分割模型中，导致动态视觉信息与静态语义纠缠，降低了分割准确性。", "method": "DeSa2VA采用文本预训练生成点级提示和文本掩码，并通过线性投影解耦隐藏状态，最后通过动态掩码融合策略结合解耦特征。", "result": "实验表明，DeSa2VA在图像分割、图像问答、视频分割和视频问答等任务中达到了最先进的性能。", "conclusion": "DeSa2VA通过解耦和动态融合，显著提升了分割和接地任务的性能。"}}
{"id": "2506.22461", "pdf": "https://arxiv.org/pdf/2506.22461", "abs": "https://arxiv.org/abs/2506.22461", "authors": ["Chuan Li", "Ruoxuan Yang"], "title": "Machine Learning for Proactive Groundwater Management: Early Warning and Resource Allocation", "categories": ["eess.SP", "cs.AI"], "comment": null, "summary": "Groundwater supports ecosystems, agriculture, and drinking water supplies\nworldwide, yet effective monitoring remains challenging due to sparse data,\ncomputational constraints, and delayed outputs from traditional approaches. We\ndevelop a machine learning pipeline that predicts groundwater level categories\nusing climate data, hydro-meteorological records, and physiographic attributes\nprocessed through AutoGluon's automated ensemble framework. Our approach\nintegrates geospatial preprocessing, domain-driven feature engineering, and\nautomated model selection to overcome conventional monitoring limitations.\nApplied to a large-scale French dataset (n $>$ 3,440,000 observations from\n1,500+ wells), the model achieves weighted F\\_1 scores of 0.927 on validation\ndata and 0.67 on temporally distinct test data. Scenario-based evaluations\ndemonstrate practical utility for early warning systems and water allocation\ndecisions under changing climate conditions. The open-source implementation\nprovides a scalable framework for integrating machine learning into national\ngroundwater monitoring networks, enabling more responsive and data-driven water\nmanagement strategies.", "AI": {"tldr": "该论文提出了一种基于机器学习的自动化管道，用于预测地下水位类别，结合气候数据、水文气象记录和地形属性，通过AutoGluon框架实现高效模型选择。", "motivation": "传统地下水监测方法存在数据稀疏、计算限制和输出延迟等问题，亟需更高效的解决方案。", "method": "采用地理空间预处理、领域驱动的特征工程和自动化模型选择，利用AutoGluon框架构建集成模型。", "result": "在法国大规模数据集上验证，加权F1分数为0.927（验证数据）和0.67（时间独立测试数据），展示了早期预警和水资源分配的实用性。", "conclusion": "该开源框架为国家级地下水监测网络提供了可扩展的机器学习解决方案，支持更快速、数据驱动的水资源管理。"}}
{"id": "2506.23431", "pdf": "https://arxiv.org/pdf/2506.23431", "abs": "https://arxiv.org/abs/2506.23431", "authors": ["Zixian Huang", "Chenxu Niu", "Yu Gu", "Gengyang Xiao", "Xinwei Huang", "Gong Cheng"], "title": "Pipelined Decoder for Efficient Context-Aware Text Generation", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As the basis of generative AI, an autoregressive model requires the\ngeneration of a new token depending on all the previously generated tokens,\nwhich brings high quality but also restricts the model to generate tokens one\nby one, forming a bottleneck limiting the generation speed. In this paper, we\npropose a new decoder architecture that efficiently generates text in parallel\nfor context-aware generation tasks. Our proposed pipelined decoder initiates\nthe generation of multiple subsequences simultaneously, and, at each time-step,\nit generates a new token for each subsequence to realize parallelism.\nExperiments on multiple text generation tasks, including question answering,\ntext summarization, and keyphrase generation, show that our pipelined decoder\nsignificantly improves the generation speed without a significant loss of\ngeneration quality or additional memory consumption.", "AI": {"tldr": "提出了一种新的并行解码器架构，显著提高生成速度，同时保持生成质量和内存效率。", "motivation": "自回归模型虽然生成质量高，但逐词生成限制了速度，成为瓶颈。", "method": "采用流水线解码器，同时生成多个子序列，实现并行化。", "result": "在问答、文本摘要和关键词生成等任务中，生成速度显著提升，且质量和内存消耗无明显损失。", "conclusion": "流水线解码器有效解决了自回归模型的生成速度瓶颈问题。"}}
{"id": "2506.22881", "pdf": "https://arxiv.org/pdf/2506.22881", "abs": "https://arxiv.org/abs/2506.22881", "authors": ["Fumiya Uchiyama", "Rintaro Yanagi", "Shohei Taniguchi", "Shota Takashiro", "Masahiro Suzuki", "Hirokatsu Kataoka", "Yusuke Iwasawa", "Yutaka Matsuo"], "title": "How Semantically Informative is an Image?: Measuring the Covariance-Weighted Norm of Contrastive Learning Embeddings", "categories": ["cs.CV"], "comment": null, "summary": "Contrastive learning has the capacity to model multimodal probability\ndistributions by embedding and aligning visual representations with semantics\nfrom captions. This approach enables the estimation of relational semantic\nsimilarity; however, it remains unclear whether it can also represent absolute\nsemantic informativeness. In this work, we introduce a semantic informativeness\nmetric for an image calculated from text samples via a contrastive learning\nmodel; similarly, the informativeness of a text is calculated from image\nsamples. We propose a redefinition of the concept of Information Gain, a\nconcept previously explored in natural language processing, extending its\napplication to the domains of vision and language. Our metric quantifies how\nconditioning on an image distorts the distribution of associated texts, and\nvice versa for text conditioning on image distributions. In OpenCLIP's\nempirical results, we observe that images with the lowest Information Gain\nscores often correspond to placeholder icons such as \"image not found.\"\nFurthermore, we propose to measure a norm-based metric of the embedding to\nestimate the Information Gain, following the theoretical results for Skip-Gram\nwith Negative Sampling (SGNS) word embedding. Information Gain can be measured\nusing either CLIP or SigLIP, and the results demonstrate a strong correlation\nwith a coefficient of determination ranging from 0.98 to 1.00. After obtaining\nthe mean and the covariance of the sample embedding, the computational cost of\nthis method is independent of the sample size, and it is compatible with\npublicly available, open-weight models.", "AI": {"tldr": "提出了一种基于对比学习的语义信息度量方法，用于量化图像和文本的绝对语义信息量，并重新定义了信息增益的概念。", "motivation": "探索对比学习是否能表示绝对语义信息量，而不仅仅是关系语义相似性。", "method": "通过对比学习模型计算图像和文本的信息增益，并基于嵌入的范数度量信息增益。", "result": "信息增益与嵌入范数强相关（R²=0.98-1.00），低信息增益图像通常为占位图标。", "conclusion": "该方法计算成本低，适用于公开模型，为视觉和语言领域的信息增益提供了新视角。"}}
{"id": "2506.22462", "pdf": "https://arxiv.org/pdf/2506.22462", "abs": "https://arxiv.org/abs/2506.22462", "authors": ["Abdallah Lakhdari", "Jiajie Li", "Amani Abusafia", "Athman Bouguettaya"], "title": "Privacy-aware IoT Fall Detection Services For Aging in Place", "categories": ["eess.SP", "cs.AI", "cs.CY", "cs.HC"], "comment": "11 pages, 12 figures, This paper is accepted in the 2025 IEEE\n  International Conference on Web Services (ICWS 2025)", "summary": "Fall detection is critical to support the growing elderly population,\nprojected to reach 2.1 billion by 2050. However, existing methods often face\ndata scarcity challenges or compromise privacy. We propose a novel IoT-based\nFall Detection as a Service (FDaaS) framework to assist the elderly in living\nindependently and safely by accurately detecting falls. We design a\nservice-oriented architecture that leverages Ultra-wideband (UWB) radar sensors\nas an IoT health-sensing service, ensuring privacy and minimal intrusion. We\naddress the challenges of data scarcity by utilizing a Fall Detection\nGenerative Pre-trained Transformer (FD-GPT) that uses augmentation techniques.\nWe developed a protocol to collect a comprehensive dataset of the elderly daily\nactivities and fall events. This resulted in a real dataset that carefully\nmimics the elderly's routine. We rigorously evaluate and compare various models\nusing this dataset. Experimental results show our approach achieves 90.72%\naccuracy and 89.33% precision in distinguishing between fall events and regular\nactivities of daily living.", "AI": {"tldr": "提出了一种基于物联网的跌倒检测服务框架（FDaaS），利用UWB雷达传感器和FD-GPT生成模型解决数据稀缺和隐私问题，实验结果显示高准确率和精确度。", "motivation": "随着老年人口增长，跌倒检测对独立生活至关重要，但现有方法面临数据稀缺和隐私问题。", "method": "设计服务导向架构，结合UWB雷达传感器和FD-GPT生成模型，通过数据增强技术解决数据稀缺问题。", "result": "实验结果显示90.72%的准确率和89.33%的精确度，能有效区分跌倒和日常活动。", "conclusion": "FDaaS框架在保障隐私的同时，显著提升了跌倒检测的性能。"}}
{"id": "2506.23463", "pdf": "https://arxiv.org/pdf/2506.23463", "abs": "https://arxiv.org/abs/2506.23463", "authors": ["Jang Won June"], "title": "What to Keep and What to Drop: Adaptive Table Filtering Framework", "categories": ["cs.CL", "I.2.7"], "comment": "26 pages, 9 figures", "summary": "Large language models (LLMs) for table-based reasoning often struggle with\nlarge tables due to input length limits. We propose ATF (Adaptive Table\nFiltering Framework), a modular and question-aware filtering pipeline that\nprunes uninformative columns and rows using LLM-generated column descriptions,\nclustering, and sparse-dense alignment scores. ATF integrates seamlessly with\nexisting models (e.g., TAPAS, TAPEX) without retraining. Experiments show that\nATF reduces table cells by ~70\\%, boosting performance on out-of-domain TableQA\ntasks while causing slight performance drops on Table Fact Verification, where\nfull-table context is more critical. These results highlight ATF's ability to\nadaptively balance informativeness and minimalism across tasks.", "AI": {"tldr": "ATF框架通过自适应过滤大表格的无用列和行，提升表格推理性能，减少70%的单元格，适用于TableQA任务。", "motivation": "解决大语言模型在处理大表格时因输入长度限制而表现不佳的问题。", "method": "提出ATF框架，利用LLM生成的列描述、聚类和稀疏-密集对齐分数进行模块化、问题感知的过滤。", "result": "ATF减少70%的表格单元格，在TableQA任务中提升性能，但在Table Fact Verification中略有下降。", "conclusion": "ATF能自适应平衡信息量和简洁性，适用于不同任务。"}}
{"id": "2506.22890", "pdf": "https://arxiv.org/pdf/2506.22890", "abs": "https://arxiv.org/abs/2506.22890", "authors": ["Senkang Hu", "Yihang Tao", "Guowen Xu", "Xinyuan Qian", "Yiqin Deng", "Xianhao Chen", "Sam Tak Wu Kwong", "Yuguang Fang"], "title": "CP-Guard: A Unified, Probability-Agnostic, and Adaptive Framework for Malicious Agent Detection and Defense in Multi-Agent Embodied Perception Systems", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "Collaborative Perception (CP) has been shown to be a promising technique for\nmulti-agent autonomous driving and multi-agent robotic systems, where multiple\nagents share their perception information to enhance the overall perception\nperformance and expand the perception range. However, in CP, an ego agent needs\nto receive messages from its collaborators, which makes it vulnerable to\nattacks from malicious agents. To address this critical issue, we propose a\nunified, probability-agnostic, and adaptive framework, namely, CP-Guard, which\nis a tailored defense mechanism for CP deployed by each agent to accurately\ndetect and eliminate malicious agents in its collaboration network. Our key\nidea is to enable CP to reach a consensus rather than a conflict against an ego\nagent's perception results. Based on this idea, we first develop a\nprobability-agnostic sample consensus (PASAC) method to effectively sample a\nsubset of the collaborators and verify the consensus without prior\nprobabilities of malicious agents. Furthermore, we define collaborative\nconsistency loss (CCLoss) for object detection task and bird's eye view (BEV)\nsegmentation task to capture the discrepancy between an ego agent and its\ncollaborators, which is used as a verification criterion for consensus. In\naddition, we propose online adaptive threshold via dual sliding windows to\ndynamically adjust the threshold for consensus verification and ensure the\nreliability of the systems in dynamic environments. Finally, we conduct\nextensive experiments and demonstrate the effectiveness of our framework. Code\nwill be released at https://github.com/CP-Security/CP-Guard", "AI": {"tldr": "CP-Guard是一个针对协作感知（CP）的统一防御框架，通过概率无关的样本共识和动态阈值调整，有效检测并消除恶意代理。", "motivation": "协作感知在多代理系统中提升感知性能，但易受恶意代理攻击，需开发防御机制。", "method": "提出PASAC方法进行样本共识验证，定义CCLoss衡量差异，并动态调整阈值。", "result": "实验证明CP-Guard能有效检测恶意代理，提升系统可靠性。", "conclusion": "CP-Guard为协作感知提供了可靠的防御机制，适用于动态环境。"}}
{"id": "2506.22468", "pdf": "https://arxiv.org/pdf/2506.22468", "abs": "https://arxiv.org/abs/2506.22468", "authors": ["Konstantinos Koutras", "Agorakis Bompotas", "Constantinos Halkiopoulos", "Athanasios Kalogeras", "Christos Alexakos"], "title": "Dimensionality Reduction on IoT Monitoring Data of Smart Building for Energy Consumption Forecasting", "categories": ["eess.SP", "cs.AI"], "comment": "Version of submitted paper on 2023 IEEE International Smart Cities\n  Conference (ISC2), 1-6, 2023", "summary": "The Internet of Things (IoT) plays a major role today in smart building\ninfrastructures, from simple smart-home applications, to more sophisticated\nindustrial type installations. The vast amounts of data generated from relevant\nsystems can be processed in different ways revealing important information.\nThis is especially true in the era of edge computing, when advanced data\nanalysis and decision-making is gradually moving to the edge of the network\nwhere devices are generally characterised by low computing resources. In this\ncontext, one of the emerging main challenges is related to maintaining data\nanalysis accuracy even with less data that can be efficiently handled by low\nresource devices. The present work focuses on correlation analysis of data\nretrieved from a pilot IoT network installation monitoring a small smart office\nby means of environmental and energy consumption sensors. The research\nmotivation was to find statistical correlation between the monitoring variables\nthat will allow the use of machine learning (ML) prediction algorithms for\nenergy consumption reducing input parameters. For this to happen, a series of\nhypothesis tests for the correlation of three different environmental variables\nwith the energy consumption were carried out. A total of ninety tests were\nperformed, thirty for each pair of variables. In these tests, p-values showed\nthe existence of strong or semi-strong correlation with two environmental\nvariables, and of a weak correlation with a third one. Using the proposed\nmethodology, we manage without examining the entire data set to exclude weak\ncorrelated variables while keeping the same score of accuracy.", "AI": {"tldr": "论文研究了物联网（IoT）在智能建筑中的应用，通过相关性分析减少机器学习预测算法的输入参数，同时保持准确性。", "motivation": "探索如何在资源有限的边缘设备上通过相关性分析减少数据输入，同时保持数据分析的准确性。", "method": "通过假设检验分析三个环境变量与能耗的相关性，共进行90次测试。", "result": "发现两个环境变量与能耗有强或半强相关性，第三个变量相关性较弱。", "conclusion": "提出的方法可以在不检查全部数据的情况下排除弱相关变量，同时保持准确性。"}}
{"id": "2506.23485", "pdf": "https://arxiv.org/pdf/2506.23485", "abs": "https://arxiv.org/abs/2506.23485", "authors": ["Haocheng Yu", "Yaxiong Wu", "Hao Wang", "Wei Guo", "Yong Liu", "Yawen Li", "Yuyang Ye", "Junping Du", "Enhong Chen"], "title": "Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Interactive recommendation is a typical information-seeking task that allows\nusers to interactively express their needs through natural language and obtain\npersonalized recommendations. Large language model-powered (LLM-powered) agents\nhave become a new paradigm in interactive recommendations, effectively\ncapturing users' real-time needs and enhancing personalized experiences.\nHowever, due to limited planning and generalization capabilities, existing\nformulations of LLM-powered interactive recommender agents struggle to\neffectively address diverse and complex user intents, such as intuitive,\nunrefined, or occasionally ambiguous requests. To tackle this challenge, we\npropose a novel thought-augmented interactive recommender agent system (TAIRA)\nthat addresses complex user intents through distilled thought patterns.\nSpecifically, TAIRA is designed as an LLM-powered multi-agent system featuring\na manager agent that orchestrates recommendation tasks by decomposing user\nneeds and planning subtasks, with its planning capacity strengthened through\nThought Pattern Distillation (TPD), a thought-augmentation method that extracts\nhigh-level thoughts from the agent's and human experts' experiences. Moreover,\nwe designed a set of user simulation schemes to generate personalized queries\nof different difficulties and evaluate the recommendations based on specific\ndatasets. Through comprehensive experiments conducted across multiple datasets,\nTAIRA exhibits significantly enhanced performance compared to existing methods.\nNotably, TAIRA shows a greater advantage on more challenging tasks while\ngeneralizing effectively on novel tasks, further validating its superiority in\nmanaging complex user intents within interactive recommendation systems. The\ncode is publicly available at:https://github.com/Alcein/TAIRA.", "AI": {"tldr": "论文提出了一种基于大语言模型的多智能体交互推荐系统TAIRA，通过思想模式蒸馏增强规划能力，有效处理复杂用户意图。", "motivation": "现有基于大语言模型的交互推荐系统因规划和泛化能力有限，难以应对多样且复杂的用户意图。", "method": "提出TAIRA系统，采用多智能体架构，通过思想模式蒸馏（TPD）增强规划能力，并设计用户模拟方案进行评估。", "result": "TAIRA在多个数据集上表现显著优于现有方法，尤其在复杂任务中优势明显。", "conclusion": "TAIRA通过思想模式蒸馏和多智能体协作，显著提升了交互推荐系统处理复杂用户意图的能力。"}}
{"id": "2506.22899", "pdf": "https://arxiv.org/pdf/2506.22899", "abs": "https://arxiv.org/abs/2506.22899", "authors": ["Ehsan Pajouheshgar", "Yitao Xu", "Ali Abbasi", "Alexander Mordvintsev", "Wenzel Jakob", "Sabine Süsstrunk"], "title": "Neural Cellular Automata: From Cells to Pixels", "categories": ["cs.CV", "cs.GR", "cs.LG", "cs.MA", "eess.IV"], "comment": "6 pages, 5 figures, first draft", "summary": "Neural Cellular Automata (NCAs) are bio-inspired systems in which identical\ncells self-organize to form complex and coherent patterns by repeatedly\napplying simple local rules. NCAs display striking emergent behaviors including\nself-regeneration, generalization and robustness to unseen situations, and\nspontaneous motion. Despite their success in texture synthesis and\nmorphogenesis, NCAs remain largely confined to low-resolution grids. This\nlimitation stems from (1) training time and memory requirements that grow\nquadratically with grid size, (2) the strictly local propagation of information\nwhich impedes long-range cell communication, and (3) the heavy compute demands\nof real-time inference at high resolution. In this work, we overcome this\nlimitation by pairing NCA with a tiny, shared implicit decoder, inspired by\nrecent advances in implicit neural representations. Following NCA evolution on\na coarse grid, a lightweight decoder renders output images at arbitrary\nresolution. We also propose novel loss functions for both morphogenesis and\ntexture synthesis tasks, specifically tailored for high-resolution output with\nminimal memory and computation overhead. Combining our proposed architecture\nand loss functions brings substantial improvement in quality, efficiency, and\nperformance. NCAs equipped with our implicit decoder can generate full-HD\noutputs in real time while preserving their self-organizing, emergent\nproperties. Moreover, because each MLP processes cell states independently,\ninference remains highly parallelizable and efficient. We demonstrate the\napplicability of our approach across multiple NCA variants (on 2D, 3D grids,\nand 3D meshes) and multiple tasks, including texture generation and\nmorphogenesis (growing patterns from a seed), showing that with our proposed\nframework, NCAs seamlessly scale to high-resolution outputs with minimal\ncomputational overhead.", "AI": {"tldr": "该论文提出了一种结合隐式解码器的神经细胞自动机（NCA）方法，解决了高分辨率网格下的训练和推理问题，实现了实时生成高清输出的能力。", "motivation": "NCA在低分辨率网格上表现良好，但在高分辨率下存在训练时间长、信息传播受限和计算需求高的问题。", "method": "通过引入共享隐式解码器，将NCA在粗网格上的演化结果渲染为任意分辨率图像，并设计了针对高分辨率输出的新型损失函数。", "result": "该方法显著提升了NCA在高分辨率下的质量和效率，实现了实时生成高清输出的能力，并保持了自组织和涌现特性。", "conclusion": "结合隐式解码器和新型损失函数的NCA框架能够高效扩展至高分辨率输出，适用于多种任务和网格类型。"}}
{"id": "2506.22477", "pdf": "https://arxiv.org/pdf/2506.22477", "abs": "https://arxiv.org/abs/2506.22477", "authors": ["Huiwen Han"], "title": "Innovative Research on IoT Architecture and Robotic Operating Platforms: Applications of Large Language Models and Generative AI", "categories": ["cs.NI", "cs.AI", "cs.ET", "cs.RO"], "comment": "Published in: 2024 6th International Conference on Robotics,\n  Intelligent Control and Artificial Intelligence (RICAI), IEEE Xplore, DOI:\n  10.1109/RICAI64321.2024.10911316. \\c{opyright} 2024 IEEE", "summary": "This paper introduces an innovative design for robotic operating platforms,\nunderpinned by a transformative Internet of Things (IoT) architecture,\nseamlessly integrating cutting-edge technologies such as large language models\n(LLMs), generative AI, edge computing, and 5G networks. The proposed platform\naims to elevate the intelligence and autonomy of IoT systems and robotics,\nenabling them to make real-time decisions and adapt dynamically to changing\nenvironments. Through a series of compelling case studies across industries\nincluding smart manufacturing, healthcare, and service sectors, this paper\ndemonstrates the substantial potential of IoT-enabled robotics to optimize\noperational workflows, enhance productivity, and deliver innovative, scalable\nsolutions. By emphasizing the roles of LLMs and generative AI, the research\nhighlights how these technologies drive the evolution of intelligent robotics\nand IoT, shaping the future of industry-specific advancements. The findings not\nonly showcase the transformative power of these technologies but also offer a\nforward-looking perspective on their broader societal and industrial\nimplications, positioning them as catalysts for next-generation automation and\ntechnological convergence.", "AI": {"tldr": "论文提出了一种基于物联网架构的机器人操作平台设计，整合了LLMs、生成式AI、边缘计算和5G网络，旨在提升机器人和物联网系统的智能与自主性。", "motivation": "通过整合先进技术，提升物联网和机器人系统的实时决策能力和动态适应性，优化各行业的工作流程和生产力。", "method": "采用物联网架构，结合LLMs、生成式AI、边缘计算和5G网络，并通过多行业案例研究验证。", "result": "展示了物联网机器人技术在智能制造、医疗和服务等领域的潜力，优化了工作流程并提供了可扩展的解决方案。", "conclusion": "LLMs和生成式AI推动了智能机器人和物联网的演进，为下一代自动化和技术融合提供了催化剂。"}}
{"id": "2506.23508", "pdf": "https://arxiv.org/pdf/2506.23508", "abs": "https://arxiv.org/abs/2506.23508", "authors": ["Zhihao Zhang", "Qiaole Dong", "Qi Zhang", "Jun Zhao", "Enyu Zhou", "Zhiheng Xi", "Senjie Jin", "Xiaoran Fan", "Yuhao Zhou", "Yanwei Fu", "Tao Ji", "Tao Gui", "Xuanjing Huang"], "title": "Reinforcement Fine-Tuning Enables MLLMs Learning Novel Tasks Stably", "categories": ["cs.CL", "cs.AI"], "comment": "18 pages (Preprint. Work in progress)", "summary": "Post-training algorithms such as Supervised Fine-Tuning (SFT) and\nReinforcement Fine-Tuning (RFT) are widely used to adapt multimodal large\nlanguage models to downstream tasks. While effective at task adaptation, their\nimpact on prior knowledge remains unclear. In this paper, we introduce jigsaw\npuzzles as a novel task absent from existing pretraining corpora and\nsystematically study the behavior of SFT and RFT on an open-source multimodal\nmodel, Qwen2.5-VL. Our experiments reveal a sharp trade-off: SFT enables rapid\ntask acquisition but leads to catastrophic forgetting, whereas RFT learns more\nslowly on novel tasks but maintains prior knowledge. We analyze this phenomenon\nthrough the lens of learning dynamics, showing that RFT reinforces correct\nsamples that are naturally aligned with the base model's probability landscape,\nmitigating interference with prior knowledge. Moreover, supervised training on\ncorrect RFT-simulated rollouts allows SFT to preserve knowledge while rapidly\nlearning new tasks. These findings suggest that data distribution, rather than\nalgorithmic differences, plays a central role in forgetting, and highlight\nRFT's potential for stable continual learning in multimodal large language\nmodels.", "AI": {"tldr": "研究探讨了SFT和RFT在多模态大语言模型中的任务适应性和知识保留效果，发现SFT快速适应任务但导致知识遗忘，而RFT学习较慢但保留知识。", "motivation": "探究SFT和RFT在多模态大语言模型中对任务适应性和知识保留的影响。", "method": "通过引入拼图任务，系统研究SFT和RFT在开源多模态模型Qwen2.5-VL上的表现。", "result": "SFT快速学习新任务但导致知识遗忘，RFT学习较慢但保留知识，数据分布是遗忘的关键因素。", "conclusion": "RFT在多模态大语言模型中具有稳定持续学习的潜力，数据分布对知识保留起关键作用。"}}
{"id": "2506.22900", "pdf": "https://arxiv.org/pdf/2506.22900", "abs": "https://arxiv.org/abs/2506.22900", "authors": ["Mai A. Shaaban", "Tausifa Jan Saleem", "Vijay Ram Papineni", "Mohammad Yaqub"], "title": "MOTOR: Multimodal Optimal Transport via Grounded Retrieval in Medical Visual Question Answering", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Medical visual question answering (MedVQA) plays a vital role in clinical\ndecision-making by providing contextually rich answers to image-based queries.\nAlthough vision-language models (VLMs) are widely used for this task, they\noften generate factually incorrect answers. Retrieval-augmented generation\naddresses this challenge by providing information from external sources, but\nrisks retrieving irrelevant context, which can degrade the reasoning\ncapabilities of VLMs. Re-ranking retrievals, as introduced in existing\napproaches, enhances retrieval relevance by focusing on query-text alignment.\nHowever, these approaches neglect the visual or multimodal context, which is\nparticularly crucial for medical diagnosis. We propose MOTOR, a novel\nmultimodal retrieval and re-ranking approach that leverages grounded captions\nand optimal transport. It captures the underlying relationships between the\nquery and the retrieved context based on textual and visual information.\nConsequently, our approach identifies more clinically relevant contexts to\naugment the VLM input. Empirical analysis and human expert evaluation\ndemonstrate that MOTOR achieves higher accuracy on MedVQA datasets,\noutperforming state-of-the-art methods by an average of 6.45%. Code is\navailable at https://github.com/BioMedIA-MBZUAI/MOTOR.", "AI": {"tldr": "MOTOR是一种新型多模态检索和重排序方法，通过结合文本和视觉信息提升医学视觉问答（MedVQA）的准确性。", "motivation": "现有方法在医学视觉问答中常生成错误答案或忽略多模态上下文，MOTOR旨在解决这一问题。", "method": "利用基于文本和视觉信息的检索重排序方法，结合最优传输技术。", "result": "在MedVQA数据集上平均准确率提升6.45%，优于现有方法。", "conclusion": "MOTOR通过多模态上下文显著提升医学视觉问答的准确性和临床相关性。"}}
{"id": "2506.22479", "pdf": "https://arxiv.org/pdf/2506.22479", "abs": "https://arxiv.org/abs/2506.22479", "authors": ["Krisanu Sarkar"], "title": "Hindsight-Guided Momentum (HGM) Optimizer: An Approach to Adaptive Learning Rate", "categories": ["math.OC", "cs.AI", "cs.LG"], "comment": null, "summary": "We introduce Hindsight-Guided Momentum (HGM), a first-order optimization\nalgorithm that adaptively scales learning rates based on the directional\nconsistency of recent updates. Traditional adaptive methods, such as Adam or\nRMSprop , adapt learning dynamics using only the magnitude of gradients, often\noverlooking important geometric cues.Geometric cues refer to directional\ninformation, such as the alignment between current gradients and past updates,\nwhich reflects the local curvature and consistency of the optimization path.\nHGM addresses this by incorporating a hindsight mechanism that evaluates the\ncosine similarity between the current gradient and accumulated momentum. This\nallows it to distinguish between coherent and conflicting gradient directions,\nincreasing the learning rate when updates align and reducing it in regions of\noscillation or noise. The result is a more responsive optimizer that\naccelerates convergence in smooth regions of the loss surface while maintaining\nstability in sharper or more erratic areas. Despite this added adaptability,\nthe method preserves the computational and memory efficiency of existing\noptimizers.By more intelligently responding to the structure of the\noptimization landscape, HGM provides a simple yet effective improvement over\nexisting approaches, particularly in non-convex settings like that of deep\nneural network training.", "AI": {"tldr": "HGM是一种基于梯度方向一致性的自适应学习率优化算法，通过评估当前梯度与累积动量的余弦相似性，智能调整学习率，提升收敛速度与稳定性。", "motivation": "传统自适应优化方法（如Adam、RMSprop）仅依赖梯度幅度调整学习率，忽略了方向性几何信息。HGM旨在通过利用梯度方向一致性改进优化过程。", "method": "HGM引入后视机制，计算当前梯度与累积动量的余弦相似性，动态调整学习率：方向一致时增加学习率，方向冲突时降低学习率。", "result": "HGM在平滑区域加速收敛，在复杂区域保持稳定性，同时保持计算和内存效率，在非凸优化（如深度学习）中表现优异。", "conclusion": "HGM通过更智能地响应优化路径结构，提供了一种简单有效的改进方法，特别适用于深度神经网络训练等非凸场景。"}}
{"id": "2506.23524", "pdf": "https://arxiv.org/pdf/2506.23524", "abs": "https://arxiv.org/abs/2506.23524", "authors": ["Phan Quoc Hung Mai", "Quang Hung Nguyen", "Phuong Giang Duong", "Hong Hanh Nguyen", "Nguyen Tuan Long"], "title": "NEU-ESC: A Comprehensive Vietnamese dataset for Educational Sentiment analysis and topic Classification toward multitask learning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In the field of education, understanding students' opinions through their\ncomments is crucial, especially in the Vietnamese language, where resources\nremain limited. Existing educational datasets often lack domain relevance and\nstudent slang. To address these gaps, we introduce NEU-ESC, a new Vietnamese\ndataset for Educational Sentiment Classification and Topic Classification,\ncurated from university forums, which offers more samples, richer class\ndiversity, longer texts, and broader vocabulary. In addition, we explore\nmultitask learning using encoder-only language models (BERT), in which we\nshowed that it achieves performance up to 83.7% and 79.8% accuracy for\nsentiment and topic classification tasks. We also benchmark our dataset and\nmodel with other datasets and models, including Large Language Models, and\ndiscuss these benchmarks. The dataset is publicly available at:\nhttps://huggingface.co/datasets/hung20gg/NEU-ESC.", "AI": {"tldr": "介绍了一个越南教育情感与主题分类数据集NEU-ESC，基于BERT的多任务学习模型在情感和主题分类任务上分别达到83.7%和79.8%的准确率。", "motivation": "现有越南教育数据集缺乏领域相关性和学生俚语，NEU-ESC填补了这一空白。", "method": "使用BERT模型进行多任务学习，并在大学论坛数据上构建数据集。", "result": "模型在情感和主题分类任务上分别达到83.7%和79.8%的准确率。", "conclusion": "NEU-ESC数据集和模型为越南教育领域的情感与主题分类提供了有效工具。"}}
{"id": "2506.22902", "pdf": "https://arxiv.org/pdf/2506.22902", "abs": "https://arxiv.org/abs/2506.22902", "authors": ["Yiling Xu", "Yujie Zhang", "Shuting Xia", "Kaifa Yang", "He Huang", "Ziyu Shan", "Wenjie Huang", "Qi Yang", "Le Yang"], "title": "Point Cloud Compression and Objective Quality Assessment: A Survey", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "The rapid growth of 3D point cloud data, driven by applications in autonomous\ndriving, robotics, and immersive environments, has led to criticals demand for\nefficient compression and quality assessment techniques. Unlike traditional 2D\nmedia, point clouds present unique challenges due to their irregular structure,\nhigh data volume, and complex attributes. This paper provides a comprehensive\nsurvey of recent advances in point cloud compression (PCC) and point cloud\nquality assessment (PCQA), emphasizing their significance for real-time and\nperceptually relevant applications. We analyze a wide range of handcrafted and\nlearning-based PCC algorithms, along with objective PCQA metrics. By\nbenchmarking representative methods on emerging datasets, we offer detailed\ncomparisons and practical insights into their strengths and limitations.\nDespite notable progress, challenges such as enhancing visual fidelity,\nreducing latency, and supporting multimodal data remain. This survey outlines\nfuture directions, including hybrid compression frameworks and advanced feature\nextraction strategies, to enable more efficient, immersive, and intelligent 3D\napplications.", "AI": {"tldr": "本文综述了3D点云压缩（PCC）和质量评估（PCQA）的最新进展，分析了手工和基于学习的算法，并指出了未来研究方向。", "motivation": "3D点云数据在自动驾驶、机器人等领域的快速增长，亟需高效的压缩和质量评估技术。", "method": "综述了多种手工和基于学习的PCC算法及PCQA指标，并在新兴数据集上进行了基准测试。", "result": "通过对比分析，揭示了现有方法的优缺点，并指出视觉保真度、延迟和多模态数据支持等挑战。", "conclusion": "未来方向包括混合压缩框架和高级特征提取策略，以实现更高效、沉浸和智能的3D应用。"}}
{"id": "2506.23527", "pdf": "https://arxiv.org/pdf/2506.23527", "abs": "https://arxiv.org/abs/2506.23527", "authors": ["Jan Kvapil", "Martin Fajcik"], "title": "On Recipe Memorization and Creativity in Large Language Models: Is Your Model a Creative Cook, a Bad Cook, or Merely a Plagiator?", "categories": ["cs.CL"], "comment": "13 pages, 5 figures", "summary": "This work-in-progress investigates the memorization, creativity, and nonsense\nfound in cooking recipes generated from Large Language Models (LLMs).\nPrecisely, we aim (i) to analyze memorization, creativity, and non-sense in\nLLMs using a small, high-quality set of human judgments and (ii) to evaluate\npotential approaches to automate such a human annotation in order to scale our\nstudy to hundreds of recipes. To achieve (i), we conduct a detailed human\nannotation on 20 preselected recipes generated by LLM (Mixtral), extracting\neach recipe's ingredients and step-by-step actions to assess which elements are\nmemorized--i.e., directly traceable to online sources possibly seen during\ntraining--and which arise from genuine creative synthesis or outright nonsense.\nWe find that Mixtral consistently reuses ingredients that can be found in\nonline documents, potentially seen during model training, suggesting strong\nreliance on memorized content. To achieve aim (ii) and scale our analysis\nbeyond small sample sizes and single LLM validation, we design an\n``LLM-as-judge'' pipeline that automates recipe generation, nonsense detection,\nparsing ingredients and recipe steps, and their annotation. For instance,\ncomparing its output against human annotations, the best ingredient extractor\nand annotator is Llama 3.1+Gemma 2 9B, achieving up to 78% accuracy on\ningredient matching. This automated framework enables large-scale\nquantification of memorization, creativity, and nonsense in generated recipes,\nproviding rigorous evidence of the models' creative capacities.", "AI": {"tldr": "研究分析了大型语言模型（LLMs）生成的食谱中的记忆、创造性和无意义内容，通过人工标注和自动化方法评估模型的依赖性和创造力。", "motivation": "探究LLMs在生成食谱时对训练数据的记忆程度、创造性合成能力以及产生的无意义内容，以评估模型的真实创造力。", "method": "通过人工标注20个LLM生成的食谱，分析记忆和创造性内容；设计自动化流程（LLM-as-judge）扩展研究规模。", "result": "发现Mixtral模型依赖记忆内容；自动化方法中，Llama 3.1+Gemma 2 9B在成分匹配上达到78%准确率。", "conclusion": "自动化框架能大规模量化LLMs的记忆、创造性和无意义内容，为模型创造力提供严谨证据。"}}
{"id": "2506.22907", "pdf": "https://arxiv.org/pdf/2506.22907", "abs": "https://arxiv.org/abs/2506.22907", "authors": ["Yunzhe Shao", "Xinyu Yi", "Lu Yin", "Shihui Guo", "Junhai Yong", "Feng Xu"], "title": "MagShield: Towards Better Robustness in Sparse Inertial Motion Capture Under Magnetic Disturbances", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "This paper proposes a novel method called MagShield, designed to address the\nissue of magnetic interference in sparse inertial motion capture (MoCap)\nsystems. Existing Inertial Measurement Unit (IMU) systems are prone to\norientation estimation errors in magnetically disturbed environments, limiting\ntheir practical application in real-world scenarios. To address this problem,\nMagShield employs a \"detect-then-correct\" strategy, first detecting magnetic\ndisturbances through multi-IMU joint analysis, and then correcting orientation\nerrors using human motion priors. MagShield can be integrated with most\nexisting sparse inertial MoCap systems, improving their performance in\nmagnetically disturbed environments. Experimental results demonstrate that\nMagShield significantly enhances the accuracy of motion capture under magnetic\ninterference and exhibits good compatibility across different sparse inertial\nMoCap systems.", "AI": {"tldr": "MagShield是一种新方法，用于解决稀疏惯性运动捕捉系统中的磁干扰问题，通过检测和校正策略提高准确性。", "motivation": "现有IMU系统在磁干扰环境下易产生方向估计误差，限制了实际应用。", "method": "MagShield采用“检测-校正”策略，通过多IMU联合分析检测磁干扰，并利用人体运动先验校正方向误差。", "result": "实验表明，MagShield显著提高了磁干扰下的运动捕捉准确性，并具有良好的兼容性。", "conclusion": "MagShield是一种有效的解决方案，可提升稀疏惯性运动捕捉系统在磁干扰环境中的性能。"}}
{"id": "2506.23601", "pdf": "https://arxiv.org/pdf/2506.23601", "abs": "https://arxiv.org/abs/2506.23601", "authors": ["Weijie Shi", "Yue Cui", "Yaguang Wu", "Jingzhi Fang", "Shibo Zhang", "Mengze Li", "Sirui Han", "Jia Zhu", "Jiajie Xu", "Xiaofang Zhou"], "title": "Semantic-guided Diverse Decoding for Large Language Model", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Diverse decoding of large language models is crucial for applications\nrequiring multiple semantically distinct responses, yet existing methods\nprimarily achieve lexical rather than semantic diversity. This limitation\nsignificantly constrains Best-of-N strategies, group-based reinforcement\nlearning, and data synthesis. While temperature sampling and diverse beam\nsearch modify token distributions or apply n-gram penalties, they fail to\nensure meaningful semantic differentiation. We introduce Semantic-guided\nDiverse Decoding (SemDiD), operating directly in embedding space that balances\nquality with diversity through three complementary mechanisms: orthogonal\ndirectional guidance, dynamic inter-group repulsion, and position-debiased\nprobability assessment. SemDiD harmonizes these competing objectives using\nadaptive gain functions and constraint optimization, ensuring both quality\nthresholds and maximal semantic differentiation. Experiments show SemDiD\nconsistently outperforms existing methods, improving Best-of-N coverage by\n1.4-5.2% across diverse tasks and accelerating RLHF training convergence by 15%\nwhile increasing accuracy by up to 2.1%.", "AI": {"tldr": "SemDiD通过嵌入空间操作提升语义多样性，优于现有方法。", "motivation": "现有方法主要实现词汇多样性而非语义多样性，限制了应用效果。", "method": "提出SemDiD，结合正交方向引导、动态组间排斥和位置去偏概率评估。", "result": "实验显示SemDiD在多样任务中表现更优，提升覆盖率和训练效率。", "conclusion": "SemDiD有效平衡质量与多样性，优于现有方法。"}}
{"id": "2506.22908", "pdf": "https://arxiv.org/pdf/2506.22908", "abs": "https://arxiv.org/abs/2506.22908", "authors": ["Yuzhu Wang", "Manni Duan", "Shu Kong"], "title": "Attention to Burstiness: Low-Rank Bilinear Prompt Tuning", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "Visual Prompt Tuning (VPT) is a parameter-efficient fune-tuning technique\nthat adapts a pre-trained vision Transformer (ViT) by learning a small set of\nparameters in the input space, known as prompts. In VPT, we uncover\n``burstiness'' in the values arising from the interaction of image patch\nembeddings, and the key and query projectors within Transformer's\nself-attention module. Furthermore, the values of patch embeddings and the key\nand query projectors exhibit Laplacian and hyper-Laplacian distribution,\nrespectively. Intuitively, these non-Gaussian distributions pose challenges for\nlearning prompts. To address this, we propose whitening these data,\nde-correlating them and equalizing their variance towards more Gaussian before\nlearning prompts. We derive the whitening matrix over random image patch\nembeddings and ViT's key and query projectors, and multiply it with the prompt\nto be learned in a bilinear manner. Surprisingly, this method significantly\naccelerates prompt tuning and boosts accuracy, e.g., $>$25 accuracy points on\nthe CUB dataset; interestingly, it learns ``bursty prompts''. Extending the\nbilinear model which is known to introduce burstiness, we present a compact,\nlow-rank version by learning two smaller matrices whose multiplication yields\nthe final prompts. We call the proposed methods Bilinear Prompt Tuning (BPT).\nExtensive experiments across multiple benchmark datasets demonstrate that BPT\nmethods not only outperform various VPT methods but also reduce parameter count\nand computation overhead.", "AI": {"tldr": "论文提出了一种名为Bilinear Prompt Tuning (BPT)的方法，通过数据白化和低秩分解优化视觉提示调优（VPT），显著提升了性能和效率。", "motivation": "VPT中图像块嵌入与Transformer自注意力模块的交互导致数据分布非高斯性，影响提示学习效果。", "method": "提出数据白化方法，并引入双线性低秩分解模型（BPT）来优化提示学习。", "result": "BPT方法在CUB数据集上提升了25个准确点，同时减少了参数和计算开销。", "conclusion": "BPT方法在性能和效率上均优于现有VPT方法，适用于多种基准数据集。"}}
{"id": "2506.22487", "pdf": "https://arxiv.org/pdf/2506.22487", "abs": "https://arxiv.org/abs/2506.22487", "authors": ["Amar Khelloufi", "Huansheng Ning", "Sahraoui Dhelim", "Jianguo Ding"], "title": "AGI Enabled Solutions For IoX Layers Bottlenecks In Cyber-Physical-Social-Thinking Space", "categories": ["cs.NI", "cs.AI"], "comment": "31 pages, 5 figures", "summary": "The integration of the Internet of Everything (IoX) and Artificial General\nIntelligence (AGI) has given rise to a transformative paradigm aimed at\naddressing critical bottlenecks across sensing, network, and application layers\nin Cyber-Physical-Social Thinking (CPST) ecosystems. In this survey, we provide\na systematic and comprehensive review of AGI-enhanced IoX research, focusing on\nthree key components: sensing-layer data management, network-layer protocol\noptimization, and application-layer decision-making frameworks. Specifically,\nthis survey explores how AGI can mitigate IoX bottlenecks challenges by\nleveraging adaptive sensor fusion, edge preprocessing, and selective attention\nmechanisms at the sensing layer, while resolving network-layer issues such as\nprotocol heterogeneity and dynamic spectrum management, neuro-symbolic\nreasoning, active inference, and causal reasoning, Furthermore, the survey\nexamines AGI-enabled frameworks for managing identity and relationship\nexplosion. Key findings suggest that AGI-driven strategies, such as adaptive\nsensor fusion, edge preprocessing, and semantic modeling, offer novel solutions\nto sensing-layer data overload, network-layer protocol heterogeneity, and\napplication-layer identity explosion. The survey underscores the importance of\ncross-layer integration, quantum-enabled communication, and ethical governance\nframeworks for future AGI-enabled IoX systems. Finally, the survey identifies\nunresolved challenges, such as computational requirements, scalability, and\nreal-world validation, calling for further research to fully realize AGI's\npotential in addressing IoX bottlenecks. we believe AGI-enhanced IoX is\nemerging as a critical research field at the intersection of interconnected\nsystems and advanced AI.", "AI": {"tldr": "本文综述了AGI增强的IoX研究，重点探讨了感知层数据管理、网络层协议优化和应用层决策框架，提出了AGI如何解决IoX的瓶颈问题。", "motivation": "研究动机在于解决CPST生态系统中感知、网络和应用层的关键瓶颈问题，探索AGI与IoX结合的潜力。", "method": "通过系统综述，分析了AGI在感知层（自适应传感器融合、边缘预处理）、网络层（协议异构性、动态频谱管理）和应用层（身份管理、关系爆炸）的解决方案。", "result": "研究发现AGI驱动的策略（如自适应传感器融合、语义建模）能有效解决数据过载、协议异构性和身份爆炸问题。", "conclusion": "结论强调了跨层集成、量子通信和伦理治理的重要性，并指出未来需解决计算需求、可扩展性和实际验证等挑战。"}}
{"id": "2506.23610", "pdf": "https://arxiv.org/pdf/2506.23610", "abs": "https://arxiv.org/abs/2506.23610", "authors": ["Manuel Pratelli", "Marinella Petrocchi"], "title": "Evaluating the Simulation of Human Personality-Driven Susceptibility to Misinformation with LLMs", "categories": ["cs.CL", "cs.CY"], "comment": "pre-print version - paper actually under submission", "summary": "Large language models (LLMs) make it possible to generate synthetic\nbehavioural data at scale, offering an ethical and low-cost alternative to\nhuman experiments. Whether such data can faithfully capture psychological\ndifferences driven by personality traits, however, remains an open question. We\nevaluate the capacity of LLM agents, conditioned on Big-Five profiles, to\nreproduce personality-based variation in susceptibility to misinformation,\nfocusing on news discernment, the ability to judge true headlines as true and\nfalse headlines as false. Leveraging published datasets in which human\nparticipants with known personality profiles rated headline accuracy, we create\nmatching LLM agents and compare their responses to the original human patterns.\nCertain trait-misinformation associations, notably those involving\nAgreeableness and Conscientiousness, are reliably replicated, whereas others\ndiverge, revealing systematic biases in how LLMs internalize and express\npersonality. The results underscore both the promise and the limits of\npersonality-aligned LLMs for behavioral simulation, and offer new insight into\nmodeling cognitive diversity in artificial agents.", "AI": {"tldr": "LLMs可生成合成行为数据，但能否准确反映人格特质驱动的心理差异尚不确定。研究发现，LLM代理在部分人格特质（如宜人性和尽责性）上能复现人类行为，但也存在系统性偏差。", "motivation": "探讨LLM生成的行为数据是否能准确模拟人格特质驱动的心理差异，特别是在新闻辨别能力上。", "method": "利用已知人格特质的人类数据，创建匹配的LLM代理，比较其与人类在新闻辨别上的反应模式。", "result": "LLM在宜人性和尽责性等特质上能复现人类行为，但其他特质存在偏差。", "conclusion": "LLM在行为模拟中具有潜力但也存在局限，为人工代理的认知多样性建模提供了新视角。"}}
{"id": "2506.22930", "pdf": "https://arxiv.org/pdf/2506.22930", "abs": "https://arxiv.org/abs/2506.22930", "authors": ["Yiwei He", "Xiangtai Li", "Zhenglin Huang", "Yi Dong", "Hao Fei", "Jiangning Zhang", "Baoyuan Wu", "Guangliang Cheng"], "title": "Towards Explainable Bilingual Multimodal Misinformation Detection and Localization", "categories": ["cs.CV"], "comment": null, "summary": "The increasing realism of multimodal content has made misinformation more\nsubtle and harder to detect, especially in news media where images are\nfrequently paired with bilingual (e.g., Chinese-English) subtitles. Such\ncontent often includes localized image edits and cross-lingual inconsistencies\nthat jointly distort meaning while remaining superficially plausible. We\nintroduce BiMi, a bilingual multimodal framework that jointly performs\nregion-level localization, cross-modal and cross-lingual consistency detection,\nand natural language explanation for misinformation analysis. To support\ngeneralization, BiMi integrates an online retrieval module that supplements\nmodel reasoning with up-to-date external context. We further release BiMiBench,\na large-scale and comprehensive benchmark constructed by systematically editing\nreal news images and subtitles, comprising 104,000 samples with realistic\nmanipulations across visual and linguistic modalities. To enhance\ninterpretability, we apply Group Relative Policy Optimization (GRPO) to improve\nexplanation quality, marking the first use of GRPO in this domain. Extensive\nexperiments demonstrate that BiMi outperforms strong baselines by up to +8.9 in\nclassification accuracy, +15.9 in localization accuracy, and +2.5 in\nexplanation BERTScore, advancing state-of-the-art performance in realistic,\nmultilingual misinformation detection. Code, models, and datasets will be\nreleased.", "AI": {"tldr": "BiMi是一个双语多模态框架，用于检测新闻媒体中的虚假信息，通过区域级定位、跨模态和跨语言一致性检测以及自然语言解释，显著提升了分类和定位准确性。", "motivation": "随着多模态内容的真实性提高，虚假信息变得更难检测，尤其是在双语新闻中，图像和字幕的局部编辑和跨语言不一致性可能误导读者。", "method": "BiMi框架结合区域级定位、跨模态和跨语言一致性检测，并引入在线检索模块增强泛化能力。使用GRPO优化解释质量。", "result": "BiMi在分类准确性上提升8.9，定位准确性提升15.9，解释BERTScore提升2.5，优于现有基线。", "conclusion": "BiMi在多语言虚假信息检测中表现优异，并发布了BiMiBench基准数据集和工具，推动了该领域的发展。"}}
{"id": "2506.23661", "pdf": "https://arxiv.org/pdf/2506.23661", "abs": "https://arxiv.org/abs/2506.23661", "authors": ["Arnisa Fazla", "Lucas Krauter", "David Guzman Piedrahita", "Andrianos Michail"], "title": "Robustness of Misinformation Classification Systems to Adversarial Examples Through BeamAttack", "categories": ["cs.CL"], "comment": "12 pages main text, 27 pages total including references and\n  appendices. 13 figures, 10 tables. Accepted for publication in the LNCS\n  proceedings of CLEF 2025 (Best-of-Labs track)", "summary": "We extend BeamAttack, an adversarial attack algorithm designed to evaluate\nthe robustness of text classification systems through word-level modifications\nguided by beam search. Our extensions include support for word deletions and\nthe option to skip substitutions, enabling the discovery of minimal\nmodifications that alter model predictions. We also integrate LIME to better\nprioritize word replacements. Evaluated across multiple datasets and victim\nmodels (BiLSTM, BERT, and adversarially trained RoBERTa) within the BODEGA\nframework, our approach achieves over a 99\\% attack success rate while\npreserving the semantic and lexical similarity of the original texts. Through\nboth quantitative and qualitative analysis, we highlight BeamAttack's\neffectiveness and its limitations. Our implementation is available at\nhttps://github.com/LucK1Y/BeamAttack", "AI": {"tldr": "扩展了BeamAttack算法，支持单词删除和跳过替换，结合LIME优化替换优先级，攻击成功率达99%以上。", "motivation": "评估文本分类系统的鲁棒性，通过最小修改改变模型预测。", "method": "扩展BeamAttack算法，支持单词删除和跳过替换，集成LIME优化替换优先级。", "result": "在多个数据集和模型上攻击成功率达99%以上，保持语义和词汇相似性。", "conclusion": "BeamAttack高效但仍有局限性，代码已开源。"}}
{"id": "2506.22939", "pdf": "https://arxiv.org/pdf/2506.22939", "abs": "https://arxiv.org/abs/2506.22939", "authors": ["Ghufran A. Omran", "Wassan Saad Abduljabbar Hayale", "Ahmad AbdulQadir AlRababah", "Israa Ibraheem Al-Barazanchi", "Ravi Sekhar", "Pritesh Shah", "Sushma Parihar", "Harshavardhan Reddy Penubadi"], "title": "Utilizing a Novel Deep Learning Method for Scene Categorization in Remote Sensing Data", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Scene categorization (SC) in remotely acquired images is an important subject\nwith broad consequences in different fields, including catastrophe control,\necological observation, architecture for cities, and more. Nevertheless, its\nseveral apps, reaching a high degree of accuracy in SC from distant observation\ndata has demonstrated to be difficult. This is because traditional conventional\ndeep learning models require large databases with high variety and high levels\nof noise to capture important visual features. To address these problems, this\ninvestigation file introduces an innovative technique referred to as the\nCuttlefish Optimized Bidirectional Recurrent Neural Network (CO- BRNN) for type\nof scenes in remote sensing data. The investigation compares the execution of\nCO-BRNN with current techniques, including Multilayer Perceptron- Convolutional\nNeural Network (MLP-CNN), Convolutional Neural Network-Long Short Term Memory\n(CNN-LSTM), and Long Short Term Memory-Conditional Random Field (LSTM-CRF),\nGraph-Based (GB), Multilabel Image Retrieval Model (MIRM-CF), Convolutional\nNeural Networks Data Augmentation (CNN-DA). The results demonstrate that\nCO-BRNN attained the maximum accuracy of 97%, followed by LSTM-CRF with 90%,\nMLP-CNN with 85%, and CNN-LSTM with 80%. The study highlights the significance\nof physical confirmation to ensure the efficiency of satellite data.", "AI": {"tldr": "论文提出了一种名为CO-BRNN的新方法，用于遥感数据的场景分类，其准确率高达97%，优于现有技术。", "motivation": "遥感图像场景分类在多个领域具有重要意义，但传统深度学习方法需要大量数据且难以处理噪声，因此需要更高效的方法。", "method": "采用Cuttlefish优化的双向循环神经网络（CO-BRNN），并与多种现有技术（如MLP-CNN、CNN-LSTM等）进行对比。", "result": "CO-BRNN表现最佳，准确率达97%，显著优于其他方法。", "conclusion": "CO-BRNN在遥感场景分类中具有高效性，同时强调了物理验证对卫星数据效率的重要性。"}}
{"id": "2506.22492", "pdf": "https://arxiv.org/pdf/2506.22492", "abs": "https://arxiv.org/abs/2506.22492", "authors": ["Rajeev Alur", "Greg Durrett", "Hadas Kress-Gazit", "Corina Păsăreanu", "René Vidal"], "title": "Report on NSF Workshop on Science of Safe AI", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Recent advances in machine learning, particularly the emergence of foundation\nmodels, are leading to new opportunities to develop technology-based solutions\nto societal problems. However, the reasoning and inner workings of today's\ncomplex AI models are not transparent to the user, and there are no safety\nguarantees regarding their predictions. Consequently, to fulfill the promise of\nAI, we must address the following scientific challenge: how to develop AI-based\nsystems that are not only accurate and performant but also safe and\ntrustworthy?\n  The criticality of safe operation is particularly evident for autonomous\nsystems for control and robotics, and was the catalyst for the Safe Learning\nEnabled Systems (SLES) program at NSF. For the broader class of AI\napplications, such as users interacting with chatbots and clinicians receiving\ntreatment recommendations, safety is, while no less important, less\nwell-defined with context-dependent interpretations. This motivated the\norganization of a day-long workshop, held at University of Pennsylvania on\nFebruary 26, 2025, to bring together investigators funded by the NSF SLES\nprogram with a broader pool of researchers studying AI safety. This report is\nthe result of the discussions in the working groups that addressed different\naspects of safety at the workshop. The report articulates a new research agenda\nfocused on developing theory, methods, and tools that will provide the\nfoundations of the next generation of AI-enabled systems.", "AI": {"tldr": "论文探讨了AI系统的安全性和可信赖性问题，提出了新的研究议程以开发下一代AI系统的基础理论、方法和工具。", "motivation": "当前复杂AI模型的透明度和安全性不足，限制了其在解决社会问题中的应用潜力。", "method": "通过组织研讨会，汇集NSF SLES项目资助的研究者和其他AI安全研究者，讨论安全问题的不同方面。", "result": "报告提出了一项新的研究议程，旨在开发下一代AI系统的理论、方法和工具。", "conclusion": "为确保AI系统的安全性和可信赖性，需要进一步研究和发展相关理论和方法。"}}
{"id": "2506.23662", "pdf": "https://arxiv.org/pdf/2506.23662", "abs": "https://arxiv.org/abs/2506.23662", "authors": ["Philip Lippmann", "Jie Yang"], "title": "Zero-Shot Contextual Embeddings via Offline Synthetic Corpus Generation", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Context-aware embedding methods boost retrieval accuracy by conditioning on\ncorpus statistics (e.g., term co-occurrence and topical patterns) extracted\nfrom neighboring documents. However, this context-aware approach requires\naccess to the target corpus or requires domain-specific finetuning, posing\npractical barriers in privacy-sensitive or resource-constrained settings. We\npresent ZEST, a zero-shot contextual adaptation framework that replaces real\ncorpus access with a one-time offline synthesis of a compact proxy. Given only\na handful exemplar documents representative of the general target domain, we\nuse a multi-step hierarchical procedure to generate a synthetic context corpus\nof several hundred documents that aims to emulate key domain-specific\ndistributions. At inference, the frozen context-aware encoder uses this proxy\ncorpus -- without any finetuning or target corpus access -- to produce\ndomain-adapted embeddings. Across the MTEB benchmark, ZEST's zero-shot\nsynthetic context adaptation using only five example documents performs within\n0.5% of models leveraging full target corpus access -- demonstrating remarkable\nefficacy without any retraining. ZEST thus provides a practical method for\ndeploying high-performance, adaptable embeddings in constrained environments.", "AI": {"tldr": "ZEST是一种零样本上下文适应框架，通过合成代理语料库实现无需目标语料库访问或微调的领域适应嵌入。", "motivation": "解决上下文感知嵌入方法需要访问目标语料库或领域特定微调的实际障碍，特别是在隐私敏感或资源受限的环境中。", "method": "使用多步分层程序生成合成上下文语料库，模拟关键领域特定分布，供冻结的上下文感知编码器使用。", "result": "在MTEB基准测试中，ZEST仅使用五个示例文档的零样本合成上下文适应性能接近完全访问目标语料库的模型（差距0.5%）。", "conclusion": "ZEST为受限环境中部署高性能、适应性强的嵌入提供了一种实用方法。"}}
{"id": "2506.22955", "pdf": "https://arxiv.org/pdf/2506.22955", "abs": "https://arxiv.org/abs/2506.22955", "authors": ["Haniyeh Nikkhah", "Jafar Tanha", "Mahdi Zarrin", "SeyedEhsan Roshan", "Amin Kazempour"], "title": "YM-WML: A new Yolo-based segmentation Model with Weighted Multi-class Loss for medical imaging", "categories": ["cs.CV"], "comment": "Accepted at The 7th International conference on Pattern Recognition\n  and Image Analysis (IPRIA 2025)", "summary": "Medical image segmentation poses significant challenges due to class\nimbalance and the complex structure of medical images. To address these\nchallenges, this study proposes YM-WML, a novel model for cardiac image\nsegmentation. The model integrates a robust backbone for effective feature\nextraction, a YOLOv11 neck for multi-scale feature aggregation, and an\nattention-based segmentation head for precise and accurate segmentation. To\naddress class imbalance, we introduce the Weighted Multi-class Exponential\n(WME) loss function. On the ACDC dataset, YM-WML achieves a Dice Similarity\nCoefficient of 91.02, outperforming state-of-the-art methods. The model\ndemonstrates stable training, accurate segmentation, and strong generalization,\nsetting a new benchmark in cardiac segmentation tasks.", "AI": {"tldr": "YM-WML模型通过结合多尺度特征提取和注意力机制，提出了一种新的心脏图像分割方法，并引入WME损失函数解决类别不平衡问题，在ACDC数据集上表现优异。", "motivation": "医学图像分割面临类别不平衡和复杂结构的挑战，需要更精确的解决方案。", "method": "YM-WML模型结合了强大的特征提取骨干、YOLOv11颈部模块和多尺度特征聚合，以及基于注意力的分割头，并使用WME损失函数处理类别不平衡。", "result": "在ACDC数据集上，YM-WML的Dice相似系数达到91.02，优于现有方法。", "conclusion": "YM-WML在心脏图像分割任务中表现出色，为相关领域设定了新的基准。"}}
{"id": "2506.22495", "pdf": "https://arxiv.org/pdf/2506.22495", "abs": "https://arxiv.org/abs/2506.22495", "authors": ["He-Yang Xu", "Hongxiang Gao", "Yuwen Li", "Xiu-Shen Wei", "Chengyu Liu"], "title": "Masked Autoencoders that Feel the Heart: Unveiling Simplicity Bias for ECG Analyses", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": null, "summary": "The diagnostic value of electrocardiogram (ECG) lies in its dynamic\ncharacteristics, ranging from rhythm fluctuations to subtle waveform\ndeformations that evolve across time and frequency domains. However, supervised\nECG models tend to overfit dominant and repetitive patterns, overlooking\nfine-grained but clinically critical cues, a phenomenon known as Simplicity\nBias (SB), where models favor easily learnable signals over subtle but\ninformative ones. In this work, we first empirically demonstrate the presence\nof SB in ECG analyses and its negative impact on diagnostic performance, while\nsimultaneously discovering that self-supervised learning (SSL) can alleviate\nit, providing a promising direction for tackling the bias. Following the SSL\nparadigm, we propose a novel method comprising two key components: 1)\nTemporal-Frequency aware Filters to capture temporal-frequency features\nreflecting the dynamic characteristics of ECG signals, and 2) building on this,\nMulti-Grained Prototype Reconstruction for coarse and fine representation\nlearning across dual domains, further mitigating SB. To advance SSL in ECG\nanalyses, we curate a large-scale multi-site ECG dataset with 1.53 million\nrecordings from over 300 clinical centers. Experiments on three downstream\ntasks across six ECG datasets demonstrate that our method effectively reduces\nSB and achieves state-of-the-art performance. Code and dataset will be released\npublicly.", "AI": {"tldr": "论文提出了一种自监督学习方法，通过时间-频率感知滤波器和多粒度原型重建，减轻ECG分析中的简单性偏差（SB），并在大规模数据集上验证了其有效性。", "motivation": "ECG诊断中，监督学习模型容易过度拟合主导模式，忽视细微但临床关键的信号（简单性偏差SB），自监督学习（SSL）可能缓解这一问题。", "method": "1）时间-频率感知滤波器捕捉ECG信号的动态特征；2）多粒度原型重建实现双域粗粒度与细粒度表示学习。", "result": "在六个ECG数据集上的三个下游任务中，方法显著减轻SB并达到最优性能。", "conclusion": "自监督学习是解决ECG分析中简单性偏差的有效方向，提出的方法具有实际应用潜力。"}}
{"id": "2506.23667", "pdf": "https://arxiv.org/pdf/2506.23667", "abs": "https://arxiv.org/abs/2506.23667", "authors": ["Junjie Zhang", "Jingyi Xi", "Zhuoyang Song", "Junyu Lu", "Yuhua Ke", "Ting Sun", "Yukun Yang", "Jiaxing Zhang", "Songxin Zhang", "Zejian Xie"], "title": "L0: Reinforcement Learning to Become General Agents", "categories": ["cs.CL"], "comment": null, "summary": "Training large language models (LLMs) to act as autonomous agents for\nmulti-turn, long-horizon tasks remains significant challenges in scalability\nand training efficiency. To address this, we introduce L-Zero (L0), a scalable,\nend-to-end training pipeline for general-purpose agents. Featuring a low-cost,\nextensible, and sandboxed concurrent agent worker pool, L0 lowers the barrier\nfor applying reinforcement learning in complex environments. We also introduce\nNB-Agent, the agent scaffold within L0, which operates in a \"code-as-action\"\nfashion via a Read-Eval-Print-Loop (REPL). We evaluate L0 on factuality\nquestion-answering benchmarks. Our experiments demonstrate that a base model\ncan develop robust problem-solving skills using solely Reinforcement Learning\nwith Verifiable Rewards (RLVR). On the Qwen2.5-7B-Instruct model, our method\nboosts accuracy on SimpleQA from 30 % to 80 % and on HotpotQA from 22 % to 41\n%. We have open-sourced the entire L0 system, including our L0 series models,\nthe NB-Agent, a complete training pipeline, and the corresponding training\nrecipes on (https://github.com/cmriat/l0).", "AI": {"tldr": "L-Zero (L0) 是一个可扩展的端到端训练管道，用于通用代理，通过低成本、可扩展的并发代理工作池和代码即动作的 NB-Agent 提升训练效率，显著提高了问答任务的准确性。", "motivation": "解决大规模语言模型在多轮、长时任务中作为自主代理的扩展性和训练效率问题。", "method": "引入 L0 训练管道和 NB-Agent，采用代码即动作的 REPL 方式，结合可验证奖励的强化学习 (RLVR)。", "result": "在 SimpleQA 上准确率从 30% 提升至 80%，HotpotQA 上从 22% 提升至 41%。", "conclusion": "L0 系统通过 RLVR 显著提升了代理的问题解决能力，并开源了完整系统。"}}
{"id": "2506.22960", "pdf": "https://arxiv.org/pdf/2506.22960", "abs": "https://arxiv.org/abs/2506.22960", "authors": ["Shreyas Dixit", "Ashhar Aziz", "Shashwat Bajpai", "Vasu Sharma", "Aman Chadha", "Vinija Jain", "Amitava Das"], "title": "Peccavi: Visual Paraphrase Attack Safe and Distortion Free Image Watermarking Technique for AI-Generated Images", "categories": ["cs.CV"], "comment": null, "summary": "A report by the European Union Law Enforcement Agency predicts that by 2026,\nup to 90 percent of online content could be synthetically generated, raising\nconcerns among policymakers, who cautioned that \"Generative AI could act as a\nforce multiplier for political disinformation. The combined effect of\ngenerative text, images, videos, and audio may surpass the influence of any\nsingle modality.\" In response, California's Bill AB 3211 mandates the\nwatermarking of AI-generated images, videos, and audio. However, concerns\nremain regarding the vulnerability of invisible watermarking techniques to\ntampering and the potential for malicious actors to bypass them entirely.\nGenerative AI-powered de-watermarking attacks, especially the newly introduced\nvisual paraphrase attack, have shown an ability to fully remove watermarks,\nresulting in a paraphrase of the original image. This paper introduces PECCAVI,\nthe first visual paraphrase attack-safe and distortion-free image watermarking\ntechnique. In visual paraphrase attacks, an image is altered while preserving\nits core semantic regions, termed Non-Melting Points (NMPs). PECCAVI\nstrategically embeds watermarks within these NMPs and employs multi-channel\nfrequency domain watermarking. It also incorporates noisy burnishing to counter\nreverse-engineering efforts aimed at locating NMPs to disrupt the embedded\nwatermark, thereby enhancing durability. PECCAVI is model-agnostic. All\nrelevant resources and codes will be open-sourced.", "AI": {"tldr": "欧盟报告预测到2026年90%的在线内容可能是AI生成的，引发对政治虚假信息的担忧。加州AB 3211法案要求对AI生成内容加水印，但现有技术易被篡改。本文提出PECCAVI，一种抗视觉转述攻击的无失真水印技术。", "motivation": "生成式AI可能加剧政治虚假信息传播，现有水印技术易被攻击，需更鲁棒的解决方案。", "method": "PECCAVI通过将水印嵌入非熔化点（NMPs）并结合多通道频域水印技术，同时采用噪声抛光防止逆向工程。", "result": "PECCAVI能有效抵抗视觉转述攻击，保持图像无失真，且模型无关。", "conclusion": "PECCAVI为AI生成内容的水印保护提供了更安全、耐用的解决方案，相关代码将开源。"}}
{"id": "2506.22496", "pdf": "https://arxiv.org/pdf/2506.22496", "abs": "https://arxiv.org/abs/2506.22496", "authors": ["Y. Du"], "title": "Mitigating Gambling-Like Risk-Taking Behaviors in Large Language Models: A Behavioral Economics Approach to AI Safety", "categories": ["cs.CY", "cs.AI", "cs.CL"], "comment": "7 pages", "summary": "Large Language Models (LLMs) exhibit systematic risk-taking behaviors\nanalogous to those observed in gambling psychology, including overconfidence\nbias, loss-chasing tendencies, and probability misjudgment. Drawing from\nbehavioral economics and prospect theory, we identify and formalize these\n\"gambling-like\" patterns where models sacrifice accuracy for high-reward\noutputs, exhibit escalating risk-taking after errors, and systematically\nmiscalibrate uncertainty. We propose the Risk-Aware Response Generation (RARG)\nframework, incorporating insights from gambling research to address these\nbehavioral biases through risk-calibrated training, loss-aversion mechanisms,\nand uncertainty-aware decision making. Our approach introduces novel evaluation\nparadigms based on established gambling psychology experiments, including AI\nadaptations of the Iowa Gambling Task and probability learning assessments.\nExperimental results demonstrate measurable reductions in gambling-like\nbehaviors: 18.7\\% decrease in overconfidence bias, 24.3\\% reduction in\nloss-chasing tendencies, and improved risk calibration across diverse\nscenarios. This work establishes the first systematic framework for\nunderstanding and mitigating gambling psychology patterns in AI systems.", "AI": {"tldr": "论文提出Risk-Aware Response Generation (RARG)框架，通过风险校准训练和损失规避机制，减少大语言模型中的赌博心理行为，如过度自信和损失追逐。", "motivation": "大语言模型（LLMs）表现出类似赌博心理的系统性风险行为，如过度自信和概率误判，需要系统性框架来理解和缓解这些行为。", "method": "基于行为经济学和前景理论，提出RARG框架，结合风险校准训练、损失规避机制和不确定性感知决策。", "result": "实验结果显示，RARG框架显著减少了赌博心理行为，如过度自信偏差降低18.7%，损失追逐倾向减少24.3%。", "conclusion": "该研究首次建立了系统性框架，用于理解和缓解AI系统中的赌博心理模式。"}}
{"id": "2506.23735", "pdf": "https://arxiv.org/pdf/2506.23735", "abs": "https://arxiv.org/abs/2506.23735", "authors": ["JiaRu Wu", "Mingwei Liu"], "title": "AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM Evaluation Data", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) have shown remarkable performance on various\ntasks, but existing evaluation benchmarks are often static and insufficient to\nfully assess their robustness and generalization in realistic scenarios. Prior\nwork using evolutionary or adversarial data augmentation has improved\nevaluation diversity but lacks systematic control over perturbation types and\nmulti-step complexity, limiting comprehensive robustness analysis. To address\nthese gaps, we propose AutoEvoEval, an evolution-based evaluation framework for\nclose-ended tasks such as multi-choice question answering. AutoEvoEval\nintroduces 22 interpretable atomic evolution operations and supports\nmulti-round compositions, enabling controlled generation of diverse,\nchallenging, and realistic test samples. We conduct extensive experiments\naddressing four research questions on a broad set of open- and closed-source\nLLMs. Our results show that atomic operations cause an average accuracy drop of\n7.283\\%, with structure-disrupting or misleading semantic edits causing the\nlargest declines. Model sensitivities vary significantly for the same\nperturbation, and combining multiple evolution steps amplifies adversarial\neffects by up to 52.932\\%. These findings suggest current benchmarks may\noverestimate true model generalization and emphasize the need for\nevolution-aware robustness evaluation. Code and resources are available at:\nhttps://github.com/SYSUSELab/AutoEvoEval.", "AI": {"tldr": "AutoEvoEval是一个基于进化的评估框架，用于生成多样化和具有挑战性的测试样本，以更全面地评估大型语言模型的鲁棒性和泛化能力。", "motivation": "现有评估基准多为静态，无法充分评估大型语言模型在现实场景中的鲁棒性和泛化能力。", "method": "提出AutoEvoEval框架，引入22种可解释的原子进化操作，支持多轮组合生成多样化测试样本。", "result": "实验显示原子操作平均导致准确率下降7.283%，结构破坏或误导性语义编辑影响最大，多步组合可放大对抗效果达52.932%。", "conclusion": "当前基准可能高估模型泛化能力，需进化感知的鲁棒性评估。"}}
{"id": "2506.22967", "pdf": "https://arxiv.org/pdf/2506.22967", "abs": "https://arxiv.org/abs/2506.22967", "authors": ["Amir Aghdam", "Vincent Tao Hu"], "title": "ActAlign: Zero-Shot Fine-Grained Video Classification via Language-Guided Sequence Alignment", "categories": ["cs.CV", "cs.LG", "cs.MM", "I.2.10; I.2.7"], "comment": "Preprint manuscript - Project page:\n  https://github.com/aghdamamir/act-align", "summary": "We address the task of zero-shot fine-grained video classification, where no\nvideo examples or temporal annotations are available for unseen action classes.\nWhile contrastive vision-language models such as SigLIP demonstrate strong\nopen-set recognition via mean-pooled image-text similarity, they fail to\ncapture the temporal structure critical for distinguishing fine-grained\nactivities. We introduce ActAlign, a zero-shot framework that formulates video\nclassification as sequence alignment. For each class, a large language model\ngenerates an ordered sub-action sequence, which is aligned with video frames\nusing Dynamic Time Warping (DTW) in a shared embedding space. Without any\nvideo-text supervision or fine-tuning, ActAlign achieves 30.5% accuracy on the\nextremely challenging ActionAtlas benchmark, where human accuracy is only\n61.6%. ActAlign outperforms billion-parameter video-language models while using\napproximately 8x less parameters. These results demonstrate that structured\nlanguage priors, combined with classical alignment techniques, offer a scalable\nand general approach to unlocking the open-set recognition potential of\nvision-language models for fine-grained video understanding.", "AI": {"tldr": "ActAlign框架通过语言模型生成子动作序列，结合动态时间规整（DTW）实现零样本细粒度视频分类，无需视频文本监督或微调。", "motivation": "解决零样本细粒度视频分类任务中，现有对比视觉语言模型无法捕捉关键时间结构的问题。", "method": "使用大型语言模型生成有序子动作序列，通过DTW在共享嵌入空间中对齐视频帧。", "result": "在ActionAtlas基准测试中达到30.5%准确率，优于十亿参数视频语言模型且参数更少。", "conclusion": "结构化语言先验结合经典对齐技术，为细粒度视频理解提供可扩展的通用方法。"}}
{"id": "2506.22497", "pdf": "https://arxiv.org/pdf/2506.22497", "abs": "https://arxiv.org/abs/2506.22497", "authors": ["Craig Steven Wright"], "title": "Peer Review as Structured Commentary: Immutable Identity, Public Dialogue, and Reproducible Scholarship", "categories": ["cs.CY", "cs.AI", "cs.DL", "cs.SI", "physics.hist-ph", "68T99, 03B30, 91D30", "I.2.0; H.3.5; K.4.4"], "comment": "66 pages, 0 figures, interdisciplinary framework, includes proposed\n  architecture and metadata layer structures", "summary": "This paper reconceptualises peer review as structured public commentary.\nTraditional academic validation is hindered by anonymity, latency, and\ngatekeeping. We propose a transparent, identity-linked, and reproducible system\nof scholarly evaluation anchored in open commentary. Leveraging blockchain for\nimmutable audit trails and AI for iterative synthesis, we design a framework\nthat incentivises intellectual contribution, captures epistemic evolution, and\nenables traceable reputational dynamics. This model empowers fields from\ncomputational science to the humanities, reframing academic knowledge as a\nliving process rather than a static credential.", "AI": {"tldr": "论文提出了一种基于公开评论的透明、身份关联且可复现的学术评价系统，利用区块链和AI技术，旨在解决传统同行评审的匿名性、延迟和把关问题。", "motivation": "传统学术评审因匿名性、延迟和把关问题阻碍了学术验证的效率与公平性。", "method": "设计了一个透明、身份关联的学术评价框架，结合区块链的不可篡改性和AI的迭代合成能力。", "result": "该系统能够激励学术贡献、记录知识演化过程，并实现可追溯的声誉动态。", "conclusion": "该模型将学术知识视为动态过程而非静态凭证，适用于从计算科学到人文的多个领域。"}}
{"id": "2506.23743", "pdf": "https://arxiv.org/pdf/2506.23743", "abs": "https://arxiv.org/abs/2506.23743", "authors": ["Tiziano Labruna", "Simone Gallo", "Giovanni Da San Martino"], "title": "Positional Bias in Binary Question Answering: How Uncertainty Shapes Model Preferences", "categories": ["cs.CL"], "comment": null, "summary": "Positional bias in binary question answering occurs when a model\nsystematically favors one choice over another based solely on the ordering of\npresented options. In this study, we quantify and analyze positional bias\nacross five large language models under varying degrees of answer uncertainty.\nWe re-adapted the SQuAD-it dataset by adding an extra incorrect answer option\nand then created multiple versions with progressively less context and more\nout-of-context answers, yielding datasets that range from low to high\nuncertainty. Additionally, we evaluate two naturally higher-uncertainty\nbenchmarks: (1) WebGPT - question pairs with unequal human-assigned quality\nscores, and (2) Winning Arguments - where models predict the more persuasive\nargument in Reddit's r/ChangeMyView exchanges. Across each dataset, the order\nof the \"correct\" (or higher-quality/persuasive) option is systematically\nflipped (first placed in position 1, then in position 2) to compute both\nPreference Fairness and Position Consistency. We observe that positional bias\nis nearly absent under low-uncertainty conditions, but grows exponentially when\nit becomes doubtful to decide which option is correct.", "AI": {"tldr": "研究量化了大型语言模型在二元问题回答中的位置偏见，发现低不确定性条件下偏见几乎不存在，但在高不确定性条件下显著增加。", "motivation": "探讨模型在二元问题回答中因选项顺序而产生的系统性偏好，尤其是在不同不确定性条件下的表现。", "method": "通过改编SQuAD-it数据集，创建不同不确定性水平的版本，并评估WebGPT和Winning Arguments两个高不确定性基准。", "result": "位置偏见在低不确定性条件下几乎不存在，但在高不确定性条件下呈指数增长。", "conclusion": "模型在不确定性高时更容易受选项顺序影响，需在设计评估时考虑位置偏见的潜在影响。"}}
{"id": "2506.22979", "pdf": "https://arxiv.org/pdf/2506.22979", "abs": "https://arxiv.org/abs/2506.22979", "authors": ["Jie Liu", "Jiayi Shen", "Pan Zhou", "Jan-Jakob Sonke", "Efstratios Gavves"], "title": "Probabilistic Prototype Calibration of Vision-Language Models for Generalized Few-shot Semantic Segmentation", "categories": ["cs.CV"], "comment": "ICCV2025 Proceeding", "summary": "Generalized Few-Shot Semantic Segmentation (GFSS) aims to extend a\nsegmentation model to novel classes with only a few annotated examples while\nmaintaining performance on base classes. Recently, pretrained vision-language\nmodels (VLMs) such as CLIP have been leveraged in GFSS to improve\ngeneralization on novel classes through multi-modal prototypes learning.\nHowever, existing prototype-based methods are inherently deterministic,\nlimiting the adaptability of learned prototypes to diverse samples,\nparticularly for novel classes with scarce annotations. To address this, we\npropose FewCLIP, a probabilistic prototype calibration framework over\nmulti-modal prototypes from the pretrained CLIP, thus providing more adaptive\nprototype learning for GFSS. Specifically, FewCLIP first introduces a prototype\ncalibration mechanism, which refines frozen textual prototypes with learnable\nvisual calibration prototypes, leading to a more discriminative and adaptive\nrepresentation. Furthermore, unlike deterministic prototype learning\ntechniques, FewCLIP introduces distribution regularization over these\ncalibration prototypes. This probabilistic formulation ensures structured and\nuncertainty-aware prototype learning, effectively mitigating overfitting to\nlimited novel class data while enhancing generalization. Extensive experimental\nresults on PASCAL-5$^i$ and COCO-20$^i$ datasets demonstrate that our proposed\nFewCLIP significantly outperforms state-of-the-art approaches across both GFSS\nand class-incremental setting. The code is available at\nhttps://github.com/jliu4ai/FewCLIP.", "AI": {"tldr": "FewCLIP提出了一种基于概率原型校准的框架，通过多模态原型学习改进广义少样本语义分割（GFSS）的性能。", "motivation": "现有基于原型的方法在少样本情况下适应性不足，FewCLIP旨在通过概率原型校准提升对稀缺标注数据的适应性和泛化能力。", "method": "FewCLIP结合视觉校准原型优化冻结的文本原型，并通过分布正则化实现不确定性感知的原型学习。", "result": "在PASCAL-5$^i$和COCO-20$^i$数据集上，FewCLIP显著优于现有方法。", "conclusion": "FewCLIP通过概率原型校准有效提升了GFSS的性能，尤其在少样本情况下表现优异。"}}
{"id": "2506.23840", "pdf": "https://arxiv.org/pdf/2506.23840", "abs": "https://arxiv.org/abs/2506.23840", "authors": ["Bowen Ding", "Yuhan Chen", "Futing Wang", "Lingfeng Ming", "Tao Lin"], "title": "Do Thinking Tokens Help or Trap? Towards More Efficient Large Reasoning Model", "categories": ["cs.CL", "cs.AI"], "comment": "13 pages, 5 figures", "summary": "Large Reasoning Models (LRMs) excel at solving complex problems but face an\noverthinking dilemma. When handling simple tasks, they often produce verbose\nresponses overloaded with thinking tokens (e.g., wait, however). These tokens\ntrigger unnecessary high-level reasoning behaviors like reflection and\nbacktracking, reducing efficiency. In this work, our pilot study reveals that\nthese thinking-token-induced behaviors are not essential for effective\nproblem-solving and may even hinder correct reasoning within constrained token\nbudgets. We identify this phenomenon as the thinking trap. To mitigate this\nissue, we propose Dual Policy Preference Optimization (DuP-PO), a novel\nalgorithm featuring: (1) A rollout sampling strategy that guarantees balanced\nexposure to responses with and without thinking tokens; (2) A fine-grained\nadvantage control technique to dynamically regulate the prediction of target\ntokens; (3) A policy shaping method ensuring stable gradient contributions from\nthinking tokens. Experimental results on five popular math reasoning benchmarks\nshow that DuP-PO performs well on the popular LRM, which significantly improves\ntheir token efficiency during reasoning, while achieving superior performance\nof the base model.", "AI": {"tldr": "DuP-PO算法通过优化策略减少大型推理模型在简单任务中的冗余思考标记，提升效率和性能。", "motivation": "大型推理模型在处理简单任务时会产生冗余的思考标记，降低效率，甚至影响正确推理。", "method": "提出Dual Policy Preference Optimization (DuP-PO)，包括均衡采样策略、细粒度优势控制技术和策略塑造方法。", "result": "在五个数学推理基准测试中，DuP-PO显著提升了模型的标记效率和性能。", "conclusion": "DuP-PO有效解决了大型推理模型的“思考陷阱”问题，提升了推理效率和准确性。"}}
{"id": "2506.22982", "pdf": "https://arxiv.org/pdf/2506.22982", "abs": "https://arxiv.org/abs/2506.22982", "authors": ["Atharv Mittal", "Agam Pandey", "Amritanshu Tiwari", "Sukrit Jindal", "Swadesh Swain"], "title": "Revisiting CroPA: A Reproducibility Study and Enhancements for Cross-Prompt Adversarial Transferability in Vision-Language Models", "categories": ["cs.CV"], "comment": "Accepted to MLRC 2025", "summary": "Large Vision-Language Models (VLMs) have revolutionized computer vision,\nenabling tasks such as image classification, captioning, and visual question\nanswering. However, they remain highly vulnerable to adversarial attacks,\nparticularly in scenarios where both visual and textual modalities can be\nmanipulated. In this study, we conduct a comprehensive reproducibility study of\n\"An Image is Worth 1000 Lies: Adversarial Transferability Across Prompts on\nVision-Language Models\" validating the Cross-Prompt Attack (CroPA) and\nconfirming its superior cross-prompt transferability compared to existing\nbaselines. Beyond replication we propose several key improvements: (1) A novel\ninitialization strategy that significantly improves Attack Success Rate (ASR).\n(2) Investigate cross-image transferability by learning universal\nperturbations. (3) A novel loss function targeting vision encoder attention\nmechanisms to improve generalization. Our evaluation across prominent VLMs --\nincluding Flamingo, BLIP-2, and InstructBLIP as well as extended experiments on\nLLaVA validates the original results and demonstrates that our improvements\nconsistently boost adversarial effectiveness. Our work reinforces the\nimportance of studying adversarial vulnerabilities in VLMs and provides a more\nrobust framework for generating transferable adversarial examples, with\nsignificant implications for understanding the security of VLMs in real-world\napplications.", "AI": {"tldr": "该论文研究了大型视觉语言模型（VLMs）的对抗攻击脆弱性，验证了Cross-Prompt Attack（CroPA）的优越性，并提出三项改进以增强攻击效果。", "motivation": "VLMs在视觉任务中表现优异，但对多模态对抗攻击高度脆弱，研究其安全性具有实际意义。", "method": "通过改进初始化策略、研究跨图像扰动和设计新损失函数，提升CroPA的攻击效果。", "result": "改进后的方法在多种VLMs（如Flamingo、BLIP-2等）上验证了CroPA的优越性，并显著提升了攻击成功率。", "conclusion": "研究强调了VLMs对抗攻击脆弱性的重要性，并提供了更鲁棒的对抗样本生成框架，对实际应用安全有重要意义。"}}
{"id": "2506.23864", "pdf": "https://arxiv.org/pdf/2506.23864", "abs": "https://arxiv.org/abs/2506.23864", "authors": ["Seyed Mahed Mousavi", "Edoardo Cecchinato", "Lucia Hornikova", "Giuseppe Riccardi"], "title": "Garbage In, Reasoning Out? Why Benchmark Scores are Unreliable and What to Do About It", "categories": ["cs.CL"], "comment": null, "summary": "We conduct a systematic audit of three widely used reasoning benchmarks,\nSocialIQa, FauxPas-EAI, and ToMi, and uncover pervasive flaws in both benchmark\nitems and evaluation methodology. Using five LLMs (GPT-{3, 3.5, 4, o1}, and\nLLaMA 3.1) as diagnostic tools, we identify structural, semantic, and pragmatic\nissues in benchmark design (e.g., duplicated items, ambiguous wording, and\nimplausible answers), as well as scoring procedures that prioritize output form\nover reasoning process. Through systematic human annotation and re-evaluation\non cleaned benchmark subsets, we find that model scores often improve not due\nto due to erratic surface wording variations and not to improved reasoning.\nInfact, further analyses show that model performance is highly sensitive to\nminor input variations such as context availability and phrasing, revealing\nthat high scores may reflect alignment with format-specific cues rather than\nconsistent inference based on the input. These findings challenge the validity\nof current benchmark-based claims about reasoning in LLMs, and highlight the\nneed for evaluation protocols that assess reasoning as a process of drawing\ninference from available information, rather than as static output selection.\nWe release audited data and evaluation tools to support more interpretable and\ndiagnostic assessments of model reasoning.", "AI": {"tldr": "论文对三个广泛使用的推理基准（SocialIQa、FauxPas-EAI和ToMi）进行了系统审计，揭示了基准设计和评估方法中的普遍缺陷。通过使用五种LLM作为诊断工具，发现了结构、语义和语用问题，并指出评分程序过于注重输出形式而非推理过程。研究发现模型性能对输入微小变化敏感，高分可能仅反映对格式特定线索的匹配，而非真正的推理能力。", "motivation": "当前推理基准的设计和评估方法存在缺陷，可能导致对LLM推理能力的错误评估。论文旨在揭示这些问题并提出改进方向。", "method": "使用五种LLM（GPT-3、GPT-3.5、GPT-4、GPT-o1和LLaMA 3.1）作为诊断工具，结合系统的人工标注和重新评估，分析基准中的问题。", "result": "发现基准中存在重复项、模糊表述和不可信答案等问题，模型性能对输入微小变化敏感，高分可能仅反映格式匹配而非推理能力。", "conclusion": "当前基于基准的LLM推理能力评估存在局限性，需要开发更注重推理过程的评估协议。论文发布了审计数据和工具以支持更透明的模型评估。"}}
{"id": "2506.23004", "pdf": "https://arxiv.org/pdf/2506.23004", "abs": "https://arxiv.org/abs/2506.23004", "authors": ["Vaigai Nayaki Yokar", "Hoa Le-Minh", "Xicong Li", "Wai Lok Woo", "Luis Nero Alves", "Stanislav Zvanovec", "Tran The Son", "Zabih Ghassemlooy"], "title": "A Novel Frame Identification and Synchronization Technique for Smartphone Visible Light Communication Systems Based on Convolutional Neural Networks", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "This paper proposes a novel, robust, and lightweight supervised Convolutional\nNeural Network (CNN)-based technique for frame identification and\nsynchronization, designed to enhance short-link communication performance in a\nscreen-to-camera (S2C) based visible light communication (VLC) system.\nDeveloped using Python and the TensorFlow Keras framework, the proposed CNN\nmodel was trained through three real-time experimental investigations conducted\nin Jupyter Notebook. These experiments incorporated a dataset created from\nscratch to address various real-time challenges in S2C communication, including\nblurring, cropping, and rotated images in mobility scenarios. Overhead frames\nwere introduced for synchronization, which leads to enhanced system\nperformance. The experimental results demonstrate that the proposed model\nachieves an overall accuracy of approximately 98.74%, highlighting its\neffectiveness in identifying and synchronizing frames in S2C VLC systems.", "AI": {"tldr": "提出一种轻量级CNN方法，用于屏幕到相机可见光通信中的帧识别与同步，实验准确率达98.74%。", "motivation": "解决屏幕到相机通信中因模糊、裁剪和旋转图像导致的实时挑战，提升短链路通信性能。", "method": "使用Python和TensorFlow Keras框架训练CNN模型，通过三次实时实验验证，引入同步帧提升性能。", "result": "模型在实验中达到98.74%的准确率，有效识别和同步帧。", "conclusion": "该方法在屏幕到相机可见光通信中表现优异，具有实际应用潜力。"}}
{"id": "2506.23888", "pdf": "https://arxiv.org/pdf/2506.23888", "abs": "https://arxiv.org/abs/2506.23888", "authors": ["André de Souza Loureiro", "Jorge Valverde-Rebaza", "Julieta Noguez", "David Escarcega", "Ricardo Marcacini"], "title": "Advancing Multi-Step Mathematical Reasoning in Large Language Models through Multi-Layered Self-Reflection with Auto-Prompting", "categories": ["cs.CL"], "comment": "Accepted for publication in: European Conference on Machine Learning\n  and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD\n  2025). Research Track", "summary": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved their problem-solving capabilities. However, these models still\nstruggle when faced with complex multi-step reasoning tasks. In this paper, we\npropose the Multi-Layered Self-Reflection with Auto-Prompting (MAPS) framework,\na novel approach designed to enhance multi-step mathematical reasoning in LLMs\nby integrating techniques such as Chain of Thought (CoT), Self-Reflection, and\nAuto-Prompting. Unlike traditional static prompting methods, MAPS employs an\niterative refinement process. Initially, the model generates a solution using\nCoT prompting. When errors are detected, an adaptive self-reflection mechanism\nidentifies and analyzes them, generating tailored prompts to guide corrections.\nThese dynamically adjusted prompts enable the model to iteratively refine its\nreasoning. Experiments on four well-established benchmarks across multiple LLMs\nshow that MAPS significantly outperforms standard CoT and achieves competitive\nresults with reasoning-optimized models. In addition, MAPS enables\ngeneral-purpose LLMs to reach performance levels comparable to specialized\nreasoning models. While deeper reflection layers improve accuracy, they also\nincrease token usage and costs. To balance this trade-off, MAPS strategically\nlimits reflection depth, ensuring an optimal balance between cost and reasoning\nperformance.", "AI": {"tldr": "提出了一种名为MAPS的新框架，通过结合Chain of Thought、自我反思和自动提示技术，提升LLMs在复杂多步数学推理任务中的表现。", "motivation": "尽管LLMs在问题解决能力上有显著提升，但在复杂多步推理任务中仍表现不佳，因此需要一种更有效的方法来增强其推理能力。", "method": "MAPS采用迭代优化方法，首先生成初步解决方案，通过自我反思机制检测错误并生成定制提示，动态调整推理过程。", "result": "实验表明，MAPS在多个基准测试中显著优于标准CoT，并与专用推理模型性能相当。", "conclusion": "MAPS通过限制反思深度平衡了性能和成本，为LLMs在多步推理任务中的应用提供了有效解决方案。"}}
{"id": "2506.23009", "pdf": "https://arxiv.org/pdf/2506.23009", "abs": "https://arxiv.org/abs/2506.23009", "authors": ["Jian Chen", "Wenye Ma", "Penghang Liu", "Wei Wang", "Tengwei Song", "Ming Li", "Chenguang Wang", "Ruiyi Zhang", "Changyou Chen"], "title": "MusiXQA: Advancing Visual Music Understanding in Multimodal Large Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable visual\nreasoning abilities in natural images, text-rich documents, and graphic\ndesigns. However, their ability to interpret music sheets remains\nunderexplored. To bridge this gap, we introduce MusiXQA, the first\ncomprehensive dataset for evaluating and advancing MLLMs in music sheet\nunderstanding. MusiXQA features high-quality synthetic music sheets generated\nvia MusiXTeX, with structured annotations covering note pitch and duration,\nchords, clefs, key/time signatures, and text, enabling diverse visual QA tasks.\nThrough extensive evaluations, we reveal significant limitations of current\nstate-of-the-art MLLMs in this domain. Beyond benchmarking, we developed\nPhi-3-MusiX, an MLLM fine-tuned on our dataset, achieving significant\nperformance gains over GPT-based methods. The proposed dataset and model\nestablish a foundation for future advances in MLLMs for music sheet\nunderstanding. Code, data, and model will be released upon acceptance.", "AI": {"tldr": "论文介绍了MusiXQA数据集和Phi-3-MusiX模型，填补了多模态大语言模型在乐谱理解领域的空白，并展示了当前模型的局限性。", "motivation": "探索多模态大语言模型在乐谱理解方面的能力，填补现有研究的空白。", "method": "使用MusiXTeX生成高质量合成乐谱，构建MusiXQA数据集，并开发Phi-3-MusiX模型进行微调。", "result": "当前最先进的多模态大语言模型在乐谱理解方面表现有限，Phi-3-MusiX模型显著优于GPT方法。", "conclusion": "MusiXQA数据集和Phi-3-MusiX模型为未来乐谱理解研究奠定了基础。"}}
{"id": "2506.23921", "pdf": "https://arxiv.org/pdf/2506.23921", "abs": "https://arxiv.org/abs/2506.23921", "authors": ["Germans Savcisens", "Tina Eliassi-Rad"], "title": "The Trilemma of Truth in Large Language Models", "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "We often attribute human characteristics to large language models (LLMs) and\nclaim that they \"know\" certain things. LLMs have an internal probabilistic\nknowledge that represents information retained during training. How can we\nassess the veracity of this knowledge? We examine two common methods for\nprobing the veracity of LLMs and discover several assumptions that are flawed.\nTo address these flawed assumptions, we introduce sAwMIL (short for Sparse\nAware Multiple-Instance Learning), a probing method that utilizes the internal\nactivations of LLMs to separate statements into true, false, and neither.\nsAwMIL is based on multiple-instance learning and conformal prediction. We\nevaluate sAwMIL on 5 validity criteria across 16 open-source LLMs, including\nboth default and chat-based variants, as well as on 3 new datasets. Among the\ninsights we provide are: (1) the veracity signal is often concentrated in the\nthird quarter of an LLM's depth; (2) truth and falsehood signals are not always\nsymmetric; (3) linear probes perform better on chat models than on default\nmodels; (4) nonlinear probes may be required to capture veracity signals for\nsome LLMs with reinforcement learning from human feedback or knowledge\ndistillation; and (5) LLMs capture a third type of signal that is distinct from\ntrue and false and is neither true nor false. These findings provide a reliable\nmethod for verifying what LLMs \"know\" and how certain they are of their\nprobabilistic internal knowledge.", "AI": {"tldr": "论文探讨了如何评估大语言模型（LLM）内部知识的真实性，提出了一种名为sAwMIL的新方法，并揭示了关于LLM知识真实性的五个关键发现。", "motivation": "由于人们常将人类特性赋予LLM并假设其“知道”某些内容，但LLM的内部知识是概率性的，因此需要一种可靠的方法来验证其真实性。", "method": "提出了sAwMIL方法，基于多实例学习和一致性预测，利用LLM的内部激活状态将陈述分为真、假和中性三类。", "result": "在16个开源LLM上评估sAwMIL，发现真实性信号多集中在模型深度的第三部分，且真伪信号不对称；线性探针在聊天模型中表现更好，非线性探针适用于某些特定LLM。", "conclusion": "sAwMIL提供了一种可靠的方法来验证LLM的内部知识真实性，揭示了LLM知识的多维特性。"}}
{"id": "2506.23030", "pdf": "https://arxiv.org/pdf/2506.23030", "abs": "https://arxiv.org/abs/2506.23030", "authors": ["Alejandro Romero Amezcua", "Mariano José Juan Rivera Meraz"], "title": "VisionScores -- A system-segmented image score dataset for deep learning tasks", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SD", "eess.AS"], "comment": "Comments: 5 pages, 3 figures. Accepted for presentation at the 2025\n  IEEE International Conference on Image Processing (ICIP). \\c{opyright} 2025\n  IEEE. Personal use of this material is permitted. Permission from IEEE must\n  be obtained for any other use", "summary": "VisionScores presents a novel proposal being the first system-segmented image\nscore dataset, aiming to offer structure-rich, high information-density images\nfor machine and deep learning tasks. Delimited to two-handed piano pieces, it\nwas built to consider not only certain graphic similarity but also composition\npatterns, as this creative process is highly instrument-dependent. It provides\ntwo scenarios in relation to composer and composition type. The first, formed\nby 14k samples, considers works from different authors but the same composition\ntype, specifically, Sonatinas. The latter, consisting of 10.8K samples,\npresents the opposite case, various composition types from the same author,\nbeing the one selected Franz Liszt. All of the 24.8k samples are formatted as\ngrayscale jpg images of $128 \\times 512$ pixels. VisionScores supplies the\nusers not only the formatted samples but the systems' order and pieces'\nmetadata. Moreover, unsegmented full-page scores and the pre-formatted images\nare included for further analysis.", "AI": {"tldr": "VisionScores是一个首个系统分割的图像乐谱数据集，专注于双钢琴曲目，提供高信息密度的图像，适用于机器和深度学习任务。", "motivation": "旨在为机器和深度学习任务提供结构丰富、信息密集的图像乐谱数据，同时考虑图形相似性和作曲模式。", "method": "数据集包含24.8k样本，分为两个场景：同一作曲类型不同作者（14k样本）和同一作者不同作曲类型（10.8k样本）。所有样本为128×512像素的灰度图像。", "result": "提供了格式化样本、系统顺序、乐谱元数据、未分割的完整乐谱和预格式化图像。", "conclusion": "VisionScores为乐谱分析和深度学习任务提供了丰富的数据支持。"}}
{"id": "2506.22506", "pdf": "https://arxiv.org/pdf/2506.22506", "abs": "https://arxiv.org/abs/2506.22506", "authors": ["Momin Ahmad Khan", "Yasra Chandio", "Fatima Muhammad Anwar"], "title": "SABRE-FL: Selective and Accurate Backdoor Rejection for Federated Prompt Learning", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Federated Prompt Learning has emerged as a communication-efficient and\nprivacy-preserving paradigm for adapting large vision-language models like CLIP\nacross decentralized clients. However, the security implications of this setup\nremain underexplored. In this work, we present the first study of backdoor\nattacks in Federated Prompt Learning. We show that when malicious clients\ninject visually imperceptible, learnable noise triggers into input images, the\nglobal prompt learner becomes vulnerable to targeted misclassification while\nstill maintaining high accuracy on clean inputs. Motivated by this\nvulnerability, we propose SABRE-FL, a lightweight, modular defense that filters\npoisoned prompt updates using an embedding-space anomaly detector trained\noffline on out-of-distribution data. SABRE-FL requires no access to raw client\ndata or labels and generalizes across diverse datasets. We show, both\ntheoretically and empirically, that malicious clients can be reliably\nidentified and filtered using an embedding-based detector. Across five diverse\ndatasets and four baseline defenses, SABRE-FL outperforms all baselines by\nsignificantly reducing backdoor accuracy while preserving clean accuracy,\ndemonstrating strong empirical performance and underscoring the need for robust\nprompt learning in future federated systems.", "AI": {"tldr": "本文研究了联邦提示学习中的后门攻击，并提出了一种轻量级防御方法SABRE-FL，通过嵌入空间异常检测器过滤恶意提示更新。", "motivation": "联邦提示学习的安全隐患尚未充分研究，特别是后门攻击的威胁。", "method": "提出SABRE-FL防御方法，利用离线训练的嵌入空间异常检测器识别并过滤恶意提示更新。", "result": "在五个数据集和四种基线防御上，SABRE-FL显著降低后门攻击准确率，同时保持干净输入的准确性。", "conclusion": "SABRE-FL展示了强大的防御性能，强调了未来联邦系统中鲁棒提示学习的必要性。"}}
{"id": "2506.23929", "pdf": "https://arxiv.org/pdf/2506.23929", "abs": "https://arxiv.org/abs/2506.23929", "authors": ["Mohammed J. Saeed", "Tommi Vehvilainen", "Evgeny Fedoseev", "Sevil Caliskan", "Tatiana Vodolazova"], "title": "IMPACT: Inflectional Morphology Probes Across Complex Typologies", "categories": ["cs.CL"], "comment": null, "summary": "Large Language Models (LLMs) have shown significant progress on various\nmultilingual benchmarks and are increasingly used to generate and evaluate text\nin non-English languages. However, while they may produce fluent outputs, it\nremains unclear to what extent these models truly grasp the underlying\nlinguistic complexity of those languages, particularly in morphology. To\ninvestigate this, we introduce IMPACT, a synthetically generated evaluation\nframework focused on inflectional morphology, which we publicly release,\ndesigned to evaluate LLM performance across five morphologically rich\nlanguages: Arabic, Russian, Finnish, Turkish, and Hebrew. IMPACT includes\nunit-test-style cases covering both shared and language-specific phenomena,\nfrom basic verb inflections (e.g., tense, number, gender) to unique features\nlike Arabic's reverse gender agreement and vowel harmony in Finnish and\nTurkish. We assess eight multilingual LLMs that, despite strong English\nperformance, struggle with other languages and uncommon morphological patterns,\nespecially when judging ungrammatical examples. We also show that Chain of\nThought and Thinking Models can degrade performance. Our work exposes gaps in\nLLMs' handling of linguistic complexity, pointing to clear room for\nimprovement. To support further research, we publicly release the IMPACT\nframework.", "AI": {"tldr": "IMPACT是一个评估框架，用于测试多语言大语言模型（LLMs）在形态学上的表现，揭示了模型在非英语语言中的不足。", "motivation": "研究LLMs是否真正理解非英语语言的形态学复杂性，尤其是在生成流利输出时。", "method": "引入IMPACT框架，通过合成数据评估五种形态丰富的语言（阿拉伯语、俄语、芬兰语、土耳其语、希伯来语）中的LLM表现。", "result": "尽管LLMs在英语上表现良好，但在其他语言和不常见形态模式上表现不佳，尤其是判断不合语法示例时。", "conclusion": "LLMs在语言复杂性处理上存在明显不足，需进一步改进。IMPACT框架已公开以支持后续研究。"}}
{"id": "2506.23038", "pdf": "https://arxiv.org/pdf/2506.23038", "abs": "https://arxiv.org/abs/2506.23038", "authors": ["Xinrong Hu", "Yiyu Shi"], "title": "Inpainting is All You Need: A Diffusion-based Augmentation Method for Semi-supervised Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Collecting pixel-level labels for medical datasets can be a laborious and\nexpensive process, and enhancing segmentation performance with a scarcity of\nlabeled data is a crucial challenge. This work introduces AugPaint, a data\naugmentation framework that utilizes inpainting to generate image-label pairs\nfrom limited labeled data. AugPaint leverages latent diffusion models, known\nfor their ability to generate high-quality in-domain images with low overhead,\nand adapts the sampling process for the inpainting task without need for\nretraining. Specifically, given a pair of image and label mask, we crop the\narea labeled with the foreground and condition on it during reversed denoising\nprocess for every noise level. Masked background area would gradually be filled\nin, and all generated images are paired with the label mask. This approach\nensures the accuracy of match between synthetic images and label masks, setting\nit apart from existing dataset generation methods. The generated images serve\nas valuable supervision for training downstream segmentation models,\neffectively addressing the challenge of limited annotations. We conducted\nextensive evaluations of our data augmentation method on four public medical\nimage segmentation datasets, including CT, MRI, and skin imaging. Results\nacross all datasets demonstrate that AugPaint outperforms state-of-the-art\nlabel-efficient methodologies, significantly improving segmentation\nperformance.", "AI": {"tldr": "AugPaint是一种数据增强框架，利用潜在扩散模型通过修复生成图像-标签对，显著提升医学图像分割性能。", "motivation": "医学数据集的像素级标注成本高昂且耗时，如何在标注数据稀缺的情况下提升分割性能是关键挑战。", "method": "AugPaint利用潜在扩散模型进行修复任务，无需重新训练，通过反向去噪过程生成与标签掩码匹配的合成图像。", "result": "在四个公共医学图像分割数据集上的实验表明，AugPaint优于现有方法，显著提升分割性能。", "conclusion": "AugPaint为标注稀缺问题提供了一种高效解决方案，生成的数据能有效提升下游分割模型的性能。"}}
{"id": "2506.23930", "pdf": "https://arxiv.org/pdf/2506.23930", "abs": "https://arxiv.org/abs/2506.23930", "authors": ["Ruhina Tabasshum Prome", "Tarikul Islam Tamiti", "Anomadarshi Barua"], "title": "Leveraging the Potential of Prompt Engineering for Hate Speech Detection in Low-Resource Languages", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The rapid expansion of social media leads to a marked increase in hate\nspeech, which threatens personal lives and results in numerous hate crimes.\nDetecting hate speech presents several challenges: diverse dialects, frequent\ncode-mixing, and the prevalence of misspelled words in user-generated content\non social media platforms. Recent progress in hate speech detection is\ntypically concentrated on high-resource languages. However, low-resource\nlanguages still face significant challenges due to the lack of large-scale,\nhigh-quality datasets. This paper investigates how we can overcome this\nlimitation via prompt engineering on large language models (LLMs) focusing on\nlow-resource Bengali language. We investigate six prompting strategies -\nzero-shot prompting, refusal suppression, flattering the classifier, multi-shot\nprompting, role prompting, and finally our innovative metaphor prompting to\ndetect hate speech effectively in low-resource languages. We pioneer the\nmetaphor prompting to circumvent the built-in safety mechanisms of LLMs that\nmarks a significant departure from existing jailbreaking methods. We\ninvestigate all six different prompting strategies on the Llama2-7B model and\ncompare the results extensively with three pre-trained word embeddings - GloVe,\nWord2Vec, and FastText for three different deep learning models - multilayer\nperceptron (MLP), convolutional neural network (CNN), and bidirectional gated\nrecurrent unit (BiGRU). To prove the effectiveness of our metaphor prompting in\nthe low-resource Bengali language, we also evaluate it in another low-resource\nlanguage - Hindi, and two high-resource languages - English and German. The\nperformance of all prompting techniques is evaluated using the F1 score, and\nenvironmental impact factor (IF), which measures CO$_2$ emissions, electricity\nusage, and computational time.", "AI": {"tldr": "论文探讨了通过提示工程在低资源孟加拉语中检测仇恨言论的方法，提出了六种提示策略，并重点介绍了创新的隐喻提示法。", "motivation": "社交媒体仇恨言论激增，但低资源语言缺乏高质量数据集，研究旨在通过大语言模型解决这一问题。", "method": "研究了六种提示策略，包括零样本提示、拒绝抑制、多样本提示等，并在Llama2-7B模型上测试。", "result": "隐喻提示法在低资源语言中表现优异，并在其他语言中验证了其有效性。", "conclusion": "隐喻提示法为低资源语言仇恨言论检测提供了新思路，且环境友好。"}}
{"id": "2506.23042", "pdf": "https://arxiv.org/pdf/2506.23042", "abs": "https://arxiv.org/abs/2506.23042", "authors": ["Hung Nguyen", "An Le", "Runfa Li", "Truong Nguyen"], "title": "From Coarse to Fine: Learnable Discrete Wavelet Transforms for Efficient 3D Gaussian Splatting", "categories": ["cs.CV"], "comment": "Accepted to ICCV Workshop", "summary": "3D Gaussian Splatting has emerged as a powerful approach in novel view\nsynthesis, delivering rapid training and rendering but at the cost of an\never-growing set of Gaussian primitives that strains memory and bandwidth. We\nintroduce AutoOpti3DGS, a training-time framework that automatically restrains\nGaussian proliferation without sacrificing visual fidelity. The key idea is to\nfeed the input images to a sequence of learnable Forward and Inverse Discrete\nWavelet Transforms, where low-pass filters are kept fixed, high-pass filters\nare learnable and initialized to zero, and an auxiliary orthogonality loss\ngradually activates fine frequencies. This wavelet-driven, coarse-to-fine\nprocess delays the formation of redundant fine Gaussians, allowing 3DGS to\ncapture global structure first and refine detail only when necessary. Through\nextensive experiments, AutoOpti3DGS requires just a single filter learning-rate\nhyper-parameter, integrates seamlessly with existing efficient 3DGS frameworks,\nand consistently produces sparser scene representations more compatible with\nmemory or storage-constrained hardware.", "AI": {"tldr": "AutoOpti3DGS通过小波变换限制高斯增长，提升3D高斯溅射的内存效率。", "motivation": "解决3D高斯溅射中高斯基元数量增长导致的内存和带宽压力。", "method": "使用可学习的离散小波变换，固定低通滤波器，学习高通滤波器，并通过正交性损失逐步激活细节。", "result": "AutoOpti3DGS生成更稀疏的场景表示，兼容内存受限硬件。", "conclusion": "AutoOpti3DGS有效控制高斯增长，保持视觉保真度，适用于资源受限环境。"}}
{"id": "2506.23940", "pdf": "https://arxiv.org/pdf/2506.23940", "abs": "https://arxiv.org/abs/2506.23940", "authors": ["Yang Dai", "Jianxiang An", "Tianwei Lin", "Hongyang He", "Hongzhe Huang", "Wenqiao Zhang", "Zheqi Lv", "Siliang Tang", "Yueting Zhuang"], "title": "Graft: Integrating the Domain Knowledge via Efficient Parameter Synergy for MLLMs", "categories": ["cs.CL"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) have achieved success across various\ndomains. However, their applicability tends to degrade when confronted with\ndifferent types of data inputs, especially for MLLMs that have been fine-tuned\nfor specific tasks. Despite its importance, the study of knowledge sharing\namong domain-specific MLLMs--such as those trained for mathematics or\ncode--remains largely underexplored. To address the fragmentation of knowledge\nacross domain-specialized MLLMs, we propose a unified parameter integration\nframework that enables modular composition of expert capabilities. Our method\nis grounded in a novel Compatibility-Aware Parameter Splicing (CAPS) strategy,\nwhich leverages both local functional attribution and global\ninformation-theoretic signals to guide selective parameter fusion. By extending\nthis mechanism to the low-rank adaptation layer granularity, we ensure\nefficient integration with minimal inference overhead. Furthermore, we\nintroduce a domain compatibility scoring mechanism that quantifies inter-expert\nalignment at the activation level and correlates with downstream task utility.\nThis principled fusion protocol allows the final model to synergize\nheterogeneous expertise while preserving structural modularity. Extensive\nevaluations across diverse multimodal benchmarks validate the effectiveness of\nour framework, offering a scalable path toward compositional, domain-adaptive\nMLLMs.", "AI": {"tldr": "提出了一种统一参数集成框架，通过兼容性感知参数拼接（CAPS）策略，实现领域专用MLLMs的知识共享与模块化组合。", "motivation": "解决领域专用MLLMs知识碎片化问题，探索跨领域知识共享的潜力。", "method": "采用CAPS策略，结合局部功能归因和全局信息理论信号，实现选择性参数融合，并引入领域兼容性评分机制。", "result": "在多样化多模态基准测试中验证了框架的有效性，为领域自适应MLLMs提供了可扩展路径。", "conclusion": "该框架成功实现了异构专业知识的协同，同时保持了结构模块化，为MLLMs的领域适应性和组合性提供了新方向。"}}
{"id": "2506.23044", "pdf": "https://arxiv.org/pdf/2506.23044", "abs": "https://arxiv.org/abs/2506.23044", "authors": ["Guo-Hua Wang", "Shanshan Zhao", "Xinjie Zhang", "Liangfu Cao", "Pengxin Zhan", "Lunhao Duan", "Shiyin Lu", "Minghao Fu", "Xiaohao Chen", "Jianshan Zhao", "Yang Li", "Qing-Guo Chen"], "title": "Ovis-U1 Technical Report", "categories": ["cs.CV", "cs.AI"], "comment": "A unified model for multimodal understanding, text-to-image\n  generation, and image editing. GitHub: https://github.com/AIDC-AI/Ovis-U1", "summary": "In this report, we introduce Ovis-U1, a 3-billion-parameter unified model\nthat integrates multimodal understanding, text-to-image generation, and image\nediting capabilities. Building on the foundation of the Ovis series, Ovis-U1\nincorporates a diffusion-based visual decoder paired with a bidirectional token\nrefiner, enabling image generation tasks comparable to leading models like\nGPT-4o. Unlike some previous models that use a frozen MLLM for generation\ntasks, Ovis-U1 utilizes a new unified training approach starting from a\nlanguage model. Compared to training solely on understanding or generation\ntasks, unified training yields better performance, demonstrating the\nenhancement achieved by integrating these two tasks. Ovis-U1 achieves a score\nof 69.6 on the OpenCompass Multi-modal Academic Benchmark, surpassing recent\nstate-of-the-art models such as Ristretto-3B and SAIL-VL-1.5-2B. In\ntext-to-image generation, it excels with scores of 83.72 and 0.89 on the\nDPG-Bench and GenEval benchmarks, respectively. For image editing, it achieves\n4.00 and 6.42 on the ImgEdit-Bench and GEdit-Bench-EN, respectively. As the\ninitial version of the Ovis unified model series, Ovis-U1 pushes the boundaries\nof multimodal understanding, generation, and editing.", "AI": {"tldr": "Ovis-U1是一个30亿参数的多模态统一模型，集成了理解、生成和编辑能力，性能优于现有模型。", "motivation": "旨在通过统一训练方法提升多模态任务（理解、生成和编辑）的性能。", "method": "采用扩散式视觉解码器和双向令牌精炼器，从语言模型开始进行统一训练。", "result": "在OpenCompass、DPG-Bench等基准测试中表现优异，超越Ristretto-3B等模型。", "conclusion": "Ovis-U1在多模态任务中表现出色，为后续统一模型系列奠定了基础。"}}
{"id": "2506.23951", "pdf": "https://arxiv.org/pdf/2506.23951", "abs": "https://arxiv.org/abs/2506.23951", "authors": ["Mathis Le Bail", "Jérémie Dentan", "Davide Buscaldi", "Sonia Vanier"], "title": "Unveiling Decision-Making in LLMs for Text Classification : Extraction of influential and interpretable concepts with Sparse Autoencoders", "categories": ["cs.CL"], "comment": null, "summary": "Sparse Autoencoders (SAEs) have been successfully used to probe Large\nLanguage Models (LLMs) and extract interpretable concepts from their internal\nrepresentations. These concepts are linear combinations of neuron activations\nthat correspond to human-interpretable features. In this paper, we investigate\nthe effectiveness of SAE-based explainability approaches for sentence\nclassification, a domain where such methods have not been extensively explored.\nWe present a novel SAE-based architecture tailored for text classification,\nleveraging a specialized classifier head and incorporating an activation rate\nsparsity loss. We benchmark this architecture against established methods such\nas ConceptShap, Independent Component Analysis, and other SAE-based concept\nextraction techniques. Our evaluation covers two classification benchmarks and\nfour fine-tuned LLMs from the Pythia family. We further enrich our analysis\nwith two novel metrics for measuring the precision of concept-based\nexplanations, using an external sentence encoder. Our empirical results show\nthat our architecture improves both the causality and interpretability of the\nextracted features.", "AI": {"tldr": "本文研究了稀疏自编码器（SAE）在句子分类中的解释性效果，提出了一种新的SAE架构，并通过实验验证了其优越性。", "motivation": "探索SAE在句子分类任务中的解释性效果，填补该领域的研究空白。", "method": "提出了一种新的SAE架构，结合专用分类器头和激活率稀疏损失，并与现有方法进行对比。", "result": "实验表明，该架构提高了特征的因果性和可解释性。", "conclusion": "SAE在句子分类中具有潜力，新架构显著提升了解释性效果。"}}
{"id": "2506.23061", "pdf": "https://arxiv.org/pdf/2506.23061", "abs": "https://arxiv.org/abs/2506.23061", "authors": ["Jiazhen Liu", "Yuchuan Deng", "Long Chen"], "title": "Empowering Small VLMs to Think with Dynamic Memorization and Exploration", "categories": ["cs.CV"], "comment": null, "summary": "Empowering Small-scale Vision-Language Models (SVLMs) with reliable thinking\ncapabilities remains fundamentally challenging due to their limited parameter\ncapacity and weak instruction-following abilities. Existing training paradigms,\nincluding Supervised Fine-Tuning (SFT) and Reinforcement Learning with\nVerifiable Reward (RLVR), impose substantial demands on the base VLM, exceeding\nthe capabilities of SVLMs. Consequently, directly applying these paradigms to\nSVLMs often suffers from severe pseudo thinking traces and advantage collapse,\nultimately undermining both thinking reliability and task performance. A\nnatural solution is to combine SFT and RLVR, leveraging their complementarity\nto reduce the dependence on model capacity. However, the widely adopted\ntwo-stage training paradigm still performs poorly on SVLMs, as their tendency\ntoward sub-optimal convergence hinders the trade-off and limits the benefits of\nthe combination. To address this, we propose DyME, a novel training paradigm\nthat Dynamically selects between Memorization (via SFT) and Exploration (via\nRLVR) modes at each optimization step, ensuring that every update contributes\nto the trade-off. Extensive experiments across diverse domains demonstrate that\nDyME consistently achieves this balance, and thus delivers substantial\nperformance improvements. These results establish DyME as a practical and\neffective solution for empowering SVLMs with reliable thinking capabilities.\nGitHub: https://github.com/HKUST-LongGroup/DyME", "AI": {"tldr": "DyME是一种动态选择记忆（SFT）和探索（RLVR）模式的训练范式，显著提升了小型视觉语言模型的可靠思维能力。", "motivation": "小型视觉语言模型（SVLMs）由于参数容量有限和指令跟随能力弱，难以实现可靠的思维能力。现有训练范式（如SFT和RLVR）对模型要求过高，直接应用会导致伪思维痕迹和优势崩溃。", "method": "提出DyME，动态选择记忆（SFT）和探索（RLVR）模式，确保每次优化步骤都贡献于平衡。", "result": "实验表明，DyME能够有效平衡记忆与探索，显著提升SVLMs的性能。", "conclusion": "DyME是一种实用且有效的解决方案，能够增强SVLMs的可靠思维能力。"}}
{"id": "2506.23979", "pdf": "https://arxiv.org/pdf/2506.23979", "abs": "https://arxiv.org/abs/2506.23979", "authors": ["Renren Jin", "Tianhao Shen", "Xinwei Wu", "Dan Shi", "Haoran Sun", "Wuwei Huang", "Quandong Wang", "Wei Liu", "Jian Luan", "Bin Wang", "Deyi Xiong"], "title": "TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference Data Generation", "categories": ["cs.CL"], "comment": "33 pages, 15 tables, 11 figures", "summary": "Conducting supervised fine-tuning and preference fine-tuning on large\nlanguage models (LLMs) requires high-quality datasets to improve their ability\nto follow instructions and align with human preferences and values. However,\nconstructing such datasets is resource-intensive, and most available datasets\nfor supervised and preference fine-tuning are in English. To address these\nchallenges, we propose the \\underline{\\textbf{Ta}}xonomy-Guided\n\\underline{\\textbf{P}}reference Data Generation (TaP) framework, which\nfacilitates automated and scalable construction of preference datasets across\nvarious languages. TaP is grounded in a structured taxonomy that allows\nfine-grained control over dataset composition, thereby ensuring both diversity\nand comprehensive coverage. We employ TaP-generated datasets to perform\nsupervised and preference fine-tuning on various LLMs. Experimental results\ndemonstrate that LLMs trained on TaP-generated datasets outperform those\ntrained on existing open-source datasets. Remarkably, LLMs trained on\nTaP-generated datasets surpass the performance of those trained on an\nopen-source dataset that is 180 times larger.", "AI": {"tldr": "提出TaP框架，通过结构化分类法自动生成多语言偏好数据集，提升LLM性能。", "motivation": "解决高质量多语言数据集构建资源密集且稀缺的问题。", "method": "基于分类法的TaP框架，自动化生成多样化偏好数据集。", "result": "TaP生成的数据集训练效果优于现有开源数据集，甚至优于规模大180倍的数据集。", "conclusion": "TaP框架高效且可扩展，显著提升LLM性能。"}}
{"id": "2506.23066", "pdf": "https://arxiv.org/pdf/2506.23066", "abs": "https://arxiv.org/abs/2506.23066", "authors": ["Jiale Meng", "Yiming Li", "Zheming Lu", "Zewei He", "Hao Luo", "Tianwei Zhang"], "title": "CoreMark: Toward Robust and Universal Text Watermarking Technique", "categories": ["cs.CV", "cs.CR", "cs.MM"], "comment": "10 pages, 16 figures", "summary": "Text watermarking schemes have gained considerable attention in recent years,\nyet still face critical challenges in achieving simultaneous robustness,\ngeneralizability, and imperceptibility. This paper introduces a new embedding\nparadigm,termed CORE, which comprises several consecutively aligned black pixel\nsegments. Its key innovation lies in its inherent noise resistance during\ntransmission and broad applicability across languages and fonts. Based on the\nCORE, we present a text watermarking framework named CoreMark. Specifically,\nCoreMark first dynamically extracts COREs from characters. Then, the characters\nwith stronger robustness are selected according to the lengths of COREs. By\nmodifying the thickness of the CORE, the hidden data is embedded into the\nselected characters without causing significant visual distortions. Moreover, a\ngeneral plug-and-play embedding strength modulator is proposed, which can\nadaptively enhance the robustness for small font sizes by adjusting the\nembedding strength according to the font size. Experimental evaluation\nindicates that CoreMark demonstrates outstanding generalizability across\nmultiple languages and fonts. Compared to existing methods, CoreMark achieves\nsignificant improvements in resisting screenshot, print-scan, and print camera\nattacks, while maintaining satisfactory imperceptibility.", "AI": {"tldr": "本文提出了一种名为CORE的新嵌入范式，并基于此构建了文本水印框架CoreMark，实现了同时具备鲁棒性、通用性和不可感知性的文本水印嵌入。", "motivation": "现有文本水印方案在同时满足鲁棒性、通用性和不可感知性方面存在挑战。", "method": "CoreMark通过动态提取字符中的CORE（连续对齐的黑色像素段），选择鲁棒性强的字符，并通过调整CORE厚度嵌入数据。此外，提出了一种通用的嵌入强度调节器，根据字体大小自适应增强鲁棒性。", "result": "实验表明，CoreMark在多种语言和字体中表现出色，显著优于现有方法，能够抵抗截图、打印扫描和打印相机攻击，同时保持不可感知性。", "conclusion": "CoreMark为文本水印提供了一种高效且通用的解决方案，适用于多种应用场景。"}}
{"id": "2506.22512", "pdf": "https://arxiv.org/pdf/2506.22512", "abs": "https://arxiv.org/abs/2506.22512", "authors": ["Pratheeksha Nair", "Gabriel Lefebvre", "Sophia Garrel", "Maryam Molamohammadi", "Reihaneh Rabbany"], "title": "Ask before you Build: Rethinking AI-for-Good in Human Trafficking Interventions", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "AI for good initiatives often rely on the assumption that technical\ninterventions can resolve complex social problems. In the context of human\ntrafficking (HT), such techno-solutionism risks oversimplifying exploitation,\nreinforcing power imbalances and causing harm to the very communities AI claims\nto support. In this paper, we introduce the Radical Questioning (RQ) framework\nas a five step, pre-project ethical assessment tool to critically evaluate\nwhether AI should be built at all, especially in domains involving marginalized\npopulations and entrenched systemic injustice. RQ does not replace principles\nbased ethics but precedes it, offering an upstream, deliberative space to\nconfront assumptions, map power, and consider harms before design. Using a case\nstudy in AI for HT, we demonstrate how RQ reveals overlooked sociocultural\ncomplexities and guides us away from surveillance based interventions toward\nsurvivor empowerment tools. While developed in the context of HT, RQ's five\nstep structure can generalize to other domains, though the specific questions\nmust be contextual. This paper situates RQ within a broader AI ethics\nphilosophy that challenges instrumentalist norms and centers relational,\nreflexive responsibility.", "AI": {"tldr": "论文提出“激进质疑”（RQ）框架，作为AI项目前的伦理评估工具，旨在避免技术解决方案主义对社会问题的简化处理，特别是在涉及边缘群体和系统性不公的领域。", "motivation": "AI技术常被用于解决复杂社会问题（如人口贩卖），但技术解决方案主义可能简化问题、加剧权力失衡，甚至对目标群体造成伤害。因此，需要一种工具在项目启动前评估AI是否应被开发。", "method": "提出五步RQ框架，通过批判性评估（如假设检验、权力映射、潜在危害分析）决定是否开发AI，并以人口贩卖为例展示其应用。", "result": "RQ框架揭示了社会文化复杂性，并引导从监控干预转向幸存者赋权工具。其五步结构可推广至其他领域，但需具体问题具体分析。", "conclusion": "RQ框架挑战了工具主义规范，强调关系性和反思性责任，为AI伦理提供了一种上游评估方法。"}}
{"id": "2506.23990", "pdf": "https://arxiv.org/pdf/2506.23990", "abs": "https://arxiv.org/abs/2506.23990", "authors": ["Dustin Wright"], "title": "Machine Understanding of Scientific Language", "categories": ["cs.CL", "cs.LG"], "comment": "PhD Thesis, 210 pages", "summary": "Scientific information expresses human understanding of nature. This\nknowledge is largely disseminated in different forms of text, including\nscientific papers, news articles, and discourse among people on social media.\nWhile important for accelerating our pursuit of knowledge, not all scientific\ntext is faithful to the underlying science. As the volume of this text has\nburgeoned online in recent years, it has become a problem of societal\nimportance to be able to identify the faithfulness of a given piece of\nscientific text automatically. This thesis is concerned with the cultivation of\ndatasets, methods, and tools for machine understanding of scientific language,\nin order to analyze and understand science communication at scale. To arrive at\nthis, I present several contributions in three areas of natural language\nprocessing and machine learning: automatic fact checking, learning with limited\ndata, and scientific text processing. These contributions include new methods\nand resources for identifying check-worthy claims, adversarial claim\ngeneration, multi-source domain adaptation, learning from crowd-sourced labels,\ncite-worthiness detection, zero-shot scientific fact checking, detecting\nexaggerated scientific claims, and modeling degrees of information change in\nscience communication. Critically, I demonstrate how the research outputs of\nthis thesis are useful for effectively learning from limited amounts of\nscientific text in order to identify misinformative scientific statements and\ngenerate new insights into the science communication process", "AI": {"tldr": "该论文探讨了如何通过自然语言处理和机器学习方法自动识别科学文本的忠实性，并提出了数据集、方法和工具。", "motivation": "科学文本的忠实性对社会至关重要，但现有方法难以应对海量文本。", "method": "提出了自动事实核查、有限数据学习和科学文本处理的新方法。", "result": "开发了识别可核查声明、对抗性声明生成、多源域适应等技术。", "conclusion": "研究成果有助于从有限科学文本中识别错误信息，并提升科学传播的理解。"}}
{"id": "2506.23072", "pdf": "https://arxiv.org/pdf/2506.23072", "abs": "https://arxiv.org/abs/2506.23072", "authors": ["Jing Gao"], "title": "Unsupervised 3D Braided Hair Reconstruction from a Single-View Image", "categories": ["cs.CV"], "comment": "6 pages, 3 figures, accepted to the 2025 International Conference on\n  Machine Vision Applications (MVA 2025)", "summary": "Reconstructing 3D braided hairstyles from single-view images remains a\nchallenging task due to the intricate interwoven structure and complex\ntopologies of braids. Existing strand-based hair reconstruction methods\ntypically focus on loose hairstyles and often struggle to capture the\nfine-grained geometry of braided hair. In this paper, we propose a novel\nunsupervised pipeline for efficiently reconstructing 3D braided hair from\nsingle-view RGB images. Leveraging a synthetic braid model inspired by braid\ntheory, our approach effectively captures the complex intertwined structures of\nbraids. Extensive experiments demonstrate that our method outperforms\nstate-of-the-art approaches, providing superior accuracy, realism, and\nefficiency in reconstructing 3D braided hairstyles, supporting expressive\nhairstyle modeling in digital humans.", "AI": {"tldr": "提出了一种无监督的3D编织发型重建方法，通过单视角RGB图像高效捕捉编织头发的复杂结构。", "motivation": "现有方法主要针对松散发型，难以捕捉编织头发的精细几何结构，因此需要一种新方法来解决这一挑战。", "method": "利用受编织理论启发的合成编织模型，设计了一种无监督的流程，从单视角图像重建3D编织发型。", "result": "实验表明，该方法在准确性、真实性和效率上优于现有技术，支持数字人类中富有表现力的发型建模。", "conclusion": "该方法为3D编织发型重建提供了高效且高质量的解决方案，推动了数字人类发型建模的发展。"}}
{"id": "2506.22515", "pdf": "https://arxiv.org/pdf/2506.22515", "abs": "https://arxiv.org/abs/2506.22515", "authors": ["Antony Dalmiere", "Guillaume Auriol", "Vincent Nicomette", "Pascal Marchand"], "title": "In-context learning for the classification of manipulation techniques in phishing emails", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Traditional phishing detection often overlooks psychological manipulation.\nThis study investigates using Large Language Model (LLM) In-Context Learning\n(ICL) for fine-grained classification of phishing emails based on a taxonomy of\n40 manipulation techniques. Using few-shot examples with GPT-4o-mini on\nreal-world French phishing emails (SignalSpam), we evaluated performance\nagainst a human-annotated test set (100 emails). The approach effectively\nidentifies prevalent techniques (e.g., Baiting, Curiosity Appeal, Request For\nMinor Favor) with a promising accuracy of 0.76. This work demonstrates ICL's\npotential for nuanced phishing analysis and provides insights into attacker\nstrategies.", "AI": {"tldr": "研究利用LLM的上下文学习（ICL）对钓鱼邮件进行细粒度分类，基于40种操纵技术分类法，在真实法语钓鱼邮件数据集上测试，准确率达0.76。", "motivation": "传统钓鱼检测常忽视心理操纵，本研究旨在填补这一空白，探索LLM在识别复杂操纵技术中的潜力。", "method": "采用GPT-4o-mini的少样本学习（ICL）方法，对SignalSpam数据集中的法语钓鱼邮件进行分类，测试集为100封人工标注邮件。", "result": "模型能有效识别常见操纵技术（如诱饵、好奇心吸引、小请求），准确率为0.76。", "conclusion": "研究表明ICL在钓鱼邮件分析中具有潜力，并为攻击者策略提供了新见解。"}}
{"id": "2506.23998", "pdf": "https://arxiv.org/pdf/2506.23998", "abs": "https://arxiv.org/abs/2506.23998", "authors": ["Seungjun Yi", "Joakim Nguyen", "Huimin Xu", "Terence Lim", "Andrew Well", "Mia Markey", "Ying Ding"], "title": "Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via Multi-Agent Large Language Models with Reinforcement Learning", "categories": ["cs.CL"], "comment": "Presented at ACL 2025 SRW", "summary": "Congenital heart disease (CHD) presents complex, lifelong challenges often\nunderrepresented in traditional clinical metrics. While unstructured narratives\noffer rich insights into patient and caregiver experiences, manual thematic\nanalysis (TA) remains labor-intensive and unscalable. We propose a fully\nautomated large language model (LLM) pipeline that performs end-to-end TA on\nclinical narratives, which eliminates the need for manual coding or full\ntranscript review. Our system employs a novel multi-agent framework, where\nspecialized LLM agents assume roles to enhance theme quality and alignment with\nhuman analysis. To further improve thematic relevance, we optionally integrate\nreinforcement learning from human feedback (RLHF). This supports scalable,\npatient-centered analysis of large qualitative datasets and allows LLMs to be\nfine-tuned for specific clinical contexts.", "AI": {"tldr": "提出了一种基于大型语言模型（LLM）的自动化主题分析（TA）流程，用于分析先天性心脏病（CHD）患者和护理者的临床叙事，替代传统手动方法。", "motivation": "传统的手动主题分析方法效率低且难以扩展，无法满足大规模定性数据分析的需求。", "method": "采用多智能体框架的LLM管道，结合强化学习（RLHF）优化主题相关性。", "result": "实现了高效、可扩展的患者中心化分析，并能针对特定临床场景微调LLM。", "conclusion": "自动化TA流程为大规模临床叙事分析提供了可行方案，提升了患者体验研究的效率。"}}
{"id": "2506.23074", "pdf": "https://arxiv.org/pdf/2506.23074", "abs": "https://arxiv.org/abs/2506.23074", "authors": ["Yu Zheng", "Boyang Gong", "Fanye Kong", "Yueqi Duan", "Bingyao Yu", "Wenzhao Zheng", "Lei Chen", "Jiwen Lu", "Jie Zhou"], "title": "Learning Counterfactually Decoupled Attention for Open-World Model Attribution", "categories": ["cs.CV", "cs.CR", "cs.LG"], "comment": "Accepted by ICCV 2025. Code: \\url{https://github.com/yzheng97/CDAL}", "summary": "In this paper, we propose a Counterfactually Decoupled Attention Learning\n(CDAL) method for open-world model attribution. Existing methods rely on\nhandcrafted design of region partitioning or feature space, which could be\nconfounded by the spurious statistical correlations and struggle with novel\nattacks in open-world scenarios. To address this, CDAL explicitly models the\ncausal relationships between the attentional visual traces and source model\nattribution, and counterfactually decouples the discriminative model-specific\nartifacts from confounding source biases for comparison. In this way, the\nresulting causal effect provides a quantification on the quality of learned\nattention maps, thus encouraging the network to capture essential generation\npatterns that generalize to unseen source models by maximizing the effect.\nExtensive experiments on existing open-world model attribution benchmarks show\nthat with minimal computational overhead, our method consistently improves\nstate-of-the-art models by large margins, particularly for unseen novel\nattacks. Source code: https://github.com/yzheng97/CDAL.", "AI": {"tldr": "提出了一种反事实解耦注意力学习方法（CDAL），用于开放世界模型归因，通过建模注意力视觉痕迹与源模型归因的因果关系，解耦混淆源偏差，提升对未见攻击的泛化能力。", "motivation": "现有方法依赖手工设计的区域划分或特征空间，易受虚假统计相关性干扰，难以应对开放世界中的新型攻击。", "method": "CDAL显式建模注意力视觉痕迹与源模型归因的因果关系，通过反事实解耦分离模型特定伪影与混淆源偏差，最大化因果效应以优化注意力图质量。", "result": "在开放世界模型归因基准测试中，CDAL以最小计算开销显著提升现有最优模型性能，尤其对未见攻击表现突出。", "conclusion": "CDAL通过因果建模和解耦方法，有效提升了模型归因的泛化能力和鲁棒性。"}}
{"id": "2506.24006", "pdf": "https://arxiv.org/pdf/2506.24006", "abs": "https://arxiv.org/abs/2506.24006", "authors": ["Anselm R. Strohmaier", "Wim Van Dooren", "Kathrin Seßler", "Brian Greer", "Lieven Verschaffel"], "title": "Large Language Models Don't Make Sense of Word Problems. A Scoping Review from a Mathematics Education Perspective", "categories": ["cs.CL", "math.HO"], "comment": null, "summary": "The progress of Large Language Models (LLMs) like ChatGPT raises the question\nof how they can be integrated into education. One hope is that they can support\nmathematics learning, including word-problem solving. Since LLMs can handle\ntextual input with ease, they appear well-suited for solving mathematical word\nproblems. Yet their real competence, whether they can make sense of the\nreal-world context, and the implications for classrooms remain unclear. We\nconducted a scoping review from a mathematics-education perspective, including\nthree parts: a technical overview, a systematic review of word problems used in\nresearch, and a state-of-the-art empirical evaluation of LLMs on mathematical\nword problems. First, in the technical overview, we contrast the\nconceptualization of word problems and their solution processes between LLMs\nand students. In computer-science research this is typically labeled\nmathematical reasoning, a term that does not align with usage in mathematics\neducation. Second, our literature review of 213 studies shows that the most\npopular word-problem corpora are dominated by s-problems, which do not require\na consideration of realities of their real-world context. Finally, our\nevaluation of GPT-3.5-turbo, GPT-4o-mini, GPT-4.1, and o3 on 287 word problems\nshows that most recent LLMs solve these s-problems with near-perfect accuracy,\nincluding a perfect score on 20 problems from PISA. LLMs still showed\nweaknesses in tackling problems where the real-world context is problematic or\nnon-sensical. In sum, we argue based on all three aspects that LLMs have\nmastered a superficial solution process but do not make sense of word problems,\nwhich potentially limits their value as instructional tools in mathematics\nclassrooms.", "AI": {"tldr": "论文探讨了大型语言模型（LLMs）在数学教育中的应用潜力，发现其在解决数学应用题时表现优异，但缺乏对现实背景的理解，限制了其教学价值。", "motivation": "研究LLMs如何支持数学学习，尤其是解决应用题的能力及其对课堂的影响。", "method": "通过技术概述、文献综述和实证评估三部分，分析LLMs在数学应用题中的表现。", "result": "LLMs在解决不涉及现实背景的问题时表现完美，但在处理有问题的现实背景时表现不佳。", "conclusion": "LLMs掌握了表面的解题过程，但未能真正理解应用题的现实背景，限制了其作为数学教学工具的潜力。"}}
{"id": "2506.23077", "pdf": "https://arxiv.org/pdf/2506.23077", "abs": "https://arxiv.org/abs/2506.23077", "authors": ["Suofei Zhang", "Xinxin Wang", "Xiaofu Wu", "Quan Zhou", "Haifeng Hu"], "title": "Dynamic Contrastive Learning for Hierarchical Retrieval: A Case Study of Distance-Aware Cross-View Geo-Localization", "categories": ["cs.CV"], "comment": null, "summary": "Existing deep learning-based cross-view geo-localization methods primarily\nfocus on improving the accuracy of cross-domain image matching, rather than\nenabling models to comprehensively capture contextual information around the\ntarget and minimize the cost of localization errors. To support systematic\nresearch into this Distance-Aware Cross-View Geo-Localization (DACVGL) problem,\nwe construct Distance-Aware Campus (DA-Campus), the first benchmark that pairs\nmulti-view imagery with precise distance annotations across three spatial\nresolutions. Based on DA-Campus, we formulate DACVGL as a hierarchical\nretrieval problem across different domains. Our study further reveals that, due\nto the inherent complexity of spatial relationships among buildings, this\nproblem can only be addressed via a contrastive learning paradigm, rather than\nconventional metric learning. To tackle this challenge, we propose Dynamic\nContrastive Learning (DyCL), a novel framework that progressively aligns\nfeature representations according to hierarchical spatial margins. Extensive\nexperiments demonstrate that DyCL is highly complementary to existing\nmulti-scale metric learning methods and yields substantial improvements in both\nhierarchical retrieval performance and overall cross-view geo-localization\naccuracy. Our code and benchmark are publicly available at\nhttps://github.com/anocodetest1/DyCL.", "AI": {"tldr": "论文提出了一种动态对比学习框架（DyCL），用于解决距离感知的跨视角地理定位问题（DACVGL），并通过构建DA-Campus基准数据集验证其有效性。", "motivation": "现有方法主要关注跨域图像匹配的准确性，而忽略了模型对目标周围上下文信息的全面捕捉和定位误差的最小化。", "method": "提出DyCL框架，通过分层空间边界的动态对比学习逐步对齐特征表示。", "result": "实验表明DyCL与现有多尺度度量学习方法高度互补，显著提升了分层检索性能和跨视角地理定位准确性。", "conclusion": "DyCL为解决DACVGL问题提供了有效方案，并通过公开代码和基准数据集推动进一步研究。"}}
{"id": "2506.24016", "pdf": "https://arxiv.org/pdf/2506.24016", "abs": "https://arxiv.org/abs/2506.24016", "authors": ["Hyunjong Kim", "Sangyeop Kim", "Jongheon Jeong", "Yeongjae Cho", "Sungzoon Cho"], "title": "EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "Accepted at ACL 2025 Findings", "summary": "Recent advances in large language models and vision-language models have led\nto growing interest in explainable evaluation metrics for image captioning.\nHowever, these metrics generate explanations without standardized criteria, and\nthe overall quality of the generated explanations remains unverified. In this\npaper, we propose EXPERT, a reference-free evaluation metric that provides\nstructured explanations based on three fundamental criteria: fluency,\nrelevance, and descriptiveness. By constructing large-scale datasets of\nhigh-quality structured explanations, we develop a two-stage evaluation\ntemplate to effectively supervise a vision-language model for both scoring and\nexplanation generation. EXPERT achieves state-of-the-art results on benchmark\ndatasets while providing significantly higher-quality explanations than\nexisting metrics, as validated through comprehensive human evaluation. Our code\nand datasets are available at https://github.com/hjkim811/EXPERT.", "AI": {"tldr": "EXPERT是一种新的图像字幕评估指标，通过结构化解释（流畅性、相关性和描述性）提供无参考评估，优于现有方法。", "motivation": "现有图像字幕评估指标的解释缺乏标准化和验证，需要更高质量的解释性评估方法。", "method": "提出EXPERT，基于三项标准生成结构化解释，并通过两阶段评估模板监督视觉语言模型。", "result": "在基准数据集上达到最优性能，且生成解释的质量显著高于现有指标。", "conclusion": "EXPERT为图像字幕评估提供了高质量的解释性指标，代码和数据集已开源。"}}
{"id": "2506.23086", "pdf": "https://arxiv.org/pdf/2506.23086", "abs": "https://arxiv.org/abs/2506.23086", "authors": ["Jian Shi", "Tianqi You", "Pingping Zhang", "Hongli Zhang", "Rui Xu", "Haojie Li"], "title": "Frequency-enhanced Multi-granularity Context Network for Efficient Vertebrae Segmentation", "categories": ["cs.CV"], "comment": "Accepted by MICCAI2025. More modifications my be performed", "summary": "Automated and accurate segmentation of individual vertebra in 3D CT and MRI\nimages is essential for various clinical applications. Due to the limitations\nof current imaging techniques and the complexity of spinal structures, existing\nmethods still struggle with reducing the impact of image blurring and\ndistinguishing similar vertebrae. To alleviate these issues, we introduce a\nFrequency-enhanced Multi-granularity Context Network (FMC-Net) to improve the\naccuracy of vertebrae segmentation. Specifically, we first apply wavelet\ntransform for lossless downsampling to reduce the feature distortion in blurred\nimages. The decomposed high and low-frequency components are then processed\nseparately. For the high-frequency components, we apply a High-frequency\nFeature Refinement (HFR) to amplify the prominence of key features and filter\nout noises, restoring fine-grained details in blurred images. For the\nlow-frequency components, we use a Multi-granularity State Space Model (MG-SSM)\nto aggregate feature representations with different receptive fields,\nextracting spatially-varying contexts while capturing long-range dependencies\nwith linear complexity. The utilization of multi-granularity contexts is\nessential for distinguishing similar vertebrae and improving segmentation\naccuracy. Extensive experiments demonstrate that our method outperforms\nstate-of-the-art approaches on both CT and MRI vertebrae segmentation datasets.\nThe source code is publicly available at https://github.com/anaanaa/FMCNet.", "AI": {"tldr": "提出了一种频率增强的多粒度上下文网络（FMC-Net），通过小波变换和无损下采样改进模糊图像中的脊椎分割精度。", "motivation": "当前成像技术和脊柱结构的复杂性导致现有方法难以减少图像模糊的影响并区分相似脊椎。", "method": "使用小波变换进行无损下采样，分别处理高频和低频成分。高频部分通过HFR放大关键特征并过滤噪声；低频部分通过MG-SSM聚合不同感受野的特征，提取空间变化上下文。", "result": "在CT和MRI脊椎分割数据集上优于现有方法。", "conclusion": "FMC-Net能有效提升脊椎分割精度，代码已开源。"}}
{"id": "2506.22520", "pdf": "https://arxiv.org/pdf/2506.22520", "abs": "https://arxiv.org/abs/2506.22520", "authors": ["Mustafa Demir", "Jacob Miratsky", "Jonathan Nguyen", "Chun Kit Chan", "Punya Mishra", "Abhishek Singharoy"], "title": "Exploring Artificial Intelligence Tutor Teammate Adaptability to Harness Discovery Curiosity and Promote Learning in the Context of Interactive Molecular Dynamics", "categories": ["cs.HC", "cs.AI", "cs.CE", "cs.CY"], "comment": null, "summary": "This study examines the impact of an Artificial Intelligence tutor teammate\n(AI) on student curiosity-driven engagement and learning effectiveness during\nInteractive Molecular Dynamics (IMD) tasks on the Visual Molecular Dynamics\nplatform. It explores the role of the AI's curiosity-triggering and response\nbehaviors in stimulating and sustaining student curiosity, affecting the\nfrequency and complexity of student-initiated questions. The study further\nassesses how AI interventions shape student engagement, foster discovery\ncuriosity, and enhance team performance within the IMD learning environment.\nUsing a Wizard-of-Oz paradigm, a human experimenter dynamically adjusts the AI\ntutor teammate's behavior through a large language model. By employing a\nmixed-methods exploratory design, a total of 11 high school students\nparticipated in four IMD tasks that involved molecular visualization and\ncalculations, which increased in complexity over a 60-minute period. Team\nperformance was evaluated through real-time observation and recordings, whereas\nteam communication was measured by question complexity and AI's\ncuriosity-triggering and response behaviors. Cross Recurrence Quantification\nAnalysis (CRQA) metrics reflected structural alignment in coordination and were\nlinked to communication behaviors. High-performing teams exhibited superior\ntask completion, deeper understanding, and increased engagement. Advanced\nquestions were associated with AI curiosity-triggering, indicating heightened\nengagement and cognitive complexity. CRQA metrics highlighted dynamic\nsynchronization in student-AI interactions, emphasizing structured yet adaptive\nengagement to promote curiosity. These proof-of-concept findings suggest that\nthe AI's dual role as a teammate and educator indicates its capacity to provide\nadaptive feedback, sustaining engagement and epistemic curiosity.", "AI": {"tldr": "研究探讨AI导师队友如何通过激发好奇心行为提升学生在分子动力学任务中的参与度和学习效果。", "motivation": "探索AI在互动学习环境中如何通过触发和响应好奇心行为，增强学生的主动提问和团队表现。", "method": "采用混合方法设计，11名高中生参与4项任务，通过实时观察和CRQA分析团队沟通与AI行为。", "result": "高绩效团队表现出更好的任务完成度和理解深度，AI的好奇心触发行为与高级问题相关。", "conclusion": "AI作为队友和教育者的双重角色能提供适应性反馈，维持学生的参与度和认知好奇心。"}}
{"id": "2506.24068", "pdf": "https://arxiv.org/pdf/2506.24068", "abs": "https://arxiv.org/abs/2506.24068", "authors": ["Ian R. McKenzie", "Oskar J. Hollinsworth", "Tom Tseng", "Xander Davies", "Stephen Casper", "Aaron D. Tucker", "Robert Kirk", "Adam Gleave"], "title": "STACK: Adversarial Attacks on LLM Safeguard Pipelines", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Frontier AI developers are relying on layers of safeguards to protect against\ncatastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus\nmodel using one such defense pipeline, and other frontier developers including\nGoogle DeepMind and OpenAI pledge to soon deploy similar defenses. However, the\nsecurity of such pipelines is unclear, with limited prior work evaluating or\nattacking these pipelines. We address this gap by developing and red-teaming an\nopen-source defense pipeline. First, we find that a novel few-shot-prompted\ninput and output classifier outperforms state-of-the-art open-weight safeguard\nmodel ShieldGemma across three attacks and two datasets, reducing the attack\nsuccess rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second,\nwe introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on\nClearHarm in a black-box attack against the few-shot-prompted classifier\npipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33%\nASR, providing initial evidence that it is feasible to design attacks with no\naccess to the target pipeline. We conclude by suggesting specific mitigations\nthat developers could use to thwart staged attacks.", "AI": {"tldr": "论文研究了AI防御管线的安全性，开发了一种开源防御管线，并通过红队测试评估其效果。研究发现新型少样本提示分类器优于现有模型，但STACK攻击方法仍能突破防御。", "motivation": "评估前沿AI防御管线的安全性，填补现有研究空白。", "method": "开发开源防御管线，使用少样本提示分类器，并设计STACK攻击方法进行红队测试。", "result": "少样本提示分类器在ClearHarm数据集上将攻击成功率降至0%，但STACK攻击仍能达到71%的成功率。", "conclusion": "建议开发者采用特定缓解措施以应对分阶段攻击。"}}
{"id": "2506.23088", "pdf": "https://arxiv.org/pdf/2506.23088", "abs": "https://arxiv.org/abs/2506.23088", "authors": ["Yuchen Zhou", "Jiayu Tang", "Xiaoyan Xiao", "Yueyao Lin", "Linkai Liu", "Zipeng Guo", "Hao Fei", "Xiaobo Xia", "Chao Gou"], "title": "Where, What, Why: Towards Explainable Driver Attention Prediction", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Modeling task-driven attention in driving is a fundamental challenge for both\nautonomous vehicles and cognitive science. Existing methods primarily predict\nwhere drivers look by generating spatial heatmaps, but fail to capture the\ncognitive motivations behind attention allocation in specific contexts, which\nlimits deeper understanding of attention mechanisms. To bridge this gap, we\nintroduce Explainable Driver Attention Prediction, a novel task paradigm that\njointly predicts spatial attention regions (where), parses attended semantics\n(what), and provides cognitive reasoning for attention allocation (why). To\nsupport this, we present W3DA, the first large-scale explainable driver\nattention dataset. It enriches existing benchmarks with detailed semantic and\ncausal annotations across diverse driving scenarios, including normal\nconditions, safety-critical situations, and traffic accidents. We further\npropose LLada, a Large Language model-driven framework for driver attention\nprediction, which unifies pixel modeling, semantic parsing, and cognitive\nreasoning within an end-to-end architecture. Extensive experiments demonstrate\nthe effectiveness of LLada, exhibiting robust generalization across datasets\nand driving conditions. This work serves as a key step toward a deeper\nunderstanding of driver attention mechanisms, with significant implications for\nautonomous driving, intelligent driver training, and human-computer\ninteraction.", "AI": {"tldr": "论文提出了一种可解释的驾驶员注意力预测任务范式（W3DA），结合空间注意力区域预测、语义解析和认知推理，并提出了基于大语言模型的框架LLada。", "motivation": "现有方法仅预测驾驶员注视的空间热图，未能捕捉特定情境下注意力分配的认知动机，限制了对其机制的深入理解。", "method": "提出W3DA数据集，包含详细语义和因果标注；设计LLada框架，统一像素建模、语义解析和认知推理。", "result": "LLada在多个数据集和驾驶条件下表现出强大的泛化能力。", "conclusion": "该研究为深入理解驾驶员注意力机制迈出关键一步，对自动驾驶等领域具有重要意义。"}}
{"id": "2506.22521", "pdf": "https://arxiv.org/pdf/2506.22521", "abs": "https://arxiv.org/abs/2506.22521", "authors": ["Kaixiang Zhao", "Lincan Li", "Kaize Ding", "Neil Zhenqiang Gong", "Yue Zhao", "Yushun Dong"], "title": "A Survey on Model Extraction Attacks and Defenses for Large Language Models", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Model extraction attacks pose significant security threats to deployed\nlanguage models, potentially compromising intellectual property and user\nprivacy. This survey provides a comprehensive taxonomy of LLM-specific\nextraction attacks and defenses, categorizing attacks into functionality\nextraction, training data extraction, and prompt-targeted attacks. We analyze\nvarious attack methodologies including API-based knowledge distillation, direct\nquerying, parameter recovery, and prompt stealing techniques that exploit\ntransformer architectures. We then examine defense mechanisms organized into\nmodel protection, data privacy protection, and prompt-targeted strategies,\nevaluating their effectiveness across different deployment scenarios. We\npropose specialized metrics for evaluating both attack effectiveness and\ndefense performance, addressing the specific challenges of generative language\nmodels. Through our analysis, we identify critical limitations in current\napproaches and propose promising research directions, including integrated\nattack methodologies and adaptive defense mechanisms that balance security with\nmodel utility. This work serves NLP researchers, ML engineers, and security\nprofessionals seeking to protect language models in production environments.", "AI": {"tldr": "本文综述了针对语言模型的提取攻击及其防御方法，分类了攻击类型并分析了攻击与防御的有效性，提出了评估指标和研究方向。", "motivation": "模型提取攻击对语言模型的安全构成威胁，可能损害知识产权和用户隐私，因此需要系统化的攻击分类和防御策略。", "method": "通过API知识蒸馏、直接查询、参数恢复和提示窃取等方法分析攻击，并评估模型保护、数据隐私保护和提示策略等防御机制。", "result": "提出了专门评估攻击和防御效果的指标，发现当前方法的局限性，并提出了集成攻击方法和自适应防御机制的研究方向。", "conclusion": "本文为NLP研究者、ML工程师和安全专家提供了保护生产环境中语言模型的参考，强调了平衡安全与模型效用的重要性。"}}
{"id": "2506.24106", "pdf": "https://arxiv.org/pdf/2506.24106", "abs": "https://arxiv.org/abs/2506.24106", "authors": ["Yanhong Li", "Ming Li", "Karen Livescu", "Jiawei Zhou"], "title": "On the Predictive Power of Representation Dispersion in Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We show that a language model's ability to predict text is tightly linked to\nthe breadth of its embedding space: models that spread their contextual\nrepresentations more widely tend to achieve lower perplexity. Concretely, we\nfind that representation dispersion - the average pairwise cosine distance\namong hidden vectors - strongly and negatively correlates with perplexity\nacross diverse model families (LLaMA, Qwen, and others) and domains (Wikipedia,\nnews, scientific abstracts). Beyond illustrating this link, we show how\ndispersion can be leveraged for a range of practical tasks without requiring\nlabeled data. First, measuring dispersion on unlabeled text allows us to\npredict downstream accuracy in new domains, offering a data-efficient tool for\nmodel selection. Next, we find that identifying layers with higher dispersion\npinpoints the best representations for retrieval-based methods such as kNN-LM,\nbypassing exhaustive layer-by-layer searches. Finally, we integrate a simple\npush-away objective into training, which increases dispersion in both\nsingle-domain and cross-domain scenarios and directly improves perplexity in\neach.", "AI": {"tldr": "语言模型的文本预测能力与其嵌入空间的广度密切相关，嵌入表示越分散，困惑度越低。", "motivation": "研究语言模型的嵌入表示分散度与困惑度之间的关系，并探索其在无标签数据下的实际应用。", "method": "通过测量隐藏向量间的平均余弦距离（表示分散度），分析其与困惑度的相关性，并设计简单的推离目标以提高分散度。", "result": "表示分散度与困惑度呈强负相关，且分散度可用于模型选择、检索方法优化及直接提升困惑度。", "conclusion": "嵌入表示的分散度是语言模型性能的重要指标，可通过优化分散度提升模型表现。"}}
{"id": "2506.23104", "pdf": "https://arxiv.org/pdf/2506.23104", "abs": "https://arxiv.org/abs/2506.23104", "authors": ["Jihun Kim", "Hoyong Kwon", "Hyeokjun Kweon", "Wooseong Jeong", "Kuk-Jin Yoon"], "title": "DC-TTA: Divide-and-Conquer Framework for Test-Time Adaptation of Interactive Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Interactive segmentation (IS) allows users to iteratively refine object\nboundaries with minimal cues, such as positive and negative clicks. While the\nSegment Anything Model (SAM) has garnered attention in the IS community for its\npromptable segmentation capabilities, it often struggles in specialized domains\nor when handling complex scenarios (e.g., camouflaged or multi-part objects).\nTo overcome these challenges, we propose DC-TTA, a novel test-time adaptation\n(TTA) framework that adapts SAM on a per-sample basis by leveraging user\ninteractions as supervision. Instead of forcing a single model to incorporate\nall user clicks at once, DC-TTA partitions the clicks into more coherent\nsubsets, each processed independently via TTA with a separated model. This\nDivide-and-Conquer strategy reduces conflicts among diverse cues and enables\nmore localized updates. Finally, we merge the adapted models to form a unified\npredictor that integrates the specialized knowledge from each subset.\nExperimental results across various benchmarks demonstrate that DC-TTA\nsignificantly outperforms SAM's zero-shot results and conventional TTA methods,\neffectively handling complex tasks such as camouflaged object segmentation with\nfewer interactions and improved accuracy.", "AI": {"tldr": "DC-TTA是一种基于测试时适应的交互式分割框架，通过分治策略优化SAM模型，显著提升了复杂场景下的分割性能。", "motivation": "解决SAM在专业领域和复杂场景（如伪装或多部分对象）中表现不佳的问题。", "method": "提出DC-TTA框架，将用户交互划分为更一致的子集，每个子集通过独立的TTA模型处理，最后合并为统一预测器。", "result": "实验表明，DC-TTA在多个基准测试中显著优于SAM的零样本结果和传统TTA方法，减少了交互次数并提高了准确性。", "conclusion": "DC-TTA通过分治策略有效解决了SAM在复杂任务中的局限性，提升了交互式分割的性能。"}}
{"id": "2506.22523", "pdf": "https://arxiv.org/pdf/2506.22523", "abs": "https://arxiv.org/abs/2506.22523", "authors": ["James Wen", "Sahil Nalawade", "Zhiwei Liang", "Catherine Bielick", "Marisa Ferrara Boston", "Alexander Chowdhury", "Adele Collin", "Luigi De Angelis", "Jacob Ellen", "Heather Frase", "Rodrigo R. Gameiro", "Juan Manuel Gutierrez", "Pooja Kadam", "Murat Keceli", "Srikanth Krishnamurthy", "Anne Kwok", "Yanan Lance Lu", "Heather Mattie", "Liam G. McCoy", "Katherine Miller", "Allison C. Morgan", "Marlene Louisa Moerig", "Trang Nguyen", "Alexander Owen-Post", "Alex D. Ruiz", "Sreekar Reddy Puchala", "Soujanya Samineni", "Takeshi Tohyama", "Varun Ullanat", "Carmine Valenza", "Camilo Velez", "Pengcheng Wang", "Anna Wuest", "Yuxiang Zhou", "Yingde Zhu", "Jason M. Johnson", "Jennifer Willcox", "Francis J. Vitiello", "Leo Anthony G. Celi", "Renato Umeton"], "title": "Red Teaming for Generative AI, Report on a Copyright-Focused Exercise Completed in an Academic Medical Center", "categories": ["cs.CY", "cs.AI"], "comment": null, "summary": "Generative AI is present in multiple industries. Dana-Farber Cancer\nInstitute, in partnership with Microsoft, has created an internal AI tool,\nGPT4DFCI. Together we hosted a red teaming event to assess whether the\nunderlying GPT models that support the tool would output copyrighted data. Our\nteams focused on reproducing content from books, news articles, scientific\narticles, and electronic health records. We found isolated instances where\nGPT4DFCI was able to identify copyrighted material and reproduce exact quotes\nfrom famous books which indicates that copyrighted material was in the training\ndata. The model was not able to reproduce content from our target news article,\nscientific article, or electronic health records. However, there were instances\nof fabrication. As a result of this event, a mitigation strategy is in\nproduction in GPT4DFCI v2.8.2, deployed on January 21, 2025. We hope this\nreport leads to similar events in which AI software tools are stress-tested to\nassess the perimeter of their legal and ethical usage.", "AI": {"tldr": "Dana-Farber与微软合作开发的GPT4DFCI工具通过红队测试发现能复现部分受版权保护的书籍内容，但未复现新闻、科学文章或电子健康记录。测试后已部署缓解策略。", "motivation": "评估GPT4DFCI工具是否输出受版权保护的数据，以确定其法律和伦理边界。", "method": "通过红队测试，尝试复现书籍、新闻、科学文章和电子健康记录中的内容。", "result": "发现工具能复现部分书籍的版权内容，但未复现其他类型内容，存在虚构现象。", "conclusion": "测试后部署了缓解策略，并呼吁类似测试以评估AI工具的合法性和伦理性。"}}
{"id": "2506.24117", "pdf": "https://arxiv.org/pdf/2506.24117", "abs": "https://arxiv.org/abs/2506.24117", "authors": ["David M. Smiley"], "title": "Computational Detection of Intertextual Parallels in Biblical Hebrew: A Benchmark Study Using Transformer-Based Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Identifying parallel passages in biblical Hebrew is foundational in biblical\nscholarship for uncovering intertextual relationships. Traditional methods rely\non manual comparison, which is labor-intensive and prone to human error. This\nstudy evaluates the potential of pre-trained transformer-based language models,\nincluding E5, AlephBERT, MPNet, and LaBSE, for detecting textual parallels in\nthe Hebrew Bible. Focusing on known parallels between the books of Samuel/Kings\nand Chronicles, I assessed each model's capability to generate word embeddings\nthat delineate parallel from non-parallel passages. Utilizing cosine similarity\nand Wasserstein Distance measures, I found that E5 and AlephBERT show\nsignificant promise, with E5 excelling in parallel detection and AlephBERT\ndemonstrating stronger non-parallel differentiation. These findings indicate\nthat pre-trained models can enhance the efficiency and accuracy of detecting\nintertextual parallels in ancient texts, suggesting broader applications for\nancient language studies.", "AI": {"tldr": "该研究评估了预训练Transformer模型（如E5、AlephBERT等）在检测希伯来圣经文本平行段落中的潜力，发现E5和AlephBERT表现突出，可提高古代文本研究的效率和准确性。", "motivation": "传统的手动比较方法耗时且易出错，研究旨在探索预训练模型在检测希伯来圣经平行段落中的有效性。", "method": "使用E5、AlephBERT、MPNet和LaBSE模型生成词嵌入，并通过余弦相似度和Wasserstein距离评估模型性能。", "result": "E5在平行段落检测中表现最佳，AlephBERT在区分非平行段落方面更强。", "conclusion": "预训练模型可显著提升古代文本平行段落的检测效率和准确性，具有广泛的应用潜力。"}}
{"id": "2506.23106", "pdf": "https://arxiv.org/pdf/2506.23106", "abs": "https://arxiv.org/abs/2506.23106", "authors": ["Ryo Ishiyama", "Shinnosuke Matsuo", "Seiichi Uchida"], "title": "Computer-Aided Multi-Stroke Character Simplification by Stroke Removal", "categories": ["cs.CV"], "comment": "ICDAR2025 (Oral)", "summary": "Multi-stroke characters in scripts such as Chinese and Japanese can be highly\ncomplex, posing significant challenges for both native speakers and,\nespecially, non-native learners. If these characters can be simplified without\ndegrading their legibility, it could reduce learning barriers for non-native\nspeakers, facilitate simpler and legible font designs, and contribute to\nefficient character-based communication systems. In this paper, we propose a\nframework to systematically simplify multi-stroke characters by selectively\nremoving strokes while preserving their overall legibility. More specifically,\nwe use a highly accurate character recognition model to assess legibility and\nremove those strokes that minimally impact it. Experimental results on 1,256\ncharacter classes with 5, 10, 15, and 20 strokes reveal several key findings,\nincluding the observation that even after removing multiple strokes, many\ncharacters remain distinguishable. These findings suggest the potential for\nmore formalized simplification strategies.", "AI": {"tldr": "提出了一种通过选择性去除笔画来简化多笔画字符的框架，旨在降低学习难度并提升字体设计效率。", "motivation": "多笔画字符（如中文和日文）复杂度高，对非母语学习者尤其困难，简化字符可降低学习门槛并优化字体设计。", "method": "使用高精度字符识别模型评估可读性，选择性去除对可读性影响最小的笔画。", "result": "实验表明，即使去除多个笔画，许多字符仍可区分，验证了简化策略的可行性。", "conclusion": "研究为多笔画字符的简化提供了潜在方向，支持更系统化的简化策略开发。"}}
{"id": "2506.22526", "pdf": "https://arxiv.org/pdf/2506.22526", "abs": "https://arxiv.org/abs/2506.22526", "authors": ["Ofer M. Shir", "Michael Emmerich"], "title": "Correlated Mutations for Integer Programming", "categories": ["math.OC", "cs.AI", "cs.NE"], "comment": null, "summary": "Even with the recent theoretical advancements that dramatically reduced the\ncomplexity of Integer Programming (IP), heuristics remain the dominant\nproblem-solvers for this difficult category. This study seeks to establish the\ngroundwork for Integer Evolution Strategies (IESs), a class of randomized\nsearch heuristics inherently designed for continuous spaces. IESs already excel\nin treating IP in practice, but accomplish it via discretization and by\napplying sophisticated patches to their continuous operators, while\npersistently using the $\\ell_2$-norm as their operation pillar. We lay\nfoundations for discrete search, by adopting the $\\ell_1$-norm, accounting for\nthe suitable step-size, and questioning alternative measures to quantify\ncorrelations over the integer lattice. We focus on mutation distributions for\nunbounded integer decision variables. We briefly discuss a couple of candidate\ndiscrete probabilities induced by the uniform and binomial distributions, which\nwe show to possess less appealing theoretical properties, and then narrow down\nto the Truncated Normal (TN) and Double Geometric (DG) distributions. We\nexplore their theoretical properties, including entropy functions, and propose\na procedure to generate scalable correlated mutation distributions. Our\ninvestigations are accompanied by extensive numerical simulations, which\nconsistently support the claim that the DG distribution is better suited for\nunbounded integer search. We link our theoretical perspective to empirical\nevidence indicating that an IES with correlated DG mutations outperformed other\nstrategies over non-separable quadratic IP. We conclude that while the\nreplacement of the default TN distribution by the DG is theoretically justified\nand practically beneficial, the truly crucial change lies in adopting the\n$\\ell_1$-norm over the $\\ell_2$-norm.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2506.22449", "pdf": "https://arxiv.org/pdf/2506.22449", "abs": "https://arxiv.org/abs/2506.22449", "authors": ["Carolyn Hicks"], "title": "Computational Analysis of Climate Policy", "categories": ["cs.CY", "cs.CL"], "comment": "Master's thesis", "summary": "This thesis explores the impact of the Climate Emergency movement on local\ngovernment climate policy, using computational methods. The Climate Emergency\nmovement sought to accelerate climate action at local government level through\nthe mechanism of Climate Emergency Declarations (CEDs), resulting in a series\nof commitments from councils to treat climate change as an emergency. With the\naim of assessing the potential of current large language models to answer\ncomplex policy questions, I first built and configured a system named PALLM\n(Policy Analysis with a Large Language Model), using the OpenAI model GPT-4.\nThis system is designed to apply a conceptual framework for climate emergency\nresponse plans to a dataset of climate policy documents. I validated the\nperformance of this system with the help of local government policymakers, by\ngenerating analyses of the climate policies of 11 local governments in Victoria\nand assessing the policymakers' level of agreement with PALLM's responses.\nHaving established that PALLM's performance is satisfactory, I used it to\nconduct a large-scale analysis of current policy documents from local\ngovernments in the state of Victoria, Australia. This thesis presents the\nmethodology and results of this analysis, comparing the results for councils\nwhich have passed a CED to those which did not. This study finds that GPT-4 is\ncapable of high-level policy analysis, with limitations including a lack of\nreliable attribution, and can also enable more nuanced analysis by researchers.\nIts use in this research shows that councils which have passed a CED are more\nlikely to have a recent and climate-specific policy, and show more attention to\nurgency, prioritisation, and equity and social justice, than councils which\nhave not. It concludes that the ability to assess policy documents at scale\nopens up exciting new opportunities for policy researchers.", "AI": {"tldr": "该论文探讨了气候紧急运动对地方政府气候政策的影响，使用计算方法和GPT-4构建的系统PALLM进行分析，发现通过气候紧急宣言（CED）的议会在政策中更注重紧迫性和社会公正。", "motivation": "评估当前大型语言模型（如GPT-4）回答复杂政策问题的潜力，并研究气候紧急运动对地方政府政策的影响。", "method": "构建并验证了名为PALLM的系统，使用GPT-4分析维多利亚州11个地方政府的政策文件，比较通过和未通过CED的议会。", "result": "通过CED的议会更可能制定近期和气候专项政策，并更关注紧迫性、优先级及社会公正。GPT-4能进行高级政策分析，但存在引用不可靠的限制。", "conclusion": "大规模政策文件分析为政策研究提供了新机会，GPT-4在此类研究中展现出潜力。"}}
{"id": "2506.23108", "pdf": "https://arxiv.org/pdf/2506.23108", "abs": "https://arxiv.org/abs/2506.23108", "authors": ["Zhiyuan Zhu", "Jian Wang", "Yong Jiang", "Tong Han", "Yuhao Huang", "Ang Zhang", "Kaiwen Yang", "Mingyuan Luo", "Zhe Liu", "Yaofei Duan", "Dong Ni", "Tianhong Tang", "Xin Yang"], "title": "Hierarchical Corpus-View-Category Refinement for Carotid Plaque Risk Grading in Ultrasound", "categories": ["cs.CV"], "comment": "Accepted at MICCAI 2025", "summary": "Accurate carotid plaque grading (CPG) is vital to assess the risk of\ncardiovascular and cerebrovascular diseases. Due to the small size and high\nintra-class variability of plaque, CPG is commonly evaluated using a\ncombination of transverse and longitudinal ultrasound views in clinical\npractice. However, most existing deep learning-based multi-view classification\nmethods focus on feature fusion across different views, neglecting the\nimportance of representation learning and the difference in class features. To\naddress these issues, we propose a novel Corpus-View-Category Refinement\nFramework (CVC-RF) that processes information from Corpus-, View-, and\nCategory-levels, enhancing model performance. Our contribution is four-fold.\nFirst, to the best of our knowledge, we are the foremost deep learning-based\nmethod for CPG according to the latest Carotid Plaque-RADS guidelines. Second,\nwe propose a novel center-memory contrastive loss, which enhances the network's\nglobal modeling capability by comparing with representative cluster centers and\ndiverse negative samples at the Corpus level. Third, we design a cascaded\ndown-sampling attention module to fuse multi-scale information and achieve\nimplicit feature interaction at the View level. Finally, a parameter-free\nmixture-of-experts weighting strategy is introduced to leverage class\nclustering knowledge to weight different experts, enabling feature decoupling\nat the Category level. Experimental results indicate that CVC-RF effectively\nmodels global features via multi-level refinement, achieving state-of-the-art\nperformance in the challenging CPG task.", "AI": {"tldr": "提出了一种新的多视图分类框架CVC-RF，用于颈动脉斑块分级（CPG），通过多级细化提升模型性能。", "motivation": "现有深度学习方法在多视图分类中忽视了表示学习和类别特征的差异，导致CPG任务表现不佳。", "method": "提出CVC-RF框架，包含语料库、视图和类别三个层次的细化，采用中心记忆对比损失、级联下采样注意力模块和无参数专家混合加权策略。", "result": "实验表明CVC-RF在多级细化下有效建模全局特征，在CPG任务中达到最先进性能。", "conclusion": "CVC-RF通过多级细化显著提升了CPG任务的性能，为相关领域提供了新思路。"}}
{"id": "2506.22481", "pdf": "https://arxiv.org/pdf/2506.22481", "abs": "https://arxiv.org/abs/2506.22481", "authors": ["Jacob Hobbs"], "title": "Theories of \"Sexuality\" in Natural Language Processing Bias Research", "categories": ["cs.CY", "cs.CL"], "comment": "17 pages, 9 tables, undergraduate senior thesis, submitted to The\n  Spectra: The Virginia Engineering and Science Research Journal", "summary": "In recent years, significant advancements in the field of Natural Language\nProcessing (NLP) have positioned commercialized language models as\nwide-reaching, highly useful tools. In tandem, there has been an explosion of\nmultidisciplinary research examining how NLP tasks reflect, perpetuate, and\namplify social biases such as gender and racial bias. A significant gap in this\nscholarship is a detailed analysis of how queer sexualities are encoded and\n(mis)represented by both NLP systems and practitioners. Following previous work\nin the field of AI fairness, we document how sexuality is defined and\noperationalized via a survey and analysis of 55 articles that quantify\nsexuality-based NLP bias. We find that sexuality is not clearly defined in a\nmajority of the literature surveyed, indicating a reliance on assumed or\nnormative conceptions of sexual/romantic practices and identities. Further, we\nfind that methods for extracting biased outputs from NLP technologies often\nconflate gender and sexual identities, leading to monolithic conceptions of\nqueerness and thus improper quantifications of bias. With the goal of improving\nsexuality-based NLP bias analyses, we conclude with recommendations that\nencourage more thorough engagement with both queer communities and\ninterdisciplinary literature.", "AI": {"tldr": "论文分析了NLP系统中对性取向的编码和误表示问题，指出文献中性取向定义模糊且常与性别混淆，提出了改进建议。", "motivation": "研究NLP任务如何反映和放大社会偏见，尤其是性取向偏见，填补了现有研究的空白。", "method": "通过调查和分析55篇量化性取向偏见的文献，定义和操作化性取向。", "result": "发现文献中性取向定义模糊，常与性别混淆，导致对酷儿身份的单一化理解。", "conclusion": "建议更深入地与酷儿社区和跨学科文献合作，改进性取向偏见的分析。"}}
{"id": "2506.23115", "pdf": "https://arxiv.org/pdf/2506.23115", "abs": "https://arxiv.org/abs/2506.23115", "authors": ["Haonan Chen", "Hong Liu", "Yuping Luo", "Liang Wang", "Nan Yang", "Furu Wei", "Zhicheng Dou"], "title": "MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional Multimodal Embeddings", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Homepage: https://haon-chen.github.io/MoCa/", "summary": "Multimodal embedding models, built upon causal Vision Language Models (VLMs),\nhave shown promise in various tasks. However, current approaches face three key\nlimitations: the use of causal attention in VLM backbones is suboptimal for\nembedding tasks; scalability issues due to reliance on high-quality labeled\npaired data for contrastive learning; and limited diversity in training\nobjectives and data. To address these issues, we propose MoCa, a two-stage\nframework for transforming pre-trained VLMs into effective bidirectional\nmultimodal embedding models. The first stage, Modality-aware Continual\nPre-training, introduces a joint reconstruction objective that simultaneously\ndenoises interleaved text and image inputs, enhancing bidirectional\ncontext-aware reasoning. The second stage, Heterogeneous Contrastive\nFine-tuning, leverages diverse, semantically rich multimodal data beyond simple\nimage-caption pairs to enhance generalization and alignment. Our method\naddresses the stated limitations by introducing bidirectional attention through\ncontinual pre-training, scaling effectively with massive unlabeled datasets via\njoint reconstruction objectives, and utilizing diverse multimodal data for\nenhanced representation robustness. Experiments demonstrate that MoCa\nconsistently improves performance across MMEB and ViDoRe-v2 benchmarks,\nachieving new state-of-the-art results, and exhibits strong scalability with\nboth model size and training data on MMEB.", "AI": {"tldr": "MoCa是一个两阶段框架，将预训练的因果视觉语言模型（VLM）转化为高效的双向多模态嵌入模型，解决了当前方法的局限性。", "motivation": "当前多模态嵌入模型存在三个主要问题：因果注意力不适用于嵌入任务、依赖高质量标注数据导致可扩展性差、训练目标和数据多样性不足。", "method": "MoCa分为两阶段：1）模态感知持续预训练，通过联合重建目标增强双向上下文推理；2）异构对比微调，利用多样化的多模态数据提升泛化和对齐能力。", "result": "MoCa在MMEB和ViDoRe-v2基准测试中表现优异，达到新的最优结果，并在模型规模和数据量上展现出强扩展性。", "conclusion": "MoCa通过双向注意力、大规模无标签数据和多样化训练目标，显著提升了多模态嵌入模型的性能和鲁棒性。"}}
{"id": "2506.22566", "pdf": "https://arxiv.org/pdf/2506.22566", "abs": "https://arxiv.org/abs/2506.22566", "authors": ["Jacob Adamczyk"], "title": "Exploration Behavior of Untrained Policies", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "High-dimensional Learning Dynamics Workshop at ICML-2025", "summary": "Exploration remains a fundamental challenge in reinforcement learning (RL),\nparticularly in environments with sparse or adversarial reward structures. In\nthis work, we study how the architecture of deep neural policies implicitly\nshapes exploration before training. We theoretically and empirically\ndemonstrate strategies for generating ballistic or diffusive trajectories from\nuntrained policies in a toy model. Using the theory of infinite-width networks\nand a continuous-time limit, we show that untrained policies return correlated\nactions and result in non-trivial state-visitation distributions. We discuss\nthe distributions of the corresponding trajectories for a standard\narchitecture, revealing insights into inductive biases for tackling\nexploration. Our results establish a theoretical and experimental framework for\nusing policy initialization as a design tool to understand exploration behavior\nin early training.", "AI": {"tldr": "论文研究了深度神经策略架构如何在训练前隐式影响探索行为，通过理论和实验展示了未训练策略在玩具模型中生成弹道或扩散轨迹的策略。", "motivation": "强化学习中的探索问题，尤其是在稀疏或对抗性奖励结构的环境中，是一个基本挑战。", "method": "利用无限宽度网络理论和连续时间极限，分析未训练策略的动作相关性及状态访问分布。", "result": "未训练策略返回相关动作并产生非平凡的状态访问分布，揭示了探索的归纳偏差。", "conclusion": "研究为利用策略初始化作为设计工具理解早期训练中的探索行为提供了理论和实验框架。"}}
{"id": "2506.22493", "pdf": "https://arxiv.org/pdf/2506.22493", "abs": "https://arxiv.org/abs/2506.22493", "authors": ["Sadia Kamal", "Lalu Prasad Yadav Prakash", "S M Rafiuddin", "Mohammed Rakib", "Arunkumar Bagavathi", "Atriya Sen", "Sagnik Ray Choudhury"], "title": "A Detailed Factor Analysis for the Political Compass Test: Navigating Ideologies of Large Language Models", "categories": ["cs.CY", "cs.CL", "cs.LG"], "comment": null, "summary": "Political Compass Test (PCT) or similar questionnaires have been used to\nquantify LLM's political leanings. Building on a recent line of work that\nexamines the validity of PCT tests, we demonstrate that variation in standard\ngeneration parameters does not significantly impact the models' PCT scores.\nHowever, external factors such as prompt variations and fine-tuning\nindividually and in combination affect the same. Finally, we demonstrate that\nwhen models are fine-tuned on text datasets with higher political content than\nothers, the PCT scores are not differentially affected. This calls for a\nthorough investigation into the validity of PCT and similar tests, as well as\nthe mechanism by which political leanings are encoded in LLMs.", "AI": {"tldr": "研究发现，生成参数的变化对LLM的政治倾向评分（PCT）影响不大，但提示变化和微调会显著影响评分。此外，政治内容较多的数据集微调不会导致PCT评分的差异变化。", "motivation": "探讨政治倾向测试（PCT）在量化大型语言模型（LLM）政治倾向时的有效性，以及外部因素如何影响评分。", "method": "通过分析生成参数、提示变化和微调对PCT评分的影响，验证PCT测试的可靠性。", "result": "生成参数变化对PCT评分影响不显著，但提示变化和微调会显著改变评分；政治内容较多的数据集微调不会导致评分差异。", "conclusion": "需要深入研究PCT测试的有效性，以及LLM中政治倾向的编码机制。"}}
{"id": "2506.23120", "pdf": "https://arxiv.org/pdf/2506.23120", "abs": "https://arxiv.org/abs/2506.23120", "authors": ["Zhenhua Ning", "Zhuotao Tian", "Shaoshuai Shi", "Guangming Lu", "Daojing He", "Wenjie Pei", "Li Jiang"], "title": "Enhancing Spatial Reasoning in Multimodal Large Language Models through Reasoning-based Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in point cloud perception have demonstrated remarkable\nprogress in scene understanding through vision-language alignment leveraging\nlarge language models (LLMs). However, existing methods may still encounter\nchallenges in handling complex instructions that require accurate spatial\nreasoning, even if the 3D point cloud data provides detailed spatial cues such\nas size and position for identifying the targets. To tackle this issue, we\npropose Relevant Reasoning Segmentation (R$^2$S), a reasoning-based\nsegmentation framework. The framework emulates human cognitive processes by\ndecomposing spatial reasoning into two sequential stages: first identifying\nrelevant elements, then processing instructions guided by their associated\nvisual priors. Furthermore, acknowledging the inadequacy of existing datasets\nin complex reasoning tasks, we introduce 3D ReasonSeg, a reasoning-based\nsegmentation dataset comprising 25,185 training samples and 3,966 validation\nsamples with precise annotations. Both quantitative and qualitative experiments\ndemonstrate that the R$^2$S and 3D ReasonSeg effectively endow 3D point cloud\nperception with stronger spatial reasoning capabilities, and we hope that they\ncan serve as a new baseline and benchmark for future work.", "AI": {"tldr": "提出了一种基于推理的分割框架R²S和数据集3D ReasonSeg，以增强3D点云感知的空间推理能力。", "motivation": "现有方法在处理需要精确空间推理的复杂指令时存在挑战，尽管3D点云数据提供了丰富的空间线索。", "method": "R²S框架模拟人类认知过程，将空间推理分为两个阶段：先识别相关元素，再根据视觉先验处理指令。同时提出3D ReasonSeg数据集。", "result": "实验表明R²S和3D ReasonSeg显著提升了3D点云的空间推理能力。", "conclusion": "R²S和3D ReasonSeg为未来研究提供了新的基准和数据集。"}}
{"id": "2506.23132", "pdf": "https://arxiv.org/pdf/2506.23132", "abs": "https://arxiv.org/abs/2506.23132", "authors": ["Sophie Zhou", "Shu Kong"], "title": "Dare to Plagiarize? Plagiarized Painting Recognition and Retrieval", "categories": ["cs.CV"], "comment": "to appear at AVSS'25", "summary": "Art plagiarism detection plays a crucial role in protecting artists'\ncopyrights and intellectual property, yet it remains a challenging problem in\nforensic analysis. In this paper, we address the task of recognizing\nplagiarized paintings and explaining the detected plagarisms by retrieving\nvisually similar authentic artworks. To support this study, we construct a\ndataset by collecting painting photos and synthesizing plagiarized versions\nusing generative AI, tailored to specific artists' styles. We first establish a\nbaseline approach using off-the-shelf features from the visual foundation model\nDINOv2 to retrieve the most similar images in the database and classify\nplagiarism based on a similarity threshold. Surprisingly, this non-learned\nmethod achieves a high recognition accuracy of 97.2\\% but suffers from low\nretrieval precision 29.0\\% average precision (AP). To improve retrieval\nquality, we finetune DINOv2 with a metric learning loss using positive and\nnegative sample pairs sampled in the database. The finetuned model greatly\nimproves retrieval performance by 12\\% AP over the baseline, though it\nunexpectedly results in a lower recognition accuracy (92.7\\%). We conclude with\ninsightful discussions and outline directions for future research.", "AI": {"tldr": "论文提出了一种检测绘画抄袭的方法，通过检索视觉相似的原创作品来识别和解释抄袭行为。使用DINOv2模型作为基线方法，并通过度量学习优化检索性能。", "motivation": "保护艺术家版权和知识产权，解决艺术抄袭检测中的挑战性问题。", "method": "构建数据集，使用DINOv2模型进行相似性检索和分类，并通过度量学习优化模型。", "result": "基线方法识别准确率达97.2%，但检索精度低（29.0% AP）。优化后检索性能提升12% AP，但识别准确率降至92.7%。", "conclusion": "论文提出了有效方法，但识别准确率与检索性能之间存在权衡，未来需进一步研究。"}}
{"id": "2506.22578", "pdf": "https://arxiv.org/pdf/2506.22578", "abs": "https://arxiv.org/abs/2506.22578", "authors": ["Xufei Lv", "Haoyuan Sun", "Xuefeng Bai", "Min Zhang", "Houde Liu", "Kehai Chen"], "title": "The Hidden Link Between RLHF and Contrastive Learning", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": null, "summary": "Alignment of large language models (LLMs) with human values has recently\ngarnered significant attention, with prominent examples including the canonical\nyet costly Reinforcement Learning from Human Feedback (RLHF) and the simple\nDirect Preference Optimization (DPO). In this work, we demonstrate that both\nRLHF and DPO can be interpreted from the perspective of mutual information (MI)\nmaximization, uncovering a profound connection to contrastive learning. Within\nthis framework, both RLHF and DPO can be viewed as methods that perform\ncontrastive learning based on the positive and negative samples derived from\nthe base model, leveraging the Donsker-Varadhan (DV) lower bound on MI\n(equivalently, the MINE estimator). This paradigm further explains why RLHF may\nnot intrinsically incentivize reasoning capacities in LLMs beyond what is\nalready present in the base model. Building on this perspective, we replace the\nDV/MINE bound with the Jensen-Shannon MI estimator and propose Mutual\nInformation Optimization (MIO). Comprehensive theoretical analysis and\nextensive empirical evaluations demonstrate that MIO mitigates the late-stage\ndecline in chosen-likelihood observed in DPO, achieving competitive or superior\nperformance across various challenging reasoning and mathematical benchmarks.\nWe will release the model and code upon acceptance.", "AI": {"tldr": "论文提出了一种基于互信息最大化的新方法MIO，用于优化大语言模型的对齐，解决了DPO中后期性能下降的问题。", "motivation": "研究大语言模型（LLMs）与人类价值观对齐的方法，揭示RLHF和DPO与对比学习的联系，并提出改进方案。", "method": "从互信息最大化角度解释RLHF和DPO，提出使用Jensen-Shannon互信息估计器的MIO方法。", "result": "MIO在推理和数学任务中表现优于DPO，避免了后期性能下降。", "conclusion": "MIO是一种有效的对齐方法，为LLMs的性能提升提供了新思路。"}}
{"id": "2506.22666", "pdf": "https://arxiv.org/pdf/2506.22666", "abs": "https://arxiv.org/abs/2506.22666", "authors": ["Anamika Lochab", "Lu Yan", "Patrick Pynadath", "Xiangyu Zhang", "Ruqi Zhang"], "title": "VERA: Variational Inference Framework for Jailbreaking Large Language Models", "categories": ["cs.CR", "cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "The rise of API-only access to state-of-the-art LLMs highlights the need for\neffective black-box jailbreak methods to identify model vulnerabilities in\nreal-world settings. Without a principled objective for gradient-based\noptimization, most existing approaches rely on genetic algorithms, which are\nlimited by their initialization and dependence on manually curated prompt\npools. Furthermore, these methods require individual optimization for each\nprompt, failing to provide a comprehensive characterization of model\nvulnerabilities. To address this gap, we introduce VERA: Variational infErence\nfRamework for jAilbreaking. VERA casts black-box jailbreak prompting as a\nvariational inference problem, training a small attacker LLM to approximate the\ntarget LLM's posterior over adversarial prompts. Once trained, the attacker can\ngenerate diverse, fluent jailbreak prompts for a target query without\nre-optimization. Experimental results show that VERA achieves strong\nperformance across a range of target LLMs, highlighting the value of\nprobabilistic inference for adversarial prompt generation.", "AI": {"tldr": "VERA是一种基于变分推理的黑盒越狱方法，通过训练小型攻击LLM生成多样化的越狱提示，无需逐条优化。", "motivation": "现有方法依赖遗传算法和手动优化的提示池，无法全面评估模型漏洞，需要更高效的方法。", "method": "VERA将黑盒越狱问题转化为变分推理问题，训练攻击LLM近似目标LLM的后验分布，生成多样化提示。", "result": "实验表明VERA在多种目标LLM上表现优异，验证了概率推理在对抗提示生成中的价值。", "conclusion": "VERA提供了一种高效、全面的黑盒越狱方法，为模型漏洞评估提供了新思路。"}}
{"id": "2506.23135", "pdf": "https://arxiv.org/pdf/2506.23135", "abs": "https://arxiv.org/abs/2506.23135", "authors": ["Yu Shang", "Xin Zhang", "Yinzhou Tang", "Lei Jin", "Chen Gao", "Wei Wu", "Yong Li"], "title": "RoboScape: Physics-informed Embodied World Model", "categories": ["cs.CV", "cs.RO"], "comment": "17 pages", "summary": "World models have become indispensable tools for embodied intelligence,\nserving as powerful simulators capable of generating realistic robotic videos\nwhile addressing critical data scarcity challenges. However, current embodied\nworld models exhibit limited physical awareness, particularly in modeling 3D\ngeometry and motion dynamics, resulting in unrealistic video generation for\ncontact-rich robotic scenarios. In this paper, we present RoboScape, a unified\nphysics-informed world model that jointly learns RGB video generation and\nphysics knowledge within an integrated framework. We introduce two key\nphysics-informed joint training tasks: temporal depth prediction that enhances\n3D geometric consistency in video rendering, and keypoint dynamics learning\nthat implicitly encodes physical properties (e.g., object shape and material\ncharacteristics) while improving complex motion modeling. Extensive experiments\ndemonstrate that RoboScape generates videos with superior visual fidelity and\nphysical plausibility across diverse robotic scenarios. We further validate its\npractical utility through downstream applications including robotic policy\ntraining with generated data and policy evaluation. Our work provides new\ninsights for building efficient physics-informed world models to advance\nembodied intelligence research. The code is available at:\nhttps://github.com/tsinghua-fib-lab/RoboScape.", "AI": {"tldr": "RoboScape是一个统一的物理感知世界模型，通过联合学习RGB视频生成和物理知识，提升了视频生成的视觉保真度和物理合理性。", "motivation": "当前的世界模型在3D几何和运动动力学建模方面存在局限性，导致接触丰富的机器人场景视频生成不真实。", "method": "RoboScape通过时间深度预测和关键点动力学学习两个物理感知联合训练任务，增强3D几何一致性和复杂运动建模。", "result": "实验表明，RoboScape生成的视频在多样机器人场景中具有更高的视觉保真度和物理合理性。", "conclusion": "RoboScape为构建高效的物理感知世界模型提供了新思路，推动了具身智能研究。"}}
{"id": "2506.22580", "pdf": "https://arxiv.org/pdf/2506.22580", "abs": "https://arxiv.org/abs/2506.22580", "authors": ["Vasilis Siomos", "Jonathan Passerat-Palmbach", "Giacomo Tarroni"], "title": "FedCLAM: Client Adaptive Momentum with Foreground Intensity Matching for Federated Medical Image Segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "10 pages, 2 figures, Accepted at MICCAI 2025", "summary": "Federated learning is a decentralized training approach that keeps data under\nstakeholder control while achieving superior performance over isolated\ntraining. While inter-institutional feature discrepancies pose a challenge in\nall federated settings, medical imaging is particularly affected due to diverse\nimaging devices and population variances, which can diminish the global model's\neffectiveness. Existing aggregation methods generally fail to adapt across\nvaried circumstances. To address this, we propose FedCLAM, which integrates\n\\textit{client-adaptive momentum} terms derived from each client's loss\nreduction during local training, as well as a \\textit{personalized dampening\nfactor} to curb overfitting. We further introduce a novel \\textit{intensity\nalignment} loss that matches predicted and ground-truth foreground\ndistributions to handle heterogeneous image intensity profiles across\ninstitutions and devices. Extensive evaluations on two datasets show that\nFedCLAM surpasses eight cutting-edge methods in medical segmentation tasks,\nunderscoring its efficacy. The code is available at\nhttps://github.com/siomvas/FedCLAM.", "AI": {"tldr": "FedCLAM提出了一种联邦学习方法，通过客户端自适应动量和个性化阻尼因子解决医学影像中的特征差异问题，并在分割任务中表现优异。", "motivation": "医学影像中设备和人群的多样性导致特征差异，现有聚合方法难以适应，影响了全局模型的效果。", "method": "FedCLAM结合客户端自适应动量、个性化阻尼因子和强度对齐损失，以处理特征差异和图像强度分布不均。", "result": "在两个数据集上的实验表明，FedCLAM在医学分割任务中优于八种前沿方法。", "conclusion": "FedCLAM有效解决了医学影像联邦学习中的特征差异问题，提升了模型性能。"}}
{"id": "2506.22696", "pdf": "https://arxiv.org/pdf/2506.22696", "abs": "https://arxiv.org/abs/2506.22696", "authors": ["Brian Mak", "Jeffrey Flanigan"], "title": "Residual Matrix Transformers: Scaling the Size of the Residual Stream", "categories": ["cs.LG", "cs.CL"], "comment": "Accepted to ICML 2025", "summary": "The residual stream acts as a memory bus where transformer layers both store\nand access features (Elhage et al., 2021). We consider changing the mechanism\nfor retrieving and storing information in the residual stream, and replace the\nresidual stream of the transformer with an outer product memory matrix\n(Kohonen, 1972, Anderson, 1972). We call this model the Residual Matrix\nTransformer (RMT). We find that the RMT enjoys a number of attractive\nproperties: 1) the size of the residual stream can be scaled independently of\ncompute and model size, improving performance, 2) the RMT can achieve the same\nloss as the transformer with 58% fewer FLOPS, 25% fewer parameters, and 41%\nfewer training tokens tokens, and 3) the RMT outperforms the transformer on\ndownstream evaluations. We theoretically analyze the transformer and the RMT,\nand show that the RMT allows for more efficient scaling of the residual stream,\nas well as improved variance propagation properties. Code for this project can\nbe found at https://github.com/bmac3/residual-matrix-transformer.", "AI": {"tldr": "论文提出了一种名为RMT的新模型，用外积记忆矩阵替代传统Transformer的残差流，具有独立扩展残差流大小、减少计算和参数需求等优势。", "motivation": "传统Transformer的残差流机制在信息存储和检索上存在效率问题，RMT旨在通过改进机制提升性能。", "method": "用外积记忆矩阵（Kohonen, 1972）替代Transformer的残差流，构建RMT模型。", "result": "RMT在相同损失下减少58% FLOPS、25%参数和41%训练token，且在下游任务中表现更优。", "conclusion": "RMT通过更高效的残差流扩展和方差传播特性，显著提升了模型性能。"}}
{"id": "2506.23138", "pdf": "https://arxiv.org/pdf/2506.23138", "abs": "https://arxiv.org/abs/2506.23138", "authors": ["Shiyu Wu", "Mingzhen Sun", "Weining Wang", "Yequan Wang", "Jing Liu"], "title": "VisualPrompter: Prompt Optimization with Visual Feedback for Text-to-Image Synthesis", "categories": ["cs.CV"], "comment": "12 pages, 5 figures", "summary": "Since there exists a notable gap between user-provided and model-preferred\nprompts, generating high-quality and satisfactory images using diffusion models\noften requires prompt engineering to optimize user inputs. Current studies on\ntext-to-image prompt engineering can effectively enhance the style and\naesthetics of generated images. However, they often neglect the semantic\nalignment between generated images and user descriptions, resulting in visually\nappealing but content-wise unsatisfying outputs. In this work, we propose\nVisualPrompter, a novel training-free prompt engineering framework that refines\nuser inputs to model-preferred sentences. In particular, VisualPrompter\nutilizes an automatic self-reflection module to identify the missing concepts\nin generated images and a target-specific prompt optimization mechanism to\nrevise the prompts in a fine-grained manner. Extensive experiments demonstrate\nthe effectiveness of our VisualPrompter, which achieves new state-of-the-art\nperformance on multiple benchmarks for text-image alignment evaluation.\nAdditionally, our framework features a plug-and-play design, making it highly\nadaptable to various generative models.", "AI": {"tldr": "VisualPrompter是一个无需训练的提示工程框架，通过自动自省模块和目标特定优化机制，提升用户输入与模型偏好之间的语义对齐，生成更符合用户描述的高质量图像。", "motivation": "现有方法在提升图像风格和美学的同时，往往忽略了生成图像与用户描述的语义对齐，导致内容不满意。", "method": "提出VisualPrompter框架，包含自动自省模块和目标特定提示优化机制，无需训练即可优化用户输入。", "result": "在多个文本-图像对齐评估基准上达到最新最优性能，且具有即插即用的设计。", "conclusion": "VisualPrompter有效解决了语义对齐问题，适用于多种生成模型。"}}
{"id": "2506.22593", "pdf": "https://arxiv.org/pdf/2506.22593", "abs": "https://arxiv.org/abs/2506.22593", "authors": ["Antonello Longo", "Chanyoung Chung", "Matteo Palieri", "Sung-Kyun Kim", "Ali Agha", "Cataldo Guaragnella", "Shehryar Khattak"], "title": "Pixels-to-Graph: Real-time Integration of Building Information Models and Scene Graphs for Semantic-Geometric Human-Robot Understanding", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Paper accepted to 2025 IEEE International Conference on Automation\n  Science and Engineering (CASE)", "summary": "Autonomous robots are increasingly playing key roles as support platforms for\nhuman operators in high-risk, dangerous applications. To accomplish challenging\ntasks, an efficient human-robot cooperation and understanding is required.\nWhile typically robotic planning leverages 3D geometric information, human\noperators are accustomed to a high-level compact representation of the\nenvironment, like top-down 2D maps representing the Building Information Model\n(BIM). 3D scene graphs have emerged as a powerful tool to bridge the gap\nbetween human readable 2D BIM and the robot 3D maps. In this work, we introduce\nPixels-to-Graph (Pix2G), a novel lightweight method to generate structured\nscene graphs from image pixels and LiDAR maps in real-time for the autonomous\nexploration of unknown environments on resource-constrained robot platforms. To\nsatisfy onboard compute constraints, the framework is designed to perform all\noperation on CPU only. The method output are a de-noised 2D top-down\nenvironment map and a structure-segmented 3D pointcloud which are seamlessly\nconnected using a multi-layer graph abstracting information from object-level\nup to the building-level. The proposed method is quantitatively and\nqualitatively evaluated during real-world experiments performed using the NASA\nJPL NeBula-Spot legged robot to autonomously explore and map cluttered garage\nand urban office like environments in real-time.", "AI": {"tldr": "Pix2G方法通过轻量级实时处理图像和LiDAR数据，生成结构化场景图，用于资源受限的机器人平台自主探索未知环境。", "motivation": "解决人类操作员与机器人之间在环境理解上的差异，同时满足资源受限平台的计算需求。", "method": "利用图像像素和LiDAR地图实时生成结构化场景图，所有操作在CPU上完成。", "result": "生成去噪的2D顶视图和结构分割的3D点云，并通过多层图连接。", "conclusion": "Pix2G方法在真实环境中验证有效，适用于自主探索任务。"}}
{"id": "2506.22716", "pdf": "https://arxiv.org/pdf/2506.22716", "abs": "https://arxiv.org/abs/2506.22716", "authors": ["Dujian Ding", "Ankur Mallick", "Shaokun Zhang", "Chi Wang", "Daniel Madrigal", "Mirian Del Carmen Hipolito Garcia", "Menglin Xia", "Laks V. S. Lakshmanan", "Qingyun Wu", "Victor Rühle"], "title": "BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.DB"], "comment": "Accepted to ICML 2025 (main conference)", "summary": "Large language models (LLMs) are powerful tools but are often expensive to\ndeploy at scale. LLM query routing mitigates this by dynamically assigning\nqueries to models of varying cost and quality to obtain a desired trade-off.\nPrior query routing approaches generate only one response from the selected\nmodel and a single response from a small (inexpensive) model was often not good\nenough to beat a response from a large (expensive) model due to which they end\nup overusing the large model and missing out on potential cost savings.\nHowever, it is well known that for small models, generating multiple responses\nand selecting the best can enhance quality while remaining cheaper than a\nsingle large-model response. We leverage this idea to propose BEST-Route, a\nnovel routing framework that chooses a model and the number of responses to\nsample from it based on query difficulty and the quality thresholds.\nExperiments on real-world datasets demonstrate that our method reduces costs by\nup to 60% with less than 1% performance drop.", "AI": {"tldr": "BEST-Route是一种新型的路由框架，通过动态分配查询到不同成本和质量的模型，并生成多个响应以优化成本与性能的平衡。", "motivation": "大型语言模型（LLMs）部署成本高，现有路由方法因仅生成单一响应而过度依赖昂贵模型，未能充分利用低成本模型的潜力。", "method": "提出BEST-Route框架，根据查询难度和质量阈值选择模型及生成响应的数量，利用小模型生成多个响应并选择最佳。", "result": "在真实数据集上，该方法将成本降低高达60%，性能下降不到1%。", "conclusion": "BEST-Route有效平衡了成本与性能，为LLM部署提供了经济高效的解决方案。"}}
{"id": "2506.23150", "pdf": "https://arxiv.org/pdf/2506.23150", "abs": "https://arxiv.org/abs/2506.23150", "authors": ["Xinyue Liang", "Zhiyuan Ma", "Lingchen Sun", "Yanjun Guo", "Lei Zhang"], "title": "AlignCVC: Aligning Cross-View Consistency for Single-Image-to-3D Generation", "categories": ["cs.CV"], "comment": null, "summary": "Single-image-to-3D models typically follow a sequential generation and\nreconstruction workflow. However, intermediate multi-view images synthesized by\npre-trained generation models often lack cross-view consistency (CVC),\nsignificantly degrading 3D reconstruction performance. While recent methods\nattempt to refine CVC by feeding reconstruction results back into the\nmulti-view generator, these approaches struggle with noisy and unstable\nreconstruction outputs that limit effective CVC improvement. We introduce\nAlignCVC, a novel framework that fundamentally re-frames single-image-to-3D\ngeneration through distribution alignment rather than relying on strict\nregression losses. Our key insight is to align both generated and reconstructed\nmulti-view distributions toward the ground-truth multi-view distribution,\nestablishing a principled foundation for improved CVC. Observing that generated\nimages exhibit weak CVC while reconstructed images display strong CVC due to\nexplicit rendering, we propose a soft-hard alignment strategy with distinct\nobjectives for generation and reconstruction models. This approach not only\nenhances generation quality but also dramatically accelerates inference to as\nfew as 4 steps. As a plug-and-play paradigm, our method, namely AlignCVC,\nseamlessly integrates various multi-view generation models with 3D\nreconstruction models. Extensive experiments demonstrate the effectiveness and\nefficiency of AlignCVC for single-image-to-3D generation.", "AI": {"tldr": "AlignCVC通过分布对齐而非严格回归损失改进单图到3D生成，提出软硬对齐策略提升跨视图一致性（CVC）和生成质量。", "motivation": "现有方法通过生成和重建流程处理单图到3D任务，但中间生成的多视图图像缺乏跨视图一致性（CVC），影响3D重建效果。", "method": "AlignCVC通过分布对齐生成和重建的多视图分布与真实分布，采用软硬对齐策略分别优化生成和重建模型。", "result": "AlignCVC显著提升生成质量和CVC，推理速度加快至仅需4步，兼容多种多视图生成和3D重建模型。", "conclusion": "AlignCVC为单图到3D生成提供了一种高效且通用的解决方案，通过分布对齐显著提升性能。"}}
{"id": "2506.23151", "pdf": "https://arxiv.org/pdf/2506.23151", "abs": "https://arxiv.org/abs/2506.23151", "authors": ["Vladislav Bargatin", "Egor Chistov", "Alexander Yakovenko", "Dmitriy Vatolin"], "title": "MEMFOF: High-Resolution Training for Memory-Efficient Multi-Frame Optical Flow Estimation", "categories": ["cs.CV", "cs.AI", "cs.MM"], "comment": "Accepted at ICCV 2025", "summary": "Recent advances in optical flow estimation have prioritized accuracy at the\ncost of growing GPU memory consumption, particularly for high-resolution\n(FullHD) inputs. We introduce MEMFOF, a memory-efficient multi-frame optical\nflow method that identifies a favorable trade-off between multi-frame\nestimation and GPU memory usage. Notably, MEMFOF requires only 2.09 GB of GPU\nmemory at runtime for 1080p inputs, and 28.5 GB during training, which uniquely\npositions our method to be trained at native 1080p without the need for\ncropping or downsampling. We systematically revisit design choices from\nRAFT-like architectures, integrating reduced correlation volumes and\nhigh-resolution training protocols alongside multi-frame estimation, to achieve\nstate-of-the-art performance across multiple benchmarks while substantially\nreducing memory overhead. Our method outperforms more resource-intensive\nalternatives in both accuracy and runtime efficiency, validating its robustness\nfor flow estimation at high resolutions. At the time of submission, our method\nranks first on the Spring benchmark with a 1-pixel (1px) outlier rate of 3.289,\nleads Sintel (clean) with an endpoint error (EPE) of 0.963, and achieves the\nbest Fl-all error on KITTI-2015 at 2.94%. The code is available at\nhttps://github.com/msu-video-group/memfof.", "AI": {"tldr": "MEMFOF是一种高效内存的多帧光流估计方法，显著降低GPU内存使用，同时保持高精度。", "motivation": "解决高分辨率光流估计中GPU内存消耗过高的问题。", "method": "结合减少的相关体积和高分辨率训练协议，优化RAFT类架构设计。", "result": "在多个基准测试中表现最佳，内存占用显著降低，训练和运行时效率高。", "conclusion": "MEMFOF在高分辨率光流估计中实现了内存效率和性能的平衡。"}}
{"id": "2506.22638", "pdf": "https://arxiv.org/pdf/2506.22638", "abs": "https://arxiv.org/abs/2506.22638", "authors": ["Aadim Nepal", "Safal Shrestha", "Anubhav Shrestha", "Minwu Kim", "Keith Ross"], "title": "Layer Importance for Mathematical Reasoning is Forged in Pre-Training and Invariant after Post-Training", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models can exhibit improved mathematical reasoning\ncapabilities following post-training with instruction tuning, reinforcement\nlearning, or knowledge distillation. However, it remains unclear whether these\nimprovements are driven by major changes in transformer layers or from minor\nadjustments that leave the relative layer importance structures of the base\nmodel largely unchanged. We investigate this question through systematic\nlayer-wise ablation experiments, examining base, instruction-tuned,\nknowledge-distilled, and reinforcement learning variants on mathematical\nreasoning benchmarks. Our findings show that mathematical reasoning gives rise\nto a specific layer importance structure, and this structure persists across\nall post-training paradigms. Removal of such layers causes accuracy drops of up\nto 80%. In contrast, non-mathematical tasks like factual recall exhibit no\ncritical layers. This distinction suggests that mathematical reasoning requires\nspecialized layers that emerge during pre-training, while other non-reasoning\ntasks do not. From an information-theoretic perspective, we also observe that\nthese critical layers are the same layers where major representational\ntransformation occurs.", "AI": {"tldr": "研究发现，数学推理能力提升依赖于特定层结构，而非大规模调整；这些关键层在预训练中形成，且对非数学任务不重要。", "motivation": "探究大语言模型数学推理能力提升是否源于层结构变化。", "method": "通过层消融实验，分析不同后训练范式下的层重要性。", "result": "数学推理任务依赖特定层结构，移除会导致准确性大幅下降；非数学任务无此现象。", "conclusion": "数学推理需要预训练中形成的专用层，信息转换主要发生在这些关键层。"}}
{"id": "2506.22809", "pdf": "https://arxiv.org/pdf/2506.22809", "abs": "https://arxiv.org/abs/2506.22809", "authors": ["Cooper Doyle"], "title": "BayesLoRA: Task-Specific Uncertainty in Low-Rank Adapters", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "13 pages, 3 figures, 1 table", "summary": "We propose BayesLoRA, a task-specific uncertainty quantification framework\nthat integrates MC-Dropout into Low-Rank Adapters (LoRA). Unlike\ngeneral-purpose transformer uncertainty methods, BayesLoRA provides guardrails\ntailored to downstream workflows, enabling agents to introspect and modulate\nbehavior under uncertainty. We demonstrate mathematically and empirically that\nLoRA adapters exhibit amplified variance outside fine-tuning distributions,\nyielding reliable confidence estimates for agentic decision-making.", "AI": {"tldr": "BayesLoRA是一个任务特定的不确定性量化框架，将MC-Dropout集成到LoRA中，为下游工作流提供定制化的不确定性评估。", "motivation": "现有通用Transformer不确定性方法无法满足下游任务需求，BayesLoRA旨在为代理行为提供更精确的不确定性评估。", "method": "将MC-Dropout集成到LoRA中，通过数学和实证分析展示LoRA适配器在微调分布外的方差放大特性。", "result": "BayesLoRA能够提供可靠的置信度估计，支持代理决策。", "conclusion": "BayesLoRA为任务特定的不确定性量化提供了一种有效方法，适用于代理行为调控。"}}
{"id": "2506.23153", "pdf": "https://arxiv.org/pdf/2506.23153", "abs": "https://arxiv.org/abs/2506.23153", "authors": ["Huiqiang Sun", "Xingyi Li", "Juewen Peng", "Liao Shen", "Zhiguo Cao", "Ke Xian", "Guosheng Lin"], "title": "Dynamic View Synthesis from Small Camera Motion Videos", "categories": ["cs.CV"], "comment": "Accepted by TVCG", "summary": "Novel view synthesis for dynamic $3$D scenes poses a significant challenge.\nMany notable efforts use NeRF-based approaches to address this task and yield\nimpressive results. However, these methods rely heavily on sufficient motion\nparallax in the input images or videos. When the camera motion range becomes\nlimited or even stationary (i.e., small camera motion), existing methods\nencounter two primary challenges: incorrect representation of scene geometry\nand inaccurate estimation of camera parameters. These challenges make prior\nmethods struggle to produce satisfactory results or even become invalid. To\naddress the first challenge, we propose a novel Distribution-based Depth\nRegularization (DDR) that ensures the rendering weight distribution to align\nwith the true distribution. Specifically, unlike previous methods that use\ndepth loss to calculate the error of the expectation, we calculate the\nexpectation of the error by using Gumbel-softmax to differentiably sample\npoints from discrete rendering weight distribution. Additionally, we introduce\nconstraints that enforce the volume density of spatial points before the object\nboundary along the ray to be near zero, ensuring that our model learns the\ncorrect geometry of the scene. To demystify the DDR, we further propose a\nvisualization tool that enables observing the scene geometry representation at\nthe rendering weight level. For the second challenge, we incorporate camera\nparameter learning during training to enhance the robustness of our model to\ncamera parameters. We conduct extensive experiments to demonstrate the\neffectiveness of our approach in representing scenes with small camera motion\ninput, and our results compare favorably to state-of-the-art methods.", "AI": {"tldr": "提出了一种基于分布深度正则化（DDR）的方法，解决了动态3D场景在小相机运动下的新视角合成问题，改进了几何表示和相机参数估计。", "motivation": "动态3D场景的新视角合成在相机运动范围有限时，现有方法难以准确表示几何和估计相机参数，导致效果不佳。", "method": "提出DDR方法，通过Gumbel-softmax采样点并约束体积密度，改进几何表示；同时学习相机参数以增强鲁棒性。", "result": "实验表明，该方法在小相机运动输入下表现优于现有方法。", "conclusion": "DDR方法有效解决了小相机运动下的新视角合成问题，提升了场景几何和相机参数的准确性。"}}
{"id": "2506.22656", "pdf": "https://arxiv.org/pdf/2506.22656", "abs": "https://arxiv.org/abs/2506.22656", "authors": ["Jiangping Huang", "Dongming Jin", "Weisong Sun", "Yang Liu", "Zhi Jin"], "title": "Knowledge-Guided Multi-Agent Framework for Automated Requirements Development: A Vision", "categories": ["cs.SE", "cs.AI", "68-04", "D.2.3; I.2.7"], "comment": null, "summary": "This paper envisions a knowledge-guided multi-agent framework named KGMAF for\nautomated requirements development. KGMAF aims to address gaps in current\nautomation systems for SE, which prioritize code development and overlook the\ncomplexities of requirements tasks. KGMAF is composed of six specialized agents\nand an artifact pool to improve efficiency and accuracy. Specifically, KGMAF\noutlines the functionality, actions, and knowledge of each agent and provides\nthe conceptual design of the artifact pool. Our case study highlights the\npotential of KGMAF in real-world scenarios. Finally, we outline several\nresearch opportunities for implementing and enhancing automated requirements\ndevelopment using multi-agent systems. We believe that KGMAF will play a\npivotal role in shaping the future of automated requirements development in the\nera of LLMs.", "AI": {"tldr": "KGMAF是一个知识引导的多智能体框架，用于自动化需求开发，填补当前SE自动化系统忽视需求任务复杂性的空白。", "motivation": "当前SE自动化系统过于关注代码开发，而忽视了需求任务的复杂性，KGMAF旨在解决这一问题。", "method": "KGMAF由六个专用智能体和一个工件池组成，详细描述了每个智能体的功能、行为和知识，并提供了工件池的概念设计。", "result": "案例研究表明KGMAF在现实场景中具有潜力。", "conclusion": "KGMAF有望在LLM时代推动自动化需求开发的未来发展，并提出了进一步研究和改进的方向。"}}
{"id": "2506.23156", "pdf": "https://arxiv.org/pdf/2506.23156", "abs": "https://arxiv.org/abs/2506.23156", "authors": ["Jiale Chen"], "title": "Self-Supervised Contrastive Learning for Multi-Label Images", "categories": ["cs.CV"], "comment": null, "summary": "Self-supervised learning (SSL) has demonstrated its effectiveness in learning\nrepresentations through comparison methods that align with human intuition.\nHowever, mainstream SSL methods heavily rely on high body datasets with single\nlabel, such as ImageNet, resulting in intolerable pre-training overhead.\nBesides, more general multi-label images are frequently overlooked in SSL,\ndespite their potential for richer semantic information and broader\napplicability in downstream scenarios. Therefore, we tailor the mainstream SSL\napproach to guarantee excellent representation learning capabilities using\nfewer multi-label images. Firstly, we propose a block-wise augmentation module\naimed at extracting additional potential positive view pairs from multi-label\nimages. Subsequently, an image-aware contrastive loss is devised to establish\nconnections between these views, thereby facilitating the extraction of\nsemantically consistent representations. Comprehensive linear fine-tuning and\ntransfer learning validate the competitiveness of our approach despite\nchallenging sample quality and quantity.", "AI": {"tldr": "论文提出了一种针对多标签图像的自监督学习方法，通过块级增强和图像感知对比损失，减少了预训练开销并提升了语义一致性。", "motivation": "主流自监督学习方法依赖单标签高体量数据集（如ImageNet），忽略了多标签图像的潜力，导致预训练开销大且语义信息不足。", "method": "提出块级增强模块提取多标签图像中的潜在正视图对，并设计图像感知对比损失以建立视图间的联系。", "result": "线性微调和迁移学习验证了该方法在样本质量和数量挑战下的竞争力。", "conclusion": "该方法在多标签图像上实现了高效的自监督学习，具有更广泛的适用性。"}}
{"id": "2506.22668", "pdf": "https://arxiv.org/pdf/2506.22668", "abs": "https://arxiv.org/abs/2506.22668", "authors": ["Selahattin Akkas", "Aditya Devarakonda", "Ariful Azad"], "title": "DistShap: Scalable GNN Explanations with Distributed Shapley Values", "categories": ["cs.LG", "cs.AI", "cs.DC", "stat.ML"], "comment": "12 pages", "summary": "With the growing adoption of graph neural networks (GNNs), explaining their\npredictions has become increasingly important. However, attributing predictions\nto specific edges or features remains computationally expensive. For example,\nclassifying a node with 100 neighbors using a 3-layer GNN may involve\nidentifying important edges from millions of candidates contributing to the\nprediction. To address this challenge, we propose DistShap, a parallel\nalgorithm that distributes Shapley value-based explanations across multiple\nGPUs. DistShap operates by sampling subgraphs in a distributed setting,\nexecuting GNN inference in parallel across GPUs, and solving a distributed\nleast squares problem to compute edge importance scores. DistShap outperforms\nmost existing GNN explanation methods in accuracy and is the first to scale to\nGNN models with millions of features by using up to 128 GPUs on the NERSC\nPerlmutter supercomputer.", "AI": {"tldr": "DistShap是一种并行算法，用于高效计算GNN预测的解释性，通过分布式计算Shapley值，显著提升了计算效率和可扩展性。", "motivation": "随着GNN的广泛应用，解释其预测变得重要，但现有方法计算成本高，难以扩展到大规模图数据。", "method": "DistShap通过分布式采样子图、并行执行GNN推理，并解决分布式最小二乘问题来计算边重要性分数。", "result": "DistShap在准确性上优于现有方法，并首次扩展到具有数百万特征的GNN模型，支持128个GPU并行计算。", "conclusion": "DistShap为大规模GNN提供了一种高效、可扩展的解释方法，解决了计算瓶颈问题。"}}
{"id": "2506.23157", "pdf": "https://arxiv.org/pdf/2506.23157", "abs": "https://arxiv.org/abs/2506.23157", "authors": ["Hanyu Zhou", "Haonan Wang", "Haoyue Liu", "Yuxing Duan", "Luxin Yan", "Gim Hee Lee"], "title": "STD-GS: Exploring Frame-Event Interaction for SpatioTemporal-Disentangled Gaussian Splatting to Reconstruct High-Dynamic Scene", "categories": ["cs.CV"], "comment": null, "summary": "High-dynamic scene reconstruction aims to represent static background with\nrigid spatial features and dynamic objects with deformed continuous\nspatiotemporal features. Typically, existing methods adopt unified\nrepresentation model (e.g., Gaussian) to directly match the spatiotemporal\nfeatures of dynamic scene from frame camera. However, this unified paradigm\nfails in the potential discontinuous temporal features of objects due to frame\nimaging and the heterogeneous spatial features between background and objects.\nTo address this issue, we disentangle the spatiotemporal features into various\nlatent representations to alleviate the spatiotemporal mismatching between\nbackground and objects. In this work, we introduce event camera to compensate\nfor frame camera, and propose a spatiotemporal-disentangled Gaussian splatting\nframework for high-dynamic scene reconstruction. As for dynamic scene, we\nfigure out that background and objects have appearance discrepancy in\nframe-based spatial features and motion discrepancy in event-based temporal\nfeatures, which motivates us to distinguish the spatiotemporal features between\nbackground and objects via clustering. As for dynamic object, we discover that\nGaussian representations and event data share the consistent spatiotemporal\ncharacteristic, which could serve as a prior to guide the spatiotemporal\ndisentanglement of object Gaussians. Within Gaussian splatting framework, the\ncumulative scene-object disentanglement can improve the spatiotemporal\ndiscrimination between background and objects to render the time-continuous\ndynamic scene. Extensive experiments have been performed to verify the\nsuperiority of the proposed method.", "AI": {"tldr": "提出了一种时空解耦的高斯泼溅框架，用于高动态场景重建，通过事件相机补偿帧相机，解决背景与物体时空特征不匹配问题。", "motivation": "现有方法采用统一表示模型（如高斯）难以处理动态场景中背景与物体的时空特征不匹配问题，尤其是帧成像导致的潜在不连续时间特征。", "method": "引入事件相机补充帧相机，提出时空解耦的高斯泼溅框架，通过聚类区分背景与物体的时空特征，并利用高斯表示与事件数据的一致性引导物体高斯解耦。", "result": "实验验证了该方法在高动态场景重建中的优越性，能够提升背景与物体的时空区分能力，实现时间连续的动态场景渲染。", "conclusion": "通过时空解耦和事件相机补偿，该方法有效解决了高动态场景重建中的时空特征不匹配问题，为动态场景表示提供了新思路。"}}
{"id": "2506.23189", "pdf": "https://arxiv.org/pdf/2506.23189", "abs": "https://arxiv.org/abs/2506.23189", "authors": ["Mustafa Hakan Kara", "Aysegul Dundar", "Uğur Güdükbay"], "title": "Trident: Detecting Face Forgeries with Adversarial Triplet Learning", "categories": ["cs.CV"], "comment": "11 pages, 3 figures, and 7 tables", "summary": "As face forgeries generated by deep neural networks become increasingly\nsophisticated, detecting face manipulations in digital media has posed a\nsignificant challenge, underscoring the importance of maintaining digital media\nintegrity and combating visual disinformation. Current detection models,\npredominantly based on supervised training with domain-specific data, often\nfalter against forgeries generated by unencountered techniques. In response to\nthis challenge, we introduce \\textit{Trident}, a face forgery detection\nframework that employs triplet learning with a Siamese network architecture for\nenhanced adaptability across diverse forgery methods. \\textit{Trident} is\ntrained on curated triplets to isolate nuanced differences of forgeries,\ncapturing fine-grained features that distinguish pristine samples from\nmanipulated ones while controlling for other variables. To further enhance\ngeneralizability, we incorporate domain-adversarial training with a forgery\ndiscriminator. This adversarial component guides our embedding model towards\nforgery-agnostic representations, improving its robustness to unseen\nmanipulations. In addition, we prevent gradient flow from the classifier head\nto the embedding model, avoiding overfitting induced by artifacts peculiar to\ncertain forgeries. Comprehensive evaluations across multiple benchmarks and\nablation studies demonstrate the effectiveness of our framework. We will\nrelease our code in a GitHub repository.", "AI": {"tldr": "提出了一种名为Trident的人脸伪造检测框架，通过三元组学习和Siamese网络架构提升对不同伪造方法的适应性，并结合领域对抗训练增强泛化能力。", "motivation": "随着深度神经网络生成的人脸伪造技术日益复杂，检测数字媒体中的人脸伪造成为重要挑战，现有基于监督学习的模型对未见过的伪造技术表现不佳。", "method": "采用三元组学习和Siamese网络架构，结合领域对抗训练和伪造判别器，避免过拟合。", "result": "在多个基准测试和消融研究中验证了框架的有效性。", "conclusion": "Trident框架在检测多样伪造方法时表现出色，具有较高的泛化能力。"}}
{"id": "2506.22703", "pdf": "https://arxiv.org/pdf/2506.22703", "abs": "https://arxiv.org/abs/2506.22703", "authors": ["Wali Mohammad Abdullah", "Azmain Kabir"], "title": "P4OMP: Retrieval-Augmented Prompting for OpenMP Parallelism in Serial Code", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "We present P4OMP, a retrieval-augmented framework for transforming serial\nC/C++ code into OpenMP-annotated parallel code using large language models\n(LLMs). To our knowledge, this is the first system to apply retrieval-based\nprompting for OpenMP pragma correctness without model fine-tuning or compiler\ninstrumentation. P4OMP leverages Retrieval-Augmented Generation (RAG) with\nstructured instructional knowledge from OpenMP tutorials to improve the\nreliability of prompt-driven code generation. By grounding generation in the\nretrieved context, P4OMP improves syntactic correctness compared to baseline\nprompting with GPT-3.5-Turbo. We evaluate P4OMP against a baseline,\nGPT-3.5-Turbo without retrieval, on a comprehensive benchmark of 108 real-world\nC++ programs drawn from Stack Overflow, PolyBench, and NAS benchmark suites.\nP4OMP achieves 100% compilation success on all parallelizable cases, while the\nbaseline fails to compile in 20 out of 108 cases. Six cases that rely on\nnon-random-access iterators or thread-unsafe constructs are excluded due to\nfundamental OpenMP limitations. A detailed analysis demonstrates how P4OMP\nconsistently avoids scoping errors, syntactic misuse, and invalid directive\ncombinations that commonly affect baseline-generated code. We further\ndemonstrate strong runtime scaling across seven compute-intensive benchmarks on\nan HPC cluster. P4OMP offers a robust, modular pipeline that significantly\nimproves the reliability and applicability of LLM-generated OpenMP code.", "AI": {"tldr": "P4OMP是一个基于检索增强的框架，利用大型语言模型（LLMs）将串行C/C++代码转换为OpenMP注释的并行代码，无需微调或编译器插桩。", "motivation": "解决传统方法在OpenMP注释生成中需要微调或编译器插桩的问题，提高代码生成的可靠性和适用性。", "method": "采用检索增强生成（RAG）技术，结合OpenMP教程的结构化知识，提升提示驱动的代码生成质量。", "result": "在108个真实C++程序测试中，P4OMP实现100%编译成功率，优于基线模型（GPT-3.5-Turbo）。", "conclusion": "P4OMP显著提升了LLM生成的OpenMP代码的可靠性和实用性，适用于高性能计算场景。"}}
{"id": "2506.23196", "pdf": "https://arxiv.org/pdf/2506.23196", "abs": "https://arxiv.org/abs/2506.23196", "authors": ["Mona Ahmadian", "Amir Shirian", "Frank Guerin", "Andrew Gilbert"], "title": "DEL: Dense Event Localization for Multi-modal Audio-Visual Understanding", "categories": ["cs.CV"], "comment": null, "summary": "Real-world videos often contain overlapping events and complex temporal\ndependencies, making multimodal interaction modeling particularly challenging.\nWe introduce DEL, a framework for dense semantic action localization, aiming to\naccurately detect and classify multiple actions at fine-grained temporal\nresolutions in long untrimmed videos. DEL consists of two key modules: the\nalignment of audio and visual features that leverage masked self-attention to\nenhance intra-mode consistency and a multimodal interaction refinement module\nthat models cross-modal dependencies across multiple scales, enabling\nhigh-level semantics and fine-grained details. Our method achieves\nstate-of-the-art performance on multiple real-world Temporal Action\nLocalization (TAL) datasets, UnAV-100, THUMOS14, ActivityNet 1.3, and\nEPIC-Kitchens-100, surpassing previous approaches with notable average mAP\ngains of +3.3%, +2.6%, +1.2%, +1.7% (verb), and +1.4% (noun), respectively.", "AI": {"tldr": "DEL框架通过多模态交互建模，在长视频中实现密集语义动作定位，性能优于现有方法。", "motivation": "现实世界视频中的重叠事件和复杂时间依赖关系使多模态交互建模具有挑战性。", "method": "DEL包含两个关键模块：音频和视觉特征对齐（利用掩码自注意力增强模态内一致性）和多模态交互细化模块（建模跨模态依赖关系）。", "result": "在多个TAL数据集上取得SOTA性能，平均mAP提升显著（如UnAV-100 +3.3%）。", "conclusion": "DEL通过多模态特征对齐和交互细化，显著提升了密集动作定位的准确性。"}}
{"id": "2506.22704", "pdf": "https://arxiv.org/pdf/2506.22704", "abs": "https://arxiv.org/abs/2506.22704", "authors": ["Sardar Fatooreh Bonabi", "Sarah Bana", "Tingting Nian", "Vijay Gurbaxani"], "title": "Beyond Code: The Multidimensional Impacts of Large Language Models in Software Development", "categories": ["econ.GN", "cs.AI", "q-fin.EC"], "comment": null, "summary": "Large language models (LLMs) are poised to significantly impact software\ndevelopment, especially in the Open-Source Software (OSS) sector. To understand\nthis impact, we first outline the mechanisms through which LLMs may influence\nOSS through code development, collaborative knowledge transfer, and skill\ndevelopment. We then empirically examine how LLMs affect OSS developers' work\nin these three key areas. Leveraging a natural experiment from a temporary\nChatGPT ban in Italy, we employ a Difference-in-Differences framework with\ntwo-way fixed effects to analyze data from all OSS developers on GitHub in\nthree similar countries, Italy, France, and Portugal, totaling 88,022 users. We\nfind that access to ChatGPT increases developer productivity by 6.4%, knowledge\nsharing by 9.6%, and skill acquisition by 8.4%. These benefits vary\nsignificantly by user experience level: novice developers primarily experience\nproductivity gains, whereas more experienced developers benefit more from\nimproved knowledge sharing and accelerated skill acquisition. In addition, we\nfind that LLM-assisted learning is highly context-dependent, with the greatest\nbenefits observed in technically complex, fragmented, or rapidly evolving\ncontexts. We show that the productivity effects of LLMs extend beyond direct\ncode generation to include enhanced collaborative learning and knowledge\nexchange among developers; dynamics that are essential for gaining a holistic\nunderstanding of LLMs' impact in OSS. Our findings offer critical managerial\nimplications: strategically deploying LLMs can accelerate novice developers'\nonboarding and productivity, empower intermediate developers to foster\nknowledge sharing and collaboration, and support rapid skill acquisition,\ntogether enhancing long-term organizational productivity and agility.", "AI": {"tldr": "研究发现，ChatGPT的使用显著提升了开源软件开发者的生产力、知识共享和技能获取，效果因开发者经验水平而异。", "motivation": "探讨大型语言模型（LLMs）如何通过代码开发、协作知识转移和技能发展影响开源软件（OSS）领域。", "method": "利用意大利临时禁止ChatGPT的自然实验，采用双重差分法和双向固定效应模型，分析GitHub上88,022名开发者的数据。", "result": "使用ChatGPT使开发者生产力提升6.4%，知识共享增加9.6%，技能获取提高8.4%，效果因经验和任务复杂性而异。", "conclusion": "LLMs的战略部署可加速新手开发者成长、促进协作学习，并支持快速技能获取，从而提升组织长期生产力和敏捷性。"}}
{"id": "2506.23202", "pdf": "https://arxiv.org/pdf/2506.23202", "abs": "https://arxiv.org/abs/2506.23202", "authors": ["Qilin Shu", "Qixian Zhang", "Qi Zhang", "Hongyun Zhang", "Duoqian Miao", "Cairong Zhao"], "title": "Transformer-Based Person Search with High-Frequency Augmentation and Multi-Wave Mixing", "categories": ["cs.CV"], "comment": null, "summary": "The person search task aims to locate a target person within a set of scene\nimages. In recent years, transformer-based models in this field have made some\nprogress. However, they still face three primary challenges: 1) the\nself-attention mechanism tends to suppress high-frequency components in the\nfeatures, which severely impacts model performance; 2) the computational cost\nof transformers is relatively high. To address these issues, we propose a novel\nHigh-frequency Augmentation and Multi-Wave mixing (HAMW) method for person\nsearch. HAMW is designed to enhance the discriminative feature extraction\ncapabilities of transformers while reducing computational overhead and\nimproving efficiency. Specifically, we develop a three-stage framework that\nprogressively optimizes both detection and re-identification performance. Our\nmodel enhances the perception of high-frequency features by learning from\naugmented inputs containing additional high-frequency components. Furthermore,\nwe replace the self-attention layers in the transformer with a strategy based\non multi-level Haar wavelet fusion to capture multi-scale features. This not\nonly lowers the computational complexity but also alleviates the suppression of\nhigh-frequency features and enhances the ability to exploit multi-scale\ninformation. Extensive experiments demonstrate that HAMW achieves\nstate-of-the-art performance on both the CUHK-SYSU and PRW datasets.", "AI": {"tldr": "提出HAMW方法，通过高频增强和多级小波融合改进transformer模型，解决高频特征抑制和计算成本高的问题，在CUHK-SYSU和PRW数据集上达到最优性能。", "motivation": "transformer模型在人物搜索任务中存在高频特征抑制和计算成本高的问题，影响了性能。", "method": "提出HAMW方法，包括高频增强输入和多级Haar小波融合策略，分三阶段优化检测和重识别性能。", "result": "在CUHK-SYSU和PRW数据集上实现了最先进的性能。", "conclusion": "HAMW有效提升了transformer模型的高频特征提取能力，降低了计算成本，提高了效率。"}}
{"id": "2506.22706", "pdf": "https://arxiv.org/pdf/2506.22706", "abs": "https://arxiv.org/abs/2506.22706", "authors": ["Arun Ramamurthy", "Neil Dhir"], "title": "General Autonomous Cybersecurity Defense: Learning Robust Policies for Dynamic Topologies and Diverse Attackers", "categories": ["cs.CR", "cs.AI", "cs.CV", "stat.ML"], "comment": null, "summary": "In the face of evolving cyber threats such as malware, ransomware and\nphishing, autonomous cybersecurity defense (ACD) systems have become essential\nfor real-time threat detection and response with optional human intervention.\nHowever, existing ACD systems rely on limiting assumptions, particularly the\nstationarity of the underlying network dynamics. In real-world scenarios,\nnetwork topologies can change due to actions taken by attackers or defenders,\nsystem failures, or time evolution of networks, leading to failures in the\nadaptive capabilities of current defense agents. Moreover, many agents are\ntrained on static environments, resulting in overfitting to specific\ntopologies, which hampers their ability to generalize to out-of-distribution\nnetwork topologies. This work addresses these challenges by exploring methods\nfor developing agents to learn generalizable policies across dynamic network\nenvironments -- general ACD (GACD).", "AI": {"tldr": "本文提出了一种通用自主网络安全防御（GACD）方法，旨在解决现有系统因网络动态变化导致的适应性不足问题。", "motivation": "现有自主网络安全防御（ACD）系统依赖静态网络假设，无法适应动态变化的网络拓扑，导致防御能力受限。", "method": "通过探索动态网络环境中的策略学习方法，开发能够适应不同网络拓扑的通用防御代理（GACD）。", "result": "GACD能够学习通用策略，适应动态网络环境，提升防御能力。", "conclusion": "GACD为解决动态网络环境下的网络安全防御提供了有效方法。"}}
{"id": "2506.23219", "pdf": "https://arxiv.org/pdf/2506.23219", "abs": "https://arxiv.org/abs/2506.23219", "authors": ["Jie Feng", "Shengyuan Wang", "Tianhui Liu", "Yanxin Xi", "Yong Li"], "title": "UrbanLLaVA: A Multi-modal Large Language Model for Urban Intelligence with Spatial Reasoning and Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Accepted by ICCV 2025", "summary": "Urban research involves a wide range of scenarios and tasks that require the\nunderstanding of multi-modal data. Current methods often focus on specific data\ntypes and lack a unified framework in urban field for processing them\ncomprehensively. The recent success of multi-modal large language models\n(MLLMs) presents a promising opportunity to overcome this limitation. In this\npaper, we introduce $\\textit{UrbanLLaVA}$, a multi-modal large language model\ndesigned to process these four types of data simultaneously and achieve strong\nperformance across diverse urban tasks compared with general MLLMs. In\n$\\textit{UrbanLLaVA}$, we first curate a diverse urban instruction dataset\nencompassing both single-modal and cross-modal urban data, spanning from\nlocation view to global view of urban environment. Additionally, we propose a\nmulti-stage training framework that decouples spatial reasoning enhancement\nfrom domain knowledge learning, thereby improving the compatibility and\ndownstream performance of $\\textit{UrbanLLaVA}$ across diverse urban tasks.\nFinally, we also extend existing benchmark for urban research to assess the\nperformance of MLLMs across a wide range of urban tasks. Experimental results\nfrom three cities demonstrate that $\\textit{UrbanLLaVA}$ outperforms\nopen-source and proprietary MLLMs in both single-modal tasks and complex\ncross-modal tasks and shows robust generalization abilities across cities.\nSource codes and data are openly accessible to the research community via\nhttps://github.com/tsinghua-fib-lab/UrbanLLaVA.", "AI": {"tldr": "论文提出了一种多模态大语言模型UrbanLLaVA，用于统一处理城市研究中的多模态数据，并在多种城市任务中表现优异。", "motivation": "当前城市研究方法通常局限于特定数据类型，缺乏统一框架，多模态大语言模型（MLLMs）为解决这一问题提供了可能。", "method": "通过构建多样化的城市指令数据集，并提出多阶段训练框架，分离空间推理增强与领域知识学习，提升模型性能。", "result": "实验证明UrbanLLaVA在单模态和跨模态任务中均优于开源和专有MLLMs，并展现出跨城市的泛化能力。", "conclusion": "UrbanLLaVA为城市研究提供了一个高效的多模态处理框架，推动了该领域的进展。"}}
{"id": "2506.23205", "pdf": "https://arxiv.org/pdf/2506.23205", "abs": "https://arxiv.org/abs/2506.23205", "authors": ["Dequan Kong", "Zhe Zhu", "Honghua Chen", "Mingqiang Wei"], "title": "BridgeShape: Latent Diffusion Schrödinger Bridge for 3D Shape Completion", "categories": ["cs.CV"], "comment": null, "summary": "Existing diffusion-based 3D shape completion methods typically use a\nconditional paradigm, injecting incomplete shape information into the denoising\nnetwork via deep feature interactions (e.g., concatenation, cross-attention) to\nguide sampling toward complete shapes, often represented by voxel-based\ndistance functions. However, these approaches fail to explicitly model the\noptimal global transport path, leading to suboptimal completions. Moreover,\nperforming diffusion directly in voxel space imposes resolution constraints,\nlimiting the generation of fine-grained geometric details. To address these\nchallenges, we propose BridgeShape, a novel framework for 3D shape completion\nvia latent diffusion Schr\\\"odinger bridge. The key innovations lie in two\naspects: (i) BridgeShape formulates shape completion as an optimal transport\nproblem, explicitly modeling the transition between incomplete and complete\nshapes to ensure a globally coherent transformation. (ii) We introduce a\nDepth-Enhanced Vector Quantized Variational Autoencoder (VQ-VAE) to encode 3D\nshapes into a compact latent space, leveraging self-projected multi-view depth\ninformation enriched with strong DINOv2 features to enhance geometric\nstructural perception. By operating in a compact yet structurally informative\nlatent space, BridgeShape effectively mitigates resolution constraints and\nenables more efficient and high-fidelity 3D shape completion. BridgeShape\nachieves state-of-the-art performance on large-scale 3D shape completion\nbenchmarks, demonstrating superior fidelity at higher resolutions and for\nunseen object classes.", "AI": {"tldr": "BridgeShape提出了一种基于潜在扩散Schrödinger桥的3D形状补全框架，通过最优传输路径和深度增强的VQ-VAE编码，解决了现有方法在全局一致性和分辨率限制上的问题。", "motivation": "现有基于扩散的3D形状补全方法通常通过深度特征交互注入不完整形状信息，但未能明确建模最优全局传输路径，且受限于体素空间的分辨率约束。", "method": "BridgeShape将形状补全建模为最优传输问题，并引入深度增强的VQ-VAE编码3D形状到紧凑潜在空间，利用多视角深度信息和DINOv2特征增强几何结构感知。", "result": "BridgeShape在大规模3D形状补全基准测试中达到最先进性能，支持更高分辨率和未见物体类别的补全。", "conclusion": "BridgeShape通过潜在扩散和最优传输路径实现了高效且高保真的3D形状补全，解决了现有方法的局限性。"}}
{"id": "2506.23225", "pdf": "https://arxiv.org/pdf/2506.23225", "abs": "https://arxiv.org/abs/2506.23225", "authors": ["Yukito Tajima", "Nakamasa Inoue", "Yusuke Sekikawa", "Ikuro Sato", "Rio Yokota"], "title": "Masked Gated Linear Unit", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Gated Linear Units (GLUs) have become essential components in the\nfeed-forward networks of state-of-the-art Large Language Models (LLMs).\nHowever, they require twice as many memory reads compared to feed-forward\nlayers without gating, due to the use of separate weight matrices for the gate\nand value streams. To address this bottleneck, we introduce Masked Gated Linear\nUnits (MGLUs), a novel family of GLUs with an efficient kernel implementation.\nThe core contribution of MGLUs include: (1) the Mixture of Element-wise Gating\n(MoEG) architecture that learns multiple binary masks, each determining gate or\nvalue assignments at the element level on a single shared weight matrix\nresulting in reduced memory transfer, and (2) FlashMGLU, a hardware-friendly\nkernel that yields up to a 19.7 $\\times$ inference-time speed-up over a naive\nPyTorch MGLU and is 47% more memory-efficient and 34% faster than standard GLUs\ndespite added architectural complexity on an RTX5090 GPU. In LLM experiments,\nthe Swish-activated variant SwiMGLU preserves its memory advantages while\nmatching - or even surpassing - the downstream accuracy of the SwiGLU baseline.", "AI": {"tldr": "论文提出了Masked Gated Linear Units (MGLUs)，一种高效的GLU变体，通过共享权重矩阵和硬件优化，显著减少了内存读取和计算时间。", "motivation": "GLUs在大型语言模型中广泛使用，但其双权重矩阵设计导致内存读取效率低下，成为性能瓶颈。", "method": "提出MGLUs，包括MoEG架构（学习二进制掩码以共享权重矩阵）和FlashMGLU内核（硬件优化实现）。", "result": "实验显示，MGLUs在RTX5090 GPU上比标准GLUs快34%，内存效率提高47%，且SwiMGLU在精度上不逊于SwiGLU。", "conclusion": "MGLUs通过高效设计和硬件优化，显著提升了GLUs的性能和内存效率，适用于现代LLMs。"}}
{"id": "2506.23207", "pdf": "https://arxiv.org/pdf/2506.23207", "abs": "https://arxiv.org/abs/2506.23207", "authors": ["Zhen Tan", "Xieyuanli Chen", "Lei Feng", "Yangbing Ge", "Shuaifeng Zhi", "Jiaxiong Liu", "Dewen Hu"], "title": "TVG-SLAM: Robust Gaussian Splatting SLAM with Tri-view Geometric Constraints", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled RGB-only SLAM\nsystems to achieve high-fidelity scene representation. However, the heavy\nreliance of existing systems on photometric rendering loss for camera tracking\nundermines their robustness, especially in unbounded outdoor environments with\nsevere viewpoint and illumination changes. To address these challenges, we\npropose TVG-SLAM, a robust RGB-only 3DGS SLAM system that leverages a novel\ntri-view geometry paradigm to ensure consistent tracking and high-quality\nmapping. We introduce a dense tri-view matching module that aggregates reliable\npairwise correspondences into consistent tri-view matches, forming robust\ngeometric constraints across frames. For tracking, we propose Hybrid Geometric\nConstraints, which leverage tri-view matches to construct complementary\ngeometric cues alongside photometric loss, ensuring accurate and stable pose\nestimation even under drastic viewpoint shifts and lighting variations. For\nmapping, we propose a new probabilistic initialization strategy that encodes\ngeometric uncertainty from tri-view correspondences into newly initialized\nGaussians. Additionally, we design a Dynamic Attenuation of Rendering Trust\nmechanism to mitigate tracking drift caused by mapping latency. Experiments on\nmultiple public outdoor datasets show that our TVG-SLAM outperforms prior\nRGB-only 3DGS-based SLAM systems. Notably, in the most challenging dataset, our\nmethod improves tracking robustness, reducing the average Absolute Trajectory\nError (ATE) by 69.0\\% while achieving state-of-the-art rendering quality. The\nimplementation of our method will be released as open-source.", "AI": {"tldr": "TVG-SLAM提出了一种基于三视图几何的RGB-only SLAM系统，通过结合几何约束和光度损失，提高了在复杂户外环境中的跟踪和建图鲁棒性。", "motivation": "现有3DGS SLAM系统过度依赖光度渲染损失，导致在户外无边界环境中因视角和光照变化而鲁棒性不足。", "method": "引入三视图匹配模块和混合几何约束，结合概率初始化策略和动态衰减渲染信任机制。", "result": "在多个户外数据集上表现优异，显著降低了轨迹误差（ATE降低69.0%），并达到最佳渲染质量。", "conclusion": "TVG-SLAM通过几何约束和动态机制显著提升了RGB-only SLAM的鲁棒性和性能。"}}
{"id": "2506.22722", "pdf": "https://arxiv.org/pdf/2506.22722", "abs": "https://arxiv.org/abs/2506.22722", "authors": ["Anmin Fu", "Fanyu Meng", "Huaibing Peng", "Hua Ma", "Zhi Zhang", "Yifeng Zheng", "Willy Susilo", "Yansong Gao"], "title": "Kill Two Birds with One Stone! Trajectory enabled Unified Online Detection of Adversarial Examples and Backdoor Attacks", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "The proposed UniGuard is the first unified online detection framework capable\nof simultaneously addressing adversarial examples and backdoor attacks.\nUniGuard builds upon two key insights: first, both AE and backdoor attacks have\nto compromise the inference phase, making it possible to tackle them\nsimultaneously during run-time via online detection. Second, an adversarial\ninput, whether a perturbed sample in AE attacks or a trigger-carrying sample in\nbackdoor attacks, exhibits distinctive trajectory signatures from a benign\nsample as it propagates through the layers of a DL model in forward inference.\nThe propagation trajectory of the adversarial sample must deviate from that of\nits benign counterpart; otherwise, the adversarial objective cannot be\nfulfilled. Detecting these trajectory signatures is inherently challenging due\nto their subtlety; UniGuard overcomes this by treating the propagation\ntrajectory as a time-series signal, leveraging LSTM and spectrum transformation\nto amplify differences between adversarial and benign trajectories that are\nsubtle in the time domain. UniGuard exceptional efficiency and effectiveness\nhave been extensively validated across various modalities (image, text, and\naudio) and tasks (classification and regression), ranging from diverse model\narchitectures against a wide range of AE attacks and backdoor attacks,\nincluding challenging partial backdoors and dynamic triggers. When compared to\nSOTA methods, including ContraNet (NDSS 22) specific for AE detection and TED\n(IEEE SP 24) specific for backdoor detection, UniGuard consistently\ndemonstrates superior performance, even when matched against each method's\nstrengths in addressing their respective threats-each SOTA fails to parts of\nattack strategies while UniGuard succeeds for all.", "AI": {"tldr": "UniGuard是一个统一的在线检测框架，能够同时检测对抗性示例和后门攻击，通过分析输入在深度学习模型中的传播轨迹差异来实现高效检测。", "motivation": "对抗性示例（AE）和后门攻击都会影响模型的推理阶段，因此需要一种统一的方法在运行时同时检测这两种攻击。", "method": "UniGuard将输入在模型中的传播轨迹视为时间序列信号，利用LSTM和频谱变换放大对抗性和良性轨迹之间的细微差异。", "result": "UniGuard在多种模态（图像、文本、音频）和任务（分类和回归）中表现出色，优于现有的针对单一攻击的先进方法。", "conclusion": "UniGuard是首个能够统一检测对抗性示例和后门攻击的框架，其高效性和广泛适用性使其成为解决多种威胁的理想选择。"}}
{"id": "2506.23209", "pdf": "https://arxiv.org/pdf/2506.23209", "abs": "https://arxiv.org/abs/2506.23209", "authors": ["Chia-Wen Huang", "Haw Hwai", "Chien-Chang Lee", "Pei-Yuan Wu"], "title": "A Hierarchical Slice Attention Network for Appendicitis Classification in 3D CT Scans", "categories": ["cs.CV"], "comment": "8 pages, 1 figure, 3 tables. Published in IEEE ISBI 2025. This\n  version corrects citation numbering errors", "summary": "Timely and accurate diagnosis of appendicitis is critical in clinical\nsettings to prevent serious complications. While CT imaging remains the\nstandard diagnostic tool, the growing number of cases can overwhelm\nradiologists, potentially causing delays. In this paper, we propose a deep\nlearning model that leverages 3D CT scans for appendicitis classification,\nincorporating Slice Attention mechanisms guided by external 2D datasets to\nenhance small lesion detection. Additionally, we introduce a hierarchical\nclassification framework using pre-trained 2D models to differentiate between\nsimple and complicated appendicitis. Our approach improves AUC by 3% for\nappendicitis and 5.9% for complicated appendicitis, offering a more efficient\nand reliable diagnostic solution compared to previous work.", "AI": {"tldr": "提出了一种基于3D CT扫描和切片注意力机制的深度学习模型，用于阑尾炎分类，并通过分层分类框架区分简单和复杂阑尾炎，性能显著提升。", "motivation": "阑尾炎的及时准确诊断至关重要，但CT影像诊断可能因病例增多而延迟，需更高效的解决方案。", "method": "利用3D CT扫描和切片注意力机制，结合外部2D数据集增强小病变检测；引入分层分类框架，使用预训练2D模型区分简单和复杂阑尾炎。", "result": "阑尾炎分类AUC提升3%，复杂阑尾炎AUC提升5.9%。", "conclusion": "该方法比现有工作更高效可靠，为临床诊断提供了更好的解决方案。"}}
{"id": "2506.22742", "pdf": "https://arxiv.org/pdf/2506.22742", "abs": "https://arxiv.org/abs/2506.22742", "authors": ["Wali Mohammad Abdullah", "Md. Morshedul Islam", "Devraj Parmar", "Happy Hasmukhbhai Patel", "Sindhuja Prabhakaran", "Baidya Saha"], "title": "RAILS: Retrieval-Augmented Intelligence for Learning Software Development", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) like GPT-3.5-Turbo are increasingly used to\nassist software development, yet they often produce incomplete code or\nincorrect imports, especially when lacking access to external or\nproject-specific documentation. We introduce RAILS (Retrieval-Augmented\nIntelligence for Learning Software Development), a framework that augments LLM\nprompts with semantically retrieved context from curated Java resources using\nFAISS and OpenAI embeddings. RAILS incorporates an iterative validation loop\nguided by compiler feedback to refine suggestions. We evaluated RAILS on 78\nreal-world Java import error cases spanning standard libraries, GUI APIs,\nexternal tools, and custom utilities. Despite using the same LLM, RAILS\noutperforms baseline prompting by preserving intent, avoiding hallucinations,\nand surfacing correct imports even when libraries are unavailable locally.\nFuture work will integrate symbolic filtering via PostgreSQL and extend support\nto other languages and IDEs.", "AI": {"tldr": "RAILS框架通过检索增强和编译器反馈优化LLM生成的代码，显著提高了Java导入准确性。", "motivation": "LLM在软件开发中常生成不完整或错误的代码，尤其是缺乏外部或项目特定文档时。", "method": "RAILS结合FAISS和OpenAI嵌入，从Java资源中检索语义上下文，并通过编译器反馈迭代验证。", "result": "在78个真实Java导入错误案例中，RAILS优于基线方法，能保持意图并避免幻觉。", "conclusion": "未来将扩展RAILS支持更多语言和IDE，并集成符号过滤。"}}
{"id": "2506.23322", "pdf": "https://arxiv.org/pdf/2506.23322", "abs": "https://arxiv.org/abs/2506.23322", "authors": ["Wei Zhou", "Ji Sun", "Xuanhe Zhou", "Guoliang Li", "Luyang Liu", "Hao Wu", "Tianyuan Wang"], "title": "GaussMaster: An LLM-based Database Copilot System", "categories": ["cs.DB", "cs.AI", "cs.CL", "cs.IR"], "comment": "We welcome contributions from the community. For reference, please\n  see the code at: https://gitcode.com/opengauss/openGauss-GaussMaster", "summary": "In the financial industry, data is the lifeblood of operations, and DBAs\nshoulder significant responsibilities for SQL tuning, database deployment,\ndiagnosis, and service repair. In recent years, both database vendors and\ncustomers have increasingly turned to autonomous database platforms in an\neffort to alleviate the heavy workload of DBAs. However, existing autonomous\ndatabase platforms are limited in their capabilities, primarily addressing\nsingle-point issues such as NL2SQL, anomaly detection, and SQL tuning. Manual\nintervention remains a necessity for comprehensive database maintenance.\nGaussMaster aims to revolutionize this landscape by introducing an LLM-based\ndatabase copilot system. This innovative solution is designed not only to\nassist developers in writing efficient SQL queries but also to provide\ncomprehensive care for database services. When database instances exhibit\nabnormal behavior, GaussMaster is capable of orchestrating the entire\nmaintenance process automatically. It achieves this by analyzing hundreds of\nmetrics and logs, employing a Tree-of-thought approach to identify root causes,\nand invoking appropriate tools to resolve issues. We have successfully\nimplemented GaussMaster in real-world scenarios, such as the banking industry,\nwhere it has achieved zero human intervention for over 34 database maintenance\nscenarios. In this paper, we present significant improvements in these tasks\nwith code at https://gitcode.com/opengauss/openGauss-GaussMaster.", "AI": {"tldr": "GaussMaster是一个基于LLM的数据库助手系统，旨在全面自动化数据库维护，减少人工干预。", "motivation": "金融行业中数据库管理员（DBA）工作繁重，现有自治数据库平台功能有限，仍需人工干预。", "method": "GaussMaster通过分析数百个指标和日志，采用Tree-of-thought方法识别根本原因，并调用工具解决问题。", "result": "在银行业等实际场景中，GaussMaster实现了34种数据库维护场景的零人工干预。", "conclusion": "GaussMaster显著提升了数据库维护的自动化水平，代码已开源。"}}
{"id": "2506.22771", "pdf": "https://arxiv.org/pdf/2506.22771", "abs": "https://arxiv.org/abs/2506.22771", "authors": ["Jingxiao Ma", "Priyadarshini Panda", "Sherief Reda"], "title": "FF-INT8: Efficient Forward-Forward DNN Training on Edge Devices with INT8 Precision", "categories": ["cs.LG", "cs.AI", "cs.NE", "I.2.0; I.2.6"], "comment": "To be published in the 62nd Design Automation Conference (DAC), 2025", "summary": "Backpropagation has been the cornerstone of neural network training for\ndecades, yet its inefficiencies in time and energy consumption limit its\nsuitability for resource-constrained edge devices. While low-precision neural\nnetwork quantization has been extensively researched to speed up model\ninference, its application in training has been less explored. Recently, the\nForward-Forward (FF) algorithm has emerged as a promising alternative to\nbackpropagation, replacing the backward pass with an additional forward pass.\nBy avoiding the need to store intermediate activations for backpropagation, FF\ncan reduce memory footprint, making it well-suited for embedded devices. This\npaper presents an INT8 quantized training approach that leverages FF's\nlayer-by-layer strategy to stabilize gradient quantization. Furthermore, we\npropose a novel \"look-ahead\" scheme to address limitations of FF and improve\nmodel accuracy. Experiments conducted on NVIDIA Jetson Orin Nano board\ndemonstrate 4.6% faster training, 8.3% energy savings, and 27.0% reduction in\nmemory usage, while maintaining competitive accuracy compared to the\nstate-of-the-art.", "AI": {"tldr": "论文提出了一种基于INT8量化的Forward-Forward训练方法，通过改进梯度量化稳定性和引入“前瞻”方案，显著提升了训练速度、能效和内存利用率。", "motivation": "反向传播在资源受限的边缘设备上存在时间和能效问题，而低精度量化在训练中的应用较少。Forward-Forward算法因其减少内存占用的潜力成为替代方案。", "method": "采用INT8量化训练，结合Forward-Forward的逐层策略稳定梯度量化，并引入“前瞻”方案提升模型精度。", "result": "在NVIDIA Jetson Orin Nano上实验显示，训练速度提升4.6%，能耗降低8.3%，内存使用减少27.0%，同时保持竞争力精度。", "conclusion": "该方法为边缘设备提供了一种高效、低内存占用的训练方案，具有实际应用潜力。"}}
{"id": "2506.23366", "pdf": "https://arxiv.org/pdf/2506.23366", "abs": "https://arxiv.org/abs/2506.23366", "authors": ["Nathaniel Imel", "Zachary Hafen"], "title": "Density, asymmetry and citation dynamics in scientific literature", "categories": ["cs.DL", "cs.CL", "cs.SI"], "comment": null, "summary": "Scientific behavior is often characterized by a tension between building upon\nestablished knowledge and introducing novel ideas. Here, we investigate whether\nthis tension is reflected in the relationship between the similarity of a\nscientific paper to previous research and its eventual citation rate. To\noperationalize similarity to previous research, we introduce two complementary\nmetrics to characterize the local geometry of a publication's semantic\nneighborhood: (1) \\emph{density} ($\\rho$), defined as the ratio between a fixed\nnumber of previously-published papers and the minimum distance enclosing those\npapers in a semantic embedding space, and (2) asymmetry ($\\alpha$), defined as\nthe average directional difference between a paper and its nearest neighbors.\nWe tested the predictive relationship between these two metrics and its\nsubsequent citation rate using a Bayesian hierarchical regression approach,\nsurveying $\\sim 53,000$ publications across nine academic disciplines and five\ndifferent document embeddings. While the individual effects of $\\rho$ on\ncitation count are small and variable, incorporating density-based predictors\nconsistently improves out-of-sample prediction when added to baseline models.\nThese results suggest that the density of a paper's surrounding scientific\nliterature may carry modest but informative signals about its eventual impact.\nMeanwhile, we find no evidence that publication asymmetry improves model\npredictions of citation rates. Our work provides a scalable framework for\nlinking document embeddings to scientometric outcomes and highlights new\nquestions regarding the role that semantic similarity plays in shaping the\ndynamics of scientific reward.", "AI": {"tldr": "论文探讨科学论文与先前研究的相似性与其引用率的关系，引入密度和不对称性两个指标，发现密度对引用率有微小但信息性的影响，而不对称性无显著作用。", "motivation": "研究科学行为中继承与创新之间的张力，探索论文与先前研究的相似性如何影响其引用率。", "method": "引入密度（ρ）和不对称性（α）两个指标，使用贝叶斯分层回归分析约53,000篇论文的引用率。", "result": "密度对引用率有微小但信息性的影响，不对称性无显著作用。", "conclusion": "论文密度可能对科学影响力有预测作用，不对称性无显著影响，为科学奖励动态研究提供了新视角。"}}
{"id": "2506.23227", "pdf": "https://arxiv.org/pdf/2506.23227", "abs": "https://arxiv.org/abs/2506.23227", "authors": ["Lunhao Duan", "Shanshan Zhao", "Xingxing Weng", "Jing Zhang", "Gui-Song Xia"], "title": "High-quality Pseudo-labeling for Point Cloud Segmentation with Scene-level Annotation", "categories": ["cs.CV"], "comment": "Accepted by TPAMI. Code: https://github.com/LHDuan/WSegPC", "summary": "This paper investigates indoor point cloud semantic segmentation under\nscene-level annotation, which is less explored compared to methods relying on\nsparse point-level labels. In the absence of precise point-level labels,\ncurrent methods first generate point-level pseudo-labels, which are then used\nto train segmentation models. However, generating accurate pseudo-labels for\neach point solely based on scene-level annotations poses a considerable\nchallenge, substantially affecting segmentation performance. Consequently, to\nenhance accuracy, this paper proposes a high-quality pseudo-label generation\nframework by exploring contemporary multi-modal information and region-point\nsemantic consistency. Specifically, with a cross-modal feature guidance module,\nour method utilizes 2D-3D correspondences to align point cloud features with\ncorresponding 2D image pixels, thereby assisting point cloud feature learning.\nTo further alleviate the challenge presented by the scene-level annotation, we\nintroduce a region-point semantic consistency module. It produces regional\nsemantics through a region-voting strategy derived from point-level semantics,\nwhich are subsequently employed to guide the point-level semantic predictions.\nLeveraging the aforementioned modules, our method can rectify inaccurate\npoint-level semantic predictions during training and obtain high-quality\npseudo-labels. Significant improvements over previous works on ScanNet v2 and\nS3DIS datasets under scene-level annotation can demonstrate the effectiveness.\nAdditionally, comprehensive ablation studies validate the contributions of our\napproach's individual components. The code is available at\nhttps://github.com/LHDuan/WSegPC .", "AI": {"tldr": "本文提出了一种基于场景级标注的室内点云语义分割方法，通过多模态信息和区域-点语义一致性生成高质量伪标签，显著提升了分割性能。", "motivation": "当前方法在缺乏精确点级标注时，依赖伪标签训练分割模型，但伪标签生成准确性不足影响性能。本文旨在解决这一问题。", "method": "提出跨模态特征引导模块和区域-点语义一致性模块，利用2D-3D对应关系和区域投票策略生成高质量伪标签。", "result": "在ScanNet v2和S3DIS数据集上显著优于现有方法，消融实验验证了各模块的有效性。", "conclusion": "通过多模态信息和语义一致性模块，本文方法能有效提升场景级标注下的点云语义分割性能。"}}
{"id": "2506.22776", "pdf": "https://arxiv.org/pdf/2506.22776", "abs": "https://arxiv.org/abs/2506.22776", "authors": ["Sen Fang", "Weiyuan Ding", "Antonio Mastropaolo", "Bowen Xu"], "title": "Smaller = Weaker? Benchmarking Robustness of Quantized LLMs in Code Generation", "categories": ["cs.SE", "cs.AI", "cs.PL"], "comment": "13 pages, 6 figures", "summary": "Quantization has emerged as a mainstream method for compressing Large\nLanguage Models (LLMs), reducing memory requirements and accelerating inference\nwithout architectural modifications. While existing research primarily focuses\non evaluating the effectiveness of quantized LLMs compared to their original\ncounterparts, the impact on robustness remains largely unexplored.In this\npaper, we present the first systematic investigation of how quantization\naffects the robustness of LLMs in code generation tasks. Through extensive\nexperiments across four prominent LLM families (LLaMA, DeepSeek, CodeGen, and\nStarCoder) with parameter scales ranging from 350M to 33B, we evaluate\nrobustness from dual perspectives: adversarial attacks on input prompts and\nnoise perturbations on model architecture. Our findings challenge conventional\nwisdom by demonstrating that quantized LLMs often exhibit superior robustness\ncompared to their full-precision counterparts, with 51.59% versus 42.86% of our\nadversarial experiments showing better resilience in quantized LLMs. Similarly,\nour noise perturbation experiments also confirm that LLMs after quantitation\ngenerally withstand higher levels of weight disturbances. These results suggest\nthat quantization not only reduces computational requirements but can actually\nenhance LLMs' reliability in code generation tasks, providing valuable insights\nfor developing more robust and efficient LLM deployment strategies.", "AI": {"tldr": "量化技术能压缩大语言模型（LLM）并提升其鲁棒性，尤其在代码生成任务中表现优于全精度模型。", "motivation": "量化对LLM鲁棒性的影响尚未被充分研究，本文旨在填补这一空白。", "method": "通过对抗攻击和噪声扰动实验，评估四种LLM家族（LLaMA、DeepSeek、CodeGen、StarCoder）的量化效果。", "result": "量化后的LLM在51.59%的对抗实验中表现更优，且能承受更高的权重扰动。", "conclusion": "量化不仅能降低计算需求，还能增强LLM在代码生成任务中的可靠性。"}}
{"id": "2506.23367", "pdf": "https://arxiv.org/pdf/2506.23367", "abs": "https://arxiv.org/abs/2506.23367", "authors": ["Paige Tuttösí", "H. Henny Yeung", "Yue Wang", "Jean-Julien Aucouturier", "Angelica Lim"], "title": "You Sound a Little Tense: L2 Tailored Clear TTS Using Durational Vowel Properties", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to ISCA Speech Synthesis Workshop, 2025", "summary": "We present the first text-to-speech (TTS) system tailored to second language\n(L2) speakers. We use duration differences between American English tense\n(longer) and lax (shorter) vowels to create a \"clarity mode\" for Matcha-TTS.\nOur perception studies showed that French-L1, English-L2 listeners had fewer\n(at least 9.15%) transcription errors when using our clarity mode, and found it\nmore encouraging and respectful than overall slowed down speech. Remarkably,\nlisteners were not aware of these effects: despite the decreased word error\nrate in clarity mode, listeners still believed that slowing all target words\nwas the most intelligible, suggesting that actual intelligibility does not\ncorrelate with perceived intelligibility. Additionally, we found that\nWhisper-ASR did not use the same cues as L2 speakers to differentiate difficult\nvowels and is not sufficient to assess the intelligibility of TTS systems for\nthese individuals.", "AI": {"tldr": "本文提出了一种针对第二语言（L2）学习者的文本转语音（TTS）系统，通过调整元音时长提升清晰度，实验表明其效果优于整体语速减慢，但学习者对此效果无感知。", "motivation": "针对第二语言学习者在语音识别中的困难，尤其是元音区分的挑战，设计更有效的TTS系统以提高其理解能力。", "method": "利用美式英语中紧元音（较长）和松元音（较短）的时长差异，在Matcha-TTS中创建“清晰模式”。", "result": "法语母语的英语L2学习者在清晰模式下的转录错误减少至少9.15%，且认为该模式更鼓舞人心和尊重，但未意识到其实际效果。", "conclusion": "实际清晰度与感知清晰度不相关，且现有ASR系统（如Whisper-ASR）无法准确评估L2学习者的语音理解需求。"}}
{"id": "2506.23236", "pdf": "https://arxiv.org/pdf/2506.23236", "abs": "https://arxiv.org/abs/2506.23236", "authors": ["Marko Mihajlovic", "Siwei Zhang", "Gen Li", "Kaifeng Zhao", "Lea Müller", "Siyu Tang"], "title": "VolumetricSMPL: A Neural Volumetric Body Model for Efficient Interactions, Contacts, and Collisions", "categories": ["cs.CV", "cs.AI"], "comment": "[ICCV 2025] https://markomih.github.io/VolumetricSMPL", "summary": "Parametric human body models play a crucial role in computer graphics and\nvision, enabling applications ranging from human motion analysis to\nunderstanding human-environment interactions. Traditionally, these models use\nsurface meshes, which pose challenges in efficiently handling interactions with\nother geometric entities, such as objects and scenes, typically represented as\nmeshes or point clouds. To address this limitation, recent research has\nexplored volumetric neural implicit body models. However, existing works are\neither insufficiently robust for complex human articulations or impose high\ncomputational and memory costs, limiting their widespread use. To this end, we\nintroduce VolumetricSMPL, a neural volumetric body model that leverages Neural\nBlend Weights (NBW) to generate compact, yet efficient MLP decoders. Unlike\nprior approaches that rely on large MLPs, NBW dynamically blends a small set of\nlearned weight matrices using predicted shape- and pose-dependent coefficients,\nsignificantly improving computational efficiency while preserving\nexpressiveness. VolumetricSMPL outperforms prior volumetric occupancy model\nCOAP with 10x faster inference, 6x lower GPU memory usage, enhanced accuracy,\nand a Signed Distance Function (SDF) for efficient and differentiable contact\nmodeling. We demonstrate VolumetricSMPL's strengths across four challenging\ntasks: (1) reconstructing human-object interactions from in-the-wild images,\n(2) recovering human meshes in 3D scenes from egocentric views, (3)\nscene-constrained motion synthesis, and (4) resolving self-intersections. Our\nresults highlight its broad applicability and significant performance and\nefficiency gains.", "AI": {"tldr": "VolumetricSMPL是一种基于神经混合权重（NBW）的神经体积人体模型，显著提高了计算效率和表达能力，适用于多种复杂任务。", "motivation": "传统人体模型使用表面网格，处理与其他几何实体的交互效率低，现有体积神经隐式模型在复杂人体动作或计算成本上存在不足。", "method": "利用NBW动态混合少量学习权重矩阵，生成紧凑高效的MLP解码器，替代传统大MLP。", "result": "相比COAP模型，推理速度快10倍，GPU内存使用降低6倍，精度更高，并支持SDF接触建模。", "conclusion": "VolumetricSMPL在多种任务中表现优异，具有广泛的应用前景和显著的性能提升。"}}
{"id": "2506.23394", "pdf": "https://arxiv.org/pdf/2506.23394", "abs": "https://arxiv.org/abs/2506.23394", "authors": ["Simeon Emanuilov"], "title": "Teaching a Language Model to Speak the Language of Tools", "categories": ["cs.IR", "cs.AI", "cs.CL", "I.2.7; I.2.1"], "comment": null, "summary": "External tool integration through function-calling is essential for practical\nlanguage model applications, yet most multilingual models lack reliable\ntool-use capabilities in non-English languages. Even state-of-the-art\nmultilingual models struggle with determining when to use tools and generating\nthe structured outputs required for function calls, often exhibiting language\nconfusion when prompted in lower-resource languages. This work presents a\nmethodology for adapting existing language models to enable robust tool use in\nany target language, using Bulgarian as a case study. The approach involves\ncontinued training of the BgGPT model series (2.6B, 9B, 27B parameters) on a\nnovel bilingual dataset of 10,035 function-calling examples designed to support\nstandardized protocols like MCP (Model Context Protocol). The research\nintroduces TUCAN (Tool-Using Capable Assistant Navigator), which achieves up to\n28.75% improvement in function-calling accuracy over base models while\npreserving core language understanding, as verified on established Bulgarian\nbenchmarks. Beyond accuracy gains, TUCAN models demonstrate production-ready\nresponse formatting with clean, parsable function calls, contrasting with the\nverbose and inconsistent outputs of base models. The models, evaluation\nframework, and dataset are released to enable replication for other languages.\nThis work demonstrates a practical approach for extending tool-augmented\ncapabilities beyond English-centric systems.", "AI": {"tldr": "提出了一种方法，使多语言模型在非英语语言中也能可靠地使用工具，并以保加利亚语为例展示了效果。", "motivation": "当前多语言模型在非英语语言中工具使用能力不足，存在语言混淆问题，需要改进。", "method": "通过继续训练BgGPT系列模型，使用包含10,035个双语函数调用示例的新数据集，支持MCP等标准化协议。", "result": "TUCAN模型在函数调用准确率上提升28.75%，同时保持语言理解能力，输出更规范。", "conclusion": "该方法为扩展工具增强能力至非英语系统提供了实用途径。"}}
{"id": "2506.23247", "pdf": "https://arxiv.org/pdf/2506.23247", "abs": "https://arxiv.org/abs/2506.23247", "authors": ["James Hinns", "David Martens"], "title": "Aggregating Local Saliency Maps for Semi-Global Explainable Image Classification", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Deep learning dominates image classification tasks, yet understanding how\nmodels arrive at predictions remains a challenge. Much research focuses on\nlocal explanations of individual predictions, such as saliency maps, which\nvisualise the influence of specific pixels on a model's prediction. However,\nreviewing many of these explanations to identify recurring patterns is\ninfeasible, while global methods often oversimplify and miss important local\nbehaviours. To address this, we propose Segment Attribution Tables (SATs), a\nmethod for summarising local saliency explanations into (semi-)global insights.\nSATs take image segments (such as \"eyes\" in Chihuahuas) and leverage saliency\nmaps to quantify their influence. These segments highlight concepts the model\nrelies on across instances and reveal spurious correlations, such as reliance\non backgrounds or watermarks, even when out-of-distribution test performance\nsees little change. SATs can explain any classifier for which a form of\nsaliency map can be produced, using segmentation maps that provide named\nsegments. SATs bridge the gap between oversimplified global summaries and\noverly detailed local explanations, offering a practical tool for analysing and\ndebugging image classifiers.", "AI": {"tldr": "论文提出Segment Attribution Tables (SATs)，通过汇总局部显著性解释提供半全局见解，弥补全局方法过于简化与局部解释过于详细之间的差距。", "motivation": "深度学习在图像分类中表现优异，但模型预测的解释仍具挑战性。现有方法要么过于局部（如显著性图），要么过于全局化，难以捕捉重要局部行为。", "method": "提出SATs方法，利用图像片段（如“眼睛”）和显著性图量化其影响，揭示模型依赖的概念和虚假相关性。", "result": "SATs能够解释任何可生成显著性图的分类器，提供介于全局和局部之间的实用分析工具。", "conclusion": "SATs为分析和调试图像分类器提供了实用工具，填补了现有解释方法的空白。"}}
{"id": "2506.23252", "pdf": "https://arxiv.org/pdf/2506.23252", "abs": "https://arxiv.org/abs/2506.23252", "authors": ["Kunwei Lv", "Ping Lan"], "title": "DGE-YOLO: Dual-Branch Gathering and Attention for Accurate UAV Object Detection", "categories": ["cs.CV"], "comment": "8 pages, 5 figures", "summary": "The rapid proliferation of unmanned aerial vehicles (UAVs) has highlighted\nthe importance of robust and efficient object detection in diverse aerial\nscenarios. Detecting small objects under complex conditions, however, remains a\nsignificant challenge. Existing approaches often prioritize inference speed,\nleading to degraded performance when handling multi-modal inputs. To address\nthis, we present DGE-YOLO, an enhanced YOLO-based detection framework designed\nto effectively fuse multi-modal information. Specifically, we introduce a\ndual-branch architecture for modality-specific feature extraction, enabling the\nmodel to process both infrared and visible images. To further enrich semantic\nrepresentation, we propose an Efficient Multi-scale Attention (EMA) mechanism\nthat enhances feature learning across spatial scales. Additionally, we replace\nthe conventional neck with a Gather-and-Distribute module to mitigate\ninformation loss during feature aggregation. Extensive experiments on the Drone\nVehicle dataset demonstrate that DGE-YOLO achieves superior performance over\nstate-of-the-art methods, validating its effectiveness in multi-modal UAV\nobject detection tasks.", "AI": {"tldr": "DGE-YOLO是一种改进的YOLO框架，用于无人机多模态物体检测，通过双分支架构和EMA机制提升性能。", "motivation": "无人机场景中复杂条件下的小物体检测需求迫切，现有方法在多模态输入处理上表现不足。", "method": "采用双分支架构处理红外和可见光图像，引入EMA机制增强多尺度特征学习，并使用Gather-and-Distribute模块优化特征聚合。", "result": "在Drone Vehicle数据集上，DGE-YOLO优于现有方法。", "conclusion": "DGE-YOLO在多模态无人机物体检测任务中表现出色。"}}
{"id": "2506.23254", "pdf": "https://arxiv.org/pdf/2506.23254", "abs": "https://arxiv.org/abs/2506.23254", "authors": ["Aradhana Mishra", "Bumshik Lee"], "title": "PixelBoost: Leveraging Brownian Motion for Realistic-Image Super-Resolution", "categories": ["cs.CV", "cs.AI", "cs.MM", "eess.IV"], "comment": null, "summary": "Diffusion-model-based image super-resolution techniques often face a\ntrade-off between realistic image generation and computational efficiency. This\nissue is exacerbated when inference times by decreasing sampling steps,\nresulting in less realistic and hazy images. To overcome this challenge, we\nintroduce a novel diffusion model named PixelBoost that underscores the\nsignificance of embracing the stochastic nature of Brownian motion in advancing\nimage super-resolution, resulting in a high degree of realism, particularly\nfocusing on texture and edge definitions. By integrating controlled\nstochasticity into the training regimen, our proposed model avoids convergence\nto local optima, effectively capturing and reproducing the inherent uncertainty\nof image textures and patterns. Our proposed model demonstrates superior\nobjective results in terms of learned perceptual image patch similarity\n(LPIPS), lightness order error (LOE), peak signal-to-noise ratio(PSNR),\nstructural similarity index measure (SSIM), as well as visual quality. To\ndetermine the edge enhancement, we evaluated the gradient magnitude and pixel\nvalue, and our proposed model exhibited a better edge reconstruction\ncapability. Additionally, our model demonstrates adaptive learning capabilities\nby effectively adjusting to Brownian noise patterns and introduces a sigmoidal\nnoise sequencing method that simplifies training, resulting in faster inference\nspeeds.", "AI": {"tldr": "PixelBoost是一种新型扩散模型，通过引入布朗运动的随机性提升图像超分辨率，在保持高效的同时生成更真实的图像。", "motivation": "现有扩散模型在图像超分辨率中存在真实性与计算效率的权衡问题，减少采样步骤会导致图像模糊。", "method": "通过将受控随机性融入训练过程，避免局部最优，并采用Sigmoidal噪声序列方法简化训练。", "result": "在LPIPS、LOE、PSNR、SSIM等指标上表现优异，边缘重建能力更强，且推理速度更快。", "conclusion": "PixelBoost通过布朗运动的随机性显著提升了图像超分辨率的真实性和效率。"}}
{"id": "2506.22789", "pdf": "https://arxiv.org/pdf/2506.22789", "abs": "https://arxiv.org/abs/2506.22789", "authors": ["Oguzhan Baser", "Ahmet Ege Tanriverdi", "Kaan Kale", "Sandeep P. Chinchali", "Sriram Vishwanath"], "title": "WavShape: Information-Theoretic Speech Representation Learning for Fair and Privacy-Aware Audio Processing", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "5 pages, 4 figures, Published at The Proceedings of Interspeech 2025,\n  code is available at http://www.github.com/UTAustin-SwarmLab/WavShape", "summary": "Speech embeddings often retain sensitive attributes such as speaker identity,\naccent, or demographic information, posing risks in biased model training and\nprivacy leakage. We propose WavShape, an information-theoretic speech\nrepresentation learning framework that optimizes embeddings for fairness and\nprivacy while preserving task-relevant information. We leverage mutual\ninformation (MI) estimation using the Donsker-Varadhan formulation to guide an\nMI-based encoder that systematically filters sensitive attributes while\nmaintaining speech content essential for downstream tasks. Experimental results\non three known datasets show that WavShape reduces MI between embeddings and\nsensitive attributes by up to 81% while retaining 97% of task-relevant\ninformation. By integrating information theory with self-supervised speech\nmodels, this work advances the development of fair, privacy-aware, and\nresource-efficient speech systems.", "AI": {"tldr": "WavShape是一种基于信息论的语音表示学习框架，旨在优化嵌入的公平性和隐私性，同时保留任务相关信息。", "motivation": "语音嵌入常保留敏感属性（如说话人身份、口音或人口统计信息），可能导致模型训练偏见和隐私泄露。", "method": "利用Donsker-Varadhan公式的互信息估计，指导基于互信息的编码器，系统性过滤敏感属性，同时保留任务相关的语音内容。", "result": "在三个已知数据集上，WavShape将嵌入与敏感属性间的互信息减少81%，同时保留97%的任务相关信息。", "conclusion": "结合信息论与自监督语音模型，WavShape推动了公平、隐私感知且资源高效的语音系统的发展。"}}
{"id": "2506.23578", "pdf": "https://arxiv.org/pdf/2506.23578", "abs": "https://arxiv.org/abs/2506.23578", "authors": ["Łukasz Kamiński", "Sławomir Lasota"], "title": "Reachability in symmetric VASS", "categories": ["cs.FL", "cs.CL"], "comment": null, "summary": "We investigate the reachability problem in symmetric vector addition systems\nwith states (VASS), where transitions are invariant under a group of\npermutations of coordinates. One extremal case, the trivial groups, yields\ngeneral VASS. In another extremal case, the symmetric groups, we show that the\nreachability problem can be solved in PSPACE, regardless of the dimension of\ninput VASS (to be contrasted with Ackermannian complexity in general VASS). We\nalso consider other groups, in particular alternating and cyclic ones.\nFurthermore, motivated by the open status of the reachability problem in data\nVASS, we estimate the gain in complexity when the group arises as a combination\nof the trivial and symmetric groups.", "AI": {"tldr": "研究了对称向量加法系统（VASS）中的可达性问题，特别关注不同群（如对称群、交替群和循环群）对复杂度的影响。对称群下可达性问题可在PSPACE内解决，而一般VASS的复杂度更高。", "motivation": "探索对称性对VASS可达性问题复杂度的影响，特别是为解决数据VASS中的开放性问题提供参考。", "method": "分析不同群（如对称群、交替群和循环群）下的VASS可达性问题，并比较其复杂度。", "result": "对称群下的VASS可达性问题可在PSPACE内解决，而一般VASS的复杂度为Ackermannian。", "conclusion": "对称性显著降低了VASS可达性问题的复杂度，为数据VASS的研究提供了新思路。"}}
{"id": "2506.23257", "pdf": "https://arxiv.org/pdf/2506.23257", "abs": "https://arxiv.org/abs/2506.23257", "authors": ["Chongke Bi", "Xin Gao", "Baofeng Fu", "Yuheng Zhao", "Siming Chen", "Ying Zhao", "Yunhai Wang"], "title": "PCLVis: Visual Analytics of Process Communication Latency in Large-Scale Simulation", "categories": ["cs.CV"], "comment": null, "summary": "Large-scale simulations on supercomputers have become important tools for\nusers. However, their scalability remains a problem due to the huge\ncommunication cost among parallel processes. Most of the existing communication\nlatency analysis methods rely on the physical link layer information, which is\nonly available to administrators. In this paper, a framework called PCLVis is\nproposed to help general users analyze process communication latency (PCL)\nevents. Instead of the physical link layer information, the PCLVis uses the MPI\nprocess communication data for the analysis. First, a spatial PCL event\nlocating method is developed. All processes with high correlation are\nclassified into a single cluster by constructing a process-correlation tree.\nSecond, the propagation path of PCL events is analyzed by constructing a\ncommunication-dependency-based directed acyclic graph (DAG), which can help\nusers interactively explore a PCL event from the temporal evolution of a\nlocated PCL event cluster. In this graph, a sliding window algorithm is\ndesigned to generate the PCL events abstraction. Meanwhile, a new glyph called\nthe communication state glyph (CS-Glyph) is designed for each process to show\nits communication states, including its in/out messages and load balance. Each\nleaf node can be further unfolded to view additional information. Third, a PCL\nevent attribution strategy is formulated to help users optimize their\nsimulations. The effectiveness of the PCLVis framework is demonstrated by\nanalyzing the PCL events of several simulations running on the TH-1A\nsupercomputer. By using the proposed framework, users can greatly improve the\nefficiency of their simulations.", "AI": {"tldr": "PCLVis框架利用MPI通信数据而非物理链路层信息，通过空间定位、传播路径分析和事件归因策略，帮助用户分析并优化超级计算机上的通信延迟事件。", "motivation": "大规模超级计算机模拟中通信延迟问题严重，现有方法依赖管理员才能获取的物理链路层信息，限制了普通用户的分析能力。", "method": "1. 空间PCL事件定位（基于进程相关性树聚类）；2. 传播路径分析（构建通信依赖DAG，滑动窗口算法生成事件抽象，CS-Glyph展示通信状态）；3. PCL事件归因策略。", "result": "在TH-1A超级计算机上验证了PCLVis的有效性，用户能显著提升模拟效率。", "conclusion": "PCLVis为普通用户提供了无需物理链路层信息的通信延迟分析工具，优化了模拟性能。"}}
{"id": "2506.22793", "pdf": "https://arxiv.org/pdf/2506.22793", "abs": "https://arxiv.org/abs/2506.22793", "authors": ["Pegah Alizadeh", "Anastasios Giovanidis", "Pradeepa Ramachandra", "Vasileios Koutsoukis", "Osama Arouk"], "title": "Offline Reinforcement Learning for Mobility Robustness Optimization", "categories": ["cs.NI", "cs.AI", "cs.PF"], "comment": "7 pages, double column, 4 figures, 6 tables, conference submission", "summary": "In this work we revisit the Mobility Robustness Optimisation (MRO) algorithm\nand study the possibility of learning the optimal Cell Individual Offset tuning\nusing offline Reinforcement Learning. Such methods make use of collected\noffline datasets to learn the optimal policy, without further exploration. We\nadapt and apply a sequence-based method called Decision Transformers as well as\na value-based method called Conservative Q-Learning to learn the optimal policy\nfor the same target reward as the vanilla rule-based MRO. The same input\nfeatures related to failures, ping-pongs, and other handover issues are used.\nEvaluation for realistic New Radio networks with 3500 MHz carrier frequency on\na traffic mix including diverse user service types and a specific tunable\ncell-pair shows that offline-RL methods outperform rule-based MRO, offering up\nto 7% improvement. Furthermore, offline-RL can be trained for diverse objective\nfunctions using the same available dataset, thus offering operational\nflexibility compared to rule-based methods.", "AI": {"tldr": "论文研究了利用离线强化学习优化移动鲁棒性（MRO）算法，通过决策变换器和保守Q学习方法，在相同目标奖励下优于基于规则的MRO，提升达7%。", "motivation": "探索通过离线强化学习优化MRO算法，避免进一步探索，利用现有数据集学习最优策略。", "method": "采用决策变换器和保守Q学习方法，使用与基于规则MRO相同的输入特征（如失败、乒乓切换等）。", "result": "在3500 MHz载频的New Radio网络中，离线强化学习方法比基于规则的MRO性能提升达7%。", "conclusion": "离线强化学习不仅性能更优，还能通过同一数据集训练多样目标函数，提供更高的操作灵活性。"}}
{"id": "2506.23670", "pdf": "https://arxiv.org/pdf/2506.23670", "abs": "https://arxiv.org/abs/2506.23670", "authors": ["Mohammadmahdi Nouriborji", "Morteza Rohanian"], "title": "Efficient Interleaved Speech Modeling through Knowledge Distillation", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": null, "summary": "Current speech language models exceed the size and latency constraints of\nmany deployment environments. We build compact, expressive speech generation\nmodels through layer-aligned distillation, matching hidden states, attention\nmaps, and softened logits to compress large multimodal transformers by 3x with\nminimal loss in performance. We introduce TinyWave, a family of 2B-parameter\nmodels for speech-to-speech and interleaved speech-text generation, trained on\n50,000 hours of public audio. TinyWave supports (i) speech-only generation\nusing phonetic or expressive tokens and (ii) mixed speech-text continuations.\nEvaluation on Libri-Light shows TinyWave within 1.4 normalized perplexity\npoints of its teacher. Accuracy on spoken StoryCloze and SALMon reaches 93-97%\nof the teacher's performance, outperforming size-matched baselines. These\nmodels are optimized for deployment on commodity hardware, enabling\napplications in real-time conversational agents, assistive technologies, and\nlow-resource environments. We release models, training code, and evaluation\nscripts to support reproducible research on compact, expressive speech\ngeneration.", "AI": {"tldr": "论文提出了一种通过层对齐蒸馏方法构建紧凑、高效的语音生成模型TinyWave，压缩大型多模态Transformer模型3倍，性能损失极小。", "motivation": "当前语音语言模型在部署环境中存在体积和延迟限制，需要更紧凑高效的解决方案。", "method": "采用层对齐蒸馏技术，匹配隐藏状态、注意力图和软化logits，训练2B参数的TinyWave模型。", "result": "TinyWave在Libri-Light上的表现接近教师模型，在StoryCloze和SALMon任务中达到教师模型93-97%的准确率。", "conclusion": "TinyWave模型适用于实时对话代理、辅助技术和低资源环境，并公开了模型和代码以支持可重复研究。"}}
{"id": "2506.23263", "pdf": "https://arxiv.org/pdf/2506.23263", "abs": "https://arxiv.org/abs/2506.23263", "authors": ["Lei-lei Li", "Jianwu Fang", "Junbin Xiao", "Shanmin Pang", "Hongkai Yu", "Chen Lv", "Jianru Xue", "Tat-Seng Chua"], "title": "Causal-Entity Reflected Egocentric Traffic Accident Video Synthesis", "categories": ["cs.CV"], "comment": "Accepted by ICCV2025", "summary": "Egocentricly comprehending the causes and effects of car accidents is crucial\nfor the safety of self-driving cars, and synthesizing causal-entity reflected\naccident videos can facilitate the capability test to respond to unaffordable\naccidents in reality. However, incorporating causal relations as seen in\nreal-world videos into synthetic videos remains challenging. This work argues\nthat precisely identifying the accident participants and capturing their\nrelated behaviors are of critical importance. In this regard, we propose a\nnovel diffusion model, Causal-VidSyn, for synthesizing egocentric traffic\naccident videos. To enable causal entity grounding in video diffusion,\nCausal-VidSyn leverages the cause descriptions and driver fixations to identify\nthe accident participants and behaviors, facilitated by accident reason\nanswering and gaze-conditioned selection modules. To support Causal-VidSyn, we\nfurther construct Drive-Gaze, the largest driver gaze dataset (with 1.54M\nframes of fixations) in driving accident scenarios. Extensive experiments show\nthat Causal-VidSyn surpasses state-of-the-art video diffusion models in terms\nof frame quality and causal sensitivity in various tasks, including accident\nvideo editing, normal-to-accident video diffusion, and text-to-video\ngeneration.", "AI": {"tldr": "提出了一种名为Causal-VidSyn的扩散模型，用于合成以自我为中心的交通事故视频，通过因果实体接地和驾驶员注视数据提升合成视频的因果敏感性。", "motivation": "理解交通事故的因果关系对自动驾驶汽车的安全至关重要，但将真实视频中的因果关系融入合成视频仍具挑战性。", "method": "利用原因描述和驾驶员注视数据，通过事故原因回答和注视条件选择模块识别事故参与者和行为。", "result": "Causal-VidSyn在帧质量和因果敏感性方面优于现有视频扩散模型，支持多种任务如视频编辑和文本到视频生成。", "conclusion": "Causal-VidSyn通过结合因果实体接地和驾驶员注视数据，显著提升了合成交通事故视频的质量和实用性。"}}
{"id": "2506.23270", "pdf": "https://arxiv.org/pdf/2506.23270", "abs": "https://arxiv.org/abs/2506.23270", "authors": ["Yi Li", "Hualiang Wang", "Xinpeng Ding", "Haonan Wang", "Xiaomeng Li"], "title": "Token Activation Map to Visually Explain Multimodal LLMs", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "ICCV2025 Accepted", "summary": "Multimodal large language models (MLLMs) are broadly empowering various\nfields. Despite their advancements, the explainability of MLLMs remains less\nexplored, hindering deeper understanding, model credibility, and effective\nvisualization. Unlike conventional vision models (e.g., CNNs, ViTs, CLIP) that\nproduce a single output, MLLMs generate sequences of tokens progressively,\nwhere each generated token depends on the previous context. Therefore, earlier\ncontext tokens can introduce redundant activations that interfere with the\nexplanation of later tokens beyond their original information. Existing studies\noften overlook this issue, but our observations reveal that these redundant\ncorrelations can significantly hurt the reliability of explanations. To address\nthis, we propose an estimated causal inference method to mitigate the\ninterference of context to achieve high-quality MLLM explanation, with a novel\nrank Gaussian filter to further reduce activation noises. We term this method\nToken Activation Map (TAM) to highlight the consideration of interactions\nbetween tokens. TAM also indicates that it excels at explaining multiple tokens\nof MLLM, which is different from the Class Activation Map (CAM) for a single\nprediction. Our TAM method significantly outperforms existing SoTA methods,\nshowcasing high-quality visualization results that can be utilized for various\nscenarios, such as object localization, failure case analysis, video\nvisualization, MLLMs visual comparison, and model understanding (e.g., color,\nshape, action, location, visual reasoning, multi-turn conversation, etc). The\ncode is available atgithub.com/xmed-lab/TAM.", "AI": {"tldr": "提出了一种名为Token Activation Map (TAM)的方法，用于提高多模态大语言模型（MLLMs）的可解释性，通过因果推断和高斯滤波减少冗余激活干扰。", "motivation": "MLLMs的可解释性研究不足，冗余激活干扰影响解释可靠性，阻碍模型理解和可视化效果。", "method": "采用估计因果推断方法和高斯滤波，提出TAM方法，关注令牌间交互。", "result": "TAM显著优于现有方法，支持多种场景应用（如目标定位、失败案例分析等）。", "conclusion": "TAM为MLLMs提供了高质量的可视化解释工具，适用于广泛场景。"}}
{"id": "2506.23714", "pdf": "https://arxiv.org/pdf/2506.23714", "abs": "https://arxiv.org/abs/2506.23714", "authors": ["Md Moinul Islam", "Sofoklis Kakouros", "Janne Heikkilä", "Mourad Oussalah"], "title": "Towards an Automated Multimodal Approach for Video Summarization: Building a Bridge Between Text, Audio and Facial Cue-Based Summarization", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted to HHAI WS 2025: Workshops at the Fourth International\n  Conference on Hybrid Human-Artificial Intelligence (HHAI)", "summary": "The increasing volume of video content in educational, professional, and\nsocial domains necessitates effective summarization techniques that go beyond\ntraditional unimodal approaches. This paper proposes a behaviour-aware\nmultimodal video summarization framework that integrates textual, audio, and\nvisual cues to generate timestamp-aligned summaries. By extracting prosodic\nfeatures, textual cues and visual indicators, the framework identifies\nsemantically and emotionally important moments. A key contribution is the\nidentification of bonus words, which are terms emphasized across multiple\nmodalities and used to improve the semantic relevance and expressive clarity of\nthe summaries. The approach is evaluated against pseudo-ground truth (pGT)\nsummaries generated using LLM-based extractive method. Experimental results\ndemonstrate significant improvements over traditional extractive method, such\nas the Edmundson method, in both text and video-based evaluation metrics.\nText-based metrics show ROUGE-1 increasing from 0.4769 to 0.7929 and BERTScore\nfrom 0.9152 to 0.9536, while in video-based evaluation, our proposed framework\nimproves F1-Score by almost 23%. The findings underscore the potential of\nmultimodal integration in producing comprehensive and behaviourally informed\nvideo summaries.", "AI": {"tldr": "本文提出了一种行为感知的多模态视频摘要框架，整合文本、音频和视觉线索，生成时间戳对齐的摘要，显著提升了摘要质量。", "motivation": "随着教育、职业和社交领域中视频内容的增加，需要超越传统单模态方法的有效摘要技术。", "method": "通过提取韵律特征、文本线索和视觉指标，识别语义和情感重要时刻，并利用跨模态强调的“奖励词”提升摘要的语义相关性和表达清晰度。", "result": "实验结果表明，与传统提取方法（如Edmundson方法）相比，文本和视频评估指标均有显著提升，ROUGE-1从0.4769提高到0.7929，BERTScore从0.9152提高到0.9536，视频F1分数提升近23%。", "conclusion": "多模态整合在生成全面且行为感知的视频摘要方面具有巨大潜力。"}}
{"id": "2506.23271", "pdf": "https://arxiv.org/pdf/2506.23271", "abs": "https://arxiv.org/abs/2506.23271", "authors": ["Jinxing Zhou", "Zhihui Li", "Yongqiang Yu", "Yanghao Zhou", "Ruohao Guo", "Guangyao Li", "Yuxin Mao", "Mingfei Han", "Xiaojun Chang", "Meng Wang"], "title": "Mettle: Meta-Token Learning for Memory-Efficient Audio-Visual Adaptation", "categories": ["cs.CV"], "comment": "Technical Report", "summary": "We present \\textbf{Met}a-\\textbf{T}oken \\textbf{Le}arning (Mettle), a simple\nand memory-efficient method for adapting large-scale pretrained transformer\nmodels to downstream audio-visual tasks. Instead of sequentially modifying the\noutput feature distribution of the transformer backbone, Mettle utilizes a\nlightweight \\textit{Layer-Centric Distillation (LCD)} module to distill in\nparallel the intact audio or visual features embedded by each transformer layer\ninto compact meta-tokens. This distillation process considers both pretrained\nknowledge preservation and task-specific adaptation. The obtained meta-tokens\ncan be directly applied to classification tasks, such as audio-visual event\nlocalization and audio-visual video parsing. To further support fine-grained\nsegmentation tasks, such as audio-visual segmentation, we introduce a\n\\textit{Meta-Token Injection (MTI)} module, which utilizes the audio and visual\nmeta-tokens distilled from the top transformer layer to guide feature\nadaptation in earlier layers. Extensive experiments on multiple audiovisual\nbenchmarks demonstrate that our method significantly reduces memory usage and\ntraining time while maintaining parameter efficiency and competitive accuracy.", "AI": {"tldr": "Mettle是一种高效适应预训练Transformer模型的方法，通过并行蒸馏音频或视觉特征为元令牌，支持分类和细粒度分割任务，显著减少内存和训练时间。", "motivation": "解决大规模预训练模型在下游音频-视觉任务中内存占用高和训练时间长的问题。", "method": "使用Layer-Centric Distillation (LCD)模块并行蒸馏特征为元令牌，并通过Meta-Token Injection (MTI)模块支持细粒度分割。", "result": "在多个音频-视觉基准测试中，显著减少内存和训练时间，同时保持参数效率和准确性。", "conclusion": "Mettle是一种简单、高效且内存友好的方法，适用于多种音频-视觉任务。"}}
{"id": "2506.22818", "pdf": "https://arxiv.org/pdf/2506.22818", "abs": "https://arxiv.org/abs/2506.22818", "authors": ["Stanislav Sedukhin", "Yoichi Tomioka", "Kazuya Matsumoto", "Yuichi Okuyama"], "title": "TriADA: Massively Parallel Trilinear Matrix-by-Tensor Multiply-Add Algorithm and Device Architecture for the Acceleration of 3D Discrete Transformations", "categories": ["cs.DC", "cs.AI", "cs.AR", "cs.ET", "eess.SP", "C.1.4; C.3; F.2.1; G.1.3; G.4"], "comment": "19 pages, 5 figures", "summary": "Multilinear transformations are key in high-performance computing (HPC) and\nartificial intelligence (AI) workloads, where data is represented as tensors.\nHowever, their high computational and memory demands, which grow with\ndimensionality, often slow down critical tasks. Moreover, scaling computation\nby enlarging the number of parallel processing units substantially increases\nenergy consumption, limiting widespread adoption, especially for sparse data,\nwhich is common in HPC and AI applications. This paper introduces the Trilinear\nAlgorithm and isomorphic to algorithm Device Architecture (TriADA) to address\nthese challenges with the following innovations: (1) a massively parallel,\nlow-rank algorithm for computing a family of trilinear (3D) discrete orthogonal\ntransformations (3D-DXTs), which is a special case of the more general 3-mode\nmatrix-by-tensor multiplication (3D-GEMT); (2) a new outer-product-based GEMM\nkernel with decoupled streaming active memory, specially designed to accelerate\n3D-GEMT operation; (3) an isomorphic to the proposed algorithm, fully\ndistributed 3D network of mesh interconnected processing elements or cells with\na coordinate-free, data-driven local processing activity, which is independent\nof problem size; (4) an elastic sparse outer-product (ESOP) method that avoids\nunnecessary computing and communication operations with zero-valued operands,\nthereby enhancing energy efficiency, computational accuracy, and stability.\nTriADA is capable of performing a variety of trilinear transformations with\nhypercubic arithmetic complexity in a linear number of time-steps. The\nmassively parallel, scalable, and energy-efficient architecture of TriADA is\nideal for accelerating multilinear tensor operations, which are the most\ndemanding parts of AI and HPC workloads.", "AI": {"tldr": "TriADA是一种新型算法和架构，用于高效计算三线性变换，特别适用于高维稀疏数据，提升AI和HPC任务的性能和能效。", "motivation": "高维数据的计算和内存需求高，并行处理单元的增加导致能耗上升，限制了稀疏数据的广泛应用。", "method": "提出TriADA，包括低秩并行算法、优化的GEMM内核、分布式3D网络和弹性稀疏外积方法。", "result": "TriADA能以线性时间步长完成多种三线性变换，显著提升性能和能效。", "conclusion": "TriADA为AI和HPC中的多线性张量操作提供了高效、可扩展的解决方案。"}}
{"id": "2506.23845", "pdf": "https://arxiv.org/pdf/2506.23845", "abs": "https://arxiv.org/abs/2506.23845", "authors": ["Kenny Peng", "Rajiv Movva", "Jon Kleinberg", "Emma Pierson", "Nikhil Garg"], "title": "Use Sparse Autoencoders to Discover Unknown Concepts, Not to Act on Known Concepts", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "While sparse autoencoders (SAEs) have generated significant excitement, a\nseries of negative results have added to skepticism about their usefulness.\nHere, we establish a conceptual distinction that reconciles competing\nnarratives surrounding SAEs. We argue that while SAEs may be less effective for\nacting on known concepts, SAEs are powerful tools for discovering unknown\nconcepts. This distinction cleanly separates existing negative and positive\nresults, and suggests several classes of SAE applications. Specifically, we\noutline use cases for SAEs in (i) ML interpretability, explainability,\nfairness, auditing, and safety, and (ii) social and health sciences.", "AI": {"tldr": "SAEs在已知概念上效果有限，但在发现未知概念上表现强大，适用于ML解释性、社会科学等领域。", "motivation": "解决关于SAEs有用性的争议，明确其适用场景。", "method": "提出概念区分，分析SAEs在不同场景下的表现。", "result": "SAEs适用于发现未知概念，尤其在ML解释性和社会科学中。", "conclusion": "SAEs的价值在于发现未知概念，为多个领域提供新工具。"}}
{"id": "2506.23275", "pdf": "https://arxiv.org/pdf/2506.23275", "abs": "https://arxiv.org/abs/2506.23275", "authors": ["Chengyou Jia", "Xin Shen", "Zhuohang Dang", "Zhuohang Dang", "Changliang Xia", "Weijia Wu", "Xinyu Zhang", "Hangwei Qian", "Ivor W. Tsang", "Minnan Luo"], "title": "Why Settle for One? Text-to-ImageSet Generation and Evaluation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Despite remarkable progress in Text-to-Image models, many real-world\napplications require generating coherent image sets with diverse consistency\nrequirements. Existing consistent methods often focus on a specific domain with\nspecific aspects of consistency, which significantly constrains their\ngeneralizability to broader applications. In this paper, we propose a more\nchallenging problem, Text-to-ImageSet (T2IS) generation, which aims to generate\nsets of images that meet various consistency requirements based on user\ninstructions. To systematically study this problem, we first introduce\n$\\textbf{T2IS-Bench}$ with 596 diverse instructions across 26 subcategories,\nproviding comprehensive coverage for T2IS generation. Building on this, we\npropose $\\textbf{T2IS-Eval}$, an evaluation framework that transforms user\ninstructions into multifaceted assessment criteria and employs effective\nevaluators to adaptively assess consistency fulfillment between criteria and\ngenerated sets. Subsequently, we propose $\\textbf{AutoT2IS}$, a training-free\nframework that maximally leverages pretrained Diffusion Transformers'\nin-context capabilities to harmonize visual elements to satisfy both\nimage-level prompt alignment and set-level visual consistency. Extensive\nexperiments on T2IS-Bench reveal that diverse consistency challenges all\nexisting methods, while our AutoT2IS significantly outperforms current\ngeneralized and even specialized approaches. Our method also demonstrates the\nability to enable numerous underexplored real-world applications, confirming\nits substantial practical value. Visit our project in\nhttps://chengyou-jia.github.io/T2IS-Home.", "AI": {"tldr": "论文提出了一种更具挑战性的问题——文本到图像集（T2IS）生成，旨在根据用户指令生成满足多种一致性要求的图像集。作者提出了T2IS-Bench基准、T2IS-Eval评估框架和AutoT2IS方法，实验表明AutoT2IS显著优于现有方法。", "motivation": "现有的一致性方法通常局限于特定领域和一致性方面，限制了其泛化能力。本文旨在解决更广泛的文本到图像集生成问题。", "method": "提出了T2IS-Bench基准、T2IS-Eval评估框架和AutoT2IS方法，后者利用预训练的Diffusion Transformers的上下文能力实现图像集的一致性生成。", "result": "实验表明，AutoT2IS在T2IS-Bench上显著优于现有方法，并能支持多种实际应用。", "conclusion": "AutoT2IS在文本到图像集生成任务中表现出色，具有重要的实际价值。"}}
{"id": "2506.23978", "pdf": "https://arxiv.org/pdf/2506.23978", "abs": "https://arxiv.org/abs/2506.23978", "authors": ["Samuele Marro", "Philip Torr"], "title": "LLM Agents Are the Antidote to Walled Gardens", "categories": ["cs.LG", "cs.CL", "cs.CY", "cs.SI", "68T50, 68M10, 91B26", "I.2.11; I.2.7; H.4.5"], "comment": null, "summary": "While the Internet's core infrastructure was designed to be open and\nuniversal, today's application layer is dominated by closed, proprietary\nplatforms. Open and interoperable APIs require significant investment, and\nmarket leaders have little incentive to enable data exchange that could erode\ntheir user lock-in. We argue that LLM-based agents fundamentally disrupt this\nstatus quo. Agents can automatically translate between data formats and\ninteract with interfaces designed for humans: this makes interoperability\ndramatically cheaper and effectively unavoidable. We name this shift universal\ninteroperability: the ability for any two digital services to exchange data\nseamlessly using AI-mediated adapters. Universal interoperability undermines\nmonopolistic behaviours and promotes data portability. However, it can also\nlead to new security risks and technical debt. Our position is that the ML\ncommunity should embrace this development while building the appropriate\nframeworks to mitigate the downsides. By acting now, we can harness AI to\nrestore user freedom and competitive markets without sacrificing security.", "AI": {"tldr": "LLM-based agents enable universal interoperability, reducing costs and breaking monopolistic behaviors, but require frameworks to address security risks.", "motivation": "The dominance of closed, proprietary platforms limits data exchange and interoperability, prompting the need for a disruptive solution.", "method": "Propose LLM-based agents to automatically translate data formats and interact with human-designed interfaces, enabling seamless data exchange.", "result": "Universal interoperability becomes cheaper and unavoidable, undermining monopolistic behaviors but introducing new risks.", "conclusion": "The ML community should adopt this shift while developing frameworks to mitigate risks, leveraging AI to restore user freedom and market competition."}}
{"id": "2506.23282", "pdf": "https://arxiv.org/pdf/2506.23282", "abs": "https://arxiv.org/abs/2506.23282", "authors": ["Hanwen Zhang", "Congqi Cao", "Qinyi Lv", "Lingtong Min", "Yanning Zhang"], "title": "Autoregressive Denoising Score Matching is a Good Video Anomaly Detector", "categories": ["cs.CV"], "comment": null, "summary": "Video anomaly detection (VAD) is an important computer vision problem. Thanks\nto the mode coverage capabilities of generative models, the likelihood-based\nparadigm is catching growing interest, as it can model normal distribution and\ndetect out-of-distribution anomalies. However, these likelihood-based methods\nare blind to the anomalies located in local modes near the learned\ndistribution. To handle these ``unseen\" anomalies, we dive into three gaps\nuniquely existing in VAD regarding scene, motion and appearance. Specifically,\nwe first build a noise-conditioned score transformer for denoising score\nmatching. Then, we introduce a scene-dependent and motion-aware score function\nby embedding the scene condition of input sequences into our model and\nassigning motion weights based on the difference between key frames of input\nsequences. Next, to solve the problem of blindness in principle, we integrate\nunaffected visual information via a novel autoregressive denoising score\nmatching mechanism for inference. Through autoregressively injecting\nintensifying Gaussian noise into the denoised data and estimating the\ncorresponding score function, we compare the denoised data with the original\ndata to get a difference and aggregate it with the score function for an\nenhanced appearance perception and accumulate the abnormal context. With all\nthree gaps considered, we can compute a more comprehensive anomaly indicator.\nExperiments on three popular VAD benchmarks demonstrate the state-of-the-art\nperformance of our method.", "AI": {"tldr": "提出了一种基于生成模型的视频异常检测方法，通过噪声条件评分变换器和场景依赖的运动感知评分函数，结合自回归去噪机制，解决了局部模式异常检测的盲区问题。", "motivation": "传统基于似然的异常检测方法无法检测到学习分布附近局部模式中的异常，因此需要解决场景、运动和外观三个方面的独特问题。", "method": "1. 构建噪声条件评分变换器进行去噪评分匹配；2. 引入场景依赖和运动感知的评分函数；3. 通过自回归去噪评分匹配机制增强异常感知。", "result": "在三个流行的视频异常检测基准测试中取得了最先进的性能。", "conclusion": "通过综合考虑场景、运动和外观三个方面的差距，提出了一种更全面的异常检测方法，显著提升了性能。"}}
{"id": "2506.22837", "pdf": "https://arxiv.org/pdf/2506.22837", "abs": "https://arxiv.org/abs/2506.22837", "authors": ["Kamil Faber", "Marcin Pietroń", "Dominik Żurek", "Roberto Corizzo"], "title": "xLSTMAD: A Powerful xLSTM-based Method for Anomaly Detection", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "The recently proposed xLSTM is a powerful model that leverages expressive\nmultiplicative gating and residual connections, providing the temporal capacity\nneeded for long-horizon forecasting and representation learning. This\narchitecture has demonstrated success in time series forecasting, lossless\ncompression, and even large-scale language modeling tasks, where its linear\nmemory footprint and fast inference make it a viable alternative to\nTransformers. Despite its growing popularity, no prior work has explored xLSTM\nfor anomaly detection. In this work, we fill this gap by proposing xLSTMAD, the\nfirst anomaly detection method that integrates a full encoder-decoder xLSTM\narchitecture, purpose-built for multivariate time series data. Our encoder\nprocesses input sequences to capture historical context, while the decoder is\ndevised in two separate variants of the method. In the forecasting approach,\nthe decoder iteratively generates forecasted future values xLSTMAD-F, while the\nreconstruction approach reconstructs the input time series from its encoded\ncounterpart xLSTMAD-R. We investigate the performance of two loss functions:\nMean Squared Error (MSE), and Soft Dynamic Time Warping (SoftDTW) to consider\nlocal reconstruction fidelity and global sequence alignment, respectively. We\nevaluate our method on the comprehensive TSB-AD-M benchmark, which spans 17\nreal-world datasets, using state-of-the-art challenging metrics such as VUS-PR.\nIn our results, xLSTM showcases state-of-the-art accuracy, outperforming 23\npopular anomaly detection baselines. Our paper is the first work revealing the\npowerful modeling capabilities of xLSTM for anomaly detection, paving the way\nfor exciting new developments on this subject. Our code is available at:\nhttps://github.com/Nyderx/xlstmad", "AI": {"tldr": "xLSTMAD是一种基于xLSTM架构的异常检测方法，首次将xLSTM应用于多变量时间序列数据的异常检测，表现优于现有基线。", "motivation": "尽管xLSTM在时间序列预测等领域表现出色，但尚未有人将其用于异常检测，本文填补了这一空白。", "method": "提出xLSTMAD，包含编码器和两种解码器变体（预测和重建），并研究了MSE和SoftDTW两种损失函数。", "result": "在TSB-AD-M基准测试中，xLSTMAD表现优异，超越23种基线方法。", "conclusion": "xLSTM在异常检测中展现出强大潜力，为未来研究开辟了新方向。"}}
{"id": "2506.24019", "pdf": "https://arxiv.org/pdf/2506.24019", "abs": "https://arxiv.org/abs/2506.24019", "authors": ["Hongxin Zhang", "Zheyuan Zhang", "Zeyuan Wang", "Zunzhe Zhang", "Lixing Fang", "Qinhong Zhou", "Chuang Gan"], "title": "Ella: Embodied Social Agents with Lifelong Memory", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We introduce Ella, an embodied social agent capable of lifelong learning\nwithin a community in a 3D open world, where agents accumulate experiences and\nacquire knowledge through everyday visual observations and social interactions.\nAt the core of Ella's capabilities is a structured, long-term multimodal memory\nsystem that stores, updates, and retrieves information effectively. It consists\nof a name-centric semantic memory for organizing acquired knowledge and a\nspatiotemporal episodic memory for capturing multimodal experiences. By\nintegrating this lifelong memory system with foundation models, Ella retrieves\nrelevant information for decision-making, plans daily activities, builds social\nrelationships, and evolves autonomously while coexisting with other intelligent\nbeings in the open world. We conduct capability-oriented evaluations in a\ndynamic 3D open world where 15 agents engage in social activities for days and\nare assessed with a suite of unseen controlled evaluations. Experimental\nresults show that Ella can influence, lead, and cooperate with other agents\nwell to achieve goals, showcasing its ability to learn effectively through\nobservation and social interaction. Our findings highlight the transformative\npotential of combining structured memory systems with foundation models for\nadvancing embodied intelligence. More videos can be found at\nhttps://umass-embodied-agi.github.io/Ella/.", "AI": {"tldr": "Ella是一个能够在3D开放世界中终身学习的社交代理，通过视觉观察和社交互动积累经验与知识。其核心是一个结构化的长期多模态记忆系统，结合基础模型实现决策、社交和自主进化。实验表明Ella能有效影响、领导和合作其他代理。", "motivation": "探索如何通过结构化记忆系统和基础模型结合，提升具身智能体的终身学习与社交能力。", "method": "Ella采用多模态记忆系统（语义记忆和情景记忆）与基础模型集成，支持信息检索、决策和社交互动。实验在动态3D开放世界中进行，评估其能力。", "result": "Ella能有效影响、领导与合作其他代理，展示出通过观察和社交互动学习的能力。", "conclusion": "结合结构化记忆系统与基础模型，具有推动具身智能发展的潜力。"}}
{"id": "2506.23283", "pdf": "https://arxiv.org/pdf/2506.23283", "abs": "https://arxiv.org/abs/2506.23283", "authors": ["Yuhuan Yang", "Chaofan Ma", "Zhenjie Mao", "Jiangchao Yao", "Ya Zhang", "Yanfeng Wang"], "title": "MoMa: Modulating Mamba for Adapting Image Foundation Models to Video Recognition", "categories": ["cs.CV"], "comment": "ICML 2025 paper", "summary": "Video understanding is a complex challenge that requires effective modeling\nof spatial-temporal dynamics. With the success of image foundation models\n(IFMs) in image understanding, recent approaches have explored\nparameter-efficient fine-tuning (PEFT) to adapt IFMs for video. However, most\nof these methods tend to process spatial and temporal information separately,\nwhich may fail to capture the full intricacy of video dynamics. In this paper,\nwe propose MoMa, an efficient adapter framework that achieves full\nspatial-temporal modeling by integrating Mamba's selective state space modeling\ninto IFMs. We propose a novel SeqMod operation to inject spatial-temporal\ninformation into pre-trained IFMs, without disrupting their original features.\nBy incorporating SeqMod into a Divide-and-Modulate architecture, MoMa enhances\nvideo understanding while maintaining computational efficiency. Extensive\nexperiments on multiple video benchmarks demonstrate the effectiveness of MoMa,\nachieving superior performance with reduced computational cost.", "AI": {"tldr": "MoMa是一个高效的适配器框架，通过将Mamba的选择性状态空间建模集成到图像基础模型中，实现了全时空建模，提升了视频理解能力。", "motivation": "现有的参数高效微调方法在处理视频时往往将空间和时间信息分开处理，无法充分捕捉视频动态的复杂性。", "method": "提出SeqMod操作和Divide-and-Modulate架构，将时空信息注入预训练的图像基础模型，同时保持计算效率。", "result": "在多个视频基准测试中表现优异，性能提升且计算成本降低。", "conclusion": "MoMa通过全时空建模显著提升了视频理解能力，同时保持了高效性。"}}
{"id": "2506.22845", "pdf": "https://arxiv.org/pdf/2506.22845", "abs": "https://arxiv.org/abs/2506.22845", "authors": ["Batuhan Hangun", "Oguz Altun", "Onder Eyecioglu"], "title": "Quantum Neural Networks for Wind Energy Forecasting: A Comparative Study of Performance and Scalability with Classical Models", "categories": ["cs.LG", "cs.AI", "cs.PF"], "comment": null, "summary": "Quantum Neural Networks (QNNs), a prominent approach in Quantum Machine\nLearning (QML), are emerging as a powerful alternative to classical machine\nlearning methods. Recent studies have focused on the applicability of QNNs to\nvarious tasks, such as time-series forecasting, prediction, and classification,\nacross a wide range of applications, including cybersecurity and medical\nimaging. With the increased use of smart grids driven by the integration of\nrenewable energy systems, machine learning plays an important role in\npredicting power demand and detecting system disturbances. This study provides\nan in-depth investigation of QNNs for predicting the power output of a wind\nturbine. We assess the predictive performance and simulation time of six QNN\nconfigurations that are based on the Z Feature Map for data encoding and\nvarying ansatz structures. Through detailed cross-validation experiments and\ntests on an unseen hold-out dataset, we experimentally demonstrate that QNNs\ncan achieve predictive performance that is competitive with, and in some cases\nmarginally better than, the benchmarked classical approaches. Our results also\nreveal the effects of dataset size and circuit complexity on predictive\nperformance and simulation time. We believe our findings will offer valuable\ninsights for researchers in the energy domain who wish to incorporate quantum\nmachine learning into their work.", "AI": {"tldr": "该论文研究了量子神经网络（QNNs）在风力涡轮机功率输出预测中的应用，通过实验验证其性能与经典方法相当甚至略优。", "motivation": "随着可再生能源系统的普及，智能电网对机器学习的需求增加，QNNs作为一种新兴的量子机器学习方法，有望在预测任务中超越经典方法。", "method": "研究评估了六种基于Z Feature Map数据编码和不同ansatz结构的QNN配置，通过交叉验证和未见数据集测试其性能。", "result": "QNNs在预测性能上与经典方法相当或略优，同时揭示了数据集大小和电路复杂度对性能的影响。", "conclusion": "QNNs在能源领域具有潜力，为研究者提供了将量子机器学习应用于实际问题的参考。"}}
{"id": "2506.24056", "pdf": "https://arxiv.org/pdf/2506.24056", "abs": "https://arxiv.org/abs/2506.24056", "authors": ["Tung-Ling Li", "Hongliang Liu"], "title": "Logit-Gap Steering: Efficient Short-Suffix Jailbreaks for Aligned Large Language Models", "categories": ["cs.CR", "cs.CL", "cs.LG"], "comment": null, "summary": "We introduce logit-gap steering, a fast jailbreak framework that casts the\nrefusal-affirmation gap of RLHF-aligned language models as a single pass over\nthe vocabulary. A forward-computable score blends gap reduction with\nlightweight proxies for KL penalty and reward shift, allowing a \"sort-sum-stop\"\nsweep to complete in under a second and return a short suffix--two orders of\nmagnitude fewer model calls than beam or gradient attacks. The same suffix\ngeneralises to unseen prompts and scales from 0.5 B to 70 B checkpoints,\nlifting one-shot attack success from baseline levels to 80-100% while\npreserving topical coherence. Beyond efficiency, these suffixes expose\nsentence-boundary reward cliffs and other alignment artefacts, offering a\nlightweight probe into how safety tuning reshapes internal representations.", "AI": {"tldr": "提出了一种快速破解RLHF对齐语言模型的方法logit-gap steering，通过单次词汇表遍历实现拒绝-肯定间隙的优化，效率极高。", "motivation": "解决RLHF对齐语言模型在拒绝与肯定响应之间的间隙问题，并探索安全调整对内部表征的影响。", "method": "使用前向可计算的分数结合间隙减少、KL惩罚和奖励偏移的轻量代理，通过“排序-求和-停止”方法快速生成短后缀。", "result": "该方法在0.5B到70B模型上均有效，攻击成功率提升至80-100%，同时保持主题一致性。", "conclusion": "logit-gap steering不仅高效，还揭示了安全调整对模型内部表征的影响，为对齐研究提供了新工具。"}}
{"id": "2506.23285", "pdf": "https://arxiv.org/pdf/2506.23285", "abs": "https://arxiv.org/abs/2506.23285", "authors": ["Daqian Shi", "Xiaolei Diao", "Xu Chen", "Cédric M. John"], "title": "Competitive Distillation: A Simple Learning Strategy for Improving Visual Classification", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Deep Neural Networks (DNNs) have significantly advanced the field of computer\nvision. To improve DNN training process, knowledge distillation methods\ndemonstrate their effectiveness in accelerating network training by introducing\na fixed learning direction from the teacher network to student networks. In\nthis context, several distillation-based optimization strategies are proposed,\ne.g., deep mutual learning and self-distillation, as an attempt to achieve\ngeneric training performance enhancement through the cooperative training of\nmultiple networks. However, such strategies achieve limited improvements due to\nthe poor understanding of the impact of learning directions among networks\nacross different iterations. In this paper, we propose a novel competitive\ndistillation strategy that allows each network in a group to potentially act as\na teacher based on its performance, enhancing the overall learning performance.\nCompetitive distillation organizes a group of networks to perform a shared task\nand engage in competition, where competitive optimization is proposed to\nimprove the parameter updating process. We further introduce stochastic\nperturbation in competitive distillation, aiming to motivate networks to induce\nmutations to achieve better visual representations and global optimum. The\nexperimental results show that competitive distillation achieves promising\nperformance in diverse tasks and datasets.", "AI": {"tldr": "提出了一种新颖的竞争蒸馏策略，通过动态选择教师网络和引入竞争优化，提升深度神经网络的训练性能。", "motivation": "现有蒸馏方法因对网络间学习方向影响的理解不足，性能提升有限。", "method": "提出竞争蒸馏策略，动态选择教师网络，引入竞争优化和随机扰动。", "result": "实验表明，该方法在多种任务和数据集上表现优异。", "conclusion": "竞争蒸馏通过动态竞争和优化，显著提升了训练性能。"}}
{"id": "2506.22848", "pdf": "https://arxiv.org/pdf/2506.22848", "abs": "https://arxiv.org/abs/2506.22848", "authors": ["Shengcai Liu", "Hui Ou-yang", "Zhiyuan Wang", "Cheng Chen", "Qijun Cai", "Yew-Soon Ong", "Ke Tang"], "title": "Scalable Structure Learning of Bayesian Networks by Learning Algorithm Ensembles", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Learning the structure of Bayesian networks (BNs) from data is challenging,\nespecially for datasets involving a large number of variables. The recently\nproposed divide-and-conquer (D\\&D) strategies present a promising approach for\nlearning large BNs. However, they still face a main issue of unstable learning\naccuracy across subproblems. In this work, we introduce the idea of employing\nstructure learning ensemble (SLE), which combines multiple BN structure\nlearning algorithms, to consistently achieve high learning accuracy. We further\npropose an automatic approach called Auto-SLE for learning near-optimal SLEs,\naddressing the challenge of manually designing high-quality SLEs. The learned\nSLE is then integrated into a D\\&D method. Extensive experiments firmly show\nthe superiority of our method over D\\&D methods with single BN structure\nlearning algorithm in learning large BNs, achieving accuracy improvement\nusually by 30\\%$\\sim$225\\% on datasets involving 10,000 variables. Furthermore,\nour method generalizes well to datasets with many more (e.g., 30000) variables\nand different network characteristics than those present in the training data\nfor learning the SLE. These results indicate the significant potential of\nemploying (automatic learning of) SLEs for scalable BN structure learning.", "AI": {"tldr": "提出了一种基于结构学习集成（SLE）的方法Auto-SLE，用于提升大规模贝叶斯网络（BN）结构学习的稳定性和准确性，并通过实验验证其优越性。", "motivation": "解决现有分治策略（D&D）在大规模BN结构学习中子问题学习精度不稳定的问题。", "method": "引入SLE结合多种BN结构学习算法，提出自动学习近最优SLE的方法Auto-SLE，并将其集成到D&D方法中。", "result": "实验显示，该方法在涉及10,000变量的数据集上通常能提升30%∼225%的准确率，并能泛化到更多变量（如30,000）和不同网络特性的数据集。", "conclusion": "SLE（尤其是自动学习的SLE）在大规模BN结构学习中具有显著潜力。"}}
{"id": "2506.24086", "pdf": "https://arxiv.org/pdf/2506.24086", "abs": "https://arxiv.org/abs/2506.24086", "authors": ["Bingfan Zhu", "Biao Jiang", "Sunyi Wang", "Shixiang Tang", "Tao Chen", "Linjie Luo", "Youyi Zheng", "Xin Chen"], "title": "MotionGPT3: Human Motion as a Second Modality", "categories": ["cs.CV", "cs.CL"], "comment": "21 pages, 8 figures", "summary": "Though recent advances in multimodal models have demonstrated strong\ncapabilities and opportunities in unified understanding and generation, the\ndevelopment of unified motion-language models remains underexplored. To enable\nsuch models with high-fidelity human motion, two core challenges must be\naddressed. The first is the reconstruction gap between the continuous motion\nmodality and discrete representation in an autoregressive manner, and the\nsecond is the degradation of language intelligence during unified training.\nInspired by the mixture of experts, we propose MotionGPT3, a bimodal\nmotion-language model that treats human motion as a second modality, decoupling\nmotion modeling via separate model parameters and enabling both effective\ncross-modal interaction and efficient multimodal scaling training. To preserve\nlanguage intelligence, the text branch retains the original structure and\nparameters of the pretrained language model, while a new motion branch is\nintegrated via a shared attention mechanism, enabling bidirectional information\nflow between two modalities. We first employ a motion Variational Autoencoder\n(VAE) to encode raw human motion into latent representations. Based on this\ncontinuous latent space, the motion branch predicts motion latents directly\nfrom intermediate hidden states using a diffusion head, bypassing discrete\ntokenization. Extensive experiments show that our approach achieves competitive\nperformance on both motion understanding and generation tasks while preserving\nstrong language capabilities, establishing a unified bimodal motion diffusion\nframework within an autoregressive manner.", "AI": {"tldr": "MotionGPT3是一个双模态运动-语言模型，通过分离参数处理运动建模，解决了运动与语言统一建模中的重建差距和语言智能退化问题。", "motivation": "当前多模态模型在统一理解与生成方面取得了进展，但运动-语言统一模型的研究仍不足。需要解决运动模态与离散表示的重建差距以及统一训练中语言智能的退化问题。", "method": "提出MotionGPT3，采用专家混合方法，将运动作为第二模态，通过共享注意力机制集成新运动分支，保留语言分支的原始结构。使用运动VAE编码原始运动，扩散头预测运动潜在表示。", "result": "实验表明，该方法在运动理解和生成任务中表现优异，同时保持了强大的语言能力。", "conclusion": "MotionGPT3建立了自回归框架下的统一双模态运动扩散模型，为运动-语言统一建模提供了有效解决方案。"}}
{"id": "2506.23292", "pdf": "https://arxiv.org/pdf/2506.23292", "abs": "https://arxiv.org/abs/2506.23292", "authors": ["Changtao Miao", "Yi Zhang", "Weize Gao", "Man Luo", "Weiwei Feng", "Zhiya Tan", "Jianshu Li", "Ajian Liu", "Yunfeng Diao", "Qi Chu", "Tao Gong", "Zhe Li", "Weibin Yao", "Joey Tianyi Zhou"], "title": "DDL: A Dataset for Interpretable Deepfake Detection and Localization in Real-World Scenarios", "categories": ["cs.CV"], "comment": "This paper is a preliminary version, with an extended and\n  comprehensive version currently under development", "summary": "Recent advances in AIGC have exacerbated the misuse of malicious deepfake\ncontent, making the development of reliable deepfake detection methods an\nessential means to address this challenge. Although existing deepfake detection\nmodels demonstrate outstanding performance in detection metrics, most methods\nonly provide simple binary classification results, lacking interpretability. In\ncritical domains such as law, interpretability is crucial for enhancing the\ncredibility and authority of decisions. Recent studies attempt to improve the\ninterpretability of classification results by providing spatial manipulation\nmasks or temporal forgery segments. However, the practical effectiveness of\nthese methods remains suboptimal due to limitations of the forgery data. Most\ncurrent deepfake datasets predominantly offer binary labels, only a few\ndatasets with localization annotations. However, they suffer from restricted\nforgery scenarios, limited diversity in deepfake types, and insufficient data\nscale, making them inadequate for complex real-world scenarios. To address this\npredicament, we construct a novel large-scale deepfake detection and\nlocalization ($\\textbf{DDL}$) dataset containing over $\\textbf{1.8M}$ forged\nsamples and encompassing up to $\\textbf{75}$ distinct deepfake methods. The DDL\ndesign incorporates four key innovations: (1) $\\textbf{Diverse Forgery\nScenarios}$, (2) $\\textbf{Comprehensive Deepfake Methods}$, (3) $\\textbf{Varied\nManipulation Modes}$, and (4) $\\textbf{Fine-grained Forgery Annotations}$.\nThrough these improvements, our DDL not only provides a more challenging\nbenchmark for complex real-world forgeries, but also offers crucial support for\nbuilding next-generation deepfake detection, localization, and interpretability\nmethods. The DDL dataset project page is on\nhttps://deepfake-workshop-ijcai2025.github.io/main/index.html.", "AI": {"tldr": "论文提出了一种新的大规模深度伪造检测与定位数据集（DDL），包含180万伪造样本和75种深度伪造方法，旨在解决现有数据集在多样性和规模上的不足，并支持下一代检测与解释性方法。", "motivation": "深度伪造技术的滥用日益严重，现有检测方法缺乏解释性，且数据集在多样性和规模上不足，无法满足复杂现实场景需求。", "method": "构建了DDL数据集，包含多样伪造场景、全面深度伪造方法、多种操纵模式和细粒度伪造标注。", "result": "DDL数据集为复杂现实伪造提供了更具挑战性的基准，并支持下一代深度伪造检测与解释性方法。", "conclusion": "DDL数据集通过创新设计解决了现有数据集的局限性，为深度伪造检测与定位提供了重要支持。"}}
{"id": "2506.23295", "pdf": "https://arxiv.org/pdf/2506.23295", "abs": "https://arxiv.org/abs/2506.23295", "authors": ["Xiang Xu"], "title": "DiffFit: Disentangled Garment Warping and Texture Refinement for Virtual Try-On", "categories": ["cs.CV"], "comment": null, "summary": "Virtual try-on (VTON) aims to synthesize realistic images of a person wearing\na target garment, with broad applications in e-commerce and digital fashion.\nWhile recent advances in latent diffusion models have substantially improved\nvisual quality, existing approaches still struggle with preserving fine-grained\ngarment details, achieving precise garment-body alignment, maintaining\ninference efficiency, and generalizing to diverse poses and clothing styles. To\naddress these challenges, we propose DiffFit, a novel two-stage latent\ndiffusion framework for high-fidelity virtual try-on. DiffFit adopts a\nprogressive generation strategy: the first stage performs geometry-aware\ngarment warping, aligning the garment with the target body through fine-grained\ndeformation and pose adaptation. The second stage refines texture fidelity via\na cross-modal conditional diffusion model that integrates the warped garment,\nthe original garment appearance, and the target person image for high-quality\nrendering. By decoupling geometric alignment and appearance refinement, DiffFit\neffectively reduces task complexity and enhances both generation stability and\nvisual realism. It excels in preserving garment-specific attributes such as\ntextures, wrinkles, and lighting, while ensuring accurate alignment with the\nhuman body. Extensive experiments on large-scale VTON benchmarks demonstrate\nthat DiffFit achieves superior performance over existing state-of-the-art\nmethods in both quantitative metrics and perceptual evaluations.", "AI": {"tldr": "DiffFit是一个两阶段潜在扩散框架，用于高保真虚拟试穿，通过几何感知的服装变形和纹理细化解决现有方法的细节保留和对齐问题。", "motivation": "虚拟试穿技术在实际应用中面临服装细节保留、对齐精度、推理效率和多样性适应等挑战。", "method": "DiffFit采用两阶段生成策略：第一阶段进行几何感知的服装变形和对齐，第二阶段通过跨模态条件扩散模型细化纹理。", "result": "DiffFit在定量指标和感知评估中优于现有方法，能更好地保留服装细节并实现精确对齐。", "conclusion": "DiffFit通过解耦几何对齐和外观细化，显著提升了虚拟试穿的生成质量和稳定性。"}}
{"id": "2506.23308", "pdf": "https://arxiv.org/pdf/2506.23308", "abs": "https://arxiv.org/abs/2506.23308", "authors": ["Yiming Huang", "Long Bai", "Beilei Cui", "Yanheng Li", "Tong Chen", "Jie Wang", "Jinlin Wu", "Zhen Lei", "Hongbin Liu", "Hongliang Ren"], "title": "Endo-4DGX: Robust Endoscopic Scene Reconstruction and Illumination Correction with Gaussian Splatting", "categories": ["cs.CV"], "comment": "MICCAI 2025. Project Page:\n  https://lastbasket.github.io/MICCAI-2025-Endo-4DGX/", "summary": "Accurate reconstruction of soft tissue is crucial for advancing automation in\nimage-guided robotic surgery. The recent 3D Gaussian Splatting (3DGS)\ntechniques and their variants, 4DGS, achieve high-quality renderings of dynamic\nsurgical scenes in real-time. However, 3D-GS-based methods still struggle in\nscenarios with varying illumination, such as low light and over-exposure.\nTraining 3D-GS in such extreme light conditions leads to severe optimization\nproblems and devastating rendering quality. To address these challenges, we\npresent Endo-4DGX, a novel reconstruction method with illumination-adaptive\nGaussian Splatting designed specifically for endoscopic scenes with uneven\nlighting. By incorporating illumination embeddings, our method effectively\nmodels view-dependent brightness variations. We introduce a region-aware\nenhancement module to model the sub-area lightness at the Gaussian level and a\nspatial-aware adjustment module to learn the view-consistent brightness\nadjustment. With the illumination adaptive design, Endo-4DGX achieves superior\nrendering performance under both low-light and over-exposure conditions while\nmaintaining geometric accuracy. Additionally, we employ an exposure control\nloss to restore the appearance from adverse exposure to the normal level for\nillumination-adaptive optimization. Experimental results demonstrate that\nEndo-4DGX significantly outperforms combinations of state-of-the-art\nreconstruction and restoration methods in challenging lighting environments,\nunderscoring its potential to advance robot-assisted surgical applications. Our\ncode is available at https://github.com/lastbasket/Endo-4DGX.", "AI": {"tldr": "Endo-4DGX是一种针对内窥镜场景光照不均问题的新型重建方法，通过光照自适应高斯泼溅技术，显著提升了低光和过曝条件下的渲染质量。", "motivation": "在图像引导机器人手术中，软组织的精确重建对自动化至关重要。现有3D高斯泼溅技术在极端光照条件下表现不佳，导致渲染质量下降。", "method": "Endo-4DGX结合光照嵌入、区域感知增强模块和空间感知调整模块，实现光照自适应优化。", "result": "实验表明，Endo-4DGX在低光和过曝条件下显著优于现有方法，同时保持几何精度。", "conclusion": "Endo-4DGX为机器人辅助手术应用提供了潜在的技术支持。"}}
{"id": "2506.23323", "pdf": "https://arxiv.org/pdf/2506.23323", "abs": "https://arxiv.org/abs/2506.23323", "authors": ["Quang-Huy Che", "Vinh-Tiep Nguyen"], "title": "FastSeg: Efficient Training-Free Open-Vocabulary Segmentation via Hierarchical Attention Refinement Method", "categories": ["cs.CV"], "comment": null, "summary": "Open-vocabulary semantic segmentation (OVSS) aims to segment objects from\narbitrary text categories without requiring densely annotated datasets.\nAlthough contrastive learning based models enable zero-shot segmentation, they\noften lose fine spatial precision at pixel level, due to global representation\nbias. In contrast, diffusion-based models naturally encode fine-grained spatial\nfeatures via attention mechanisms that capture both global context and local\ndetails. However, they often face challenges in balancing the number of\niterations with the quality of the segmentation. In this work, we propose\nFastSeg, a novel and efficient training-free framework with only (1+1)-step of\nreverse process of a pretrained diffusion model (e.g., Stable Diffusion).\nMoreover, instead of running multiple times for different classes, FastSeg\nperforms segmentation for all classes at once. To further enhance the\nsegmentation quality, FastSeg introduces three key components: (i) a\ndual-prompt mechanism for discriminative, class-aware attention extraction,\n(ii) a Hierarchical Attention Refinement Method (HARD) that enhances fused\ncross-attention using scale-aligned selfattention maps, and (iii) a Test-Time\nFlipping (TTF) scheme designed to improve spatial consistency. Extensive\nexperiments show that FastSeg achieves state-of-the-art training-free\nperformance, obtaining 43.8% average mIoU across PASCAL VOC, PASCAL Context,\nand COCO Object benchmarks while maintaining superior inference efficiency. Our\nresults demonstrate that FastSeg provides a strong foundation for\nextendability, bridging the gap between segmentation quality and inference\nefficiency.", "AI": {"tldr": "FastSeg是一种高效的无训练框架，通过预训练扩散模型（如Stable Diffusion）的（1+1）步反向过程实现开放词汇语义分割（OVSS），并在多类别分割中表现出色。", "motivation": "解决现有对比学习模型在像素级空间精度上的不足，以及扩散模型在迭代次数与分割质量之间的平衡问题。", "method": "提出FastSeg框架，包含双提示机制、分层注意力细化方法（HARD）和测试时翻转（TTF）方案，以提升分割质量和效率。", "result": "在PASCAL VOC、PASCAL Context和COCO Object基准测试中达到43.8%的平均mIoU，表现优异。", "conclusion": "FastSeg在分割质量和推理效率之间取得了平衡，为扩展性提供了坚实基础。"}}
{"id": "2506.23329", "pdf": "https://arxiv.org/pdf/2506.23329", "abs": "https://arxiv.org/abs/2506.23329", "authors": ["Parker Liu", "Chenxin Li", "Zhengxin Li", "Yipeng Wu", "Wuyang Li", "Zhiqin Yang", "Zhenyuan Zhang", "Yunlong Lin", "Sirui Han", "Brandon Y. Feng"], "title": "IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as Agentic Inverse Rendering", "categories": ["cs.CV"], "comment": "Project Page: https://ir3d-bench.github.io/", "summary": "Vision-language models (VLMs) excel at descriptive tasks, but whether they\ntruly understand scenes from visual observations remains uncertain. We\nintroduce IR3D-Bench, a benchmark challenging VLMs to demonstrate understanding\nthrough active creation rather than passive recognition. Grounded in the\nanalysis-by-synthesis paradigm, IR3D-Bench tasks Vision-Language Agents (VLAs)\nwith actively using programming and rendering tools to recreate the underlying\n3D structure of an input image, achieving agentic inverse rendering through\ntool use. This \"understanding-by-creating\" approach probes the tool-using\ngenerative capacity of VLAs, moving beyond the descriptive or conversational\ncapacity measured by traditional scene understanding benchmarks. We provide a\ncomprehensive suite of metrics to evaluate geometric accuracy, spatial\nrelations, appearance attributes, and overall plausibility. Initial experiments\non agentic inverse rendering powered by various state-of-the-art VLMs highlight\ncurrent limitations, particularly in visual precision rather than basic tool\nusage. IR3D-Bench, including data and evaluation protocols, is released to\nfacilitate systematic study and development of tool-using VLAs towards genuine\nscene understanding by creating.", "AI": {"tldr": "IR3D-Bench是一个新的基准测试，通过要求视觉语言模型（VLMs）主动使用工具重建图像的3D结构，以评估其场景理解能力，而非仅依赖被动识别。", "motivation": "探讨VLMs是否真正理解场景，而非仅进行描述性任务。", "method": "基于分析-合成范式，要求视觉语言代理（VLAs）使用编程和渲染工具逆向渲染输入图像的3D结构。", "result": "实验显示当前VLMs在视觉精度上存在局限，而非工具使用能力。", "conclusion": "IR3D-Bench为系统研究和开发工具使用型VLAs提供了数据与评估协议，推动真正的场景理解。"}}
{"id": "2506.23347", "pdf": "https://arxiv.org/pdf/2506.23347", "abs": "https://arxiv.org/abs/2506.23347", "authors": ["Yi Liu", "Shengqian Li", "Zuzeng Lin", "Feng Wang", "Si Liu"], "title": "CycleVAR: Repurposing Autoregressive Model for Unsupervised One-Step Image Translation", "categories": ["cs.CV"], "comment": null, "summary": "The current conditional autoregressive image generation methods have shown\npromising results, yet their potential remains largely unexplored in the\npractical unsupervised image translation domain, which operates without\nexplicit cross-domain correspondences. A critical limitation stems from the\ndiscrete quantization inherent in traditional Vector Quantization-based\nframeworks, which disrupts gradient flow between the Variational Autoencoder\ndecoder and causal Transformer, impeding end-to-end optimization during\nadversarial training in image space. To tackle this issue, we propose using\nSoftmax Relaxed Quantization, a novel approach that reformulates codebook\nselection as a continuous probability mixing process via Softmax, thereby\npreserving gradient propagation. Building upon this differentiable foundation,\nwe introduce CycleVAR, which reformulates image-to-image translation as\nimage-conditional visual autoregressive generation by injecting multi-scale\nsource image tokens as contextual prompts, analogous to prefix-based\nconditioning in language models. CycleVAR exploits two modes to generate the\ntarget image tokens, including (1) serial multi-step generation, enabling\niterative refinement across scales, and (2) parallel one-step generation\nsynthesizing all resolution outputs in a single forward pass. Experimental\nfindings indicate that the parallel one-step generation mode attains superior\ntranslation quality with quicker inference speed than the serial multi-step\nmode in unsupervised scenarios. Furthermore, both quantitative and qualitative\nresults indicate that CycleVAR surpasses previous state-of-the-art unsupervised\nimage translation models, \\textit{e}.\\textit{g}., CycleGAN-Turbo.", "AI": {"tldr": "论文提出CycleVAR方法，通过Softmax Relaxed Quantization解决传统量化方法梯度中断问题，实现无监督图像翻译的端到端优化。", "motivation": "现有条件自回归图像生成方法在无监督图像翻译领域潜力未充分挖掘，传统量化方法导致梯度中断，阻碍端到端优化。", "method": "提出Softmax Relaxed Quantization，将码本选择转化为连续概率混合过程；基于此提出CycleVAR，通过多尺度源图像标记实现图像条件自回归生成。", "result": "实验表明CycleVAR在无监督场景下优于现有方法（如CycleGAN-Turbo），并行单步生成模式效果更优。", "conclusion": "CycleVAR通过梯度保留的量化方法和多尺度条件生成，显著提升无监督图像翻译性能。"}}
{"id": "2506.22884", "pdf": "https://arxiv.org/pdf/2506.22884", "abs": "https://arxiv.org/abs/2506.22884", "authors": ["Praveen Kumar Donta", "Qiyang Zhang", "Schahram Dustdar"], "title": "Performance Measurements in the AI-Centric Computing Continuum Systems", "categories": ["cs.DC", "cs.AI", "cs.ET", "cs.NI", "cs.SY", "eess.SY"], "comment": null, "summary": "Over the Eight decades, computing paradigms have shifted from large,\ncentralized systems to compact, distributed architectures, leading to the rise\nof the Distributed Computing Continuum (DCC). In this model, multiple layers\nsuch as cloud, edge, Internet of Things (IoT), and mobile platforms work\ntogether to support a wide range of applications. Recently, the emergence of\nGenerative AI and large language models has further intensified the demand for\ncomputational resources across this continuum. Although traditional performance\nmetrics have provided a solid foundation, they need to be revisited and\nexpanded to keep pace with changing computational demands and application\nrequirements. Accurate performance measurements benefit both system designers\nand users by supporting improvements in efficiency and promoting alignment with\nsystem goals. In this context, we review commonly used metrics in DCC and IoT\nenvironments. We also discuss emerging performance dimensions that address\nevolving computing needs, such as sustainability, energy efficiency, and system\nobservability. We also outline criteria and considerations for selecting\nappropriate metrics, aiming to inspire future research and development in this\ncritical area.", "AI": {"tldr": "论文回顾了分布式计算连续体（DCC）中的性能指标，讨论了新兴需求如可持续性和能效，并提出了选择指标的准则。", "motivation": "随着计算范式向分布式架构转变，生成式AI和大语言模型对计算资源的需求增加，传统性能指标需扩展以适应新需求。", "method": "回顾DCC和IoT环境中常用指标，讨论新兴性能维度如可持续性和系统可观测性。", "result": "提出了选择性能指标的准则，以支持系统效率和目标对齐。", "conclusion": "论文为未来研究和开发提供了方向，强调了适应新计算需求的性能指标的重要性。"}}
{"id": "2506.23352", "pdf": "https://arxiv.org/pdf/2506.23352", "abs": "https://arxiv.org/abs/2506.23352", "authors": ["Shunsuke Yasuki", "Taiki Miyanishi", "Nakamasa Inoue", "Shuhei Kurita", "Koya Sakamoto", "Daichi Azuma", "Masato Taki", "Yutaka Matsuo"], "title": "GeoProg3D: Compositional Visual Reasoning for City-Scale 3D Language Fields", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "The advancement of 3D language fields has enabled intuitive interactions with\n3D scenes via natural language. However, existing approaches are typically\nlimited to small-scale environments, lacking the scalability and compositional\nreasoning capabilities necessary for large, complex urban settings. To overcome\nthese limitations, we propose GeoProg3D, a visual programming framework that\nenables natural language-driven interactions with city-scale high-fidelity 3D\nscenes. GeoProg3D consists of two key components: (i) a Geography-aware\nCity-scale 3D Language Field (GCLF) that leverages a memory-efficient\nhierarchical 3D model to handle large-scale data, integrated with geographic\ninformation for efficiently filtering vast urban spaces using directional cues,\ndistance measurements, elevation data, and landmark references; and (ii)\nGeographical Vision APIs (GV-APIs), specialized geographic vision tools such as\narea segmentation and object detection. Our framework employs large language\nmodels (LLMs) as reasoning engines to dynamically combine GV-APIs and operate\nGCLF, effectively supporting diverse geographic vision tasks. To assess\nperformance in city-scale reasoning, we introduce GeoEval3D, a comprehensive\nbenchmark dataset containing 952 query-answer pairs across five challenging\ntasks: grounding, spatial reasoning, comparison, counting, and measurement.\nExperiments demonstrate that GeoProg3D significantly outperforms existing 3D\nlanguage fields and vision-language models across multiple tasks. To our\nknowledge, GeoProg3D is the first framework enabling compositional geographic\nreasoning in high-fidelity city-scale 3D environments via natural language. The\ncode is available at https://snskysk.github.io/GeoProg3D/.", "AI": {"tldr": "GeoProg3D是一个视觉编程框架，通过自然语言实现城市规模高保真3D场景的交互，结合地理感知和大语言模型，显著优于现有方法。", "motivation": "现有3D语言方法局限于小规模环境，缺乏处理大规模复杂城市场景的能力。", "method": "GeoProg3D包含地理感知的3D语言场（GCLF）和地理视觉API（GV-APIs），利用大语言模型动态组合工具。", "result": "在GeoEval3D基准测试中，GeoProg3D在多项任务中表现优异。", "conclusion": "GeoProg3D首次实现了通过自然语言在城市规模3D环境中进行组合地理推理。"}}
{"id": "2506.22895", "pdf": "https://arxiv.org/pdf/2506.22895", "abs": "https://arxiv.org/abs/2506.22895", "authors": ["Xinyu Chen", "Vassilis Digalakis Jr", "Lijun Ding", "Dingyi Zhuang", "Jinhua Zhao"], "title": "Interpretable Time Series Autoregression for Periodicity Quantification", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time series autoregression is a classical statistical model for capturing\nauto-correlations and identifying temporal patterns such as periodicity and\nseasonality. In this work, we propose a novel sparse autoregression framework\nfrom an interpretable machine learning perspective and the model\ninterpretability for periodicity quantification is reinforced by $\\ell_0$-norm\ninduced sparsity constraints. On the time-varying time series data, we\nreformulate the sparse autoregression and convert the involved optimization\nproblem into a mixed-integer optimization (MIO). To accelerate it, we develop a\nsubspace pursuit based decision variable pruning (DVP) strategy to reduce the\nsearch space. On the multidimensional time series that involves complicated\nspatial and temporal dimensions, we propose a spatially- and time-varying\nsparse autoregression model and resolve the corresponding MIO problem by\ndeveloping a two-stage optimization scheme. In particular, the proposed scheme\nmakes the model scalable to large problems even with millions of decision\nvariables. Empirically, we conduct extensive experiments to evaluate the\nproposed models on real-world time series data. First, we demonstrate that the\nMIO solver can be drastically accelerated through the DVP strategy, while\nmaintaining the same solution quality as a full MIO solver. Applying the\ntime-varying sparse autoregression model to ridesharing trip data, we uncover\nboth daily and weekly periodicities and reveal long-term changes in regularity\nof human mobility. Second, we demonstrate the spatial patterns of yearly\nseasonality in climate variable time series such as temperature and\nprecipitation across the past four decades, and our model allows to discover\ndynamic climate patterns and identify climate phenomena such as El Nino in sea\nsurface temperature.", "AI": {"tldr": "提出了一种基于稀疏自回归框架的新方法，通过ℓ0范数增强模型可解释性，并利用混合整数优化（MIO）解决时间序列问题。通过决策变量剪枝（DVP）加速求解，并在多维时间序列中扩展为空间和时间变化的模型。实验验证了方法的有效性和可扩展性。", "motivation": "传统时间序列自回归模型在捕捉自相关性和周期性方面存在局限性，需要更可解释且高效的模型来应对复杂时空数据。", "method": "提出稀疏自回归框架，引入ℓ0范数约束增强可解释性；将优化问题转化为MIO，并通过DVP策略加速求解；针对多维数据，提出两阶段优化方案。", "result": "实验表明，DVP策略显著加速MIO求解且不损失解的质量；在拼车数据和气候数据中成功识别周期性和动态模式。", "conclusion": "该方法在可解释性和可扩展性上表现优异，适用于复杂时空数据的模式发现。"}}
{"id": "2506.23353", "pdf": "https://arxiv.org/pdf/2506.23353", "abs": "https://arxiv.org/abs/2506.23353", "authors": ["Siyuan Chai", "Xiaodong Guo", "Tong Liu"], "title": "Layer Decomposition and Morphological Reconstruction for Task-Oriented Infrared Image Enhancement", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Infrared image helps improve the perception capabilities of autonomous\ndriving in complex weather conditions such as fog, rain, and low light.\nHowever, infrared image often suffers from low contrast, especially in\nnon-heat-emitting targets like bicycles, which significantly affects the\nperformance of downstream high-level vision tasks. Furthermore, achieving\ncontrast enhancement without amplifying noise and losing important information\nremains a challenge. To address these challenges, we propose a task-oriented\ninfrared image enhancement method. Our approach consists of two key components:\nlayer decomposition and saliency information extraction. First, we design an\nlayer decomposition method for infrared images, which enhances scene details\nwhile preserving dark region features, providing more features for subsequent\nsaliency information extraction. Then, we propose a morphological\nreconstruction-based saliency extraction method that effectively extracts and\nenhances target information without amplifying noise. Our method improves the\nimage quality for object detection and semantic segmentation tasks. Extensive\nexperiments demonstrate that our approach outperforms state-of-the-art methods.", "AI": {"tldr": "提出了一种面向任务的红外图像增强方法，通过分层分解和显著性信息提取提升图像质量，适用于自动驾驶中的复杂天气条件。", "motivation": "红外图像在复杂天气条件下（如雾、雨、低光）对自动驾驶感知至关重要，但其低对比度和噪声问题影响下游视觉任务性能。", "method": "方法包括分层分解（增强细节并保留暗区特征）和基于形态学重建的显著性提取（避免噪声放大）。", "result": "实验表明，该方法在目标检测和语义分割任务中优于现有技术。", "conclusion": "该方法有效解决了红外图像低对比度和噪声问题，提升了自动驾驶感知能力。"}}
{"id": "2506.22901", "pdf": "https://arxiv.org/pdf/2506.22901", "abs": "https://arxiv.org/abs/2506.22901", "authors": ["Sina Tabakhi", "Haiping Lu"], "title": "Missing-Modality-Aware Graph Neural Network for Cancer Classification", "categories": ["cs.LG", "cs.AI", "q-bio.BM", "q-bio.GN"], "comment": "15 pages, 7 figures", "summary": "A key challenge in learning from multimodal biological data is missing\nmodalities, where all data from some modalities are missing for some patients.\nCurrent fusion methods address this by excluding patients with missing\nmodalities, imputing missing modalities, or making predictions directly with\npartial modalities. However, they often struggle with diverse missing-modality\npatterns and the exponential growth of the number of such patterns as the\nnumber of modalities increases. To address these limitations, we propose MAGNET\n(Missing-modality-Aware Graph neural NETwork) for direct prediction with\npartial modalities, which introduces a patient-modality multi-head attention\nmechanism to fuse lower-dimensional modality embeddings based on their\nimportance and missingness. MAGNET's complexity increases linearly with the\nnumber of modalities while adapting to missing-pattern variability. To generate\npredictions, MAGNET further constructs a patient graph with fused multimodal\nembeddings as node features and the connectivity determined by the modality\nmissingness, followed by a conventional graph neural network. Experiments on\nthree public multiomics datasets for cancer classification, with real-world\ninstead of artificial missingness, show that MAGNET outperforms the\nstate-of-the-art fusion methods. The data and code are available at\nhttps://github.com/SinaTabakhi/MAGNET.", "AI": {"tldr": "MAGNET提出了一种基于图神经网络的模型，用于处理多模态生物数据中的缺失模态问题，通过注意力机制和患者图结构实现高效预测。", "motivation": "多模态生物数据中常见模态缺失问题，现有方法难以应对多样化的缺失模式和模态数量的增加。", "method": "MAGNET结合患者-模态多头注意力机制和患者图结构，线性复杂度适应缺失模式。", "result": "在三个公共多组学数据集上，MAGNET在癌症分类任务中优于现有方法。", "conclusion": "MAGNET有效解决了多模态数据中的缺失问题，性能优于现有技术。"}}
{"id": "2506.23361", "pdf": "https://arxiv.org/pdf/2506.23361", "abs": "https://arxiv.org/abs/2506.23361", "authors": ["Yuanhao Cai", "He Zhang", "Xi Chen", "Jinbo Xing", "Yiwei Hu", "Yuqian Zhou", "Kai Zhang", "Zhifei Zhang", "Soo Ye Kim", "Tianyu Wang", "Yulun Zhang", "Xiaokang Yang", "Zhe Lin", "Alan Yuille"], "title": "OmniVCus: Feedforward Subject-driven Video Customization with Multimodal Control Conditions", "categories": ["cs.CV"], "comment": "A data construction pipeline and a diffusion Transformer framework\n  for controllable subject-driven video customization", "summary": "Existing feedforward subject-driven video customization methods mainly study\nsingle-subject scenarios due to the difficulty of constructing multi-subject\ntraining data pairs. Another challenging problem that how to use the signals\nsuch as depth, mask, camera, and text prompts to control and edit the subject\nin the customized video is still less explored. In this paper, we first propose\na data construction pipeline, VideoCus-Factory, to produce training data pairs\nfor multi-subject customization from raw videos without labels and control\nsignals such as depth-to-video and mask-to-video pairs. Based on our\nconstructed data, we develop an Image-Video Transfer Mixed (IVTM) training with\nimage editing data to enable instructive editing for the subject in the\ncustomized video. Then we propose a diffusion Transformer framework, OmniVCus,\nwith two embedding mechanisms, Lottery Embedding (LE) and Temporally Aligned\nEmbedding (TAE). LE enables inference with more subjects by using the training\nsubjects to activate more frame embeddings. TAE encourages the generation\nprocess to extract guidance from temporally aligned control signals by\nassigning the same frame embeddings to the control and noise tokens.\nExperiments demonstrate that our method significantly surpasses\nstate-of-the-art methods in both quantitative and qualitative evaluations.\nVideo demos are at our project page:\nhttps://caiyuanhao1998.github.io/project/OmniVCus/. Our code will be released\nat https://github.com/caiyuanhao1998/Open-OmniVCus", "AI": {"tldr": "本文提出了一种多主题视频定制方法，通过数据构造管道VideoCus-Factory和扩散Transformer框架OmniVCus，实现了对视频中主题的控制和编辑。", "motivation": "现有方法主要针对单主题场景，且缺乏对多主题训练数据对的控制信号（如深度、遮罩等）的探索。", "method": "提出VideoCus-Factory数据构造管道和IVTM训练方法，结合扩散Transformer框架OmniVCus，采用Lottery Embedding和Temporally Aligned Embedding机制。", "result": "实验表明，该方法在定量和定性评估上均显著优于现有技术。", "conclusion": "该方法为多主题视频定制和控制信号的应用提供了有效解决方案。"}}
{"id": "2506.22911", "pdf": "https://arxiv.org/pdf/2506.22911", "abs": "https://arxiv.org/abs/2506.22911", "authors": ["Yunxuan Ma", "Siqiang Wang", "Zhijian Duan", "Yukun Cheng", "Xiaotie Deng"], "title": "Learning Truthful Mechanisms without Discretization", "categories": ["cs.GT", "cs.AI", "cs.LG"], "comment": "66 pages", "summary": "This paper introduces TEDI (Truthful, Expressive, and Dimension-Insensitive\napproach), a discretization-free algorithm to learn truthful and\nutility-maximizing mechanisms. Existing learning-based approaches often rely on\ndiscretization of outcome spaces to ensure truthfulness, which leads to\ninefficiency with increasing problem size. To address this limitation, we\nformalize the concept of pricing rules, defined as functions that map outcomes\nto prices. Based on this concept, we propose a novel menu mechanism, which can\nbe equivalent to a truthful direct mechanism under specific conditions. The\ncore idea of TEDI lies in its parameterization of pricing rules using Partial\nGroupMax Network, a new network architecture designed to universally\napproximate partial convex functions. To learn optimal pricing rules, we\ndevelop novel training techniques, including covariance trick and continuous\nsampling, to derive unbiased gradient estimators compatible with first-order\noptimization. Theoretical analysis establishes that TEDI guarantees\ntruthfulness, full expressiveness, and dimension-insensitivity. Experimental\nevaluation in the studied auction setting demonstrates that TEDI achieves\nstrong performance, competitive with or exceeding state-of-the-art methods.\n  This work presents the first approaches to learn truthful mechanisms without\noutcome discretization, thereby enhancing algorithmic efficiency. The proposed\nconcepts, network architecture, and learning techniques might offer potential\nvalue and provide new insights for automated mechanism design and\ndifferentiable economics.", "AI": {"tldr": "TEDI是一种无需离散化的算法，用于学习真实且效用最大化的机制，解决了现有方法因离散化导致的效率问题。", "motivation": "现有基于学习的方法通常依赖结果空间的离散化以确保真实性，但随着问题规模的增加会导致效率低下。", "method": "提出了一种基于定价规则的新菜单机制，使用Partial GroupMax Network参数化定价规则，并开发了新的训练技术（如协方差技巧和连续采样）来学习最优定价规则。", "result": "TEDI在理论分析中保证了真实性、完全表达性和维度不敏感性，实验表明其性能优于或与现有方法相当。", "conclusion": "TEDI是首个无需结果离散化的真实机制学习方法，提升了算法效率，为自动化机制设计和可微分经济学提供了新思路。"}}
{"id": "2506.23382", "pdf": "https://arxiv.org/pdf/2506.23382", "abs": "https://arxiv.org/abs/2506.23382", "authors": ["Vikram Rangarajan", "Shishira Maiya", "Max Ehrlich", "Abhinav Shrivastava"], "title": "SIEDD: Shared-Implicit Encoder with Discrete Decoders", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Project page at https://vikramrangarajan.github.io/SIEDD . Project\n  code at https://github.com/VikramRangarajan/SIEDD", "summary": "Implicit Neural Representations (INRs) offer exceptional fidelity for video\ncompression by learning per-video optimized functions, but their adoption is\ncrippled by impractically slow encoding times. Existing attempts to accelerate\nINR encoding often sacrifice reconstruction quality or crucial coordinate-level\ncontrol essential for adaptive streaming and transcoding. We introduce SIEDD\n(Shared-Implicit Encoder with Discrete Decoders), a novel architecture that\nfundamentally accelerates INR encoding without these compromises. SIEDD first\nrapidly trains a shared, coordinate-based encoder on sparse anchor frames to\nefficiently capture global, low-frequency video features. This encoder is then\nfrozen, enabling massively parallel training of lightweight, discrete decoders\nfor individual frame groups, further expedited by aggressive coordinate-space\nsampling. This synergistic design delivers a remarkable 20-30X encoding\nspeed-up over state-of-the-art INR codecs on HD and 4K benchmarks, while\nmaintaining competitive reconstruction quality and compression ratios.\nCritically, SIEDD retains full coordinate-based control, enabling continuous\nresolution decoding and eliminating costly transcoding. Our approach\nsignificantly advances the practicality of high-fidelity neural video\ncompression, demonstrating a scalable and efficient path towards real-world\ndeployment. Our codebase is available at\nhttps://github.com/VikramRangarajan/SIEDD .", "AI": {"tldr": "SIEDD通过共享编码器和离散解码器架构，显著加速了隐式神经表示（INR）的视频编码速度（20-30倍），同时保持重建质量和压缩比。", "motivation": "现有INR编码方法速度慢，且加速时往往牺牲重建质量或坐标级控制能力。", "method": "SIEDD采用共享坐标编码器和轻量级离散解码器，通过稀疏锚帧训练和并行化设计实现快速编码。", "result": "在HD和4K基准测试中，SIEDD实现了20-30倍的编码速度提升，同时保持高质量重建和压缩比。", "conclusion": "SIEDD为高保真神经视频压缩提供了实用且高效的解决方案，支持连续分辨率解码和无损转码。"}}
{"id": "2506.22929", "pdf": "https://arxiv.org/pdf/2506.22929", "abs": "https://arxiv.org/abs/2506.22929", "authors": ["Chen Zhang"], "title": "Mathematical Computation on High-dimensional Data via Array Programming and Parallel Acceleration", "categories": ["cs.LG", "cs.AI", "eess.IV", "eess.SP"], "comment": null, "summary": "While deep learning excels in natural image and language processing, its\napplication to high-dimensional data faces computational challenges due to the\ndimensionality curse. Current large-scale data tools focus on business-oriented\ndescriptive statistics, lacking mathematical statistics support for advanced\nanalysis. We propose a parallel computation architecture based on space\ncompleteness, decomposing high-dimensional data into dimension-independent\nstructures for distributed processing. This framework enables seamless\nintegration of data mining and parallel-optimized machine learning methods,\nsupporting scientific computations across diverse data types like medical and\nnatural images within a unified system.", "AI": {"tldr": "提出一种基于空间完备性的并行计算架构，用于处理高维数据，支持科学计算和机器学习方法的无缝集成。", "motivation": "深度学习在高维数据应用中面临计算挑战，现有工具缺乏对高级分析的数学统计支持。", "method": "通过空间完备性将高维数据分解为维度无关的结构，实现分布式处理。", "result": "该框架支持跨数据类型（如医学和自然图像）的科学计算，并集成了数据挖掘和并行优化的机器学习方法。", "conclusion": "提出的架构为高维数据处理提供了高效且统一的解决方案。"}}
{"id": "2506.23414", "pdf": "https://arxiv.org/pdf/2506.23414", "abs": "https://arxiv.org/abs/2506.23414", "authors": ["Ming-Zher Poh", "Jonathan Wang", "Jonathan Hsu", "Lawrence Cai", "Eric Teasley", "James A. Taylor", "Jameson K. Rogers", "Anupam Pathak", "Shwetak Patel"], "title": "A High-Throughput Platform to Bench Test Smartphone-Based Heart Rate Measurements Derived From Video", "categories": ["cs.CV"], "comment": null, "summary": "Smartphone-based heart rate (HR) monitoring apps using finger-over-camera\nphotoplethysmography (PPG) face significant challenges in performance\nevaluation and device compatibility due to device variability and\nfragmentation. Manual testing is impractical, and standardized methods are\nlacking. This paper presents a novel, high-throughput bench-testing platform to\naddress this critical need. We designed a system comprising a test rig capable\nof holding 12 smartphones for parallel testing, a method for generating\nsynthetic PPG test videos with controllable HR and signal quality, and a host\nmachine for coordinating video playback and data logging. The system achieved a\nmean absolute percentage error (MAPE) of 0.11% +/- 0.001% between input and\nmeasured HR, and a correlation coefficient of 0.92 +/- 0.008 between input and\nmeasured PPG signals using a clinically-validated smartphone-based HR app.\nBench-testing results of 20 different smartphone models correctly classified\nall the devices as meeting the ANSI/CTA accuracy standards for HR monitors\n(MAPE <10%) when compared to a prospective clinical study with 80 participants,\ndemonstrating high positive predictive value. This platform offers a scalable\nsolution for pre-deployment testing of smartphone HR apps to improve app\nperformance, ensure device compatibility, and advance the field of mobile\nhealth.", "AI": {"tldr": "本文提出了一种高通量测试平台，用于评估智能手机心率监测应用的性能，解决了设备兼容性和标准化测试的难题。", "motivation": "由于智能手机设备的多样性和碎片化，基于摄像头的心率监测应用在性能评估和设备兼容性方面面临挑战，缺乏标准化测试方法。", "method": "设计了一个包含12部智能手机并行测试的测试装置，生成可控心率和信号质量的合成PPG测试视频，并通过主机协调视频播放和数据记录。", "result": "系统在输入和测量心率之间的平均绝对百分比误差为0.11%，PPG信号的相关系数为0.92，测试结果符合ANSI/CTA标准。", "conclusion": "该平台为智能手机心率应用提供了可扩展的预部署测试解决方案，有助于提升应用性能和设备兼容性，推动移动健康领域发展。"}}
{"id": "2506.22941", "pdf": "https://arxiv.org/pdf/2506.22941", "abs": "https://arxiv.org/abs/2506.22941", "authors": ["Kaixuan Wang", "Jason T. Jacques", "Chenxin Diao"], "title": "Positioning AI Tools to Support Online Harm Reduction Practice: Applications and Design Directions", "categories": ["cs.HC", "cs.AI"], "comment": "16 pages, 4 figures, with appendix", "summary": "Access to accurate and actionable harm reduction information can directly\nimpact the health outcomes of People Who Use Drugs (PWUD), yet existing online\nchannels often fail to meet their diverse and dynamic needs due to limitations\nin adaptability, accessibility, and the pervasive impact of stigma. Large\nLanguage Models (LLMs) present a novel opportunity to enhance information\nprovision, but their application in such a high-stakes domain is under-explored\nand presents socio-technical challenges. This paper investigates how LLMs can\nbe responsibly designed to support the information needs of PWUD. Through a\nqualitative workshop involving diverse stakeholder groups (academics, harm\nreduction practitioners, and an online community moderator), we explored LLM\ncapabilities, identified potential use cases, and delineated core design\nconsiderations. Our findings reveal that while LLMs can address some existing\ninformation barriers (e.g., by offering responsive, multilingual, and\npotentially less stigmatising interactions), their effectiveness is contingent\nupon overcoming challenges related to ethical alignment with harm reduction\nprinciples, nuanced contextual understanding, effective communication, and\nclearly defined operational boundaries. We articulate design pathways\nemphasising collaborative co-design with experts and PWUD to develop LLM\nsystems that are helpful, safe, and responsibly governed. This work contributes\nempirically grounded insights and actionable design considerations for the\nresponsible development of LLMs as supportive tools within the harm reduction\necosystem.", "AI": {"tldr": "研究探讨如何利用大型语言模型（LLMs）为吸毒者（PWUD）提供准确且实用的减害信息，通过定性研讨会识别设计挑战与机遇。", "motivation": "现有在线渠道在适应性、可访问性和减少污名化方面不足，LLMs为改善信息提供提供了新机会，但其在高风险领域的应用尚未充分探索。", "method": "通过定性研讨会，汇集学者、减害实践者和在线社区管理者，探讨LLMs的潜力、用例和设计考量。", "result": "LLMs能解决部分信息障碍（如多语言、低污名化互动），但需克服伦理对齐、情境理解等挑战。", "conclusion": "强调与专家和PWUD共同设计，开发安全、有效的LLM系统，为减害领域提供支持。"}}
{"id": "2506.23418", "pdf": "https://arxiv.org/pdf/2506.23418", "abs": "https://arxiv.org/abs/2506.23418", "authors": ["Parham Rezaei", "Arash Marioriyad", "Mahdieh Soleymani Baghshah", "Mohammad Hossein Rohban"], "title": "Why Settle for Mid: A Probabilistic Viewpoint to Spatial Relationship Alignment in Text-to-image Models", "categories": ["cs.CV"], "comment": "12 main pages, 18 figures, and 16 tables", "summary": "Despite the ability of text-to-image models to generate high-quality,\nrealistic, and diverse images, they face challenges in compositional\ngeneration, often struggling to accurately represent details specified in the\ninput prompt. A prevalent issue in compositional generation is the misalignment\nof spatial relationships, as models often fail to faithfully generate images\nthat reflect the spatial configurations specified between objects in the input\nprompts. To address this challenge, we propose a novel probabilistic framework\nfor modeling the relative spatial positioning of objects in a scene, leveraging\nthe concept of Probability of Superiority (PoS). Building on this insight, we\nmake two key contributions. First, we introduce a novel evaluation metric,\nPoS-based Evaluation (PSE), designed to assess the alignment of 2D and 3D\nspatial relationships between text and image, with improved adherence to human\njudgment. Second, we propose PoS-based Generation (PSG), an inference-time\nmethod that improves the alignment of 2D and 3D spatial relationships in T2I\nmodels without requiring fine-tuning. PSG employs a Part-of-Speech PoS-based\nreward function that can be utilized in two distinct ways: (1) as a\ngradient-based guidance mechanism applied to the cross-attention maps during\nthe denoising steps, or (2) as a search-based strategy that evaluates a set of\ninitial noise vectors to select the best one. Extensive experiments demonstrate\nthat the PSE metric exhibits stronger alignment with human judgment compared to\ntraditional center-based metrics, providing a more nuanced and reliable measure\nof complex spatial relationship accuracy in text-image alignment. Furthermore,\nPSG significantly enhances the ability of text-to-image models to generate\nimages with specified spatial configurations, outperforming state-of-the-art\nmethods across multiple evaluation metrics and benchmarks.", "AI": {"tldr": "论文提出了一种基于概率优势（PoS）的框架，用于改进文本到图像模型在空间关系生成上的准确性，并引入了新的评估指标（PSE）和生成方法（PSG）。", "motivation": "文本到图像模型在生成复杂空间关系时存在困难，尤其是无法准确反映输入提示中的空间配置。", "method": "提出PoS框架，包括PSE评估指标和PSG生成方法，后者通过梯度引导或搜索策略优化空间关系生成。", "result": "PSE指标比传统方法更符合人类判断，PSG方法显著提升了空间关系生成的准确性。", "conclusion": "PoS框架有效解决了文本到图像模型在空间关系生成上的挑战，提供了更可靠的评估和生成方法。"}}
{"id": "2506.22949", "pdf": "https://arxiv.org/pdf/2506.22949", "abs": "https://arxiv.org/abs/2506.22949", "authors": ["Ehsan Hallaji", "Vaishnavi Shanmugam", "Roozbeh Razavi-Far", "Mehrdad Saif"], "title": "A Study on Semi-Supervised Detection of DDoS Attacks under Class Imbalance", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "Accepted for publication in IEEE CCECE 2025", "summary": "One of the most difficult challenges in cybersecurity is eliminating\nDistributed Denial of Service (DDoS) attacks. Automating this task using\nartificial intelligence is a complex process due to the inherent class\nimbalance and lack of sufficient labeled samples of real-world datasets. This\nresearch investigates the use of Semi-Supervised Learning (SSL) techniques to\nimprove DDoS attack detection when data is imbalanced and partially labeled. In\nthis process, 13 state-of-the-art SSL algorithms are evaluated for detecting\nDDoS attacks in several scenarios. We evaluate their practical efficacy and\nshortcomings, including the extent to which they work in extreme environments.\nThe results will offer insight into designing intelligent Intrusion Detection\nSystems (IDSs) that are robust against class imbalance and handle partially\nlabeled data.", "AI": {"tldr": "研究探讨了半监督学习（SSL）在解决DDoS攻击检测中数据不平衡和部分标签问题中的应用，评估了13种SSL算法的效果。", "motivation": "DDoS攻击检测面临数据不平衡和标签不足的挑战，传统方法难以应对，因此探索SSL技术的潜力。", "method": "评估了13种先进的SSL算法，在不同场景下测试其对DDoS攻击的检测能力，并分析其优缺点。", "result": "研究结果为设计鲁棒的入侵检测系统（IDS）提供了参考，尤其是在处理数据不平衡和部分标签问题时。", "conclusion": "SSL技术能有效提升DDoS攻击检测的准确性，尤其在极端环境下表现突出，为未来IDS设计提供了新思路。"}}
{"id": "2506.23426", "pdf": "https://arxiv.org/pdf/2506.23426", "abs": "https://arxiv.org/abs/2506.23426", "authors": ["Menna Taha", "Aya Ahmed", "Mohammed Karmoose", "Yasser Gadallah"], "title": "Detecting What Matters: A Novel Approach for Out-of-Distribution 3D Object Detection in Autonomous Vehicles", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Autonomous vehicles (AVs) use object detection models to recognize their\nsurroundings and make driving decisions accordingly. Conventional object\ndetection approaches classify objects into known classes, which limits the AV's\nability to detect and appropriately respond to Out-of-Distribution (OOD)\nobjects. This problem is a significant safety concern since the AV may fail to\ndetect objects or misclassify them, which can potentially lead to hazardous\nsituations such as accidents. Consequently, we propose a novel object detection\napproach that shifts the emphasis from conventional class-based classification\nto object harmfulness determination. Instead of object detection by their\nspecific class, our method identifies them as either 'harmful' or 'harmless'\nbased on whether they pose a danger to the AV. This is done based on the object\nposition relative to the AV and its trajectory. With this metric, our model can\neffectively detect previously unseen objects to enable the AV to make safer\nreal-time decisions. Our results demonstrate that the proposed model\neffectively detects OOD objects, evaluates their harmfulness, and classifies\nthem accordingly, thus enhancing the AV decision-making effectiveness in\ndynamic environments.", "AI": {"tldr": "论文提出了一种新的目标检测方法，将重点从传统的基于类别的分类转向对象危害性判定，以提高自动驾驶车辆对未知物体的检测能力。", "motivation": "传统目标检测方法局限于已知类别，无法有效检测和处理分布外（OOD）物体，可能导致安全隐患。", "method": "通过对象相对于自动驾驶车辆的位置和轨迹，将其分类为‘有害’或‘无害’，而非具体类别。", "result": "模型能有效检测OOD物体并评估其危害性，从而提升自动驾驶车辆在动态环境中的决策能力。", "conclusion": "该方法显著增强了自动驾驶车辆对未知物体的检测和响应能力，提高了安全性。"}}
{"id": "2506.23434", "pdf": "https://arxiv.org/pdf/2506.23434", "abs": "https://arxiv.org/abs/2506.23434", "authors": ["Tianran Liu", "Shengwen Zhao", "Nicholas Rhinehart"], "title": "Towards foundational LiDAR world models with efficient latent flow matching", "categories": ["cs.CV", "cs.RO"], "comment": "25 pages, 13 figures", "summary": "LiDAR-based world models offer more structured and geometry-aware\nrepresentations than their image-based counterparts. However, existing LiDAR\nworld models are narrowly trained; each model excels only in the domain for\nwhich it was built. Can we develop LiDAR world models that exhibit strong\ntransferability across multiple domains? We conduct the first systematic domain\ntransfer study across three demanding scenarios: (i) outdoor to indoor\ngeneralization, (ii) sparse-beam \\& dense-beam adaptation, and (iii)\nnon-semantic to semantic transfer. Given different amounts of fine-tuning data,\nour experiments show that a single pre-trained model can achieve up to 11%\nabsolute improvement (83\\% relative) over training from scratch and outperforms\ntraining from scratch in 30/36 of our comparisons. This transferability of\ndynamic learning significantly reduces the reliance on manually annotated data\nfor semantic occupancy forecasting: our method exceed the previous semantic\noccupancy forecasting models with only 5% of the labeled training data required\nby prior models. We also observed inefficiencies of current LiDAR world models,\nmainly through their under-compression of LiDAR data and inefficient training\nobjectives. To address this, we propose a latent conditional flow matching\n(CFM)-based frameworks that achieves state-of-the-art reconstruction accuracy\nusing only half the training data and a compression ratio 6 times higher than\nthat of prior methods. Our model achieves SOTA performance on\nfuture-trajectory-conditioned semantic occupancy forecasting while being 23x\nmore computationally efficient (a 28x FPS speedup); and achieves SOTA\nperformance on semantic occupancy forecasting while being 2x more\ncomputationally efficient (a 1.1x FPS speedup).", "AI": {"tldr": "LiDAR世界模型在跨领域迁移中表现出色，显著减少对标注数据的依赖，并提出高效训练框架。", "motivation": "探索LiDAR世界模型在多领域的可迁移性，解决现有模型领域狭窄和训练效率低的问题。", "method": "进行跨领域迁移研究，提出基于条件流匹配（CFM）的高效训练框架。", "result": "单预训练模型在多个任务中优于从头训练，标注数据需求减少95%，计算效率显著提升。", "conclusion": "LiDAR世界模型在多领域迁移中具有潜力，高效框架显著提升性能与效率。"}}
{"id": "2506.22968", "pdf": "https://arxiv.org/pdf/2506.22968", "abs": "https://arxiv.org/abs/2506.22968", "authors": ["Daniel Mwesigwa"], "title": "Against 'softmaxing' culture", "categories": ["cs.HC", "cs.AI"], "comment": "7 pages", "summary": "AI is flattening culture. Evaluations of \"culture\" are showing the myriad\nways in which large AI models are homogenizing language and culture, averaging\nout rich linguistic differences into generic expressions. I call this\nphenomenon \"softmaxing culture,\" and it is one of the fundamental challenges\nfacing AI evaluations today. Efforts to improve and strengthen evaluations of\nculture are central to the project of cultural alignment in large AI systems.\nThis position paper argues that machine learning (ML) and human-computer\ninteraction (HCI) approaches to evaluation are limited. I propose two key\nshifts. First, instead of asking \"what is culture?\" at the start of system\nevaluations, I propose beginning with the question: \"when is culture?\" Second,\nwhile I acknowledge the philosophical claim that cultural universals exist, the\nchallenge is not simply to describe them, but to situate them in relation to\ntheir particulars. Taken together, these conceptual shifts invite evaluation\napproaches that move beyond technical requirements, toward perspectives more\nresponsive to the complexities of culture.", "AI": {"tldr": "论文探讨了AI如何通过“softmaxing culture”现象同质化语言和文化，提出评估文化时应从“文化是什么”转向“文化何时存在”，并强调需结合普遍性与特殊性。", "motivation": "AI模型正在同质化语言和文化，导致文化多样性被平均化。现有评估方法（如ML和HCI）对此问题有限，需新的视角。", "method": "提出两个关键转变：1. 从“文化是什么”转向“文化何时存在”；2. 将文化普遍性与特殊性结合。", "result": "现有评估方法不足以应对文化复杂性，需更灵活的框架。", "conclusion": "呼吁超越技术需求，采用更响应文化复杂性的评估方法。"}}
{"id": "2506.23440", "pdf": "https://arxiv.org/pdf/2506.23440", "abs": "https://arxiv.org/abs/2506.23440", "authors": ["Mahesh Bhosale", "Abdul Wasi", "Yuanhao Zhai", "Yunjie Tian", "Samuel Border", "Nan Xi", "Pinaki Sarder", "Junsong Yuan", "David Doermann", "Xuan Gong"], "title": "PathDiff: Histopathology Image Synthesis with Unpaired Text and Mask Conditions", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025", "summary": "Diffusion-based generative models have shown promise in synthesizing\nhistopathology images to address data scarcity caused by privacy constraints.\nDiagnostic text reports provide high-level semantic descriptions, and masks\noffer fine-grained spatial structures essential for representing distinct\nmorphological regions. However, public datasets lack paired text and mask data\nfor the same histopathological images, limiting their joint use in image\ngeneration. This constraint restricts the ability to fully exploit the benefits\nof combining both modalities for enhanced control over semantics and spatial\ndetails. To overcome this, we propose PathDiff, a diffusion framework that\neffectively learns from unpaired mask-text data by integrating both modalities\ninto a unified conditioning space. PathDiff allows precise control over\nstructural and contextual features, generating high-quality, semantically\naccurate images. PathDiff also improves image fidelity, text-image alignment,\nand faithfulness, enhancing data augmentation for downstream tasks like nuclei\nsegmentation and classification. Extensive experiments demonstrate its\nsuperiority over existing methods.", "AI": {"tldr": "PathDiff是一种扩散框架，利用未配对的掩码-文本数据生成高质量的病理学图像，提升语义和空间细节控制。", "motivation": "解决病理学图像数据稀缺问题，同时利用文本和掩码数据增强图像生成的语义和空间控制。", "method": "提出PathDiff框架，将未配对的掩码和文本数据整合到统一的条件空间中，生成高质量图像。", "result": "PathDiff在图像保真度、文本-图像对齐和下游任务（如核分割和分类）中表现优于现有方法。", "conclusion": "PathDiff通过结合文本和掩码数据，显著提升了病理学图像生成的质量和控制能力。"}}
{"id": "2506.23460", "pdf": "https://arxiv.org/pdf/2506.23460", "abs": "https://arxiv.org/abs/2506.23460", "authors": ["Dewen Zeng", "Xinrong Hu", "Yu-Jen Chen", "Yawen Wu", "Xiaowei Xu", "Yiyu Shi"], "title": "Contrastive Learning with Diffusion Features for Weakly Supervised Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Weakly supervised semantic segmentation (WSSS) methods using class labels\noften rely on class activation maps (CAMs) to localize objects. However,\ntraditional CAM-based methods struggle with partial activations and imprecise\nobject boundaries due to optimization discrepancies between classification and\nsegmentation. Recently, the conditional diffusion model (CDM) has been used as\nan alternative for generating segmentation masks in WSSS, leveraging its strong\nimage generation capabilities tailored to specific class distributions. By\nmodifying or perturbing the condition during diffusion sampling, the related\nobjects can be highlighted in the generated images. Yet, the saliency maps\ngenerated by CDMs are prone to noise from background alterations during reverse\ndiffusion. To alleviate the problem, we introduce Contrastive Learning with\nDiffusion Features (CLDF), a novel method that uses contrastive learning to\ntrain a pixel decoder to map the diffusion features from a frozen CDM to a\nlow-dimensional embedding space for segmentation. Specifically, we integrate\ngradient maps generated from CDM external classifier with CAMs to identify\nforeground and background pixels with fewer false positives/negatives for\ncontrastive learning, enabling robust pixel embedding learning. Experimental\nresults on four segmentation tasks from two public medical datasets demonstrate\nthat our method significantly outperforms existing baselines.", "AI": {"tldr": "提出了一种基于对比学习和扩散特征的新方法（CLDF），用于弱监督语义分割（WSSS），显著优于现有基线。", "motivation": "传统基于CAM的方法在对象定位中存在部分激活和边界不精确的问题，而基于条件扩散模型（CDM）的方法在生成分割掩码时易受背景噪声干扰。", "method": "结合对比学习与扩散特征（CLDF），利用冻结CDM的特征映射到低维嵌入空间，并通过梯度图与CAM整合以减少误判。", "result": "在两个公共医学数据集的四个分割任务中，CLDF显著优于现有基线。", "conclusion": "CLDF通过对比学习和扩散特征的结合，有效解决了WSSS中的噪声和误判问题，提升了分割性能。"}}
{"id": "2506.23014", "pdf": "https://arxiv.org/pdf/2506.23014", "abs": "https://arxiv.org/abs/2506.23014", "authors": ["Wilder Baldwin", "Shashank Chintakuntla", "Shreyah Parajuli", "Ali Pourghasemi", "Ryan Shanz", "Sepideh Ghanavati"], "title": "Generating Privacy Stories From Software Documentation", "categories": ["cs.SE", "cs.AI"], "comment": "Accepted to RENext!'25 at the 33rd IEEE International Requirements\n  Engineering 2025 conference", "summary": "Research shows that analysts and developers consider privacy as a security\nconcept or as an afterthought, which may lead to non-compliance and violation\nof users' privacy. Most current approaches, however, focus on extracting legal\nrequirements from the regulations and evaluating the compliance of software and\nprocesses with them. In this paper, we develop a novel approach based on\nchain-of-thought prompting (CoT), in-context-learning (ICL), and Large Language\nModels (LLMs) to extract privacy behaviors from various software documents\nprior to and during software development, and then generate privacy\nrequirements in the format of user stories. Our results show that most commonly\nused LLMs, such as GPT-4o and Llama 3, can identify privacy behaviors and\ngenerate privacy user stories with F1 scores exceeding 0.8. We also show that\nthe performance of these models could be improved through parameter-tuning. Our\nfindings provide insight into using and optimizing LLMs for generating privacy\nrequirements given software documents created prior to or throughout the\nsoftware development lifecycle.", "AI": {"tldr": "论文提出了一种基于CoT、ICL和LLMs的新方法，从软件文档中提取隐私行为并生成隐私用户故事，结果显示主流LLMs表现良好且可通过调参优化。", "motivation": "当前隐私常被视为安全概念或事后考虑，导致合规问题和隐私侵犯，现有方法多聚焦法规合规性评估。", "method": "结合CoT、ICL和LLMs，从软件文档中提取隐私行为并生成隐私用户故事。", "result": "主流LLMs（如GPT-4o和Llama 3）在隐私行为识别和用户故事生成上F1分数超过0.8，调参可进一步提升性能。", "conclusion": "研究为LLMs在软件开发生命周期中生成隐私需求的应用与优化提供了见解。"}}
{"id": "2506.23461", "pdf": "https://arxiv.org/pdf/2506.23461", "abs": "https://arxiv.org/abs/2506.23461", "authors": ["Yun Xing", "Qing Guo", "Xiaoguang Li", "Yihao Huang", "Xiaofeng Cao", "Di Lin", "Ivor Tsang", "Lei Ma"], "title": "Time-variant Image Inpainting via Interactive Distribution Transition Estimation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In this work, we focus on a novel and practical task, i.e., Time-vAriant\niMage inPainting (TAMP). The aim of TAMP is to restore a damaged target image\nby leveraging the complementary information from a reference image, where both\nimages captured the same scene but with a significant time gap in between,\ni.e., time-variant images. Different from conventional reference-guided image\ninpainting, the reference image under TAMP setup presents significant content\ndistinction to the target image and potentially also suffers from damages. Such\nan application frequently happens in our daily lives to restore a damaged image\nby referring to another reference image, where there is no guarantee of the\nreference image's source and quality. In particular, our study finds that even\nstate-of-the-art (SOTA) reference-guided image inpainting methods fail to\nachieve plausible results due to the chaotic image complementation. To address\nsuch an ill-posed problem, we propose a novel Interactive Distribution\nTransition Estimation (InDiTE) module which interactively complements the\ntime-variant images with adaptive semantics thus facilitate the restoration of\ndamaged regions. To further boost the performance, we propose our TAMP\nsolution, namely Interactive Distribution Transition Estimation-driven\nDiffusion (InDiTE-Diff), which integrates InDiTE with SOTA diffusion model and\nconducts latent cross-reference during sampling. Moreover, considering the lack\nof benchmarks for TAMP task, we newly assembled a dataset, i.e., TAMP-Street,\nbased on existing image and mask datasets. We conduct experiments on the\nTAMP-Street datasets under two different time-variant image inpainting\nsettings, which show our method consistently outperform SOTA reference-guided\nimage inpainting methods for solving TAMP.", "AI": {"tldr": "本文提出了一种新的时间变异图像修复任务（TAMP），通过参考图像修复目标图像，解决了时间差异和图像损坏带来的挑战。作者提出了InDiTE模块和InDiTE-Diff方法，显著提升了修复效果，并构建了TAMP-Street数据集进行验证。", "motivation": "解决时间变异图像修复中的挑战，即参考图像与目标图像内容差异大且可能损坏，现有方法效果不佳。", "method": "提出InDiTE模块自适应补充语义，结合扩散模型提出InDiTE-Diff方法，并在采样时进行潜在交叉参考。", "result": "实验表明，InDiTE-Diff在TAMP-Street数据集上优于现有方法。", "conclusion": "InDiTE-Diff方法有效解决了时间变异图像修复问题，为实际应用提供了新思路。"}}
{"id": "2506.23023", "pdf": "https://arxiv.org/pdf/2506.23023", "abs": "https://arxiv.org/abs/2506.23023", "authors": ["M. Youssef Abdelhamid", "Lennart Vater", "Zlatan Ajanovic"], "title": "Scenario-Based Hierarchical Reinforcement Learning for Automated Driving Decision Making", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "6 pages, 10 figures, submitted to a conference", "summary": "Developing decision-making algorithms for highly automated driving systems\nremains challenging, since these systems have to operate safely in an open and\ncomplex environments. Reinforcement Learning (RL) approaches can learn\ncomprehensive decision policies directly from experience and already show\npromising results in simple driving tasks. However, current approaches fail to\nachieve generalizability for more complex driving tasks and lack learning\nefficiency. Therefore, we present Scenario-based Automated Driving\nReinforcement Learning (SAD-RL), the first framework that integrates\nReinforcement Learning (RL) of hierarchical policy in a scenario-based\nenvironment. A high-level policy selects maneuver templates that are evaluated\nand executed by a low-level control logic. The scenario-based environment\nallows to control the training experience for the agent and to explicitly\nintroduce challenging, but rate situations into the training process. Our\nexperiments show that an agent trained using the SAD-RL framework can achieve\nsafe behaviour in easy as well as challenging situations efficiently. Our\nablation studies confirmed that both HRL and scenario diversity are essential\nfor achieving these results.", "AI": {"tldr": "提出SAD-RL框架，结合分层强化学习和场景化环境，提升自动驾驶决策算法的泛化能力和学习效率。", "motivation": "自动驾驶系统需在复杂开放环境中安全运行，但现有强化学习方法在复杂任务中泛化性和学习效率不足。", "method": "SAD-RL框架整合分层强化学习（高层策略选择动作模板，低层逻辑执行）和场景化环境，控制训练体验并引入挑战性场景。", "result": "实验表明，SAD-RL训练的智能体在简单和复杂场景中均能高效实现安全行为，分层学习和场景多样性是关键。", "conclusion": "SAD-RL通过分层策略和场景化训练，显著提升了自动驾驶决策算法的性能和泛化能力。"}}
{"id": "2506.23465", "pdf": "https://arxiv.org/pdf/2506.23465", "abs": "https://arxiv.org/abs/2506.23465", "authors": ["Nazanin Mahjourian", "Vinh Nguyen"], "title": "Sanitizing Manufacturing Dataset Labels Using Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "The success of machine learning models in industrial applications is heavily\ndependent on the quality of the datasets used to train the models. However,\nlarge-scale datasets, specially those constructed from crowd-sourcing and\nweb-scraping, often suffer from label noise, inconsistencies, and errors. This\nproblem is particularly pronounced in manufacturing domains, where obtaining\nhigh-quality labels is costly and time-consuming. This paper introduces\nVision-Language Sanitization and Refinement (VLSR), which is a\nvision-language-based framework for label sanitization and refinement in\nmulti-label manufacturing image datasets. This method embeds both images and\ntheir associated textual labels into a shared semantic space leveraging the\nCLIP vision-language model. Then two key tasks are addressed in this process by\ncomputing the cosine similarity between embeddings. First, label sanitization\nis performed to identify irrelevant, misspelled, or semantically weak labels,\nand surface the most semantically aligned label for each image by comparing\nimage-label pairs using cosine similarity between image and label embeddings.\nSecond, the method applies density-based clustering on text embeddings,\nfollowed by iterative cluster merging, to group semantically similar labels\ninto unified label groups. The Factorynet dataset, which includes noisy labels\nfrom both human annotations and web-scraped sources, is employed to evaluate\nthe effectiveness of the proposed framework. Experimental results demonstrate\nthat the VLSR framework successfully identifies problematic labels and improves\nlabel consistency. This method enables a significant reduction in label\nvocabulary through clustering, which ultimately enhances the dataset's quality\nfor training robust machine learning models in industrial applications with\nminimal human intervention.", "AI": {"tldr": "本文提出了一种基于视觉-语言的标签清洗与优化框架（VLSR），用于处理制造业多标签图像数据集中的噪声标签问题，通过CLIP模型嵌入图像和文本标签到共享语义空间，利用余弦相似度和聚类技术提升标签质量。", "motivation": "工业应用中机器学习模型的性能高度依赖数据集质量，但大规模数据集（如众包或网络爬取）常存在标签噪声和不一致问题，尤其在制造业领域，高质量标签获取成本高且耗时。", "method": "VLSR框架利用CLIP模型将图像和标签嵌入共享语义空间，通过余弦相似度进行标签清洗（识别并修正问题标签）和密度聚类（合并语义相似标签）。", "result": "在Factorynet数据集上的实验表明，VLSR能有效识别问题标签并提升标签一致性，显著减少标签词汇量，从而提升数据集质量。", "conclusion": "VLSR框架能以最小人工干预提升工业应用数据集的标签质量，为训练鲁棒模型提供支持。"}}
{"id": "2506.23024", "pdf": "https://arxiv.org/pdf/2506.23024", "abs": "https://arxiv.org/abs/2506.23024", "authors": ["Jerry Liu", "Yasa Baig", "Denise Hui Jean Lee", "Rajat Vadiraj Dwaraknath", "Atri Rudra", "Chris Ré"], "title": "BWLer: Barycentric Weight Layer Elucidates a Precision-Conditioning Tradeoff for PINNs", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA"], "comment": "Workshop for the Theory of AI for Scientific Computing @ COLT 2025\n  (Best Paper). 39 pages, 24 figures", "summary": "Physics-informed neural networks (PINNs) offer a flexible way to solve\npartial differential equations (PDEs) with machine learning, yet they still\nfall well short of the machine-precision accuracy many scientific tasks demand.\nIn this work, we investigate whether the precision ceiling comes from the\nill-conditioning of the PDEs or from the typical multi-layer perceptron (MLP)\narchitecture. We introduce the Barycentric Weight Layer (BWLer), which models\nthe PDE solution through barycentric polynomial interpolation. A BWLer can be\nadded on top of an existing MLP (a BWLer-hat) or replace it completely\n(explicit BWLer), cleanly separating how we represent the solution from how we\ntake derivatives for the PDE loss. Using BWLer, we identify fundamental\nprecision limitations within the MLP: on a simple 1-D interpolation task, even\nMLPs with O(1e5) parameters stall around 1e-8 RMSE -- about eight orders above\nfloat64 machine precision -- before any PDE terms are added. In PDE learning,\nadding a BWLer lifts this ceiling and exposes a tradeoff between achievable\naccuracy and the conditioning of the PDE loss. For linear PDEs we fully\ncharacterize this tradeoff with an explicit error decomposition and navigate it\nduring training with spectral derivatives and preconditioning. Across five\nbenchmark PDEs, adding a BWLer on top of an MLP improves RMSE by up to 30x for\nconvection, 10x for reaction, and 1800x for wave equations while remaining\ncompatible with first-order optimizers. Replacing the MLP entirely lets an\nexplicit BWLer reach near-machine-precision on convection, reaction, and wave\nproblems (up to 10 billion times better than prior results) and match the\nperformance of standard PINNs on stiff Burgers' and irregular-geometry Poisson\nproblems. Together, these findings point to a practical path for combining the\nflexibility of PINNs with the precision of classical spectral solvers.", "AI": {"tldr": "论文提出Barycentric Weight Layer（BWLer）以解决PINNs在求解PDE时的精度限制问题，通过多项式插值提升精度。", "motivation": "PINNs在求解PDE时精度不足，研究目标是确定精度限制源于PDE的病态性还是MLP架构。", "method": "引入BWLer，通过多项式插值建模PDE解，可叠加或替换MLP，结合谱导数和预处理优化训练。", "result": "BWLer显著提升精度，在多个PDE基准测试中RMSE改善达30x至1800x，甚至接近机器精度。", "conclusion": "BWLer结合了PINNs的灵活性和经典谱求解器的精度，为高精度PDE求解提供实用路径。"}}
{"id": "2506.23467", "pdf": "https://arxiv.org/pdf/2506.23467", "abs": "https://arxiv.org/abs/2506.23467", "authors": ["Chenlang Yi", "Zizhan Xiong", "Qi Qi", "Xiyuan Wei", "Girish Bathla", "Ching-Long Lin", "Bobak Jack Mortazavi", "Tianbao Yang"], "title": "AdFair-CLIP: Adversarial Fair Contrastive Language-Image Pre-training for Chest X-rays", "categories": ["cs.CV", "cs.LG"], "comment": "This preprint has been accepted by MICCAI 2025", "summary": "Contrastive Language-Image Pre-training (CLIP) models have demonstrated\nsuperior performance across various visual tasks including medical image\nclassification. However, fairness concerns, including demographic biases, have\nreceived limited attention for CLIP models. This oversight leads to critical\nissues, particularly those related to race and gender, resulting in disparities\nin diagnostic outcomes and reduced reliability for underrepresented groups. To\naddress these challenges, we introduce AdFair-CLIP, a novel framework employing\nadversarial feature intervention to suppress sensitive attributes, thereby\nmitigating spurious correlations and improving prediction fairness. We conduct\ncomprehensive experiments on chest X-ray (CXR) datasets, and show that\nAdFair-CLIP significantly enhances both fairness and diagnostic accuracy, while\nmaintaining robust generalization in zero-shot and few-shot scenarios. These\nresults establish new benchmarks for fairness-aware learning in CLIP-based\nmedical diagnostic models, particularly for CXR analysis.", "AI": {"tldr": "AdFair-CLIP通过对抗性特征干预减少CLIP模型中的敏感属性偏差，提高胸部X光分类的公平性和准确性。", "motivation": "CLIP模型在医学图像分类中表现优异，但存在种族和性别等公平性问题，可能导致诊断结果不公。", "method": "提出AdFair-CLIP框架，利用对抗性特征干预抑制敏感属性，减少虚假相关性。", "result": "在胸部X光数据集上，AdFair-CLIP显著提升了公平性和诊断准确性，并在零样本和少样本场景中保持稳健。", "conclusion": "AdFair-CLIP为CLIP医学诊断模型设定了公平性学习的新基准，尤其适用于胸部X光分析。"}}
{"id": "2506.23025", "pdf": "https://arxiv.org/pdf/2506.23025", "abs": "https://arxiv.org/abs/2506.23025", "authors": ["Tejas Vaidhya", "Ayush Kaushal", "Vineet Jain", "Francis Couture Harpin", "Prashant Shishodia", "Majid Behbahani", "Yuriy Nevmyvaka", "Irina Rish"], "title": "Spectra 1.1: Scaling Laws and Efficient Inference for Ternary Language Models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) are increasingly used across research and\nindustry applications, yet their inference efficiency remains a significant\nchallenge. As the computational power of modern GPU architectures continuously\nimproves, their memory bandwidth and capacity have not scaled proportionally,\ncreating a critical bottleneck during inference. To address this, we\ninvestigate ternary language models (TriLMs) that employ quantization-aware\ntraining to significantly reduce memory requirements. We first analyze the\nscalability of TriLMs by conducting a scaling law analysis, revealing that\nTriLMs benefit more from increasing training data than from scaling model\nparameters. Based on this observation, we introduce Spectra-1.1, an open suite\nof TriLMs trained on up to 1.2 trillion tokens, demonstrating sustained\nperformance gains at scale. Furthermore, to improve inference efficiency, we\npropose novel 2-bit and 1.6-bit packing schemes for ternary weights, which\ndemonstrate accelerated inference across various CPU architectures. Also,\nbuilding on the 2-bit packing, we develop a GPU kernel called TriRun that\naccelerates end-to-end model inference by up to 5 times compared to\nfloating-point baselines. To encourage further exploration and development of\nTriLMs, we will release the Spectra-1.1 suite and TriRun inference kernels.\nOverall, our work lays the foundation for building and deploying efficient\nLLMs, providing a valuable resource for the research community.", "AI": {"tldr": "论文研究了三元语言模型（TriLMs）及其量化感知训练方法，以减少内存需求并提升推理效率。提出了Spectra-1.1模型套件和TriRun GPU内核，显著加速推理。", "motivation": "大型语言模型（LLMs）的推理效率受限于内存带宽和容量，亟需优化。", "method": "采用量化感知训练的三元语言模型（TriLMs），分析其扩展性，并提出2-bit和1.6-bit权重压缩方案及TriRun GPU内核。", "result": "TriLMs在扩展数据时表现更优；Spectra-1.1和TriRun分别提升了性能和推理速度（最高5倍）。", "conclusion": "为高效LLMs的构建和部署奠定了基础，推动了相关研究的发展。"}}
{"id": "2506.23468", "pdf": "https://arxiv.org/pdf/2506.23468", "abs": "https://arxiv.org/abs/2506.23468", "authors": ["Xuan Yao", "Junyu Gao", "Changsheng Xu"], "title": "NavMorph: A Self-Evolving World Model for Vision-and-Language Navigation in Continuous Environments", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Vision-and-Language Navigation in Continuous Environments (VLN-CE) requires\nagents to execute sequential navigation actions in complex environments guided\nby natural language instructions. Current approaches often struggle with\ngeneralizing to novel environments and adapting to ongoing changes during\nnavigation. Inspired by human cognition, we present NavMorph, a self-evolving\nworld model framework that enhances environmental understanding and\ndecision-making in VLN-CE tasks. NavMorph employs compact latent\nrepresentations to model environmental dynamics, equipping agents with\nforesight for adaptive planning and policy refinement. By integrating a novel\nContextual Evolution Memory, NavMorph leverages scene-contextual information to\nsupport effective navigation while maintaining online adaptability. Extensive\nexperiments demonstrate that our method achieves notable performance\nimprovements on popular VLN-CE benchmarks. Code is available at\n\\href{https://github.com/Feliciaxyao/NavMorph}{this https URL}.", "AI": {"tldr": "NavMorph是一个自演化的世界模型框架，用于提升视觉与语言导航（VLN-CE）任务中的环境理解和决策能力。", "motivation": "当前方法在新环境中的泛化能力和导航过程中的适应性不足，受人类认知启发，提出了NavMorph。", "method": "使用紧凑的潜在表示建模环境动态，结合上下文演化记忆（Contextual Evolution Memory）支持自适应规划和策略优化。", "result": "在VLN-CE基准测试中表现出显著性能提升。", "conclusion": "NavMorph通过自演化和上下文感知，有效提升了导航任务的适应性和性能。"}}
{"id": "2506.23470", "pdf": "https://arxiv.org/pdf/2506.23470", "abs": "https://arxiv.org/abs/2506.23470", "authors": ["Ngoc-Do Tran", "Minh-Tuan Huynh", "Tam V. Nguyen", "Minh-Triet Tran", "Trung-Nghia Le"], "title": "Interactive Interface For Semantic Segmentation Dataset Synthesis", "categories": ["cs.CV"], "comment": null, "summary": "The rapid advancement of AI and computer vision has significantly increased\nthe demand for high-quality annotated datasets, particularly for semantic\nsegmentation. However, creating such datasets is resource-intensive, requiring\nsubstantial time, labor, and financial investment, and often raises privacy\nconcerns due to the use of real-world data. To mitigate these challenges, we\npresent SynthLab, consisting of a modular platform for visual data synthesis\nand a user-friendly interface. The modular architecture of SynthLab enables\neasy maintenance, scalability with centralized updates, and seamless\nintegration of new features. Each module handles distinct aspects of computer\nvision tasks, enhancing flexibility and adaptability. Meanwhile, its\ninteractive, user-friendly interface allows users to quickly customize their\ndata pipelines through drag-and-drop actions. Extensive user studies involving\na diverse range of users across different ages, professions, and expertise\nlevels, have demonstrated flexible usage, and high accessibility of SynthLab,\nenabling users without deep technical expertise to harness AI for real-world\napplications.", "AI": {"tldr": "SynthLab是一个模块化平台，用于合成视觉数据，解决高质量标注数据集资源密集和隐私问题。", "motivation": "AI和计算机视觉的快速发展需要高质量标注数据集，但创建这些数据集成本高且涉及隐私问题。", "method": "SynthLab采用模块化架构和用户友好界面，支持拖拽操作定制数据流程。", "result": "用户研究表明SynthLab具有高灵活性和易用性，适合非技术用户。", "conclusion": "SynthLab为合成视觉数据提供了高效、可扩展的解决方案，降低了资源需求和隐私风险。"}}
{"id": "2506.23040", "pdf": "https://arxiv.org/pdf/2506.23040", "abs": "https://arxiv.org/abs/2506.23040", "authors": ["Samuel J. Weisenthal"], "title": "Treatment, evidence, imitation, and chat", "categories": ["stat.OT", "cs.AI"], "comment": "12 pages", "summary": "Large language models are thought to have potential to aid in medical\ndecision making. We investigate this here. We start with the treatment problem,\nthe patient's core medical decision-making task, which is solved in\ncollaboration with a healthcare provider. We discuss approaches to solving the\ntreatment problem, including -- within evidence-based medicine -- trials and\nobservational data. We then discuss the chat problem, and how this differs from\nthe treatment problem -- in particular as it relates to imitation. We then\ndiscuss how a large language model might be used to solve the treatment problem\nand highlight some of the challenges that emerge. We finally discuss how these\nchallenges relate to evidence-based medicine, and how this might inform next\nsteps.", "AI": {"tldr": "探讨大型语言模型在医疗决策中的潜力，重点分析治疗问题和聊天问题的区别及挑战。", "motivation": "研究大型语言模型如何辅助医疗决策，尤其是患者与医疗提供者协作的治疗问题。", "method": "分析治疗问题的解决途径（如临床试验和观察数据），对比聊天问题，探讨语言模型的应用及挑战。", "result": "识别了语言模型在解决治疗问题中的潜在应用和挑战，并关联到循证医学。", "conclusion": "需进一步研究语言模型在医疗决策中的实际应用，以解决现有挑战。"}}
{"id": "2506.23478", "pdf": "https://arxiv.org/pdf/2506.23478", "abs": "https://arxiv.org/abs/2506.23478", "authors": ["Pedro Alonso", "Tianrui Li", "Chongshou Li"], "title": "GeoCD: A Differential Local Approximation for Geodesic Chamfer Distance", "categories": ["cs.CV"], "comment": null, "summary": "Chamfer Distance (CD) is a widely adopted metric in 3D point cloud learning\ndue to its simplicity and efficiency. However, it suffers from a fundamental\nlimitation: it relies solely on Euclidean distances, which often fail to\ncapture the intrinsic geometry of 3D shapes. To address this limitation, we\npropose GeoCD, a topology-aware and fully differentiable approximation of\ngeodesic distance designed to serve as a metric for 3D point cloud learning.\nOur experiments show that GeoCD consistently improves reconstruction quality\nover standard CD across various architectures and datasets. We demonstrate this\nby fine-tuning several models, initially trained with standard CD, using GeoCD.\nRemarkably, fine-tuning for a single epoch with GeoCD yields significant gains\nacross multiple evaluation metrics.", "AI": {"tldr": "GeoCD是一种基于拓扑感知和完全可微分的地球距离近似方法，用于改进3D点云学习中的Chamfer Distance（CD）指标。", "motivation": "Chamfer Distance（CD）因其简单高效而被广泛采用，但其仅依赖欧几里得距离，无法捕捉3D形状的内在几何特征。", "method": "提出GeoCD，一种拓扑感知且完全可微分的地球距离近似方法，用于替代CD。通过微调原本使用CD训练的模型，验证其效果。", "result": "实验表明，GeoCD在各种架构和数据集上均能显著提升重建质量，仅需一个epoch的微调即可在多指标上取得显著改进。", "conclusion": "GeoCD是一种有效的改进CD的方法，能够更好地捕捉3D形状的几何特征。"}}
{"id": "2506.23479", "pdf": "https://arxiv.org/pdf/2506.23479", "abs": "https://arxiv.org/abs/2506.23479", "authors": ["Zhaojie Zeng", "Yuesong Wang", "Chao Yang", "Tao Guan", "Lili Ju"], "title": "Instant GaussianImage: A Generalizable and Self-Adaptive Image Representation via 2D Gaussian Splatting", "categories": ["cs.CV"], "comment": null, "summary": "Implicit Neural Representation (INR) has demonstrated remarkable advances in\nthe field of image representation but demands substantial GPU resources.\nGaussianImage recently pioneered the use of Gaussian Splatting to mitigate this\ncost, however, the slow training process limits its practicality, and the fixed\nnumber of Gaussians per image limits its adaptability to varying information\nentropy. To address these issues, we propose in this paper a generalizable and\nself-adaptive image representation framework based on 2D Gaussian Splatting.\nOur method employs a network to quickly generate a coarse Gaussian\nrepresentation, followed by minimal fine-tuning steps, achieving comparable\nrendering quality of GaussianImage while significantly reducing training time.\nMoreover, our approach dynamically adjusts the number of Gaussian points based\non image complexity to further enhance flexibility and efficiency in practice.\nExperiments on DIV2K and Kodak datasets show that our method matches or exceeds\nGaussianImage's rendering performance with far fewer iterations and shorter\ntraining times. Specifically, our method reduces the training time by up to one\norder of magnitude while achieving superior rendering performance with the same\nnumber of Gaussians.", "AI": {"tldr": "提出了一种基于2D高斯泼溅的自适应图像表示框架，显著减少训练时间并动态调整高斯点数量。", "motivation": "解决高斯泼溅方法训练慢和固定高斯点数量限制适应性的问题。", "method": "使用网络快速生成粗略高斯表示，再通过少量微调步骤，动态调整高斯点数量。", "result": "在DIV2K和Kodak数据集上，训练时间减少一个数量级，渲染性能优于或匹配GaussianImage。", "conclusion": "该方法在高效性和适应性上优于现有技术。"}}
{"id": "2506.23481", "pdf": "https://arxiv.org/pdf/2506.23481", "abs": "https://arxiv.org/abs/2506.23481", "authors": ["Xian Zhang", "Xiang Cheng"], "title": "Evaluation of Geolocation Capabilities of Multimodal Large Language Models and Analysis of Associated Privacy Risks", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Objectives: The rapid advancement of Multimodal Large Language Models (MLLMs)\nhas significantly enhanced their reasoning capabilities, enabling a wide range\nof intelligent applications. However, these advancements also raise critical\nconcerns regarding privacy and ethics. MLLMs are now capable of inferring the\ngeographic location of images -- such as those shared on social media or\ncaptured from street views -- based solely on visual content, thereby posing\nserious risks of privacy invasion, including doxxing, surveillance, and other\nsecurity threats.\n  Methods: This study provides a comprehensive analysis of existing geolocation\ntechniques based on MLLMs. It systematically reviews relevant litera-ture and\nevaluates the performance of state-of-the-art visual reasoning models on\ngeolocation tasks, particularly in identifying the origins of street view\nimagery.\n  Results: Empirical evaluation reveals that the most advanced visual large\nmodels can successfully localize the origin of street-level imagery with up to\n$49\\%$ accuracy within a 1-kilometer radius. This performance underscores the\nmodels' powerful capacity to extract and utilize fine-grained geographic cues\nfrom visual data.\n  Conclusions: Building on these findings, the study identifies key visual\nelements that contribute to suc-cessful geolocation, such as text,\narchitectural styles, and environmental features. Furthermore, it discusses the\npotential privacy implications associated with MLLM-enabled geolocation and\ndiscuss several technical and policy-based coun-termeasures to mitigate\nassociated risks. Our code and dataset are available at\nhttps://github.com/zxyl1003/MLLM-Geolocation-Evaluation.", "AI": {"tldr": "多模态大语言模型（MLLMs）在图像地理定位方面表现出色，但也引发隐私和伦理问题。研究分析了现有技术，发现模型能在1公里半径内以49%准确率定位街景图像，并探讨了应对措施。", "motivation": "随着MLLMs推理能力的提升，其在图像地理定位方面的应用带来了隐私入侵风险（如人肉搜索、监控等），研究旨在评估技术现状并提出缓解措施。", "method": "系统综述现有文献，评估最先进的视觉推理模型在街景图像地理定位任务中的表现。", "result": "先进视觉大模型在1公里半径内定位准确率达49%，表明其能从视觉数据中提取细粒度地理线索。", "conclusion": "研究总结了成功定位的关键视觉元素（如文字、建筑风格等），并讨论了技术及政策层面的应对措施以降低隐私风险。"}}
{"id": "2506.23055", "pdf": "https://arxiv.org/pdf/2506.23055", "abs": "https://arxiv.org/abs/2506.23055", "authors": ["Hiro Taiyo Hamada", "Ippei Fujisawa", "Genji Kawakita", "Yuki Yamada"], "title": "Measuring How LLMs Internalize Human Psychological Concepts: A preliminary analysis", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) such as ChatGPT have shown remarkable abilities\nin producing human-like text. However, it is unclear how accurately these\nmodels internalize concepts that shape human thought and behavior. Here, we\ndeveloped a quantitative framework to assess concept alignment between LLMs and\nhuman psychological dimensions using 43 standardized psychological\nquestionnaires, selected for their established validity in measuring distinct\npsychological constructs. Our method evaluates how accurately language models\nreconstruct and classify questionnaire items through pairwise similarity\nanalysis. We compared resulting cluster structures with the original\ncategorical labels using hierarchical clustering. A GPT-4 model achieved\nsuperior classification accuracy (66.2\\%), significantly outperforming GPT-3.5\n(55.9\\%) and BERT (48.1\\%), all exceeding random baseline performance (31.9\\%).\nWe also demonstrated that the estimated semantic similarity from GPT-4 is\nassociated with Pearson's correlation coefficients of human responses in\nmultiple psychological questionnaires. This framework provides a novel approach\nto evaluate the alignment of the human-LLM concept and identify potential\nrepresentational biases. Our findings demonstrate that modern LLMs can\napproximate human psychological constructs with measurable accuracy, offering\ninsights for developing more interpretable AI systems.", "AI": {"tldr": "该论文提出了一个定量框架，通过43个标准化心理问卷评估大型语言模型（如GPT-4）与人类心理维度的概念对齐，发现GPT-4在分类准确性上显著优于其他模型。", "motivation": "研究动机是探究大型语言模型是否准确内化了塑造人类思维和行为的概念。", "method": "方法是通过心理问卷的成对相似性分析，评估语言模型对问卷项目的重构和分类能力，并使用层次聚类比较结果。", "result": "结果显示，GPT-4的分类准确率最高（66.2%），显著优于GPT-3.5（55.9%）和BERT（48.1%），且与人类反应的皮尔逊相关系数相关。", "conclusion": "结论是现代大型语言模型能以可测量的准确性近似人类心理结构，为开发更可解释的AI系统提供了见解。"}}
{"id": "2506.23482", "pdf": "https://arxiv.org/pdf/2506.23482", "abs": "https://arxiv.org/abs/2506.23482", "authors": ["Jun Huang", "Ting Liu", "Yihang Wu", "Xiaochao Qu", "Luoqi Liu", "Xiaolin Hu"], "title": "MTADiffusion: Mask Text Alignment Diffusion Model for Object Inpainting", "categories": ["cs.CV"], "comment": "CVPR 2025", "summary": "Advancements in generative models have enabled image inpainting models to\ngenerate content within specific regions of an image based on provided prompts\nand masks. However, existing inpainting methods often suffer from problems such\nas semantic misalignment, structural distortion, and style inconsistency. In\nthis work, we present MTADiffusion, a Mask-Text Alignment diffusion model\ndesigned for object inpainting. To enhance the semantic capabilities of the\ninpainting model, we introduce MTAPipeline, an automatic solution for\nannotating masks with detailed descriptions. Based on the MTAPipeline, we\nconstruct a new MTADataset comprising 5 million images and 25 million mask-text\npairs. Furthermore, we propose a multi-task training strategy that integrates\nboth inpainting and edge prediction tasks to improve structural stability. To\npromote style consistency, we present a novel inpainting style-consistency loss\nusing a pre-trained VGG network and the Gram matrix. Comprehensive evaluations\non BrushBench and EditBench demonstrate that MTADiffusion achieves\nstate-of-the-art performance compared to other methods.", "AI": {"tldr": "MTADiffusion是一种用于对象修复的Mask-Text Alignment扩散模型，通过MTAPipeline自动标注掩码和详细描述，构建了包含500万图像和2500万掩码-文本对的数据集，结合多任务训练和风格一致性损失，显著提升了修复效果。", "motivation": "现有修复方法存在语义不对齐、结构扭曲和风格不一致等问题，MTADiffusion旨在解决这些问题。", "method": "提出MTAPipeline自动标注掩码和描述，构建MTADataset；采用多任务训练策略（修复和边缘预测）；引入风格一致性损失。", "result": "在BrushBench和EditBench上，MTADiffusion表现优于其他方法。", "conclusion": "MTADiffusion通过改进语义对齐、结构稳定性和风格一致性，实现了最先进的修复性能。"}}
{"id": "2506.23068", "pdf": "https://arxiv.org/pdf/2506.23068", "abs": "https://arxiv.org/abs/2506.23068", "authors": ["Zhiyu Zhao", "Haoxuan Li", "Haifeng Zhang", "Jun Wang", "Francesco Faccio", "Jürgen Schmidhuber", "Mengyue Yang"], "title": "Curious Causality-Seeking Agents Learn Meta Causal World", "categories": ["cs.LG", "cs.AI", "stat.AP"], "comment": "33 pages", "summary": "When building a world model, a common assumption is that the environment has\na single, unchanging underlying causal rule, like applying Newton's laws to\nevery situation. In reality, what appears as a drifting causal mechanism is\noften the manifestation of a fixed underlying mechanism seen through a narrow\nobservational window. This brings about a problem that, when building a world\nmodel, even subtle shifts in policy or environment states can alter the very\nobserved causal mechanisms. In this work, we introduce the \\textbf{Meta-Causal\nGraph} as world models, a minimal unified representation that efficiently\nencodes the transformation rules governing how causal structures shift across\ndifferent latent world states. A single Meta-Causal Graph is composed of\nmultiple causal subgraphs, each triggered by meta state, which is in the latent\nstate space. Building on this representation, we introduce a\n\\textbf{Causality-Seeking Agent} whose objectives are to (1) identify the meta\nstates that trigger each subgraph, (2) discover the corresponding causal\nrelationships by agent curiosity-driven intervention policy, and (3)\niteratively refine the Meta-Causal Graph through ongoing curiosity-driven\nexploration and agent experiences. Experiments on both synthetic tasks and a\nchallenging robot arm manipulation task demonstrate that our method robustly\ncaptures shifts in causal dynamics and generalizes effectively to previously\nunseen contexts.", "AI": {"tldr": "论文提出了一种名为“元因果图”的世界模型，用于统一表示不同潜在世界状态下因果结构的变化规则，并通过好奇心驱动的探索优化模型。", "motivation": "现实环境中，因果机制常因观察窗口狭窄而显得漂移，导致世界模型构建时因果机制易受策略或环境状态细微变化的影响。", "method": "引入元因果图作为世界模型，包含多个由潜在状态触发的因果子图，并通过好奇心驱动的干预策略发现因果关系。", "result": "在合成任务和机器人手臂操作任务中，该方法能有效捕捉因果动态变化并泛化到新场景。", "conclusion": "元因果图是一种高效表示因果结构变化的方法，好奇心驱动的探索能持续优化模型。"}}
{"id": "2506.23491", "pdf": "https://arxiv.org/pdf/2506.23491", "abs": "https://arxiv.org/abs/2506.23491", "authors": ["ZongHan Hsieh", "Tzer-Jen Wei"], "title": "Qwen-GUI-3B: A Lightweight Vision-Language Model for Cross-Resolution GUI Grounding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "This paper introduces Qwen-GUI-3B, a lightweight Vision-Language Model (VLM)\nspecifically designed for Graphical User Interface grounding tasks, achieving\nperformance competitive with significantly larger models. Unlike large-scale\nVLMs (>7B parameters) that are computationally intensive and impractical for\nconsumer-grade hardware, Qwen-GUI-3B delivers strong grounding accuracy while\nbeing fully trainable on a single GPU (RTX 4090). The model incorporates\nseveral key innovations: (i) combine cross-platform, multi-resolution dataset\nof 24K examples from diverse sources including mobile, desktop, and web GUI\nscreenshots to effectively address data scarcity in high-resolution desktop\nenvironments; (ii) a two-stage fine-tuning strategy, where initial\ncross-platform training establishes robust GUI understanding, followed by\nspecialized fine-tuning on high-resolution data to significantly enhance model\nadaptability; and (iii) data curation and redundancy reduction strategies,\ndemonstrating that randomly sampling a smaller subset with reduced redundancy\nachieves performance comparable to larger datasets, emphasizing data diversity\nover sheer volume. Empirical evaluation on standard GUI grounding\nbenchmarks-including ScreenSpot, ScreenSpot-v2, and the challenging\nScreenSpot-Pro, highlights Qwen-GUI-3B's exceptional accuracy, achieving 84.9%\non ScreenSpot and 86.4% on ScreenSpot-v2, surpassing prior models under 4B\nparameters. Ablation studies validate the critical role of balanced sampling\nand two-stage fine-tuning in enhancing robustness, particularly in\nhigh-resolution desktop scenarios. The Qwen-GUI-3B is available at:\nhttps://github.com/Han1018/Qwen-GUI-3B", "AI": {"tldr": "Qwen-GUI-3B是一个轻量级的视觉语言模型，专为图形用户界面（GUI）任务设计，性能接近更大模型，但可在单GPU上训练。", "motivation": "解决大规模视觉语言模型计算资源需求高的问题，同时提升GUI任务的准确性和适应性。", "method": "结合跨平台多分辨率数据集、两阶段微调策略及数据去冗余技术。", "result": "在标准基准测试中表现优异，如ScreenSpot（84.9%）和ScreenSpot-v2（86.4%）。", "conclusion": "Qwen-GUI-3B在轻量级模型中表现出色，平衡采样和两阶段微调是关键。"}}
{"id": "2506.23085", "pdf": "https://arxiv.org/pdf/2506.23085", "abs": "https://arxiv.org/abs/2506.23085", "authors": ["Saeid Aghasoleymani Najafabadi"], "title": "Enhancing Live Broadcast Engagement: A Multi-modal Approach to Short Video Recommendations Using MMGCN and User Preferences", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "The purpose of this paper is to explore a multi-modal approach to enhancing\nlive broadcast engagement by developing a short video recommendation system\nthat incorporates Multi-modal Graph Convolutional Networks (MMGCN) with user\npreferences. In order to provide personalized recommendations tailored to\nindividual interests, the proposed system takes into account user interaction\ndata, video content features, and contextual information. With the aid of a\nhybrid approach combining collaborative filtering and content-based filtering\ntechniques, the system is able to capture nuanced relationships between users,\nvideo attributes, and engagement patterns. Three datasets are used to evaluate\nthe effectiveness of the system: Kwai, TikTok, and MovieLens. Compared to\nbaseline models, such as DeepFM, Wide & Deep, LightGBM, and XGBoost, the\nproposed MMGCN-based model shows superior performance. A notable feature of the\nproposed model is that it outperforms all baseline methods in capturing diverse\nuser preferences and making accurate, personalized recommendations, resulting\nin a Kwai F1 score of 0.574, a Tiktok F1 score of 0.506, and a MovieLens F1\nscore of 0.197. We emphasize the importance of multi-modal integration and\nuser-centric approaches in advancing recommender systems, emphasizing the role\nthey play in enhancing content discovery and audience interaction on live\nbroadcast platforms.", "AI": {"tldr": "本文提出了一种基于多模态图卷积网络（MMGCN）的短视频推荐系统，结合用户偏好提升直播互动。系统整合用户交互数据、视频内容特征和上下文信息，采用协同过滤与内容过滤混合方法，显著优于基准模型。", "motivation": "探索多模态方法以提升直播互动，通过个性化推荐系统满足用户兴趣。", "method": "开发基于MMGCN的推荐系统，结合协同过滤与内容过滤，利用用户交互、视频内容和上下文信息。", "result": "在Kwai、TikTok和MovieLens数据集上，MMGCN模型表现优于DeepFM等基准模型，F1分数分别为0.574、0.506和0.197。", "conclusion": "多模态整合和用户中心方法对提升推荐系统性能至关重要，有助于直播平台内容发现和用户互动。"}}
{"id": "2506.23502", "pdf": "https://arxiv.org/pdf/2506.23502", "abs": "https://arxiv.org/abs/2506.23502", "authors": ["Mengxiao Tian", "Xinxiao Wu", "Shuo Yang"], "title": "LLM-enhanced Action-aware Multi-modal Prompt Tuning for Image-Text Matching", "categories": ["cs.CV"], "comment": "accepted by ICCV 2025", "summary": "Driven by large-scale contrastive vision-language pre-trained models such as\nCLIP, recent advancements in the image-text matching task have achieved\nremarkable success in representation learning. Due to image-level\nvisual-language alignment, CLIP falls short in understanding fine-grained\ndetails such as object attributes and spatial relationships between objects.\nRecent efforts have attempted to compel CLIP to acquire structured visual\nrepresentations by introducing prompt learning to achieve object-level\nalignment. While achieving promising results, they still lack the capability to\nperceive actions, which are crucial for describing the states or relationships\nbetween objects. Therefore, we propose to endow CLIP with fine-grained\naction-level understanding by introducing an LLM-enhanced action-aware\nmulti-modal prompt-tuning method, incorporating the action-related external\nknowledge generated by large language models (LLMs). Specifically, we design an\naction triplet prompt and an action state prompt to exploit compositional\nsemantic knowledge and state-related causal knowledge implicitly stored in\nLLMs. Subsequently, we propose an adaptive interaction module to aggregate\nattentive visual features conditioned on action-aware prompted knowledge for\nestablishing discriminative and action-aware visual representations, which\nfurther improves the performance. Comprehensive experimental results on two\nbenchmark datasets demonstrate the effectiveness of our method.", "AI": {"tldr": "论文提出了一种基于LLM增强的动作感知多模态提示调优方法，以提升CLIP在细粒度动作理解上的能力。", "motivation": "CLIP在细粒度细节（如对象属性和空间关系）理解上存在不足，尤其是对动作的感知能力较弱。", "method": "设计了动作三元组提示和动作状态提示，结合LLM生成的外部知识，并通过自适应交互模块聚合视觉特征。", "result": "在两个基准数据集上的实验证明了方法的有效性。", "conclusion": "该方法显著提升了CLIP在动作感知任务上的表现。"}}
{"id": "2506.23094", "pdf": "https://arxiv.org/pdf/2506.23094", "abs": "https://arxiv.org/abs/2506.23094", "authors": ["Qi He", "Gus Xia", "Ziyu Wang"], "title": "TOMI: Transforming and Organizing Music Ideas for Multi-Track Compositions with Full-Song Structure", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "9 pages, 4 figures, 2 tables. To be published in ISMIR 2025", "summary": "Hierarchical planning is a powerful approach to model long sequences\nstructurally. Aside from considering hierarchies in the temporal structure of\nmusic, this paper explores an even more important aspect: concept hierarchy,\nwhich involves generating music ideas, transforming them, and ultimately\norganizing them--across musical time and space--into a complete composition. To\nthis end, we introduce TOMI (Transforming and Organizing Music Ideas) as a\nnovel approach in deep music generation and develop a TOMI-based model via\ninstruction-tuned foundation LLM. Formally, we represent a multi-track\ncomposition process via a sparse, four-dimensional space characterized by clips\n(short audio or MIDI segments), sections (temporal positions), tracks\n(instrument layers), and transformations (elaboration methods). Our model is\ncapable of generating multi-track electronic music with full-song structure,\nand we further integrate the TOMI-based model with the REAPER digital audio\nworkstation, enabling interactive human-AI co-creation. Experimental results\ndemonstrate that our approach produces higher-quality electronic music with\nstronger structural coherence compared to baselines.", "AI": {"tldr": "论文提出了一种基于概念层次结构的音乐生成方法TOMI，通过指令调优的基础LLM实现多轨电子音乐的生成与组织。", "motivation": "探索音乐生成中的概念层次结构，以生成更具结构连贯性的完整音乐作品。", "method": "引入TOMI方法，利用四维空间（片段、段落、音轨、变换）表示多轨音乐生成过程，并与REAPER工作站集成实现人机协作。", "result": "实验表明，TOMI生成的电子音乐质量更高，结构更连贯。", "conclusion": "TOMI为音乐生成提供了一种新的层次化方法，显著提升了生成音乐的质量和结构。"}}
{"id": "2506.23505", "pdf": "https://arxiv.org/pdf/2506.23505", "abs": "https://arxiv.org/abs/2506.23505", "authors": ["Tinh Nguyen"], "title": "Improve Underwater Object Detection through YOLOv12 Architecture and Physics-informed Augmentation", "categories": ["cs.CV"], "comment": null, "summary": "Underwater object detection is crucial for autonomous navigation,\nenvironmental monitoring, and marine exploration, but it is severely hampered\nby light attenuation, turbidity, and occlusion. Current methods balance\naccuracy and computational efficiency, but they have trouble deploying in\nreal-time under low visibility conditions. Through the integration of\nphysics-informed augmentation techniques with the YOLOv12 architecture, this\nstudy advances underwater detection. With Residual ELAN blocks to preserve\nstructural features in turbid waters and Area Attention to maintain large\nreceptive fields for occluded objects while reducing computational complexity.\nUnderwater optical properties are addressed by domain-specific augmentations\nsuch as turbulence adaptive blurring, biologically grounded occlusion\nsimulation, and spectral HSV transformations for color distortion. Extensive\ntests on four difficult datasets show state-of-the-art performance, with\nBrackish data registering 98.30% mAP at 142 FPS. YOLOv12 improves occlusion\nrobustness by 18.9%, small-object recall by 22.4%, and detection precision by\nup to 7.94% compared to previous models. The crucial role of augmentation\nstrategy is validated by ablation studies. This work offers a precise and\neffective solution for conservation and underwater robotics applications.", "AI": {"tldr": "该论文提出了一种基于YOLOv12架构的水下目标检测方法，结合物理增强技术和新型注意力机制，显著提升了在低能见度条件下的检测性能。", "motivation": "水下目标检测在自主导航和环境监测中至关重要，但受限于光线衰减、浑浊和遮挡等问题，现有方法难以在实时性和准确性之间取得平衡。", "method": "通过集成物理增强技术（如湍流自适应模糊、生物遮挡模拟和光谱HSV变换）与YOLOv12架构，并引入Residual ELAN块和区域注意力机制，以提升检测性能。", "result": "在四个数据集上实现了98.30%的mAP和142 FPS的性能，遮挡鲁棒性提升18.9%，小目标召回率提升22.4%，检测精度最高提升7.94%。", "conclusion": "该研究为水下机器人和保护应用提供了一种高效且精确的解决方案，并通过消融实验验证了增强策略的关键作用。"}}
{"id": "2506.23513", "pdf": "https://arxiv.org/pdf/2506.23513", "abs": "https://arxiv.org/abs/2506.23513", "authors": ["Zixun Fang", "Kai Zhu", "Zhiheng Liu", "Yu Liu", "Wei Zhai", "Yang Cao", "Zheng-Jun Zha"], "title": "ViewPoint: Panoramic Video Generation with Pretrained Diffusion Models", "categories": ["cs.CV"], "comment": "https://becauseimbatman0.github.io/ViewPoint", "summary": "Panoramic video generation aims to synthesize 360-degree immersive videos,\nholding significant importance in the fields of VR, world models, and spatial\nintelligence. Existing works fail to synthesize high-quality panoramic videos\ndue to the inherent modality gap between panoramic data and perspective data,\nwhich constitutes the majority of the training data for modern diffusion\nmodels. In this paper, we propose a novel framework utilizing pretrained\nperspective video models for generating panoramic videos. Specifically, we\ndesign a novel panorama representation named ViewPoint map, which possesses\nglobal spatial continuity and fine-grained visual details simultaneously. With\nour proposed Pano-Perspective attention mechanism, the model benefits from\npretrained perspective priors and captures the panoramic spatial correlations\nof the ViewPoint map effectively. Extensive experiments demonstrate that our\nmethod can synthesize highly dynamic and spatially consistent panoramic videos,\nachieving state-of-the-art performance and surpassing previous methods.", "AI": {"tldr": "提出了一种利用预训练视角视频模型生成全景视频的新框架，通过设计的ViewPoint map和Pano-Perspective注意力机制，解决了全景数据与视角数据之间的模态差距问题。", "motivation": "现有方法因全景数据与视角数据之间的模态差距，无法生成高质量的全景视频，而视角数据是现代扩散模型的主要训练数据。", "method": "设计了一种名为ViewPoint map的全景表示方法，结合Pano-Perspective注意力机制，利用预训练视角模型的先验知识。", "result": "实验表明，该方法能生成动态性强且空间一致的全景视频，性能优于现有方法。", "conclusion": "提出的框架有效解决了全景视频生成中的模态差距问题，实现了最先进的性能。"}}
{"id": "2506.23518", "pdf": "https://arxiv.org/pdf/2506.23518", "abs": "https://arxiv.org/abs/2506.23518", "authors": ["Jiwoo Park", "Tae Eun Choi", "Youngjun Jun", "Seong Jae Hwang"], "title": "WAVE: Warp-Based View Guidance for Consistent Novel View Synthesis Using a Single Image", "categories": ["cs.CV"], "comment": null, "summary": "Generating high-quality novel views of a scene from a single image requires\nmaintaining structural coherence across different views, referred to as view\nconsistency. While diffusion models have driven advancements in novel view\nsynthesis, they still struggle to preserve spatial continuity across views.\nDiffusion models have been combined with 3D models to address the issue, but\nsuch approaches lack efficiency due to their complex multi-step pipelines. This\npaper proposes a novel view-consistent image generation method which utilizes\ndiffusion models without additional modules. Our key idea is to enhance\ndiffusion models with a training-free method that enables adaptive attention\nmanipulation and noise reinitialization by leveraging view-guided warping to\nensure view consistency. Through our comprehensive metric framework suitable\nfor novel-view datasets, we show that our method improves view consistency\nacross various diffusion models, demonstrating its broader applicability.", "AI": {"tldr": "提出了一种无需额外模块的扩散模型方法，通过自适应注意力操纵和噪声重初始化提升视图一致性。", "motivation": "单图像生成高质量新视角时，现有扩散模型难以保持空间连续性，且结合3D模型的方法效率低下。", "method": "利用视图引导的变形技术，实现自适应注意力操纵和噪声重初始化，无需训练额外模块。", "result": "通过适用于新视角数据集的综合指标框架，验证了方法在多种扩散模型中提升视图一致性的有效性。", "conclusion": "该方法在保持高效的同时，显著提升了扩散模型在新视角生成中的视图一致性。"}}
{"id": "2506.23121", "pdf": "https://arxiv.org/pdf/2506.23121", "abs": "https://arxiv.org/abs/2506.23121", "authors": ["Xinlei Yu", "Chanmiao Wang", "Hui Jin", "Ahmed Elazab", "Gangyong Jia", "Xiang Wan", "Changqing Zou", "Ruiquan Ge"], "title": "CRISP-SAM2: SAM2 with Cross-Modal Interaction and Semantic Prompting for Multi-Organ Segmentation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "19 pages, 9 figures, 10 tables", "summary": "Multi-organ medical segmentation is a crucial component of medical image\nprocessing, essential for doctors to make accurate diagnoses and develop\neffective treatment plans. Despite significant progress in this field, current\nmulti-organ segmentation models often suffer from inaccurate details,\ndependence on geometric prompts and loss of spatial information. Addressing\nthese challenges, we introduce a novel model named CRISP-SAM2 with CRoss-modal\nInteraction and Semantic Prompting based on SAM2. This model represents a\npromising approach to multi-organ medical segmentation guided by textual\ndescriptions of organs. Our method begins by converting visual and textual\ninputs into cross-modal contextualized semantics using a progressive\ncross-attention interaction mechanism. These semantics are then injected into\nthe image encoder to enhance the detailed understanding of visual information.\nTo eliminate reliance on geometric prompts, we use a semantic prompting\nstrategy, replacing the original prompt encoder to sharpen the perception of\nchallenging targets. In addition, a similarity-sorting self-updating strategy\nfor memory and a mask-refining process is applied to further adapt to medical\nimaging and enhance localized details. Comparative experiments conducted on\nseven public datasets indicate that CRISP-SAM2 outperforms existing models.\nExtensive analysis also demonstrates the effectiveness of our method, thereby\nconfirming its superior performance, especially in addressing the limitations\nmentioned earlier. Our code is available at:\nhttps://github.com/YU-deep/CRISP\\_SAM2.git.", "AI": {"tldr": "CRISP-SAM2是一种基于SAM2的多器官医学分割模型，通过跨模态交互和语义提示解决现有模型的细节不准确、依赖几何提示和空间信息丢失问题。", "motivation": "当前多器官分割模型存在细节不准确、依赖几何提示和空间信息丢失等问题，CRISP-SAM2旨在解决这些挑战。", "method": "模型通过跨模态注意力机制将视觉和文本输入转换为语义信息，并采用语义提示策略替代几何提示，结合记忆自更新和掩码细化策略优化细节。", "result": "在七个公开数据集上的实验表明，CRISP-SAM2性能优于现有模型，尤其在解决上述限制方面表现突出。", "conclusion": "CRISP-SAM2在多器官医学分割中表现出色，有效解决了现有模型的局限性。"}}
{"id": "2506.23519", "pdf": "https://arxiv.org/pdf/2506.23519", "abs": "https://arxiv.org/abs/2506.23519", "authors": ["Qi Qin", "Runmin Cong", "Gen Zhan", "Yiting Liao", "Sam Kwong"], "title": "From Sight to Insight: Unleashing Eye-Tracking in Weakly Supervised Video Salient Object Detection", "categories": ["cs.CV"], "comment": "15 Pages, 9 Figures", "summary": "The eye-tracking video saliency prediction (VSP) task and video salient\nobject detection (VSOD) task both focus on the most attractive objects in video\nand show the result in the form of predictive heatmaps and pixel-level saliency\nmasks, respectively. In practical applications, eye tracker annotations are\nmore readily obtainable and align closely with the authentic visual patterns of\nhuman eyes. Therefore, this paper aims to introduce fixation information to\nassist the detection of video salient objects under weak supervision. On the\none hand, we ponder how to better explore and utilize the information provided\nby fixation, and then propose a Position and Semantic Embedding (PSE) module to\nprovide location and semantic guidance during the feature learning process. On\nthe other hand, we achieve spatiotemporal feature modeling under weak\nsupervision from the aspects of feature selection and feature contrast. A\nSemantics and Locality Query (SLQ) Competitor with semantic and locality\nconstraints is designed to effectively select the most matching and accurate\nobject query for spatiotemporal modeling. In addition, an Intra-Inter Mixed\nContrastive (IIMC) model improves the spatiotemporal modeling capabilities\nunder weak supervision by forming an intra-video and inter-video contrastive\nlearning paradigm. Experimental results on five popular VSOD benchmarks\nindicate that our model outperforms other competitors on various evaluation\nmetrics.", "AI": {"tldr": "论文提出了一种利用眼动注视信息辅助弱监督下视频显著物体检测的方法，通过PSE模块和SLQ Competitor提升特征学习，并通过IIMC模型增强时空建模能力。", "motivation": "眼动注视信息更易获取且更符合人眼真实视觉模式，因此论文旨在利用这些信息改进弱监督下的视频显著物体检测。", "method": "设计了PSE模块提供位置和语义指导，SLQ Competitor选择最佳对象查询进行时空建模，IIMC模型通过对比学习增强建模能力。", "result": "在五个流行的VSOD基准测试中，模型在多种评估指标上优于其他竞争对手。", "conclusion": "论文成功利用注视信息提升了弱监督下的视频显著物体检测性能。"}}
{"id": "2506.23523", "pdf": "https://arxiv.org/pdf/2506.23523", "abs": "https://arxiv.org/abs/2506.23523", "authors": ["Tuong Do", "Binh X. Nguyen", "Quang D. Tran", "Erman Tjiputra", "Te-Chuan Chiu", "Anh Nguyen"], "title": "Lightweight Temporal Transformer Decomposition for Federated Autonomous Driving", "categories": ["cs.CV"], "comment": "Accepted in IROS 2025", "summary": "Traditional vision-based autonomous driving systems often face difficulties\nin navigating complex environments when relying solely on single-image inputs.\nTo overcome this limitation, incorporating temporal data such as past image\nframes or steering sequences, has proven effective in enhancing robustness and\nadaptability in challenging scenarios. While previous high-performance methods\nexist, they often rely on resource-intensive fusion networks, making them\nimpractical for training and unsuitable for federated learning. To address\nthese challenges, we propose lightweight temporal transformer decomposition, a\nmethod that processes sequential image frames and temporal steering data by\nbreaking down large attention maps into smaller matrices. This approach reduces\nmodel complexity, enabling efficient weight updates for convergence and\nreal-time predictions while leveraging temporal information to enhance\nautonomous driving performance. Intensive experiments on three datasets\ndemonstrate that our method outperforms recent approaches by a clear margin\nwhile achieving real-time performance. Additionally, real robot experiments\nfurther confirm the effectiveness of our method.", "AI": {"tldr": "提出轻量级时序变换分解方法，通过分解大注意力图为小矩阵，降低模型复杂度，提升自动驾驶性能。", "motivation": "传统基于视觉的自动驾驶系统在复杂环境中依赖单帧图像输入表现不佳，需要引入时序数据提升鲁棒性。", "method": "采用轻量级时序变换分解，处理连续图像帧和时序转向数据，分解大注意力图为小矩阵。", "result": "在三个数据集上表现优于现有方法，且实现实时性能，真实机器人实验验证了有效性。", "conclusion": "该方法显著提升了自动驾驶系统的性能和实用性。"}}
{"id": "2506.23529", "pdf": "https://arxiv.org/pdf/2506.23529", "abs": "https://arxiv.org/abs/2506.23529", "authors": ["Jisu Han", "Jihee Park", "Dongyoon Han", "Wonjun Hwang"], "title": "When Test-Time Adaptation Meets Self-Supervised Models", "categories": ["cs.CV", "cs.LG"], "comment": "15 pages, 7 figures", "summary": "Training on test-time data enables deep learning models to adapt to dynamic\nenvironmental changes, enhancing their practical applicability. Online\nadaptation from source to target domains is promising but it remains highly\nreliant on the performance of source pretrained model. In this paper, we\ninvestigate whether test-time adaptation (TTA) methods can continuously improve\nmodels trained via self-supervised learning (SSL) without relying on source\npretraining. We introduce a self-supervised TTA protocol after observing that\nexisting TTA approaches struggle when directly applied to self-supervised\nmodels with low accuracy on the source domain. Furthermore, we propose a\ncollaborative learning framework that integrates SSL and TTA models, leveraging\ncontrastive learning and knowledge distillation for stepwise representation\nrefinement. We validate our method on diverse self-supervised models, including\nDINO, MoCo, and iBOT, across TTA benchmarks. Extensive experiments validate the\neffectiveness of our approach in SSL, showing that it achieves competitive\nperformance even without source pretraining.", "AI": {"tldr": "论文提出了一种自监督测试时适应（TTA）协议，通过协作学习框架结合自监督学习和TTA，提升模型性能，无需依赖源域预训练。", "motivation": "现有TTA方法在直接应用于低准确率的自监督模型时表现不佳，因此需要一种不依赖源预训练的自适应方法。", "method": "提出协作学习框架，结合对比学习和知识蒸馏，逐步优化表示。", "result": "在多种自监督模型（如DINO、MoCo、iBOT）上验证，性能表现优异，无需源预训练。", "conclusion": "该方法为自监督学习中的测试时适应提供了有效解决方案，具有实际应用潜力。"}}
{"id": "2506.23532", "pdf": "https://arxiv.org/pdf/2506.23532", "abs": "https://arxiv.org/abs/2506.23532", "authors": ["Jefferson Hernandez", "Ruozhen He", "Guha Balakrishnan", "Alexander C. Berg", "Vicente Ordonez"], "title": "GViT: Representing Images as Gaussians for Visual Recognition", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We introduce GVIT, a classification framework that abandons conventional\npixel or patch grid input representations in favor of a compact set of\nlearnable 2D Gaussians. Each image is encoded as a few hundred Gaussians whose\npositions, scales, orientations, colors, and opacities are optimized jointly\nwith a ViT classifier trained on top of these representations. We reuse the\nclassifier gradients as constructive guidance, steering the Gaussians toward\nclass-salient regions while a differentiable renderer optimizes an image\nreconstruction loss. We demonstrate that by 2D Gaussian input representations\ncoupled with our GVIT guidance, using a relatively standard ViT architecture,\nclosely matches the performance of a traditional patch-based ViT, reaching a\n76.9% top-1 accuracy on Imagenet-1k using a ViT-B architecture.", "AI": {"tldr": "GVIT是一种分类框架，用可学习的2D高斯表示替代传统像素或块输入，结合ViT分类器，性能接近传统ViT。", "motivation": "传统ViT基于像素或块输入，GVIT旨在通过更紧凑的高斯表示提升效率和性能。", "method": "图像编码为数百个高斯参数，位置、尺度、方向等与ViT分类器联合优化，利用梯度引导高斯聚焦类别显著区域。", "result": "GVIT在Imagenet-1k上达到76.9% top-1准确率，性能接近传统ViT。", "conclusion": "GVIT通过高斯表示和梯度引导，实现了与传统ViT相当的性能，展示了替代输入表示的潜力。"}}
{"id": "2506.23538", "pdf": "https://arxiv.org/pdf/2506.23538", "abs": "https://arxiv.org/abs/2506.23538", "authors": ["Yuhao Huang", "Yueyue Xu", "Haoran Dou", "Jiaxiao Deng", "Xin Yang", "Hongyu Zheng", "Dong Ni"], "title": "Uncertainty-aware Diffusion and Reinforcement Learning for Joint Plane Localization and Anomaly Diagnosis in 3D Ultrasound", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted by MICCAI 2025;10 pages, 3 figures", "summary": "Congenital uterine anomalies (CUAs) can lead to infertility, miscarriage,\npreterm birth, and an increased risk of pregnancy complications. Compared to\ntraditional 2D ultrasound (US), 3D US can reconstruct the coronal plane,\nproviding a clear visualization of the uterine morphology for assessing CUAs\naccurately. In this paper, we propose an intelligent system for simultaneous\nautomated plane localization and CUA diagnosis. Our highlights are: 1) we\ndevelop a denoising diffusion model with local (plane) and global (volume/text)\nguidance, using an adaptive weighting strategy to optimize attention allocation\nto different conditions; 2) we introduce a reinforcement learning-based\nframework with unsupervised rewards to extract the key slice summary from\nredundant sequences, fully integrating information across multiple planes to\nreduce learning difficulty; 3) we provide text-driven uncertainty modeling for\ncoarse prediction, and leverage it to adjust the classification probability for\noverall performance improvement. Extensive experiments on a large 3D uterine US\ndataset show the efficacy of our method, in terms of plane localization and CUA\ndiagnosis. Code is available at https://github.com/yuhoo0302/CUA-US.", "AI": {"tldr": "本文提出了一种智能系统，用于同时实现自动化平面定位和先天性子宫异常（CUA）诊断，结合去噪扩散模型和强化学习框架，显著提升了诊断效果。", "motivation": "先天性子宫异常（CUAs）可能导致不孕、流产、早产等并发症，传统2D超声难以准确评估，3D超声虽能重建冠状面但仍需智能系统辅助。", "method": "1) 开发了基于局部和全局引导的去噪扩散模型；2) 引入强化学习框架提取关键切片；3) 提供文本驱动的不确定性建模以优化分类。", "result": "在大规模3D子宫超声数据集上的实验验证了方法的有效性，显著提升了平面定位和CUA诊断的准确性。", "conclusion": "所提出的智能系统通过多模态信息融合和优化策略，为CUA诊断提供了高效且准确的解决方案。"}}
{"id": "2506.23164", "pdf": "https://arxiv.org/pdf/2506.23164", "abs": "https://arxiv.org/abs/2506.23164", "authors": ["Maarten Hugenholtz", "Anna Meszaros", "Jens Kober", "Zlatan Ajanovic"], "title": "Mode Collapse Happens: Evaluating Critical Interactions in Joint Trajectory Prediction Models", "categories": ["cs.RO", "cs.AI"], "comment": "12 pages, 8 figures, submitted to a journal", "summary": "Autonomous Vehicle decisions rely on multimodal prediction models that\naccount for multiple route options and the inherent uncertainty in human\nbehavior. However, models can suffer from mode collapse, where only the most\nlikely mode is predicted, posing significant safety risks. While existing\nmethods employ various strategies to generate diverse predictions, they often\noverlook the diversity in interaction modes among agents. Additionally,\ntraditional metrics for evaluating prediction models are dataset-dependent and\ndo not evaluate inter-agent interactions quantitatively. To our knowledge, none\nof the existing metrics explicitly evaluates mode collapse. In this paper, we\npropose a novel evaluation framework that assesses mode collapse in joint\ntrajectory predictions, focusing on safety-critical interactions. We introduce\nmetrics for mode collapse, mode correctness, and coverage, emphasizing the\nsequential dimension of predictions. By testing four multi-agent trajectory\nprediction models, we demonstrate that mode collapse indeed happens. When\nlooking at the sequential dimension, although prediction accuracy improves\ncloser to interaction events, there are still cases where the models are unable\nto predict the correct interaction mode, even just before the interaction mode\nbecomes inevitable. We hope that our framework can help researchers gain new\ninsights and advance the development of more consistent and accurate prediction\nmodels, thus enhancing the safety of autonomous driving systems.", "AI": {"tldr": "提出了一种评估多模态预测模型模式崩溃的新框架，重点关注安全关键交互，并引入新指标。", "motivation": "现有模型易出现模式崩溃，且缺乏定量评估交互多样性的指标。", "method": "提出评估框架，引入模式崩溃、模式正确性和覆盖率的指标，测试四种多智能体轨迹预测模型。", "result": "发现模式崩溃确实存在，且模型在交互事件临近时仍可能无法预测正确模式。", "conclusion": "该框架有助于提升预测模型的一致性和准确性，增强自动驾驶安全性。"}}
{"id": "2506.23542", "pdf": "https://arxiv.org/pdf/2506.23542", "abs": "https://arxiv.org/abs/2506.23542", "authors": ["Weida Wang", "Changyong He", "Jin Zeng", "Di Qiu"], "title": "Consistent Time-of-Flight Depth Denoising via Graph-Informed Geometric Attention", "categories": ["cs.CV"], "comment": "This paper has been accepted for publication at the International\n  Conference on Computer Vision (ICCV) 2025", "summary": "Depth images captured by Time-of-Flight (ToF) sensors are prone to noise,\nrequiring denoising for reliable downstream applications. Previous works either\nfocus on single-frame processing, or perform multi-frame processing without\nconsidering depth variations at corresponding pixels across frames, leading to\nundesirable temporal inconsistency and spatial ambiguity. In this paper, we\npropose a novel ToF depth denoising network leveraging motion-invariant graph\nfusion to simultaneously enhance temporal stability and spatial sharpness.\nSpecifically, despite depth shifts across frames, graph structures exhibit\ntemporal self-similarity, enabling cross-frame geometric attention for graph\nfusion. Then, by incorporating an image smoothness prior on the fused graph and\ndata fidelity term derived from ToF noise distribution, we formulate a maximum\na posterior problem for ToF denoising. Finally, the solution is unrolled into\niterative filters whose weights are adaptively learned from the graph-informed\ngeometric attention, producing a high-performance yet interpretable network.\nExperimental results demonstrate that the proposed scheme achieves\nstate-of-the-art performance in terms of accuracy and consistency on synthetic\nDVToF dataset and exhibits robust generalization on the real Kinectv2 dataset.\nSource code will be released at\n\\href{https://github.com/davidweidawang/GIGA-ToF}{https://github.com/davidweidawang/GIGA-ToF}.", "AI": {"tldr": "提出了一种基于运动不变图融合的ToF深度去噪网络，通过跨帧几何注意力提升时空一致性。", "motivation": "ToF传感器捕获的深度图像噪声多，现有方法未能同时处理时空一致性和空间清晰度。", "method": "利用图的时序自相似性进行图融合，结合图像平滑先验和ToF噪声分布，构建最大后验问题并展开为迭代滤波器。", "result": "在合成DVToF数据集和真实Kinectv2数据集上均取得最优性能，且泛化能力强。", "conclusion": "该方法在ToF深度去噪中实现了高精度和一致性，网络结构可解释且高效。"}}
{"id": "2506.23173", "pdf": "https://arxiv.org/pdf/2506.23173", "abs": "https://arxiv.org/abs/2506.23173", "authors": ["Tomer Slor", "Dean Oren", "Shira Baneth", "Tom Coen", "Haim Suchowski"], "title": "Deep Learning for Optical Misalignment Diagnostics in Multi-Lens Imaging Systems", "categories": ["physics.optics", "cs.AI", "cs.LG"], "comment": null, "summary": "In the rapidly evolving field of optical engineering, precise alignment of\nmulti-lens imaging systems is critical yet challenging, as even minor\nmisalignments can significantly degrade performance. Traditional alignment\nmethods rely on specialized equipment and are time-consuming processes,\nhighlighting the need for automated and scalable solutions. We present two\ncomplementary deep learning-based inverse-design methods for diagnosing\nmisalignments in multi-element lens systems using only optical measurements.\nFirst, we use ray-traced spot diagrams to predict five-degree-of-freedom\n(5-DOF) errors in a 6-lens photographic prime, achieving a mean absolute error\nof 0.031mm in lateral translation and 0.011$^\\circ$ in tilt. We also introduce\na physics-based simulation pipeline that utilizes grayscale synthetic camera\nimages, enabling a deep learning model to estimate 4-DOF, decenter and tilt\nerrors in both two- and six-lens multi-lens systems. These results show the\npotential to reshape manufacturing and quality control in precision imaging.", "AI": {"tldr": "论文提出两种基于深度学习的逆向设计方法，用于通过光学测量诊断多透镜系统的对准误差，展示了在精密成像制造和质量控制中的潜力。", "motivation": "多透镜成像系统的精确对准是关键但具有挑战性，传统方法依赖专业设备且耗时，亟需自动化和可扩展的解决方案。", "method": "1. 使用光线追踪光斑图预测6透镜系统的5自由度误差；2. 引入基于物理的仿真流程，利用灰度合成相机图像估计2和6透镜系统的4自由度误差。", "result": "在6透镜系统中，横向平移平均绝对误差为0.031mm，倾斜误差为0.011°。", "conclusion": "该方法有望重塑精密成像制造和质量控制领域。"}}
{"id": "2506.23543", "pdf": "https://arxiv.org/pdf/2506.23543", "abs": "https://arxiv.org/abs/2506.23543", "authors": ["Hui Li", "Baoyou Chen", "Liwei Zhang", "Jiaye Li", "Jingdong Wang", "Siyu Zhu"], "title": "Pyramidal Patchification Flow for Visual Generation", "categories": ["cs.CV"], "comment": "10 pages, 9figures", "summary": "Diffusion transformers (DiTs) adopt Patchify, mapping patch representations\nto token representations through linear projections, to adjust the number of\ntokens input to DiT blocks and thus the computation cost. Instead of a single\npatch size for all the timesteps, we introduce a Pyramidal Patchification Flow\n(PPFlow) approach: Large patch sizes are used for high noise timesteps and\nsmall patch sizes for low noise timesteps; Linear projections are learned for\neach patch size; and Unpatchify is accordingly modified. Unlike Pyramidal Flow,\nour approach operates over full latent representations other than pyramid\nrepresentations, and adopts the normal denoising process without requiring the\nrenoising trick. We demonstrate the effectiveness of our approach through two\ntraining manners. Training from scratch achieves a $1.6\\times$ ($2.0\\times$)\ninference speed over SiT-B/2 for 2-level (3-level) pyramid patchification with\nslightly lower training FLOPs and similar image generation performance.\nTraining from pretrained normal DiTs achieves even better performance with\nsmall training time. The code and checkpoint are at\nhttps://github.com/fudan-generative-vision/PPFlow.", "AI": {"tldr": "PPFlow通过动态调整patch大小和线性投影，优化了Diffusion Transformers的计算成本，提升了推理速度，同时保持了生成性能。", "motivation": "传统DiTs使用固定patch大小，计算成本高。PPFlow旨在通过动态patch大小调整优化计算效率和性能。", "method": "提出PPFlow方法：根据噪声水平动态调整patch大小，学习不同patch大小的线性投影，并修改Unpatchify。", "result": "训练结果显示，PPFlow在推理速度上提升1.6-2.0倍，同时保持相似的生成性能。", "conclusion": "PPFlow通过动态patch调整显著提升了计算效率，且从预训练模型微调可进一步优化性能。"}}
{"id": "2506.23174", "pdf": "https://arxiv.org/pdf/2506.23174", "abs": "https://arxiv.org/abs/2506.23174", "authors": ["Chen Gong", "Bo Liang", "Wei Gao", "Chenren Xu"], "title": "Data Can Speak for Itself: Quality-guided Utilization of Wireless Synthetic Data", "categories": ["cs.LG", "cs.AI"], "comment": "Published in MobiSys 2025", "summary": "Generative models have gained significant attention for their ability to\nproduce realistic synthetic data that supplements the quantity of real-world\ndatasets. While recent studies show performance improvements in wireless\nsensing tasks by incorporating all synthetic data into training sets, the\nquality of synthetic data remains unpredictable and the resulting performance\ngains are not guaranteed. To address this gap, we propose tractable and\ngeneralizable metrics to quantify quality attributes of synthetic data -\naffinity and diversity. Our assessment reveals prevalent affinity limitation in\ncurrent wireless synthetic data, leading to mislabeled data and degraded task\nperformance. We attribute the quality limitation to generative models' lack of\nawareness of untrained conditions and domain-specific processing. To mitigate\nthese issues, we introduce SynCheck, a quality-guided synthetic data\nutilization scheme that refines synthetic data quality during task model\ntraining. Our evaluation demonstrates that SynCheck consistently outperforms\nquality-oblivious utilization of synthetic data, and achieves 4.3% performance\nimprovement even when the previous utilization degrades performance by 13.4%.", "AI": {"tldr": "提出SynCheck方案，通过量化合成数据的亲和性与多样性，优化无线传感任务中的合成数据质量，显著提升性能。", "motivation": "当前合成数据质量不可预测，影响无线传感任务性能，需解决这一问题。", "method": "提出亲和性与多样性指标，设计SynCheck方案优化合成数据质量。", "result": "SynCheck优于传统方法，性能提升4.3%，而传统方法可能降低13.4%。", "conclusion": "SynCheck能有效提升合成数据质量，改善任务性能。"}}
{"id": "2506.23547", "pdf": "https://arxiv.org/pdf/2506.23547", "abs": "https://arxiv.org/abs/2506.23547", "authors": ["Jiwon Kim", "Soohyun Hwang", "Dong-O Kim", "Changsu Han", "Min Kyu Park", "Chang-Su Kim"], "title": "Oneta: Multi-Style Image Enhancement Using Eigentransformation Functions", "categories": ["cs.CV"], "comment": null, "summary": "The first algorithm, called Oneta, for a novel task of multi-style image\nenhancement is proposed in this work. Oneta uses two point operators\nsequentially: intensity enhancement with a transformation function (TF) and\ncolor correction with a color correction matrix (CCM). This two-step\nenhancement model, though simple, achieves a high performance upper bound.\nAlso, we introduce eigentransformation function (eigenTF) to represent TF\ncompactly. The Oneta network comprises Y-Net and C-Net to predict eigenTF and\nCCM parameters, respectively. To support $K$ styles, Oneta employs $K$\nlearnable tokens. During training, each style token is learned using image\npairs from the corresponding dataset. In testing, Oneta selects one of the $K$\nstyle tokens to enhance an image accordingly. Extensive experiments show that\nthe single Oneta network can effectively undertake six enhancement tasks --\nretouching, image signal processing, low-light image enhancement, dehazing,\nunderwater image enhancement, and white balancing -- across 30 datasets.", "AI": {"tldr": "Oneta是一种用于多风格图像增强的新算法，通过两步操作（强度增强和颜色校正）实现高性能，支持多种风格任务。", "motivation": "提出一种简单但高效的多风格图像增强方法，解决多种图像处理任务的需求。", "method": "使用两步操作：强度增强（通过eigenTF）和颜色校正（通过CCM），并利用Y-Net和C-Net预测参数，支持K种风格。", "result": "实验表明，Oneta能有效处理六种图像增强任务，覆盖30个数据集。", "conclusion": "Oneta是一种高效且通用的多风格图像增强算法，适用于多种场景。"}}
{"id": "2506.23184", "pdf": "https://arxiv.org/pdf/2506.23184", "abs": "https://arxiv.org/abs/2506.23184", "authors": ["Anran Liu", "Xiaofei Wang", "Jing Cai", "Chao Li"], "title": "Score-based Diffusion Model for Unpaired Virtual Histology Staining", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "11 pages, 3 figures", "summary": "Hematoxylin and eosin (H&E) staining visualizes histology but lacks\nspecificity for diagnostic markers. Immunohistochemistry (IHC) staining\nprovides protein-targeted staining but is restricted by tissue availability and\nantibody specificity. Virtual staining, i.e., computationally translating the\nH&E image to its IHC counterpart while preserving the tissue structure, is\npromising for efficient IHC generation. Existing virtual staining methods still\nface key challenges: 1) effective decomposition of staining style and tissue\nstructure, 2) controllable staining process adaptable to diverse tissue and\nproteins, and 3) rigorous structural consistency modelling to handle the\nnon-pixel-aligned nature of paired H&E and IHC images. This study proposes a\nmutual-information (MI)-guided score-based diffusion model for unpaired virtual\nstaining. Specifically, we design 1) a global MI-guided energy function that\ndisentangles the tissue structure and staining characteristics across\nmodalities, 2) a novel timestep-customized reverse diffusion process for\nprecise control of the staining intensity and structural reconstruction, and 3)\na local MI-driven contrastive learning strategy to ensure the cellular level\nstructural consistency between H&E-IHC images. Extensive experiments\ndemonstrate the our superiority over state-of-the-art approaches, highlighting\nits biomedical potential. Codes will be open-sourced upon acceptance.", "AI": {"tldr": "提出了一种基于互信息的扩散模型，用于从H&E图像虚拟生成IHC图像，解决了现有方法在分解染色风格与组织结构、可控染色过程及结构一致性建模方面的挑战。", "motivation": "H&E染色缺乏特异性标记，IHC染色受限于组织和抗体特异性，虚拟染色可高效生成IHC图像，但现有方法在分解、控制和一致性方面存在不足。", "method": "设计了全局互信息引导的能量函数、时间步定制的反向扩散过程和局部互信息驱动的对比学习策略，以分解染色风格与组织结构、精确控制染色强度并确保细胞级结构一致性。", "result": "实验表明，该方法优于现有技术，展现了生物医学应用的潜力。", "conclusion": "提出的方法在虚拟染色任务中表现优异，具有实际应用价值，代码将开源。"}}
{"id": "2506.23552", "pdf": "https://arxiv.org/pdf/2506.23552", "abs": "https://arxiv.org/abs/2506.23552", "authors": ["Mingi Kwon", "Joonghyuk Shin", "Jaeseok Jung", "Jaesik Park", "Youngjung Uh"], "title": "JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching", "categories": ["cs.CV", "cs.SD", "eess.AS"], "comment": "project page: https://joonghyuk.com/jamflow-web Under review.\n  Preprint published on arXiv", "summary": "The intrinsic link between facial motion and speech is often overlooked in\ngenerative modeling, where talking head synthesis and text-to-speech (TTS) are\ntypically addressed as separate tasks. This paper introduces JAM-Flow, a\nunified framework to simultaneously synthesize and condition on both facial\nmotion and speech. Our approach leverages flow matching and a novel Multi-Modal\nDiffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT\nand Audio-DiT modules. These are coupled via selective joint attention layers\nand incorporate key architectural choices, such as temporally aligned\npositional embeddings and localized joint attention masking, to enable\neffective cross-modal interaction while preserving modality-specific strengths.\nTrained with an inpainting-style objective, JAM-Flow supports a wide array of\nconditioning inputs-including text, reference audio, and reference\nmotion-facilitating tasks such as synchronized talking head generation from\ntext, audio-driven animation, and much more, within a single, coherent model.\nJAM-Flow significantly advances multi-modal generative modeling by providing a\npractical solution for holistic audio-visual synthesis. project page:\nhttps://joonghyuk.com/jamflow-web", "AI": {"tldr": "JAM-Flow是一个统一框架，通过流匹配和多模态扩散变换器（MM-DiT）同时合成面部运动和语音，支持多种输入条件，如文本、参考音频和运动。", "motivation": "现有生成模型通常将面部运动和语音合成视为独立任务，忽略了二者的内在联系。", "method": "采用流匹配和MM-DiT架构，结合Motion-DiT和Audio-DiT模块，通过选择性联合注意力层实现跨模态交互。", "result": "JAM-Flow支持多种任务（如文本驱动的同步说话头部生成、音频驱动动画等），在统一模型中实现高效的多模态合成。", "conclusion": "JAM-Flow为音频-视觉合成提供了实用解决方案，推动了多模态生成建模的发展。"}}
{"id": "2506.23203", "pdf": "https://arxiv.org/pdf/2506.23203", "abs": "https://arxiv.org/abs/2506.23203", "authors": ["Feng Shu", "Jiatong Bai", "Di Wu", "Wei Zhu", "Bin Deng", "Fuhui Zhou", "Jiangzhou Wang"], "title": "Multi-Branch DNN and CRLB-Ratio-Weight Fusion for Enhanced DOA Sensing via a Massive H$^2$AD MIMO Receiver", "categories": ["eess.SP", "cs.AI"], "comment": null, "summary": "As a green MIMO structure, massive H$^2$AD is viewed as a potential\ntechnology for the future 6G wireless network. For such a structure, it is a\nchallenging task to design a low-complexity and high-performance fusion of\ntarget direction values sensed by different sub-array groups with fewer use of\nprior knowledge. To address this issue, a lightweight Cramer-Rao lower bound\n(CRLB)-ratio-weight fusion (WF) method is proposed, which approximates inverse\nCRLB of each subarray using antenna number reciprocals to eliminate real-time\nCRLB computation. This reduces complexity and prior knowledge dependence while\npreserving fusion performance. Moreover, a multi-branch deep neural network\n(MBDNN) is constructed to further enhance direction-of-arrival (DOA) sensing by\nleveraging candidate angles from multiple subarrays. The subarray-specific\nbranch networks are integrated with a shared regression module to effectively\neliminate pseudo-solutions and fuse true angles. Simulation results show that\nthe proposed CRLB-ratio-WF method achieves DOA sensing performance comparable\nto CRLB-based methods, while significantly reducing the reliance on prior\nknowledge. More notably, the proposed MBDNN has superior performance in low-SNR\nranges. At SNR $= -15$ dB, it achieves an order-of-magnitude improvement in\nestimation accuracy compared to CRLB-ratio-WF method.", "AI": {"tldr": "论文提出了一种轻量级的CRLB-ratio-WF方法和多分支深度神经网络（MBDNN），用于6G无线网络中大规模H$^2$AD结构的低复杂度高性能目标方向值融合。", "motivation": "设计一种低复杂度、高性能的目标方向值融合方法，减少对先验知识的依赖，适用于未来6G无线网络的大规模H$^2$AD结构。", "method": "提出CRLB-ratio-WF方法，通过天线数倒数近似CRLB，避免实时计算；构建MBDNN，利用多子阵列候选角度增强DOA感知。", "result": "CRLB-ratio-WF方法性能接近基于CRLB的方法，且显著减少先验知识依赖；MBDNN在低信噪比下性能更优，-15dB时估计精度提升一个数量级。", "conclusion": "所提方法在降低复杂度和先验知识依赖的同时，保持了高性能，MBDNN在低信噪比下表现尤为突出。"}}
{"id": "2506.23555", "pdf": "https://arxiv.org/pdf/2506.23555", "abs": "https://arxiv.org/abs/2506.23555", "authors": ["Fan Xie", "Pan Cao"], "title": "LH2Face: Loss function for Hard High-quality Face", "categories": ["cs.CV"], "comment": null, "summary": "In current practical face authentication systems, most face recognition (FR)\nalgorithms are based on cosine similarity with softmax classification. Despite\nits reliable classification performance, this method struggles with hard\nsamples. A popular strategy to improve FR performance is incorporating angular\nor cosine margins. However, it does not take face quality or recognition\nhardness into account, simply increasing the margin value and thus causing an\noverly uniform training strategy. To address this problem, a novel loss\nfunction is proposed, named Loss function for Hard High-quality Face (LH2Face).\nFirstly, a similarity measure based on the von Mises-Fisher (vMF) distribution\nis stated, specifically focusing on the logarithm of the Probability Density\nFunction (PDF), which represents the distance between a probability\ndistribution and a vector. Then, an adaptive margin-based multi-classification\nmethod using softmax, called the Uncertainty-Aware Margin Function, is\nimplemented in the article. Furthermore, proxy-based loss functions are used to\napply extra constraints between the proxy and sample to optimize their\nrepresentation space distribution. Finally, a renderer is constructed that\noptimizes FR through face reconstruction and vice versa. Our LH2Face is\nsuperior to similiar schemes on hard high-quality face datasets, achieving\n49.39% accuracy on the IJB-B dataset, which surpasses the second-place method\nby 2.37%.", "AI": {"tldr": "提出了一种名为LH2Face的新损失函数，通过结合vMF分布和自适应边距方法，优化了高质量困难样本的人脸识别性能。", "motivation": "现有基于余弦相似度和softmax分类的人脸识别方法在处理困难样本时表现不佳，且未考虑人脸质量或识别难度，导致训练策略过于统一。", "method": "1. 使用vMF分布的概率密度函数对数作为相似性度量；2. 提出自适应边距的多分类方法；3. 引入代理损失函数优化表示空间分布；4. 构建渲染器通过人脸重建优化识别。", "result": "在IJB-B数据集上达到49.39%的准确率，比第二名方法高2.37%。", "conclusion": "LH2Face通过自适应边距和代理损失函数，显著提升了高质量困难样本的识别性能。"}}
{"id": "2506.23210", "pdf": "https://arxiv.org/pdf/2506.23210", "abs": "https://arxiv.org/abs/2506.23210", "authors": ["Taehwan Yoon", "Bongjun Choi"], "title": "FedRef: Communication-Efficient Bayesian Fine Tuning with Reference Model", "categories": ["cs.LG", "cs.AI", "cs.DC"], "comment": "6 pages,14 equation", "summary": "Federated learning(FL) is used for distributed scenarios to train artificial\nintelligence(AI) models while ensuring users' privacy. In federated learning\nscenario, the server generally never knows about users' data. This type of\nconcept makes the AI training process efficient in terms of data privacy.\nHowever, regarding model performance, federated AI models may not sufficiently\nsatisfy AI users' expectations. Furthermore, AI users have a wide range of\ndifferent needs. It is not easy to satisfy the whole users needs. These types\nof issues can be addressed through AI model optimization, fine-tuning, or\npersonalization to achieve optimal model performance. To address model\noptimization challenges, we propose reference model-based federated learning\nfor optimal fine-tuning, which overcomes catastrophic forgetting in each round.\nThis method is derived from Bayesian parameter-efficient transfer learning,\nwhich includes an optimal proximal term and enables overcoming the catastrophic\nforgetting issue in each round by utilizing a reference model that incorporates\nprevious model parameters. As a result, this method achieves both high model\nperformance and low computing cost.", "AI": {"tldr": "论文提出了一种基于参考模型的联邦学习方法，通过贝叶斯参数高效迁移学习优化模型性能，同时避免灾难性遗忘。", "motivation": "联邦学习在保护用户隐私的同时，模型性能可能无法满足用户多样化需求，需通过优化、微调或个性化提升性能。", "method": "采用基于参考模型的联邦学习方法，结合贝叶斯参数高效迁移学习，引入最优近端项，利用参考模型参数避免灾难性遗忘。", "result": "该方法实现了高模型性能和低计算成本。", "conclusion": "提出的方法有效解决了联邦学习中的模型优化挑战，平衡了性能与隐私需求。"}}
{"id": "2506.23565", "pdf": "https://arxiv.org/pdf/2506.23565", "abs": "https://arxiv.org/abs/2506.23565", "authors": ["Mingqian Ji", "Jian Yang", "Shanshan Zhang"], "title": "OcRFDet: Object-Centric Radiance Fields for Multi-View 3D Object Detection in Autonomous Driving", "categories": ["cs.CV"], "comment": "Accepted by ICCV2025", "summary": "Current multi-view 3D object detection methods typically transfer 2D features\ninto 3D space using depth estimation or 3D position encoder, but in a fully\ndata-driven and implicit manner, which limits the detection performance.\nInspired by the success of radiance fields on 3D reconstruction, we assume they\ncan be used to enhance the detector's ability of 3D geometry estimation.\nHowever, we observe a decline in detection performance, when we directly use\nthem for 3D rendering as an auxiliary task. From our analysis, we find the\nperformance drop is caused by the strong responses on the background when\nrendering the whole scene. To address this problem, we propose object-centric\nradiance fields, focusing on modeling foreground objects while discarding\nbackground noises. Specifically, we employ Object-centric Radiance Fields\n(OcRF) to enhance 3D voxel features via an auxiliary task of rendering\nforeground objects. We further use opacity - the side-product of rendering- to\nenhance the 2D foreground BEV features via Height-aware Opacity-based Attention\n(HOA), where attention maps at different height levels are generated separately\nvia multiple networks in parallel. Extensive experiments on the nuScenes\nvalidation and test datasets demonstrate that our OcRFDet achieves superior\nperformance, outperforming previous state-of-the-art methods with 57.2$\\%$ mAP\nand 64.8$\\%$ NDS on the nuScenes test benchmark. Code will be available at\nhttps://github.com/Mingqj/OcRFDet.", "AI": {"tldr": "论文提出了一种基于物体中心辐射场（OcRF）的多视角3D目标检测方法，通过专注于前景物体建模并忽略背景噪声，显著提升了检测性能。", "motivation": "现有方法通过深度估计或3D位置编码器将2D特征隐式转换为3D空间，限制了检测性能。受辐射场在3D重建中的成功启发，作者尝试将其用于增强3D几何估计能力。", "method": "提出物体中心辐射场（OcRF），专注于前景物体建模，并通过高度感知不透明度注意力（HOA）增强2D BEV特征。", "result": "在nuScenes测试集上，OcRFDet达到了57.2% mAP和64.8% NDS，优于现有方法。", "conclusion": "OcRFDet通过结合辐射场和注意力机制，显著提升了多视角3D目标检测的性能。"}}
{"id": "2506.23566", "pdf": "https://arxiv.org/pdf/2506.23566", "abs": "https://arxiv.org/abs/2506.23566", "authors": ["Luigi Sigillo", "Renato Giamba", "Danilo Comminiello"], "title": "Metadata, Wavelet, and Time Aware Diffusion Models for Satellite Image Super Resolution", "categories": ["cs.CV", "cs.LG"], "comment": "ICLR 2025 Workshop on Machine Learning for Remote Sensing (ML4RS)", "summary": "The acquisition of high-resolution satellite imagery is often constrained by\nthe spatial and temporal limitations of satellite sensors, as well as the high\ncosts associated with frequent observations. These challenges hinder\napplications such as environmental monitoring, disaster response, and\nagricultural management, which require fine-grained and high-resolution data.\nIn this paper, we propose MWT-Diff, an innovative framework for satellite image\nsuper-resolution (SR) that combines latent diffusion models with wavelet\ntransforms to address these challenges. At the core of the framework is a novel\nmetadata-, wavelet-, and time-aware encoder (MWT-Encoder), which generates\nembeddings that capture metadata attributes, multi-scale frequency information,\nand temporal relationships. The embedded feature representations steer the\nhierarchical diffusion dynamics, through which the model progressively\nreconstructs high-resolution satellite imagery from low-resolution inputs. This\nprocess preserves critical spatial characteristics including textural patterns,\nboundary discontinuities, and high-frequency spectral components essential for\ndetailed remote sensing analysis. The comparative analysis of MWT-Diff across\nmultiple datasets demonstrated favorable performance compared to recent\napproaches, as measured by standard perceptual quality metrics including FID\nand LPIPS.", "AI": {"tldr": "MWT-Diff是一种结合潜在扩散模型和小波变换的卫星图像超分辨率框架，通过MWT-Encoder生成嵌入特征，逐步重建高分辨率图像，优于现有方法。", "motivation": "卫星图像的高分辨率获取受限于传感器时空限制和高成本，影响环境监测等应用，需要精细数据。", "method": "提出MWT-Diff框架，结合潜在扩散模型和小波变换，利用MWT-Encoder生成嵌入特征，指导扩散过程重建高分辨率图像。", "result": "在多个数据集上表现优于现有方法，通过FID和LPIPS等指标验证。", "conclusion": "MWT-Diff能有效解决卫星图像超分辨率问题，保留关键空间特征，适用于遥感分析。"}}
{"id": "2506.23575", "pdf": "https://arxiv.org/pdf/2506.23575", "abs": "https://arxiv.org/abs/2506.23575", "authors": ["Nuo Chen", "Chao Xiao", "Yimian Dai", "Shiman He", "Miao Li", "Wei An"], "title": "Event-based Tiny Object Detection: A Benchmark Dataset and Baseline", "categories": ["cs.CV"], "comment": null, "summary": "Small object detection (SOD) in anti-UAV task is a challenging problem due to\nthe small size of UAVs and complex backgrounds. Traditional frame-based cameras\nstruggle to detect small objects in complex environments due to their low frame\nrates, limited dynamic range, and data redundancy. Event cameras, with\nmicrosecond temporal resolution and high dynamic range, provide a more\neffective solution for SOD. However, existing event-based object detection\ndatasets are limited in scale, feature large targets size, and lack diverse\nbackgrounds, making them unsuitable for SOD benchmarks. In this paper, we\nintroduce a Event-based Small object detection (EVSOD) dataset (namely EV-UAV),\nthe first large-scale, highly diverse benchmark for anti-UAV tasks. It includes\n147 sequences with over 2.3 million event-level annotations, featuring\nextremely small targets (averaging 6.8 $\\times$ 5.4 pixels) and diverse\nscenarios such as urban clutter and extreme lighting conditions. Furthermore,\nbased on the observation that small moving targets form continuous curves in\nspatiotemporal event point clouds, we propose Event based Sparse Segmentation\nNetwork (EV-SpSegNet), a novel baseline for event segmentation in point cloud\nspace, along with a Spatiotemporal Correlation (STC) loss that leverages motion\ncontinuity to guide the network in retaining target events. Extensive\nexperiments on the EV-UAV dataset demonstrate the superiority of our method and\nprovide a benchmark for future research in EVSOD. The dataset and code are at\nhttps://github.com/ChenYichen9527/Ev-UAV.", "AI": {"tldr": "论文提出了首个针对反无人机任务的大规模事件相机小目标检测数据集EV-UAV，并提出了基于时空相关性的EV-SpSegNet方法。", "motivation": "传统帧相机在复杂背景下检测小目标（如无人机）存在困难，而事件相机的高动态范围和微秒级分辨率更适合此类任务，但现有数据集规模小且目标大，缺乏多样性。", "method": "提出了EV-UAV数据集，包含147个序列和230万事件级标注；设计了EV-SpSegNet网络和时空相关性损失（STC loss），利用目标运动的连续性优化分割。", "result": "在EV-UAV数据集上的实验证明了方法的优越性，为未来研究提供了基准。", "conclusion": "EV-UAV数据集和EV-SpSegNet方法为事件相机小目标检测提供了有效解决方案和基准。"}}
{"id": "2506.23577", "pdf": "https://arxiv.org/pdf/2506.23577", "abs": "https://arxiv.org/abs/2506.23577", "authors": ["Yanning Hou", "Yanran Ruan", "Junfa Li", "Shanshan Wang", "Jianfeng Qiu", "Ke Xu"], "title": "StackCLIP: Clustering-Driven Stacked Prompt in Zero-Shot Industrial Anomaly Detection", "categories": ["cs.CV"], "comment": null, "summary": "Enhancing the alignment between text and image features in the CLIP model is\na critical challenge in zero-shot industrial anomaly detection tasks. Recent\nstudies predominantly utilize specific category prompts during pretraining,\nwhich can cause overfitting to the training categories and limit model\ngeneralization. To address this, we propose a method that transforms category\nnames through multicategory name stacking to create stacked prompts, forming\nthe basis of our StackCLIP model. Our approach introduces two key components.\nThe Clustering-Driven Stacked Prompts (CSP) module constructs generic prompts\nby stacking semantically analogous categories, while utilizing multi-object\ntextual feature fusion to amplify discriminative anomalies among similar\nobjects. The Ensemble Feature Alignment (EFA) module trains knowledge-specific\nlinear layers tailored for each stack cluster and adaptively integrates them\nbased on the attributes of test categories. These modules work together to\ndeliver superior training speed, stability, and convergence, significantly\nboosting anomaly segmentation performance. Additionally, our stacked prompt\nframework offers robust generalization across classification tasks. To further\nimprove performance, we introduce the Regulating Prompt Learning (RPL) module,\nwhich leverages the generalization power of stacked prompts to refine prompt\nlearning, elevating results in anomaly detection classification tasks.\nExtensive testing on seven industrial anomaly detection datasets demonstrates\nthat our method achieves state-of-the-art performance in both zero-shot anomaly\ndetection and segmentation tasks.", "AI": {"tldr": "论文提出StackCLIP模型，通过多类别名称堆叠生成堆叠提示，结合CSP和EFA模块提升零样本工业异常检测性能。", "motivation": "解决CLIP模型中文本与图像特征对齐的挑战，避免传统方法因特定类别提示导致的过拟合和泛化能力不足。", "method": "采用多类别名称堆叠生成堆叠提示，结合CSP模块构建通用提示，EFA模块训练知识特定线性层并自适应集成。", "result": "在七个工业异常检测数据集上实现零样本异常检测和分割任务的SOTA性能。", "conclusion": "StackCLIP模型通过堆叠提示和模块化设计显著提升性能，具有广泛适用性。"}}
{"id": "2506.23580", "pdf": "https://arxiv.org/pdf/2506.23580", "abs": "https://arxiv.org/abs/2506.23580", "authors": ["Yawen Zou", "Guang Li", "Duo Su", "Zi Wang", "Jun Yu", "Chao Zhang"], "title": "Dataset Distillation via Vision-Language Category Prototype", "categories": ["cs.CV"], "comment": "accepted by ICCV2025", "summary": "Dataset distillation (DD) condenses large datasets into compact yet\ninformative substitutes, preserving performance comparable to the original\ndataset while reducing storage, transmission costs, and computational\nconsumption. However, previous DD methods mainly focus on distilling\ninformation from images, often overlooking the semantic information inherent in\nthe data. The disregard for context hinders the model's generalization ability,\nparticularly in tasks involving complex datasets, which may result in illogical\noutputs or the omission of critical objects. In this study, we integrate\nvision-language methods into DD by introducing text prototypes to distill\nlanguage information and collaboratively synthesize data with image prototypes,\nthereby enhancing dataset distillation performance. Notably, the text\nprototypes utilized in this study are derived from descriptive text information\ngenerated by an open-source large language model. This framework demonstrates\nbroad applicability across datasets without pre-existing text descriptions,\nexpanding the potential of dataset distillation beyond traditional image-based\napproaches. Compared to other methods, the proposed approach generates\nlogically coherent images containing target objects, achieving state-of-the-art\nvalidation performance and demonstrating robust generalization. Source code and\ngenerated data are available in\nhttps://github.com/zou-yawen/Dataset-Distillation-via-Vision-Language-Category-Prototype/", "AI": {"tldr": "该研究提出了一种结合视觉语言方法的数据集蒸馏技术，通过引入文本原型和图像原型协同合成数据，提升了性能。", "motivation": "传统数据集蒸馏方法主要关注图像信息，忽略了语义信息，导致模型泛化能力不足。", "method": "利用开源大语言模型生成的描述性文本信息作为文本原型，与图像原型协同合成数据。", "result": "该方法生成的图像逻辑一致且包含目标对象，验证性能达到最优，泛化能力强。", "conclusion": "该框架扩展了数据集蒸馏的应用范围，超越了传统的基于图像的方法。"}}
{"id": "2506.23260", "pdf": "https://arxiv.org/pdf/2506.23260", "abs": "https://arxiv.org/abs/2506.23260", "authors": ["Mohamed Amine Ferrag", "Norbert Tihanyi", "Djallel Hamouda", "Leandros Maglaras", "Merouane Debbah"], "title": "From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI Agents Workflows", "categories": ["cs.CR", "cs.AI"], "comment": "29 pages, 15 figures, 6 tables", "summary": "Autonomous AI agents powered by large language models (LLMs) with structured\nfunction-calling interfaces have dramatically expanded capabilities for\nreal-time data retrieval, complex computation, and multi-step orchestration.\nYet, the explosive proliferation of plugins, connectors, and inter-agent\nprotocols has outpaced discovery mechanisms and security practices, resulting\nin brittle integrations vulnerable to diverse threats. In this survey, we\nintroduce the first unified, end-to-end threat model for LLM-agent ecosystems,\nspanning host-to-tool and agent-to-agent communications, formalize adversary\ncapabilities and attacker objectives, and catalog over thirty attack\ntechniques. Specifically, we organized the threat model into four domains:\nInput Manipulation (e.g., prompt injections, long-context hijacks, multimodal\nadversarial inputs), Model Compromise (e.g., prompt- and parameter-level\nbackdoors, composite and encrypted multi-backdoors, poisoning strategies),\nSystem and Privacy Attacks (e.g., speculative side-channels, membership\ninference, retrieval poisoning, social-engineering simulations), and Protocol\nVulnerabilities (e.g., exploits in Model Context Protocol (MCP), Agent\nCommunication Protocol (ACP), Agent Network Protocol (ANP), and Agent-to-Agent\n(A2A) protocol). For each category, we review representative scenarios, assess\nreal-world feasibility, and evaluate existing defenses. Building on our threat\ntaxonomy, we identify key open challenges and future research directions, such\nas securing MCP deployments through dynamic trust management and cryptographic\nprovenance tracking; designing and hardening Agentic Web Interfaces; and\nachieving resilience in multi-agent and federated environments. Our work\nprovides a comprehensive reference to guide the design of robust defense\nmechanisms and establish best practices for resilient LLM-agent workflows.", "AI": {"tldr": "该论文提出了一个统一的威胁模型，针对基于大语言模型（LLM）的自主AI代理生态系统，分析了四大领域的攻击技术，并评估了现有防御措施。", "motivation": "随着LLM代理生态系统的快速发展，插件、连接器和协议的安全问题日益突出，但发现机制和防御措施未能跟上，导致系统脆弱。", "method": "论文通过分类威胁模型，涵盖输入操纵、模型妥协、系统与隐私攻击及协议漏洞四大领域，分析攻击技术和防御措施。", "result": "提出了一个全面的威胁分类法，总结了三十多种攻击技术，并评估了其现实可行性和现有防御效果。", "conclusion": "论文为设计稳健防御机制和建立最佳实践提供了参考，并指出了未来研究方向，如动态信任管理和加密溯源跟踪。"}}
{"id": "2506.23581", "pdf": "https://arxiv.org/pdf/2506.23581", "abs": "https://arxiv.org/abs/2506.23581", "authors": ["Xiao Li", "Yiming Zhu", "Yifan Huang", "Wei Zhang", "Yingzhe He", "Jie Shi", "Xiaolin Hu"], "title": "PBCAT: Patch-based composite adversarial training against physically realizable attacks on object detection", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Accepted by ICCV 2025", "summary": "Object detection plays a crucial role in many security-sensitive\napplications. However, several recent studies have shown that object detectors\ncan be easily fooled by physically realizable attacks, \\eg, adversarial patches\nand recent adversarial textures, which pose realistic and urgent threats.\nAdversarial Training (AT) has been recognized as the most effective defense\nagainst adversarial attacks. While AT has been extensively studied in the\n$l_\\infty$ attack settings on classification models, AT against physically\nrealizable attacks on object detectors has received limited exploration. Early\nattempts are only performed to defend against adversarial patches, leaving AT\nagainst a wider range of physically realizable attacks under-explored. In this\nwork, we consider defending against various physically realizable attacks with\na unified AT method. We propose PBCAT, a novel Patch-Based Composite\nAdversarial Training strategy. PBCAT optimizes the model by incorporating the\ncombination of small-area gradient-guided adversarial patches and imperceptible\nglobal adversarial perturbations covering the entire image. With these designs,\nPBCAT has the potential to defend against not only adversarial patches but also\nunseen physically realizable attacks such as adversarial textures. Extensive\nexperiments in multiple settings demonstrated that PBCAT significantly improved\nrobustness against various physically realizable attacks over state-of-the-art\ndefense methods. Notably, it improved the detection accuracy by 29.7\\% over\nprevious defense methods under one recent adversarial texture attack.", "AI": {"tldr": "该论文提出了一种名为PBCAT的对抗训练方法，用于防御物体检测器中的多种物理可实现攻击。", "motivation": "物体检测在安全敏感应用中至关重要，但现有检测器易受物理可实现攻击（如对抗性补丁和纹理）的威胁。对抗训练（AT）虽有效，但针对物理可实现攻击的研究有限，尤其是广泛攻击类型。", "method": "提出PBCAT策略，结合小区域梯度引导对抗补丁和覆盖全图的不可察觉全局对抗扰动，优化模型。", "result": "实验表明，PBCAT显著提升了对抗多种物理可实现攻击的鲁棒性，检测准确率在对抗纹理攻击下比现有方法提高29.7%。", "conclusion": "PBCAT为防御物理可实现攻击提供了统一且高效的解决方案，具有广泛适用性。"}}
{"id": "2506.23590", "pdf": "https://arxiv.org/pdf/2506.23590", "abs": "https://arxiv.org/abs/2506.23590", "authors": ["Qiming Li", "Zekai Ye", "Xiaocheng Feng", "Weihong Zhong", "Libo Qin", "Ruihan Chen", "Baohang Li", "Kui Jiang", "Yaowei Wang", "Ting Liu", "Bing Qin"], "title": "CAI: Caption-Sensitive Attention Intervention for Mitigating Object Hallucination in Large Vision-Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Although Large Vision-Language Models (LVLMs) have demonstrated powerful\ncapabilities in interpreting visual information, they frequently produce\ncontent that deviates from visual information, leading to object hallucination.\nTo tackle this, recent works mostly depend on expensive manual annotations and\ntraining cost, or significantly increase inference time. In this work, we\nobserve that LVLMs' attention to visual information is significantly stronger\nwhen answering caption queries compared to non-caption queries. Inspired by\nthis phenomenon, we propose Caption-sensitive Attention Intervention (CAI), a\ntraining-free, plug-and-play hallucination mitigation method that leverages the\nattention activation pattern in response to caption queries to enhance LVLMs'\nvisual perception capability. Extensive experimental results across four\nbenchmarks covering both discriminative and generative tasks, demonstrate that\nCAI achieves state-of-the-art (SOTA) hallucination mitigating performance only\nwith minimal additional inference cost.", "AI": {"tldr": "论文提出了一种无需训练、即插即用的幻觉缓解方法CAI，通过利用LVLMs在回答标题查询时的注意力模式，显著提升了视觉感知能力，并在多个基准测试中达到SOTA性能。", "motivation": "大型视觉语言模型（LVLMs）在解释视觉信息时经常产生与视觉信息不符的内容（对象幻觉），现有方法依赖昂贵的手动标注和训练成本，或显著增加推理时间。", "method": "提出Caption-sensitive Attention Intervention (CAI)，利用LVLMs在回答标题查询时的注意力激活模式，增强其视觉感知能力，无需额外训练。", "result": "在四个基准测试（涵盖判别性和生成性任务）中，CAI仅需最小额外推理成本即可实现SOTA的幻觉缓解性能。", "conclusion": "CAI是一种高效且无需训练的幻觉缓解方法，显著提升了LVLMs的视觉感知能力，适用于多种任务。"}}
{"id": "2506.23274", "pdf": "https://arxiv.org/pdf/2506.23274", "abs": "https://arxiv.org/abs/2506.23274", "authors": ["Hans Peter Lynsgøe Raaschou-jensen", "Constanza Fierro", "Anders Søgaard"], "title": "Predicting thinking time in Reasoning models", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reasoning models that produce long, hidden chains of thought have emerged as\npowerful tools for complex, reasoning-intensive\ntasks\\citep{deepseekai2025deepseekr1incentivizingreasoningcapability,\nopenai2024openaio1card}. However, this paradigm introduces a new user\nexperience challenge: users have little insight into how much time the model\nwill spend reasoning before returning an answer. This unpredictability, can\nlead to user frustration and is likely to compound as LLMs can produce\nincreasingly long tasks asynchronously\n\\citep{kwa2025measuringaiabilitycomplete}. In this paper, we introduce and\nevaluate methods for both online and offline prediction of model \"thinking\ntime,\" aiming to develop a practical \"progress bar for reasoning.\" We discuss\nthe implications for user interaction and future research directions.", "AI": {"tldr": "论文探讨了推理模型在复杂任务中的“思考时间”不可预测性问题，并提出了在线和离线预测方法，旨在为用户提供“推理进度条”。", "motivation": "由于推理模型生成长且隐藏的思维链，用户无法预知模型返回答案的时间，导致用户体验不佳和挫败感。", "method": "提出并评估了在线和离线预测模型“思考时间”的方法，目标是开发实用的“推理进度条”。", "result": "未明确提及具体结果，但讨论了方法的潜在应用和对用户交互的影响。", "conclusion": "研究为改善推理模型的用户体验提供了方向，并指出了未来研究的可能路径。"}}
{"id": "2506.23605", "pdf": "https://arxiv.org/pdf/2506.23605", "abs": "https://arxiv.org/abs/2506.23605", "authors": ["Suyash Maniyar", "Vishvesh Trivedi", "Ajoy Mondal", "Anand Mishra", "C. V. Jawahar"], "title": "AI-Generated Lecture Slides for Improving Slide Element Detection and Retrieval", "categories": ["cs.CV", "cs.AI"], "comment": "40 pages including supplementary, accepted at ICDAR 2025", "summary": "Lecture slide element detection and retrieval are key problems in slide\nunderstanding. Training effective models for these tasks often depends on\nextensive manual annotation. However, annotating large volumes of lecture\nslides for supervised training is labor intensive and requires domain\nexpertise. To address this, we propose a large language model (LLM)-guided\nsynthetic lecture slide generation pipeline, SynLecSlideGen, which produces\nhigh-quality, coherent and realistic slides. We also create an evaluation\nbenchmark, namely RealSlide by manually annotating 1,050 real lecture slides.\nTo assess the utility of our synthetic slides, we perform few-shot transfer\nlearning on real data using models pre-trained on them. Experimental results\nshow that few-shot transfer learning with pretraining on synthetic slides\nsignificantly improves performance compared to training only on real data. This\ndemonstrates that synthetic data can effectively compensate for limited labeled\nlecture slides. The code and resources of our work are publicly available on\nour project website: https://synslidegen.github.io/.", "AI": {"tldr": "提出了一种基于大语言模型（LLM）的合成幻灯片生成方法SynLecSlideGen，并通过实验证明其能有效提升小样本迁移学习性能。", "motivation": "解决讲座幻灯片元素检测和检索任务中标注数据稀缺且标注成本高的问题。", "method": "利用LLM生成高质量合成幻灯片，并创建真实幻灯片标注数据集RealSlide进行评估。", "result": "实验表明，基于合成幻灯片的预训练显著优于仅使用真实数据的小样本学习。", "conclusion": "合成数据可有效弥补真实标注数据的不足，提升模型性能。"}}
{"id": "2506.23606", "pdf": "https://arxiv.org/pdf/2506.23606", "abs": "https://arxiv.org/abs/2506.23606", "authors": ["Zhengkang Xiang", "Zizhao Li", "Amir Khodabandeh", "Kourosh Khoshelham"], "title": "SG-LDM: Semantic-Guided LiDAR Generation via Latent-Aligned Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "Lidar point cloud synthesis based on generative models offers a promising\nsolution to augment deep learning pipelines, particularly when real-world data\nis scarce or lacks diversity. By enabling flexible object manipulation, this\nsynthesis approach can significantly enrich training datasets and enhance\ndiscriminative models. However, existing methods focus on unconditional lidar\npoint cloud generation, overlooking their potential for real-world\napplications. In this paper, we propose SG-LDM, a Semantic-Guided Lidar\nDiffusion Model that employs latent alignment to enable robust\nsemantic-to-lidar synthesis. By directly operating in the native lidar space\nand leveraging explicit semantic conditioning, SG-LDM achieves state-of-the-art\nperformance in generating high-fidelity lidar point clouds guided by semantic\nlabels. Moreover, we propose the first diffusion-based lidar translation\nframework based on SG-LDM, which enables cross-domain translation as a domain\nadaptation strategy to enhance downstream perception performance. Systematic\nexperiments demonstrate that SG-LDM significantly outperforms existing lidar\ndiffusion models and the proposed lidar translation framework further improves\ndata augmentation performance in the downstream lidar segmentation task.", "AI": {"tldr": "SG-LDM是一种基于语义引导的激光雷达扩散模型，通过潜在对齐实现语义到激光雷达的合成，显著提升了激光雷达点云生成的质量和下游任务性能。", "motivation": "解决现有激光雷达点云生成方法无条件生成、缺乏实际应用潜力的问题，提出语义引导的生成模型以增强数据多样性和下游任务表现。", "method": "提出SG-LDM模型，利用潜在对齐和显式语义条件，直接在激光雷达空间操作，并基于此开发了首个扩散式激光雷达翻译框架。", "result": "SG-LDM在生成高保真激光雷达点云方面表现最优，其翻译框架显著提升了下游激光雷达分割任务的数据增强效果。", "conclusion": "SG-LDM不仅提升了激光雷达点云生成的质量，还通过跨域翻译为下游任务提供了有效的数据增强策略。"}}
{"id": "2506.23286", "pdf": "https://arxiv.org/pdf/2506.23286", "abs": "https://arxiv.org/abs/2506.23286", "authors": ["Alan Jeffares", "Mihaela van der Schaar"], "title": "Not All Explanations for Deep Learning Phenomena Are Equally Valuable", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Accepted at ICML 2025 for oral presentation", "summary": "Developing a better understanding of surprising or counterintuitive phenomena\nhas constituted a significant portion of deep learning research in recent\nyears. These include double descent, grokking, and the lottery ticket\nhypothesis -- among many others. Works in this area often develop ad hoc\nhypotheses attempting to explain these observed phenomena on an isolated,\ncase-by-case basis. This position paper asserts that, in many prominent cases,\nthere is little evidence to suggest that these phenomena appear in real-world\napplications and these efforts may be inefficient in driving progress in the\nbroader field. Consequently, we argue against viewing them as isolated puzzles\nthat require bespoke resolutions or explanations. However, despite this, we\nsuggest that deep learning phenomena do still offer research value by providing\nunique settings in which we can refine our broad explanatory theories of more\ngeneral deep learning principles. This position is reinforced by analyzing the\nresearch outcomes of several prominent examples of these phenomena from the\nrecent literature. We revisit the current norms in the research community in\napproaching these problems and propose practical recommendations for future\nresearch, aiming to ensure that progress on deep learning phenomena is well\naligned with the ultimate pragmatic goal of progress in the broader field of\ndeep learning.", "AI": {"tldr": "论文认为当前深度学习研究中许多反直觉现象的研究缺乏实际应用证据，建议将其作为验证通用理论的工具而非孤立问题。", "motivation": "探讨深度学习中的反直觉现象（如双下降、grokking等）是否具有实际研究价值，避免低效的孤立研究。", "method": "通过分析文献中的典型案例，评估这些现象的研究成果及其对领域的贡献。", "result": "发现许多现象缺乏实际应用证据，但可作为验证通用深度学习理论的独特场景。", "conclusion": "建议调整研究方向，将现象研究作为工具，以推动深度学习领域的整体进步。"}}
{"id": "2506.23607", "pdf": "https://arxiv.org/pdf/2506.23607", "abs": "https://arxiv.org/abs/2506.23607", "authors": ["Shiqi Zhang", "Sha Zhang", "Jiajun Deng", "Yedong Shen", "Mingxiao MA", "Yanyong Zhang"], "title": "PGOV3D: Open-Vocabulary 3D Semantic Segmentation with Partial-to-Global Curriculum", "categories": ["cs.CV"], "comment": null, "summary": "Existing open-vocabulary 3D semantic segmentation methods typically supervise\n3D segmentation models by merging text-aligned features (e.g., CLIP) extracted\nfrom multi-view images onto 3D points. However, such approaches treat\nmulti-view images merely as intermediaries for transferring open-vocabulary\ninformation, overlooking their rich semantic content and cross-view\ncorrespondences, which limits model effectiveness. To address this, we propose\nPGOV3D, a novel framework that introduces a Partial-to-Global curriculum for\nimproving open-vocabulary 3D semantic segmentation. The key innovation lies in\na two-stage training strategy. In the first stage, we pre-train the model on\npartial scenes that provide dense semantic information but relatively simple\ngeometry. These partial point clouds are derived from multi-view RGB-D inputs\nvia pixel-wise depth projection. To enable open-vocabulary learning, we\nleverage a multi-modal large language model (MLLM) and a 2D segmentation\nfoundation model to generate open-vocabulary labels for each viewpoint,\noffering rich and aligned supervision. An auxiliary inter-frame consistency\nmodule is introduced to enforce feature consistency across varying viewpoints\nand enhance spatial understanding. In the second stage, we fine-tune the model\non complete scene-level point clouds, which are sparser and structurally more\ncomplex. We aggregate the partial vocabularies associated with each scene and\ngenerate pseudo labels using the pre-trained model, effectively bridging the\nsemantic gap between dense partial observations and large-scale 3D\nenvironments. Extensive experiments on ScanNet, ScanNet200, and S3DIS\nbenchmarks demonstrate that PGOV3D achieves competitive performance in\nopen-vocabulary 3D semantic segmentation.", "AI": {"tldr": "PGOV3D提出了一种新颖的两阶段训练框架，通过部分到全局的课程学习改进开放词汇3D语义分割。", "motivation": "现有方法将多视角图像仅视为传递开放词汇信息的中介，忽略了其丰富的语义内容和跨视角对应关系，限制了模型效果。", "method": "PGOV3D采用两阶段训练策略：第一阶段在提供密集语义信息的局部场景上预训练，第二阶段在稀疏且结构复杂的完整场景上微调。", "result": "在ScanNet、ScanNet200和S3DIS基准测试中，PGOV3D表现出色。", "conclusion": "PGOV3D通过部分到全局的课程学习，有效提升了开放词汇3D语义分割的性能。"}}
{"id": "2506.23611", "pdf": "https://arxiv.org/pdf/2506.23611", "abs": "https://arxiv.org/abs/2506.23611", "authors": ["Ziao Liu", "Zhenjia Li", "Yifeng Shi", "Xiangang Li"], "title": "AttentionGS: Towards Initialization-Free 3D Gaussian Splatting via Structural Attention", "categories": ["cs.CV"], "comment": null, "summary": "3D Gaussian Splatting (3DGS) is a powerful alternative to Neural Radiance\nFields (NeRF), excelling in complex scene reconstruction and efficient\nrendering. However, it relies on high-quality point clouds from\nStructure-from-Motion (SfM), limiting its applicability. SfM also fails in\ntexture-deficient or constrained-view scenarios, causing severe degradation in\n3DGS reconstruction. To address this limitation, we propose AttentionGS, a\nnovel framework that eliminates the dependency on high-quality initial point\nclouds by leveraging structural attention for direct 3D reconstruction from\nrandomly initialization. In the early training stage, we introduce geometric\nattention to rapidly recover the global scene structure. As training\nprogresses, we incorporate texture attention to refine fine-grained details and\nenhance rendering quality. Furthermore, we employ opacity-weighted gradients to\nguide Gaussian densification, leading to improved surface reconstruction.\nExtensive experiments on multiple benchmark datasets demonstrate that\nAttentionGS significantly outperforms state-of-the-art methods, particularly in\nscenarios where point cloud initialization is unreliable. Our approach paves\nthe way for more robust and flexible 3D Gaussian Splatting in real-world\napplications.", "AI": {"tldr": "AttentionGS 是一种新框架，通过结构注意力直接从随机初始化进行3D重建，解决了3D高斯泼溅（3DGS）对高质量点云的依赖问题。", "motivation": "3DGS 依赖于结构从运动（SfM）的高质量点云，在纹理不足或视角受限的场景中表现不佳。", "method": "AttentionGS 结合几何注意力和纹理注意力，早期恢复全局结构，后期优化细节，并使用不透明度加权梯度指导高斯密度化。", "result": "在多个基准数据集上，AttentionGS 显著优于现有方法，特别是在点云初始化不可靠的场景中。", "conclusion": "AttentionGS 为3DGS在现实应用中提供了更鲁棒和灵活的解决方案。"}}
{"id": "2506.23296", "pdf": "https://arxiv.org/pdf/2506.23296", "abs": "https://arxiv.org/abs/2506.23296", "authors": ["Naoto Kiribuchi", "Kengo Zenitani", "Takayuki Semitsu"], "title": "Securing AI Systems: A Guide to Known Attacks and Impacts", "categories": ["cs.CR", "cs.AI"], "comment": "34 pages, 16 figures", "summary": "Embedded into information systems, artificial intelligence (AI) faces\nsecurity threats that exploit AI-specific vulnerabilities. This paper provides\nan accessible overview of adversarial attacks unique to predictive and\ngenerative AI systems. We identify eleven major attack types and explicitly\nlink attack techniques to their impacts -- including information leakage,\nsystem compromise, and resource exhaustion -- mapped to the confidentiality,\nintegrity, and availability (CIA) security triad. We aim to equip researchers,\ndevelopers, security practitioners, and policymakers, even those without\nspecialized AI security expertise, with foundational knowledge to recognize\nAI-specific risks and implement effective defenses, thereby enhancing the\noverall security posture of AI systems.", "AI": {"tldr": "本文概述了针对预测和生成AI系统的独特对抗攻击，识别了11种主要攻击类型，并将其影响映射到CIA安全三要素。", "motivation": "AI系统面临特定的安全威胁，需要为非专业AI安全人员提供基础知识以识别风险并实施防御。", "method": "识别并分类11种主要攻击类型，明确攻击技术与CIA安全三要素的关联。", "result": "提供了对抗攻击的全面概述，帮助理解其对机密性、完整性和可用性的影响。", "conclusion": "通过增强对AI特定风险的认识和防御能力，提升AI系统的整体安全性。"}}
{"id": "2506.23618", "pdf": "https://arxiv.org/pdf/2506.23618", "abs": "https://arxiv.org/abs/2506.23618", "authors": ["Zhongdao Wang", "Guodongfang Zhao", "Jingjing Ren", "Bailan Feng", "Shifeng Zhang", "Wenbo Li"], "title": "TurboVSR: Fantastic Video Upscalers and Where to Find Them", "categories": ["cs.CV"], "comment": "ICCV, 2025", "summary": "Diffusion-based generative models have demonstrated exceptional promise in\nthe video super-resolution (VSR) task, achieving a substantial advancement in\ndetail generation relative to prior methods. However, these approaches face\nsignificant computational efficiency challenges. For instance, current\ntechniques may require tens of minutes to super-resolve a mere 2-second, 1080p\nvideo. In this paper, we present TurboVSR, an ultra-efficient diffusion-based\nvideo super-resolution model. Our core design comprises three key aspects: (1)\nWe employ an autoencoder with a high compression ratio of 32$\\times$32$\\times$8\nto reduce the number of tokens. (2) Highly compressed latents pose substantial\nchallenges for training. We introduce factorized conditioning to mitigate the\nlearning complexity: we first learn to super-resolve the initial frame;\nsubsequently, we condition the super-resolution of the remaining frames on the\nhigh-resolution initial frame and the low-resolution subsequent frames. (3) We\nconvert the pre-trained diffusion model to a shortcut model to enable fewer\nsampling steps, further accelerating inference. As a result, TurboVSR performs\non par with state-of-the-art VSR methods, while being 100+ times faster, taking\nonly 7 seconds to process a 2-second long 1080p video. TurboVSR also supports\nimage resolution by considering image as a one-frame video. Our efficient\ndesign makes SR beyond 1080p possible, results on 4K (3648$\\times$2048) image\nSR show surprising fine details.", "AI": {"tldr": "TurboVSR是一种基于扩散模型的超高效视频超分辨率方法，通过高压缩比自动编码器、因子化条件和捷径模型设计，显著提升计算效率，性能与现有方法相当但速度快100倍以上。", "motivation": "现有基于扩散模型的视频超分辨率方法在细节生成上表现优异，但计算效率低下，处理短视频耗时过长。", "method": "1. 使用高压缩比（32×32×8）自动编码器减少token数量；2. 引入因子化条件，先学习超分辨率初始帧，再以高分辨率初始帧和低分辨率后续帧为条件；3. 将预训练扩散模型转换为捷径模型以减少采样步骤。", "result": "TurboVSR性能与现有最优方法相当，但速度快100倍以上（7秒处理2秒1080p视频），并支持4K图像超分辨率。", "conclusion": "TurboVSR通过高效设计解决了扩散模型的计算效率问题，为高分辨率视频和图像超分辨率提供了实用解决方案。"}}
{"id": "2506.23314", "pdf": "https://arxiv.org/pdf/2506.23314", "abs": "https://arxiv.org/abs/2506.23314", "authors": ["Joner Assolin", "Gabriel Canto", "Diego Kreutz", "Eduardo Feitosa", "Hendrio Bragança", "Angelo Nogueira", "Vanderson Rocha"], "title": "Interpretable by Design: MH-AutoML for Transparent and Efficient Android Malware Detection without Compromising Performance", "categories": ["cs.CR", "cs.AI", "68T99", "I.2"], "comment": "18 pages, 10 figures, 7 tabelas, paper submitted to JBCS", "summary": "Malware detection in Android systems requires both cybersecurity expertise\nand machine learning (ML) techniques. Automated Machine Learning (AutoML) has\nemerged as an approach to simplify ML development by reducing the need for\nspecialized knowledge. However, current AutoML solutions typically operate as\nblack-box systems with limited transparency, interpretability, and experiment\ntraceability. To address these limitations, we present MH-AutoML, a\ndomain-specific framework for Android malware detection. MH-AutoML automates\nthe entire ML pipeline, including data preprocessing, feature engineering,\nalgorithm selection, and hyperparameter tuning. The framework incorporates\ncapabilities for interpretability, debugging, and experiment tracking that are\noften missing in general-purpose solutions. In this study, we compare MH-AutoML\nagainst seven established AutoML frameworks: Auto-Sklearn, AutoGluon, TPOT,\nHyperGBM, Auto-PyTorch, LightAutoML, and MLJAR. Results show that MH-AutoML\nachieves better recall rates while providing more transparency and control. The\nframework maintains computational efficiency comparable to other solutions,\nmaking it suitable for cybersecurity applications where both performance and\nexplainability matter.", "AI": {"tldr": "MH-AutoML是一个针对Android恶意软件检测的领域特定AutoML框架，解决了现有AutoML的黑盒问题，提供更高的透明度和可控性。", "motivation": "现有AutoML解决方案在透明性、可解释性和实验可追溯性方面存在不足，限制了其在网络安全领域的应用。", "method": "MH-AutoML自动化整个ML流程，包括数据预处理、特征工程、算法选择和超参数调优，并加入可解释性、调试和实验追踪功能。", "result": "MH-AutoML在召回率上优于七种主流AutoML框架，同时保持计算效率，适合对性能和可解释性要求高的网络安全应用。", "conclusion": "MH-AutoML通过提升透明性和可控性，为Android恶意软件检测提供了一种高效且可解释的解决方案。"}}
{"id": "2506.23623", "pdf": "https://arxiv.org/pdf/2506.23623", "abs": "https://arxiv.org/abs/2506.23623", "authors": ["Shaofei Huang", "Rui Ling", "Tianrui Hui", "Hongyu Li", "Xu Zhou", "Shifeng Zhang", "Si Liu", "Richang Hong", "Meng Wang"], "title": "Revisiting Audio-Visual Segmentation with Vision-Centric Transformer", "categories": ["cs.CV"], "comment": "Accepted by CVPR 2025; Code: https://github.com/spyflying/VCT_AVS;\n  Models: https://huggingface.co/nowherespyfly/VCT_AVS", "summary": "Audio-Visual Segmentation (AVS) aims to segment sound-producing objects in\nvideo frames based on the associated audio signal. Prevailing AVS methods\ntypically adopt an audio-centric Transformer architecture, where object queries\nare derived from audio features. However, audio-centric Transformers suffer\nfrom two limitations: perception ambiguity caused by the mixed nature of audio,\nand weakened dense prediction ability due to visual detail loss. To address\nthese limitations, we propose a new Vision-Centric Transformer (VCT) framework\nthat leverages vision-derived queries to iteratively fetch corresponding audio\nand visual information, enabling queries to better distinguish between\ndifferent sounding objects from mixed audio and accurately delineate their\ncontours. Additionally, we also introduce a Prototype Prompted Query Generation\n(PPQG) module within our VCT framework to generate vision-derived queries that\nare both semantically aware and visually rich through audio prototype prompting\nand pixel context grouping, facilitating audio-visual information aggregation.\nExtensive experiments demonstrate that our VCT framework achieves new\nstate-of-the-art performances on three subsets of the AVSBench dataset. The\ncode is available at https://github.com/spyflying/VCT_AVS.", "AI": {"tldr": "论文提出了一种基于视觉中心的Transformer（VCT）框架，通过视觉驱动的查询迭代获取音频和视觉信息，解决了音频中心Transformer的感知模糊和视觉细节丢失问题。", "motivation": "音频中心Transformer存在感知模糊和视觉细节丢失的局限性，影响了音频-视觉分割（AVS）的性能。", "method": "提出VCT框架，利用视觉驱动的查询迭代获取音频和视觉信息，并引入原型提示查询生成（PPQG）模块生成语义感知和视觉丰富的查询。", "result": "在AVSBench数据集的三个子集上实现了新的最先进性能。", "conclusion": "VCT框架有效解决了音频中心Transformer的局限性，提升了音频-视觉分割的准确性和鲁棒性。"}}
{"id": "2506.23627", "pdf": "https://arxiv.org/pdf/2506.23627", "abs": "https://arxiv.org/abs/2506.23627", "authors": ["Roham Maiti", "Debasmita Bhoumik"], "title": "Brain Tumor Detection through Thermal Imaging and MobileNET", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Brain plays a crucial role in regulating body functions and cognitive\nprocesses, with brain tumors posing significant risks to human health. Precise\nand prompt detection is a key factor in proper treatment and better patient\noutcomes. Traditional methods for detecting brain tumors, that include\nbiopsies, MRI, and CT scans often face challenges due to their high costs and\nthe need for specialized medical expertise. Recent developments in machine\nlearning (ML) and deep learning (DL) has exhibited strong capabilities in\nautomating the identification and categorization of brain tumors from medical\nimages, especially MRI scans. However, these classical ML models have\nlimitations, such as high computational demands, the need for large datasets,\nand long training times, which hinder their accessibility and efficiency. Our\nresearch uses MobileNET model for efficient detection of these tumors. The\nnovelty of this project lies in building an accurate tumor detection model\nwhich use less computing re-sources and runs in less time followed by efficient\ndecision making through the use of image processing technique for accurate\nresults. The suggested method attained an average accuracy of 98.5%.", "AI": {"tldr": "论文提出了一种基于MobileNET的高效脑肿瘤检测方法，解决了传统方法和经典机器学习模型的局限性，实现了高准确率和低计算资源需求。", "motivation": "脑肿瘤对人类健康构成重大威胁，传统检测方法成本高且依赖专业医疗知识，经典机器学习模型存在计算资源需求大和训练时间长的问题。", "method": "采用MobileNET模型结合图像处理技术，构建了一个计算资源需求低、运行时间短的脑肿瘤检测模型。", "result": "提出的方法平均准确率达到98.5%。", "conclusion": "该方法在脑肿瘤检测中表现出高效性和准确性，为临床诊断提供了潜在支持。"}}
{"id": "2506.23325", "pdf": "https://arxiv.org/pdf/2506.23325", "abs": "https://arxiv.org/abs/2506.23325", "authors": ["Yitian Gong", "Luozhijie Jin", "Ruifan Deng", "Dong Zhang", "Xin Zhang", "Qinyuan Cheng", "Zhaoye Fei", "Shimin Li", "Xipeng Qiu"], "title": "XY-Tokenizer: Mitigating the Semantic-Acoustic Conflict in Low-Bitrate Speech Codecs", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Speech codecs serve as bridges between speech signals and large language\nmodels. An ideal codec for speech language models should not only preserve\nacoustic information but also capture rich semantic information. However,\nexisting speech codecs struggle to balance high-quality audio reconstruction\nwith ease of modeling by language models. In this study, we analyze the\nlimitations of previous codecs in balancing semantic richness and acoustic\nfidelity. We propose XY-Tokenizer, a novel codec that mitigates the conflict\nbetween semantic and acoustic capabilities through multi-stage, multi-task\nlearning. Experimental results demonstrate that XY-Tokenizer achieves\nperformance in both semantic and acoustic tasks comparable to that of\nstate-of-the-art codecs operating at similar bitrates, even though those\nexisting codecs typically excel in only one aspect. Specifically, XY-Tokenizer\nachieves strong text alignment, surpassing distillation-based semantic modeling\nmethods such as SpeechTokenizer and Mimi, while maintaining a speaker\nsimilarity score of 0.83 between reconstructed and original audio. The\nreconstruction performance of XY-Tokenizer is comparable to that of BigCodec,\nthe current state-of-the-art among acoustic-only codecs, which achieves a\nspeaker similarity score of 0.84 at a similar bitrate. Code and models are\navailable at https://github.com/gyt1145028706/XY-Tokenizer.", "AI": {"tldr": "XY-Tokenizer是一种新型语音编解码器，通过多阶段多任务学习平衡语义丰富性和声学保真度，性能优于现有编解码器。", "motivation": "现有语音编解码器难以同时满足高质量音频重建和语言模型建模的需求，需要一种能平衡语义和声学能力的解决方案。", "method": "提出XY-Tokenizer，采用多阶段多任务学习方法，缓解语义与声学能力之间的冲突。", "result": "XY-Tokenizer在语义和声学任务上表现优异，文本对齐性能超越SpeechTokenizer和Mimi，声学重建性能接近BigCodec。", "conclusion": "XY-Tokenizer成功平衡了语义和声学能力，为语音语言模型提供了更优的编解码器选择。"}}
{"id": "2506.23630", "pdf": "https://arxiv.org/pdf/2506.23630", "abs": "https://arxiv.org/abs/2506.23630", "authors": ["Lorenzo Olearo", "Giorgio Longari", "Alessandro Raganato", "Rafael Peñaloza", "Simone Melzi"], "title": "Blending Concepts with Text-to-Image Diffusion Models", "categories": ["cs.CV"], "comment": "Currently under review", "summary": "Diffusion models have dramatically advanced text-to-image generation in\nrecent years, translating abstract concepts into high-fidelity images with\nremarkable ease. In this work, we examine whether they can also blend distinct\nconcepts, ranging from concrete objects to intangible ideas, into coherent new\nvisual entities under a zero-shot framework. Specifically, concept blending\nmerges the key attributes of multiple concepts (expressed as textual prompts)\ninto a single, novel image that captures the essence of each concept. We\ninvestigate four blending methods, each exploiting different aspects of the\ndiffusion pipeline (e.g., prompt scheduling, embedding interpolation, or\nlayer-wise conditioning). Through systematic experimentation across diverse\nconcept categories, such as merging concrete concepts, synthesizing compound\nwords, transferring artistic styles, and blending architectural landmarks, we\nshow that modern diffusion models indeed exhibit creative blending capabilities\nwithout further training or fine-tuning. Our extensive user study, involving\n100 participants, reveals that no single approach dominates in all scenarios:\neach blending technique excels under certain conditions, with factors like\nprompt ordering, conceptual distance, and random seed affecting the outcome.\nThese findings highlight the remarkable compositional potential of diffusion\nmodels while exposing their sensitivity to seemingly minor input variations.", "AI": {"tldr": "扩散模型在零样本框架下能够将不同概念（从具体对象到抽象想法）融合为连贯的新视觉实体，展示了其创造性能力。", "motivation": "探索扩散模型是否能够在不进行额外训练或微调的情况下，将多个概念的关键属性融合为单一新颖的图像。", "method": "研究了四种融合方法，包括提示调度、嵌入插值和分层条件等，通过系统实验验证其效果。", "result": "实验表明扩散模型具有创造性融合能力，但不同方法在不同场景下表现各异，用户研究显示无单一方法在所有情况下占优。", "conclusion": "扩散模型展现出显著的组合潜力，但对输入变化的敏感性较高。"}}
{"id": "2506.23334", "pdf": "https://arxiv.org/pdf/2506.23334", "abs": "https://arxiv.org/abs/2506.23334", "authors": ["Hongyi Pan", "Ziliang Hong", "Gorkem Durak", "Ziyue Xu", "Ulas Bagci"], "title": "Federated Breast Cancer Detection Enhanced by Synthetic Ultrasound Image Augmentation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Federated learning (FL) has emerged as a promising paradigm for\ncollaboratively training deep learning models across institutions without\nexchanging sensitive medical data. However, its effectiveness is often hindered\nby limited data availability and non-independent, identically distributed data\nacross participating clients, which can degrade model performance and\ngeneralization. To address these challenges, we propose a generative AI based\ndata augmentation framework that integrates synthetic image sharing into the\nfederated training process for breast cancer diagnosis via ultrasound images.\nSpecifically, we train two simple class-specific Deep Convolutional Generative\nAdversarial Networks: one for benign and one for malignant lesions. We then\nsimulate a realistic FL setting using three publicly available breast\nultrasound image datasets: BUSI, BUS-BRA, and UDIAT. FedAvg and FedProx are\nadopted as baseline FL algorithms. Experimental results show that incorporating\na suitable number of synthetic images improved the average AUC from 0.9206 to\n0.9237 for FedAvg and from 0.9429 to 0.9538 for FedProx. We also note that\nexcessive use of synthetic data reduced performance, underscoring the\nimportance of maintaining a balanced ratio of real and synthetic samples. Our\nfindings highlight the potential of generative AI based data augmentation to\nenhance FL results in the breast ultrasound image classification task.", "AI": {"tldr": "论文提出了一种基于生成式AI的数据增强框架，通过合成图像共享提升联邦学习在乳腺癌超声图像诊断中的性能。", "motivation": "联邦学习在医疗数据协作训练中面临数据不足和非独立同分布问题，影响模型性能和泛化能力。", "method": "训练两个类别特定的深度卷积生成对抗网络（DCGAN），分别生成良性和恶性病变的合成图像，并在联邦学习框架中结合真实与合成数据进行训练。", "result": "实验显示，适量合成图像将FedAvg的AUC从0.9206提升至0.9237，FedProx的AUC从0.9429提升至0.9538。", "conclusion": "生成式AI数据增强可有效提升联邦学习性能，但需平衡真实与合成数据的比例。"}}
{"id": "2506.23639", "pdf": "https://arxiv.org/pdf/2506.23639", "abs": "https://arxiv.org/abs/2506.23639", "authors": ["Wanpeng Zhang", "Yicheng Feng", "Hao Luo", "Yijiang Li", "Zihao Yue", "Sipeng Zheng", "Zongqing Lu"], "title": "Unified Multimodal Understanding via Byte-Pair Visual Encoding", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Multimodal large language models (MLLMs) have made significant progress in\nvision-language understanding, yet effectively aligning different modalities\nremains a fundamental challenge. We present a framework that unifies multimodal\nunderstanding by applying byte-pair encoding to visual tokens. Unlike\nconventional approaches that rely on modality-specific encoders, our method\ndirectly incorporates structural information into visual tokens, mirroring\nsuccessful tokenization strategies in text-only language models. We introduce a\npriority-guided encoding scheme that considers both frequency and spatial\nconsistency, coupled with a multi-stage training procedure based on\ncurriculum-driven data composition. These enhancements enable the transformer\nmodel to better capture cross-modal relationships and reason with visual\ninformation. Comprehensive experiments demonstrate improved performance across\ndiverse vision-language tasks. By bridging the gap between visual and textual\nrepresentations, our approach contributes to the advancement of more capable\nand efficient multimodal foundation models.", "AI": {"tldr": "提出了一种基于字节对编码的统一多模态理解框架，通过优先级引导编码和多阶段训练提升跨模态关系建模。", "motivation": "多模态大语言模型在视觉-语言理解方面取得进展，但如何有效对齐不同模态仍是挑战。", "method": "采用字节对编码将视觉标记结构化，引入优先级引导编码和多阶段训练策略。", "result": "实验表明该方法在多种视觉-语言任务中表现更优。", "conclusion": "通过弥合视觉与文本表征的差距，推动了更高效的多模态基础模型发展。"}}
{"id": "2506.23339", "pdf": "https://arxiv.org/pdf/2506.23339", "abs": "https://arxiv.org/abs/2506.23339", "authors": ["Malikussaid", "Hilal Hudan Nuha"], "title": "VALID-Mol: a Systematic Framework for Validated LLM-Assisted Molecular Design", "categories": ["cs.LG", "cs.AI", "physics.chem-ph", "q-bio.QM"], "comment": "16 pages, 1 figure, 5 algorithms, 7 tables, to be published in ICSECS\n  Conference 2025, unabridged version", "summary": "Large Language Models (LLMs) demonstrate remarkable potential for scientific\ndiscovery, but their application in domains requiring factual accuracy and\ndomain-specific constraints remains challenging. In molecular design for drug\ndiscovery, LLMs can suggest creative molecular modifications but often produce\nchemically invalid or impractical structures. We present VALID-Mol, a\nsystematic framework for integrating chemical validation with LLM-driven\nmolecular design that increases the rate of generating valid chemical\nstructures from 3% to 83%. Our approach combines methodical prompt engineering,\nautomated chemical validation, and a fine-tuned domain-adapted LLM to ensure\nreliable generation of synthesizable molecules with improved properties. Beyond\nthe specific implementation, we contribute a generalizable methodology for\nscientifically-constrained LLM applications, with quantifiable reliability\nimprovements. Computational predictions suggest our framework can generate\npromising candidates for synthesis with up to 17-fold computationally predicted\nimprovements in target affinity while maintaining synthetic accessibility. We\nprovide a detailed analysis of our prompt engineering process, validation\narchitecture, and fine-tuning approach, offering a reproducible blueprint for\napplying LLMs to other scientific domains where domain-specific validation is\nessential.", "AI": {"tldr": "VALID-Mol框架通过结合提示工程、化学验证和微调LLM，将生成有效化学结构的比例从3%提升至83%，适用于科学约束的LLM应用。", "motivation": "解决LLM在分子设计中生成无效或不实用结构的问题，提升其在科学发现中的实用性。", "method": "结合提示工程、自动化化学验证和微调领域适应的LLM，确保生成可合成且性能优化的分子。", "result": "生成有效化学结构的比例显著提升（3%到83%），计算预测目标亲和力提升17倍。", "conclusion": "VALID-Mol为科学约束的LLM应用提供了可推广的方法论，显著提升了可靠性和实用性。"}}
{"id": "2506.23641", "pdf": "https://arxiv.org/pdf/2506.23641", "abs": "https://arxiv.org/abs/2506.23641", "authors": ["Peng Huang", "Junhu Fu", "Bowen Guo", "Zeju Li", "Yuanyuan Wang", "Yi Guo"], "title": "VAP-Diffusion: Enriching Descriptions with MLLMs for Enhanced Medical Image Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "As the appearance of medical images is influenced by multiple underlying\nfactors, generative models require rich attribute information beyond labels to\nproduce realistic and diverse images. For instance, generating an image of skin\nlesion with specific patterns demands descriptions that go beyond diagnosis,\nsuch as shape, size, texture, and color. However, such detailed descriptions\nare not always accessible. To address this, we explore a framework, termed\nVisual Attribute Prompts (VAP)-Diffusion, to leverage external knowledge from\npre-trained Multi-modal Large Language Models (MLLMs) to improve the quality\nand diversity of medical image generation. First, to derive descriptions from\nMLLMs without hallucination, we design a series of prompts following\nChain-of-Thoughts for common medical imaging tasks, including dermatologic,\ncolorectal, and chest X-ray images. Generated descriptions are utilized during\ntraining and stored across different categories. During testing, descriptions\nare randomly retrieved from the corresponding category for inference. Moreover,\nto make the generator robust to unseen combination of descriptions at the test\ntime, we propose a Prototype Condition Mechanism that restricts test embeddings\nto be similar to those from training. Experiments on three common types of\nmedical imaging across four datasets verify the effectiveness of VAP-Diffusion.", "AI": {"tldr": "VAP-Diffusion框架利用多模态大语言模型（MLLMs）生成医学图像的详细描述，提升生成质量和多样性。通过设计链式思维提示和原型条件机制，解决了描述不足和未见组合的问题。", "motivation": "医学图像的生成需要丰富的属性信息，但详细描述通常难以获取。", "method": "设计链式思维提示从MLLMs生成描述，并提出原型条件机制以增强生成器的鲁棒性。", "result": "在四种数据集的三种医学图像类型上验证了VAP-Diffusion的有效性。", "conclusion": "VAP-Diffusion通过外部知识和创新机制，显著提升了医学图像生成的多样性和质量。"}}
{"id": "2506.23648", "pdf": "https://arxiv.org/pdf/2506.23648", "abs": "https://arxiv.org/abs/2506.23648", "authors": ["Zhe Liu", "Yuhao Huang", "Lian Liu", "Chengrui Zhang", "Haotian Lin", "Tong Han", "Zhiyuan Zhu", "Yanlin Chen", "Yuerui Chen", "Dong Ni", "Zhongshan Gou", "Xin Yang"], "title": "MReg: A Novel Regression Model with MoE-based Video Feature Mining for Mitral Regurgitation Diagnosis", "categories": ["cs.CV"], "comment": "10 pages, 5 figures, accepted by MICCAI 2025", "summary": "Color Doppler echocardiography is a crucial tool for diagnosing mitral\nregurgitation (MR). Recent studies have explored intelligent methods for MR\ndiagnosis to minimize user dependence and improve accuracy. However, these\napproaches often fail to align with clinical workflow and may lead to\nsuboptimal accuracy and interpretability. In this study, we introduce an\nautomated MR diagnosis model (MReg) developed on the 4-chamber cardiac color\nDoppler echocardiography video (A4C-CDV). It follows comprehensive feature\nmining strategies to detect MR and assess its severity, considering clinical\nrealities. Our contribution is threefold. First, we formulate the MR diagnosis\nas a regression task to capture the continuity and ordinal relationships\nbetween categories. Second, we design a feature selection and amplification\nmechanism to imitate the sonographer's diagnostic logic for accurate MR\ngrading. Third, inspired by the Mixture-of-Experts concept, we introduce a\nfeature summary module to extract the category-level features, enhancing the\nrepresentational capacity for more accurate grading. We trained and evaluated\nour proposed MReg on a large in-house A4C-CDV dataset comprising 1868 cases\nwith three graded regurgitation labels. Compared to other weakly supervised\nvideo anomaly detection and supervised classification methods, MReg\ndemonstrated superior performance in MR diagnosis. Our code is available at:\nhttps://github.com/cskdstz/MReg.", "AI": {"tldr": "提出了一种基于四腔心彩色多普勒超声视频的自动化二尖瓣反流诊断模型（MReg），通过回归任务、特征选择与放大机制以及专家混合概念，显著提高了诊断准确性和可解释性。", "motivation": "现有智能诊断方法常与临床工作流不符，导致准确性和可解释性不足，需开发更符合临床实际的自动化模型。", "method": "将二尖瓣反流诊断建模为回归任务，设计特征选择与放大机制模仿超声医师逻辑，并引入专家混合概念提取类别级特征。", "result": "在大规模内部数据集上验证，MReg在二尖瓣反流诊断中表现优于其他弱监督和监督分类方法。", "conclusion": "MReg通过结合临床逻辑和先进特征提取方法，显著提升了二尖瓣反流诊断的准确性和实用性。"}}
{"id": "2506.23351", "pdf": "https://arxiv.org/pdf/2506.23351", "abs": "https://arxiv.org/abs/2506.23351", "authors": ["Tianxing Chen", "Kaixuan Wang", "Zhaohui Yang", "Yuhao Zhang", "Zanxin Chen", "Baijun Chen", "Wanxi Dong", "Ziyuan Liu", "Dong Chen", "Tianshuo Yang", "Haibao Yu", "Xiaokang Yang", "Yusen Qin", "Zhiqiang Xie", "Yao Mu", "Ping Luo", "Tian Nian", "Weiliang Deng", "Yiheng Ge", "Yibin Liu", "Zixuan Li", "Dehui Wang", "Zhixuan Liang", "Haohui Xie", "Rijie Zeng", "Yunfei Ge", "Peiqing Cong", "Guannan He", "Zhaoming Han", "Ruocheng Yin", "Jingxiang Guo", "Lunkai Lin", "Tianling Xu", "Hongzhe Bi", "Xuewu Lin", "Tianwei Lin", "Shujie Luo", "Keyu Li", "Ziyan Zhao", "Ke Fan", "Heyang Xu", "Bo Peng", "Wenlong Gao", "Dongjiang Li", "Feng Jin", "Hui Shen", "Jinming Li", "Chaowei Cui", "Yuchen", "Yaxin Peng", "Lingdong Zeng", "Wenlong Dong", "Tengfei Li", "Weijie Ke", "Jun Chen", "Erdemt Bao", "Tian Lan", "Tenglong Liu", "Jin Yang", "Huiping Zhuang", "Baozhi Jia", "Shuai Zhang", "Zhengfeng Zou", "Fangheng Guan", "Tianyi Jia", "Ke Zhou", "Hongjiu Zhang", "Yating Han", "Cheng Fang", "Yixian Zou", "Chongyang Xu", "Qinglun Zhang", "Shen Cheng", "Xiaohe Wang", "Ping Tan", "Haoqiang Fan", "Shuaicheng Liu", "Jiaheng Chen", "Chuxuan Huang", "Chengliang Lin", "Kaijun Luo", "Boyu Yue", "Yi Liu", "Jinyu Chen", "Zichang Tan", "Liming Deng", "Shuo Xu", "Zijian Cai", "Shilong Yin", "Hao Wang", "Hongshan Liu", "Tianyang Li", "Long Shi", "Ran Xu", "Huilin Xu", "Zhengquan Zhang", "Congsheng Xu", "Jinchang Yang", "Feng Xu"], "title": "Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA"], "comment": "Challenge Webpage:\n  https://robotwin-benchmark.github.io/cvpr-2025-challenge/", "summary": "Embodied Artificial Intelligence (Embodied AI) is an emerging frontier in\nrobotics, driven by the need for autonomous systems that can perceive, reason,\nand act in complex physical environments. While single-arm systems have shown\nstrong task performance, collaborative dual-arm systems are essential for\nhandling more intricate tasks involving rigid, deformable, and\ntactile-sensitive objects. To advance this goal, we launched the RoboTwin\nDual-Arm Collaboration Challenge at the 2nd MEIS Workshop, CVPR 2025. Built on\nthe RoboTwin Simulation platform (1.0 and 2.0) and the AgileX COBOT-Magic Robot\nplatform, the competition consisted of three stages: Simulation Round 1,\nSimulation Round 2, and a final Real-World Round. Participants totally tackled\n17 dual-arm manipulation tasks, covering rigid, deformable, and tactile-based\nscenarios. The challenge attracted 64 global teams and over 400 participants,\nproducing top-performing solutions like SEM and AnchorDP3 and generating\nvaluable insights into generalizable bimanual policy learning. This report\noutlines the competition setup, task design, evaluation methodology, key\nfindings and future direction, aiming to support future research on robust and\ngeneralizable bimanual manipulation policies. The Challenge Webpage is\navailable at https://robotwin-benchmark.github.io/cvpr-2025-challenge/.", "AI": {"tldr": "论文介绍了RoboTwin双臂协作挑战赛，旨在推动双臂机器人处理复杂任务的研究，吸引了全球团队参与，并提出了通用双臂策略学习的关键见解。", "motivation": "推动自主系统在复杂物理环境中的感知、推理和行动能力，特别是双臂协作系统在处理刚性、可变形和触觉敏感物体任务中的重要性。", "method": "基于RoboTwin仿真平台和AgileX COBOT-Magic机器人平台，设计了三个阶段的竞赛（仿真两轮和真实世界一轮），涵盖17种双臂操作任务。", "result": "吸引了64支全球团队和400多名参与者，产生了如SEM和AnchorDP3等高性能解决方案，并提供了通用双臂策略学习的关键见解。", "conclusion": "竞赛为未来研究提供了宝贵的经验和方向，支持开发鲁棒且通用的双臂操作策略。"}}
{"id": "2506.23657", "pdf": "https://arxiv.org/pdf/2506.23657", "abs": "https://arxiv.org/abs/2506.23657", "authors": ["Connor Daly", "Elettra Marconi", "Marco Riva", "Jinendra Ekanayake", "Daniel S. Elson", "Ferdinando Rodriguez y Baena"], "title": "Towards Markerless Intraoperative Tracking of Deformable Spine Tissue", "categories": ["cs.CV"], "comment": "Preprint of paper, submitted", "summary": "Consumer-grade RGB-D imaging for intraoperative orthopedic tissue tracking is\na promising method with high translational potential. Unlike bone-mounted\ntracking devices, markerless tracking can reduce operating time and complexity.\nHowever, its use has been limited to cadaveric studies. This paper introduces\nthe first real-world clinical RGB-D dataset for spine surgery and develops\nSpineAlign, a system for capturing deformation between preoperative and\nintraoperative spine states. We also present an intraoperative segmentation\nnetwork trained on this data and introduce CorrespondNet, a multi-task\nframework for predicting key regions for registration in both intraoperative\nand preoperative scenes.", "AI": {"tldr": "论文介绍了首个用于脊柱手术的临床RGB-D数据集，开发了SpineAlign系统用于捕捉术前和术中脊柱状态的变形，并提出了一个术中分割网络和CorrespondNet多任务框架。", "motivation": "研究动机是开发一种无需标记的骨科组织跟踪方法，以减少手术时间和复杂性，填补了现有技术仅限于尸体研究的空白。", "method": "方法包括创建临床RGB-D数据集，开发SpineAlign系统，训练术中分割网络，以及提出CorrespondNet多任务框架。", "result": "结果包括首个临床RGB-D数据集的发布，以及SpineAlign和CorrespondNet系统的成功开发。", "conclusion": "结论表明，该方法具有高转化潜力，能够有效减少手术时间和复杂性。"}}
{"id": "2506.23358", "pdf": "https://arxiv.org/pdf/2506.23358", "abs": "https://arxiv.org/abs/2506.23358", "authors": ["Pawel Renc", "Michal K. Grzeszczyk", "Linglong Qian", "Nassim Oufattole", "Jeff Rasley", "Arkadiusz Sitek"], "title": "Federated Timeline Synthesis: Scalable and Private Methodology For Model Training and Deployment", "categories": ["cs.LG", "cs.AI"], "comment": "conference paper", "summary": "We present Federated Timeline Synthesis (FTS), a novel framework for training\ngenerative foundation models across distributed timeseries data applied to\nelectronic health records (EHR). At its core, FTS represents patient history as\ntokenized Patient Health Timelines (PHTs), language-agnostic sequences encoding\ntemporal, categorical, and continuous clinical information. Each institution\ntrains an autoregressive transformer on its local PHTs and transmits only model\nweights to a central server. The server uses the generators to synthesize a\nlarge corpus of trajectories and train a Global Generator (GG), enabling\nzero-shot inference via Monte Carlo simulation of future PHTs. We evaluate FTS\non five clinically meaningful prediction tasks using MIMIC-IV data, showing\nthat models trained on synthetic data generated by GG perform comparably to\nthose trained on real data. FTS offers strong privacy guarantees, scalability\nacross institutions, and extensibility to diverse prediction and simulation\ntasks especially in healthcare, including counterfactual inference, early\nwarning detection, and synthetic trial design.", "AI": {"tldr": "FTS是一种用于分布式时间序列数据（如电子健康记录）的生成基础模型训练框架，通过合成数据实现隐私保护和高效预测。", "motivation": "解决分布式医疗数据隐私问题，同时支持高效的时间序列预测和模拟任务。", "method": "将患者历史表示为语言无关的PHT序列，本地训练自回归变换器，通过中央服务器合成全局生成器。", "result": "在MIMIC-IV数据上验证，合成数据训练的模型性能与真实数据相当。", "conclusion": "FTS在隐私保护、可扩展性和多样化医疗任务中具有潜力。"}}
{"id": "2506.23663", "pdf": "https://arxiv.org/pdf/2506.23663", "abs": "https://arxiv.org/abs/2506.23663", "authors": ["Mario Koddenbrock", "Rudolf Hoffmann", "David Brodmann", "Erik Rodner"], "title": "On the Domain Robustness of Contrastive Vision-Language Models", "categories": ["cs.CV", "cs.LG", "I.4"], "comment": "Deepbench is available at https://github.com/ml-lab-htw/deepbench", "summary": "In real-world vision-language applications, practitioners increasingly rely\non large, pretrained foundation models rather than custom-built solutions,\ndespite limited transparency regarding their training data and processes. While\nthese models achieve impressive performance on general benchmarks, their\neffectiveness can decline notably under specialized domain shifts, such as\nunique imaging conditions or environmental variations. In this work, we\nintroduce Deepbench, a framework designed to assess domain-specific robustness\nof vision-language models (VLMs). Deepbench leverages a large language model\n(LLM) to generate realistic, context-aware image corruptions tailored to\nspecific deployment domains without requiring labeled data. We evaluate a range\nof contrastive vision-language architectures and architectural variants across\nsix real-world domains and observe substantial variability in robustness,\nhighlighting the need for targeted, domain-aware evaluation. Deepbench is\nreleased as open-source software to support further research into domain-aware\nrobustness assessment.", "AI": {"tldr": "Deepbench是一个评估视觉语言模型在特定领域鲁棒性的框架，利用LLM生成领域相关的图像损坏，无需标注数据。", "motivation": "现实应用中，预训练大模型在领域变化时性能下降，需要针对性评估其鲁棒性。", "method": "利用LLM生成领域特定的图像损坏，评估多种视觉语言模型架构。", "result": "在不同领域中，模型鲁棒性差异显著，凸显领域感知评估的必要性。", "conclusion": "Deepbench为领域感知鲁棒性评估提供了开源工具，支持进一步研究。"}}
{"id": "2506.23674", "pdf": "https://arxiv.org/pdf/2506.23674", "abs": "https://arxiv.org/abs/2506.23674", "authors": ["Dongyue Wu", "Zilin Guo", "Jialong Zuo", "Nong Sang", "Changxin Gao"], "title": "Partial Forward Blocking: A Novel Data Pruning Paradigm for Lossless Training Acceleration", "categories": ["cs.CV"], "comment": "Accepted by ICCV2025", "summary": "The ever-growing size of training datasets enhances the generalization\ncapability of modern machine learning models but also incurs exorbitant\ncomputational costs. Existing data pruning approaches aim to accelerate\ntraining by removing those less important samples. However, they often rely on\ngradients or proxy models, leading to prohibitive additional costs of gradient\nback-propagation and proxy model training. In this paper, we propose Partial\nForward Blocking (PFB), a novel framework for lossless training acceleration.\nThe efficiency of PFB stems from its unique adaptive pruning pipeline: sample\nimportance is assessed based on features extracted from the shallow layers of\nthe target model. Less important samples are then pruned, allowing only the\nretained ones to proceed with the subsequent forward pass and loss\nback-propagation. This mechanism significantly reduces the computational\noverhead of deep-layer forward passes and back-propagation for pruned samples,\nwhile also eliminating the need for auxiliary backward computations and proxy\nmodel training. Moreover, PFB introduces probability density as an indicator of\nsample importance. Combined with an adaptive distribution estimation module,\nour method dynamically prioritizes relatively rare samples, aligning with the\nconstantly evolving training state. Extensive experiments demonstrate the\nsignificant superiority of PFB in performance and speed. On ImageNet, PFB\nachieves a 0.5% accuracy improvement and 33% training time reduction with 40%\ndata pruned.", "AI": {"tldr": "提出了一种名为PFB的无损训练加速框架，通过浅层特征评估样本重要性并动态剪枝，显著减少计算开销，同时提升性能。", "motivation": "解决现有数据剪枝方法因依赖梯度或代理模型而带来的额外计算成本问题。", "method": "基于浅层特征评估样本重要性，动态剪枝不重要样本，仅保留样本进行后续计算，并引入概率密度作为重要性指标。", "result": "在ImageNet上，剪枝40%数据时，PFB实现了0.5%的准确率提升和33%的训练时间减少。", "conclusion": "PFB在性能和速度上均显著优于现有方法，为高效训练提供了新思路。"}}
{"id": "2506.23675", "pdf": "https://arxiv.org/pdf/2506.23675", "abs": "https://arxiv.org/abs/2506.23675", "authors": ["Patrick Glandorf", "Bodo Rosenhahn"], "title": "Pruning by Block Benefit: Exploring the Properties of Vision Transformer Blocks during Domain Adaptation", "categories": ["cs.CV"], "comment": "ICCV'25 Workshops", "summary": "Vision Transformer have set new benchmarks in several tasks, but these models\ncome with the lack of high computational costs which makes them impractical for\nresource limited hardware. Network pruning reduces the computational complexity\nby removing less important operations while maintaining performance. However,\npruning a model on an unseen data domain, leads to a misevaluation of weight\nsignificance, resulting in suboptimal resource assignment. In this work, we\nfind that task-sensitive layers initially fail to improve the feature\nrepresentation on downstream tasks, leading to performance loss for early\npruning decisions. To address this problem, we introduce Pruning by Block\nBenefit (P3B), a pruning method that utilizes the relative contribution on\nblock level to globally assign parameter resources. P3B identifies low-impact\ncomponents to reduce parameter allocation while preserving critical ones.\nClassical pruning mask optimization struggles to reactivate zero-mask-elements.\nIn contrast, P3B sets a layerwise keep ratio based on global performance\nmetrics, ensuring the reactivation of late-converging blocks. We show in\nextensive experiments that P3B is a state of the art pruning method with most\nnoticeable gains in transfer learning tasks. Notably, P3B is able to conserve\nhigh performance, even in high sparsity regimes of 70% parameter reduction\nwhile only losing 0.64% in accuracy.", "AI": {"tldr": "P3B是一种基于块级别贡献的剪枝方法，旨在全局分配参数资源，减少低影响组件的同时保留关键部分，显著提升资源受限硬件上的模型效率。", "motivation": "Vision Transformer在多个任务中表现优异，但计算成本高，难以在资源有限的硬件上应用。传统剪枝方法在未见数据域上会导致权重重要性误判，性能下降。", "method": "提出P3B方法，通过块级别的相对贡献全局分配参数资源，动态调整层间保留比例，确保后期收敛块能被重新激活。", "result": "P3B在实验中表现优异，尤其在迁移学习任务中，即使参数减少70%，精度仅下降0.64%。", "conclusion": "P3B是一种先进的剪枝方法，能在高稀疏度下保持高性能，适用于资源受限场景。"}}
{"id": "2506.23676", "pdf": "https://arxiv.org/pdf/2506.23676", "abs": "https://arxiv.org/abs/2506.23676", "authors": ["Gaozheng Pei", "Ke Ma", "Dongpeng Zhang", "Chengzhi Sun", "Qianqian Xu", "Qingming Huang"], "title": "A Unified Framework for Stealthy Adversarial Generation via Latent Optimization and Transferability Enhancement", "categories": ["cs.CV"], "comment": null, "summary": "Due to their powerful image generation capabilities, diffusion-based\nadversarial example generation methods through image editing are rapidly\ngaining popularity. However, due to reliance on the discriminative capability\nof the diffusion model, these diffusion-based methods often struggle to\ngeneralize beyond conventional image classification tasks, such as in Deepfake\ndetection. Moreover, traditional strategies for enhancing adversarial example\ntransferability are challenging to adapt to these methods. To address these\nchallenges, we propose a unified framework that seamlessly incorporates\ntraditional transferability enhancement strategies into diffusion model-based\nadversarial example generation via image editing, enabling their application\nacross a wider range of downstream tasks. Our method won first place in the\n\"1st Adversarial Attacks on Deepfake Detectors: A Challenge in the Era of\nAI-Generated Media\" competition at ACM MM25, which validates the effectiveness\nof our approach.", "AI": {"tldr": "提出了一种统一框架，将传统对抗样本增强策略融入基于扩散模型的图像编辑方法，提升了其在更广泛下游任务中的适用性。", "motivation": "扩散模型在图像生成方面表现强大，但在对抗样本生成中难以泛化到传统图像分类以外的任务（如Deepfake检测），且传统增强策略难以适配。", "method": "设计了一个统一框架，将传统对抗样本增强策略无缝整合到基于扩散模型的图像编辑方法中。", "result": "该方法在ACM MM25竞赛中获胜，验证了其有效性。", "conclusion": "提出的框架成功解决了扩散模型在对抗样本生成中的泛化和适配问题，具有广泛的应用潜力。"}}
{"id": "2506.23690", "pdf": "https://arxiv.org/pdf/2506.23690", "abs": "https://arxiv.org/abs/2506.23690", "authors": ["Shuai Tan", "Biao Gong", "Yujie Wei", "Shiwei Zhang", "Zhuoxin Liu", "Dandan Zheng", "Jingdong Chen", "Yan Wang", "Hao Ouyang", "Kecheng Zheng", "Yujun Shen"], "title": "SynMotion: Semantic-Visual Adaptation for Motion Customized Video Generation", "categories": ["cs.CV"], "comment": "Project page: https://lucaria-academy.github.io/SynMotion/", "summary": "Diffusion-based video motion customization facilitates the acquisition of\nhuman motion representations from a few video samples, while achieving\narbitrary subjects transfer through precise textual conditioning. Existing\napproaches often rely on semantic-level alignment, expecting the model to learn\nnew motion concepts and combine them with other entities (e.g., ''cats'' or\n''dogs'') to produce visually appealing results. However, video data involve\ncomplex spatio-temporal patterns, and focusing solely on semantics cause the\nmodel to overlook the visual complexity of motion. Conversely, tuning only the\nvisual representation leads to semantic confusion in representing the intended\naction. To address these limitations, we propose SynMotion, a new\nmotion-customized video generation model that jointly leverages semantic\nguidance and visual adaptation. At the semantic level, we introduce the\ndual-embedding semantic comprehension mechanism which disentangles subject and\nmotion representations, allowing the model to learn customized motion features\nwhile preserving its generative capabilities for diverse subjects. At the\nvisual level, we integrate parameter-efficient motion adapters into a\npre-trained video generation model to enhance motion fidelity and temporal\ncoherence. Furthermore, we introduce a new embedding-specific training strategy\nwhich \\textbf{alternately optimizes} subject and motion embeddings, supported\nby the manually constructed Subject Prior Video (SPV) training dataset. This\nstrategy promotes motion specificity while preserving generalization across\ndiverse subjects. Lastly, we introduce MotionBench, a newly curated benchmark\nwith diverse motion patterns. Experimental results across both T2V and I2V\nsettings demonstrate that \\method outperforms existing baselines. Project page:\nhttps://lucaria-academy.github.io/SynMotion/", "AI": {"tldr": "SynMotion提出了一种结合语义指导和视觉适应的视频运动定制生成模型，通过双嵌入语义理解机制和参数高效的运动适配器，提升了运动保真度和时间一致性。", "motivation": "现有方法仅依赖语义对齐或视觉调整，难以同时处理视频数据的复杂时空模式和语义需求。", "method": "引入双嵌入语义理解机制分离主体和运动表示，结合运动适配器和交替优化训练策略。", "result": "在T2V和I2V设置下，SynMotion优于现有基线。", "conclusion": "SynMotion通过联合优化语义和视觉表示，有效解决了运动定制中的语义混淆和视觉复杂性挑战。"}}
{"id": "2506.23419", "pdf": "https://arxiv.org/pdf/2506.23419", "abs": "https://arxiv.org/abs/2506.23419", "authors": ["Amanda S Barnard"], "title": "BenchMake: Turn any scientific data set into a reproducible benchmark", "categories": ["cs.LG", "cs.AI", "cs.DL", "62G09", "J.1"], "comment": "10 pages, 15 pages in Appendix, 15 figures, 5 tables, 57 references", "summary": "Benchmark data sets are a cornerstone of machine learning development and\napplications, ensuring new methods are robust, reliable and competitive. The\nrelative rarity of benchmark sets in computational science, due to the\nuniqueness of the problems and the pace of change in the associated domains,\nmakes evaluating new innovations difficult for computational scientists. In\nthis paper a new tool is developed and tested to potentially turn any of the\nincreasing numbers of scientific data sets made openly available into a\nbenchmark accessible to the community. BenchMake uses non-negative matrix\nfactorisation to deterministically identify and isolate challenging edge cases\non the convex hull (the smallest convex set that contains all existing data\ninstances) and partitions a required fraction of matched data instances into a\ntesting set that maximises divergence and statistical significance, across\ntabular, graph, image, signal and textual modalities. BenchMake splits are\ncompared to establish splits and random splits using ten publicly available\nbenchmark sets from different areas of science, with different sizes, shapes,\ndistributions.", "AI": {"tldr": "论文提出了一种名为BenchMake的新工具，可将公开科学数据集转化为基准测试集，通过非负矩阵分解识别边缘案例，并生成具有统计显著性的测试集。", "motivation": "由于计算科学中基准数据集的稀缺性，评估新方法变得困难，因此需要一种工具将现有科学数据集转化为可用的基准测试集。", "method": "使用非负矩阵分解确定边缘案例，并在凸包上划分测试集，以最大化差异和统计显著性。", "result": "在十个公开基准数据集上测试，BenchMake生成的测试集优于现有划分和随机划分。", "conclusion": "BenchMake为计算科学提供了一种有效的基准测试集生成工具，适用于多种数据类型。"}}
{"id": "2506.23705", "pdf": "https://arxiv.org/pdf/2506.23705", "abs": "https://arxiv.org/abs/2506.23705", "authors": ["Smriti Joshi", "Richard Osuala", "Lidia Garrucho", "Kaisar Kushibar", "Dimitri Kessler", "Oliver Diaz", "Karim Lekadir"], "title": "Single Image Test-Time Adaptation via Multi-View Co-Training", "categories": ["cs.CV"], "comment": "MICCAI 2025", "summary": "Test-time adaptation enables a trained model to adjust to a new domain during\ninference, making it particularly valuable in clinical settings where such\non-the-fly adaptation is required. However, existing techniques depend on large\ntarget domain datasets, which are often impractical and unavailable in medical\nscenarios that demand per-patient, real-time inference. Moreover, current\nmethods commonly focus on two-dimensional images, failing to leverage the\nvolumetric richness of medical imaging data. Bridging this gap, we propose a\nPatch-Based Multi-View Co-Training method for Single Image Test-Time\nadaptation. Our method enforces feature and prediction consistency through\nuncertainty-guided self-training, enabling effective volumetric segmentation in\nthe target domain with only a single test-time image. Validated on three\npublicly available breast magnetic resonance imaging datasets for tumor\nsegmentation, our method achieves performance close to the upper bound\nsupervised benchmark while also outperforming all existing state-of-the-art\nmethods, on average by a Dice Similarity Coefficient of 3.75%. We publicly\nshare our accessible codebase, readily integrable with the popular nnUNet\nframework, at https://github.com/smriti-joshi/muvi.git.", "AI": {"tldr": "提出了一种基于单图像测试时适应的补丁多视图协同训练方法，用于医学影像的实时分割，性能优于现有方法。", "motivation": "解决现有测试时适应方法依赖大数据集且无法充分利用医学影像体积信息的问题。", "method": "采用不确定性引导的自训练，通过特征和预测一致性实现单图像测试时适应。", "result": "在三个乳腺癌MRI数据集上验证，性能接近监督基准，平均Dice系数提升3.75%。", "conclusion": "方法有效且实用，代码已开源，易于集成到nnUNet框架。"}}
{"id": "2506.23711", "pdf": "https://arxiv.org/pdf/2506.23711", "abs": "https://arxiv.org/abs/2506.23711", "authors": ["Haoyang Chen", "Dongfang Sun", "Caoyuan Ma", "Shiqin Wang", "Kewei Zhang", "Zheng Wang", "Zhixiang Wang"], "title": "Subjective Camera: Bridging Human Cognition and Visual Reconstruction through Sequence-Aware Sketch-Guided Diffusion", "categories": ["cs.CV"], "comment": null, "summary": "We propose Subjective Camera, a human-as-imaging-device paradigm that\nreconstructs real-world scenes from mental impressions through synergistic use\nof verbal descriptions and progressive rough sketches. This approach overcomes\ndual limitations of language ambiguity and sketch abstraction by treating the\nuser's drawing sequence as priors, effectively translating subjective\nperceptual expectations into photorealistic images.\n  Existing approaches face three fundamental barriers: (1) user-specific\nsubjective input biases, (2) huge modality gap between planar sketch and 3D\npriors in diffusion, and (3) sketch quality-sensitive performance degradation.\nCurrent solutions either demand resource-intensive model adaptation or impose\nimpractical requirements on sketch precision.\n  Our framework addresses these challenges through concept-sequential\ngeneration. (1) We establish robust appearance priors through text-reward\noptimization, and then implement sequence-aware disentangled generation that\nprocesses concepts in sketching order; these steps accommodate user-specific\nsubjective expectation in a train-free way. (2) We employ latent optimization\nthat effectively bridges the modality gap between planar sketches and 3D priors\nin diffusion. (3) Our hierarchical reward-guided framework enables the use of\nrough sketches without demanding artistic expertise. Comprehensive evaluation\nacross diverse datasets demonstrates that our approach achieves\nstate-of-the-art performance in maintaining both semantic and spatial\ncoherence.", "AI": {"tldr": "提出了一种名为Subjective Camera的方法，通过结合语言描述和渐进草图，将用户的主观感知转化为逼真图像。", "motivation": "解决现有方法在语言模糊性、草图抽象性以及用户主观输入偏差等方面的局限性。", "method": "采用概念顺序生成框架，通过文本奖励优化建立外观先验，并利用序列感知解耦生成和潜在优化技术。", "result": "在多种数据集上验证了方法的有效性，实现了语义和空间一致性的最先进性能。", "conclusion": "该方法无需训练即可适应用户主观期望，并能处理粗糙草图，具有实际应用潜力。"}}
{"id": "2506.23424", "pdf": "https://arxiv.org/pdf/2506.23424", "abs": "https://arxiv.org/abs/2506.23424", "authors": ["Heitor R. Medeiros", "Hossein Sharifi-Noghabi", "Gabriel L. Oliveira", "Saghar Irandoust"], "title": "Accurate Parameter-Efficient Test-Time Adaptation for Time Series Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": "Second Workshop on Test-Time Adaptation: Putting Updates to the Test!\n  at ICML 2025, Vancouver, Canada. 2025", "summary": "Real-world time series often exhibit a non-stationary nature, degrading the\nperformance of pre-trained forecasting models. Test-Time Adaptation (TTA)\naddresses this by adjusting models during inference, but existing methods\ntypically update the full model, increasing memory and compute costs. We\npropose PETSA, a parameter-efficient method that adapts forecasters at test\ntime by only updating small calibration modules on the input and output. PETSA\nuses low-rank adapters and dynamic gating to adjust representations without\nretraining. To maintain accuracy despite limited adaptation capacity, we\nintroduce a specialized loss combining three components: (1) a robust term, (2)\na frequency-domain term to preserve periodicity, and (3) a patch-wise\nstructural term for structural alignment. PETSA improves the adaptability of\nvarious forecasting backbones while requiring fewer parameters than baselines.\nExperimental results on benchmark datasets show that PETSA achieves competitive\nor better performance across all horizons. Our code is available at:\nhttps://github.com/BorealisAI/PETSA", "AI": {"tldr": "PETSA是一种参数高效的方法，通过仅更新输入和输出的小型校准模块，在测试时调整预测模型，减少内存和计算成本。", "motivation": "现实世界的时间序列通常具有非平稳性，这会降低预训练预测模型的性能。现有方法通常更新整个模型，增加了内存和计算成本。", "method": "PETSA使用低秩适配器和动态门控调整表示，无需重新训练。通过结合鲁棒项、频域项和结构对齐项的专用损失函数，保持准确性。", "result": "PETSA在基准数据集上表现出色，性能优于或与基线方法相当，同时需要更少的参数。", "conclusion": "PETSA通过高效参数调整，显著提升了预测模型的适应性和性能。"}}
{"id": "2506.23724", "pdf": "https://arxiv.org/pdf/2506.23724", "abs": "https://arxiv.org/abs/2506.23724", "authors": ["Chang'an Yi", "Xiaohui Deng", "Guohao Chen", "Yan Zhou", "Qinghua Lu", "Shuaicheng Niu"], "title": "When Small Guides Large: Cross-Model Co-Learning for Test-Time Adaptation", "categories": ["cs.CV", "cs.AI"], "comment": "15 pages, 5 figures", "summary": "Test-time Adaptation (TTA) adapts a given model to testing domain data with\npotential domain shifts through online unsupervised learning, yielding\nimpressive performance. However, to date, existing TTA methods primarily focus\non single-model adaptation. In this work, we investigate an intriguing\nquestion: how does cross-model knowledge influence the TTA process? Our\nfindings reveal that, in TTA's unsupervised online setting, each model can\nprovide complementary, confident knowledge to the others, even when there are\nsubstantial differences in model size. For instance, a smaller model like\nMobileViT (10.6M parameters) can effectively guide a larger model like ViT-Base\n(86.6M parameters). In light of this, we propose COCA, a Cross-Model\nCo-Learning framework for TTA, which mainly consists of two main strategies. 1)\nCo-adaptation adaptively integrates complementary knowledge from other models\nthroughout the TTA process, reducing individual model biases. 2)\nSelf-adaptation enhances each model's unique strengths via unsupervised\nlearning, enabling diverse adaptation to the target domain. Extensive\nexperiments show that COCA, which can also serve as a plug-and-play module,\nsignificantly boosts existing SOTAs, on models with various sizes--including\nResNets, ViTs, and Mobile-ViTs--via cross-model co-learned TTA. For example,\nwith Mobile-ViT's guidance, COCA raises ViT-Base's average adaptation accuracy\non ImageNet-C from 51.7% to 64.5%. The code is publicly available at\nhttps://github.com/ycarobot/COCA.", "AI": {"tldr": "本文提出了一种跨模型协同学习框架COCA，用于测试时适应（TTA），通过整合不同模型的互补知识提升性能。", "motivation": "研究跨模型知识如何影响TTA过程，发现不同模型（如大小模型）能提供互补知识。", "method": "提出COCA框架，包含协同适应（整合互补知识）和自适应（增强模型独特优势）。", "result": "COCA显著提升现有方法性能，例如将ViT-Base在ImageNet-C上的准确率从51.7%提升至64.5%。", "conclusion": "跨模型协同学习在TTA中具有显著优势，COCA可作为即插即用模块提升多种模型性能。"}}
{"id": "2506.23437", "pdf": "https://arxiv.org/pdf/2506.23437", "abs": "https://arxiv.org/abs/2506.23437", "authors": ["Stefano Giacomelli", "Marco Giordano", "Claudia Rinaldi", "Fabio Graziosi"], "title": "From Large-scale Audio Tagging to Real-Time Explainable Emergency Vehicle Sirens Detection", "categories": ["cs.SD", "cs.AI", "eess.AS", "68T07", "E.1; H.1; I.2; I.5; J.2; K.4; C.4"], "comment": "pre-print (submitted to the IEEE/ACM Transactions on Audio, Speech,\n  and Language Processing)", "summary": "Accurate recognition of Emergency Vehicle (EV) sirens is critical for the\nintegration of intelligent transportation systems, smart city monitoring\nsystems, and autonomous driving technologies. Modern automatic solutions are\nlimited by the lack of large scale, curated datasets and by the computational\ndemands of state of the art sound event detection models. This work introduces\nE2PANNs (Efficient Emergency Pre trained Audio Neural Networks), a lightweight\nConvolutional Neural Network architecture derived from the PANNs framework,\nspecifically optimized for binary EV siren detection. Leveraging our dedicated\nsubset of AudioSet (AudioSet EV) we fine-tune and evaluate E2PANNs across\nmultiple reference datasets and test its viability on embedded hardware. The\nexperimental campaign includes ablation studies, cross-domain benchmarking, and\nreal-time inference deployment on edge device. Interpretability analyses\nexploiting Guided Backpropagation and ScoreCAM algorithms provide insights into\nthe model internal representations and validate its ability to capture distinct\nspectrotemporal patterns associated with different types of EV sirens. Real\ntime performance is assessed through frame wise and event based detection\nmetrics, as well as a detailed analysis of false positive activations. Results\ndemonstrate that E2PANNs establish a new state of the art in this research\ndomain, with high computational efficiency, and suitability for edge-based\naudio monitoring and safety-critical applications.", "AI": {"tldr": "论文提出了一种轻量级卷积神经网络E2PANNs，专门用于紧急车辆警笛的二元检测，并在嵌入式硬件上验证了其高效性。", "motivation": "现有自动解决方案因缺乏大规模数据集和计算需求高而受限，急需高效模型用于智能交通和自动驾驶。", "method": "基于PANNs框架优化E2PANNs，利用AudioSet EV数据集进行微调和评估，包括消融研究、跨域基准测试和边缘设备实时部署。", "result": "E2PANNs在计算效率和性能上达到新水平，适合边缘音频监控和安全关键应用。", "conclusion": "E2PANNs为紧急车辆警笛检测提供了高效解决方案，适用于实时和边缘计算场景。"}}
{"id": "2506.23729", "pdf": "https://arxiv.org/pdf/2506.23729", "abs": "https://arxiv.org/abs/2506.23729", "authors": ["Guiyu Zhang", "Chen Shi", "Zijian Jiang", "Xunzhi Xiang", "Jingjing Qian", "Shaoshuai Shi", "Li Jiang"], "title": "Proteus-ID: ID-Consistent and Motion-Coherent Video Customization", "categories": ["cs.CV"], "comment": "Preprint. Work in progress", "summary": "Video identity customization seeks to synthesize realistic, temporally\ncoherent videos of a specific subject, given a single reference image and a\ntext prompt. This task presents two core challenges: (1) maintaining identity\nconsistency while aligning with the described appearance and actions, and (2)\ngenerating natural, fluid motion without unrealistic stiffness. To address\nthese challenges, we introduce Proteus-ID, a novel diffusion-based framework\nfor identity-consistent and motion-coherent video customization. First, we\npropose a Multimodal Identity Fusion (MIF) module that unifies visual and\ntextual cues into a joint identity representation using a Q-Former, providing\ncoherent guidance to the diffusion model and eliminating modality imbalance.\nSecond, we present a Time-Aware Identity Injection (TAII) mechanism that\ndynamically modulates identity conditioning across denoising steps, improving\nfine-detail reconstruction. Third, we propose Adaptive Motion Learning (AML), a\nself-supervised strategy that reweights the training loss based on\noptical-flow-derived motion heatmaps, enhancing motion realism without\nrequiring additional inputs. To support this task, we construct Proteus-Bench,\na high-quality dataset comprising 200K curated clips for training and 150\nindividuals from diverse professions and ethnicities for evaluation. Extensive\nexperiments demonstrate that Proteus-ID outperforms prior methods in identity\npreservation, text alignment, and motion quality, establishing a new benchmark\nfor video identity customization. Codes and data are publicly available at\nhttps://grenoble-zhang.github.io/Proteus-ID/.", "AI": {"tldr": "Proteus-ID是一种基于扩散的框架，用于身份一致且运动连贯的视频定制，通过多模态身份融合和时间感知身份注入解决身份一致性和运动自然性问题。", "motivation": "视频身份定制任务面临身份一致性和运动自然性两大挑战，需要一种方法在保持身份的同时生成流畅动作。", "method": "提出Proteus-ID框架，包括多模态身份融合模块、时间感知身份注入机制和自适应运动学习策略。", "result": "实验表明Proteus-ID在身份保持、文本对齐和运动质量上优于现有方法。", "conclusion": "Proteus-ID为视频身份定制设立了新基准，代码和数据已公开。"}}
{"id": "2506.23751", "pdf": "https://arxiv.org/pdf/2506.23751", "abs": "https://arxiv.org/abs/2506.23751", "authors": ["Annika Mütze", "Sadia Ilyas", "Christian Dörpelkus", "Matthias Rottmann"], "title": "Can We Challenge Open-Vocabulary Object Detectors with Generated Content in Street Scenes?", "categories": ["cs.CV"], "comment": null, "summary": "Open-vocabulary object detectors such as Grounding DINO are trained on vast\nand diverse data, achieving remarkable performance on challenging datasets. Due\nto that, it is unclear where to find their limitations, which is of major\nconcern when using in safety-critical applications. Real-world data does not\nprovide sufficient control, required for a rigorous evaluation of model\ngeneralization. In contrast, synthetically generated data allows to\nsystematically explore the boundaries of model competence/generalization. In\nthis work, we address two research questions: 1) Can we challenge\nopen-vocabulary object detectors with generated image content? 2) Can we find\nsystematic failure modes of those models? To address these questions, we design\ntwo automated pipelines using stable diffusion to inpaint unusual objects with\nhigh diversity in semantics, by sampling multiple substantives from WordNet and\nChatGPT. On the synthetically generated data, we evaluate and compare multiple\nopen-vocabulary object detectors as well as a classical object detector. The\nsynthetic data is derived from two real-world datasets, namely LostAndFound, a\nchallenging out-of-distribution (OOD) detection benchmark, and the NuImages\ndataset. Our results indicate that inpainting can challenge open-vocabulary\nobject detectors in terms of overlooking objects. Additionally, we find a\nstrong dependence of open-vocabulary models on object location, rather than on\nobject semantics. This provides a systematic approach to challenge\nopen-vocabulary models and gives valuable insights on how data could be\nacquired to effectively improve these models.", "AI": {"tldr": "论文探讨了如何通过合成数据挑战开放词汇目标检测器的局限性，发现其对物体位置的依赖强于语义。", "motivation": "开放词汇目标检测器在安全关键应用中存在局限性，但真实数据难以系统评估其泛化能力。", "method": "设计了两个自动化流程，使用稳定扩散技术生成多样化语义的合成数据，评估多种检测器。", "result": "合成数据能有效挑战检测器，发现其对物体位置的依赖强于语义。", "conclusion": "合成数据为挑战和改进开放词汇目标检测器提供了系统方法。"}}
{"id": "2506.23462", "pdf": "https://arxiv.org/pdf/2506.23462", "abs": "https://arxiv.org/abs/2506.23462", "authors": ["Manaswi Kulahara", "Gautam Siddharth Kashyap", "Nipun Joshi", "Arpita Soni"], "title": "Can We Predict the Unpredictable? Leveraging DisasterNet-LLM for Multimodal Disaster Classification", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted in the 2025 IEEE International Geoscience and Remote Sensing\n  Symposium (IGARSS 2025), scheduled for 3 - 8 August 2025 in Brisbane,\n  Australia", "summary": "Effective disaster management requires timely and accurate insights, yet\ntraditional methods struggle to integrate multimodal data such as images,\nweather records, and textual reports. To address this, we propose\nDisasterNet-LLM, a specialized Large Language Model (LLM) designed for\ncomprehensive disaster analysis. By leveraging advanced pretraining,\ncross-modal attention mechanisms, and adaptive transformers, DisasterNet-LLM\nexcels in disaster classification. Experimental results demonstrate its\nsuperiority over state-of-the-art models, achieving higher accuracy of 89.5%,\nan F1 score of 88.0%, AUC of 0.92%, and BERTScore of 0.88% in multimodal\ndisaster classification tasks.", "AI": {"tldr": "DisasterNet-LLM是一种专为灾害分析设计的大型语言模型，通过多模态数据整合和先进技术，显著提升了灾害分类的准确性。", "motivation": "传统方法难以整合多模态数据（如图像、天气记录和文本报告），限制了灾害管理的时效性和准确性。", "method": "采用预训练、跨模态注意力机制和自适应变压器技术，构建DisasterNet-LLM模型。", "result": "在灾害分类任务中表现优异，准确率达89.5%，F1分数88.0%，AUC 0.92，BERTScore 0.88。", "conclusion": "DisasterNet-LLM在多模态灾害分析中优于现有模型，为灾害管理提供了更高效的解决方案。"}}
{"id": "2506.23783", "pdf": "https://arxiv.org/pdf/2506.23783", "abs": "https://arxiv.org/abs/2506.23783", "authors": ["Shiao Wang", "Ju Huang", "Qingchuan Ma", "Jinfeng Gao", "Chunyi Xu", "Xiao Wang", "Lan Chen", "Bo Jiang"], "title": "Mamba-FETrack V2: Revisiting State Space Model for Frame-Event based Visual Object Tracking", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "Journal extension of Mamba-FETrack which was published on Pattern\n  Recognition and Computer Vision (PRCV) 2024", "summary": "Combining traditional RGB cameras with bio-inspired event cameras for robust\nobject tracking has garnered increasing attention in recent years. However,\nmost existing multimodal tracking algorithms depend heavily on high-complexity\nVision Transformer architectures for feature extraction and fusion across\nmodalities. This not only leads to substantial computational overhead but also\nlimits the effectiveness of cross-modal interactions. In this paper, we propose\nan efficient RGB-Event object tracking framework based on the linear-complexity\nVision Mamba network, termed Mamba-FETrack V2. Specifically, we first design a\nlightweight Prompt Generator that utilizes embedded features from each\nmodality, together with a shared prompt pool, to dynamically generate\nmodality-specific learnable prompt vectors. These prompts, along with the\nmodality-specific embedded features, are then fed into a Vision Mamba-based\nFEMamba backbone, which facilitates prompt-guided feature extraction,\ncross-modal interaction, and fusion in a unified manner. Finally, the fused\nrepresentations are passed to the tracking head for accurate target\nlocalization. Extensive experimental evaluations on multiple RGB-Event tracking\nbenchmarks, including short-term COESOT dataset and long-term datasets, i.e.,\nFE108 and FELT V2, demonstrate the superior performance and efficiency of the\nproposed tracking framework. The source code and pre-trained models will be\nreleased on https://github.com/Event-AHU/Mamba_FETrack", "AI": {"tldr": "提出了一种基于线性复杂度Vision Mamba网络的高效RGB-Event目标跟踪框架Mamba-FETrack V2，通过轻量级Prompt Generator和FEMamba主干实现跨模态特征提取与融合。", "motivation": "现有跨模态跟踪算法依赖高复杂度Vision Transformer，计算开销大且跨模态交互效果有限。", "method": "设计轻量级Prompt Generator生成模态特定提示向量，结合Vision Mamba主干实现统一特征提取与融合。", "result": "在多个RGB-Event跟踪基准测试中表现优越，包括短期的COESOT和长期的FE108、FELT V2数据集。", "conclusion": "Mamba-FETrack V2在性能和效率上均优于现有方法，代码和预训练模型将开源。"}}
{"id": "2506.23785", "pdf": "https://arxiv.org/pdf/2506.23785", "abs": "https://arxiv.org/abs/2506.23785", "authors": ["Yongjian Wu", "Yang Zhou", "Jiya Saiyin", "Bingzheng Wei", "Yan Xu"], "title": "Visual Textualization for Image Prompted Object Detection", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "We propose VisTex-OVLM, a novel image prompted object detection method that\nintroduces visual textualization -- a process that projects a few visual\nexemplars into the text feature space to enhance Object-level Vision-Language\nModels' (OVLMs) capability in detecting rare categories that are difficult to\ndescribe textually and nearly absent from their pre-training data, while\npreserving their pre-trained object-text alignment. Specifically, VisTex-OVLM\nleverages multi-scale textualizing blocks and a multi-stage fusion strategy to\nintegrate visual information from visual exemplars, generating textualized\nvisual tokens that effectively guide OVLMs alongside text prompts. Unlike\nprevious methods, our method maintains the original architecture of OVLM,\nmaintaining its generalization capabilities while enhancing performance in\nfew-shot settings. VisTex-OVLM demonstrates superior performance across\nopen-set datasets which have minimal overlap with OVLM's pre-training data and\nachieves state-of-the-art results on few-shot benchmarks PASCAL VOC and MSCOCO.\nThe code will be released at https://github.com/WitGotFlg/VisTex-OVLM.", "AI": {"tldr": "VisTex-OVLM是一种基于图像提示的目标检测方法，通过视觉文本化增强对象级视觉语言模型（OVLM）对罕见类别的检测能力，同时保持预训练的对象-文本对齐。", "motivation": "解决OVLM在罕见类别检测中的局限性，尤其是那些难以用文本描述且预训练数据中几乎不存在的类别。", "method": "采用多尺度文本化块和多阶段融合策略，将视觉示例信息转化为文本化视觉标记，与文本提示共同指导OVLM。", "result": "在开放数据集和少样本基准（如PASCAL VOC和MSCOCO）上取得最优性能。", "conclusion": "VisTex-OVLM在保持OVLM原有架构和泛化能力的同时，显著提升了少样本场景下的检测性能。"}}
{"id": "2506.23801", "pdf": "https://arxiv.org/pdf/2506.23801", "abs": "https://arxiv.org/abs/2506.23801", "authors": ["Ce Wang", "Wanjie Sun"], "title": "Controllable Reference-Based Real-World Remote Sensing Image Super-Resolution with Generative Diffusion Priors", "categories": ["cs.CV"], "comment": null, "summary": "Super-resolution (SR) techniques can enhance the spatial resolution of remote\nsensing images by utilizing low-resolution (LR) images to reconstruct\nhigh-resolution (HR) images, enabling more efficient large-scale earth\nobservation applications. While single-image super-resolution (SISR) methods\nhave shown progress, reference-based super-resolution (RefSR) offers superior\nperformance by incorporating historical HR images alongside current LR\nobservations. However, existing RefSR methods struggle with real-world\ncomplexities, such as cross-sensor resolution gap and significant land cover\nchanges, often leading to under-generation or over-reliance on reference image.\nTo address these challenges, we propose CRefDiff, a novel controllable\nreference-based diffusion model for real-world remote sensing image SR. To\naddress the under-generation problem, CRefDiff is built upon the pretrained\nStable Diffusion model, leveraging its powerful generative prior to produce\naccurate structures and textures. To mitigate over-reliance on the reference,\nwe introduce a dual-branch fusion mechanism that adaptively integrates both\nlocal and global information from the reference image. Moreover, this novel\ndual-branch design enables reference strength control during inference,\nenhancing interactivity and flexibility of the model. Finally, a strategy named\nBetter Start is proposed to significantly reduce the number of denoising steps,\nthereby accelerating the inference process. To support further research, we\nintroduce Real-RefRSSRD, a new real-world RefSR dataset for remote sensing\nimages, consisting of HR NAIP and LR Sentinel-2 image pairs with diverse land\ncover changes and significant temporal gaps. Extensive experiments on\nReal-RefRSSRD show that CRefDiff achieves state-of-the-art performance across\nvarious metrics and improves downstream tasks such as scene classification and\nsemantic segmentation.", "AI": {"tldr": "CRefDiff是一种基于扩散模型的新型可控参考超分辨率方法，通过双分支融合机制和预训练的Stable Diffusion模型，解决了现有RefSR方法在真实世界中的不足，并在新数据集Real-RefRSSRD上取得了最优性能。", "motivation": "现有参考超分辨率（RefSR）方法在真实世界应用中面临跨传感器分辨率差异和地表覆盖变化的挑战，导致生成不足或过度依赖参考图像。", "method": "提出CRefDiff模型，基于预训练的Stable Diffusion，采用双分支融合机制自适应整合参考图像的局部和全局信息，并引入Better Start策略加速推理。", "result": "在Real-RefRSSRD数据集上，CRefDiff在多项指标上达到最优，并提升了场景分类和语义分割等下游任务性能。", "conclusion": "CRefDiff通过创新的设计和策略，有效解决了真实世界RefSR的挑战，为遥感图像超分辨率提供了高效灵活的解决方案。"}}
{"id": "2506.23490", "pdf": "https://arxiv.org/pdf/2506.23490", "abs": "https://arxiv.org/abs/2506.23490", "authors": ["Junxuan Yu", "Yaofei Duan", "Yuhao Huang", "Yu Wang", "Rongbo Ling", "Weihao Luo", "Ang Zhang", "Jingxian Xu", "Qiongying Ni", "Yongsong Zhou", "Binghan Li", "Haoran Dou", "Liping Liu", "Yanfen Chu", "Feng Geng", "Zhe Sheng", "Zhifeng Ding", "Dingxin Zhang", "Rui Huang", "Yuhang Zhang", "Xiaowei Xu", "Tao Tan", "Dong Ni", "Zhongshan Gou", "Xin Yang"], "title": "UltraTwin: Towards Cardiac Anatomical Twin Generation from Multi-view 2D Ultrasound", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "accepted by miccai 2025", "summary": "Echocardiography is routine for cardiac examination. However, 2D ultrasound\n(US) struggles with accurate metric calculation and direct observation of 3D\ncardiac structures. Moreover, 3D US is limited by low resolution, small field\nof view and scarce availability in practice. Constructing the cardiac\nanatomical twin from 2D images is promising to provide precise treatment\nplanning and clinical quantification. However, it remains challenging due to\nthe rare paired data, complex structures, and US noises. In this study, we\nintroduce a novel generative framework UltraTwin, to obtain cardiac anatomical\ntwin from sparse multi-view 2D US. Our contribution is three-fold. First,\npioneered the construction of a real-world and high-quality dataset containing\nstrictly paired multi-view 2D US and CT, and pseudo-paired data. Second, we\npropose a coarse-to-fine scheme to achieve hierarchical reconstruction\noptimization. Last, we introduce an implicit autoencoder for topology-aware\nconstraints. Extensive experiments show that UltraTwin reconstructs\nhigh-quality anatomical twins versus strong competitors. We believe it advances\nanatomical twin modeling for potential applications in personalized cardiac\ncare.", "AI": {"tldr": "论文提出了一种名为UltraTwin的生成框架，用于从稀疏多视角2D超声图像中构建心脏解剖孪生体，解决了现有技术的局限性。", "motivation": "2D超声在心脏检查中难以精确计算指标和直接观察3D结构，而3D超声又存在分辨率低、视野小和实际可用性差的问题。构建心脏解剖孪生体有助于精确治疗规划和临床量化。", "method": "研究提出了UltraTwin框架，包括构建高质量数据集、采用粗到细的分层重建优化方案，以及引入隐式自编码器进行拓扑感知约束。", "result": "实验表明，UltraTwin能够重建高质量的心脏解剖孪生体，优于现有方法。", "conclusion": "UltraTwin推动了心脏解剖孪生体建模的发展，有望在个性化心脏护理中发挥作用。"}}
{"id": "2506.23808", "pdf": "https://arxiv.org/pdf/2506.23808", "abs": "https://arxiv.org/abs/2506.23808", "authors": ["Carl Olsson", "Amanda Nilsson"], "title": "Towards Initialization-free Calibrated Bundle Adjustment", "categories": ["cs.CV"], "comment": null, "summary": "A recent series of works has shown that initialization-free BA can be\nachieved using pseudo Object Space Error (pOSE) as a surrogate objective. The\ninitial reconstruction-step optimizes an objective where all terms are\nprojectively invariant and it cannot incorporate knowledge of the camera\ncalibration. As a result, the solution is only determined up to a projective\ntransformation of the scene and the process requires more data for successful\nreconstruction.\n  In contrast, we present a method that is able to use the known camera\ncalibration thereby producing near metric solutions, that is, reconstructions\nthat are accurate up to a similarity transformation. To achieve this we\nintroduce pairwise relative rotation estimates that carry information about\ncamera calibration. These are only invariant to similarity transformations,\nthus encouraging solutions that preserve metric features of the real scene. Our\nmethod can be seen as integrating rotation averaging into the pOSE framework\nstriving towards initialization-free calibrated SfM.\n  Our experimental evaluation shows that we are able to reliably optimize our\nobjective, achieving convergence to the global minimum with high probability\nfrom random starting solutions, resulting in accurate near metric\nreconstructions.", "AI": {"tldr": "论文提出了一种利用已知相机标定的方法，通过引入成对相对旋转估计，实现无需初始化的近度量重建。", "motivation": "传统方法因无法利用相机标定信息，导致重建结果仅达到投影变换精度，且需要更多数据。本文旨在解决这一问题。", "method": "引入成对相对旋转估计，结合pOSE框架，实现无需初始化的标定SfM。", "result": "实验表明，该方法能可靠优化目标，从随机初始解高概率收敛到全局最优，实现精确的近度量重建。", "conclusion": "该方法成功整合了旋转平均与pOSE框架，显著提升了重建的度量精度。"}}
{"id": "2506.23810", "pdf": "https://arxiv.org/pdf/2506.23810", "abs": "https://arxiv.org/abs/2506.23810", "authors": ["Mahshid Shiri", "Cigdem Beyan", "Vittorio Murino"], "title": "MadCLIP: Few-shot Medical Anomaly Detection with CLIP", "categories": ["cs.CV"], "comment": "Accepted to MICCAI 2025 (this version is not peer-reviewed; it is the\n  submitted version). MICCAI proceedings DOI will appear here", "summary": "An innovative few-shot anomaly detection approach is presented, leveraging\nthe pre-trained CLIP model for medical data, and adapting it for both\nimage-level anomaly classification (AC) and pixel-level anomaly segmentation\n(AS). A dual-branch design is proposed to separately capture normal and\nabnormal features through learnable adapters in the CLIP vision encoder. To\nimprove semantic alignment, learnable text prompts are employed to link visual\nfeatures. Furthermore, SigLIP loss is applied to effectively handle the\nmany-to-one relationship between images and unpaired text prompts, showcasing\nits adaptation in the medical field for the first time. Our approach is\nvalidated on multiple modalities, demonstrating superior performance over\nexisting methods for AC and AS, in both same-dataset and cross-dataset\nevaluations. Unlike prior work, it does not rely on synthetic data or memory\nbanks, and an ablation study confirms the contribution of each component. The\ncode is available at https://github.com/mahshid1998/MadCLIP.", "AI": {"tldr": "提出了一种基于预训练CLIP模型的少样本异常检测方法，适用于医学数据的图像级和像素级异常检测，通过双分支设计和可学习文本提示提升性能。", "motivation": "解决医学数据中少样本异常检测的挑战，利用CLIP模型的泛化能力，避免依赖合成数据或记忆库。", "method": "采用双分支设计捕获正常和异常特征，使用可学习文本提示增强语义对齐，并应用SigLIP损失处理图像与文本提示的多对一关系。", "result": "在多种模态上验证了方法，表现优于现有方法，适用于同数据集和跨数据集评估。", "conclusion": "该方法无需合成数据或记忆库，通过消融实验验证了各组件的重要性，代码已开源。"}}
{"id": "2506.23492", "pdf": "https://arxiv.org/pdf/2506.23492", "abs": "https://arxiv.org/abs/2506.23492", "authors": ["Haolan Guo", "Linwei Tao", "Haoyang Luo", "Minjing Dong", "Chang Xu"], "title": "Sample Margin-Aware Recalibration of Temperature Scaling", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Recent advances in deep learning have significantly improved predictive\naccuracy. However, modern neural networks remain systematically overconfident,\nposing risks for deployment in safety-critical scenarios. Current post-hoc\ncalibration methods face a fundamental dilemma: global approaches like\nTemperature Scaling apply uniform adjustments across all samples, introducing\nhigh bias despite computational efficiency, while more expressive methods that\noperate on full logit distributions suffer from high variance due to noisy\nhigh-dimensional inputs and insufficient validation data. To address these\nchallenges, we propose Sample Margin-Aware Recalibration of Temperature\n(SMART), a lightweight, data-efficient recalibration method that precisely\nscales logits based on the margin between the top two logits -- termed the\nlogit gap. Specifically, the logit gap serves as a denoised, scalar signal\ndirectly tied to decision boundary uncertainty, providing a robust indicator\nthat avoids the noise inherent in high-dimensional logit spaces while\npreserving model prediction invariance. Meanwhile, SMART employs a novel\nsoft-binned Expected Calibration Error (SoftECE) objective that balances model\nbias and variance through adaptive binning, enabling stable parameter updates\neven with extremely limited calibration data. Extensive evaluations across\ndiverse datasets and architectures demonstrate that SMART achieves\nstate-of-the-art calibration performance even with substantially fewer\nparameters compared to existing parametric methods, offering a principled,\nrobust, and highly efficient solution for practical uncertainty quantification\nin neural network predictions. The source code is available at:\nhttps://anonymous.4open.science/r/SMART-8B11.", "AI": {"tldr": "SMART是一种轻量级、数据高效的校准方法，通过基于logit间隙的精确调整和自适应分箱目标，显著提升了神经网络的校准性能。", "motivation": "现代神经网络存在系统性过度自信问题，现有校准方法在全局调整和表达性之间难以平衡，导致高偏差或高方差。", "method": "提出SMART方法，利用logit间隙作为去噪信号，结合SoftECE目标进行自适应分箱，实现高效校准。", "result": "SMART在多种数据集和架构上表现优异，参数更少且校准性能达到最优。", "conclusion": "SMART为神经网络预测中的不确定性量化提供了高效、稳健的解决方案。"}}
{"id": "2506.23822", "pdf": "https://arxiv.org/pdf/2506.23822", "abs": "https://arxiv.org/abs/2506.23822", "authors": ["Shiming Chen", "Bowen Duan", "Salman Khan", "Fahad Shahbaz Khan"], "title": "Interpretable Zero-Shot Learning with Locally-Aligned Vision-Language Model", "categories": ["cs.CV"], "comment": "Accepted to ICCV'25", "summary": "Large-scale vision-language models (VLMs), such as CLIP, have achieved\nremarkable success in zero-shot learning (ZSL) by leveraging large-scale\nvisual-text pair datasets. However, these methods often lack interpretability,\nas they compute the similarity between an entire query image and the embedded\ncategory words, making it difficult to explain their predictions. One approach\nto address this issue is to develop interpretable models by integrating\nlanguage, where classifiers are built using discrete attributes, similar to\nhuman perception. This introduces a new challenge: how to effectively align\nlocal visual features with corresponding attributes based on pre-trained VLMs.\nTo tackle this, we propose LaZSL, a locally-aligned vision-language model for\ninterpretable ZSL. LaZSL employs local visual-semantic alignment via optimal\ntransport to perform interaction between visual regions and their associated\nattributes, facilitating effective alignment and providing interpretable\nsimilarity without the need for additional training. Extensive experiments\ndemonstrate that our method offers several advantages, including enhanced\ninterpretability, improved accuracy, and strong domain generalization. Codes\navailable at: https://github.com/shiming-chen/LaZSL.", "AI": {"tldr": "LaZSL是一种基于局部对齐的视觉语言模型，通过最优传输实现视觉区域与属性的交互，提升零样本学习的可解释性和准确性。", "motivation": "大规模视觉语言模型（如CLIP）在零样本学习中表现优异，但缺乏可解释性。本文旨在通过局部视觉-语义对齐提升模型的可解释性。", "method": "提出LaZSL模型，利用最优传输技术实现局部视觉特征与属性的对齐，无需额外训练即可提供可解释的相似性。", "result": "实验表明，LaZSL在可解释性、准确性和领域泛化性方面均有显著提升。", "conclusion": "LaZSL通过局部对齐解决了视觉语言模型的可解释性问题，同时保持了高性能。"}}
{"id": "2506.23506", "pdf": "https://arxiv.org/pdf/2506.23506", "abs": "https://arxiv.org/abs/2506.23506", "authors": ["Bowen Xin", "Rohan Hickey", "Tamara Blake", "Jin Jin", "Claire E Wainwright", "Thomas Benkert", "Alto Stemmer", "Peter Sly", "David Coman", "Jason Dowling"], "title": "Artificial Intelligence-assisted Pixel-level Lung (APL) Scoring for Fast and Accurate Quantification in Ultra-short Echo-time MRI", "categories": ["eess.IV", "cs.AI", "cs.CV", "physics.med-ph"], "comment": "Oral presentation in ISMRM2025", "summary": "Lung magnetic resonance imaging (MRI) with ultrashort echo-time (UTE)\nrepresents a recent breakthrough in lung structure imaging, providing image\nresolution and quality comparable to computed tomography (CT). Due to the\nabsence of ionising radiation, MRI is often preferred over CT in paediatric\ndiseases such as cystic fibrosis (CF), one of the most common genetic disorders\nin Caucasians. To assess structural lung damage in CF imaging, CT scoring\nsystems provide valuable quantitative insights for disease diagnosis and\nprogression. However, few quantitative scoring systems are available in\nstructural lung MRI (e.g., UTE-MRI). To provide fast and accurate\nquantification in lung MRI, we investigated the feasibility of novel Artificial\nintelligence-assisted Pixel-level Lung (APL) scoring for CF. APL scoring\nconsists of 5 stages, including 1) image loading, 2) AI lung segmentation, 3)\nlung-bounded slice sampling, 4) pixel-level annotation, and 5) quantification\nand reporting. The results shows that our APL scoring took 8.2 minutes per\nsubject, which was more than twice as fast as the previous grid-level scoring.\nAdditionally, our pixel-level scoring was statistically more accurate\n(p=0.021), while strongly correlating with grid-level scoring (R=0.973,\np=5.85e-9). This tool has great potential to streamline the workflow of UTE\nlung MRI in clinical settings, and be extended to other structural lung MRI\nsequences (e.g., BLADE MRI), and for other lung diseases (e.g.,\nbronchopulmonary dysplasia).", "AI": {"tldr": "论文提出了一种基于人工智能的像素级肺部评分系统（APL），用于快速准确评估囊性纤维化（CF）患者的肺部结构损伤，相比传统网格级评分更快更准确。", "motivation": "由于MRI无电离辐射，特别适合儿科疾病如囊性纤维化的肺部成像，但目前缺乏定量评分系统。", "method": "APL评分系统包括图像加载、AI肺部分割、切片采样、像素级标注和量化报告五个步骤。", "result": "APL评分每例仅需8.2分钟，速度是传统方法的两倍，且准确性更高（p=0.021），与网格级评分强相关（R=0.973）。", "conclusion": "APL评分系统有潜力优化临床工作流程，并扩展至其他肺部疾病和MRI序列。"}}
{"id": "2506.23825", "pdf": "https://arxiv.org/pdf/2506.23825", "abs": "https://arxiv.org/abs/2506.23825", "authors": ["Haoji Zhang", "Yiqin Wang", "Yansong Tang", "Yong Liu", "Jiashi Feng", "Xiaojie Jin"], "title": "Flash-VStream: Efficient Real-Time Understanding for Long Video Streams", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Benefiting from the advances in large language models and cross-modal\nalignment, existing multimodal large language models have achieved prominent\nperformance in image and short video understanding. However, the understanding\nof long videos is still challenging, as their long-context nature results in\nsignificant computational and memory overhead. Most existing work treats long\nvideos in the same way as short videos, which is inefficient for real-world\napplications and hard to generalize to even longer videos. To address these\nissues, we propose Flash-VStream, an efficient video language model capable of\nprocessing extremely long videos and responding to user queries in real time.\nParticularly, we design a Flash Memory module, containing a low-capacity\ncontext memory to aggregate long-context temporal information and model the\ndistribution of information density, and a high-capacity augmentation memory to\nretrieve detailed spatial information based on this distribution. Compared to\nexisting models, Flash-VStream achieves significant reductions in inference\nlatency. Extensive experiments on long video benchmarks and comprehensive video\nbenchmarks, i.e., EgoSchema, MLVU, LVBench, MVBench and Video-MME, demonstrate\nthe state-of-the-art performance and outstanding efficiency of our method. Code\nis available at https://github.com/IVGSZ/Flash-VStream.", "AI": {"tldr": "Flash-VStream是一种高效的长视频理解模型，通过设计Flash Memory模块显著降低推理延迟，并在多个基准测试中表现优异。", "motivation": "现有模型处理长视频时计算和内存开销大，且难以泛化到更长视频。", "method": "提出Flash Memory模块，包含低容量上下文记忆和高容量增强记忆，分别聚合长期信息和检索详细空间信息。", "result": "在多个长视频和综合视频基准测试中达到最先进性能，显著降低推理延迟。", "conclusion": "Flash-VStream在长视频理解任务中表现出高效性和卓越性能。"}}
{"id": "2506.23827", "pdf": "https://arxiv.org/pdf/2506.23827", "abs": "https://arxiv.org/abs/2506.23827", "authors": ["Mingcheng Qu", "Yuncong Wu", "Donglin Di", "Yue Gao", "Tonghua Su", "Yang Song", "Lei Fan"], "title": "Spatially Gene Expression Prediction using Dual-Scale Contrastive Learning", "categories": ["cs.CV"], "comment": "Our paper has been accepted by MICCAI 2025", "summary": "Spatial transcriptomics (ST) provides crucial insights into tissue\nmicro-environments, but is limited to its high cost and complexity. As an\nalternative, predicting gene expression from pathology whole slide images (WSI)\nis gaining increasing attention. However, existing methods typically rely on\nsingle patches or a single pathology modality, neglecting the complex spatial\nand molecular interactions between target and neighboring information (e.g.,\ngene co-expression). This leads to a failure in establishing connections among\nadjacent regions and capturing intricate cross-modal relationships. To address\nthese issues, we propose NH2ST, a framework that integrates spatial context and\nboth pathology and gene modalities for gene expression prediction. Our model\ncomprises a query branch and a neighbor branch to process paired target patch\nand gene data and their neighboring regions, where cross-attention and\ncontrastive learning are employed to capture intrinsic associations and ensure\nalignments between pathology and gene expression. Extensive experiments on six\ndatasets demonstrate that our model consistently outperforms existing methods,\nachieving over 20% in PCC metrics. Codes are available at\nhttps://github.com/MCPathology/NH2ST", "AI": {"tldr": "NH2ST框架通过整合空间上下文和病理/基因模态，显著提升了基因表达预测性能。", "motivation": "现有方法忽视空间和分子交互，导致预测性能受限。", "method": "采用查询分支和邻居分支处理目标区域及邻近信息，结合交叉注意力和对比学习。", "result": "在六个数据集上表现优于现有方法，PCC指标提升20%以上。", "conclusion": "NH2ST有效解决了复杂交互和跨模态关系问题，为基因表达预测提供了新思路。"}}
{"id": "2506.23514", "pdf": "https://arxiv.org/pdf/2506.23514", "abs": "https://arxiv.org/abs/2506.23514", "authors": ["Sai Krishna Ghanta", "Ramviyas Parasuraman"], "title": "MGPRL: Distributed Multi-Gaussian Processes for Wi-Fi-based Multi-Robot Relative Localization in Large Indoor Environments", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": "Accepted to IROS 2025", "summary": "Relative localization is a crucial capability for multi-robot systems\noperating in GPS-denied environments. Existing approaches for multi-robot\nrelative localization often depend on costly or short-range sensors like\ncameras and LiDARs. Consequently, these approaches face challenges such as high\ncomputational overhead (e.g., map merging) and difficulties in disjoint\nenvironments. To address this limitation, this paper introduces MGPRL, a novel\ndistributed framework for multi-robot relative localization using convex-hull\nof multiple Wi-Fi access points (AP). To accomplish this, we employ\nco-regionalized multi-output Gaussian Processes for efficient Radio Signal\nStrength Indicator (RSSI) field prediction and perform uncertainty-aware\nmulti-AP localization, which is further coupled with weighted convex hull-based\nalignment for robust relative pose estimation. Each robot predicts the RSSI\nfield of the environment by an online scan of APs in its environment, which are\nutilized for position estimation of multiple APs. To perform relative\nlocalization, each robot aligns the convex hull of its predicted AP locations\nwith that of the neighbor robots. This approach is well-suited for devices with\nlimited computational resources and operates solely on widely available Wi-Fi\nRSSI measurements without necessitating any dedicated pre-calibration or\noffline fingerprinting. We rigorously evaluate the performance of the proposed\nMGPRL in ROS simulations and demonstrate it with real-world experiments,\ncomparing it against multiple state-of-the-art approaches. The results showcase\nthat MGPRL outperforms existing methods in terms of localization accuracy and\ncomputational efficiency. Finally, we open source MGPRL as a ROS package\nhttps://github.com/herolab-uga/MGPRL.", "AI": {"tldr": "MGPRL是一种基于Wi-Fi信号的多机器人相对定位框架，利用高斯过程和凸包对齐实现高效定位，优于现有方法。", "motivation": "解决GPS缺失环境下多机器人定位依赖昂贵或短程传感器的问题，降低计算开销并适应分离环境。", "method": "使用多输出高斯过程预测RSSI场，结合加权凸包对齐进行相对位姿估计，无需离线校准。", "result": "仿真和实验表明，MGPRL在定位精度和计算效率上优于现有方法。", "conclusion": "MGPRL为资源受限设备提供了一种高效、无需校准的多机器人相对定位解决方案。"}}
{"id": "2506.23832", "pdf": "https://arxiv.org/pdf/2506.23832", "abs": "https://arxiv.org/abs/2506.23832", "authors": ["Ronit D. Gross", "Tal Halevi", "Ella Koresh", "Yarden Tzach", "Ido Kanter"], "title": "Low-latency vision transformers via large-scale multi-head attention", "categories": ["cs.CV"], "comment": "23 pages, 4 figures, 7 tables", "summary": "The emergence of spontaneous symmetry breaking among a few heads of\nmulti-head attention (MHA) across transformer blocks in classification tasks\nwas recently demonstrated through the quantification of single-nodal\nperformance (SNP). This finding indicates that each head focuses its attention\non a subset of labels through cooperation among its SNPs. This underlying\nlearning mechanism is generalized to large-scale MHA (LS-MHA) using a single\nmatrix value representing single-head performance (SHP), analogous to\nsingle-filter performance in convolutional neural networks (CNNs). The results\nindicate that each SHP matrix comprises multiple unit clusters such that each\nlabel being explicitly recognized by a few heads with negligible noise. This\nleads to an increased signal-to-noise ratio (SNR) along the transformer blocks,\nthereby improving classification accuracy. These features give rise to several\ndistinct vision transformer (ViT) architectures that achieve the same accuracy\nbut differ in their LS-MHA structures. As a result, their soft committee yields\nsuperior accuracy, an outcome not typically observed in CNNs which rely on\nhundreds of filters. In addition, a significant reduction in latency is\nachieved without affecting the accuracy by replacing the initial transformer\nblocks with convolutional layers. This substitution accelerates early-stage\nlearning, which is then improved by subsequent transformer layers. The\nextension of this learning mechanism to natural language processing tasks,\nbased on quantitative differences between CNNs and ViT architectures, has the\npotential to yield new insights in deep learning. The findings are demonstrated\nusing compact convolutional transformer architectures trained on the CIFAR-100\ndataset.", "AI": {"tldr": "研究发现多头注意力（MHA）在分类任务中通过单节点性能（SNP）表现出对称性自发破缺，并推广到大尺度MHA（LS-MHA），通过单头性能（SHP）矩阵提高信噪比（SNR），提升分类精度。", "motivation": "探索多头注意力机制在分类任务中的学习机制，并推广到更大规模的应用中。", "method": "使用单头性能（SHP）矩阵量化多头注意力的表现，分析其结构和性能。", "result": "发现SHP矩阵由多个单元簇组成，显著提高信噪比和分类精度，并设计出高效的视觉Transformer架构。", "conclusion": "该机制可扩展至自然语言处理任务，为深度学习提供新见解，并在CIFAR-100数据集上验证了其有效性。"}}
{"id": "2506.23516", "pdf": "https://arxiv.org/pdf/2506.23516", "abs": "https://arxiv.org/abs/2506.23516", "authors": ["Seung-Wook Kim", "Seongyeol Kim", "Jiah Kim", "Seowon Ji", "Se-Ho Lee"], "title": "FedWSQ: Efficient Federated Learning with Weight Standardization and Distribution-Aware Non-Uniform Quantization", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Federated learning (FL) often suffers from performance degradation due to key\nchallenges such as data heterogeneity and communication constraints. To address\nthese limitations, we present a novel FL framework called FedWSQ, which\nintegrates weight standardization (WS) and the proposed distribution-aware\nnon-uniform quantization (DANUQ). WS enhances FL performance by filtering out\nbiased components in local updates during training, thereby improving the\nrobustness of the model against data heterogeneity and unstable client\nparticipation. In addition, DANUQ minimizes quantization errors by leveraging\nthe statistical properties of local model updates. As a result, FedWSQ\nsignificantly reduces communication overhead while maintaining superior model\naccuracy. Extensive experiments on FL benchmark datasets demonstrate that\nFedWSQ consistently outperforms existing FL methods across various challenging\nFL settings, including extreme data heterogeneity and ultra-low-bit\ncommunication scenarios.", "AI": {"tldr": "FedWSQ框架通过权重标准化和非均匀量化提升联邦学习性能，减少通信开销并保持高精度。", "motivation": "解决联邦学习中数据异构和通信限制导致的性能下降问题。", "method": "结合权重标准化（WS）和分布感知非均匀量化（DANUQ），过滤本地更新的偏差并最小化量化误差。", "result": "在多种挑战性联邦学习场景下，FedWSQ显著优于现有方法。", "conclusion": "FedWSQ有效提升了联邦学习的鲁棒性和通信效率。"}}
{"id": "2506.23833", "pdf": "https://arxiv.org/pdf/2506.23833", "abs": "https://arxiv.org/abs/2506.23833", "authors": ["Oscar Ovanger", "Ragnar Hauge", "Jacob Skauvold", "Michael J. Pyrcz", "Jo Eidsvik"], "title": "PointSSIM: A novel low dimensional resolution invariant image-to-image comparison metric", "categories": ["cs.CV"], "comment": "13 pages, 20 figures", "summary": "This paper presents PointSSIM, a novel low-dimensional image-to-image\ncomparison metric that is resolution invariant. Drawing inspiration from the\nstructural similarity index measure and mathematical morphology, PointSSIM\nenables robust comparison across binary images of varying resolutions by\ntransforming them into marked point pattern representations. The key features\nof the image, referred to as anchor points, are extracted from binary images by\nidentifying locally adaptive maxima from the minimal distance transform. Image\ncomparisons are then performed using a summary vector, capturing intensity,\nconnectivity, complexity, and structural attributes. Results show that this\napproach provides an efficient and reliable method for image comparison,\nparticularly suited to applications requiring structural analysis across\ndifferent resolutions.", "AI": {"tldr": "PointSSIM是一种新颖的低维图像比较指标，具有分辨率不变性，适用于不同分辨率的二值图像比较。", "motivation": "现有图像比较方法在处理不同分辨率的二值图像时存在局限性，需要一种更鲁棒的方法。", "method": "通过将二值图像转换为标记点模式表示，提取关键特征（锚点），并使用总结向量比较图像的强度、连通性、复杂性和结构属性。", "result": "PointSSIM提供了一种高效可靠的图像比较方法，特别适用于跨分辨率的结构分析。", "conclusion": "PointSSIM是一种有效的低维图像比较指标，适用于多分辨率场景。"}}
{"id": "2506.23835", "pdf": "https://arxiv.org/pdf/2506.23835", "abs": "https://arxiv.org/abs/2506.23835", "authors": ["Ziwei Chen", "Ziling Liu", "Zitong Huang", "Mingqi Gao", "Feng Zheng"], "title": "Refine Any Object in Any Scene", "categories": ["cs.CV"], "comment": "9 pages with 6 figures", "summary": "Viewpoint missing of objects is common in scene reconstruction, as camera\npaths typically prioritize capturing the overall scene structure rather than\nindividual objects. This makes it highly challenging to achieve high-fidelity\nobject-level modeling while maintaining accurate scene-level representation.\nAddressing this issue is critical for advancing downstream tasks requiring\ndetailed object understanding and appearance modeling. In this paper, we\nintroduce Refine Any object In any ScenE (RAISE), a novel 3D enhancement\nframework that leverages 3D generative priors to recover fine-grained object\ngeometry and appearance under missing views. Starting from substituting\ndegraded objects with proxies, via a 3D generative model with strong 3D\nunderstanding, RAISE progressively refines geometry and texture by aligning\neach proxy to its degraded counterpart in 7-DOF pose, followed by correcting\nspatial and appearance inconsistencies via registration-constrained\nenhancement. This two-stage refinement ensures the high-fidelity geometry and\nappearance of the original object in unseen views while maintaining consistency\nin spatial positioning, observed geometry, and appearance. Extensive\nexperiments on challenging benchmarks show that RAISE significantly outperforms\nstate-of-the-art methods in both novel view synthesis and geometry completion\ntasks. RAISE is made publicly available at https://github.com/PolySummit/RAISE.", "AI": {"tldr": "RAISE是一个3D增强框架，利用3D生成先验恢复缺失视角下的物体几何和外观，显著优于现有方法。", "motivation": "解决场景重建中因视角缺失导致的高保真物体建模难题，推动需要详细物体理解的下游任务。", "method": "通过两阶段细化：先用3D生成模型替换退化物体，再通过7-DOF对齐和注册约束增强修正几何与纹理。", "result": "在多个基准测试中，RAISE在新视角合成和几何补全任务上显著优于现有方法。", "conclusion": "RAISE能有效恢复缺失视角下的物体细节，同时保持场景一致性，具有广泛应用潜力。"}}
{"id": "2506.23852", "pdf": "https://arxiv.org/pdf/2506.23852", "abs": "https://arxiv.org/abs/2506.23852", "authors": ["Jianing Jin", "Jiangyong Ying", "Huiyu Duan", "Liu Yang", "Sijing Wu", "Yunhao Li", "Yushuo Zheng", "Xiongkuo Min", "Guangtao Zhai"], "title": "RGC-VQA: An Exploration Database for Robotic-Generated Video Quality Assessment", "categories": ["cs.CV"], "comment": null, "summary": "As camera-equipped robotic platforms become increasingly integrated into\ndaily life, robotic-generated videos have begun to appear on streaming media\nplatforms, enabling us to envision a future where humans and robots coexist. We\ninnovatively propose the concept of Robotic-Generated Content (RGC) to term\nthese videos generated from egocentric perspective of robots. The perceptual\nquality of RGC videos is critical in human-robot interaction scenarios, and RGC\nvideos exhibit unique distortions and visual requirements that differ markedly\nfrom those of professionally-generated content (PGC) videos and user-generated\ncontent (UGC) videos. However, dedicated research on quality assessment of RGC\nvideos is still lacking. To address this gap and to support broader robotic\napplications, we establish the first Robotic-Generated Content Database (RGCD),\nwhich contains a total of 2,100 videos drawn from three robot categories and\nsourced from diverse platforms. A subjective VQA experiment is conducted\nsubsequently to assess human visual perception of robotic-generated videos.\nFinally, we conduct a benchmark experiment to evaluate the performance of 11\nstate-of-the-art VQA models on our database. Experimental results reveal\nsignificant limitations in existing VQA models when applied to complex,\nrobotic-generated content, highlighting a critical need for RGC-specific VQA\nmodels. Our RGCD is publicly available at:\nhttps://github.com/IntMeGroup/RGC-VQA.", "AI": {"tldr": "论文提出了机器人生成内容（RGC）的概念，并建立了首个RGC视频数据库（RGCD），通过主观实验和基准测试揭示了现有VQA模型在RGC视频质量评估中的不足。", "motivation": "随着机器人平台的普及，RGC视频的质量评估成为人机交互中的关键问题，但目前缺乏专门研究。", "method": "建立了包含2,100个视频的RGCD数据库，进行主观VQA实验，并评估了11种现有VQA模型的性能。", "result": "实验结果表明，现有VQA模型在复杂RGC视频上的表现存在显著局限性。", "conclusion": "需要开发专门针对RGC的VQA模型，RGCD数据库为未来研究提供了基础。"}}
{"id": "2506.23560", "pdf": "https://arxiv.org/pdf/2506.23560", "abs": "https://arxiv.org/abs/2506.23560", "authors": ["Shakir Showkat Sofi", "Charlotte Vermeylen", "Lieven De Lathauwer"], "title": "Tensor Train Quantum State Tomography using Compressed Sensing", "categories": ["quant-ph", "cs.AI", "eess.SP", "math.OC"], "comment": "Accepted for publication in EUSIPCO 2025", "summary": "Quantum state tomography (QST) is a fundamental technique for estimating the\nstate of a quantum system from measured data and plays a crucial role in\nevaluating the performance of quantum devices. However, standard estimation\nmethods become impractical due to the exponential growth of parameters in the\nstate representation. In this work, we address this challenge by parameterizing\nthe state using a low-rank block tensor train decomposition and demonstrate\nthat our approach is both memory- and computationally efficient. This framework\napplies to a broad class of quantum states that can be well approximated by\nlow-rank decompositions, including pure states, nearly pure states, and ground\nstates of Hamiltonians.", "AI": {"tldr": "论文提出了一种基于低秩块张量链分解的量子态层析方法，解决了传统方法因参数指数增长导致的不实用问题。", "motivation": "量子态层析是评估量子设备性能的关键技术，但传统方法因参数指数增长而变得不实用。", "method": "采用低秩块张量链分解参数化量子态，实现内存和计算效率的提升。", "result": "该方法适用于低秩分解近似良好的量子态，包括纯态、近纯态和哈密顿量的基态。", "conclusion": "提出的框架在内存和计算效率上均表现出色，适用于广泛的量子态类别。"}}
{"id": "2506.23854", "pdf": "https://arxiv.org/pdf/2506.23854", "abs": "https://arxiv.org/abs/2506.23854", "authors": ["Yida Wang", "Xueyang Zhang", "Kun Zhan", "Peng Jia", "Xianpeng Lang"], "title": "HiNeuS: High-fidelity Neural Surface Mitigating Low-texture and Reflective Ambiguity", "categories": ["cs.CV", "cs.GR"], "comment": "Published in International Conference on Computer Vision (ICCV) 2025", "summary": "Neural surface reconstruction faces persistent challenges in reconciling\ngeometric fidelity with photometric consistency under complex scene conditions.\nWe present HiNeuS, a unified framework that holistically addresses three core\nlimitations in existing approaches: multi-view radiance inconsistency, missing\nkeypoints in textureless regions, and structural degradation from over-enforced\nEikonal constraints during joint optimization. To resolve these issues through\na unified pipeline, we introduce: 1) Differential visibility verification\nthrough SDF-guided ray tracing, resolving reflection ambiguities via continuous\nocclusion modeling; 2) Planar-conformal regularization via ray-aligned geometry\npatches that enforce local surface coherence while preserving sharp edges\nthrough adaptive appearance weighting; and 3) Physically-grounded Eikonal\nrelaxation that dynamically modulates geometric constraints based on local\nradiance gradients, enabling detail preservation without sacrificing global\nregularity. Unlike prior methods that handle these aspects through sequential\noptimizations or isolated modules, our approach achieves cohesive integration\nwhere appearance-geometry constraints evolve synergistically throughout\ntraining. Comprehensive evaluations across synthetic and real-world datasets\ndemonstrate state-of-the-art performance, including a 21.4% reduction in\nChamfer distance over reflection-aware baselines and 2.32 dB PSNR improvement\nagainst neural rendering counterparts. Qualitative analyses reveal superior\ncapability in recovering specular instruments, urban layouts with\ncentimeter-scale infrastructure, and low-textured surfaces without local patch\ncollapse. The method's generalizability is further validated through successful\napplication to inverse rendering tasks, including material decomposition and\nview-consistent relighting.", "AI": {"tldr": "HiNeuS是一个统一的神经表面重建框架，通过解决多视角辐射不一致、无纹理区域关键点缺失和Eikonal约束过度导致的几何退化问题，实现了几何保真度和光度一致性的平衡。", "motivation": "现有方法在复杂场景下难以同时满足几何保真度和光度一致性，HiNeuS旨在通过统一框架解决多视角辐射不一致、无纹理区域关键点缺失和Eikonal约束过度的问题。", "method": "HiNeuS引入差分可见性验证、平面共形正则化和物理基础的Eikonal松弛，通过统一管道协同优化外观和几何约束。", "result": "实验表明，HiNeuS在合成和真实数据集上表现优异，Chamfer距离减少21.4%，PSNR提升2.32 dB，并能恢复高光物体和低纹理表面。", "conclusion": "HiNeuS通过协同优化外观和几何约束，实现了高性能的表面重建，并展示了在逆渲染任务中的泛化能力。"}}
{"id": "2506.23573", "pdf": "https://arxiv.org/pdf/2506.23573", "abs": "https://arxiv.org/abs/2506.23573", "authors": ["Siddhartha Mondal", "Avik Mitra", "Chayan Sarkar"], "title": "Online Human Action Detection during Escorting", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": "Accepted in IEEE RO-MAN '25", "summary": "The deployment of robot assistants in large indoor spaces has seen\nsignificant growth, with escorting tasks becoming a key application. However,\nmost current escorting robots primarily rely on navigation-focused strategies,\nassuming that the person being escorted will follow without issue. In crowded\nenvironments, this assumption often falls short, as individuals may struggle to\nkeep pace, become obstructed, get distracted, or need to stop unexpectedly. As\na result, conventional robotic systems are often unable to provide effective\nescorting services due to their limited understanding of human movement\ndynamics. To address these challenges, an effective escorting robot must\ncontinuously detect and interpret human actions during the escorting process\nand adjust its movement accordingly. However, there is currently no existing\ndataset designed specifically for human action detection in the context of\nescorting. Given that escorting often occurs in crowded environments, where\nother individuals may enter the robot's camera view, the robot also needs to\nidentify the specific human it is escorting (the subject) before predicting\ntheir actions. Since no existing model performs both person re-identification\nand action prediction in real-time, we propose a novel neural network\narchitecture that can accomplish both tasks. This enables the robot to adjust\nits speed dynamically based on the escortee's movements and seamlessly resume\nescorting after any disruption. In comparative evaluations against strong\nbaselines, our system demonstrates superior efficiency and effectiveness,\nshowcasing its potential to significantly improve robotic escorting services in\ncomplex, real-world scenarios.", "AI": {"tldr": "论文提出了一种新型神经网络架构，用于实时行人重识别和动作预测，以提升拥挤环境中机器人护送服务的效率。", "motivation": "当前护送机器人主要依赖导航策略，假设被护送者会顺利跟随，但在拥挤环境中这一假设常不成立，导致服务效果不佳。", "method": "提出了一种能够同时完成行人重识别和动作预测的神经网络架构，使机器人能动态调整速度并应对中断。", "result": "在对比评估中，该系统表现出更高的效率和有效性。", "conclusion": "该研究显著提升了复杂现实场景中机器人护送服务的性能。"}}
{"id": "2506.23856", "pdf": "https://arxiv.org/pdf/2506.23856", "abs": "https://arxiv.org/abs/2506.23856", "authors": ["Ji Zhang", "Shihan Wu", "Lianli Gao", "Jingkuan Song", "Nicu Sebe", "Heng Tao Shen"], "title": "A Closer Look at Conditional Prompt Tuning for Vision-Language Models", "categories": ["cs.CV"], "comment": "18 pages", "summary": "Despite the great promise of Prompt Tuning (PT) in adapting large\nVision-Language Pretrained Models (VLPMs) to downstream tasks, they often\nstruggle to overcome the Base-New Tradeoff (BNT) dilemma: as VLPMs are better\ntuned to a base task, their ability to generalize to new tasks diminishes.\nRecent work on conditional PT addresses this problem by replacing static\nprompts with dynamic Visual Image Information (VII)-conditioned prompts,\nimproving the model's generalization to new tasks to some extent. In this work,\nwe first identify a critical issue with existing conditional PT methods: using\nVII as the \"condition\" of prompts yields suboptimal performance, and even\nrandom noise-conditioned prompts can outperform the VII-conditioned\ncounterparts. On further analysis, we find that learning dynamic prompts\nconditioned on Textual Class Information (TCI) is the key to solving the BNT\nproblem. Motivated by this, we then propose Class-adaptive Prompt Tuning\n(CaPT), which enables fast adaptation of tuned models to new classes by\nlearning TCI-conditioned prompts from base classes. Remarkably, CaPT can be\nused as a plugin to mitigate the BNT problem for existing unconditional PT\nschemes. Extensive experiments on 11 datasets show that CaPT consistently\nimproves the performance of five strong unconditional PT baselines with\nnegligible additional computational cost. Additionally, by integrating CaPT\nwith our recently proposed DePT framework, we devise a new conditional PT\napproach, termed DeCaPT, which outperforms the H ACC of the state-of-the-art\nconditional PT scheme by 3.49%, averaged over the 11 datasets. Code:\nhttps://github.com/Koorye/CaPT.", "AI": {"tldr": "论文提出了一种名为CaPT的类自适应提示调优方法，通过基于文本类别信息（TCI）的动态提示解决基础-新任务权衡（BNT）问题，并进一步结合DePT框架提出DeCaPT，显著提升了性能。", "motivation": "现有基于视觉图像信息（VII）的条件提示调优方法在解决BNT问题时表现不佳，甚至不如随机噪声条件提示，因此需要探索更有效的条件信息。", "method": "提出Class-adaptive Prompt Tuning (CaPT)，通过学习基于文本类别信息（TCI）的动态提示，快速适应新类别，并可作为插件提升现有无条件提示调优方法。", "result": "在11个数据集上的实验表明，CaPT显著提升了五种无条件提示调优基线的性能，且计算成本几乎不增加。结合DePT框架的DeCaPT方法在性能上超越了现有最佳条件提示调优方案3.49%。", "conclusion": "CaPT和DeCaPT通过利用TCI解决了BNT问题，显著提升了模型在新任务上的泛化能力，且计算成本低，具有广泛应用潜力。"}}
{"id": "2506.23858", "pdf": "https://arxiv.org/pdf/2506.23858", "abs": "https://arxiv.org/abs/2506.23858", "authors": ["Jianzong Wu", "Liang Hou", "Haotian Yang", "Xin Tao", "Ye Tian", "Pengfei Wan", "Di Zhang", "Yunhai Tong"], "title": "VMoBA: Mixture-of-Block Attention for Video Diffusion Models", "categories": ["cs.CV"], "comment": "Code is at https://github.com/KwaiVGI/VMoBA", "summary": "The quadratic complexity of full attention mechanisms poses a significant\nbottleneck for Video Diffusion Models (VDMs) aiming to generate long-duration,\nhigh-resolution videos. While various sparse attention methods have been\nproposed, many are designed as training-free inference accelerators or do not\noptimally capture the unique spatio-temporal characteristics inherent in video\ndata when trained natively. This paper introduces Video Mixture of Block\nAttention (VMoBA), a novel sparse attention mechanism specifically adapted for\nVDMs. Motivated by an in-depth analysis of attention patterns within\npre-trained video transformers, which revealed strong spatio-temporal locality,\nvarying query importance, and head-specific concentration levels, VMoBA\nenhances the original MoBA framework with three key modifications: (1) a\nlayer-wise recurrent block partition scheme (1D-2D-3D) to dynamically adapt to\ndiverse spatio-temporal attention patterns and improve efficiency; (2) global\nblock selection to prioritize the most salient query-key block interactions\nacross an entire attention head; and (3) threshold-based block selection to\ndynamically determine the number of attended blocks based on their cumulative\nsimilarity. Extensive experiments demonstrate that VMoBA significantly\naccelerates the training of VDMs on longer sequences, achieving 2.92x FLOPs and\n1.48x latency speedup, while attaining comparable or even superior generation\nquality to full attention. Furthermore, VMoBA exhibits competitive performance\nin training-free inference, offering 2.40x FLOPs and 1.35x latency speedup for\nhigh-res video generation.", "AI": {"tldr": "论文提出了一种名为VMoBA的新型稀疏注意力机制，用于解决视频扩散模型中全注意力机制的二次复杂度问题，显著提升了训练和推理效率。", "motivation": "全注意力机制在视频扩散模型中的高复杂度限制了长时长、高分辨率视频的生成效率，现有稀疏注意力方法未能充分捕捉视频数据的时空特性。", "method": "VMoBA通过层级的块划分方案（1D-2D-3D）、全局块选择和基于阈值的块选择，优化了注意力机制，提升了效率。", "result": "实验表明，VMoBA在训练和推理中分别实现了2.92x和2.40x的FLOPs加速，生成质量与全注意力相当或更优。", "conclusion": "VMoBA是一种高效且性能优越的稀疏注意力机制，适用于视频扩散模型。"}}
{"id": "2506.23584", "pdf": "https://arxiv.org/pdf/2506.23584", "abs": "https://arxiv.org/abs/2506.23584", "authors": ["Renjie Liang", "Zhengkang Fan", "Jinqian Pan", "Chenkun Sun", "Russell Terry", "Jie Xu"], "title": "A Clinically-Grounded Two-Stage Framework for Renal CT Report Generation", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Generating radiology reports from CT scans remains a complex task due to the\nnuanced nature of medical imaging and the variability in clinical\ndocumentation. In this study, we propose a two-stage framework for generating\nrenal radiology reports from 2D CT slices. First, we extract structured\nabnormality features using a multi-task learning model trained to identify\nlesion attributes such as location, size, enhancement, and attenuation. These\nextracted features are subsequently combined with the corresponding CT image\nand fed into a fine-tuned vision-language model to generate natural language\nreport sentences aligned with clinical findings. We conduct experiments on a\ncurated dataset of renal CT studies with manually annotated\nsentence-slice-feature triplets and evaluate performance using both\nclassification metrics and natural language generation metrics. Our results\ndemonstrate that the proposed model outperforms random baselines across all\nabnormality types, and the generated reports capture key clinical content with\nreasonable textual accuracy. This exploratory work highlights the feasibility\nof modular, feature-informed report generation for renal imaging. Future\nefforts will focus on extending this pipeline to 3D CT volumes and further\nimproving clinical fidelity in multimodal medical AI systems.", "AI": {"tldr": "提出了一种两阶段框架，用于从2D CT切片生成肾脏放射学报告，结合多任务学习模型提取异常特征，并通过微调的视觉语言模型生成自然语言报告。", "motivation": "解决医学影像的复杂性和临床文档的变异性，提高放射学报告的生成质量。", "method": "1. 使用多任务学习模型提取结构化异常特征（如位置、大小、增强和衰减）；2. 将特征与CT图像结合，输入微调的视觉语言模型生成报告。", "result": "模型在所有异常类型上均优于随机基线，生成的报告能合理捕捉关键临床内容。", "conclusion": "模块化、基于特征的报告生成在肾脏影像中具有可行性，未来将扩展至3D CT体积并提升临床保真度。"}}
{"id": "2506.23863", "pdf": "https://arxiv.org/pdf/2506.23863", "abs": "https://arxiv.org/abs/2506.23863", "authors": ["Jiahao Ma", "Lei Wang", "Miaomiao liu", "David Ahmedt-Aristizabal", "Chuong Nguyen"], "title": "Puzzles: Unbounded Video-Depth Augmentation for Scalable End-to-End 3D Reconstruction", "categories": ["cs.CV"], "comment": "Feed-forward 3D reconstruction, Data Augmentation", "summary": "Multi-view 3D reconstruction remains a core challenge in computer vision.\nRecent methods, such as DUST3R and its successors, directly regress pointmaps\nfrom image pairs without relying on known scene geometry or camera parameters.\nHowever, the performance of these models is constrained by the diversity and\nscale of available training data. In this work, we introduce Puzzles, a data\naugmentation strategy that synthesizes an unbounded volume of high-quality\nposed video-depth data from a single image or video clip. By simulating diverse\ncamera trajectories and realistic scene geometry through targeted image\ntransformations, Puzzles significantly enhances data variety. Extensive\nexperiments show that integrating Puzzles into existing video-based 3D\nreconstruction pipelines consistently boosts performance without modifying the\nunderlying network architecture. Notably, models trained on only ten percent of\nthe original data augmented with Puzzles still achieve accuracy comparable to\nthose trained on the full dataset. Code is available at\nhttps://jiahao-ma.github.io/puzzles/.", "AI": {"tldr": "论文提出了一种名为Puzzles的数据增强策略，通过单张图像或视频片段合成高质量的视频-深度数据，显著提升了3D重建模型的性能。", "motivation": "多视图3D重建是计算机视觉的核心挑战，现有方法受限于训练数据的多样性和规模。", "method": "引入Puzzles策略，通过模拟多样化的相机轨迹和场景几何，从单张图像或视频片段生成大量高质量数据。", "result": "实验表明，Puzzles显著提升了性能，仅用10%的原始数据即可达到与完整数据集相当的精度。", "conclusion": "Puzzles是一种高效的数据增强方法，无需修改网络架构即可提升3D重建模型的性能。"}}
{"id": "2506.23589", "pdf": "https://arxiv.org/pdf/2506.23589", "abs": "https://arxiv.org/abs/2506.23589", "authors": ["Neta Shaul", "Uriel Singer", "Itai Gat", "Yaron Lipman"], "title": "Transition Matching: Scalable and Flexible Generative Modeling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Diffusion and flow matching models have significantly advanced media\ngeneration, yet their design space is well-explored, somewhat limiting further\nimprovements. Concurrently, autoregressive (AR) models, particularly those\ngenerating continuous tokens, have emerged as a promising direction for\nunifying text and media generation. This paper introduces Transition Matching\n(TM), a novel discrete-time, continuous-state generative paradigm that unifies\nand advances both diffusion/flow models and continuous AR generation. TM\ndecomposes complex generation tasks into simpler Markov transitions, allowing\nfor expressive non-deterministic probability transition kernels and arbitrary\nnon-continuous supervision processes, thereby unlocking new flexible design\navenues. We explore these choices through three TM variants: (i) Difference\nTransition Matching (DTM), which generalizes flow matching to discrete-time by\ndirectly learning transition probabilities, yielding state-of-the-art image\nquality and text adherence as well as improved sampling efficiency. (ii)\nAutoregressive Transition Matching (ARTM) and (iii) Full History Transition\nMatching (FHTM) are partially and fully causal models, respectively, that\ngeneralize continuous AR methods. They achieve continuous causal AR generation\nquality comparable to non-causal approaches and potentially enable seamless\nintegration with existing AR text generation techniques. Notably, FHTM is the\nfirst fully causal model to match or surpass the performance of flow-based\nmethods on text-to-image task in continuous domains. We demonstrate these\ncontributions through a rigorous large-scale comparison of TM variants and\nrelevant baselines, maintaining a fixed architecture, training data, and\nhyperparameters.", "AI": {"tldr": "论文提出了一种新的生成范式Transition Matching (TM)，统一并改进了扩散/流模型和连续自回归生成方法，通过三种变体展示了其优越性能。", "motivation": "扩散和流匹配模型的设计空间已较为成熟，限制了进一步改进；同时连续自回归模型在统一文本和媒体生成方面表现出潜力，但仍有提升空间。", "method": "TM将复杂生成任务分解为简单的马尔可夫转移，支持非确定性概率转移核和任意非连续监督过程。提出了三种变体：DTM（直接学习转移概率）、ARTM（部分因果模型）和FHTM（完全因果模型）。", "result": "DTM在图像质量和文本一致性上达到SOTA，采样效率提升；ARTM和FHTM在连续因果自回归生成中表现优异，FHTM首次在文本到图像任务中超越流方法。", "conclusion": "TM为生成任务提供了灵活的设计空间，性能优于现有方法，尤其在连续因果生成方面表现出色。"}}
{"id": "2506.23881", "pdf": "https://arxiv.org/pdf/2506.23881", "abs": "https://arxiv.org/abs/2506.23881", "authors": ["Reihaneh Zohrabi", "Hosein Hasani", "Mahdieh Soleymani Baghshah", "Anna Rohrbach", "Marcus Rohrbach", "Mohammad Hossein Rohban"], "title": "Spurious-Aware Prototype Refinement for Reliable Out-of-Distribution Detection", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Out-of-distribution (OOD) detection is crucial for ensuring the reliability\nand safety of machine learning models in real-world applications, where they\nfrequently face data distributions unseen during training. Despite progress,\nexisting methods are often vulnerable to spurious correlations that mislead\nmodels and compromise robustness. To address this, we propose SPROD, a novel\nprototype-based OOD detection approach that explicitly addresses the challenge\nposed by unknown spurious correlations. Our post-hoc method refines class\nprototypes to mitigate bias from spurious features without additional data or\nhyperparameter tuning, and is broadly applicable across diverse backbones and\nOOD detection settings. We conduct a comprehensive spurious correlation OOD\ndetection benchmarking, comparing our method against existing approaches and\ndemonstrating its superior performance across challenging OOD datasets, such as\nCelebA, Waterbirds, UrbanCars, Spurious Imagenet, and the newly introduced\nAnimals MetaCoCo. On average, SPROD improves AUROC by 4.7% and FPR@95 by 9.3%\nover the second best.", "AI": {"tldr": "SPROD是一种新型的基于原型的OOD检测方法，通过优化类别原型减少虚假相关性带来的偏差，无需额外数据或调参，显著提升性能。", "motivation": "现有OOD检测方法易受虚假相关性影响，导致模型鲁棒性不足，需要一种更有效的方法来解决这一问题。", "method": "提出SPROD方法，通过后处理优化类别原型，减少虚假特征带来的偏差，适用于多种模型和OOD检测场景。", "result": "在多个挑战性OOD数据集上，SPROD平均AUROC提升4.7%，FPR@95降低9.3%，优于现有方法。", "conclusion": "SPROD是一种高效、通用的OOD检测方法，显著提升了模型在未知数据分布下的鲁棒性。"}}
{"id": "2506.23596", "pdf": "https://arxiv.org/pdf/2506.23596", "abs": "https://arxiv.org/abs/2506.23596", "authors": ["Min-Yeong Park", "Won-Jeong Lee", "Seong Tae Kim", "Gyeong-Moon Park"], "title": "When Will It Fail?: Anomaly to Prompt for Forecasting Future Anomalies in Time Series", "categories": ["cs.LG", "cs.AI"], "comment": "18 pages, 10 figures, 12 tables, ICML 2025", "summary": "Recently, forecasting future abnormal events has emerged as an important\nscenario to tackle real-world necessities. However, the solution of predicting\nspecific future time points when anomalies will occur, known as Anomaly\nPrediction (AP), remains under-explored. Existing methods dealing with time\nseries data fail in AP, focusing only on immediate anomalies or failing to\nprovide precise predictions for future anomalies. To address the AP task, we\npropose a novel framework called Anomaly to Prompt (A2P), comprised of\nAnomaly-Aware Forecasting (AAF) and Synthetic Anomaly Prompting (SAP). To\nenable the forecasting model to forecast abnormal time points, we adopt a\nstrategy to learn the relationships of anomalies. For the robust detection of\nanomalies, our proposed SAP introduces a learnable Anomaly Prompt Pool (APP)\nthat simulates diverse anomaly patterns using signal adaptive prompt.\nComprehensive experiments on multiple real-world datasets demonstrate the\nsuperiority of A2P over state-of-the-art methods, showcasing its ability to\npredict future anomalies. Our implementation code is available at\nhttps://github.com/KU-VGI/AP.", "AI": {"tldr": "论文提出了一种名为A2P的新框架，用于预测未来异常事件，包含异常感知预测（AAF）和合成异常提示（SAP），在多个真实数据集上表现优于现有方法。", "motivation": "现有方法无法准确预测未来异常事件的时间点，A2P框架旨在解决这一问题。", "method": "A2P框架包括AAF和SAP两部分，AAF学习异常关系，SAP通过可学习的异常提示池（APP）模拟多样异常模式。", "result": "在多个真实数据集上，A2P表现优于现有方法，能有效预测未来异常。", "conclusion": "A2P框架为异常预测任务提供了有效解决方案，代码已开源。"}}
{"id": "2506.23897", "pdf": "https://arxiv.org/pdf/2506.23897", "abs": "https://arxiv.org/abs/2506.23897", "authors": ["Longliang Liu", "Miaojie Feng", "Junda Cheng", "Jijun Xiang", "Xuan Zhu", "Xin Yang"], "title": "PriOr-Flow: Enhancing Primitive Panoramic Optical Flow with Orthogonal View", "categories": ["cs.CV"], "comment": "11 pages", "summary": "Panoramic optical flow enables a comprehensive understanding of temporal\ndynamics across wide fields of view. However, severe distortions caused by\nsphere-to-plane projections, such as the equirectangular projection (ERP),\nsignificantly degrade the performance of conventional perspective-based optical\nflow methods, especially in polar regions. To address this challenge, we\npropose PriOr-Flow, a novel dual-branch framework that leverages the\nlow-distortion nature of the orthogonal view to enhance optical flow estimation\nin these regions. Specifically, we introduce the Dual-Cost Collaborative Lookup\n(DCCL) operator, which jointly retrieves correlation information from both the\nprimitive and orthogonal cost volumes, effectively mitigating distortion noise\nduring cost volume construction. Furthermore, our Ortho-Driven Distortion\nCompensation (ODDC) module iteratively refines motion features from both\nbranches, further suppressing polar distortions. Extensive experiments\ndemonstrate that PriOr-Flow is compatible with various perspective-based\niterative optical flow methods and consistently achieves state-of-the-art\nperformance on publicly available panoramic optical flow datasets, setting a\nnew benchmark for wide-field motion estimation. The code is publicly available\nat: https://github.com/longliangLiu/PriOr-Flow.", "AI": {"tldr": "PriOr-Flow提出了一种双分支框架，通过正交视图的低失真特性提升全景光流估计性能，特别是在极地区域。", "motivation": "传统基于透视的光流方法在全景投影（如ERP）中因严重失真而性能下降，尤其是在极地区域。", "method": "PriOr-Flow采用双分支框架，引入DCCL操作符联合检索原始和正交成本体积信息，并通过ODDC模块迭代优化运动特征。", "result": "实验表明，PriOr-Flow兼容多种迭代光流方法，并在公开全景光流数据集上达到最优性能。", "conclusion": "PriOr-Flow为宽视场运动估计设定了新基准，代码已开源。"}}
{"id": "2506.23903", "pdf": "https://arxiv.org/pdf/2506.23903", "abs": "https://arxiv.org/abs/2506.23903", "authors": ["Hamza Rasaee", "Taha Koleilat", "Hassan Rivaz"], "title": "GroundingDINO-US-SAM: Text-Prompted Multi-Organ Segmentation in Ultrasound with LoRA-Tuned Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": "11 pages, 3 figures, 6 figures", "summary": "Accurate and generalizable object segmentation in ultrasound imaging remains\na significant challenge due to anatomical variability, diverse imaging\nprotocols, and limited annotated data. In this study, we propose a\nprompt-driven vision-language model (VLM) that integrates Grounding DINO with\nSAM2 to enable object segmentation across multiple ultrasound organs. A total\nof 18 public ultrasound datasets, encompassing the breast, thyroid, liver,\nprostate, kidney, and paraspinal muscle, were utilized. These datasets were\ndivided into 15 for fine-tuning and validation of Grounding DINO using Low Rank\nAdaptation (LoRA) to the ultrasound domain, and 3 were held out entirely for\ntesting to evaluate performance in unseen distributions. Comprehensive\nexperiments demonstrate that our approach outperforms state-of-the-art\nsegmentation methods, including UniverSeg, MedSAM, MedCLIP-SAM, BiomedParse,\nand SAMUS on most seen datasets while maintaining strong performance on unseen\ndatasets without additional fine-tuning. These results underscore the promise\nof VLMs in scalable and robust ultrasound image analysis, reducing dependence\non large, organ-specific annotated datasets. We will publish our code on\ncode.sonography.ai after acceptance.", "AI": {"tldr": "提出了一种基于提示驱动的视觉语言模型（VLM），结合Grounding DINO和SAM2，用于多器官超声图像分割，性能优于现有方法。", "motivation": "超声图像分割因解剖变异、成像协议多样和标注数据有限而具有挑战性。", "method": "利用18个公共超声数据集，15个用于微调Grounding DINO（使用LoRA），3个用于测试未见分布的性能。", "result": "在多数数据集上优于UniverSeg、MedSAM等方法，未见数据集上表现稳健。", "conclusion": "VLM在超声图像分析中具有潜力，减少对大规模标注数据的依赖。"}}
{"id": "2506.23603", "pdf": "https://arxiv.org/pdf/2506.23603", "abs": "https://arxiv.org/abs/2506.23603", "authors": ["Baihe Ma", "Yanna Jiang", "Xu Wang", "Guangshen Yu", "Qin Wang", "Caijun Sun", "Chen Li", "Xuelei Qi", "Ying He", "Wei Ni", "Ren Ping Liu"], "title": "SoK: Semantic Privacy in Large Language Models", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "As Large Language Models (LLMs) are increasingly deployed in sensitive\ndomains, traditional data privacy measures prove inadequate for protecting\ninformation that is implicit, contextual, or inferable - what we define as\nsemantic privacy. This Systematization of Knowledge (SoK) introduces a\nlifecycle-centric framework to analyze how semantic privacy risks emerge across\ninput processing, pretraining, fine-tuning, and alignment stages of LLMs. We\ncategorize key attack vectors and assess how current defenses, such as\ndifferential privacy, embedding encryption, edge computing, and unlearning,\naddress these threats. Our analysis reveals critical gaps in semantic-level\nprotection, especially against contextual inference and latent representation\nleakage. We conclude by outlining open challenges, including quantifying\nsemantic leakage, protecting multimodal inputs, balancing de-identification\nwith generation quality, and ensuring transparency in privacy enforcement. This\nwork aims to inform future research on designing robust, semantically aware\nprivacy-preserving techniques for LLMs.", "AI": {"tldr": "本文提出了一种以生命周期为中心的框架，分析大语言模型（LLMs）在输入处理、预训练、微调和对齐阶段中语义隐私风险的产生，并评估现有防御措施的不足。", "motivation": "随着大语言模型在敏感领域的广泛应用，传统数据隐私措施无法有效保护隐含、上下文或可推断的信息（即语义隐私），因此需要系统性研究。", "method": "通过分类关键攻击向量，评估现有防御措施（如差分隐私、嵌入加密、边缘计算和遗忘技术）对语义隐私威胁的应对能力。", "result": "分析揭示了语义层面保护的关键缺陷，尤其是针对上下文推断和潜在表示泄漏的防御不足。", "conclusion": "未来研究需解决量化语义泄漏、保护多模态输入、平衡去标识与生成质量、确保隐私执行透明度等挑战，以设计更鲁棒的语义隐私保护技术。"}}
{"id": "2506.23916", "pdf": "https://arxiv.org/pdf/2506.23916", "abs": "https://arxiv.org/abs/2506.23916", "authors": ["Radhika Juglan", "Marta Ligero", "Zunamys I. Carrero", "Asier Rabasco", "Tim Lenz", "Leo Misera", "Gregory Patrick Veldhuizen", "Paul Kuntke", "Hagen H. Kitzler", "Sven Nebelung", "Daniel Truhn", "Jakob Nikolas Kather"], "title": "Three-dimensional end-to-end deep learning for brain MRI analysis", "categories": ["cs.CV"], "comment": null, "summary": "Deep learning (DL) methods are increasingly outperforming classical\napproaches in brain imaging, yet their generalizability across diverse imaging\ncohorts remains inadequately assessed. As age and sex are key neurobiological\nmarkers in clinical neuroscience, influencing brain structure and disease risk,\nthis study evaluates three of the existing three-dimensional architectures,\nnamely Simple Fully Connected Network (SFCN), DenseNet, and Shifted Window\n(Swin) Transformers, for age and sex prediction using T1-weighted MRI from four\nindependent cohorts: UK Biobank (UKB, n=47,390), Dallas Lifespan Brain Study\n(DLBS, n=132), Parkinson's Progression Markers Initiative (PPMI, n=108 healthy\ncontrols), and Information eXtraction from Images (IXI, n=319). We found that\nSFCN consistently outperformed more complex architectures with AUC of 1.00\n[1.00-1.00] in UKB (internal test set) and 0.85-0.91 in external test sets for\nsex classification. For the age prediction task, SFCN demonstrated a mean\nabsolute error (MAE) of 2.66 (r=0.89) in UKB and 4.98-5.81 (r=0.55-0.70) across\nexternal datasets. Pairwise DeLong and Wilcoxon signed-rank tests with\nBonferroni corrections confirmed SFCN's superiority over Swin Transformer\nacross most cohorts (p<0.017, for three comparisons). Explainability analysis\nfurther demonstrates the regional consistency of model attention across cohorts\nand specific to each task. Our findings reveal that simpler convolutional\nnetworks outperform the denser and more complex attention-based DL\narchitectures in brain image analysis by demonstrating better generalizability\nacross different datasets.", "AI": {"tldr": "研究发现，简单的卷积网络（SFCN）在脑影像分析中表现优于更复杂的深度学习架构（如DenseNet和Swin Transformer），具有更好的泛化能力。", "motivation": "评估深度学习模型在脑影像分析中的泛化能力，尤其是针对年龄和性别预测任务。", "method": "使用三种3D架构（SFCN、DenseNet、Swin Transformer）在四个独立队列（UKB、DLBS、PPMI、IXI）中进行年龄和性别预测。", "result": "SFCN在性别分类（AUC 1.00-0.85）和年龄预测（MAE 2.66-5.81）中表现最佳，且优于其他复杂架构。", "conclusion": "简单卷积网络在脑影像分析中更具泛化性和实用性，优于复杂架构。"}}
{"id": "2506.23918", "pdf": "https://arxiv.org/pdf/2506.23918", "abs": "https://arxiv.org/abs/2506.23918", "authors": ["Zhaochen Su", "Peng Xia", "Hangyu Guo", "Zhenhua Liu", "Yan Ma", "Xiaoye Qu", "Jiaqi Liu", "Yanshu Li", "Kaide Zeng", "Zhengyuan Yang", "Linjie Li", "Yu Cheng", "Heng Ji", "Junxian He", "Yi R.", "Fung"], "title": "Thinking with Images for Multimodal Reasoning: Foundations, Methods, and Future Frontiers", "categories": ["cs.CV"], "comment": "We maintain a real-time GitHub repository tracking progress at:\n  https://github.com/zhaochen0110/Awesome_Think_With_Images", "summary": "Recent progress in multimodal reasoning has been significantly advanced by\ntextual Chain-of-Thought (CoT), a paradigm where models conduct reasoning\nwithin language. This text-centric approach, however, treats vision as a\nstatic, initial context, creating a fundamental \"semantic gap\" between rich\nperceptual data and discrete symbolic thought. Human cognition often transcends\nlanguage, utilizing vision as a dynamic mental sketchpad. A similar evolution\nis now unfolding in AI, marking a fundamental paradigm shift from models that\nmerely think about images to those that can truly think with images. This\nemerging paradigm is characterized by models leveraging visual information as\nintermediate steps in their thought process, transforming vision from a passive\ninput into a dynamic, manipulable cognitive workspace. In this survey, we chart\nthis evolution of intelligence along a trajectory of increasing cognitive\nautonomy, which unfolds across three key stages: from external tool\nexploration, through programmatic manipulation, to intrinsic imagination. To\nstructure this rapidly evolving field, our survey makes four key contributions.\n(1) We establish the foundational principles of the think with image paradigm\nand its three-stage framework. (2) We provide a comprehensive review of the\ncore methods that characterize each stage of this roadmap. (3) We analyze the\ncritical landscape of evaluation benchmarks and transformative applications.\n(4) We identify significant challenges and outline promising future directions.\nBy providing this structured overview, we aim to offer a clear roadmap for\nfuture research towards more powerful and human-aligned multimodal AI.", "AI": {"tldr": "论文探讨了多模态推理中的新范式——视觉动态思维，从静态视觉输入转向动态视觉认知，提出了三阶段发展框架，并总结了方法、评估和应用。", "motivation": "解决文本链式思维（CoT）在视觉推理中的语义鸿沟问题，推动AI从静态视觉输入转向动态视觉认知，以更接近人类认知方式。", "method": "提出三阶段发展框架：外部工具探索、程序化操作和内在想象，并总结了各阶段的核心方法。", "result": "建立了视觉动态思维范式的基础原则，全面综述了相关方法、评估基准和应用，指出了未来研究方向。", "conclusion": "视觉动态思维是多模态AI的重要发展方向，未来需解决挑战以实现更强大且与人类认知一致的AI。"}}
{"id": "2506.23628", "pdf": "https://arxiv.org/pdf/2506.23628", "abs": "https://arxiv.org/abs/2506.23628", "authors": ["Antonio Ojea"], "title": "The Kubernetes Network Driver Model: A Composable Architecture for High-Performance Networking", "categories": ["cs.NI", "cs.AI"], "comment": "6 pages, 9 figures, submitted to IEEE LCN Special Track on\n  Cloud-AI-Native Mobile Networks Powered by eBPF (CAMe 2025)", "summary": "Traditional Kubernetes networking struggles to meet the escalating demands of\nAI/ML and evolving Telco infrastructure. This paper introduces Kubernetes\nNetwork Drivers (KNDs), a transformative, modular, and declarative architecture\ndesigned to overcome current imperative provisioning and API limitations. KNDs\nintegrate network resource management into Kubernetes' core by utilizing\nDynamic Resource Allocation (DRA), Node Resource Interface (NRI) improvements,\nand upcoming OCI Runtime Specification changes. Our DraNet implementation\ndemonstrates declarative attachment of network interfaces, including Remote\nDirect Memory Access (RDMA) devices, significantly boosting high-performance\nAI/ML workloads. This capability enables sophisticated cloud-native\napplications and lays crucial groundwork for future Telco solutions, fostering\na \"galaxy\" of specialized KNDs for enhanced application delivery and reduced\noperational complexity.", "AI": {"tldr": "Kubernetes Network Drivers (KNDs) 提出了一种模块化、声明式架构，解决了传统Kubernetes网络在AI/ML和电信基础设施中的不足。", "motivation": "传统Kubernetes网络无法满足AI/ML和高性能电信基础设施的需求，需要更灵活、高效的解决方案。", "method": "KNDs通过Dynamic Resource Allocation (DRA)、Node Resource Interface (NRI)改进和OCI Runtime Specification变更，将网络资源管理集成到Kubernetes核心中。", "result": "DraNet实现展示了声明式网络接口附加功能，显著提升了高性能AI/ML工作负载，并为未来电信解决方案奠定了基础。", "conclusion": "KNDs为云原生应用和电信基础设施提供了更高效、灵活的解决方案，减少了操作复杂性。"}}
{"id": "2506.23963", "pdf": "https://arxiv.org/pdf/2506.23963", "abs": "https://arxiv.org/abs/2506.23963", "authors": ["Vannkinh Nom", "Souhail Bakkali", "Muhammad Muzzamil Luqman", "Mickael Coustaty", "Jean-Marc Ogier"], "title": "Evaluating the Impact of Khmer Font Types on Text Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Text recognition is significantly influenced by font types, especially for\ncomplex scripts like Khmer. The variety of Khmer fonts, each with its unique\ncharacter structure, presents challenges for optical character recognition\n(OCR) systems. In this study, we evaluate the impact of 19 randomly selected\nKhmer font types on text recognition accuracy using Pytesseract. The fonts\ninclude Angkor, Battambang, Bayon, Bokor, Chenla, Dangrek, Freehand, Kh Kompong\nChhnang, Kh SN Kampongsom, Khmer, Khmer CN Stueng Songke, Khmer Savuth Pen,\nMetal, Moul, Odor MeanChey, Preah Vihear, Siemreap, Sithi Manuss, and iSeth\nFirst. Our comparison of OCR performance across these fonts reveals that Khmer,\nOdor MeanChey, Siemreap, Sithi Manuss, and Battambang achieve high accuracy,\nwhile iSeth First, Bayon, and Dangrek perform poorly. This study underscores\nthe critical importance of font selection in optimizing Khmer text recognition\nand provides valuable insights for developing more robust OCR systems.", "AI": {"tldr": "研究评估了19种高棉字体对OCR准确率的影响，发现Khmer、Odor MeanChey等字体表现优异，而iSeth First等表现较差，强调了字体选择对高棉文本识别的重要性。", "motivation": "高棉字体多样性对OCR系统提出了挑战，研究旨在评估不同字体对识别准确率的影响。", "method": "使用Pytesseract对19种随机选择的高棉字体进行OCR性能测试。", "result": "Khmer、Odor MeanChey等字体识别准确率高，iSeth First、Bayon等表现较差。", "conclusion": "字体选择对高棉文本识别至关重要，研究为开发更鲁棒的OCR系统提供了参考。"}}
{"id": "2506.23629", "pdf": "https://arxiv.org/pdf/2506.23629", "abs": "https://arxiv.org/abs/2506.23629", "authors": ["Xin Liao", "Bing Yang", "Cai Yu"], "title": "A Nonlinear Low-rank Representation Model with Convolutional Neural Network for Imputing Water Quality Data", "categories": ["cs.LG", "cs.AI", "68T07(Primary) 62M10, 65C60 (Secondary)", "I.2.7"], "comment": "7 pages, 2 figures, conference", "summary": "The integrity of Water Quality Data (WQD) is critical in environmental\nmonitoring for scientific decision-making and ecological protection. However,\nwater quality monitoring systems are often challenged by large amounts of\nmissing data due to unavoidable problems such as sensor failures and\ncommunication delays, which further lead to water quality data becoming\nHigh-Dimensional and Sparse (HDS). Traditional data imputation methods are\ndifficult to depict the potential dynamics and fail to capture the deep data\nfeatures, resulting in unsatisfactory imputation performance. To effectively\naddress the above issues, this paper proposes a Nonlinear Low-rank\nRepresentation model (NLR) with Convolutional Neural Networks (CNN) for\nimputing missing WQD, which utilizes CNNs to implement two ideas: a) fusing\ntemporal features to model the temporal dependence of data between time slots,\nand b) Extracting nonlinear interactions and local patterns to mine\nhigher-order relationships features and achieve deep fusion of multidimensional\ninformation. Experimental studies on three real water quality datasets\ndemonstrate that the proposed model significantly outperforms existing\nstate-of-the-art data imputation models in terms of estimation accuracy. It\nprovides an effective approach for handling water quality monitoring data in\ncomplex dynamic environments.", "AI": {"tldr": "本文提出了一种结合非线性低秩表示（NLR）和卷积神经网络（CNN）的模型，用于填补水质数据（WQD）中的缺失值，显著优于现有方法。", "motivation": "水质监测数据常因传感器故障和通信延迟导致大量缺失，传统方法难以捕捉数据的潜在动态和深层特征。", "method": "利用CNN融合时间特征以建模时间依赖性，并提取非线性交互和局部模式以实现多维信息的深度融合。", "result": "在三个真实水质数据集上的实验表明，该模型在估计精度上显著优于现有最先进的数据填补模型。", "conclusion": "该模型为复杂动态环境中的水质监测数据处理提供了有效方法。"}}
{"id": "2506.23972", "pdf": "https://arxiv.org/pdf/2506.23972", "abs": "https://arxiv.org/abs/2506.23972", "authors": ["Boyue Xu", "Ruichao Hou", "Tongwei Ren", "Gangshan Wu"], "title": "Visual and Memory Dual Adapter for Multi-Modal Object Tracking", "categories": ["cs.CV"], "comment": null, "summary": "Prompt-learning-based multi-modal trackers have achieved promising progress\nby employing lightweight visual adapters to incorporate auxiliary modality\nfeatures into frozen foundation models. However, existing approaches often\nstruggle to learn reliable prompts due to limited exploitation of critical cues\nacross frequency and temporal domains. In this paper, we propose a novel visual\nand memory dual adapter (VMDA) to construct more robust and discriminative\nrepresentations for multi-modal tracking. Specifically, we develop a simple but\neffective visual adapter that adaptively transfers discriminative cues from\nauxiliary modality to dominant modality by jointly modeling the frequency,\nspatial, and channel-wise features. Additionally, we design the memory adapter\ninspired by the human memory mechanism, which stores global temporal cues and\nperforms dynamic update and retrieval operations to ensure the consistent\npropagation of reliable temporal information across video sequences. Extensive\nexperiments demonstrate that our method achieves state-of-the-art performance\non the various multi-modal tracking tasks, including RGB-Thermal, RGB-Depth,\nand RGB-Event tracking. Code and models are available at\nhttps://github.com/xuboyue1999/mmtrack.git.", "AI": {"tldr": "提出了一种新颖的视觉和记忆双重适配器（VMDA），通过联合建模频率、空间和通道特征，以及利用人类记忆机制存储全局时间线索，提升了多模态跟踪的鲁棒性和判别性。", "motivation": "现有方法因未能充分利用频率和时间域的关键线索，导致提示学习不可靠。", "method": "设计了视觉适配器和记忆适配器，前者自适应转移辅助模态的判别性线索，后者存储全局时间线索并进行动态更新和检索。", "result": "在RGB-热成像、RGB-深度和RGB-事件等多模态跟踪任务中实现了最先进的性能。", "conclusion": "VMDA通过结合视觉和记忆适配器，显著提升了多模态跟踪的鲁棒性和性能。"}}
{"id": "2506.23634", "pdf": "https://arxiv.org/pdf/2506.23634", "abs": "https://arxiv.org/abs/2506.23634", "authors": ["Youjeong Noh", "Joon-Young Paik", "Jingun Kwon", "Eun-Sun Cho"], "title": "gMBA: Expression Semantic Guided Mixed Boolean-Arithmetic Deobfuscation Using Transformer Architectures", "categories": ["cs.CR", "cs.AI"], "comment": null, "summary": "Mixed Boolean-Arithmetic (MBA) obfuscation protects intellectual property by\nconverting programs into forms that are more complex to analyze. However, MBA\nhas been increasingly exploited by malware developers to evade detection and\ncause significant real-world problems. Traditional MBA deobfuscation methods\noften consider these expressions as part of a black box and overlook their\ninternal semantic information. To bridge this gap, we propose a truth table,\nwhich is an automatically constructed semantic representation of an\nexpression's behavior that does not rely on external resources. The truth table\nis a mathematical form that represents the output of expression for all\npossible combinations of input. We also propose a general and extensible guided\nMBA deobfuscation framework (gMBA) that modifies a Transformer-based neural\nencoder-decoder Seq2Seq architecture to incorporate this semantic guidance.\nExperimental results and in-depth analysis show that integrating expression\nsemantics significantly improves performance and highlights the importance of\ninternal semantic expressions in recovering obfuscated code to its original\nform.", "AI": {"tldr": "论文提出了一种基于真值表的MBA反混淆方法，结合语义信息显著提升了性能。", "motivation": "MBA混淆被恶意软件利用，传统方法忽视语义信息，需改进。", "method": "提出真值表作为语义表示，并设计基于Transformer的gMBA框架。", "result": "实验表明语义集成显著提升反混淆效果。", "conclusion": "语义信息对恢复混淆代码至关重要。"}}
{"id": "2506.23975", "pdf": "https://arxiv.org/pdf/2506.23975", "abs": "https://arxiv.org/abs/2506.23975", "authors": ["Yuliia Kaidashova", "Bettina Finzel", "Ute Schmid"], "title": "Toward Simple and Robust Contrastive Explanations for Image Classification by Leveraging Instance Similarity and Concept Relevance", "categories": ["cs.CV", "68T07", "I.2; I.4"], "comment": "17 pages, 6 figures, KI2025 - 48th German Conference on Artificial\n  Intelligence", "summary": "Understanding why a classification model prefers one class over another for\nan input instance is the challenge of contrastive explanation. This work\nimplements concept-based contrastive explanations for image classification by\nleveraging the similarity of instance embeddings and relevance of\nhuman-understandable concepts used by a fine-tuned deep learning model. Our\napproach extracts concepts with their relevance score, computes contrasts for\nsimilar instances, and evaluates the resulting contrastive explanations based\non explanation complexity. Robustness is tested for different image\naugmentations. Two research questions are addressed: (1) whether explanation\ncomplexity varies across different relevance ranges, and (2) whether\nexplanation complexity remains consistent under image augmentations such as\nrotation and noise. The results confirm that for our experiments higher concept\nrelevance leads to shorter, less complex explanations, while lower relevance\nresults in longer, more diffuse explanations. Additionally, explanations show\nvarying degrees of robustness. The discussion of these findings offers insights\ninto the potential of building more interpretable and robust AI systems.", "AI": {"tldr": "该论文提出了一种基于概念的对比解释方法，用于图像分类模型，通过分析嵌入相似性和概念相关性生成解释，并研究了解释复杂性和鲁棒性。", "motivation": "解决分类模型为何偏好某一类的对比解释问题，提升模型的可解释性和鲁棒性。", "method": "利用实例嵌入的相似性和人类可理解概念的相关性，提取概念及其相关性分数，计算相似实例的对比，并评估解释复杂性。", "result": "高相关性概念生成更简洁的解释，低相关性概念生成更复杂的解释；解释在不同图像增强下表现出不同程度的鲁棒性。", "conclusion": "研究为构建更可解释和鲁棒的AI系统提供了潜在方向。"}}
{"id": "2506.23635", "pdf": "https://arxiv.org/pdf/2506.23635", "abs": "https://arxiv.org/abs/2506.23635", "authors": ["Mu-Chi Chen", "Po-Hsuan Huang", "Xiangrui Ke", "Chia-Heng Tu", "Chun Jason Xue", "Shih-Hao Hung"], "title": "Towards Building Private LLMs: Exploring Multi-Node Expert Parallelism on Apple Silicon for Mixture-of-Experts Large Language Model", "categories": ["cs.DC", "cs.AI", "cs.PF", "I.6.4; I.2.7; I.2.11"], "comment": "International Conference on Research in Adaptive and Convergent\n  Systems (RACS '24), November 5--8, 2024, Pompei, Italy", "summary": "Large Language Models (LLMs) have revolutionized Artificial Intelligence (AI)\nwith significant advancements such as OpenAI's ChatGPT, Meta's Llama, and\nDatabricks' DBRX. This paper addresses the cost and scalability challenges\nencountered when constructing private LLM systems for personal or small group\nservices, as aimed by Apple Intelligence. A Mac Studio cluster with Apple's M2\nUltra chips is established as a cost-efficient solution to host and accelerate\nthe pretrained DBRX model with the Mixture-of-Experts (MoE) architecture. Our\nperformance analysis reveal that parallel execution of the model's experts\nacross two to four machine nodes significantly reduces inference time. We find\nthat computation time for the experts is comparable to the communication time\nfor exchanging their outputs, emphasizing the importance of network latency\nover bandwidth. We also observe significant management overhead due to Apple\nsoftware stack's memory management logic. Based on these findings, we develop\noptimization schemes to eliminate the memory management overhead. As a result,\nthe Mac Studio cluster is 1.15 times more cost-efficient than the\nstate-of-the-art AI supercomputer with NVIDIA H100 GPUs. In addition, we\nconstruct a performance model to estimate system performance under varying\nconfigurations, and the model provides valuable insights for designing private\nLLM systems.", "AI": {"tldr": "该论文探讨了在构建私有LLM系统时面临的成本和可扩展性挑战，提出了一种基于Mac Studio集群和M2 Ultra芯片的高效解决方案，通过优化内存管理和并行执行显著提升了性能。", "motivation": "解决个人或小团体服务中构建私有LLM系统的高成本和可扩展性问题，特别是针对Apple Intelligence的需求。", "method": "使用Mac Studio集群和M2 Ultra芯片托管和加速DBRX模型，采用MoE架构，并通过并行执行和优化内存管理提升性能。", "result": "Mac Studio集群比NVIDIA H100 GPU的AI超级计算机成本效率高1.15倍，并开发了性能模型以指导系统设计。", "conclusion": "Mac Studio集群是一种高效且成本优化的私有LLM系统解决方案，适用于个人或小团体服务。"}}
{"id": "2506.23982", "pdf": "https://arxiv.org/pdf/2506.23982", "abs": "https://arxiv.org/abs/2506.23982", "authors": ["Ruiyang Hao", "Bowen Jing", "Haibao Yu", "Zaiqing Nie"], "title": "StyleDrive: Towards Driving-Style Aware Benchmarking of End-To-End Autonomous Driving", "categories": ["cs.CV", "cs.RO", "I.4.9"], "comment": "14 pages, 4 figures", "summary": "While personalization has been explored in traditional autonomous driving\nsystems, it remains largely overlooked in end-to-end autonomous driving\n(E2EAD), despite its growing prominence. This gap is critical, as user-aligned\nbehavior is essential for trust, comfort, and widespread adoption of autonomous\nvehicles. A core challenge is the lack of large-scale real-world datasets\nannotated with diverse and fine-grained driving preferences, hindering the\ndevelopment and evaluation of personalized E2EAD models. In this work, we\npresent the first large-scale real-world dataset enriched with annotations\ncapturing diverse driving preferences, establishing a foundation for\npersonalization in E2EAD. We extract static environmental features from\nreal-world road topology and infer dynamic contextual cues using a fine-tuned\nvisual language model (VLM), enabling consistent and fine-grained scenario\nconstruction. Based on these scenarios, we derive objective preference\nannotations through behavioral distribution analysis and rule-based heuristics.\nTo address the inherent subjectivity of driving style, we further employ the\nVLM to generate subjective annotations by jointly modeling scene semantics and\ndriver behavior. Final high-quality labels are obtained through a\nhuman-in-the-loop verification process that fuses both perspectives. Building\non this dataset, we propose the first benchmark for evaluating personalized\nE2EAD models. We assess several state-of-the-art models with and without\npreference conditioning, demonstrating that incorporating personalized\npreferences results in behavior more aligned with human driving. Our work lays\nthe foundation for personalized E2EAD by providing a standardized platform to\nsystematically integrate human preferences into data-driven E2EAD systems,\ncatalyzing future research in human-centric autonomy.", "AI": {"tldr": "论文提出首个大规模真实世界数据集，用于端到端自动驾驶（E2EAD）的个性化研究，通过结合静态环境特征和动态上下文线索生成高质量偏好标注，并建立首个个性化E2EAD评估基准。", "motivation": "端到端自动驾驶（E2EAD）中个性化研究被忽视，而用户对齐行为对信任和广泛采用至关重要。缺乏大规模标注数据集阻碍了相关模型的开发与评估。", "method": "提取静态环境特征和动态上下文线索，结合行为分布分析和规则启发式生成客观偏好标注，利用视觉语言模型（VLM）生成主观标注，并通过人工验证融合两者。", "result": "实验表明，结合个性化偏好的模型行为更接近人类驾驶，验证了方法的有效性。", "conclusion": "研究为个性化E2EAD奠定了基础，提供了标准化平台，推动以人为中心的自动驾驶研究。"}}
{"id": "2506.24039", "pdf": "https://arxiv.org/pdf/2506.24039", "abs": "https://arxiv.org/abs/2506.24039", "authors": ["Shubhabrata Mukherjee", "Jack Lang", "Obeen Kwon", "Iryna Zenyuk", "Valerie Brogden", "Adam Weber", "Daniela Ushizima"], "title": "Foundation Models for Zero-Shot Segmentation of Scientific Images without AI-Ready Data", "categories": ["cs.CV", "cs.HC"], "comment": "This manuscript is a draft on arxiv. A final version has been\n  submitted to the 59th ICPP 2025, DRAI workshop", "summary": "Zero-shot and prompt-based technologies capitalized on using frequently\noccurring images to transform visual reasoning tasks, which explains why such\ntechnologies struggle with valuable yet scarce scientific image sets. In this\nwork, we propose Zenesis, a comprehensive no-code interactive platform designed\nto minimize barriers posed by data readiness for scientific images. We develop\nlightweight multi-modal adaptation techniques that enable zero-shot operation\non raw scientific data, along with human-in-the-loop refinement and\nheuristic-based temporal enhancement options. We demonstrate the performance of\nour approach through comprehensive comparison and validation on challenging\nFocused Ion Beam Scanning Electron Microscopy (FIB-SEM) data of catalyst-loaded\nmembranes. Zenesis significantly outperforms baseline methods, achieving an\naverage accuracy of 0.947, an Intersection over Union (IOU) of 0.858, and a\nDice score of 0.923 for amorphous catalyst samples and accuracy of 0.987, an\nIOU of 0.857, and a Dice score of 0.923 for crystalline samples. These results\nmark a substantial improvement over traditional methods like Otsu thresholding\nand even advanced models like Segment Anything Model (SAM) when used in\nisolation. Our results demonstrate that Zenesis is a powerful tool for\nscientific applications, particularly in fields where high-quality annotated\ndatasets are unavailable, accelerating accurate analysis of experimental\nimaging.", "AI": {"tldr": "Zenesis是一个无需代码的交互式平台，通过多模态适应技术和人机协同优化，显著提升了科学图像分析的准确性。", "motivation": "解决零样本和基于提示的技术在处理稀缺科学图像时的局限性。", "method": "开发轻量级多模态适应技术，支持零样本操作，并结合人机协同和启发式时间增强。", "result": "在FIB-SEM数据上表现优异，准确率和IOU显著优于传统方法。", "conclusion": "Zenesis是科学图像分析的高效工具，尤其适用于缺乏高质量标注数据的领域。"}}
{"id": "2506.23644", "pdf": "https://arxiv.org/pdf/2506.23644", "abs": "https://arxiv.org/abs/2506.23644", "authors": ["Junze Hu", "Xiangyu Jin", "Yizhe Zeng", "Yuling Liu", "Yunpeng Li", "Dan Du", "Kaiyu Xie", "Hongsong Zhu"], "title": "QLPro: Automated Code Vulnerability Discovery via LLM and Static Code Analysis Integration", "categories": ["cs.SE", "cs.AI", "cs.CR"], "comment": null, "summary": "We introduce QLPro, a vulnerability detection framework that systematically\nintegrates LLMs and static analysis tools to enable comprehensive vulnerability\ndetection across entire open-source projects.We constructed a new dataset,\nJavaTest, comprising 10 open-source projects from GitHub with 62 confirmed\nvulnerabilities. CodeQL, a state-of-the-art static analysis tool, detected only\n24 of these vulnerabilities while QLPro detected 41. Furthermore, QLPro\ndiscovered 6 previously unknown vulnerabilities, 2 of which have been confirmed\nas 0-days.", "AI": {"tldr": "QLPro是一个结合LLMs和静态分析工具的漏洞检测框架，在开源项目中检测漏洞表现优异。", "motivation": "传统静态分析工具（如CodeQL）在漏洞检测中存在局限性，需要更全面的解决方案。", "method": "QLPro系统整合LLMs和静态分析工具，构建新数据集JavaTest进行验证。", "result": "QLPro检测到41个漏洞（CodeQL仅24个），并发现6个新漏洞（含2个0-day）。", "conclusion": "QLPro在漏洞检测方面显著优于传统工具，具有实际应用潜力。"}}
{"id": "2506.24044", "pdf": "https://arxiv.org/pdf/2506.24044", "abs": "https://arxiv.org/abs/2506.24044", "authors": ["Sicong Jiang", "Zilin Huang", "Kangan Qian", "Ziang Luo", "Tianze Zhu", "Yang Zhong", "Yihong Tang", "Menglin Kong", "Yunlong Wang", "Siwen Jiao", "Hao Ye", "Zihao Sheng", "Xin Zhao", "Tuopu Wen", "Zheng Fu", "Sikai Chen", "Kun Jiang", "Diange Yang", "Seongjin Choi", "Lijun Sun"], "title": "A Survey on Vision-Language-Action Models for Autonomous Driving", "categories": ["cs.CV", "cs.AI", "cs.RO"], "comment": null, "summary": "The rapid progress of multimodal large language models (MLLM) has paved the\nway for Vision-Language-Action (VLA) paradigms, which integrate visual\nperception, natural language understanding, and control within a single policy.\nResearchers in autonomous driving are actively adapting these methods to the\nvehicle domain. Such models promise autonomous vehicles that can interpret\nhigh-level instructions, reason about complex traffic scenes, and make their\nown decisions. However, the literature remains fragmented and is rapidly\nexpanding. This survey offers the first comprehensive overview of VLA for\nAutonomous Driving (VLA4AD). We (i) formalize the architectural building blocks\nshared across recent work, (ii) trace the evolution from early explainer to\nreasoning-centric VLA models, and (iii) compare over 20 representative models\naccording to VLA's progress in the autonomous driving domain. We also\nconsolidate existing datasets and benchmarks, highlighting protocols that\njointly measure driving safety, accuracy, and explanation quality. Finally, we\ndetail open challenges - robustness, real-time efficiency, and formal\nverification - and outline future directions of VLA4AD. This survey provides a\nconcise yet complete reference for advancing interpretable socially aligned\nautonomous vehicles. Github repo is available at\n\\href{https://github.com/JohnsonJiang1996/Awesome-VLA4AD}{SicongJiang/Awesome-VLA4AD}.", "AI": {"tldr": "该论文综述了视觉-语言-动作（VLA）模型在自动驾驶领域的应用，总结了架构、演变、代表性模型、数据集及未来挑战。", "motivation": "整合视觉感知、自然语言理解和控制的VLA模型为自动驾驶提供了新范式，但相关研究分散且快速扩展，需系统梳理。", "method": "通过分析20多个代表性模型，总结VLA4AD的架构演变，并整理现有数据集和评测协议。", "result": "提供了VLA4AD的全面概述，包括模型比较、数据集整合及评测标准。", "conclusion": "VLA4AD面临鲁棒性、实时效率和形式验证等挑战，未来需进一步推动可解释与社会对齐的自动驾驶发展。"}}
{"id": "2506.23678", "pdf": "https://arxiv.org/pdf/2506.23678", "abs": "https://arxiv.org/abs/2506.23678", "authors": ["Rock Yuren Pang", "K. J. Kevin Feng", "Shangbin Feng", "Chu Li", "Weijia Shi", "Yulia Tsvetkov", "Jeffrey Heer", "Katharina Reinecke"], "title": "Interactive Reasoning: Visualizing and Controlling Chain-of-Thought Reasoning in Large Language Models", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The output quality of large language models (LLMs) can be improved via\n\"reasoning\": generating segments of chain-of-thought (CoT) content to further\ncondition the model prior to producing user-facing output. While these chains\ncontain valuable information, they are verbose and lack explicit organization,\nmaking them tedious to review. Moreover, they lack opportunities for user\nfeedback, such as to remove unwanted considerations, add desired ones, or\nclarify unclear assumptions. We introduce Interactive Reasoning, an interaction\ndesign that visualizes chain-of-thought outputs as a hierarchy of topics and\nenables user review and modification. We implement interactive reasoning in\nHippo, a prototype for AI-assisted decision making in the face of uncertain\ntrade-offs. In a user study with 16 participants, we find that interactive\nreasoning in Hippo allows users to quickly identify and interrupt erroneous\ngenerations, efficiently steer the model towards customized responses, and\nbetter understand both model reasoning and model outputs. Our work contributes\nto a new paradigm that incorporates user oversight into LLM reasoning\nprocesses.", "AI": {"tldr": "Interactive Reasoning通过可视化思维链（CoT）并支持用户修改，提升了大型语言模型（LLM）的输出质量和用户控制。", "motivation": "现有思维链内容冗长且缺乏组织，用户难以审查和反馈。", "method": "提出Interactive Reasoning设计，将CoT输出可视化为层级主题，并支持用户审查和修改。在Hippo原型中实现。", "result": "用户研究表明，该方法能快速识别错误、高效定制响应，并增强对模型推理的理解。", "conclusion": "Interactive Reasoning为LLM推理过程引入用户监督，开创了新范式。"}}
{"id": "2506.24063", "pdf": "https://arxiv.org/pdf/2506.24063", "abs": "https://arxiv.org/abs/2506.24063", "authors": ["Deng Li", "Aming Wu", "Yang Li", "Yaowei Wang", "Yahong Han"], "title": "Continual Adaptation: Environment-Conditional Parameter Generation for Object Detection in Dynamic Scenarios", "categories": ["cs.CV"], "comment": null, "summary": "In practice, environments constantly change over time and space, posing\nsignificant challenges for object detectors trained based on a closed-set\nassumption, i.e., training and test data share the same distribution. To this\nend, continual test-time adaptation has attracted much attention, aiming to\nimprove detectors' generalization by fine-tuning a few specific parameters,\ne.g., BatchNorm layers. However, based on a small number of test images,\nfine-tuning certain parameters may affect the representation ability of other\nfixed parameters, leading to performance degradation. Instead, we explore a new\nmechanism, i.e., converting the fine-tuning process to a specific-parameter\ngeneration. Particularly, we first design a dual-path LoRA-based domain-aware\nadapter that disentangles features into domain-invariant and domain-specific\ncomponents, enabling efficient adaptation. Additionally, a conditional\ndiffusion-based parameter generation mechanism is presented to synthesize the\nadapter's parameters based on the current environment, preventing the\noptimization from getting stuck in local optima. Finally, we propose a\nclass-centered optimal transport alignment method to mitigate catastrophic\nforgetting. Extensive experiments conducted on various continuous domain\nadaptive object detection tasks demonstrate the effectiveness. Meanwhile,\nvisualization results show that the representation extracted by the generated\nparameters can capture more object-related information and strengthen the\ngeneralization ability.", "AI": {"tldr": "论文提出了一种新的机制，将微调过程转化为特定参数生成，通过双路径LoRA适配器和条件扩散参数生成，提升目标检测器在持续测试时适应环境变化的能力。", "motivation": "现实环境中目标检测器因训练和测试数据分布不同而表现不佳，传统微调方法可能影响固定参数性能，需探索新方法。", "method": "设计双路径LoRA适配器分离域不变和域特定特征，结合条件扩散参数生成机制，并提出类中心最优传输对齐方法。", "result": "在多种持续域自适应目标检测任务中表现优异，生成参数能捕获更多目标相关信息。", "conclusion": "新机制有效提升检测器泛化能力，避免局部最优和灾难性遗忘。"}}
{"id": "2506.23679", "pdf": "https://arxiv.org/pdf/2506.23679", "abs": "https://arxiv.org/abs/2506.23679", "authors": ["David Demitri Africa", "Sara M. Kapoor", "Theo Simon Sorg"], "title": "Learning Modular Exponentiation with Transformers", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "Modular exponentiation is crucial to number theory and cryptography, yet\nremains largely unexplored from a mechanistic interpretability standpoint. We\ntrain a 4-layer encoder-decoder Transformer model to perform this operation and\ninvestigate the emergence of numerical reasoning during training. Utilizing\nprincipled sampling strategies, PCA-based embedding analysis, and activation\npatching, we examine how number-theoretic properties are encoded within the\nmodel. We find that reciprocal operand training leads to strong performance\ngains, with sudden generalization across related moduli. These synchronized\naccuracy surges reflect grokking-like dynamics, suggesting the model\ninternalizes shared arithmetic structure. We also find a subgraph consisting\nentirely of attention heads in the final layer sufficient to achieve full\nperformance on the task of regular exponentiation. These results suggest that\ntransformer models learn modular arithmetic through specialized computational\ncircuits, paving the way for more interpretable and efficient neural approaches\nto modular exponentiation.", "AI": {"tldr": "论文研究了Transformer模型在模幂运算中的数值推理能力，发现通过特定训练策略可以显著提升性能，并揭示了模型内部算术结构的编码方式。", "motivation": "模幂运算在数论和密码学中至关重要，但机制可解释性研究较少，本文旨在探索Transformer模型如何学习并执行这一运算。", "method": "使用4层编码器-解码器Transformer模型，结合PCA嵌入分析和激活修补技术，研究模型对数值特性的编码方式。", "result": "特定训练策略（如倒数操作数训练）显著提升性能，模型表现出类似“顿悟”的泛化能力，并发现仅需最后一层的注意力头子图即可完成任务。", "conclusion": "Transformer通过专用计算电路学习模算术，为更可解释和高效的神经方法奠定了基础。"}}
{"id": "2506.24085", "pdf": "https://arxiv.org/pdf/2506.24085", "abs": "https://arxiv.org/abs/2506.24085", "authors": ["Wonwoong Cho", "Yanxia Zhang", "Yan-Ying Chen", "David I. Inouye"], "title": "Imagine for Me: Creative Conceptual Blending of Real Images and Text via Blended Attention", "categories": ["cs.CV", "cs.AI"], "comment": "Project website is available at https://imagineforme.github.io/", "summary": "Blending visual and textual concepts into a new visual concept is a unique\nand powerful trait of human beings that can fuel creativity. However, in\npractice, cross-modal conceptual blending for humans is prone to cognitive\nbiases, like design fixation, which leads to local minima in the design space.\nIn this paper, we propose a T2I diffusion adapter \"IT-Blender\" that can\nautomate the blending process to enhance human creativity. Prior works related\nto cross-modal conceptual blending are limited in encoding a real image without\nloss of details or in disentangling the image and text inputs. To address these\ngaps, IT-Blender leverages pretrained diffusion models (SD and FLUX) to blend\nthe latent representations of a clean reference image with those of the noisy\ngenerated image. Combined with our novel blended attention, IT-Blender encodes\nthe real reference image without loss of details and blends the visual concept\nwith the object specified by the text in a disentangled way. Our experiment\nresults show that IT-Blender outperforms the baselines by a large margin in\nblending visual and textual concepts, shedding light on the new application of\nimage generative models to augment human creativity.", "AI": {"tldr": "IT-Blender是一种基于T2I扩散适配器的工具，用于自动化视觉与文本概念的混合，提升人类创造力。", "motivation": "人类在跨模态概念混合中存在认知偏差，如设计固定性，导致设计空间局部最优。IT-Blender旨在解决这一问题。", "method": "利用预训练扩散模型（SD和FLUX）混合干净参考图像和生成图像的潜在表示，结合混合注意力机制，实现细节保留和解耦混合。", "result": "IT-Blender在视觉与文本概念混合任务中大幅优于基线方法。", "conclusion": "IT-Blender展示了图像生成模型在增强人类创造力方面的新应用潜力。"}}
{"id": "2506.23717", "pdf": "https://arxiv.org/pdf/2506.23717", "abs": "https://arxiv.org/abs/2506.23717", "authors": ["Xingting Yao", "Qinghao Hu", "Fei Zhou", "Tielong Liu", "Gang Li", "Peisong Wang", "Jian Cheng"], "title": "Towards Efficient and Accurate Spiking Neural Networks via Adaptive Bit Allocation", "categories": ["cs.NE", "cs.AI", "cs.CV", "cs.LG"], "comment": null, "summary": "Multi-bit spiking neural networks (SNNs) have recently become a heated\nresearch spot, pursuing energy-efficient and high-accurate AI. However, with\nmore bits involved, the associated memory and computation demands escalate to\nthe point where the performance improvements become disproportionate. Based on\nthe insight that different layers demonstrate different importance and extra\nbits could be wasted and interfering, this paper presents an adaptive bit\nallocation strategy for direct-trained SNNs, achieving fine-grained layer-wise\nallocation of memory and computation resources. Thus, SNN's efficiency and\naccuracy can be improved. Specifically, we parametrize the temporal lengths and\nthe bit widths of weights and spikes, and make them learnable and controllable\nthrough gradients. To address the challenges caused by changeable bit widths\nand temporal lengths, we propose the refined spiking neuron, which can handle\ndifferent temporal lengths, enable the derivation of gradients for temporal\nlengths, and suit spike quantization better. In addition, we theoretically\nformulate the step-size mismatch problem of learnable bit widths, which may\nincur severe quantization errors to SNN, and accordingly propose the step-size\nrenewal mechanism to alleviate this issue. Experiments on various datasets,\nincluding the static CIFAR and ImageNet and the dynamic CIFAR-DVS and\nDVS-GESTURE, demonstrate that our methods can reduce the overall memory and\ncomputation cost while achieving higher accuracy. Particularly, our\nSEWResNet-34 can achieve a 2.69\\% accuracy gain and 4.16$\\times$ lower bit\nbudgets over the advanced baseline work on ImageNet. This work will be fully\nopen-sourced.", "AI": {"tldr": "该论文提出了一种自适应比特分配策略，用于直接训练的脉冲神经网络（SNNs），通过层级的精细资源分配提升效率和准确性。", "motivation": "多比特SNNs在追求高能效和高精度时，因比特数增加导致内存和计算需求不成比例增长，部分比特可能浪费或干扰。", "method": "参数化权重和脉冲的时间长度与比特宽度，使其可通过梯度学习和控制；提出改进的脉冲神经元以处理可变比特宽度和时间长度；提出步长更新机制解决比特宽度学习中的步长不匹配问题。", "result": "在多个数据集上实验表明，该方法能降低内存和计算成本，同时提高准确性，例如在ImageNet上SEWResNet-34实现了2.69%的准确率提升和4.16倍的比特预算降低。", "conclusion": "自适应比特分配策略显著提升了SNNs的效率和准确性，为解决多比特SNNs的资源浪费问题提供了有效方案。"}}
{"id": "2506.23719", "pdf": "https://arxiv.org/pdf/2506.23719", "abs": "https://arxiv.org/abs/2506.23719", "authors": ["Alex Egg", "Martin Iglesias Goyanes", "Friso Kingma", "Andreu Mora", "Leandro von Werra", "Thomas Wolf"], "title": "DABstep: Data Agent Benchmark for Multi-step Reasoning", "categories": ["cs.LG", "cs.AI"], "comment": "13 pages, 5 figures", "summary": "We introduce DABstep, a novel benchmark for evaluating AI agents on realistic\nmulti-step data analysis tasks. DABstep comprises over 450 real-world\nchallenges derived from a financial analytics platform, requiring models to\ncombine code-based data processing with contextual reasoning over heterogeneous\ndocumentation. Each task demands an iterative, multi-step problem-solving\napproach, testing capabilities in data manipulation, cross-referencing multiple\nsources, and precise result reporting. The benchmark provides a factoid-style\nanswer format with automatic correctness checks for objective scoring at scale.\nWe evaluate leading LLM-based agents, revealing a substantial performance gap:\neven the best agent achieves only 14.55% accuracy on the hardest tasks. We\ndetail our benchmark's design, dataset composition, task formulation,\nevaluation protocol, report baseline results and analyze failure modes. DABstep\nis released with a public leaderboard and toolkit to accelerate research in\nautonomous data analysis.", "AI": {"tldr": "DABstep是一个用于评估AI代理在多步骤数据分析任务中的新基准，包含450多个现实挑战，测试数据操作和上下文推理能力。最佳代理在最难任务中准确率仅为14.55%。", "motivation": "为评估AI代理在复杂数据分析任务中的能力，填补现有基准的不足。", "method": "基于金融分析平台的真实挑战设计任务，结合代码处理和文档推理，采用多步骤问题解决和自动评分。", "result": "领先的LLM代理在最难任务中表现不佳，准确率仅14.55%。", "conclusion": "DABstep为自主数据分析研究提供了新工具和基准，揭示了当前模型的局限性。"}}
{"id": "2506.24092", "pdf": "https://arxiv.org/pdf/2506.24092", "abs": "https://arxiv.org/abs/2506.24092", "authors": ["Moein Heidari", "Yasamin Medghalchi", "Mahdi Khoursha", "Reza Rezaeian", "Ilker Hacihaliloglu"], "title": "WaRA: Wavelet Low Rank Adaptation", "categories": ["cs.CV", "eess.IV"], "comment": "Submitted to BMVC 2025", "summary": "Parameter-efficient fine-tuning (PEFT) has gained widespread adoption across\nvarious applications. Among PEFT techniques, Low-Rank Adaptation (LoRA) and its\nextensions have emerged as particularly effective, allowing efficient model\nadaptation while significantly reducing computational overhead. However,\nexisting approaches typically rely on global low-rank factorizations, which\noverlook local or multi-scale structure, failing to capture complex patterns in\nthe weight updates. To address this, we propose WaRA, a novel PEFT method that\nleverages wavelet transforms to decompose the weight update matrix into a\nmulti-resolution representation. By performing low-rank factorization in the\nwavelet domain and reconstructing updates through an inverse transform, WaRA\nobtains compressed adaptation parameters that harness multi-resolution\nanalysis, enabling it to capture both coarse and fine-grained features while\nproviding greater flexibility and sparser representations than standard LoRA.\nThrough comprehensive experiments and analysis, we demonstrate that WaRA\nperforms superior on diverse vision tasks, including image generation,\nclassification, and semantic segmentation, significantly enhancing generated\nimage quality while reducing computational complexity. Although WaRA was\nprimarily designed for vision tasks, we further showcase its effectiveness in\nlanguage tasks, highlighting its broader applicability and generalizability.\nThe code is publicly available at\n\\href{GitHub}{https://github.com/moeinheidari7829/WaRA}.", "AI": {"tldr": "WaRA是一种新的参数高效微调方法，利用小波变换分解权重更新矩阵，通过多分辨率分析捕获复杂模式，优于现有方法。", "motivation": "现有PEFT方法（如LoRA）依赖全局低秩分解，忽略了局部或多尺度结构，无法捕捉权重更新中的复杂模式。", "method": "WaRA通过小波变换将权重更新矩阵分解为多分辨率表示，在变换域进行低秩分解并通过逆变换重建更新。", "result": "WaRA在视觉任务（如图像生成、分类和语义分割）中表现优异，显著提升生成图像质量并降低计算复杂度，同时在语言任务中也有效。", "conclusion": "WaRA是一种通用且高效的PEFT方法，适用于多种任务，代码已开源。"}}
{"id": "2506.23721", "pdf": "https://arxiv.org/pdf/2506.23721", "abs": "https://arxiv.org/abs/2506.23721", "authors": ["Gijs Luijten", "Roberto Maria Scardigno", "Lisle Faray de Paiva", "Peter Hoyer", "Jens Kleesiek", "Domenico Buongiorno", "Vitoantonio Bevilacqua", "Jan Egger"], "title": "Deep Learning-Based Semantic Segmentation for Real-Time Kidney Imaging and Measurements with Augmented Reality-Assisted Ultrasound", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": null, "summary": "Ultrasound (US) is widely accessible and radiation-free but has a steep\nlearning curve due to its dynamic nature and non-standard imaging planes.\nAdditionally, the constant need to shift focus between the US screen and the\npatient poses a challenge. To address these issues, we integrate deep learning\n(DL)-based semantic segmentation for real-time (RT) automated kidney volumetric\nmeasurements, which are essential for clinical assessment but are traditionally\ntime-consuming and prone to fatigue. This automation allows clinicians to\nconcentrate on image interpretation rather than manual measurements.\nComplementing DL, augmented reality (AR) enhances the usability of US by\nprojecting the display directly into the clinician's field of view, improving\nergonomics and reducing the cognitive load associated with screen-to-patient\ntransitions. Two AR-DL-assisted US pipelines on HoloLens-2 are proposed: one\nstreams directly via the application programming interface for a wireless\nsetup, while the other supports any US device with video output for broader\naccessibility. We evaluate RT feasibility and accuracy using the Open Kidney\nDataset and open-source segmentation models (nnU-Net, Segmenter, YOLO with\nMedSAM and LiteMedSAM). Our open-source GitHub pipeline includes model\nimplementations, measurement algorithms, and a Wi-Fi-based streaming solution,\nenhancing US training and diagnostics, especially in point-of-care settings.", "AI": {"tldr": "论文提出了一种结合深度学习和增强现实的超声系统，用于实时自动化肾脏体积测量，旨在解决超声学习曲线陡峭和操作疲劳的问题。", "motivation": "超声技术虽广泛可用且无辐射，但其动态性和非标准成像平面导致学习曲线陡峭，且操作时需频繁切换注意力，增加了认知负担。", "method": "通过深度学习实现实时语义分割，结合增强现实技术将超声图像投射到医生视野中，提出了两种基于HoloLens-2的AR-DL辅助超声流程。", "result": "使用开源数据集和模型验证了实时性和准确性，并提供了开源GitHub工具链，支持模型实现、测量算法和无线流媒体解决方案。", "conclusion": "该技术提升了超声培训和诊断效率，尤其在即时护理场景中具有显著优势。"}}
{"id": "2506.24096", "pdf": "https://arxiv.org/pdf/2506.24096", "abs": "https://arxiv.org/abs/2506.24096", "authors": ["Antoine Guédon", "Diego Gomez", "Nissim Maruani", "Bingchen Gong", "George Drettakis", "Maks Ovsjanikov"], "title": "MILo: Mesh-In-the-Loop Gaussian Splatting for Detailed and Efficient Surface Reconstruction", "categories": ["cs.CV"], "comment": "10 pages. A presentation video of our approach is available at\n  https://youtu.be/_SGNhhNz0fE", "summary": "While recent advances in Gaussian Splatting have enabled fast reconstruction\nof high-quality 3D scenes from images, extracting accurate surface meshes\nremains a challenge. Current approaches extract the surface through costly\npost-processing steps, resulting in the loss of fine geometric details or\nrequiring significant time and leading to very dense meshes with millions of\nvertices. More fundamentally, the a posteriori conversion from a volumetric to\na surface representation limits the ability of the final mesh to preserve all\ngeometric structures captured during training. We present MILo, a novel\nGaussian Splatting framework that bridges the gap between volumetric and\nsurface representations by differentiably extracting a mesh from the 3D\nGaussians. We design a fully differentiable procedure that constructs the\nmesh-including both vertex locations and connectivity-at every iteration\ndirectly from the parameters of the Gaussians, which are the only quantities\noptimized during training. Our method introduces three key technical\ncontributions: a bidirectional consistency framework ensuring both\nrepresentations-Gaussians and the extracted mesh-capture the same underlying\ngeometry during training; an adaptive mesh extraction process performed at each\ntraining iteration, which uses Gaussians as differentiable pivots for Delaunay\ntriangulation; a novel method for computing signed distance values from the 3D\nGaussians that enables precise surface extraction while avoiding geometric\nerosion. Our approach can reconstruct complete scenes, including backgrounds,\nwith state-of-the-art quality while requiring an order of magnitude fewer mesh\nvertices than previous methods. Due to their light weight and empty interior,\nour meshes are well suited for downstream applications such as physics\nsimulations or animation.", "AI": {"tldr": "MILo是一种新型高斯泼溅框架，通过可微分地从3D高斯中提取网格，解决了从体积表示到表面表示的转换问题，实现了高质量、轻量级的3D场景重建。", "motivation": "当前方法在从高斯泼溅中提取表面网格时存在效率低、细节丢失或网格过于密集的问题，且后处理步骤限制了最终网格对训练中几何结构的保留能力。", "method": "MILo设计了一种完全可微分的过程，直接从高斯参数中构建网格（包括顶点位置和连接性），并引入了双向一致性框架、自适应网格提取和基于高斯的有符号距离计算方法。", "result": "该方法能够以最先进的质量重建完整场景，网格顶点数量比现有方法少一个数量级，且适合下游应用如物理模拟或动画。", "conclusion": "MILo通过可微分网格提取，成功弥合了体积与表面表示之间的差距，为3D重建提供了高效且高质量的解决方案。"}}
{"id": "2506.24102", "pdf": "https://arxiv.org/pdf/2506.24102", "abs": "https://arxiv.org/abs/2506.24102", "authors": ["Xiangtai Li", "Tao Zhang", "Yanwei Li", "Haobo Yuan", "Shihao Chen", "Yikang Zhou", "Jiahao Meng", "Yueyi Sun", "Shilin Xu", "Lu Qi", "Tianheng Cheng", "Yi Lin", "Zilong Huang", "Wenhao Huang", "Jiashi Feng", "Guang Shi"], "title": "DenseWorld-1M: Towards Detailed Dense Grounded Caption in the Real World", "categories": ["cs.CV"], "comment": "Datasets and Models: https://github.com/lxtGH/DenseWorld-1M", "summary": "Multimodal Large Language Models (MLLMs) demonstrate a complex understanding\nof scenes, benefiting from large-scale and high-quality datasets. Most existing\ncaption datasets lack the ground locations and relations for visual entities.\nSeveral grounded caption datasets face the problems of missing detailed\ndescriptions, relations, and massive object descriptions on high-resolution\nimages. To fill this gap for the community, we present DenseWorld-1M, the first\nmassive, detailed, dense grounded caption dataset in the real world. We design\na three-stage labeling pipeline, containing open-world perception, detailed\nobject caption generation, and dense caption merging. The first stage obtains\nentity-level masks and labels. The second stage generates the object-level,\ndetailed captions with the guidance of masks and labels from the first stage.\nThe final stage merges object captions and masks into spatial and relational\ndense captions. To accelerate the labeling process and improve caption quality,\nwe present two VLM models: the Detailed Region Caption model and the Spatial\nCaption Merging model. Extensive experiments on various settings, including\nvision-language understanding, visual grounding, and region caption generation,\ndemonstrate the effectiveness of our DenseWorld-1M dataset and labeling models.", "AI": {"tldr": "DenseWorld-1M是一个大规模、详细且密集的接地标注数据集，填补了现有数据集的空白，并通过三阶段标注流程和两个VLM模型提升标注效率和质量。", "motivation": "现有标注数据集缺乏视觉实体的位置和关系信息，且细节描述不足，DenseWorld-1M旨在解决这些问题。", "method": "采用三阶段标注流程：开放世界感知、详细对象标注生成、密集标注合并，并引入两个VLM模型辅助标注。", "result": "实验证明DenseWorld-1M在视觉语言理解、视觉定位和区域标注生成等任务中表现优异。", "conclusion": "DenseWorld-1M为社区提供了高质量的数据集和标注方法，推动了多模态大语言模型的发展。"}}
{"id": "2506.23725", "pdf": "https://arxiv.org/pdf/2506.23725", "abs": "https://arxiv.org/abs/2506.23725", "authors": ["Atharva Gundawar", "Som Sagar", "Ransalu Senanayake"], "title": "PAC Bench: Do Foundation Models Understand Prerequisites for Executing Manipulation Policies?", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Vision-Language Models (VLMs) are increasingly pivotal for generalist robot\nmanipulation, enabling tasks such as physical reasoning, policy generation, and\nfailure detection. However, their proficiency in these high-level applications\noften assumes a deep understanding of low-level physical prerequisites, a\ncapability that remains largely unverified. For robots to perform actions\nreliably, they must comprehend intrinsic object properties (e.g., material,\nweight), action affordances (e.g., graspable, stackable), and physical\nconstraints (e.g., stability, reachability, or an object's state, such as being\nclosed). Despite the widespread use of VLMs in manipulation tasks, we argue\nthat off-the-shelf models may lack this granular, physically grounded\nunderstanding, as such prerequisites are often overlooked during training.\n  To address this critical gap, we introduce PAC Bench, a comprehensive\nbenchmark designed to systematically evaluate VLMs on their understanding of\ncore Properties, Affordances, and Constraints (PAC) from a task executability\nperspective. PAC Bench features a diverse dataset with over 30,000 annotations,\ncomprising 673 real-world images (115 object classes, 15 property types, and 1\nto 3 affordances defined per class), 100 real-world humanoid-view scenarios,\nand 120 unique simulated constraint scenarios across four tasks.\n  Our evaluations reveal significant gaps in the ability of current VLMs to\ngrasp fundamental physical concepts, highlighting limitations in their\nsuitability for reliable robot manipulation and pointing to key areas for\ntargeted research. PAC Bench also serves as a standardized benchmark for\nrigorously evaluating physical reasoning in VLMs and guiding the development of\nmore robust, physically grounded models for robotic applications.\n  Project Page: https://pacbench.github.io/", "AI": {"tldr": "PAC Bench是一个新基准，用于评估视觉语言模型（VLMs）在物理属性、可操作性和约束方面的理解能力，发现现有模型在这些基础物理概念上存在显著不足。", "motivation": "尽管VLMs在机器人操作任务中广泛应用，但其对低层物理前提的理解能力尚未得到验证，这影响了任务的可靠性。", "method": "提出PAC Bench，包含多样化的数据集（30,000+标注，673张真实图像，100个真实场景，120个模拟场景），用于系统评估VLMs的物理理解能力。", "result": "评估显示当前VLMs在基础物理概念理解上存在显著不足，限制了其在机器人操作中的可靠性。", "conclusion": "PAC Bench为标准化评估物理推理提供了工具，并指导开发更稳健的VLMs，以支持机器人应用。"}}
{"id": "2506.24113", "pdf": "https://arxiv.org/pdf/2506.24113", "abs": "https://arxiv.org/abs/2506.24113", "authors": ["Kaiwen Zhang", "Zhenyu Tang", "Xiaotao Hu", "Xingang Pan", "Xiaoyang Guo", "Yuan Liu", "Jingwei Huang", "Li Yuan", "Qian Zhang", "Xiao-Xiao Long", "Xun Cao", "Wei Yin"], "title": "Epona: Autoregressive Diffusion World Model for Autonomous Driving", "categories": ["cs.CV"], "comment": "ICCV2025, Project Page: https://kevin-thu.github.io/Epona/", "summary": "Diffusion models have demonstrated exceptional visual quality in video\ngeneration, making them promising for autonomous driving world modeling.\nHowever, existing video diffusion-based world models struggle with\nflexible-length, long-horizon predictions and integrating trajectory planning.\nThis is because conventional video diffusion models rely on global joint\ndistribution modeling of fixed-length frame sequences rather than sequentially\nconstructing localized distributions at each timestep. In this work, we propose\nEpona, an autoregressive diffusion world model that enables localized\nspatiotemporal distribution modeling through two key innovations: 1) Decoupled\nspatiotemporal factorization that separates temporal dynamics modeling from\nfine-grained future world generation, and 2) Modular trajectory and video\nprediction that seamlessly integrate motion planning with visual modeling in an\nend-to-end framework. Our architecture enables high-resolution, long-duration\ngeneration while introducing a novel chain-of-forward training strategy to\naddress error accumulation in autoregressive loops. Experimental results\ndemonstrate state-of-the-art performance with 7.4\\% FVD improvement and minutes\nlonger prediction duration compared to prior works. The learned world model\nfurther serves as a real-time motion planner, outperforming strong end-to-end\nplanners on NAVSIM benchmarks. Code will be publicly available at\n\\href{https://github.com/Kevin-thu/Epona/}{https://github.com/Kevin-thu/Epona/}.", "AI": {"tldr": "Epona提出了一种自回归扩散世界模型，通过解耦时空建模和模块化轨迹视频预测，解决了传统视频扩散模型在长时预测和轨迹规划中的局限性。", "motivation": "现有视频扩散模型在自动驾驶世界建模中难以实现灵活长度和长时预测，且无法有效整合轨迹规划。", "method": "提出Epona模型，采用解耦时空因子化和模块化轨迹视频预测，结合链式前向训练策略。", "result": "实验显示性能提升7.4% FVD，预测时长显著延长，并在NAVSIM基准测试中优于端到端规划器。", "conclusion": "Epona在长时高分辨率视频生成和实时运动规划中表现优异，代码将开源。"}}
{"id": "2506.23726", "pdf": "https://arxiv.org/pdf/2506.23726", "abs": "https://arxiv.org/abs/2506.23726", "authors": ["Bartlomiej Sobieski", "Matthew Tivnan", "Yuang Wang", "Siyeop Yoon", "Pengfei Jin", "Dufan Wu", "Quanzheng Li", "Przemyslaw Biecek"], "title": "System-Embedded Diffusion Bridge Models", "categories": ["cs.LG", "cs.AI"], "comment": "Preprint", "summary": "Solving inverse problems -- recovering signals from incomplete or noisy\nmeasurements -- is fundamental in science and engineering. Score-based\ngenerative models (SGMs) have recently emerged as a powerful framework for this\ntask. Two main paradigms have formed: unsupervised approaches that adapt\npretrained generative models to inverse problems, and supervised bridge methods\nthat train stochastic processes conditioned on paired clean and corrupted data.\nWhile the former typically assume knowledge of the measurement model, the\nlatter have largely overlooked this structural information. We introduce System\nembedded Diffusion Bridge Models (SDBs), a new class of supervised bridge\nmethods that explicitly embed the known linear measurement system into the\ncoefficients of a matrix-valued SDE. This principled integration yields\nconsistent improvements across diverse linear inverse problems and demonstrates\nrobust generalization under system misspecification between training and\ndeployment, offering a promising solution to real-world applications.", "AI": {"tldr": "提出了一种新的监督桥接方法SDB，通过显式嵌入已知线性测量系统，改进了逆问题的解决效果。", "motivation": "解决逆问题（从噪声或不完整测量中恢复信号）是科学和工程中的基础任务，现有方法或忽略测量模型结构信息，或假设其已知。", "method": "引入系统嵌入扩散桥模型（SDB），将已知线性测量系统嵌入矩阵值SDE的系数中。", "result": "在多种线性逆问题中表现一致改进，并在系统训练与部署不一致时展现出鲁棒性。", "conclusion": "SDB为实际应用提供了一种有前景的解决方案。"}}
{"id": "2506.24121", "pdf": "https://arxiv.org/pdf/2506.24121", "abs": "https://arxiv.org/abs/2506.24121", "authors": ["Sisi Dai", "Xinxin Su", "Boyan Wan", "Ruizhen Hu", "Kai Xu"], "title": "TextMesh4D: High-Quality Text-to-4D Mesh Generation", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in diffusion generative models significantly advanced\nimage, video, and 3D content creation from user-provided text prompts. However,\nthe challenging problem of dynamic 3D content generation (text-to-4D) with\ndiffusion guidance remains largely unexplored. In this paper, we introduce\nTextMesh4D, a novel framework for high-quality text-to-4D generation. Our\napproach leverages per-face Jacobians as a differentiable mesh representation\nand decomposes 4D generation into two stages: static object creation and\ndynamic motion synthesis. We further propose a flexibility-rigidity\nregularization term to stabilize Jacobian optimization under video diffusion\npriors, ensuring robust geometric performance. Experiments demonstrate that\nTextMesh4D achieves state-of-the-art results in terms of temporal consistency,\nstructural fidelity, and visual realism. Moreover, TextMesh4D operates with a\nlow GPU memory overhead-requiring only a single 24GB GPU-offering a\ncost-effective yet high-quality solution for text-driven 4D mesh generation.\nThe code will be released to facilitate future research in text-to-4D\ngeneration.", "AI": {"tldr": "TextMesh4D是一个新颖的框架，用于高质量文本到4D生成，通过分解为静态对象创建和动态运动合成两阶段，结合灵活-刚性正则化，实现了高效的4D网格生成。", "motivation": "解决动态3D内容生成（文本到4D）的挑战性问题，填补现有扩散生成模型在此领域的空白。", "method": "利用每面Jacobian作为可微分网格表示，分两阶段生成4D内容，并引入灵活-刚性正则化以稳定优化。", "result": "实验表明TextMesh4D在时间一致性、结构保真度和视觉真实感上达到最优，且GPU内存需求低。", "conclusion": "TextMesh4D为文本驱动的4D网格生成提供了高效且高质量的解决方案，代码将开源以促进未来研究。"}}
{"id": "2506.23734", "pdf": "https://arxiv.org/pdf/2506.23734", "abs": "https://arxiv.org/abs/2506.23734", "authors": ["Hao Shi", "Xi Li", "Fangfang Xie"], "title": "Marker Gene Method : Identifying Stable Solutions in a Dynamic Environment", "categories": ["cs.NE", "cs.AI", "cs.GT"], "comment": "Submitted to IEEE Transactions on Evolutionary Computation. 13 pages,\n  10 figures. Supplementary material is included", "summary": "Competitive Co-evolutionary Algorithms (CCEAs) are often hampered by complex\ndynamics like intransitivity and the Red Queen effect, leading to unstable\nconvergence. To counter these challenges, this paper introduces the Marker Gene\nMethod (MGM), a framework that establishes stability by using a 'marker gene'\nas a dynamic benchmark and an adaptive weighting mechanism to balance\nexploration and exploitation. We provide rigorous mathematical proofs\ndemonstrating that MGM creates strong attractors near Nash Equilibria within\nthe Strictly Competitive Game framework. Empirically, MGM demonstrates its\nefficacy across a spectrum of challenges: it stabilizes the canonical\nRock-Paper-Scissors game, significantly improves the performance of C-RMOEA/D\non ZDT benchmarks, and, when augmented with a Memory Pool (MP) extension, it\nsuccessfully tames the notoriously pathological Shapley Biased Game. This work\npresents a theoretically sound and empirically validated framework that\nsubstantially enhances the stability and robustness of CCEAs in complex\ncompetitive environments.", "AI": {"tldr": "论文提出Marker Gene Method (MGM)，通过动态基准和自适应权重机制增强竞争性协同进化算法的稳定性。", "motivation": "解决竞争性协同进化算法中的不稳定收敛问题，如非传递性和红皇后效应。", "method": "引入MGM框架，使用标记基因作为动态基准，并结合自适应权重机制。", "result": "数学证明MGM在严格竞争博弈中接近纳什均衡，实证表明其在多种挑战中有效。", "conclusion": "MGM显著提升了竞争性协同进化算法在复杂环境中的稳定性和鲁棒性。"}}
{"id": "2506.24123", "pdf": "https://arxiv.org/pdf/2506.24123", "abs": "https://arxiv.org/abs/2506.24123", "authors": ["Yue Ma", "Qingyan Bai", "Hao Ouyang", "Ka Leong Cheng", "Qiuyu Wang", "Hongyu Liu", "Zichen Liu", "Haofan Wang", "Jingye Chen", "Yujun Shen", "Qifeng Chen"], "title": "Calligrapher: Freestyle Text Image Customization", "categories": ["cs.CV"], "comment": "Project page: https://calligrapher2025.github.io/Calligrapher Code:\n  https://github.com/Calligrapher2025/Calligrapher", "summary": "We introduce Calligrapher, a novel diffusion-based framework that\ninnovatively integrates advanced text customization with artistic typography\nfor digital calligraphy and design applications. Addressing the challenges of\nprecise style control and data dependency in typographic customization, our\nframework incorporates three key technical contributions. First, we develop a\nself-distillation mechanism that leverages the pre-trained text-to-image\ngenerative model itself alongside the large language model to automatically\nconstruct a style-centric typography benchmark. Second, we introduce a\nlocalized style injection framework via a trainable style encoder, which\ncomprises both Qformer and linear layers, to extract robust style features from\nreference images. An in-context generation mechanism is also employed to\ndirectly embed reference images into the denoising process, further enhancing\nthe refined alignment of target styles. Extensive quantitative and qualitative\nevaluations across diverse fonts and design contexts confirm Calligrapher's\naccurate reproduction of intricate stylistic details and precise glyph\npositioning. By automating high-quality, visually consistent typography,\nCalligrapher surpasses traditional models, empowering creative practitioners in\ndigital art, branding, and contextual typographic design.", "AI": {"tldr": "Calligrapher是一个基于扩散模型的框架，结合文本定制与艺术字体设计，解决了风格控制和数据依赖问题。", "motivation": "解决字体定制中精确风格控制和数据依赖的挑战，为数字艺术和设计提供高效工具。", "method": "1. 自蒸馏机制构建风格基准；2. 可训练风格编码器提取特征；3. 上下文生成机制嵌入参考图像。", "result": "在多种字体和设计场景中，Calligrapher能准确复现风格细节和字形定位，优于传统模型。", "conclusion": "Calligrapher自动化生成高质量字体设计，为数字艺术和品牌设计提供强大支持。"}}
{"id": "2506.24125", "pdf": "https://arxiv.org/pdf/2506.24125", "abs": "https://arxiv.org/abs/2506.24125", "authors": ["Jiacheng Cui", "Xinyue Bi", "Yaxin Luo", "Xiaohan Zhao", "Jiacheng Liu", "Zhiqiang Shen"], "title": "FADRM: Fast and Accurate Data Residual Matching for Dataset Distillation", "categories": ["cs.CV", "cs.AI"], "comment": "Code at: https://github.com/Jiacheng8/FADRM", "summary": "Residual connection has been extensively studied and widely applied at the\nmodel architecture level. However, its potential in the more challenging\ndata-centric approaches remains unexplored. In this work, we introduce the\nconcept of Data Residual Matching for the first time, leveraging data-level\nskip connections to facilitate data generation and mitigate data information\nvanishing. This approach maintains a balance between newly acquired knowledge\nthrough pixel space optimization and existing core local information\nidentification within raw data modalities, specifically for the dataset\ndistillation task. Furthermore, by incorporating optimization-level\nrefinements, our method significantly improves computational efficiency,\nachieving superior performance while reducing training time and peak GPU memory\nusage by 50%. Consequently, the proposed method Fast and Accurate Data Residual\nMatching for Dataset Distillation (FADRM) establishes a new state-of-the-art,\ndemonstrating substantial improvements over existing methods across multiple\ndataset benchmarks in both efficiency and effectiveness. For instance, with\nResNet-18 as the student model and a 0.8% compression ratio on ImageNet-1K, the\nmethod achieves 47.7% test accuracy in single-model dataset distillation and\n50.0% in multi-model dataset distillation, surpassing RDED by +5.7% and\noutperforming state-of-the-art multi-model approaches, EDC and CV-DD, by +1.4%\nand +4.0%. Code is available at: https://github.com/Jiacheng8/FADRM.", "AI": {"tldr": "本文提出了一种名为FADRM的新方法，通过数据级残差连接优化数据集蒸馏任务，显著提升了计算效率和性能。", "motivation": "探索数据级残差连接的潜力，解决数据信息消失问题，并平衡新知识与原始数据信息。", "method": "引入数据残差匹配（Data Residual Matching）概念，结合优化级改进，提升计算效率和性能。", "result": "在ImageNet-1K上，单模型和多模型数据集蒸馏分别达到47.7%和50.0%的测试准确率，显著优于现有方法。", "conclusion": "FADRM在数据集蒸馏任务中实现了高效和准确的新标杆。"}}
{"id": "2506.23762", "pdf": "https://arxiv.org/pdf/2506.23762", "abs": "https://arxiv.org/abs/2506.23762", "authors": ["Hongzhou Rao", "Yanjie Zhao", "Xinyi Hou", "Shenao Wang", "Haoyu Wang"], "title": "Software Engineering for Large Language Models: Research Status, Challenges and the Road Ahead", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "The rapid advancement of large language models (LLMs) has redefined\nartificial intelligence (AI), pushing the boundaries of AI research and\nenabling unbounded possibilities for both academia and the industry. However,\nLLM development faces increasingly complex challenges throughout its lifecycle,\nyet no existing research systematically explores these challenges and solutions\nfrom the perspective of software engineering (SE) approaches. To fill the gap,\nwe systematically analyze research status throughout the LLM development\nlifecycle, divided into six phases: requirements engineering, dataset\nconstruction, model development and enhancement, testing and evaluation,\ndeployment and operations, and maintenance and evolution. We then conclude by\nidentifying the key challenges for each phase and presenting potential research\ndirections to address these challenges. In general, we provide valuable\ninsights from an SE perspective to facilitate future advances in LLM\ndevelopment.", "AI": {"tldr": "本文从软件工程角度系统分析了大语言模型（LLM）开发全周期的研究现状，识别了六个阶段的关键挑战，并提出了潜在研究方向。", "motivation": "现有研究未从软件工程角度系统探讨LLM开发中的复杂挑战及其解决方案，本文旨在填补这一空白。", "method": "将LLM开发生命周期分为六个阶段（需求工程、数据集构建、模型开发与增强、测试与评估、部署与运维、维护与演进），分析每阶段的研究现状与挑战。", "result": "总结了每阶段的关键挑战，并提出了潜在的研究方向。", "conclusion": "本文从软件工程视角为LLM开发的未来进展提供了有价值的见解。"}}
{"id": "2506.24127", "pdf": "https://arxiv.org/pdf/2506.24127", "abs": "https://arxiv.org/abs/2506.24127", "authors": ["Matthew Gwilliam", "Roy Zhang", "Namitha Padmanabhan", "Hongyang Du", "Abhinav Shrivastava"], "title": "How to Design and Train Your Implicit Neural Representation for Video Compression", "categories": ["cs.CV"], "comment": "21 pages, 41 figures, 5 tables", "summary": "Implicit neural representation (INR) methods for video compression have\nrecently achieved visual quality and compression ratios that are competitive\nwith traditional pipelines. However, due to the need for per-sample network\ntraining, the encoding speeds of these methods are too slow for practical\nadoption. We develop a library to allow us to disentangle and review the\ncomponents of methods from the NeRV family, reframing their performance in\nterms of not only size-quality trade-offs, but also impacts on training time.\nWe uncover principles for effective video INR design and propose a\nstate-of-the-art configuration of these components, Rabbit NeRV (RNeRV). When\nall methods are given equal training time (equivalent to 300 NeRV epochs) for 7\ndifferent UVG videos at 1080p, RNeRV achieves +1.27% PSNR on average compared\nto the best-performing alternative for each video in our NeRV library. We then\ntackle the encoding speed issue head-on by investigating the viability of\nhyper-networks, which predict INR weights from video inputs, to disentangle\ntraining from encoding to allow for real-time encoding. We propose masking the\nweights of the predicted INR during training to allow for variable, higher\nquality compression, resulting in 1.7% improvements to both PSNR and MS-SSIM at\n0.037 bpp on the UCF-101 dataset, and we increase hyper-network parameters by\n0.4% for 2.5%/2.7% improvements to PSNR/MS-SSIM with equal bpp and similar\nspeeds. Our project website is available at https://mgwillia.github.io/vinrb/\nand our code is available at https://github.com/mgwillia/vinrb.", "AI": {"tldr": "论文提出了一种改进的隐式神经表示（INR）视频压缩方法RNeRV，通过优化组件设计和引入超网络解决编码速度问题，提升了压缩质量和效率。", "motivation": "当前INR视频压缩方法因需要逐样本训练网络而编码速度过慢，限制了实际应用。", "method": "开发了一个库来分析NeRV家族方法的组件，提出RNeRV配置，并研究超网络预测INR权重的可行性。", "result": "RNeRV在相同训练时间下平均PSNR提升1.27%；超网络方法在UCF-101数据集上PSNR和MS-SSIM均提升1.7%。", "conclusion": "通过优化设计和超网络，RNeRV显著提升了视频压缩的性能和编码速度，为实际应用提供了可能。"}}
{"id": "2506.23771", "pdf": "https://arxiv.org/pdf/2506.23771", "abs": "https://arxiv.org/abs/2506.23771", "authors": ["Guizhe Jin", "Zhuoren Li", "Bo Leng", "Ran Yu", "Lu Xiong"], "title": "Multi-Timescale Hierarchical Reinforcement Learning for Unified Behavior and Control of Autonomous Driving", "categories": ["cs.RO", "cs.AI"], "comment": "8 pages, Submitted to IEEE Robotics and Automation Letters", "summary": "Reinforcement Learning (RL) is increasingly used in autonomous driving (AD)\nand shows clear advantages. However, most RL-based AD methods overlook policy\nstructure design. An RL policy that only outputs short-timescale vehicle\ncontrol commands results in fluctuating driving behavior due to fluctuations in\nnetwork outputs, while one that only outputs long-timescale driving goals\ncannot achieve unified optimality of driving behavior and control. Therefore,\nwe propose a multi-timescale hierarchical reinforcement learning approach. Our\napproach adopts a hierarchical policy structure, where high- and low-level RL\npolicies are unified-trained to produce long-timescale motion guidance and\nshort-timescale control commands, respectively. Therein, motion guidance is\nexplicitly represented by hybrid actions to capture multimodal driving\nbehaviors on structured road and support incremental low-level extend-state\nupdates. Additionally, a hierarchical safety mechanism is designed to ensure\nmulti-timescale safety. Evaluation in simulator-based and HighD dataset-based\nhighway multi-lane scenarios demonstrates that our approach significantly\nimproves AD performance, effectively increasing driving efficiency, action\nconsistency and safety.", "AI": {"tldr": "提出了一种多时间尺度分层强化学习方法，用于自动驾驶，通过统一训练高低层策略，显著提升驾驶效率、行为一致性和安全性。", "motivation": "现有RL方法在自动驾驶中忽视策略结构设计，导致驾驶行为波动或无法统一优化驾驶行为与控制。", "method": "采用分层策略结构，高层策略生成长时间尺度运动指导，低层策略生成短时间尺度控制命令，并设计分层安全机制。", "result": "在仿真和HighD数据集的高速公路多车道场景中，显著提升自动驾驶性能。", "conclusion": "多时间尺度分层强化学习方法有效解决了驾驶行为波动和统一优化问题，提升了驾驶效率与安全性。"}}
{"id": "2506.21629", "pdf": "https://arxiv.org/pdf/2506.21629", "abs": "https://arxiv.org/abs/2506.21629", "authors": ["Chenhao Zhang", "Yezhi Shen", "Fengqing Zhu"], "title": "ICP-3DGS: SfM-free 3D Gaussian Splatting for Large-scale Unbounded Scenes", "categories": ["cs.GR", "cs.CV"], "comment": "6 pages, Source code is available at\n  https://github.com/Chenhao-Z/ICP-3DGS. To appear at ICIP 2025", "summary": "In recent years, neural rendering methods such as NeRFs and 3D Gaussian\nSplatting (3DGS) have made significant progress in scene reconstruction and\nnovel view synthesis. However, they heavily rely on preprocessed camera poses\nand 3D structural priors from structure-from-motion (SfM), which are\nchallenging to obtain in outdoor scenarios. To address this challenge, we\npropose to incorporate Iterative Closest Point (ICP) with optimization-based\nrefinement to achieve accurate camera pose estimation under large camera\nmovements. Additionally, we introduce a voxel-based scene densification\napproach to guide the reconstruction in large-scale scenes. Experiments\ndemonstrate that our approach ICP-3DGS outperforms existing methods in both\ncamera pose estimation and novel view synthesis across indoor and outdoor\nscenes of various scales. Source code is available at\nhttps://github.com/Chenhao-Z/ICP-3DGS.", "AI": {"tldr": "论文提出了一种结合ICP与优化细化方法的新技术ICP-3DGS，用于解决神经渲染方法在户外场景中依赖预处理相机位姿的问题，并在大尺度场景中表现优异。", "motivation": "神经渲染方法（如NeRFs和3DGS）依赖预处理相机位姿和3D结构先验，这在户外场景中难以获取。", "method": "结合ICP与优化细化方法进行相机位姿估计，并引入基于体素的场景密集化方法指导大尺度场景重建。", "result": "ICP-3DGS在相机位姿估计和新视角合成方面优于现有方法，适用于不同尺度的室内外场景。", "conclusion": "ICP-3DGS解决了神经渲染方法在户外场景中的依赖问题，并在实验中表现出色。"}}
{"id": "2506.23782", "pdf": "https://arxiv.org/pdf/2506.23782", "abs": "https://arxiv.org/abs/2506.23782", "authors": ["Xiaoyang Li", "Linwei Tao", "Haohui Lu", "Minjing Dong", "Junbin Gao", "Chang Xu"], "title": "Calibrating Graph Neural Networks with Wavelet-Aware Temperature Scaling", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Graph Neural Networks (GNNs) have demonstrated strong predictive performance\non relational data; however, their confidence estimates often misalign with\nactual predictive correctness, posing significant limitations for deployment in\nsafety-critical settings. While existing graph-aware calibration methods seek\nto mitigate this limitation, they primarily depend on coarse one-hop\nstatistics, such as neighbor-predicted confidence, or latent node embeddings,\nthereby neglecting the fine-grained structural heterogeneity inherent in graph\ntopology. In this work, we propose Wavelet-Aware Temperature Scaling (WATS), a\npost-hoc calibration framework that assigns node-specific temperatures based on\ntunable heat-kernel graph wavelet features. Specifically, WATS harnesses the\nscalability and topology sensitivity of graph wavelets to refine confidence\nestimates, all without necessitating model retraining or access to neighboring\nlogits or predictions. Extensive evaluations across seven benchmark datasets\nwith varying graph structures and two GNN backbones demonstrate that WATS\nachieves the lowest Expected Calibration Error (ECE) among all compared\nmethods, outperforming both classical and graph-specific baselines by up to\n42.3\\% in ECE and reducing calibration variance by 17.24\\% on average compared\nwith graph-specific methods. Moreover, WATS remains computationally efficient,\nscaling well across graphs of diverse sizes and densities. Code will be\nreleased based on publication.", "AI": {"tldr": "提出了一种基于图小波的校准方法WATS，显著提升了GNN的置信度校准性能。", "motivation": "GNN的置信度估计与实际预测准确性不一致，限制了其在安全关键场景中的应用。现有方法依赖粗粒度统计或潜在嵌入，忽略了图拓扑的细粒度结构异质性。", "method": "提出了WATS框架，利用可调热核图小波特征为节点分配特定温度，无需重新训练模型或访问邻居信息。", "result": "在七个基准数据集上，WATS的预期校准误差最低，比现有方法最高提升42.3%，平均减少校准方差17.24%。", "conclusion": "WATS是一种高效且通用的GNN校准方法，适用于不同规模和密度的图。"}}
{"id": "2506.22467", "pdf": "https://arxiv.org/pdf/2506.22467", "abs": "https://arxiv.org/abs/2506.22467", "authors": ["Roy Colglazier", "Jisoo Lee", "Haoyu Dong", "Hanxue Gu", "Yaqian Chen", "Joseph Cao", "Zafer Yildiz", "Zhonghao Liu", "Nicholas Konz", "Jichen Yang", "Jikai Zhang", "Yuwen Chen", "Lin Li", "Adrian Camarena", "Maciej A. Mazurowski"], "title": "SegmentAnyMuscle: A universal muscle segmentation model across different locations in MRI", "categories": ["eess.SP", "cs.CV"], "comment": "24 pages, 6 figures", "summary": "The quantity and quality of muscles are increasingly recognized as important\npredictors of health outcomes. While MRI offers a valuable modality for such\nassessments, obtaining precise quantitative measurements of musculature remains\nchallenging. This study aimed to develop a publicly available model for muscle\nsegmentation in MRIs and demonstrate its applicability across various\nanatomical locations and imaging sequences. A total of 362 MRIs from 160\npatients at a single tertiary center (Duke University Health System, 2016-2020)\nwere included, with 316 MRIs from 114 patients used for model development. The\nmodel was tested on two separate sets: one with 28 MRIs representing common\nsequence types, achieving an average Dice Similarity Coefficient (DSC) of\n88.45%, and another with 18 MRIs featuring less frequent sequences and\nabnormalities such as muscular atrophy, hardware, and significant noise,\nachieving 86.21% DSC. These results demonstrate the feasibility of a fully\nautomated deep learning algorithm for segmenting muscles on MRI across diverse\nsettings. The public release of this model enables consistent, reproducible\nresearch into the relationship between musculature and health.", "AI": {"tldr": "开发了一个公开可用的深度学习模型，用于MRI肌肉分割，适用于多种解剖位置和成像序列，表现出高准确性。", "motivation": "肌肉数量和质量的评估对健康结果预测至关重要，但MRI的精确定量测量仍具挑战性。", "method": "使用362例MRI数据（160名患者）开发模型，测试集包括常见序列和罕见序列/异常情况。", "result": "模型在常见序列测试集上DSC为88.45%，在罕见序列/异常情况测试集上为86.21%。", "conclusion": "该模型实现了全自动MRI肌肉分割，支持肌肉与健康关系的可重复研究。"}}
{"id": "2506.22482", "pdf": "https://arxiv.org/pdf/2506.22482", "abs": "https://arxiv.org/abs/2506.22482", "authors": ["Divya Alok Gupta", "Dwith Chenna", "B. Aditya Vighnesh Ramakanth"], "title": "Wireless Home Automation Using Social Networking Websites", "categories": ["cs.NI", "cs.CR", "cs.CV"], "comment": "20th Annual International Conference on Advanced Computing and\n  Communications (ADCOM) 2014", "summary": "With the advent of Internet of Things, Wireless Home Automation Systems WHAS\nare gradually gaining popularity. These systems are faced with multiple\nchallenges such as security; controlling a variety of home appliances with a\nsingle interface and user friendliness. In this paper we propose a system that\nuses secure authentication systems of social networking websites such as\nTwitter, tracks the end-users activities on the social network and then control\nhis or her domestic appliances. At the end, we highlight the applications of\nthe proposed WHAS and compare the advantages of our proposed system over\ntraditional home automation systems.", "AI": {"tldr": "提出了一种基于社交媒体认证的无线家庭自动化系统（WHAS），通过追踪用户在社交网络的活动来控制家电，并对比了传统系统的优势。", "motivation": "随着物联网的发展，无线家庭自动化系统（WHAS）面临安全、多设备统一控制和用户友好性等挑战。", "method": "利用Twitter等社交网站的认证系统，追踪用户活动并控制家电。", "result": "展示了WHAS的应用场景，并对比了其与传统系统的优势。", "conclusion": "提出的WHAS在安全性和用户体验上优于传统家庭自动化系统。"}}
{"id": "2506.23815", "pdf": "https://arxiv.org/pdf/2506.23815", "abs": "https://arxiv.org/abs/2506.23815", "authors": ["Patrick Stokkink"], "title": "The Impact of AI on Educational Assessment: A Framework for Constructive Alignment", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "The influence of Artificial Intelligence (AI), and specifically Large\nLanguage Models (LLM), on education is continuously increasing. These models\nare frequently used by students, giving rise to the question whether current\nforms of assessment are still a valid way to evaluate student performance and\ncomprehension. The theoretical framework developed in this paper is grounded in\nConstructive Alignment (CA) theory and Bloom's taxonomy for defining learning\nobjectives. We argue that AI influences learning objectives of different Bloom\nlevels in a different way, and assessment has to be adopted accordingly.\nFurthermore, in line with Bloom's vision, formative and summative assessment\nshould be aligned on whether the use of AI is permitted or not.\n  Although lecturers tend to agree that education and assessment need to be\nadapted to the presence of AI, a strong bias exists on the extent to which\nlecturers want to allow for AI in assessment. This bias is caused by a\nlecturer's familiarity with AI and specifically whether they use it themselves.\nTo avoid this bias, we propose structured guidelines on a university or faculty\nlevel, to foster alignment among the staff. Besides that, we argue that\nteaching staff should be trained on the capabilities and limitations of AI\ntools. In this way, they are better able to adapt their assessment methods.", "AI": {"tldr": "论文探讨了AI（特别是LLM）对教育的影响，提出评估方式需根据AI对不同学习目标的影响进行调整，并建议通过结构化指南和培训避免教师偏见。", "motivation": "研究AI在教育中的日益增长的影响，尤其是LLM对学生评估方式的挑战，旨在提出适应性的评估框架。", "method": "基于Constructive Alignment理论和Bloom分类法，分析AI对不同层次学习目标的影响，并提出评估调整建议。", "result": "发现教师对AI在评估中的允许程度存在偏见，建议通过大学层面的结构化指南和教师培训来统一标准。", "conclusion": "教育评估需适应AI的影响，通过结构化指南和教师培训确保评估的一致性和有效性。"}}
{"id": "2506.22494", "pdf": "https://arxiv.org/pdf/2506.22494", "abs": "https://arxiv.org/abs/2506.22494", "authors": ["Shihong Ling", "Yue Wan", "Xiaowei Jia", "Na Du"], "title": "DriveBLIP2: Attention-Guided Explanation Generation for Complex Driving Scenarios", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025. 7 pages, 3 figures", "summary": "This paper introduces a new framework, DriveBLIP2, built upon the BLIP2-OPT\narchitecture, to generate accurate and contextually relevant explanations for\nemerging driving scenarios. While existing vision-language models perform well\nin general tasks, they encounter difficulties in understanding complex,\nmulti-object environments, particularly in real-time applications such as\nautonomous driving, where the rapid identification of key objects is crucial.\nTo address this limitation, an Attention Map Generator is proposed to highlight\nsignificant objects relevant to driving decisions within critical video frames.\nBy directing the model's focus to these key regions, the generated attention\nmap helps produce clear and relevant explanations, enabling drivers to better\nunderstand the vehicle's decision-making process in critical situations.\nEvaluations on the DRAMA dataset reveal significant improvements in explanation\nquality, as indicated by higher BLEU, ROUGE, CIDEr, and SPICE scores compared\nto baseline models. These findings underscore the potential of targeted\nattention mechanisms in vision-language models for enhancing explainability in\nreal-time autonomous driving.", "AI": {"tldr": "DriveBLIP2框架基于BLIP2-OPT架构，通过注意力图生成器提升自动驾驶场景中的解释质量。", "motivation": "现有视觉语言模型在复杂多目标环境中表现不佳，尤其在实时自动驾驶场景中难以快速识别关键对象。", "method": "提出注意力图生成器，突出关键视频帧中对驾驶决策重要的对象，以生成更清晰的相关解释。", "result": "在DRAMA数据集上评估显示，BLEU、ROUGE、CIDEr和SPICE分数显著优于基线模型。", "conclusion": "目标注意力机制在提升实时自动驾驶解释性方面具有潜力。"}}
{"id": "2506.23826", "pdf": "https://arxiv.org/pdf/2506.23826", "abs": "https://arxiv.org/abs/2506.23826", "authors": ["Lluís C. Coll", "Martin W. Lauer-Schmaltz", "Philip Cash", "John P. Hansen", "Anja Maier"], "title": "Towards the \"Digital Me\": A vision of authentic Conversational Agents powered by personal Human Digital Twins", "categories": ["cs.ET", "cs.AI", "cs.CY", "cs.HC", "cs.IR"], "comment": "24 pages, 9 figures", "summary": "Human Digital Twins (HDTs) have traditionally been conceptualized as\ndata-driven models designed to support decision-making across various domains.\nHowever, recent advancements in conversational AI open new possibilities for\nHDTs to function as authentic, interactive digital counterparts of individuals.\nThis paper introduces a novel HDT system architecture that integrates large\nlanguage models with dynamically updated personal data, enabling it to mirror\nan individual's conversational style, memories, and behaviors. To achieve this,\nour approach implements context-aware memory retrieval, neural\nplasticity-inspired consolidation, and adaptive learning mechanisms, creating a\nmore natural and evolving digital persona. The resulting system does not only\nreplicate an individual's unique conversational style depending on who they are\nspeaking with, but also enriches responses with dynamically captured personal\nexperiences, opinions, and memories. While this marks a significant step toward\ndeveloping authentic virtual counterparts, it also raises critical ethical\nconcerns regarding privacy, accountability, and the long-term implications of\npersistent digital identities. This study contributes to the field of HDTs by\ndescribing our novel system architecture, demonstrating its capabilities, and\ndiscussing future directions and emerging challenges to ensure the responsible\nand ethical development of HDTs.", "AI": {"tldr": "本文提出了一种新型人类数字孪生（HDT）系统架构，结合大型语言模型和动态更新的个人数据，模拟个体的对话风格、记忆和行为。", "motivation": "传统HDT是数据驱动的决策支持模型，但对话式AI的进步为HDT提供了成为真实交互式数字个体的可能性。", "method": "采用上下文感知记忆检索、神经可塑性启发巩固和自适应学习机制，构建自然且不断演化的数字人格。", "result": "系统不仅能根据对话对象复制个体的独特对话风格，还能通过动态捕捉的个人经历、观点和记忆丰富回应。", "conclusion": "研究推动了HDT的发展，但也引发隐私、责任和持久数字身份的伦理问题，需确保其负责任和伦理发展。"}}
{"id": "2506.22532", "pdf": "https://arxiv.org/pdf/2506.22532", "abs": "https://arxiv.org/abs/2506.22532", "authors": ["Mark Wrobel", "Michele Pascale", "Tina Yao", "Ruaraidh Campbell", "Elena Milano", "Michael Quail", "Jennifer Steeden", "Vivek Muthurangu"], "title": "High Resolution Isotropic 3D Cine imaging with Automated Segmentation using Concatenated 2D Real-time Imaging and Deep Learning", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": null, "summary": "Background: Conventional cardiovascular magnetic resonance (CMR) in\npaediatric and congenital heart disease uses 2D, breath-hold, balanced steady\nstate free precession (bSSFP) cine imaging for assessment of function and\ncardiac-gated, respiratory-navigated, static 3D bSSFP whole-heart imaging for\nanatomical assessment. Our aim is to concatenate a stack 2D free-breathing\nreal-time cines and use Deep Learning (DL) to create an isotropic a fully\nsegmented 3D cine dataset from these images. Methods: Four DL models were\ntrained on open-source data that performed: a) Interslice contrast correction;\nb) Interslice respiratory motion correction; c) Super-resolution (slice\ndirection); and d) Segmentation of right and left atria and ventricles (RA, LA,\nRV, and LV), thoracic aorta (Ao) and pulmonary arteries (PA). In 10 patients\nundergoing routine cardiovascular examination, our method was validated on\nprospectively acquired sagittal stacks of real-time cine images. Quantitative\nmetrics (ventricular volumes and vessel diameters) and image quality of the 3D\ncines were compared to conventional breath hold cine and whole heart imaging.\nResults: All real-time data were successfully transformed into 3D cines with a\ntotal post-processing time of <1 min in all cases. There were no significant\nbiases in any LV or RV metrics with reasonable limits of agreement and\ncorrelation. There is also reasonable agreement for all vessel diameters,\nalthough there was a small but significant overestimation of RPA diameter.\nConclusion: We have demonstrated the potential of creating a 3D-cine data from\nconcatenated 2D real-time cine images using a series of DL models. Our method\nhas short acquisition and reconstruction times with fully segmented data being\navailable within 2 minutes. The good agreement with conventional imaging\nsuggests that our method could help to significantly speed up CMR in clinical\npractice.", "AI": {"tldr": "利用深度学习将2D实时自由呼吸电影图像拼接并转换为3D电影数据集，验证了其在心血管磁共振中的潜力。", "motivation": "传统心血管磁共振（CMR）在儿科和先天性心脏病中使用2D呼吸门控电影成像和3D静态成像，但存在效率问题。本研究旨在通过深度学习技术快速生成3D电影数据集。", "method": "训练了四个深度学习模型，分别用于对比校正、呼吸运动校正、超分辨率和心脏结构分割。在10名患者中验证了方法的有效性。", "result": "所有实时数据成功转换为3D电影，处理时间小于1分钟。心室体积和血管直径与传统方法结果一致，但右肺动脉直径略有高估。", "conclusion": "该方法展示了利用深度学习从2D图像生成3D电影的潜力，有望显著提升临床CMR的效率。"}}
{"id": "2506.22568", "pdf": "https://arxiv.org/pdf/2506.22568", "abs": "https://arxiv.org/abs/2506.22568", "authors": ["Gladston Moreira", "Ivan Meneghini", "Elzabeth Wanner"], "title": "Maximum Dispersion, Maximum Concentration: Enhancing the Quality of MOP Solutions", "categories": ["math.OC", "cs.CV"], "comment": "11 pages", "summary": "Multi-objective optimization problems (MOPs) often require a trade-off\nbetween conflicting objectives, maximizing diversity and convergence in the\nobjective space. This study presents an approach to improve the quality of MOP\nsolutions by optimizing the dispersion in the decision space and the\nconvergence in a specific region of the objective space. Our approach defines a\nRegion of Interest (ROI) based on a cone representing the decision maker's\npreferences in the objective space, while enhancing the dispersion of solutions\nin the decision space using a uniformity measure. Combining solution\nconcentration in the objective space with dispersion in the decision space\nintensifies the search for Pareto-optimal solutions while increasing solution\ndiversity. When combined, these characteristics improve the quality of\nsolutions and avoid the bias caused by clustering solutions in a specific\nregion of the decision space. Preliminary experiments suggest that this method\nenhances multi-objective optimization by generating solutions that effectively\nbalance dispersion and concentration, thereby mitigating bias in the decision\nspace.", "AI": {"tldr": "提出了一种多目标优化方法，通过优化决策空间的分散性和目标空间特定区域的收敛性，提高解的质量。", "motivation": "多目标优化问题需要在冲突目标之间权衡，同时最大化目标空间的多样性和收敛性。现有方法可能在决策空间中产生偏差，本研究旨在解决这一问题。", "method": "定义基于锥形的兴趣区域（ROI）以反映决策者偏好，并使用均匀性度量增强决策空间的分散性。结合目标空间的收敛性和决策空间的分散性，优化帕累托解。", "result": "初步实验表明，该方法通过平衡分散性和集中性，有效提高了多目标优化的解质量，减少了决策空间的偏差。", "conclusion": "该方法通过结合目标空间的收敛性和决策空间的分散性，显著提升了多目标优化的解质量，避免了决策空间的偏差。"}}
{"id": "2506.23855", "pdf": "https://arxiv.org/pdf/2506.23855", "abs": "https://arxiv.org/abs/2506.23855", "authors": ["Travis Dick", "Alessandro Epasto", "Adel Javanmard", "Josh Karlin", "Andres Munoz Medina", "Vahab Mirrokni", "Sergei Vassilvitskii", "Peilin Zhong"], "title": "Differentially Private Synthetic Data Release for Topics API Outputs", "categories": ["cs.CR", "cs.AI", "cs.LG"], "comment": "20 pages, 8 figures", "summary": "The analysis of the privacy properties of Privacy-Preserving Ads APIs is an\narea of research that has received strong interest from academics, industry,\nand regulators. Despite this interest, the empirical study of these methods is\nhindered by the lack of publicly available data. Reliable empirical analysis of\nthe privacy properties of an API, in fact, requires access to a dataset\nconsisting of realistic API outputs; however, privacy concerns prevent the\ngeneral release of such data to the public.\n  In this work, we develop a novel methodology to construct synthetic API\noutputs that are simultaneously realistic enough to enable accurate study and\nprovide strong privacy protections. We focus on one Privacy-Preserving Ads\nAPIs: the Topics API, part of Google Chrome's Privacy Sandbox. We developed a\nmethodology to generate a differentially-private dataset that closely matches\nthe re-identification risk properties of the real Topics API data. The use of\ndifferential privacy provides strong theoretical bounds on the leakage of\nprivate user information from this release.\n  Our methodology is based on first computing a large number of\ndifferentially-private statistics describing how output API traces evolve over\ntime. Then, we design a parameterized distribution over sequences of API traces\nand optimize its parameters so that they closely match the statistics obtained.\nFinally, we create the synthetic data by drawing from this distribution.\n  Our work is complemented by an open-source release of the anonymized dataset\nobtained by this methodology. We hope this will enable external researchers to\nanalyze the API in-depth and replicate prior and future work on a realistic\nlarge-scale dataset. We believe that this work will contribute to fostering\ntransparency regarding the privacy properties of Privacy-Preserving Ads APIs.", "AI": {"tldr": "提出了一种生成合成API输出的方法，用于研究隐私保护广告API的隐私属性，同时保护用户隐私。", "motivation": "由于缺乏公开的真实数据，研究隐私保护广告API的隐私属性存在困难。", "method": "通过差分隐私技术生成合成数据，匹配真实API输出的重识别风险特性。", "result": "开发了开源数据集，支持外部研究，促进隐私保护API的透明度。", "conclusion": "该方法为隐私保护广告API的研究提供了可靠的数据支持，同时保护用户隐私。"}}
{"id": "2506.23869", "pdf": "https://arxiv.org/pdf/2506.23869", "abs": "https://arxiv.org/abs/2506.23869", "authors": ["Louis Bradshaw", "Honglu Fan", "Alexander Spangher", "Stella Biderman", "Simon Colton"], "title": "Scaling Self-Supervised Representation Learning for Symbolic Piano Performance", "categories": ["cs.SD", "cs.AI", "cs.LG", "eess.AS"], "comment": "ISMIR (2025)", "summary": "We study the capabilities of generative autoregressive transformer models\ntrained on large amounts of symbolic solo-piano transcriptions. After first\npretraining on approximately 60,000 hours of music, we use a comparatively\nsmaller, high-quality subset, to finetune models to produce musical\ncontinuations, perform symbolic classification tasks, and produce\ngeneral-purpose contrastive MIDI embeddings by adapting the SimCLR framework to\nsymbolic music. When evaluating piano continuation coherence, our generative\nmodel outperforms leading symbolic generation techniques and remains\ncompetitive with proprietary audio generation models. On MIR classification\nbenchmarks, frozen representations from our contrastive model achieve\nstate-of-the-art results in linear probe experiments, while direct finetuning\ndemonstrates the generalizability of pretrained representations, often\nrequiring only a few hundred labeled examples to specialize to downstream\ntasks.", "AI": {"tldr": "研究了基于大量钢琴符号转录的生成自回归Transformer模型，通过预训练和微调，在音乐生成和分类任务中表现优异。", "motivation": "探索生成自回归Transformer模型在符号音乐领域的潜力，特别是在音乐生成和分类任务中的应用。", "method": "预训练60,000小时音乐数据，微调高质量子集，使用SimCLR框架生成对比MIDI嵌入。", "result": "生成模型在钢琴音乐连贯性上优于现有符号生成技术，对比模型在分类任务中达到SOTA。", "conclusion": "预训练模型在符号音乐任务中表现出色，微调后能高效适应下游任务。"}}
{"id": "2506.23875", "pdf": "https://arxiv.org/pdf/2506.23875", "abs": "https://arxiv.org/abs/2506.23875", "authors": ["Yuta Sato", "Kazuhiko Kawamoto", "Hiroshi Kera"], "title": "Chain of Thought in Order: Discovering Learning-Friendly Orders for Arithmetic", "categories": ["cs.LG", "cs.AI"], "comment": "14 pages, 10 figures", "summary": "The chain of thought is fundamental in Transformers, which is to perform\nstep-by-step reasoning. Besides what intermediate steps work, the order of\nthese steps critically affects the difficulty of the reasoning. This study\naddresses a novel task of unraveling chain of thought - reordering decoder\ninput tokens to a learning-friendly sequence for Transformers to learn\narithmetic tasks. The proposed pipeline first trains a Transformer on a mixture\nof target sequences arranged in different orders and then identifies benign\norders as those with fast loss drops in the early stage. As the search space\ngrows factorially with sequence length, we propose a two-stage hierarchical\napproach for inter- and intra-block reordering. Experiments on four\norder-sensitive arithmetic tasks show that our method identifies a\nlearning-friendly order out of a few billion candidates. Notably, on the\nmultiplication task, it recovered the reverse-digit order reported in prior\nstudies.", "AI": {"tldr": "研究提出了一种重新排列解码器输入标记的方法，以优化Transformer在算术任务中的学习顺序。", "motivation": "探索中间步骤的顺序如何影响Transformer的推理难度，并寻找学习友好的顺序。", "method": "通过训练Transformer在不同顺序的目标序列上，识别早期损失下降快的顺序，并采用两阶段分层方法进行重新排序。", "result": "在四个顺序敏感的算术任务中，成功从数十亿候选顺序中识别出学习友好顺序，并在乘法任务中复现了先前研究的反向数字顺序。", "conclusion": "重新排列输入顺序可以显著提升Transformer在算术任务中的学习效果。"}}
{"id": "2506.22790", "pdf": "https://arxiv.org/pdf/2506.22790", "abs": "https://arxiv.org/abs/2506.22790", "authors": ["Yixu Chen", "Bowen Chen", "Hai Wei", "Alan C. Bovik", "Baojun Li", "Wei Sun", "Linhan Cao", "Kang Fu", "Dandan Zhu", "Jun Jia", "Menghan Hu", "Xiongkuo Min", "Guangtao Zhai", "Dounia Hammou", "Fei Yin", "Rafal Mantiuk", "Amritha Premkumar", "Prajit T Rajendran", "Vignesh V Menon"], "title": "ICME 2025 Generalizable HDR and SDR Video Quality Measurement Grand Challenge", "categories": ["eess.IV", "cs.CV", "cs.MM"], "comment": "ICME 2025 Grand Challenges", "summary": "This paper reports IEEE International Conference on Multimedia \\& Expo (ICME)\n2025 Grand Challenge on Generalizable HDR and SDR Video Quality Measurement.\nWith the rapid development of video technology, especially High Dynamic Range\n(HDR) and Standard Dynamic Range (SDR) contents, the need for robust and\ngeneralizable Video Quality Assessment (VQA) methods has become increasingly\ndemanded. Existing VQA models often struggle to deliver consistent performance\nacross varying dynamic ranges, distortion types, and diverse content. This\nchallenge was established to benchmark and promote VQA approaches capable of\njointly handling HDR and SDR content. In the final evaluation phase, five teams\nsubmitted seven models along with technical reports to the Full Reference (FR)\nand No Reference (NR) tracks. Among them, four methods outperformed VMAF\nbaseline, while the top-performing model achieved state-of-the-art performance,\nsetting a new benchmark for generalizable video quality assessment.", "AI": {"tldr": "ICME 2025挑战赛聚焦于开发通用的HDR和SDR视频质量评估方法，现有模型在动态范围和失真类型上表现不佳，最终有四个方法超越VMAF基准。", "motivation": "随着HDR和SDR视频技术的发展，需要更鲁棒和通用的视频质量评估方法。", "method": "挑战赛评估了五种团队的七个模型，分为全参考和无参考两个赛道。", "result": "四个方法表现优于VMAF基准，最佳模型达到最新技术水平。", "conclusion": "挑战赛为通用视频质量评估设立了新基准。"}}
{"id": "2506.22799", "pdf": "https://arxiv.org/pdf/2506.22799", "abs": "https://arxiv.org/abs/2506.22799", "authors": ["Minchao Jiang", "Shunyu Jia", "Jiaming Gu", "Xiaoyuan Lu", "Guangming Zhu", "Anqi Dong", "Liang Zhang"], "title": "VoteSplat: Hough Voting Gaussian Splatting for 3D Scene Understanding", "categories": ["cs.GR", "cs.CV", "cs.LG"], "comment": "Accepted to ICCV 2025", "summary": "3D Gaussian Splatting (3DGS) has become horsepower in high-quality, real-time\nrendering for novel view synthesis of 3D scenes. However, existing methods\nfocus primarily on geometric and appearance modeling, lacking deeper scene\nunderstanding while also incurring high training costs that complicate the\noriginally streamlined differentiable rendering pipeline. To this end, we\npropose VoteSplat, a novel 3D scene understanding framework that integrates\nHough voting with 3DGS. Specifically, Segment Anything Model (SAM) is utilized\nfor instance segmentation, extracting objects, and generating 2D vote maps. We\nthen embed spatial offset vectors into Gaussian primitives. These offsets\nconstruct 3D spatial votes by associating them with 2D image votes, while depth\ndistortion constraints refine localization along the depth axis. For\nopen-vocabulary object localization, VoteSplat maps 2D image semantics to 3D\npoint clouds via voting points, reducing training costs associated with\nhigh-dimensional CLIP features while preserving semantic unambiguity. Extensive\nexperiments demonstrate effectiveness of VoteSplat in open-vocabulary 3D\ninstance localization, 3D point cloud understanding, click-based 3D object\nlocalization, hierarchical segmentation, and ablation studies. Our code is\navailable at https://sy-ja.github.io/votesplat/", "AI": {"tldr": "VoteSplat结合Hough投票与3D高斯泼溅（3DGS），通过SAM实例分割和2D投票图实现3D场景理解，降低训练成本并提升语义明确性。", "motivation": "现有3DGS方法缺乏深层场景理解且训练成本高，需改进以支持高效、语义明确的3D场景分析。", "method": "利用SAM进行实例分割生成2D投票图，将空间偏移向量嵌入高斯基元，结合深度约束优化定位。", "result": "实验证明VoteSplat在开放词汇3D实例定位、点云理解等任务中高效且语义明确。", "conclusion": "VoteSplat为3D场景理解提供了一种高效、低成本的解决方案，适用于多种应用场景。"}}
{"id": "2506.23923", "pdf": "https://arxiv.org/pdf/2506.23923", "abs": "https://arxiv.org/abs/2506.23923", "authors": ["Miguel Camacho-Sánchez", "Fernando García-Torres", "Jesper John Lisegaard", "Rocío del Amor", "Sankhya Mohanty", "Valery Naranjo"], "title": "Reinforcement Learning for Synchronised Flow Control in a Dual-Gate Resin Infusion System", "categories": ["cs.LG", "cs.AI"], "comment": "11 pages, 4 figures, 45th Ris{\\o} International Symposium on\n  Materials Science", "summary": "Resin infusion (RI) and resin transfer moulding (RTM) are critical processes\nfor the manufacturing of high-performance fibre-reinforced polymer composites,\nparticularly for large-scale applications such as wind turbine blades.\nControlling the resin flow dynamics in these processes is critical to ensure\nthe uniform impregnation of the fibre reinforcements, thereby preventing\nresidual porosities and dry spots that impact the consequent structural\nintegrity of the final component. This paper presents a reinforcement learning\n(RL) based strategy, established using process simulations, for synchronising\nthe different resin flow fronts in an infusion scenario involving two resin\ninlets and a single outlet. Using Proximal Policy Optimisation (PPO), our\napproach addresses the challenge of managing the fluid dynamics in a partially\nobservable environment. The results demonstrate the effectiveness of the RL\napproach in achieving an accurate flow convergence, highlighting its potential\ntowards improving process control and product quality in composites\nmanufacturing.", "AI": {"tldr": "本文提出了一种基于强化学习（RL）的策略，用于同步树脂灌注过程中的多个流动前沿，以提升复合材料制造的工艺控制。", "motivation": "树脂灌注（RI）和树脂传递模塑（RTM）是制造高性能纤维增强复合材料的关键工艺，但树脂流动动态控制不当会导致孔隙和干斑，影响最终产品的结构完整性。", "method": "采用近端策略优化（PPO）的强化学习方法，通过过程模拟，管理两个树脂入口和一个出口的部分可观测环境中的流体动态。", "result": "结果表明，RL方法能有效实现流动前沿的精确收敛，提升工艺控制和产品质量。", "conclusion": "强化学习方法在复合材料制造中具有改善工艺控制和产品质量的潜力。"}}
{"id": "2506.22802", "pdf": "https://arxiv.org/pdf/2506.22802", "abs": "https://arxiv.org/abs/2506.22802", "authors": ["Hae Jin Song", "Laurent Itti"], "title": "Riemannian-Geometric Fingerprints of Generative Models", "categories": ["cs.LG", "cs.CR", "cs.CV", "I.2.6"], "comment": null, "summary": "Recent breakthroughs and rapid integration of generative models (GMs) have\nsparked interest in the problem of model attribution and their fingerprints.\nFor instance, service providers need reliable methods of authenticating their\nmodels to protect their IP, while users and law enforcement seek to verify the\nsource of generated content for accountability and trust. In addition, a\ngrowing threat of model collapse is arising, as more model-generated data are\nbeing fed back into sources (e.g., YouTube) that are often harvested for\ntraining (\"regurgitative training\"), heightening the need to differentiate\nsynthetic from human data. Yet, a gap still exists in understanding generative\nmodels' fingerprints, we believe, stemming from the lack of a formal framework\nthat can define, represent, and analyze the fingerprints in a principled way.\nTo address this gap, we take a geometric approach and propose a new definition\nof artifact and fingerprint of GMs using Riemannian geometry, which allows us\nto leverage the rich theory of differential geometry. Our new definition\ngeneralizes previous work (Song et al., 2024) to non-Euclidean manifolds by\nlearning Riemannian metrics from data and replacing the Euclidean distances and\nnearest-neighbor search with geodesic distances and kNN-based Riemannian center\nof mass. We apply our theory to a new gradient-based algorithm for computing\nthe fingerprints in practice. Results show that it is more effective in\ndistinguishing a large array of GMs, spanning across 4 different datasets in 2\ndifferent resolutions (64 by 64, 256 by 256), 27 model architectures, and 2\nmodalities (Vision, Vision-Language). Using our proposed definition\nsignificantly improves the performance on model attribution, as well as a\ngeneralization to unseen datasets, model types, and modalities, suggesting its\npractical efficacy.", "AI": {"tldr": "论文提出了一种基于黎曼几何的生成模型指纹定义方法，解决了现有方法在区分合成与人类数据方面的不足，并通过实验验证了其有效性。", "motivation": "随着生成模型的广泛应用，模型归属和指纹识别问题日益重要，但缺乏一个正式框架来定义和分析指纹。", "method": "采用黎曼几何方法，提出新的生成模型指纹定义，利用微分几何理论，并通过梯度算法计算指纹。", "result": "实验表明，该方法能有效区分多种生成模型，并在模型归属和泛化能力上表现优异。", "conclusion": "提出的黎曼几何方法为生成模型指纹识别提供了理论基础和实用工具，具有广泛的应用潜力。"}}
{"id": "2506.22826", "pdf": "https://arxiv.org/pdf/2506.22826", "abs": "https://arxiv.org/abs/2506.22826", "authors": ["Robert Beinert", "Jonas Bresch"], "title": "Denoising Multi-Color QR Codes and Stiefel-Valued Data by Relaxed Regularizations", "categories": ["math.OC", "cs.CV", "cs.NA", "math.NA", "94A08, 94A12, 65J22, 90C22, 90C25"], "comment": "9 pages, 2 figures, 3 algorithms", "summary": "The handling of manifold-valued data, for instance, plays a central role in\ncolor restoration tasks relying on circle- or sphere-valued color models, in\nthe study of rotational or directional information related to the special\northogonal group, and in Gaussian image processing, where the pixel statistics\nare interpreted as values on the hyperbolic sheet. Especially, to denoise these\nkind of data, there have been proposed several generalizations of total\nvariation (TV) and Tikhonov-type denoising models incorporating the underlying\nmanifolds. Recently, a novel, numerically efficient denoising approach has been\nintroduced, where the data are embedded in an Euclidean ambient space, the\nnon-convex manifolds are encoded by a series of positive semi-definite,\nfixed-rank matrices, and the rank constraint is relaxed to obtain a\nconvexification that can be solved using standard algorithms from convex\nanalysis. The aim of the present paper is to extent this approach to new kinds\nof data like multi-binary and Stiefel-valued data. Multi-binary data can, for\ninstance, be used to model multi-color QR codes whereas Stiefel-valued data\noccur in image and video-based recognition. For both new data types, we propose\nTV- and Tikhonov-based denoising modelstogether with easy-to-solve\nconvexification. All derived methods are evaluated on proof-of-concept,\nsynthetic experiments.", "AI": {"tldr": "论文提出了一种新的高效去噪方法，适用于多二元和Stiefel值数据，扩展了现有的流形值数据处理技术。", "motivation": "处理流形值数据在颜色恢复、旋转方向信息研究和高斯图像处理中至关重要，但现有方法需要进一步扩展以适应多二元和Stiefel值数据。", "method": "通过将数据嵌入欧几里得环境空间，使用半正定矩阵编码非凸流形，并松弛秩约束以实现凸化，提出TV和Tikhonov去噪模型。", "result": "在概念验证和合成实验中验证了所提方法的有效性。", "conclusion": "该方法为多二元和Stiefel值数据的去噪提供了高效且易于实现的解决方案。"}}
{"id": "2506.23934", "pdf": "https://arxiv.org/pdf/2506.23934", "abs": "https://arxiv.org/abs/2506.23934", "authors": ["Xiangchen Li", "Saeid Ghafouri", "Bo Ji", "Hans Vandierendonck", "Deepu John", "Dimitrios S. Nikolopoulos"], "title": "QPART: Adaptive Model Quantization and Dynamic Workload Balancing for Accuracy-aware Edge Inference", "categories": ["cs.DC", "cs.AI", "cs.LG", "cs.PF"], "comment": null, "summary": "As machine learning inferences increasingly move to edge devices, adapting to\ndiverse computational capabilities, hardware, and memory constraints becomes\nmore critical. Instead of relying on a pre-trained model fixed for all future\ninference queries across diverse edge devices, we argue that planning an\ninference pattern with a request-specific model tailored to the device's\ncomputational capacity, accuracy requirements, and time constraints is more\ncost-efficient and robust to diverse scenarios. To this end, we propose an\naccuracy-aware and workload-balanced inference system that integrates joint\nmodel quantization and inference partitioning. In this approach, the server\ndynamically responds to inference queries by sending a quantized model and\nadaptively sharing the inference workload with the device. Meanwhile, the\ndevice's computational power, channel capacity, and accuracy requirements are\nconsidered when deciding.\n  Furthermore, we introduce a new optimization framework for the inference\nsystem, incorporating joint model quantization and partitioning. Our approach\noptimizes layer-wise quantization bit width and partition points to minimize\ntime consumption and cost while accounting for varying accuracy requirements of\ntasks through an accuracy degradation metric in our optimization model. To our\nknowledge, this work represents the first exploration of optimizing\nquantization layer-wise bit-width in the inference serving system, by\nintroducing theoretical measurement of accuracy degradation. Simulation results\ndemonstrate a substantial reduction in overall time and power consumption, with\ncomputation payloads decreasing by over 80% and accuracy degradation kept below\n1%.", "AI": {"tldr": "提出了一种动态适应边缘设备计算能力的推理系统，结合模型量化和推理分区，显著降低时间和功耗。", "motivation": "适应边缘设备的多样化计算能力和资源限制，提高推理效率和成本效益。", "method": "提出联合模型量化和推理分区的优化框架，动态调整量化位宽和分区点。", "result": "计算负载减少80%以上，精度损失低于1%。", "conclusion": "该方法在边缘设备上实现了高效、低成本的推理，具有广泛适用性。"}}
{"id": "2506.22882", "pdf": "https://arxiv.org/pdf/2506.22882", "abs": "https://arxiv.org/abs/2506.22882", "authors": ["Qilong Xing", "Zikai Song", "Yuteng Ye", "Yuke Chen", "Youjia Zhang", "Na Feng", "Junqing Yu", "Wei Yang"], "title": "CA-Diff: Collaborative Anatomy Diffusion for Brain Tissue Segmentation", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "ICME 2025", "summary": "Segmentation of brain structures from MRI is crucial for evaluating brain\nmorphology, yet existing CNN and transformer-based methods struggle to\ndelineate complex structures accurately. While current diffusion models have\nshown promise in image segmentation, they are inadequate when applied directly\nto brain MRI due to neglecting anatomical information. To address this, we\npropose Collaborative Anatomy Diffusion (CA-Diff), a framework integrating\nspatial anatomical features to enhance segmentation accuracy of the diffusion\nmodel. Specifically, we introduce distance field as an auxiliary anatomical\ncondition to provide global spatial context, alongside a collaborative\ndiffusion process to model its joint distribution with anatomical structures,\nenabling effective utilization of anatomical features for segmentation.\nFurthermore, we introduce a consistency loss to refine relationships between\nthe distance field and anatomical structures and design a time adapted channel\nattention module to enhance the U-Net feature fusion procedure. Extensive\nexperiments show that CA-Diff outperforms state-of-the-art (SOTA) methods.", "AI": {"tldr": "提出了一种结合解剖学特征的扩散模型CA-Diff，用于提高脑MRI分割的准确性。", "motivation": "现有CNN和基于Transformer的方法在复杂脑结构分割上表现不佳，且扩散模型直接应用于脑MRI时忽略了解剖学信息。", "method": "引入距离场作为辅助解剖条件，设计协作扩散过程建模其联合分布，并提出一致性损失和时间适应通道注意力模块。", "result": "实验表明CA-Diff优于现有最优方法。", "conclusion": "CA-Diff通过整合解剖学特征显著提升了脑MRI分割的准确性。"}}
{"id": "2506.23944", "pdf": "https://arxiv.org/pdf/2506.23944", "abs": "https://arxiv.org/abs/2506.23944", "authors": ["Fuhang Kuang", "Jiacheng You", "Yingdong Hu", "Tong Zhang", "Chuan Wen", "Yang Gao"], "title": "Adapt Your Body: Mitigating Proprioception Shifts in Imitation Learning", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Imitation learning models for robotic tasks typically rely on multi-modal\ninputs, such as RGB images, language, and proprioceptive states. While\nproprioception is intuitively important for decision-making and obstacle\navoidance, simply incorporating all proprioceptive states leads to a surprising\ndegradation in imitation learning performance. In this work, we identify the\nunderlying issue as the proprioception shift problem, where the distributions\nof proprioceptive states diverge significantly between training and deployment.\nTo address this challenge, we propose a domain adaptation framework that\nbridges the gap by utilizing rollout data collected during deployment. Using\nWasserstein distance, we quantify the discrepancy between expert and rollout\nproprioceptive states and minimize this gap by adding noise to both sets of\nstates, proportional to the Wasserstein distance. This strategy enhances\nrobustness against proprioception shifts by aligning the training and\ndeployment distributions. Experiments on robotic manipulation tasks demonstrate\nthe efficacy of our method, enabling the imitation policy to leverage\nproprioception while mitigating its adverse effects. Our approach outperforms\nthe naive solution which discards proprioception, and other baselines designed\nto address distributional shifts.", "AI": {"tldr": "论文提出了一种解决模仿学习中本体感受偏移问题的领域适应框架，通过Wasserstein距离量化并最小化训练与部署时的分布差异，提升了性能。", "motivation": "模仿学习中直接使用所有本体感受状态会导致性能下降，原因是训练与部署时的本体感受状态分布存在显著差异。", "method": "提出一种领域适应框架，利用部署时收集的数据，通过Wasserstein距离量化分布差异，并通过添加噪声来最小化差异。", "result": "实验表明，该方法在机器人操作任务中有效，优于直接丢弃本体感受或其他基线方法。", "conclusion": "通过领域适应框架解决了本体感受偏移问题，使模仿策略能够有效利用本体感受信息。"}}
{"id": "2506.22952", "pdf": "https://arxiv.org/pdf/2506.22952", "abs": "https://arxiv.org/abs/2506.22952", "authors": ["Yanwu Yang", "Thomas Wolfers"], "title": "Hierarchical Characterization of Brain Dynamics via State Space-based Vector Quantization", "categories": ["eess.IV", "cs.CV", "q-bio.NC"], "comment": null, "summary": "Understanding brain dynamics through functional Magnetic Resonance Imaging\n(fMRI) remains a fundamental challenge in neuroscience, particularly in\ncapturing how the brain transitions between various functional states.\nRecently, metastability, which refers to temporarily stable brain states, has\noffered a promising paradigm to quantify complex brain signals into\ninterpretable, discretized representations. In particular, compared to\ncluster-based machine learning approaches, tokenization approaches leveraging\nvector quantization have shown promise in representation learning with powerful\nreconstruction and predictive capabilities. However, most existing methods\nignore brain transition dependencies and lack a quantification of brain\ndynamics into representative and stable embeddings. In this study, we propose a\nHierarchical State space-based Tokenization network, termed HST, which\nquantizes brain states and transitions in a hierarchical structure based on a\nstate space-based model. We introduce a refined clustered Vector-Quantization\nVariational AutoEncoder (VQ-VAE) that incorporates quantization error feedback\nand clustering to improve quantization performance while facilitating\nmetastability with representative and stable token representations. We validate\nour HST on two public fMRI datasets, demonstrating its effectiveness in\nquantifying the hierarchical dynamics of the brain and its potential in disease\ndiagnosis and reconstruction performance. Our method offers a promising\nframework for the characterization of brain dynamics, facilitating the analysis\nof metastability.", "AI": {"tldr": "该研究提出了一种名为HST的分层状态空间标记化网络，用于量化大脑状态和动态，并通过改进的VQ-VAE方法提升表征学习能力。", "motivation": "理解大脑动态是神经科学的核心挑战，现有方法在捕捉状态转换依赖性和生成稳定表征方面存在不足。", "method": "采用分层状态空间模型和聚类VQ-VAE，结合量化误差反馈和聚类，生成稳定且具代表性的标记。", "result": "在公开fMRI数据集上验证了HST的有效性，展示了其在疾病诊断和重建性能上的潜力。", "conclusion": "HST为大脑动态表征提供了新框架，有助于分析大脑的亚稳态特性。"}}
{"id": "2506.23952", "pdf": "https://arxiv.org/pdf/2506.23952", "abs": "https://arxiv.org/abs/2506.23952", "authors": ["Stefan Buijsman", "Sarah Carter", "Juan Pablo Bermúdez"], "title": "Autonomy by Design: Preserving Human Autonomy in AI Decision-Support", "categories": ["cs.HC", "cs.AI", "cs.LG", "econ.GN", "q-fin.EC"], "comment": null, "summary": "AI systems increasingly support human decision-making across domains of\nprofessional, skill-based, and personal activity. While previous work has\nexamined how AI might affect human autonomy globally, the effects of AI on\ndomain-specific autonomy -- the capacity for self-governed action within\ndefined realms of skill or expertise -- remain understudied. We analyze how AI\ndecision-support systems affect two key components of domain-specific autonomy:\nskilled competence (the ability to make informed judgments within one's domain)\nand authentic value-formation (the capacity to form genuine domain-relevant\nvalues and preferences). By engaging with prior investigations and analyzing\nempirical cases across medical, financial, and educational domains, we\ndemonstrate how the absence of reliable failure indicators and the potential\nfor unconscious value shifts can erode domain-specific autonomy both\nimmediately and over time. We then develop a constructive framework for\nautonomy-preserving AI support systems. We propose specific socio-technical\ndesign patterns -- including careful role specification, implementation of\ndefeater mechanisms, and support for reflective practice -- that can help\nmaintain domain-specific autonomy while leveraging AI capabilities. This\nframework provides concrete guidance for developing AI systems that enhance\nrather than diminish human agency within specialized domains of action.", "AI": {"tldr": "论文探讨了AI决策支持系统如何影响领域特定自主性，提出了一个保护自主性的框架。", "motivation": "研究AI对领域特定自主性的影响，填补现有研究的空白。", "method": "通过分析医学、金融和教育领域的案例，探讨AI对技能能力和真实价值形成的影响。", "result": "发现缺乏可靠的失败指标和潜在的无意识价值转变会削弱自主性。", "conclusion": "提出了一种保护自主性的AI支持系统框架，包括角色规范、失败机制和反思实践等设计模式。"}}
{"id": "2506.22973", "pdf": "https://arxiv.org/pdf/2506.22973", "abs": "https://arxiv.org/abs/2506.22973", "authors": ["AmirHossein Naghi Razlighi", "Elaheh Badali Golezani", "Shohreh Kasaei"], "title": "Confident Splatting: Confidence-Based Compression of 3D Gaussian Splatting via Learnable Beta Distributions", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "3D Gaussian Splatting enables high-quality real-time rendering but often\nproduces millions of splats, resulting in excessive storage and computational\noverhead. We propose a novel lossy compression method based on learnable\nconfidence scores modeled as Beta distributions. Each splat's confidence is\noptimized through reconstruction-aware losses, enabling pruning of\nlow-confidence splats while preserving visual fidelity. The proposed approach\nis architecture-agnostic and can be applied to any Gaussian Splatting variant.\nIn addition, the average confidence values serve as a new metric to assess the\nquality of the scene. Extensive experiments demonstrate favorable trade-offs\nbetween compression and fidelity compared to prior work. Our code and data are\npublicly available at\nhttps://github.com/amirhossein-razlighi/Confident-Splatting", "AI": {"tldr": "提出一种基于可学习Beta分布置信度分数的3D高斯泼溅压缩方法，减少存储和计算开销，同时保持视觉保真度。", "motivation": "解决3D高斯泼溅技术中因生成数百万个泼溅点导致的存储和计算资源过高的问题。", "method": "通过基于Beta分布的可学习置信度分数优化每个泼溅点，利用重建感知损失进行剪枝，保留高置信度泼溅点。", "result": "实验表明该方法在压缩和保真度之间取得了优于先前工作的平衡。", "conclusion": "该方法架构无关，适用于任何高斯泼溅变体，并提供了一种新的场景质量评估指标。"}}
{"id": "2506.23960", "pdf": "https://arxiv.org/pdf/2506.23960", "abs": "https://arxiv.org/abs/2506.23960", "authors": ["Mingfei Cheng", "Xiaofei Xie", "Renzhi Wang", "Yuan Zhou", "Ming Hu"], "title": "ADReFT: Adaptive Decision Repair for Safe Autonomous Driving via Reinforcement Fine-Tuning", "categories": ["cs.LG", "cs.AI", "cs.SE"], "comment": null, "summary": "Autonomous Driving Systems (ADSs) continue to face safety-critical risks due\nto the inherent limitations in their design and performance capabilities.\nOnline repair plays a crucial role in mitigating such limitations, ensuring the\nruntime safety and reliability of ADSs. Existing online repair solutions\nenforce ADS compliance by transforming unacceptable trajectories into\nacceptable ones based on predefined specifications, such as rule-based\nconstraints or training datasets. However, these approaches often lack\ngeneralizability, adaptability and tend to be overly conservative, resulting in\nineffective repairs that not only fail to mitigate safety risks sufficiently\nbut also degrade the overall driving experience. To address this issue, we\npropose Adaptive Decision Repair (ADReFT), a novel and effective repair method\nthat identifies safety-critical states through offline learning from failed\ntests and generates appropriate mitigation actions to improve ADS safety.\nSpecifically, ADReFT incorporates a transformer-based model with two joint\nheads, State Monitor and Decision Adapter, designed to capture complex driving\nenvironment interactions to evaluate state safety severity and generate\nadaptive repair actions. Given the absence of oracles for state safety\nidentification, we first pretrain ADReFT using supervised learning with coarse\nannotations, i.e., labeling states preceding violations as positive samples and\nothers as negative samples. It establishes ADReFT's foundational capability to\nmitigate safety-critical violations, though it may result in somewhat\nconservative mitigation strategies. Therefore, we subsequently finetune ADReFT\nusing reinforcement learning to improve its initial capability and generate\nmore precise and contextually appropriate repair decisions. Our evaluation\nresults illustrate that ADReFT achieves better repair performance.", "AI": {"tldr": "论文提出了一种名为ADReFT的自适应决策修复方法，通过离线学习和在线修复提升自动驾驶系统的安全性和可靠性。", "motivation": "现有在线修复方法缺乏通用性和适应性，导致修复效果不佳，甚至影响驾驶体验。", "method": "ADReFT结合了基于Transformer的模型，通过状态监控和决策适配器生成自适应修复动作，并采用监督学习和强化学习进行训练。", "result": "ADReFT在修复性能上表现优于现有方法。", "conclusion": "ADReFT通过自适应修复策略显著提升了自动驾驶系统的安全性和可靠性。"}}
{"id": "2506.23995", "pdf": "https://arxiv.org/pdf/2506.23995", "abs": "https://arxiv.org/abs/2506.23995", "authors": ["Mingfei Cheng", "Renzhi Wang", "Xiaofei Xie", "Yuan Zhou", "Lei Ma"], "title": "STCLocker: Deadlock Avoidance Testing for Autonomous Driving Systems", "categories": ["cs.SE", "cs.AI", "cs.RO"], "comment": null, "summary": "Autonomous Driving System (ADS) testing is essential to ensure the safety and\nreliability of autonomous vehicles (AVs) before deployment. However, existing\ntechniques primarily focus on evaluating ADS functionalities in single-AV\nsettings. As ADSs are increasingly deployed in multi-AV traffic, it becomes\ncrucial to assess their cooperative performance, particularly regarding\ndeadlocks, a fundamental coordination failure in which multiple AVs enter a\ncircular waiting state indefinitely, resulting in motion planning failures.\nDespite its importance, the cooperative capability of ADSs to prevent deadlocks\nremains insufficiently underexplored. To address this gap, we propose the first\ndedicated Spatio-Temporal Conflict-Guided Deadlock Avoidance Testing technique,\nSTCLocker, for generating DeadLock Scenarios (DLSs), where a group of AVs\ncontrolled by the ADS under test are in a circular wait state. STCLocker\nconsists of three key components: Deadlock Oracle, Conflict Feedback, and\nConflict-aware Scenario Generation. Deadlock Oracle provides a reliable\nblack-box mechanism for detecting deadlock cycles among multiple AVs within a\ngiven scenario. Conflict Feedback and Conflict-aware Scenario Generation\ncollaborate to actively guide AVs into simultaneous competition over spatial\nconflict resources (i.e., shared passing regions) and temporal competitive\nbehaviors (i.e., reaching the conflict region at the same time), thereby\nincreasing the effectiveness of generating conflict-prone deadlocks. We\nevaluate STCLocker on two types of ADSs: Roach, an end-to-end ADS, and OpenCDA,\na module-based ADS supporting cooperative communication. Experimental results\nshow that, on average, STCLocker generates more DLS than the best-performing\nbaseline.", "AI": {"tldr": "STCLocker是一种用于测试多自动驾驶车辆（AVs）协同性能的技术，专注于生成死锁场景（DLSs），以提高自动驾驶系统（ADS）的可靠性。", "motivation": "现有技术主要关注单AV环境下的ADS功能评估，而多AV交通中的协同性能（尤其是死锁问题）尚未充分研究。", "method": "STCLocker包含三个关键组件：死锁检测器（Deadlock Oracle）、冲突反馈（Conflict Feedback）和冲突感知场景生成（Conflict-aware Scenario Generation），通过引导AVs竞争共享资源来生成死锁场景。", "result": "实验表明，STCLocker在生成DLSs方面优于现有基线方法，适用于端到端和模块化ADS。", "conclusion": "STCLocker填补了多AV协同性能测试的空白，为ADS的可靠性提供了新的评估工具。"}}
{"id": "2506.23016", "pdf": "https://arxiv.org/pdf/2506.23016", "abs": "https://arxiv.org/abs/2506.23016", "authors": ["Tomás Silva Santos Rocha", "Anastasiia Mikhailova", "Moreno I. Coco", "José Santos-Victor"], "title": "Deep Learning in Mild Cognitive Impairment Diagnosis using Eye Movements and Image Content in Visual Memory Tasks", "categories": ["cs.HC", "cs.CV"], "comment": "13 pages, 5 figures", "summary": "The global prevalence of dementia is projected to double by 2050,\nhighlighting the urgent need for scalable diagnostic tools. This study utilizes\ndigital cognitive tasks with eye-tracking data correlated with memory processes\nto distinguish between Healthy Controls (HC) and Mild Cognitive Impairment\n(MCI), a precursor to dementia. A deep learning model based on VTNet was\ntrained using eye-tracking data from 44 participants (24 MCI, 20 HCs) who\nperformed a visual memory task. The model utilizes both time series and spatial\ndata derived from eye-tracking. It was modified to incorporate scan paths, heat\nmaps, and image content. These modifications also enabled testing parameters\nsuch as image resolution and task performance, analyzing their impact on model\nperformance. The best model, utilizing $700\\times700px$ resolution heatmaps,\nachieved 68% sensitivity and 76% specificity. Despite operating under more\nchallenging conditions (e.g., smaller dataset size, shorter task duration, or a\nless standardized task), the model's performance is comparable to an\nAlzheimer's study using similar methods (70% sensitivity and 73% specificity).\nThese findings contribute to the development of automated diagnostic tools for\nMCI. Future work should focus on refining the model and using a standardized\nlong-term visual memory task.", "AI": {"tldr": "研究利用眼动数据和深度学习模型VTNet区分健康人群与轻度认知障碍（MCI），模型性能与现有研究相当。", "motivation": "全球痴呆症患病率预计到2050年将翻倍，亟需可扩展的诊断工具。", "method": "使用眼动数据训练VTNet模型，结合时间序列和空间数据，并测试图像分辨率和任务表现对模型的影响。", "result": "最佳模型在700×700像素热图下达到68%敏感性和76%特异性，性能与类似研究相当。", "conclusion": "研究为MCI自动诊断工具开发提供了基础，未来需优化模型并采用标准化任务。"}}
{"id": "2506.24009", "pdf": "https://arxiv.org/pdf/2506.24009", "abs": "https://arxiv.org/abs/2506.24009", "authors": ["Xinquan Wang", "Fenghao Zhu", "Zhaohui Yang", "Chongwen Huang", "Xiaoming Chen", "Zhaoyang Zhang", "Sami Muhaidat", "Mérouane Debbah"], "title": "Bridging Physical and Digital Worlds: Embodied Large AI for Future Wireless Systems", "categories": ["cs.IT", "cs.AI", "math.IT"], "comment": "7 pages, 4 figures", "summary": "Large artificial intelligence (AI) models offer revolutionary potential for\nfuture wireless systems, promising unprecedented capabilities in network\noptimization and performance. However, current paradigms largely overlook\ncrucial physical interactions. This oversight means they primarily rely on\noffline datasets, leading to difficulties in handling real-time wireless\ndynamics and non-stationary environments. Furthermore, these models often lack\nthe capability for active environmental probing. This paper proposes a\nfundamental paradigm shift towards wireless embodied large AI (WELAI), moving\nfrom passive observation to active embodiment. We first identify key challenges\nfaced by existing models, then we explore the design principles and system\nstructure of WELAI. Besides, we outline prospective applications in\nnext-generation wireless. Finally, through an illustrative case study, we\ndemonstrate the effectiveness of WELAI and point out promising research\ndirections for realizing adaptive, robust, and autonomous wireless systems.", "AI": {"tldr": "论文提出了一种名为WELAI的新范式，旨在通过主动感知和交互解决现有AI模型在无线系统中的局限性。", "motivation": "现有AI模型在无线系统中主要依赖离线数据，难以应对实时动态和非平稳环境，且缺乏主动探测能力。", "method": "论文提出了WELAI的设计原则和系统结构，并通过案例研究验证其有效性。", "result": "WELAI展示了在下一代无线系统中实现自适应、鲁棒和自主的潜力。", "conclusion": "WELAI为无线系统提供了一种新的研究方向，有望实现更高效的网络优化和性能提升。"}}
{"id": "2506.23041", "pdf": "https://arxiv.org/pdf/2506.23041", "abs": "https://arxiv.org/abs/2506.23041", "authors": ["Chengyu Dong", "Huan Gui", "Noveen Sachdeva", "Long Jin", "Ke Yin", "Jingbo Shang", "Lichan Hong", "Ed H. Chi", "Zhe Zhao"], "title": "ReMem: Mutual Information-Aware Fine-tuning of Pretrained Vision Transformers for Effective Knowledge Distillation", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Knowledge distillation from pretrained visual representation models offers an\neffective approach to improve small, task-specific production models. However,\nthe effectiveness of such knowledge transfer drops significantly when\ndistilling from strong models that are pretrained in a large scale. In this\npaper, we address this challenge for pretrained Vision Transformers (ViTs) by\nexploring methods to fine-tune them for more effective knowledge transfer.\nMotivated by the connection between mutual information and distillation\neffectiveness, we propose to employ mutual information-aware optimization\nduring finetuning. For small or highly-imbalanced downstream datasets where\nsuch optimization becomes less effective, we introduce a simple yet effective\nheuristic of reweighting MLP blocks. This approach is inspired by our\nobservation that top MLP blocks are primarily responsible for mutual\ninformation loss. Our method enables small student models to benefit from those\npretrained models among the strongest.", "AI": {"tldr": "本文提出了一种改进视觉Transformer（ViT）知识蒸馏的方法，通过互信息感知优化和MLP块重加权，解决了大规模预训练模型在小规模或不平衡数据集上知识转移效果下降的问题。", "motivation": "大规模预训练的视觉Transformer（ViT）在知识蒸馏时效果下降，尤其是在小规模或不平衡数据集上。本文旨在通过优化互信息和调整MLP块权重来提升知识转移效果。", "method": "提出互信息感知优化方法，并在小规模或不平衡数据集上引入MLP块重加权策略，以提升知识蒸馏效果。", "result": "该方法使得小规模学生模型能够从最强预训练模型中受益，提升了知识转移的效果。", "conclusion": "通过互信息感知优化和MLP块重加权，本文成功改进了ViT的知识蒸馏效果，尤其是在小规模或不平衡数据集上。"}}
{"id": "2506.24018", "pdf": "https://arxiv.org/pdf/2506.24018", "abs": "https://arxiv.org/abs/2506.24018", "authors": ["Veronica Lachi", "Francesco Ferrini", "Antonio Longa", "Bruno Lepri", "Andrea Passerini", "Manfred Jaeger"], "title": "Bridging Theory and Practice in Link Representation with Graph Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Graph Neural Networks (GNNs) are widely used to compute representations of\nnode pairs for downstream tasks such as link prediction. Yet, theoretical\nunderstanding of their expressive power has focused almost entirely on\ngraph-level representations. In this work, we shift the focus to links and\nprovide the first comprehensive study of GNN expressiveness in link\nrepresentation. We introduce a unifying framework, the $k_\\phi$-$k_\\rho$-$m$\nframework, that subsumes existing message-passing link models and enables\nformal expressiveness comparisons. Using this framework, we derive a hierarchy\nof state-of-the-art methods and offer theoretical tools to analyze future\narchitectures. To complement our analysis, we propose a synthetic evaluation\nprotocol comprising the first benchmark specifically designed to assess\nlink-level expressiveness. Finally, we ask: does expressiveness matter in\npractice? We use a graph symmetry metric that quantifies the difficulty of\ndistinguishing links and show that while expressive models may underperform on\nstandard benchmarks, they significantly outperform simpler ones as symmetry\nincreases, highlighting the need for dataset-aware model selection.", "AI": {"tldr": "该论文首次全面研究了GNN在链接表示中的表达能力，提出了一个统一框架$k_\\phi$-$k_\\rho$-$m$，并建立了方法层次结构。通过合成评估协议和对称性指标，发现表达能力强的模型在对称性高的数据中表现更优。", "motivation": "现有研究主要关注图级表示，而对GNN在链接表示中的表达能力缺乏系统性研究。", "method": "提出$k_\\phi$-$k_\\rho$-$m$框架，统一现有消息传递链接模型，并建立方法层次结构。设计合成评估协议和对称性指标。", "result": "表达能力强的模型在对称性高的数据中表现显著优于简单模型。", "conclusion": "研究强调了数据集感知的模型选择的重要性，并为未来架构分析提供了理论工具。"}}
{"id": "2506.23102", "pdf": "https://arxiv.org/pdf/2506.23102", "abs": "https://arxiv.org/abs/2506.23102", "authors": ["Sunggu Kyung", "Jinyoung Seo", "Hyunseok Lim", "Dongyeong Kim", "Hyungbin Park", "Jimin Sung", "Jihyun Kim", "Wooyoung Jo", "Yoojin Nam", "Namkug Kim"], "title": "MedRegion-CT: Region-Focused Multimodal LLM for Comprehensive 3D CT Report Generation", "categories": ["eess.IV", "cs.CV"], "comment": "14 pages, 5 figures, submitted to ICCV 2025", "summary": "The recent release of RadGenome-Chest CT has significantly advanced CT-based\nreport generation. However, existing methods primarily focus on global\nfeatures, making it challenging to capture region-specific details, which may\ncause certain abnormalities to go unnoticed. To address this, we propose\nMedRegion-CT, a region-focused Multi-Modal Large Language Model (MLLM)\nframework, featuring three key innovations. First, we introduce Region\nRepresentative ($R^2$) Token Pooling, which utilizes a 2D-wise pretrained\nvision model to efficiently extract 3D CT features. This approach generates\nglobal tokens representing overall slice features and region tokens\nhighlighting target areas, enabling the MLLM to process comprehensive\ninformation effectively. Second, a universal segmentation model generates\npseudo-masks, which are then processed by a mask encoder to extract\nregion-centric features. This allows the MLLM to focus on clinically relevant\nregions, using six predefined region masks. Third, we leverage segmentation\nresults to extract patient-specific attributions, including organ size,\ndiameter, and locations. These are converted into text prompts, enriching the\nMLLM's understanding of patient-specific contexts. To ensure rigorous\nevaluation, we conducted benchmark experiments on report generation using the\nRadGenome-Chest CT. MedRegion-CT achieved state-of-the-art performance,\noutperforming existing methods in natural language generation quality and\nclinical relevance while maintaining interpretability. The code for our\nframework is publicly available.", "AI": {"tldr": "MedRegion-CT提出了一种区域聚焦的多模态大语言模型框架，通过区域代表性标记池化、通用分割模型和患者特定属性提取，显著提升了CT报告生成的性能。", "motivation": "现有方法主要关注全局特征，难以捕捉区域特异性细节，可能导致异常被忽略。", "method": "1. 引入区域代表性标记池化（$R^2$ Token Pooling）提取3D CT特征；2. 使用通用分割模型生成伪掩码并提取区域特征；3. 提取患者特定属性并转换为文本提示。", "result": "在RadGenome-Chest CT上，MedRegion-CT在自然语言生成质量和临床相关性方面表现最优，同时保持可解释性。", "conclusion": "MedRegion-CT通过区域聚焦方法显著提升了CT报告生成的性能，代码已公开。"}}
{"id": "2506.23145", "pdf": "https://arxiv.org/pdf/2506.23145", "abs": "https://arxiv.org/abs/2506.23145", "authors": ["Shahad Hardan", "Darya Taratynova", "Abdelmajid Essofi", "Karthik Nandakumar", "Mohammad Yaqub"], "title": "Forget-MI: Machine Unlearning for Forgetting Multimodal Information in Healthcare Settings", "categories": ["cs.LG", "cs.CR", "cs.CV"], "comment": null, "summary": "Privacy preservation in AI is crucial, especially in healthcare, where models\nrely on sensitive patient data. In the emerging field of machine unlearning,\nexisting methodologies struggle to remove patient data from trained multimodal\narchitectures, which are widely used in healthcare. We propose Forget-MI, a\nnovel machine unlearning method for multimodal medical data, by establishing\nloss functions and perturbation techniques. Our approach unlearns unimodal and\njoint representations of the data requested to be forgotten while preserving\nknowledge from the remaining data and maintaining comparable performance to the\noriginal model. We evaluate our results using performance on the forget\ndataset, performance on the test dataset, and Membership Inference Attack\n(MIA), which measures the attacker's ability to distinguish the forget dataset\nfrom the training dataset. Our model outperforms the existing approaches that\naim to reduce MIA and the performance on the forget dataset while keeping an\nequivalent performance on the test set. Specifically, our approach reduces MIA\nby 0.202 and decreases AUC and F1 scores on the forget set by 0.221 and 0.305,\nrespectively. Additionally, our performance on the test set matches that of the\nretrained model, while allowing forgetting. Code is available at\nhttps://github.com/BioMedIA-MBZUAI/Forget-MI.git", "AI": {"tldr": "Forget-MI是一种新型的多模态医疗数据机器学习遗忘方法，通过损失函数和扰动技术，有效移除敏感数据，同时保持模型性能。", "motivation": "医疗AI中隐私保护至关重要，但现有方法难以从多模态架构中移除患者数据。", "method": "提出Forget-MI方法，通过损失函数和扰动技术移除需遗忘数据的单模态和联合表示。", "result": "Forget-MI显著降低成员推理攻击（MIA）和遗忘数据集性能，同时测试集性能与原模型相当。", "conclusion": "Forget-MI在多模态医疗数据遗忘中表现优异，平衡了隐私保护与模型性能。"}}
{"id": "2506.24081", "pdf": "https://arxiv.org/pdf/2506.24081", "abs": "https://arxiv.org/abs/2506.24081", "authors": ["Rahul Kumar", "Wenqi Wei", "Ying Mao", "Junaid Farooq", "Ying Wang", "Juntao Chen"], "title": "SQUASH: A SWAP-Based Quantum Attack to Sabotage Hybrid Quantum Neural Networks", "categories": ["quant-ph", "cs.AI", "cs.LG"], "comment": "Keywords: Quantum Machine Learning, Hybrid Quantum Neural Networks,\n  SWAP Test, Fidelity, Circuit-level Attack", "summary": "We propose a circuit-level attack, SQUASH, a SWAP-Based Quantum Attack to\nsabotage Hybrid Quantum Neural Networks (HQNNs) for classification tasks.\nSQUASH is executed by inserting SWAP gate(s) into the variational quantum\ncircuit of the victim HQNN. Unlike conventional noise-based or adversarial\ninput attacks, SQUASH directly manipulates the circuit structure, leading to\nqubit misalignment and disrupting quantum state evolution. This attack is\nhighly stealthy, as it does not require access to training data or introduce\ndetectable perturbations in input states. Our results demonstrate that SQUASH\nsignificantly degrades classification performance, with untargeted SWAP attacks\nreducing accuracy by up to 74.08\\% and targeted SWAP attacks reducing target\nclass accuracy by up to 79.78\\%. These findings reveal a critical vulnerability\nin HQNN implementations, underscoring the need for more resilient architectures\nagainst circuit-level adversarial interventions.", "AI": {"tldr": "提出了一种名为SQUASH的电路级攻击，通过插入SWAP门破坏混合量子神经网络（HQNN）的分类性能。", "motivation": "揭示HQNN在电路层面的脆弱性，强调需要更鲁棒的架构抵御此类攻击。", "method": "在受害HQNN的变分量子电路中插入SWAP门，导致量子比特错位和状态演化中断。", "result": "非目标攻击使分类准确率下降74.08%，目标攻击使目标类准确率下降79.78%。", "conclusion": "SQUASH攻击暴露了HQNN的关键漏洞，需开发更抗干扰的架构。"}}
{"id": "2506.23147", "pdf": "https://arxiv.org/pdf/2506.23147", "abs": "https://arxiv.org/abs/2506.23147", "authors": ["Jonathan Schuster", "Fabian Transchel"], "title": "maneuverRecognition -- A Python package for Timeseries Classification in the domain of Vehicle Telematics", "categories": ["cs.LG", "cs.CV"], "comment": "6 pages, 2 figures", "summary": "In the domain of vehicle telematics the automated recognition of driving\nmaneuvers is used to classify and evaluate driving behaviour. This not only\nserves as a component to enhance the personalization of insurance policies, but\nalso to increase road safety, reduce accidents and the associated costs as well\nas to reduce fuel consumption and support environmentally friendly driving. In\nthis context maneuver recognition technically requires a continuous application\nof time series classification which poses special challenges to the transfer,\npreprocessing and storage of telematic sensor data, the training of predictive\nmodels, and the prediction itself. Although much research has been done in the\nfield of gathering relevant data or regarding the methods to build predictive\nmodels for the task of maneuver recognition, there is a practical need for\npython packages and functions that allow to quickly transform data into the\nrequired structure as well as to build and evaluate such models. The\nmaneuverRecognition package was therefore developed to provide the necessary\nfunctions for preprocessing, modelling and evaluation and also includes a ready\nto use LSTM based network structure that can be modified. The implementation of\nthe package is demonstrated using real driving data of three different persons\nrecorded via smartphone sensors.", "AI": {"tldr": "论文介绍了maneuverRecognition包，用于车辆遥测中的驾驶动作识别，支持数据预处理、建模和评估，并提供了一个可修改的LSTM网络结构。", "motivation": "驾驶动作识别可用于个性化保险政策、提高道路安全、减少事故和燃料消耗，但目前缺乏快速处理数据和构建模型的工具。", "method": "开发了maneuverRecognition包，包含预处理、建模和评估功能，并提供了一个基于LSTM的网络结构。", "result": "使用智能手机传感器记录的三人真实驾驶数据验证了该包的有效性。", "conclusion": "maneuverRecognition包为驾驶动作识别提供了实用的工具，支持快速数据处理和模型构建。"}}
{"id": "2506.24093", "pdf": "https://arxiv.org/pdf/2506.24093", "abs": "https://arxiv.org/abs/2506.24093", "authors": ["Paul Wachter", "Lukas Niehaus", "Julius Schöning"], "title": "Development of Hybrid Artificial Intelligence Training on Real and Synthetic Data: Benchmark on Two Mixed Training Strategies", "categories": ["cs.LG", "cs.AI", "I.2.1; I.2.0; F.2.3"], "comment": "21pages, 14 figures, 2 tables", "summary": "Synthetic data has emerged as a cost-effective alternative to real data for\ntraining artificial neural networks (ANN). However, the disparity between\nsynthetic and real data results in a domain gap. That gap leads to poor\nperformance and generalization of the trained ANN when applied to real-world\nscenarios. Several strategies have been developed to bridge this gap, which\ncombine synthetic and real data, known as mixed training using hybrid datasets.\nWhile these strategies have been shown to mitigate the domain gap, a systematic\nevaluation of their generalizability and robustness across various tasks and\narchitectures remains underexplored. To address this challenge, our study\ncomprehensively analyzes two widely used mixing strategies on three prevalent\narchitectures and three distinct hybrid datasets. From these datasets, we\nsample subsets with varying proportions of synthetic to real data to\ninvestigate the impact of synthetic and real components. The findings of this\npaper provide valuable insights into optimizing the use of synthetic data in\nthe training process of any ANN, contributing to enhancing robustness and\nefficacy.", "AI": {"tldr": "本文系统评估了混合训练策略在不同任务和架构中的泛化性和鲁棒性，为优化合成数据在人工神经网络训练中的应用提供了见解。", "motivation": "合成数据是训练人工神经网络的低成本替代方案，但与真实数据之间的领域差距导致性能不佳。研究旨在填补混合训练策略系统评估的空白。", "method": "分析两种混合策略在三种架构和三种混合数据集上的表现，通过调整合成与真实数据的比例研究其影响。", "result": "研究结果为优化合成数据在训练中的应用提供了有价值的见解，增强了模型的鲁棒性和有效性。", "conclusion": "混合训练策略能有效缓解领域差距，研究为人工神经网络训练中合成数据的使用提供了优化方向。"}}
{"id": "2506.23208", "pdf": "https://arxiv.org/pdf/2506.23208", "abs": "https://arxiv.org/abs/2506.23208", "authors": ["Runtian Yuan", "Qingqiu Li", "Junlin Hou", "Jilan Xu", "Yuejie Zhang", "Rui Feng", "Hao Chen"], "title": "Multi-Source COVID-19 Detection via Variance Risk Extrapolation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "We present our solution for the Multi-Source COVID-19 Detection Challenge,\nwhich aims to classify chest CT scans into COVID and Non-COVID categories\nacross data collected from four distinct hospitals and medical centers. A major\nchallenge in this task lies in the domain shift caused by variations in imaging\nprotocols, scanners, and patient populations across institutions. To enhance\nthe cross-domain generalization of our model, we incorporate Variance Risk\nExtrapolation (VREx) into the training process. VREx encourages the model to\nmaintain consistent performance across multiple source domains by explicitly\nminimizing the variance of empirical risks across environments. This\nregularization strategy reduces overfitting to center-specific features and\npromotes learning of domain-invariant representations. We further apply Mixup\ndata augmentation to improve generalization and robustness. Mixup interpolates\nboth the inputs and labels of randomly selected pairs of training samples,\nencouraging the model to behave linearly between examples and enhancing its\nresilience to noise and limited data. Our method achieves an average macro F1\nscore of 0.96 across the four sources on the validation set, demonstrating\nstrong generalization.", "AI": {"tldr": "提出了一种结合VREx和Mixup的方法，用于多源COVID-19检测任务，显著提升了跨域泛化能力。", "motivation": "解决多源CT扫描数据中因成像协议、扫描仪和患者群体差异导致的域偏移问题。", "method": "采用VREx减少跨域风险方差，结合Mixup数据增强提升泛化和鲁棒性。", "result": "在验证集上平均宏F1得分为0.96，表现优异。", "conclusion": "该方法有效提升了模型在多源数据上的泛化性能。"}}
{"id": "2506.23221", "pdf": "https://arxiv.org/pdf/2506.23221", "abs": "https://arxiv.org/abs/2506.23221", "authors": ["Bálint Horváth", "Balázs Csanád Csáji"], "title": "Single Image Inpainting and Super-Resolution with Simultaneous Uncertainty Guarantees by Universal Reproducing Kernels", "categories": ["cs.LG", "cs.CV"], "comment": "23 pages, 8 figures, 6 tables", "summary": "The paper proposes a statistical learning approach to the problem of\nestimating missing pixels of images, crucial for image inpainting and\nsuper-resolution problems. One of the main novelties of the method is that it\nalso provides uncertainty quantifications together with the estimated values.\nOur core assumption is that the underlying data-generating function comes from\na Reproducing Kernel Hilbert Space (RKHS). A special emphasis is put on\nband-limited functions, central to signal processing, which form Paley-Wiener\ntype RKHSs. The proposed method, which we call Simultaneously Guaranteed Kernel\nInterpolation (SGKI), is an extension and refinement of a recently developed\nkernel method. An advantage of SGKI is that it not only estimates the missing\npixels, but also builds non-asymptotic confidence bands for the unobserved\nvalues, which are simultaneously guaranteed for all missing pixels. We also\nshow how to compute these bands efficiently using Schur complements, we discuss\na generalization to vector-valued functions, and we present a series of\nnumerical experiments on various datasets containing synthetically generated\nand benchmark images, as well.", "AI": {"tldr": "论文提出了一种基于统计学习的方法SGKI，用于图像修复和超分辨率问题中的缺失像素估计，并提供不确定性量化。", "motivation": "解决图像修复和超分辨率中缺失像素估计问题，并量化估计的不确定性。", "method": "基于Reproducing Kernel Hilbert Space (RKHS)假设，提出SGKI方法，扩展了现有核方法，利用Schur补高效计算非渐近置信带。", "result": "SGKI不仅能估计缺失像素，还能为所有缺失像素构建同时保证的非渐近置信带，并通过实验验证了其有效性。", "conclusion": "SGKI方法在图像修复和超分辨率任务中表现出色，提供了可靠的像素估计和不确定性量化。"}}
{"id": "2506.24108", "pdf": "https://arxiv.org/pdf/2506.24108", "abs": "https://arxiv.org/abs/2506.24108", "authors": ["Shai Yehezkel", "Omer Dahary", "Andrey Voynov", "Daniel Cohen-Or"], "title": "Navigating with Annealing Guidance Scale in Diffusion Space", "categories": ["cs.GR", "cs.AI", "cs.CV", "cs.LG"], "comment": "Project page:\n  https://annealing-guidance.github.io/annealing-guidance/", "summary": "Denoising diffusion models excel at generating high-quality images\nconditioned on text prompts, yet their effectiveness heavily relies on careful\nguidance during the sampling process. Classifier-Free Guidance (CFG) provides a\nwidely used mechanism for steering generation by setting the guidance scale,\nwhich balances image quality and prompt alignment. However, the choice of the\nguidance scale has a critical impact on the convergence toward a visually\nappealing and prompt-adherent image. In this work, we propose an annealing\nguidance scheduler which dynamically adjusts the guidance scale over time based\non the conditional noisy signal. By learning a scheduling policy, our method\naddresses the temperamental behavior of CFG. Empirical results demonstrate that\nour guidance scheduler significantly enhances image quality and alignment with\nthe text prompt, advancing the performance of text-to-image generation.\nNotably, our novel scheduler requires no additional activations or memory\nconsumption, and can seamlessly replace the common classifier-free guidance,\noffering an improved trade-off between prompt alignment and quality.", "AI": {"tldr": "提出了一种动态调整引导尺度的调度器，显著提升了文本到图像生成的质量和提示对齐。", "motivation": "现有的无分类器引导（CFG）方法在生成过程中对引导尺度的选择非常敏感，影响了图像质量和提示对齐。", "method": "设计了一种基于条件噪声信号的动态引导尺度调度器，通过学习调度策略优化CFG的行为。", "result": "实验结果表明，该方法显著提升了图像质量和提示对齐，且无需额外计算资源。", "conclusion": "提出的调度器能够无缝替代传统CFG，在提示对齐和质量之间提供了更好的平衡。"}}
{"id": "2506.23259", "pdf": "https://arxiv.org/pdf/2506.23259", "abs": "https://arxiv.org/abs/2506.23259", "authors": ["Lachin Naghashyar"], "title": "Improving Myocardial Infarction Detection via Synthetic ECG Pretraining", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Myocardial infarction is a major cause of death globally, and accurate early\ndiagnosis from electrocardiograms (ECGs) remains a clinical priority. Deep\nlearning models have shown promise for automated ECG interpretation, but\nrequire large amounts of labeled data, which are often scarce in practice. We\npropose a physiology-aware pipeline that (i) synthesizes 12-lead ECGs with\ntunable MI morphology and realistic noise, and (ii) pre-trains recurrent and\ntransformer classifiers with self-supervised masked-autoencoding plus a joint\nreconstruction-classification objective. We validate the realism of synthetic\nECGs via statistical and visual analysis, confirming that key morphological\nfeatures are preserved. Pretraining on synthetic data consistently improved\nclassification performance, particularly in low-data settings, with AUC gains\nof up to 4 percentage points. These results show that controlled synthetic ECGs\ncan help improve MI detection when real clinical data is limited.", "AI": {"tldr": "提出一种生理感知的ECG合成与预训练方法，提升心肌梗死检测性能，尤其在数据稀缺时。", "motivation": "心肌梗死是全球主要死因，现有深度学习模型依赖大量标注数据，但实际中数据稀缺。", "method": "合成可调MI形态的12导联ECG，结合自监督掩码自编码和联合重建分类目标预训练模型。", "result": "合成ECG保留关键形态特征，预训练提升分类性能（AUC最高提升4个百分点）。", "conclusion": "可控合成ECG可有效改善数据稀缺时的心肌梗死检测。"}}
{"id": "2506.24120", "pdf": "https://arxiv.org/pdf/2506.24120", "abs": "https://arxiv.org/abs/2506.24120", "authors": ["Yuqing Wang", "Shangding Gu"], "title": "Data Uniformity Improves Training Efficiency and More, with a Convergence Framework Beyond the NTK Regime", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "comment": null, "summary": "Data selection plays a crucial role in data-driven decision-making, including\nin large language models (LLMs), and is typically task-dependent. Properties\nsuch as data quality and diversity have been extensively studied and are known\nto enhance model performance. However, it remains unclear whether there exist\nother quantitative and general principles of data selection that can\nconsistently improve performance, especially for complex tasks with limited\nprior knowledge. In this paper, we demonstrate that selecting more uniformly\ndistributed data can improve training efficiency while enhancing performance.\nSpecifically, we establish that more uniform (less biased) distribution leads\nto a larger minimum pairwise distance between data points, denoted by\n$h_{\\min}$, and prove that a smaller $h_{\\min}$ can slow down the training\ndynamics of gradient descent (GD). Moreover, we theoretically show that the\napproximation error of neural networks decreases as $h_{\\min}$ increases. Our\nanalysis introduces a convergence framework for GD beyond the Neural Tangent\nKernel (NTK) regime, applicable to a broad class of architectures, including\ntransformers, without requiring Lipschitz smoothness. This framework further\nprovides theoretical justification for the use of residual connections and\nfunction compositions in deep neural architectures. In the end, we conduct\ncomprehensive experiments for supervised fine-tuning across various settings,\nincluding different optimization strategies, model sizes, and training\ndatasets. The results consistently demonstrate that selecting data by\nmaximizing pairwise distance significantly accelerates training and achieves\ncomparable or better performance in LLMs across diverse datasets. Code and\nDatasets are available at the link:\nhttps://github.com/SafeRL-Lab/data-uniformity.", "AI": {"tldr": "论文提出了一种基于数据均匀分布的选择方法，通过最大化数据点间的最小距离（$h_{\\min}$）来提升训练效率和模型性能。", "motivation": "研究动机在于探索是否存在通用的数据选择原则，能够在不依赖任务先验知识的情况下提升复杂任务的性能。", "method": "方法包括理论证明数据均匀分布（更大的$h_{\\min}$）能加速梯度下降（GD）训练动态，并降低神经网络的近似误差。", "result": "实验结果表明，通过最大化数据点间距离选择数据，能显著加速训练并在多种数据集上取得更好或相当的性能。", "conclusion": "结论是数据均匀分布是一种有效的通用选择原则，适用于包括Transformer在内的多种架构，且无需Lipschitz平滑性假设。"}}
{"id": "2506.23305", "pdf": "https://arxiv.org/pdf/2506.23305", "abs": "https://arxiv.org/abs/2506.23305", "authors": ["Rachit Saluja", "Arzu Kovanlikaya", "Candace Chien", "Lauren Kathryn Blatt", "Jeffrey M. Perlman", "Stefan Worgall", "Mert R. Sabuncu", "Jonathan P. Dyke"], "title": "BPD-Neo: An MRI Dataset for Lung-Trachea Segmentation with Clinical Data for Neonatal Bronchopulmonary Dysplasia", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Bronchopulmonary dysplasia (BPD) is a common complication among preterm\nneonates, with portable X-ray imaging serving as the standard diagnostic\nmodality in neonatal intensive care units (NICUs). However, lung magnetic\nresonance imaging (MRI) offers a non-invasive alternative that avoids sedation\nand radiation while providing detailed insights into the underlying mechanisms\nof BPD. Leveraging high-resolution 3D MRI data, advanced image processing and\nsemantic segmentation algorithms can be developed to assist clinicians in\nidentifying the etiology of BPD. In this dataset, we present MRI scans paired\nwith corresponding semantic segmentations of the lungs and trachea for 40\nneonates, the majority of whom are diagnosed with BPD. The imaging data consist\nof free-breathing 3D stack-of-stars radial gradient echo acquisitions, known as\nthe StarVIBE series. Additionally, we provide comprehensive clinical data and\nbaseline segmentation models, validated against clinical assessments, to\nsupport further research and development in neonatal lung imaging.", "AI": {"tldr": "论文提出利用高分辨率3D MRI和语义分割算法辅助诊断支气管肺发育不良（BPD），并提供了40名新生儿的MRI扫描和分割数据。", "motivation": "传统便携式X射线诊断BPD存在辐射和镇静问题，MRI提供无创替代方案，并能更详细揭示BPD机制。", "method": "使用自由呼吸3D StarVIBE序列MRI数据，结合语义分割算法处理图像。", "result": "提供了40名BPD患儿的MRI扫描和分割数据，验证了基线分割模型的临床适用性。", "conclusion": "MRI和语义分割技术为BPD诊断提供了新工具，支持进一步研究和开发。"}}
{"id": "2506.23309", "pdf": "https://arxiv.org/pdf/2506.23309", "abs": "https://arxiv.org/abs/2506.23309", "authors": ["Yiming Huang", "Long Bai", "Beilei Cui", "Kun Yuan", "Guankun Wang", "Mobarakol Islam", "Nicolas Padoy", "Nassir Navab", "Hongliang Ren"], "title": "SurgTPGS: Semantic 3D Surgical Scene Understanding with Text Promptable Gaussian Splatting", "categories": ["eess.IV", "cs.CV"], "comment": "MICCAI 2025. Project Page:\n  https://lastbasket.github.io/MICCAI-2025-SurgTPGS/", "summary": "In contemporary surgical research and practice, accurately comprehending 3D\nsurgical scenes with text-promptable capabilities is particularly crucial for\nsurgical planning and real-time intra-operative guidance, where precisely\nidentifying and interacting with surgical tools and anatomical structures is\nparamount. However, existing works focus on surgical vision-language model\n(VLM), 3D reconstruction, and segmentation separately, lacking support for\nreal-time text-promptable 3D queries. In this paper, we present SurgTPGS, a\nnovel text-promptable Gaussian Splatting method to fill this gap. We introduce\na 3D semantics feature learning strategy incorporating the Segment Anything\nmodel and state-of-the-art vision-language models. We extract the segmented\nlanguage features for 3D surgical scene reconstruction, enabling a more\nin-depth understanding of the complex surgical environment. We also propose\nsemantic-aware deformation tracking to capture the seamless deformation of\nsemantic features, providing a more precise reconstruction for both texture and\nsemantic features. Furthermore, we present semantic region-aware optimization,\nwhich utilizes regional-based semantic information to supervise the training,\nparticularly promoting the reconstruction quality and semantic smoothness. We\nconduct comprehensive experiments on two real-world surgical datasets to\ndemonstrate the superiority of SurgTPGS over state-of-the-art methods,\nhighlighting its potential to revolutionize surgical practices. SurgTPGS paves\nthe way for developing next-generation intelligent surgical systems by\nenhancing surgical precision and safety. Our code is available at:\nhttps://github.com/lastbasket/SurgTPGS.", "AI": {"tldr": "SurgTPGS是一种新型的文本提示高斯泼溅方法，填补了实时文本提示3D查询的空白，结合了3D语义特征学习和语义感知变形跟踪，提升了手术场景重建的精确性和语义平滑性。", "motivation": "在手术规划和实时术中指导中，准确理解3D手术场景并支持文本提示功能至关重要，但现有方法缺乏实时文本提示3D查询的支持。", "method": "结合Segment Anything模型和先进的视觉语言模型，提出3D语义特征学习策略、语义感知变形跟踪和语义区域感知优化。", "result": "在两个真实手术数据集上的实验表明，SurgTPGS优于现有方法，显著提升了重建质量和语义平滑性。", "conclusion": "SurgTPGS通过增强手术精确性和安全性，为下一代智能手术系统的发展奠定了基础。"}}
{"id": "2506.23316", "pdf": "https://arxiv.org/pdf/2506.23316", "abs": "https://arxiv.org/abs/2506.23316", "authors": ["Zhenghao Peng", "Yuxin Liu", "Bolei Zhou"], "title": "InfGen: Scenario Generation as Next Token Group Prediction", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Realistic and interactive traffic simulation is essential for training and\nevaluating autonomous driving systems. However, most existing data-driven\nsimulation methods rely on static initialization or log-replay data, limiting\ntheir ability to model dynamic, long-horizon scenarios with evolving agent\npopulations. We propose InfGen, a scenario generation framework that outputs\nagent states and trajectories in an autoregressive manner. InfGen represents\nthe entire scene as a sequence of tokens, including traffic light signals,\nagent states, and motion vectors, and uses a transformer model to simulate\ntraffic over time. This design enables InfGen to continuously insert new agents\ninto traffic, supporting infinite scene generation. Experiments demonstrate\nthat InfGen produces realistic, diverse, and adaptive traffic behaviors.\nFurthermore, reinforcement learning policies trained in InfGen-generated\nscenarios achieve superior robustness and generalization, validating its\nutility as a high-fidelity simulation environment for autonomous driving. More\ninformation is available at https://metadriverse.github.io/infgen/.", "AI": {"tldr": "InfGen是一个基于Transformer的交通场景生成框架，支持动态、长期场景模拟，并能持续插入新车辆，生成多样且真实的交通行为。", "motivation": "现有数据驱动的交通模拟方法依赖静态初始化或日志回放数据，难以模拟动态、长期场景和变化的交通参与者。", "method": "InfGen将整个场景表示为包含交通信号、车辆状态和运动向量的令牌序列，并通过Transformer模型进行自回归模拟。", "result": "实验表明InfGen能生成真实、多样且自适应的交通行为，且在其生成的场景中训练的强化学习策略具有更强的鲁棒性和泛化能力。", "conclusion": "InfGen是一个高保真度的自动驾驶模拟环境，适用于训练和评估自动驾驶系统。"}}
{"id": "2506.23466", "pdf": "https://arxiv.org/pdf/2506.23466", "abs": "https://arxiv.org/abs/2506.23466", "authors": ["Qiqing Liu", "Guoquan Wei", "Zekun Zhou", "Yiyang Wen", "Liu Shi", "Qiegen Liu"], "title": "FD-DiT: Frequency Domain-Directed Diffusion Transformer for Low-Dose CT Reconstruction", "categories": ["eess.IV", "cs.CV", "physics.med-ph"], "comment": "11pages, 11 figures", "summary": "Low-dose computed tomography (LDCT) reduces radiation exposure but suffers\nfrom image artifacts and loss of detail due to quantum and electronic noise,\npotentially impacting diagnostic accuracy. Transformer combined with diffusion\nmodels has been a promising approach for image generation. Nevertheless,\nexisting methods exhibit limitations in preserving finegrained image details.\nTo address this issue, frequency domain-directed diffusion transformer (FD-DiT)\nis proposed for LDCT reconstruction. FD-DiT centers on a diffusion strategy\nthat progressively introduces noise until the distribution statistically aligns\nwith that of LDCT data, followed by denoising processing. Furthermore, we\nemploy a frequency decoupling technique to concentrate noise primarily in\nhigh-frequency domain, thereby facilitating effective capture of essential\nanatomical structures and fine details. A hybrid denoising network is then\nutilized to optimize the overall data reconstruction process. To enhance the\ncapability in recognizing high-frequency noise, we incorporate sliding sparse\nlocal attention to leverage the sparsity and locality of shallow-layer\ninformation, propagating them via skip connections for improving feature\nrepresentation. Finally, we propose a learnable dynamic fusion strategy for\noptimal component integration. Experimental results demonstrate that at\nidentical dose levels, LDCT images reconstructed by FD-DiT exhibit superior\nnoise and artifact suppression compared to state-of-the-art methods.", "AI": {"tldr": "FD-DiT是一种基于频率域导向的扩散变换器方法，用于低剂量CT图像重建，通过噪声逐步引入和去噪处理，结合频率解耦技术和混合去噪网络，显著提升了图像细节保留和噪声抑制能力。", "motivation": "低剂量CT（LDCT）虽减少辐射暴露，但图像因噪声和伪影导致细节丢失，影响诊断准确性。现有方法在保留细粒度细节方面存在局限。", "method": "提出FD-DiT，采用扩散策略逐步引入噪声至与LDCT数据统计对齐，结合频率解耦技术聚焦高频噪声，使用混合去噪网络优化重建，并通过滑动稀疏局部注意力增强高频噪声识别。", "result": "实验表明，FD-DiT在相同剂量下，噪声和伪影抑制优于现有方法。", "conclusion": "FD-DiT通过频率域导向和动态融合策略，显著提升了LDCT图像重建质量。"}}
{"id": "2506.23471", "pdf": "https://arxiv.org/pdf/2506.23471", "abs": "https://arxiv.org/abs/2506.23471", "authors": ["Thanh-Tung Phan-Nguyen", "Khoi-Nguyen Nguyen-Ngoc", "Tam V. Nguyen", "Minh-Triet Tran", "Trung-Nghia Le"], "title": "KiseKloset: Comprehensive System For Outfit Retrieval, Recommendation, And Try-On", "categories": ["cs.IR", "cs.CV"], "comment": null, "summary": "The global fashion e-commerce industry has become integral to people's daily\nlives, leveraging technological advancements to offer personalized shopping\nexperiences, primarily through recommendation systems that enhance customer\nengagement through personalized suggestions. To improve customers' experience\nin online shopping, we propose a novel comprehensive KiseKloset system for\noutfit retrieval, recommendation, and try-on. We explore two approaches for\noutfit retrieval: similar item retrieval and text feedback-guided item\nretrieval. Notably, we introduce a novel transformer architecture designed to\nrecommend complementary items from diverse categories. Furthermore, we enhance\nthe overall performance of the search pipeline by integrating approximate\nalgorithms to optimize the search process. Additionally, addressing the crucial\nneeds of online shoppers, we employ a lightweight yet efficient virtual try-on\nframework capable of real-time operation, memory efficiency, and maintaining\nrealistic outputs compared to its predecessors. This virtual try-on module\nempowers users to visualize specific garments on themselves, enhancing the\ncustomers' experience and reducing costs associated with damaged items for\nretailers. We deployed our end-to-end system for online users to test and\nprovide feedback, enabling us to measure their satisfaction levels. The results\nof our user study revealed that 84% of participants found our comprehensive\nsystem highly useful, significantly improving their online shopping experience.", "AI": {"tldr": "论文提出了一种名为KiseKloset的系统，用于服装检索、推荐和虚拟试穿，通过两种检索方法和新型Transformer架构提升用户体验，并引入轻量级虚拟试穿框架。用户测试显示84%的参与者认为系统非常有用。", "motivation": "提升全球时尚电商行业的个性化购物体验，通过推荐系统和虚拟试穿技术增强客户参与度和满意度。", "method": "1. 两种服装检索方法：相似物品检索和文本反馈引导检索；2. 新型Transformer架构用于跨品类互补物品推荐；3. 近似算法优化搜索流程；4. 轻量级虚拟试穿框架，支持实时操作和高效内存使用。", "result": "用户研究显示84%的参与者对系统高度满意，显著改善了在线购物体验。", "conclusion": "KiseKloset系统通过综合技术和用户反馈，有效提升了时尚电商的个性化服务和用户体验。"}}
{"id": "2506.23484", "pdf": "https://arxiv.org/pdf/2506.23484", "abs": "https://arxiv.org/abs/2506.23484", "authors": ["Yuzhuo Chen", "Zehua Ma", "Han Fang", "Weiming Zhang", "Nenghai Yu"], "title": "TAG-WM: Tamper-Aware Generative Image Watermarking via Diffusion Inversion Sensitivity", "categories": ["cs.MM", "cs.CV", "eess.IV", "I.3.3; I.4.9"], "comment": "Accepted by ICCV 2025 (2025 IEEE/CVF International Conference on\n  Computer Vision)", "summary": "AI-generated content (AIGC) enables efficient visual creation but raises\ncopyright and authenticity risks. As a common technique for integrity\nverification and source tracing, digital image watermarking is regarded as a\npotential solution to above issues. Among these, watermarking methods capable\nof preserving the generation quality are receiving increased attention.\nHowever, the proliferation and high performance of generative image editing\napplications have elevated the risks of malicious tampering, creating new\ndemands. 1) The tamper robustness of current lossless visual quality watermarks\nremains constrained by the modification-sensitive diffusion inversion process,\nnecessitating enhanced robustness. 2) The improved tampering quality and rapid\niteration cycles render passive tampering detection methods inadequate, making\nproactive tampering localization capability a desired feature for watermarks.\nTo address these requirements, this paper proposes a Tamper-Aware Generative\nimage WaterMarking method named TAG-WM. The proposed method comprises four key\nmodules: a dual-mark joint sampling (DMJS) algorithm for embedding copyright\nand localization watermarks into the latent space while preserving generative\nquality, the watermark latent reconstruction (WLR) utilizing reversed DMJS, a\ndense variation region detector (DVRD) leveraging diffusion inversion\nsensitivity to identify tampered areas via statistical deviation analysis, and\nthe tamper-aware decoding (TAD) guided by localization results. The\nexperimental results indicate that TAG-WM achieves SOTA tampering robustness\nand tampering localization capability with distortions while maintaining\nlossless generation quality and a considerable capacity of 256 bits.", "AI": {"tldr": "论文提出了一种名为TAG-WM的防篡改生成图像水印方法，解决了AIGC中的版权和真实性风险，同时保持生成质量和增强篡改鲁棒性。", "motivation": "AI生成内容（AIGC）带来高效视觉创作的同时，也引发版权和真实性风险。现有无损视觉质量水印的篡改鲁棒性不足，且被动篡改检测方法难以应对高性能生成图像编辑工具的威胁。", "method": "TAG-WM包含四个关键模块：双标记联合采样（DMJS）、水印潜在重构（WLR）、密集变化区域检测器（DVRD）和篡改感知解码（TAD）。", "result": "实验表明，TAG-WM在保持无损生成质量和256位容量的同时，实现了最先进的篡改鲁棒性和篡改定位能力。", "conclusion": "TAG-WM为AIGC提供了一种有效的版权保护和篡改检测解决方案，兼具高质量生成和强鲁棒性。"}}
{"id": "2506.23537", "pdf": "https://arxiv.org/pdf/2506.23537", "abs": "https://arxiv.org/abs/2506.23537", "authors": ["Xinyue Li", "Zhangkai Ni", "Wenhan Yang"], "title": "AFUNet: Cross-Iterative Alignment-Fusion Synergy for HDR Reconstruction via Deep Unfolding Paradigm", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted to International Conference on Computer Vision (ICCV) 2025", "summary": "Existing learning-based methods effectively reconstruct HDR images from\nmulti-exposure LDR inputs with extended dynamic range and improved detail, but\nthey rely more on empirical design rather than theoretical foundation, which\ncan impact their reliability. To address these limitations, we propose the\ncross-iterative Alignment and Fusion deep Unfolding Network (AFUNet), where HDR\nreconstruction is systematically decoupled into two interleaved subtasks --\nalignment and fusion -- optimized through alternating refinement, achieving\nsynergy between the two subtasks to enhance the overall performance. Our method\nformulates multi-exposure HDR reconstruction from a Maximum A Posteriori (MAP)\nestimation perspective, explicitly incorporating spatial correspondence priors\nacross LDR images and naturally bridging the alignment and fusion subproblems\nthrough joint constraints. Building on the mathematical foundation, we\nreimagine traditional iterative optimization through unfolding -- transforming\nthe conventional solution process into an end-to-end trainable AFUNet with\ncarefully designed modules that work progressively. Specifically, each\niteration of AFUNet incorporates an Alignment-Fusion Module (AFM) that\nalternates between a Spatial Alignment Module (SAM) for alignment and a Channel\nFusion Module (CFM) for adaptive feature fusion, progressively bridging\nmisaligned content and exposure discrepancies. Extensive qualitative and\nquantitative evaluations demonstrate AFUNet's superior performance,\nconsistently surpassing state-of-the-art methods. Our code is available at:\nhttps://github.com/eezkni/AFUNet", "AI": {"tldr": "AFUNet通过交替优化的对齐与融合子任务，从MAP估计角度重建HDR图像，性能优于现有方法。", "motivation": "现有基于学习的方法依赖经验设计，缺乏理论支持，影响可靠性。", "method": "AFUNet将HDR重建分解为对齐与融合子任务，通过交替优化实现协同，并基于MAP估计设计端到端可训练网络。", "result": "AFUNet在定性和定量评估中均优于现有方法。", "conclusion": "AFUNet通过理论驱动的设计，显著提升了HDR重建的性能和可靠性。"}}
{"id": "2506.23664", "pdf": "https://arxiv.org/pdf/2506.23664", "abs": "https://arxiv.org/abs/2506.23664", "authors": ["Fangyijie Wang", "Kevin Whelan", "Félix Balado", "Guénolé Silvestre", "Kathleen M. Curran"], "title": "Diffusion Model-based Data Augmentation Method for Fetal Head Ultrasound Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Medical image data is less accessible than in other domains due to privacy\nand regulatory constraints. In addition, labeling requires costly,\ntime-intensive manual image annotation by clinical experts. To overcome these\nchallenges, synthetic medical data generation offers a promising solution.\nGenerative AI (GenAI), employing generative deep learning models, has proven\neffective at producing realistic synthetic images. This study proposes a novel\nmask-guided GenAI approach using diffusion models to generate synthetic fetal\nhead ultrasound images paired with segmentation masks. These synthetic pairs\naugment real datasets for supervised fine-tuning of the Segment Anything Model\n(SAM). Our results show that the synthetic data captures real image features\neffectively, and this approach reaches state-of-the-art fetal head\nsegmentation, especially when trained with a limited number of real image-mask\npairs. In particular, the segmentation reaches Dice Scores of 94.66\\% and\n94.38\\% using a handful of ultrasound images from the Spanish and African\ncohorts, respectively. Our code, models, and data are available on GitHub.", "AI": {"tldr": "提出了一种基于扩散模型的掩码引导生成AI方法，用于生成合成胎儿头部超声图像及其分割掩码，以增强真实数据集，提升分割模型的性能。", "motivation": "医疗图像数据因隐私和监管限制难以获取，且标注成本高，合成数据生成成为解决方案。", "method": "采用扩散模型生成合成胎儿头部超声图像及其分割掩码，用于增强真实数据集并微调Segment Anything Model (SAM)。", "result": "合成数据有效捕捉真实图像特征，在少量真实图像-掩码对的情况下达到最佳分割效果，Dice分数分别为94.66%和94.38%。", "conclusion": "该方法通过合成数据显著提升了分割性能，尤其在数据有限的情况下表现优异。"}}
{"id": "2506.23700", "pdf": "https://arxiv.org/pdf/2506.23700", "abs": "https://arxiv.org/abs/2506.23700", "authors": ["Peiting Tian", "Xi Chen", "Haixia Bi", "Fan Li"], "title": "MedSAM-CA: A CNN-Augmented ViT with Attention-Enhanced Multi-Scale Fusion for Medical Image Segmentation", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Medical image segmentation plays a crucial role in clinical diagnosis and\ntreatment planning, where accurate boundary delineation is essential for\nprecise lesion localization, organ identification, and quantitative assessment.\nIn recent years, deep learning-based methods have significantly advanced\nsegmentation accuracy. However, two major challenges remain. First, the\nperformance of these methods heavily relies on large-scale annotated datasets,\nwhich are often difficult to obtain in medical scenarios due to privacy\nconcerns and high annotation costs. Second, clinically challenging scenarios,\nsuch as low contrast in certain imaging modalities and blurry lesion boundaries\ncaused by malignancy, still pose obstacles to precise segmentation. To address\nthese challenges, we propose MedSAM-CA, an architecture-level fine-tuning\napproach that mitigates reliance on extensive manual annotations by adapting\nthe pretrained foundation model, Medical Segment Anything (MedSAM). MedSAM-CA\nintroduces two key components: the Convolutional Attention-Enhanced Boundary\nRefinement Network (CBR-Net) and the Attention-Enhanced Feature Fusion Block\n(Atte-FFB). CBR-Net operates in parallel with the MedSAM encoder to recover\nboundary information potentially overlooked by long-range attention mechanisms,\nleveraging hierarchical convolutional processing. Atte-FFB, embedded in the\nMedSAM decoder, fuses multi-level fine-grained features from skip connections\nin CBR-Net with global representations upsampled within the decoder to enhance\nboundary delineation accuracy. Experiments on publicly available datasets\ncovering dermoscopy, CT, and MRI imaging modalities validate the effectiveness\nof MedSAM-CA. On dermoscopy dataset, MedSAM-CA achieves 94.43% Dice with only\n2% of full training data, reaching 97.25% of full-data training performance,\ndemonstrating strong effectiveness in low-resource clinical settings.", "AI": {"tldr": "MedSAM-CA提出了一种基于预训练模型MedSAM的架构级微调方法，通过引入CBR-Net和Atte-FFB组件，减少对大规模标注数据的依赖，并提升医学图像分割的边界准确性。", "motivation": "医学图像分割在临床诊断中至关重要，但现有深度学习方法依赖大量标注数据且难以处理低对比度和模糊边界等挑战。", "method": "结合CBR-Net（卷积注意力增强边界细化网络）和Atte-FFB（注意力增强特征融合块），对MedSAM进行微调。", "result": "在公开数据集上验证，仅用2%训练数据即可达到94.43% Dice分数，接近全数据训练性能的97.25%。", "conclusion": "MedSAM-CA在低资源临床场景中表现出色，显著减少标注需求并提升分割精度。"}}
{"id": "2506.23701", "pdf": "https://arxiv.org/pdf/2506.23701", "abs": "https://arxiv.org/abs/2506.23701", "authors": ["Lingtong Zhang", "Mengdie Song", "Xiaohan Hao", "Huayu Mai", "Bensheng Qiu"], "title": "MDPG: Multi-domain Diffusion Prior Guidance for MRI Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": "Accept by MICCAI2025", "summary": "Magnetic Resonance Imaging (MRI) reconstruction is essential in medical\ndiagnostics. As the latest generative models, diffusion models (DMs) have\nstruggled to produce high-fidelity images due to their stochastic nature in\nimage domains. Latent diffusion models (LDMs) yield both compact and detailed\nprior knowledge in latent domains, which could effectively guide the model\ntowards more effective learning of the original data distribution. Inspired by\nthis, we propose Multi-domain Diffusion Prior Guidance (MDPG) provided by\npre-trained LDMs to enhance data consistency in MRI reconstruction tasks.\nSpecifically, we first construct a Visual-Mamba-based backbone, which enables\nefficient encoding and reconstruction of under-sampled images. Then pre-trained\nLDMs are integrated to provide conditional priors in both latent and image\ndomains. A novel Latent Guided Attention (LGA) is proposed for efficient fusion\nin multi-level latent domains. Simultaneously, to effectively utilize a prior\nin both the k-space and image domain, under-sampled images are fused with\ngenerated full-sampled images by the Dual-domain Fusion Branch (DFB) for\nself-adaption guidance. Lastly, to further enhance the data consistency, we\npropose a k-space regularization strategy based on the non-auto-calibration\nsignal (NACS) set. Extensive experiments on two public MRI datasets fully\ndemonstrate the effectiveness of the proposed methodology. The code is\navailable at https://github.com/Zolento/MDPG.", "AI": {"tldr": "提出了一种基于预训练潜在扩散模型（LDMs）的多域扩散先验引导（MDPG）方法，用于提升MRI重建任务的数据一致性。", "motivation": "扩散模型（DMs）在图像域中因随机性难以生成高保真图像，而潜在扩散模型（LDMs）在潜在域中提供了紧凑且详细的先验知识，可有效指导模型学习原始数据分布。", "method": "1. 构建基于Visual-Mamba的主干网络，高效编码和重建欠采样图像；2. 集成预训练LDMs，在潜在域和图像域提供条件先验；3. 提出潜在引导注意力（LGA）实现多级潜在域高效融合；4. 设计双域融合分支（DFB）融合欠采样图像与生成的全采样图像；5. 提出基于非自动校准信号（NACS）的k空间正则化策略。", "result": "在两个公开MRI数据集上的实验验证了方法的有效性。", "conclusion": "MDPG通过多域先验引导和k空间正则化，显著提升了MRI重建的数据一致性。"}}
{"id": "2506.23731", "pdf": "https://arxiv.org/pdf/2506.23731", "abs": "https://arxiv.org/abs/2506.23731", "authors": ["Michel Meintz", "Jan Dubiński", "Franziska Boenisch", "Adam Dziedzic"], "title": "Radioactive Watermarks in Diffusion and Autoregressive Image Generative Models", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Image generative models have become increasingly popular, but training them\nrequires large datasets that are costly to collect and curate. To circumvent\nthese costs, some parties may exploit existing models by using the generated\nimages as training data for their own models. In general, watermarking is a\nvaluable tool for detecting unauthorized use of generated images. However, when\nthese images are used to train a new model, watermarking can only enable\ndetection if the watermark persists through training and remains identifiable\nin the outputs of the newly trained model - a property known as radioactivity.\nWe analyze the radioactivity of watermarks in images generated by diffusion\nmodels (DMs) and image autoregressive models (IARs). We find that existing\nwatermarking methods for DMs fail to retain radioactivity, as watermarks are\neither erased during encoding into the latent space or lost in the\nnoising-denoising process (during the training in the latent space). Meanwhile,\ndespite IARs having recently surpassed DMs in image generation quality and\nefficiency, no radioactive watermarking methods have been proposed for them. To\novercome this limitation, we propose the first watermarking method tailored for\nIARs and with radioactivity in mind - drawing inspiration from techniques in\nlarge language models (LLMs), which share IARs' autoregressive paradigm. Our\nextensive experimental evaluation highlights our method's effectiveness in\npreserving radioactivity within IARs, enabling robust provenance tracking, and\npreventing unauthorized use of their generated images.", "AI": {"tldr": "论文分析了扩散模型和图像自回归模型中水印的放射性问题，并提出了一种针对图像自回归模型的新型水印方法。", "motivation": "训练图像生成模型需要大量数据集，成本高昂。为防止未经授权使用生成图像，水印技术是关键，但现有方法在模型训练中无法保持水印的放射性。", "method": "提出了一种针对图像自回归模型的水印方法，借鉴了大型语言模型的技术。", "result": "实验证明该方法能有效保持水印的放射性，实现可靠的来源追踪。", "conclusion": "该方法为图像自回归模型提供了首个放射性水印解决方案，防止生成图像的未经授权使用。"}}
{"id": "2506.23759", "pdf": "https://arxiv.org/pdf/2506.23759", "abs": "https://arxiv.org/abs/2506.23759", "authors": ["Zheng Fang", "Xiaoming Qi", "Chun-Mei Feng", "Jialun Pei", "Weixin Si", "Yueming Jin"], "title": "Spatio-Temporal Representation Decoupling and Enhancement for Federated Instrument Segmentation in Surgical Videos", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Surgical instrument segmentation under Federated Learning (FL) is a promising\ndirection, which enables multiple surgical sites to collaboratively train the\nmodel without centralizing datasets. However, there exist very limited FL works\nin surgical data science, and FL methods for other modalities do not consider\ninherent characteristics in surgical domain: i) different scenarios show\ndiverse anatomical backgrounds while highly similar instrument representation;\nii) there exist surgical simulators which promote large-scale synthetic data\ngeneration with minimal efforts. In this paper, we propose a novel Personalized\nFL scheme, Spatio-Temporal Representation Decoupling and Enhancement (FedST),\nwhich wisely leverages surgical domain knowledge during both local-site and\nglobal-server training to boost segmentation. Concretely, our model embraces a\nRepresentation Separation and Cooperation (RSC) mechanism in local-site\ntraining, which decouples the query embedding layer to be trained privately, to\nencode respective backgrounds. Meanwhile, other parameters are optimized\nglobally to capture the consistent representations of instruments, including\nthe temporal layer to capture similar motion patterns. A textual-guided channel\nselection is further designed to highlight site-specific features, facilitating\nmodel adapta tion to each site. Moreover, in global-server training, we propose\nSynthesis-based Explicit Representation Quantification (SERQ), which defines an\nexplicit representation target based on synthetic data to synchronize the model\nconvergence during fusion for improving model generalization.", "AI": {"tldr": "提出了一种个性化联邦学习方案FedST，通过空间-时间表示解耦和增强，利用手术领域知识提升分割性能。", "motivation": "解决手术器械分割在联邦学习中的挑战，如多样解剖背景与相似器械表示，以及手术模拟器生成大规模合成数据的潜力。", "method": "本地训练采用表示分离与合作机制（RSC），全局训练提出基于合成数据的显式表示量化（SERQ）。", "result": "通过解耦和增强表示，提升了模型对手术器械的分割性能和泛化能力。", "conclusion": "FedST方案有效结合手术领域知识，为联邦学习在手术数据科学中的应用提供了新思路。"}}
{"id": "2506.23824", "pdf": "https://arxiv.org/pdf/2506.23824", "abs": "https://arxiv.org/abs/2506.23824", "authors": ["Durgesh Singh", "Ahcene Boubekki", "Robert Jenssen", "Michael C. Kampffmeyer"], "title": "Supercm: Revisiting Clustering for Semi-Supervised Learning", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "The development of semi-supervised learning (SSL) has in recent years largely\nfocused on the development of new consistency regularization or entropy\nminimization approaches, often resulting in models with complex training\nstrategies to obtain the desired results. In this work, we instead propose a\nnovel approach that explicitly incorporates the underlying clustering\nassumption in SSL through extending a recently proposed differentiable\nclustering module. Leveraging annotated data to guide the cluster centroids\nresults in a simple end-to-end trainable deep SSL approach. We demonstrate that\nthe proposed model improves the performance over the supervised-only baseline\nand show that our framework can be used in conjunction with other SSL methods\nto further boost their performance.", "AI": {"tldr": "提出了一种新的半监督学习方法，通过可微分聚类模块显式结合聚类假设，简化训练策略并提升性能。", "motivation": "近年来半监督学习主要关注一致性正则化或熵最小化方法，导致模型训练复杂。本文旨在通过显式结合聚类假设简化方法。", "method": "扩展可微分聚类模块，利用标注数据引导聚类中心，实现端到端训练。", "result": "模型性能优于仅监督基线，并能与其他半监督学习方法结合进一步提升效果。", "conclusion": "提出的方法简化了半监督学习，同时提升了性能，具有广泛适用性。"}}
{"id": "2506.23957", "pdf": "https://arxiv.org/pdf/2506.23957", "abs": "https://arxiv.org/abs/2506.23957", "authors": ["Zinuo You", "Stamatios Georgoulis", "Anpei Chen", "Siyu Tang", "Dengxin Dai"], "title": "GaVS: 3D-Grounded Video Stabilization via Temporally-Consistent Local Reconstruction and Rendering", "categories": ["cs.GR", "cs.CV"], "comment": "siggraph 2025, project website: https://sinoyou.github.io/gavs", "summary": "Video stabilization is pivotal for video processing, as it removes unwanted\nshakiness while preserving the original user motion intent. Existing\napproaches, depending on the domain they operate, suffer from several issues\n(e.g. geometric distortions, excessive cropping, poor generalization) that\ndegrade the user experience. To address these issues, we introduce\n\\textbf{GaVS}, a novel 3D-grounded approach that reformulates video\nstabilization as a temporally-consistent `local reconstruction and rendering'\nparadigm. Given 3D camera poses, we augment a reconstruction model to predict\nGaussian Splatting primitives, and finetune it at test-time, with multi-view\ndynamics-aware photometric supervision and cross-frame regularization, to\nproduce temporally-consistent local reconstructions. The model are then used to\nrender each stabilized frame. We utilize a scene extrapolation module to avoid\nframe cropping. Our method is evaluated on a repurposed dataset, instilled with\n3D-grounded information, covering samples with diverse camera motions and scene\ndynamics. Quantitatively, our method is competitive with or superior to\nstate-of-the-art 2D and 2.5D approaches in terms of conventional task metrics\nand new geometry consistency. Qualitatively, our method produces noticeably\nbetter results compared to alternatives, validated by the user study.", "AI": {"tldr": "论文提出了一种基于3D的视频稳定方法GaVS，通过局部重建和渲染范式解决现有方法的几何失真、过度裁剪和泛化能力差等问题。", "motivation": "视频稳定对视频处理至关重要，但现有方法存在几何失真、过度裁剪和泛化能力差等问题，影响用户体验。", "method": "GaVS采用3D相机姿态，通过高斯散射基元预测和测试时微调，结合多视角动态感知光度监督和跨帧正则化，实现时间一致的局部重建和渲染。", "result": "定量和定性评估表明，GaVS在任务指标和几何一致性上优于现有2D和2.5D方法，用户研究也验证了其优越性。", "conclusion": "GaVS通过3D基础的方法显著提升了视频稳定效果，解决了现有技术的局限性。"}}
{"id": "2506.24000", "pdf": "https://arxiv.org/pdf/2506.24000", "abs": "https://arxiv.org/abs/2506.24000", "authors": ["Lijun Sheng", "Jian Liang", "Ran He", "Zilei Wang", "Tieniu Tan"], "title": "The Illusion of Progress? A Critical Look at Test-Time Adaptation for Vision-Language Models", "categories": ["cs.LG", "cs.CV"], "comment": "Github link: https://github.com/TomSheng21/tta-vlm", "summary": "Test-time adaptation (TTA) methods have gained significant attention for\nenhancing the performance of vision-language models (VLMs) such as CLIP during\ninference, without requiring additional labeled data. However, current TTA\nresearches generally suffer from major limitations such as duplication of\nbaseline results, limited evaluation metrics, inconsistent experimental\nsettings, and insufficient analysis. These problems hinder fair comparisons\nbetween TTA methods and obscure their practical strengths and weaknesses. To\naddress these challenges, we introduce TTA-VLM, a comprehensive benchmark for\nevaluating TTA methods on VLMs. Our benchmark implements 8 episodic TTA and 7\nonline TTA methods within a unified and reproducible framework, and evaluates\nthem across 15 widely used datasets. Unlike prior studies focused solely on\nCLIP, we extend the evaluation to SigLIP--a model trained with a Sigmoid\nloss--and include training-time tuning methods such as CoOp, MaPLe, and TeCoA\nto assess generality. Beyond classification accuracy, TTA-VLM incorporates\nvarious evaluation metrics, including robustness, calibration,\nout-of-distribution detection, and stability, enabling a more holistic\nassessment of TTA methods. Through extensive experiments, we find that 1)\nexisting TTA methods produce limited gains compared to the previous pioneering\nwork; 2) current TTA methods exhibit poor collaboration with training-time\nfine-tuning methods; 3) accuracy gains frequently come at the cost of reduced\nmodel trustworthiness. We release TTA-VLM to provide fair comparison and\ncomprehensive evaluation of TTA methods for VLMs, and we hope it encourages the\ncommunity to develop more reliable and generalizable TTA strategies.", "AI": {"tldr": "TTA-VLM是一个全面的基准测试，用于评估视觉语言模型（VLM）的测试时适应（TTA）方法，解决了现有研究的局限性，并提供了多维度评估。", "motivation": "当前TTA研究存在结果重复、评估指标有限、实验设置不一致和不足分析等问题，阻碍了公平比较和实际应用。", "method": "TTA-VLM实现了8种情景TTA和7种在线TTA方法，统一评估15个数据集，并扩展到SigLIP模型和训练时调优方法。", "result": "实验发现现有TTA方法提升有限，与训练时调优方法协作不佳，且准确性提升常以模型可信度下降为代价。", "conclusion": "TTA-VLM的发布旨在促进更可靠和通用的TTA策略发展。"}}
{"id": "2506.24003", "pdf": "https://arxiv.org/pdf/2506.24003", "abs": "https://arxiv.org/abs/2506.24003", "authors": ["Junqi Liu", "Dongli He", "Wenxuan Li", "Ningyu Wang", "Alan L. Yuille", "Zongwei Zhou"], "title": "ShapeKit", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "In this paper, we present a practical approach to improve anatomical shape\naccuracy in whole-body medical segmentation. Our analysis shows that a\nshape-focused toolkit can enhance segmentation performance by over 8%, without\nthe need for model re-training or fine-tuning. In comparison, modifications to\nmodel architecture typically lead to marginal gains of less than 3%. Motivated\nby this observation, we introduce ShapeKit, a flexible and easy-to-integrate\ntoolkit designed to refine anatomical shapes. This work highlights the\nunderappreciated value of shape-based tools and calls attention to their\npotential impact within the medical segmentation community.", "AI": {"tldr": "提出了一种无需重新训练模型即可提升医学分割中解剖形状准确性的工具ShapeKit，性能提升超过8%。", "motivation": "观察到形状优化工具在医学分割中的潜力，而传统模型架构修改效果有限。", "method": "开发了ShapeKit工具包，专注于优化解剖形状，易于集成。", "result": "ShapeKit显著提升了分割性能（超过8%），优于模型架构修改（仅3%）。", "conclusion": "强调了形状优化工具在医学分割中的重要性，呼吁社区关注其潜力。"}}
{"id": "2506.24034", "pdf": "https://arxiv.org/pdf/2506.24034", "abs": "https://arxiv.org/abs/2506.24034", "authors": ["George Webber", "Alexander Hammers", "Andrew P King", "Andrew J Reader"], "title": "Supervised Diffusion-Model-Based PET Image Reconstruction", "categories": ["physics.med-ph", "cs.CV"], "comment": "12 pages, 6 figures. Submitted to MICCAI 2025, not peer-reviewed", "summary": "Diffusion models (DMs) have recently been introduced as a regularizing prior\nfor PET image reconstruction, integrating DMs trained on high-quality PET\nimages with unsupervised schemes that condition on measured data. While these\napproaches have potential generalization advantages due to their independence\nfrom the scanner geometry and the injected activity level, they forgo the\nopportunity to explicitly model the interaction between the DM prior and noisy\nmeasurement data, potentially limiting reconstruction accuracy. To address\nthis, we propose a supervised DM-based algorithm for PET reconstruction. Our\nmethod enforces the non-negativity of PET's Poisson likelihood model and\naccommodates the wide intensity range of PET images. Through experiments on\nrealistic brain PET phantoms, we demonstrate that our approach outperforms or\nmatches state-of-the-art deep learning-based methods quantitatively across a\nrange of dose levels. We further conduct ablation studies to demonstrate the\nbenefits of the proposed components in our model, as well as its dependence on\ntraining data, parameter count, and number of diffusion steps. Additionally, we\nshow that our approach enables more accurate posterior sampling than\nunsupervised DM-based methods, suggesting improved uncertainty estimation.\nFinally, we extend our methodology to a practical approach for fully 3D PET and\npresent example results from real [$^{18}$F]FDG brain PET data.", "AI": {"tldr": "提出了一种基于监督扩散模型（DM）的PET图像重建方法，优于现有深度学习方法，并改进了不确定性估计。", "motivation": "现有DM方法未显式建模与噪声数据的交互，可能限制重建精度。", "method": "结合PET的Poisson似然模型非负性和宽强度范围，提出监督DM算法。", "result": "在真实脑PET数据上表现优于或匹配现有方法，支持更准确的后验采样。", "conclusion": "方法在3D PET中实用，且对训练数据、参数和扩散步数有依赖性。"}}
{"id": "2506.24074", "pdf": "https://arxiv.org/pdf/2506.24074", "abs": "https://arxiv.org/abs/2506.24074", "authors": ["Mayank V. Golhar", "Lucas Sebastian Galeano Fretes", "Loren Ayers", "Venkata S. Akshintala", "Taylor L. Bobrow", "Nicholas J. Durr"], "title": "C3VDv2 -- Colonoscopy 3D video dataset with enhanced realism", "categories": ["eess.IV", "cs.CV"], "comment": "19 pages, 7 figures", "summary": "Computer vision techniques have the potential to improve the diagnostic\nperformance of colonoscopy, but the lack of 3D colonoscopy datasets for\ntraining and validation hinders their development. This paper introduces\nC3VDv2, the second version (v2) of the high-definition Colonoscopy 3D Video\nDataset, featuring enhanced realism designed to facilitate the quantitative\nevaluation of 3D colon reconstruction algorithms. 192 video sequences were\ncaptured by imaging 60 unique, high-fidelity silicone colon phantom segments.\nGround truth depth, surface normals, optical flow, occlusion,\nsix-degree-of-freedom pose, coverage maps, and 3D models are provided for 169\ncolonoscopy videos. Eight simulated screening colonoscopy videos acquired by a\ngastroenterologist are provided with ground truth poses. The dataset includes\n15 videos featuring colon deformations for qualitative assessment. C3VDv2\nemulates diverse and challenging scenarios for 3D reconstruction algorithms,\nincluding fecal debris, mucous pools, blood, debris obscuring the colonoscope\nlens, en-face views, and fast camera motion. The enhanced realism of C3VDv2\nwill allow for more robust and representative development and evaluation of 3D\nreconstruction algorithms.", "AI": {"tldr": "C3VDv2是一个高仿真结肠镜3D视频数据集，旨在支持3D结肠重建算法的开发和评估。", "motivation": "现有3D结肠镜数据集的缺乏限制了计算机视觉技术在结肠镜诊断中的应用。", "method": "通过高仿真硅胶结肠模型采集192个视频序列，并提供多种真实场景和标注数据。", "result": "数据集包含169个视频的真实深度、表面法线等标注，以及15个变形结肠视频。", "conclusion": "C3VDv2的高仿真性将促进更鲁棒和代表性的3D重建算法发展。"}}
{"id": "2506.24124", "pdf": "https://arxiv.org/pdf/2506.24124", "abs": "https://arxiv.org/abs/2506.24124", "authors": ["Dong Sixun", "Fan Wei", "Teresa Wu", "Fu Yanjie"], "title": "Teaching Time Series to See and Speak: Forecasting with Aligned Visual and Textual Perspectives", "categories": ["cs.LG", "cs.CV"], "comment": "Code: https://github.com/Ironieser/TimesCLIP", "summary": "Time series forecasting traditionally relies on unimodal numerical inputs,\nwhich often struggle to capture high-level semantic patterns due to their dense\nand unstructured nature. While recent approaches have explored representing\ntime series as text using large language models (LLMs), these methods remain\nlimited by the discrete nature of token sequences and lack the perceptual\nintuition humans typically apply, such as interpreting visual patterns. In this\npaper, we propose a multimodal contrastive learning framework that transforms\nraw time series into structured visual and textual perspectives. Rather than\nusing natural language or real-world images, we construct both modalities\ndirectly from numerical sequences. We then align these views in a shared\nsemantic space via contrastive learning, enabling the model to capture richer\nand more complementary representations. Furthermore, we introduce a variate\nselection module that leverages the aligned representations to identify the\nmost informative variables for multivariate forecasting. Extensive experiments\non fifteen short-term and six long-term forecasting benchmarks demonstrate that\nour approach consistently outperforms strong unimodal and cross-modal\nbaselines, highlighting the effectiveness of multimodal alignment in enhancing\ntime series forecasting. Code is available at:\nhttps://github.com/Ironieser/TimesCLIP.", "AI": {"tldr": "提出了一种多模态对比学习框架，将时间序列转化为视觉和文本模态，并通过对比学习对齐，提升了时间序列预测性能。", "motivation": "传统单模态数值输入难以捕捉高级语义模式，而现有基于文本的方法受限于离散性，缺乏人类视觉直觉。", "method": "构建视觉和文本模态，通过对比学习对齐，并引入变量选择模块。", "result": "在多个基准测试中优于单模态和跨模态基线。", "conclusion": "多模态对齐能有效增强时间序列预测。"}}
