{"id": "2507.00008", "pdf": "https://arxiv.org/pdf/2507.00008", "abs": "https://arxiv.org/abs/2507.00008", "authors": ["Hang Wu", "Hongkai Chen", "Yujun Cai", "Chang Liu", "Qingwen Ye", "Ming-Hsuan Yang", "Yiwei Wang"], "title": "DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning", "categories": ["cs.AI", "cs.CV", "cs.HC"], "comment": "8 pages, 6 figures", "summary": "Grounding natural language queries in graphical user interfaces (GUIs) poses\nunique challenges due to the diversity of visual elements, spatial clutter, and\nthe ambiguity of language. In this paper, we introduce DiMo-GUI, a\ntraining-free framework for GUI grounding that leverages two core strategies:\ndynamic visual grounding and modality-aware optimization. Instead of treating\nthe GUI as a monolithic image, our method splits the input into textual\nelements and iconic elements, allowing the model to reason over each modality\nindependently using general-purpose vision-language models. When predictions\nare ambiguous or incorrect, DiMo-GUI dynamically focuses attention by\ngenerating candidate focal regions centered on the model's initial predictions\nand incrementally zooms into subregions to refine the grounding result. This\nhierarchical refinement process helps disambiguate visually crowded layouts\nwithout the need for additional training or annotations. We evaluate our\napproach on standard GUI grounding benchmarks and demonstrate consistent\nimprovements over baseline inference pipelines, highlighting the effectiveness\nof combining modality separation with region-focused reasoning.", "AI": {"tldr": "DiMo-GUI是一个无需训练的GUI自然语言查询框架，通过动态视觉定位和模态感知优化，有效解决GUI中的视觉元素多样性和语言歧义问题。", "motivation": "GUI中的视觉元素多样性和语言歧义问题使得自然语言查询的定位具有挑战性。", "method": "将GUI拆分为文本和图标元素，利用通用视觉语言模型独立处理，并通过动态聚焦候选区域逐步细化定位结果。", "result": "在标准GUI基准测试中表现优于基线方法，证明了模态分离与区域聚焦推理的有效性。", "conclusion": "DiMo-GUI通过动态视觉定位和模态感知优化，无需额外训练即可显著提升GUI自然语言查询的准确性。"}}
{"id": "2507.00041", "pdf": "https://arxiv.org/pdf/2507.00041", "abs": "https://arxiv.org/abs/2507.00041", "authors": ["Varun Mannam", "Fang Wang", "Chaochun Liu", "Xin Chen"], "title": "TalentMine: LLM-Based Extraction and Question-Answering from Multimodal Talent Tables", "categories": ["cs.AI", "cs.CV", "cs.IR"], "comment": "Submitted to KDD conference, workshop: Talent and Management\n  Computing (TMC 2025), https://tmcworkshop.github.io/2025/", "summary": "In talent management systems, critical information often resides in complex\ntabular formats, presenting significant retrieval challenges for conventional\nlanguage models. These challenges are pronounced when processing Talent\ndocumentation that requires precise interpretation of tabular relationships for\naccurate information retrieval and downstream decision-making. Current table\nextraction methods struggle with semantic understanding, resulting in poor\nperformance when integrated into retrieval-augmented chat applications. This\npaper identifies a key bottleneck - while structural table information can be\nextracted, the semantic relationships between tabular elements are lost,\ncausing downstream query failures. To address this, we introduce TalentMine, a\nnovel LLM-enhanced framework that transforms extracted tables into semantically\nenriched representations. Unlike conventional approaches relying on CSV or text\nlinearization, our method employs specialized multimodal reasoning to preserve\nboth structural and semantic dimensions of tabular data. Experimental\nevaluation across employee benefits document collections demonstrates\nTalentMine's superior performance, achieving 100% accuracy in query answering\ntasks compared to 0% for standard AWS Textract extraction and 40% for AWS\nTextract Visual Q&A capabilities. Our comparative analysis also reveals that\nthe Claude v3 Haiku model achieves optimal performance for talent management\napplications. The key contributions of this work include (1) a systematic\nanalysis of semantic information loss in current table extraction pipelines,\n(2) a novel LLM-based method for semantically enriched table representation,\n(3) an efficient integration framework for retrieval-augmented systems as\nend-to-end systems, and (4) comprehensive benchmarks on talent analytics tasks\nshowing substantial improvements across multiple categories.", "AI": {"tldr": "论文提出TalentMine框架，通过增强语义表表示解决传统表格提取方法在人才管理中的信息丢失问题，显著提升查询准确性。", "motivation": "传统表格提取方法在语义理解上表现不佳，导致人才管理系统中关键信息检索失败，影响决策。", "method": "引入TalentMine框架，结合多模态推理生成语义丰富的表格表示，优于传统CSV或文本线性化方法。", "result": "实验显示TalentMine在查询任务中达到100%准确率，远超AWS Textract（0%）及其视觉Q&A功能（40%）。", "conclusion": "TalentMine通过语义增强有效解决了表格信息丢失问题，为人才管理系统提供了高效检索方案。"}}
{"id": "2507.00048", "pdf": "https://arxiv.org/pdf/2507.00048", "abs": "https://arxiv.org/abs/2507.00048", "authors": ["Thomas M. Deucher", "Juan C. Verduzco", "Michael Titus", "Alejandro Strachan"], "title": "A collaborative digital twin built on FAIR data and compute infrastructure", "categories": ["cs.AI", "cond-mat.mtrl-sci", "cs.CE", "cs.LG"], "comment": "10 pages, 5 figures", "summary": "The integration of machine learning with automated experimentation in\nself-driving laboratories (SDL) offers a powerful approach to accelerate\ndiscovery and optimization tasks in science and engineering applications. When\nsupported by findable, accessible, interoperable, and reusable (FAIR) data\ninfrastructure, SDLs with overlapping interests can collaborate more\neffectively. This work presents a distributed SDL implementation built on\nnanoHUB services for online simulation and FAIR data management. In this\nframework, geographically dispersed collaborators conducting independent\noptimization tasks contribute raw experimental data to a shared central\ndatabase. These researchers can then benefit from analysis tools and machine\nlearning models that automatically update as additional data become available.\nNew data points are submitted through a simple web interface and automatically\nprocessed using a nanoHUB Sim2L, which extracts derived quantities and indexes\nall inputs and outputs in a FAIR data repository called ResultsDB. A separate\nnanoHUB workflow enables sequential optimization using active learning, where\nresearchers define the optimization objective, and machine learning models are\ntrained on-the-fly with all existing data, guiding the selection of future\nexperiments. Inspired by the concept of ``frugal twin\", the optimization task\nseeks to find the optimal recipe to combine food dyes to achieve the desired\ntarget color. With easily accessible and inexpensive materials, researchers and\nstudents can set up their own experiments, share data with collaborators, and\nexplore the combination of FAIR data, predictive ML models, and sequential\noptimization. The tools introduced are generally applicable and can easily be\nextended to other optimization problems.", "AI": {"tldr": "论文提出了一种基于FAIR数据基础设施的分布式自驱动实验室（SDL）框架，结合机器学习和自动化实验，加速科学和工程中的发现与优化任务。", "motivation": "通过整合FAIR数据基础设施和SDL，促进地理分散的研究者协作，共享数据并利用机器学习模型进行优化。", "method": "使用nanoHUB服务构建分布式SDL框架，包括Sim2L处理数据、ResultsDB存储FAIR数据，以及基于主动学习的顺序优化工作流。", "result": "实现了通过简单网页界面提交数据、自动处理和分析，并成功应用于食品染料配方的优化任务。", "conclusion": "该框架具有通用性，可扩展至其他优化问题，同时为研究者和学生提供了低成本、易操作的实验平台。"}}
{"id": "2507.00050", "pdf": "https://arxiv.org/pdf/2507.00050", "abs": "https://arxiv.org/abs/2507.00050", "authors": ["Devin Y. De Silva", "Sandareka Wickramanayake", "Dulani Meedeniya", "Sanka Rasnayaka"], "title": "SEZ-HARN: Self-Explainable Zero-shot Human Activity Recognition Network", "categories": ["cs.AI", "cs.HC", "cs.LG", "I.2.0"], "comment": null, "summary": "Human Activity Recognition (HAR), which uses data from Inertial Measurement\nUnit (IMU) sensors, has many practical applications in healthcare and assisted\nliving environments. However, its use in real-world scenarios has been limited\nby the lack of comprehensive IMU-based HAR datasets that cover a wide range of\nactivities and the lack of transparency in existing HAR models. Zero-shot HAR\n(ZS-HAR) overcomes the data limitations, but current models struggle to explain\ntheir decisions, making them less transparent. This paper introduces a novel\nIMU-based ZS-HAR model called the Self-Explainable Zero-shot Human Activity\nRecognition Network (SEZ-HARN). It can recognize activities not encountered\nduring training and provide skeleton videos to explain its decision-making\nprocess. We evaluate the effectiveness of the proposed SEZ-HARN on four\nbenchmark datasets PAMAP2, DaLiAc, HTD-MHAD and MHealth and compare its\nperformance against three state-of-the-art black-box ZS-HAR models. The\nexperiment results demonstrate that SEZ-HARN produces realistic and\nunderstandable explanations while achieving competitive Zero-shot recognition\naccuracy. SEZ-HARN achieves a Zero-shot prediction accuracy within 3\\% of the\nbest-performing black-box model on PAMAP2 while maintaining comparable\nperformance on the other three datasets.", "AI": {"tldr": "本文提出了一种新型的自解释零样本人类活动识别模型（SEZ-HARN），能够识别未在训练中遇到的活动并提供解释性骨架视频。", "motivation": "解决现有零样本HAR模型缺乏透明性和数据限制的问题。", "method": "提出SEZ-HARN模型，结合零样本学习和自解释能力，生成骨架视频解释决策过程。", "result": "在四个基准数据集上表现优异，零样本预测准确率接近最佳黑盒模型，同时提供可理解的解释。", "conclusion": "SEZ-HARN在保持高准确率的同时，显著提升了模型的透明性和解释性。"}}
{"id": "2507.00152", "pdf": "https://arxiv.org/pdf/2507.00152", "abs": "https://arxiv.org/abs/2507.00152", "authors": ["Ekaterina Borisova", "Fabio Barth", "Nils Feldhus", "Raia Abu Ahmad", "Malte Ostendorff", "Pedro Ortiz Suarez", "Georg Rehm", "Sebastian Möller"], "title": "Table Understanding and (Multimodal) LLMs: A Cross-Domain Case Study on Scientific vs. Non-Scientific Data", "categories": ["cs.CL"], "comment": "TRL@ACL 2025, camera-ready version", "summary": "Tables are among the most widely used tools for representing structured data\nin research, business, medicine, and education. Although LLMs demonstrate\nstrong performance in downstream tasks, their efficiency in processing tabular\ndata remains underexplored. In this paper, we investigate the effectiveness of\nboth text-based and multimodal LLMs on table understanding tasks through a\ncross-domain and cross-modality evaluation. Specifically, we compare their\nperformance on tables from scientific vs. non-scientific contexts and examine\ntheir robustness on tables represented as images vs. text. Additionally, we\nconduct an interpretability analysis to measure context usage and input\nrelevance. We also introduce the TableEval benchmark, comprising 3017 tables\nfrom scholarly publications, Wikipedia, and financial reports, where each table\nis provided in five different formats: Image, Dictionary, HTML, XML, and LaTeX.\nOur findings indicate that while LLMs maintain robustness across table\nmodalities, they face significant challenges when processing scientific tables.", "AI": {"tldr": "论文研究了文本和多模态LLM在表格理解任务中的表现，发现LLM在处理科学表格时存在挑战。", "motivation": "表格是结构化数据的重要工具，但LLM在表格处理方面的效率尚未充分研究。", "method": "通过跨领域和跨模态评估，比较LLM在不同表格类型（科学与非科学）和格式（图像与文本）中的表现，并进行可解释性分析。", "result": "LLM在表格模态间表现稳健，但在处理科学表格时面临显著挑战。", "conclusion": "研究提出了TableEval基准，并强调LLM在科学表格处理中的局限性。"}}
{"id": "2507.00033", "pdf": "https://arxiv.org/pdf/2507.00033", "abs": "https://arxiv.org/abs/2507.00033", "authors": ["Mustafa Chasmai", "Gauri Jagatap", "Gouthaman KV", "Grant Van Horn", "Subhransu Maji", "Andrea Fanelli"], "title": "Moment Sampling in Video LLMs for Long-Form Video QA", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": "Workshop on Video Large Language Models (VidLLMs) at CVPR 2025", "summary": "Recent advancements in video large language models (Video LLMs) have\nsignificantly advanced the field of video question answering (VideoQA). While\nexisting methods perform well on short videos, they often struggle with\nlong-range reasoning in longer videos. To scale Video LLMs for longer video\ncontent, frame sub-sampling (selecting frames at regular intervals) is commonly\nused. However, this approach is suboptimal, often leading to the loss of\ncrucial frames or the inclusion of redundant information from multiple similar\nframes. Missing key frames impairs the model's ability to answer questions\naccurately, while redundant frames lead the model to focus on irrelevant video\nsegments and increase computational resource consumption. In this paper, we\ninvestigate the use of a general-purpose text-to-video moment retrieval model\nto guide the frame sampling process. We propose \"moment sampling\", a novel,\nmodel-agnostic approach that enables the model to select the most relevant\nframes according to the context of the question. Specifically, we employ a\nlightweight moment retrieval model to prioritize frame selection. By focusing\non the frames most pertinent to the given question, our method enhances\nlong-form VideoQA performance in Video LLMs. Through extensive experiments on\nfour long-form VideoQA datasets, using four state-of-the-art Video LLMs, we\ndemonstrate the effectiveness of the proposed approach.", "AI": {"tldr": "提出了一种名为“moment sampling”的新方法，通过文本到视频时刻检索模型指导帧采样，提升长视频问答性能。", "motivation": "现有视频大语言模型在长视频中表现不佳，帧采样方法常丢失关键帧或包含冗余信息。", "method": "使用轻量级时刻检索模型选择与问题最相关的帧。", "result": "在四个长视频问答数据集和四种先进视频大语言模型上验证了方法的有效性。", "conclusion": "moment sampling方法显著提升了长视频问答的性能。"}}
{"id": "2507.00054", "pdf": "https://arxiv.org/pdf/2507.00054", "abs": "https://arxiv.org/abs/2507.00054", "authors": ["Shreyansh Padarha"], "title": "Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "17 Pages, 7 figures", "summary": "The push to compress and impart the proficiency of Large Language Models\n(LLMs) into more deployable and efficient Small Language Models (SLMs) has\nbenefited from improvements in knowledge distillation (KD) techniques. These\ntechniques allow a smaller student model to learn from a more capable and\nlarger teacher model's responses. However, distillation often revolves around\nthe student model merely copying the teacher's in-distribution responses,\nlimiting its generalisability. This limitation is amplified on reasoning tasks\nand can be computationally expensive. In this study, we propose AdvDistill, a\nreward-guided dataset distillation framework. We utilise multiple generations\n(responses) from a teacher for each prompt and assign rewards based on\nrule-based verifiers. These varying and normally distributed rewards serve as\nweights when training student models. Our methods and their subsequent\nbehavioural analysis demonstrate a significant improvement in student model\nperformance for mathematical and complex reasoning tasks, showcasing the\nefficacy and benefits of incorporating a rewarding mechanism in dataset\ndistillation processes.", "AI": {"tldr": "AdvDistill提出了一种基于奖励的数据集蒸馏框架，通过多生成响应和规则验证器分配奖励，显著提升了小型语言模型在数学和复杂推理任务上的性能。", "motivation": "当前的知识蒸馏技术通常局限于学生模型复制教师模型的分布内响应，限制了其泛化能力，尤其是在推理任务上，且计算成本较高。", "method": "AdvDistill利用教师模型对每个提示生成多个响应，并通过规则验证器分配奖励，这些奖励作为训练学生模型的权重。", "result": "实验表明，AdvDistill显著提升了学生模型在数学和复杂推理任务上的性能。", "conclusion": "AdvDistill证明了在数据集蒸馏过程中引入奖励机制的有效性和优势。"}}
{"id": "2507.00163", "pdf": "https://arxiv.org/pdf/2507.00163", "abs": "https://arxiv.org/abs/2507.00163", "authors": ["Ari Holtzman", "Chenhao Tan"], "title": "Prompting as Scientific Inquiry", "categories": ["cs.CL"], "comment": null, "summary": "Prompting is the primary method by which we study and control large language\nmodels. It is also one of the most powerful: nearly every major capability\nattributed to LLMs-few-shot learning, chain-of-thought, constitutional AI-was\nfirst unlocked through prompting. Yet prompting is rarely treated as science\nand is frequently frowned upon as alchemy. We argue that this is a category\nerror. If we treat LLMs as a new kind of complex and opaque organism that is\ntrained rather than programmed, then prompting is not a workaround: it is\nbehavioral science. Mechanistic interpretability peers into the neural\nsubstrate, prompting probes the model in its native interface: language. We\ncontend that prompting is not inferior, but rather a key component in the\nscience of LLMs.", "AI": {"tldr": "论文认为提示（prompting）是研究大型语言模型（LLMs）的核心方法，应被视为行为科学而非玄学。", "motivation": "探讨提示在LLMs研究中的科学地位，反驳将其视为‘玄学’的观点。", "method": "通过类比LLMs为复杂生物体，将提示视为行为科学的研究工具。", "result": "提示是LLMs科学的关键组成部分，而非权宜之计。", "conclusion": "提示应被正式认可为LLMs研究的重要科学方法。"}}
{"id": "2507.00042", "pdf": "https://arxiv.org/pdf/2507.00042", "abs": "https://arxiv.org/abs/2507.00042", "authors": ["Xinrun Xu", "Jianwen Yang", "Qiuhong Zhang", "Zhanbiao Lian", "Zhiming Ding", "Shan Jiang"], "title": "Catastrophic Forgetting Mitigation via Discrepancy-Weighted Experience Replay", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "ICANN 2025", "summary": "Continually adapting edge models in cloud-edge collaborative object detection\nfor traffic monitoring suffers from catastrophic forgetting, where models lose\npreviously learned knowledge when adapting to new data distributions. This is\nespecially problematic in dynamic traffic environments characterised by\nperiodic variations (e.g., day/night, peak hours), where past knowledge remains\nvaluable. Existing approaches like experience replay and visual prompts offer\nsome mitigation, but struggle to effectively prioritize and leverage historical\ndata for optimal knowledge retention and adaptation. Specifically, simply\nstoring and replaying all historical data can be inefficient, while treating\nall historical experiences as equally important overlooks their varying\nrelevance to the current domain. This paper proposes ER-EMU, an edge model\nupdate algorithm based on adaptive experience replay, to address these\nlimitations. ER-EMU utilizes a limited-size experience buffer managed using a\nFirst-In-First-Out (FIFO) principle, and a novel Domain Distance Metric-based\nExperience Selection (DDM-ES) algorithm. DDM-ES employs the multi-kernel\nmaximum mean discrepancy (MK-MMD) to quantify the dissimilarity between target\ndomains, prioritizing the selection of historical data that is most dissimilar\nto the current target domain. This ensures training diversity and facilitates\nthe retention of knowledge from a wider range of past experiences, while also\npreventing overfitting to the new domain. The experience buffer is also updated\nusing a simple random sampling strategy to maintain a balanced representation\nof previous domains. Experiments on the Bellevue traffic video dataset,\ninvolving repeated day/night cycles, demonstrate that ER-EMU consistently\nimproves the performance of several state-of-the-art cloud-edge collaborative\nobject detection frameworks.", "AI": {"tldr": "论文提出ER-EMU算法，通过自适应经验回放解决边缘模型在动态交通环境中因灾难性遗忘导致的知识丢失问题。", "motivation": "动态交通环境（如昼夜变化）中，边缘模型在适应新数据分布时会遗忘旧知识，现有方法无法高效利用历史数据。", "method": "ER-EMU结合FIFO经验缓冲区和基于域距离度量的经验选择算法（DDM-ES），优先选择与当前目标域差异大的历史数据。", "result": "在Bellevue交通视频数据集上，ER-EMU显著提升了云边协同目标检测框架的性能。", "conclusion": "ER-EMU通过优化历史数据选择和缓冲管理，有效缓解灾难性遗忘，提升模型适应性。"}}
{"id": "2507.00079", "pdf": "https://arxiv.org/pdf/2507.00079", "abs": "https://arxiv.org/abs/2507.00079", "authors": ["Ethan Smyth", "Alessandro Suglia"], "title": "VoyagerVision: Investigating the Role of Multi-modal Information for Open-ended Learning Systems", "categories": ["cs.AI", "cs.LG"], "comment": "website: https://esmyth-dev.github.io/VoyagerVision.github.io/", "summary": "Open-endedness is an active field of research in the pursuit of capable\nArtificial General Intelligence (AGI), allowing models to pursue tasks of their\nown choosing. Simultaneously, recent advancements in Large Language Models\n(LLMs) such as GPT-4o [9] have allowed such models to be capable of\ninterpreting image inputs. Implementations such as OMNI-EPIC [4] have made use\nof such features, providing an LLM with pixel data of an agent's POV to parse\nthe environment and allow it to solve tasks. This paper proposes that providing\nthese visual inputs to a model gives it greater ability to interpret spatial\nenvironments, and as such, can increase the number of tasks it can successfully\nperform, extending its open-ended potential. To this aim, this paper proposes\nVoyagerVision -- a multi-modal model capable of creating structures within\nMinecraft using screenshots as a form of visual feedback, building on the\nfoundation of Voyager. VoyagerVision was capable of creating an average of 2.75\nunique structures within fifty iterations of the system, as Voyager was\nincapable of this, it is an extension in an entirely new direction.\nAdditionally, in a set of building unit tests VoyagerVision was successful in\nhalf of all attempts in flat worlds, with most failures arising in more complex\nstructures. Project website is available at\nhttps://esmyth-dev.github.io/VoyagerVision.github.io/", "AI": {"tldr": "论文提出VoyagerVision，一种多模态模型，通过视觉反馈在Minecraft中构建结构，扩展了模型的开放性和任务能力。", "motivation": "研究开放性是实现通用人工智能（AGI）的关键，而大型语言模型（LLMs）如GPT-4o已具备图像输入解析能力。通过视觉输入增强模型的空间理解能力，可以扩展其任务范围。", "method": "提出VoyagerVision模型，利用屏幕截图作为视觉反馈，在Minecraft中构建结构。基于Voyager模型，通过多模态输入实现任务扩展。", "result": "VoyagerVision在50次迭代中平均构建2.75个独特结构，而Voyager无法实现。在平坦世界中，成功率为50%，复杂结构失败较多。", "conclusion": "视觉输入显著提升了模型的空间理解和任务能力，为开放性和多模态研究提供了新方向。"}}
{"id": "2507.00210", "pdf": "https://arxiv.org/pdf/2507.00210", "abs": "https://arxiv.org/abs/2507.00210", "authors": ["Imene Kerboua", "Sahar Omidi Shayegan", "Megh Thakkar", "Xing Han Lù", "Massimo Caccia", "Véronique Eglin", "Alexandre Aussem", "Jérémy Espinas", "Alexandre Lacoste"], "title": "LineRetriever: Planning-Aware Observation Reduction for Web Agents", "categories": ["cs.CL"], "comment": null, "summary": "While large language models have demonstrated impressive capabilities in web\nnavigation tasks, the extensive context of web pages, often represented as DOM\nor Accessibility Tree (AxTree) structures, frequently exceeds model context\nlimits. Current approaches like bottom-up truncation or embedding-based\nretrieval lose critical information about page state and action history. This\nis particularly problematic for adaptive planning in web agents, where\nunderstanding the current state is essential for determining future actions. We\nhypothesize that embedding models lack sufficient capacity to capture\nplan-relevant information, especially when retrieving content that supports\nfuture action prediction. This raises a fundamental question: how can retrieval\nmethods be optimized for adaptive planning in web navigation tasks? In\nresponse, we introduce \\textit{LineRetriever}, a novel approach that leverages\na language model to identify and retrieve observation lines most relevant to\nfuture navigation steps. Unlike traditional retrieval methods that focus solely\non semantic similarity, \\textit{LineRetriever} explicitly considers the\nplanning horizon, prioritizing elements that contribute to action prediction.\nOur experiments demonstrate that \\textit{LineRetriever} can reduce the size of\nthe observation at each step for the web agent while maintaining consistent\nperformance within the context limitations.", "AI": {"tldr": "论文提出了一种名为LineRetriever的新方法，通过语言模型识别并检索与未来导航步骤最相关的观察行，解决了现有方法在网页导航任务中因上下文过长而丢失关键信息的问题。", "motivation": "当前基于嵌入的检索方法在网页导航任务中无法充分捕捉与规划相关的信息，尤其是在预测未来动作时。这限制了自适应规划的效果。", "method": "提出LineRetriever方法，利用语言模型识别和检索与未来导航步骤最相关的观察行，而非仅依赖语义相似性。", "result": "实验表明，LineRetriever能在保持性能的同时，减少每一步的观察数据量，适应上下文限制。", "conclusion": "LineRetriever通过优化检索方法，显著提升了网页导航任务中自适应规划的效果。"}}
{"id": "2507.00043", "pdf": "https://arxiv.org/pdf/2507.00043", "abs": "https://arxiv.org/abs/2507.00043", "authors": ["Mehmet Yigit Avci", "Pedro Borges", "Paul Wright", "Mehmet Yigitsoy", "Sebastien Ourselin", "Jorge Cardoso"], "title": "MR-CLIP: Efficient Metadata-Guided Learning of MRI Contrast Representations", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Accurate interpretation of Magnetic Resonance Imaging scans in clinical\nsystems is based on a precise understanding of image contrast. This contrast is\nprimarily governed by acquisition parameters, such as echo time and repetition\ntime, which are stored in the DICOM metadata. To simplify contrast\nidentification, broad labels such as T1-weighted or T2-weighted are commonly\nused, but these offer only a coarse approximation of the underlying acquisition\nsettings. In many real-world datasets, such labels are entirely missing,\nleaving raw acquisition parameters as the only indicators of contrast. Adding\nto this challenge, the available metadata is often incomplete, noisy, or\ninconsistent. The lack of reliable and standardized metadata complicates tasks\nsuch as image interpretation, retrieval, and integration into clinical\nworkflows. Furthermore, robust contrast-aware representations are essential to\nenable more advanced clinical applications, such as achieving\nmodality-invariant representations and data harmonization. To address these\nchallenges, we propose MR-CLIP, a multimodal contrastive learning framework\nthat aligns MR images with their DICOM metadata to learn contrast-aware\nrepresentations, without relying on manual labels. Trained on a diverse\nclinical dataset that spans various scanners and protocols, MR-CLIP captures\ncontrast variations across acquisitions and within scans, enabling\nanatomy-invariant representations. We demonstrate its effectiveness in\ncross-modal retrieval and contrast classification, highlighting its scalability\nand potential for further clinical applications. The code and weights are\npublicly available at https://github.com/myigitavci/MR-CLIP.", "AI": {"tldr": "MR-CLIP是一种多模态对比学习框架，通过对齐MRI图像与DICOM元数据学习对比感知表示，无需依赖人工标签。", "motivation": "MRI扫描的精确解释依赖于图像对比度的理解，但现有标签（如T1/T2加权）粗糙且元数据常缺失或不完整，阻碍了临床应用。", "method": "提出MR-CLIP框架，利用对比学习对齐图像与元数据，学习对比感知表示，训练数据涵盖多种扫描仪和协议。", "result": "MR-CLIP能捕捉对比度变化，支持跨模态检索和对比分类，表现优异且可扩展。", "conclusion": "MR-CLIP解决了元数据不可靠问题，为临床工作流和高级应用提供了潜力，代码已开源。"}}
{"id": "2507.00092", "pdf": "https://arxiv.org/pdf/2507.00092", "abs": "https://arxiv.org/abs/2507.00092", "authors": ["Basab Jha", "Firoj Paudel", "Ujjwal Puri", "Zhang Yuting", "Choi Donghyuk", "Wang Junhao"], "title": "Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "19 pages, 2 figures, 9 tables", "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities at\nsolving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but\ntheir decision-making processes remain somewhat blackbox. We introduce\ntextbfinverse reasoning, a novel paradigm enabling LLMs to decompose and\nexplain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a\n4-billion-parameter reasoning model, employs a metacognitive structure that\nreflects back via attention processes to identify major decision points and\ngenerate explanations of reasoning choices. While typical CoT approaches are\ndirected towards forward reasoning generation, inverse reasoning provides\ninsight into why specific reasoning chains were selected over others. Through\nthorough testing of logical reasoning puzzles, math problems and ethical\ndilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we\ndemonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy\n(74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for\nits task, and offers performance almost on par with models like Claude-3.5\nSonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for\nLLM self-reflection via inverse reasoning, (ii) a novel metalearning framework\nto reverse the attention flow, (iii) comprehensive evaluation frameworks for\nreasoning transparency, and (iv) evidence that increasing reasoning using\ninverse reasoning improves interpretability along with reasoning performance.\nOur work creates new avenues for transparent AI systems and closes significant\ngaps in AI safety, education, and scientific discovery.", "AI": {"tldr": "论文提出了一种名为“逆向推理”的新范式，使大语言模型（LLMs）能够分解并解释其推理过程，通过SAGE-nano模型展示了其在推理准确性和解释质量上的优越表现。", "motivation": "解决LLMs在推理过程中决策过程的黑箱问题，提升模型的透明度和可解释性。", "method": "采用逆向推理范式，通过元认知结构和注意力机制反向分析推理链，生成解释。", "result": "SAGE-nano在AQUA-RAT等测试中表现出色，推理准确率达74.6%，解释质量获92.1%的人类偏好评分。", "conclusion": "逆向推理不仅提升了LLMs的推理性能，还增强了其可解释性，为透明AI系统开辟了新途径。"}}
{"id": "2507.00214", "pdf": "https://arxiv.org/pdf/2507.00214", "abs": "https://arxiv.org/abs/2507.00214", "authors": ["Mads Henrichsen", "Rasmus Krebs"], "title": "Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Standard classification models often map inputs directly to labels without\nexplicit reasoning, potentially limiting their performance, robustness, and\ninterpretability. This paper introduces a novel two-stage approach to enhance\ntext classification by leveraging Large Language Model (LLM)-generated\nreasonings. In the first stage, we fine-tune a Llama-3.2-1B-Instruct model\n(henceforth Llama-R-Gen) on a general-purpose reasoning dataset\n(syvai/reasoning-gen) to generate textual reasoning (R) given a question and\nits answer. In the second stage, this generally trained Llama-R-Gen is used\noffline to create an augmented training dataset for a downstream generative\nmodel. This downstream model, based on Llama-3.2-1B-Instruct, takes only the\ninput text (Q) and is trained to output the generated reasoning (R) immediately\nfollowed by the predicted emotion (A). We demonstrate this methodology on the\ndair-ai/emotion dataset for emotion classification. Our experiments show that\nthe generative model trained to output reasoning and the emotion (Classifier\nQ->RA) achieves a significant improvement of 8.7 percentage points in accuracy\n(for emotion prediction) compared to a baseline generative model trained solely\nto output the emotion (Classifier Q->A), highlighting the strong generalization\ncapabilities of the reasoning generation and the benefit of explicit reasoning\ntraining. This work underscores the potential of LLM-generated reasonings for\ncreating richer training datasets, thereby improving the performance of diverse\ndownstream NLP tasks and providing explicit explanations.", "AI": {"tldr": "论文提出了一种两阶段方法，利用LLM生成推理增强文本分类，显著提升了情感分类的准确性。", "motivation": "传统分类模型缺乏显式推理，限制了性能、鲁棒性和可解释性。", "method": "1. 微调Llama-3.2-1B-Instruct生成推理；2. 用推理增强数据集训练下游生成模型。", "result": "生成推理和情感的分类器比仅生成情感的分类器准确率提高了8.7个百分点。", "conclusion": "LLM生成的推理能丰富训练数据，提升下游任务性能并提供显式解释。"}}
{"id": "2507.00044", "pdf": "https://arxiv.org/pdf/2507.00044", "abs": "https://arxiv.org/abs/2507.00044", "authors": ["Seyed Kahaki", "Alexander R. Webber", "Ghada Zamzmi", "Adarsh Subbaswamy", "Rucha Deshpande", "Aldo Badano"], "title": "HistoART: Histopathology Artifact Detection and Reporting Tool", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "14 pages, 5 figures", "summary": "In modern cancer diagnostics, Whole Slide Imaging (WSI) is widely used to\ndigitize tissue specimens for detailed, high-resolution examination; however,\nother diagnostic approaches, such as liquid biopsy and molecular testing, are\nalso utilized based on the cancer type and clinical context. While WSI has\nrevolutionized digital histopathology by enabling automated, precise analysis,\nit remains vulnerable to artifacts introduced during slide preparation and\nscanning. These artifacts can compromise downstream image analysis. To address\nthis challenge, we propose and compare three robust artifact detection\napproaches for WSIs: (1) a foundation model-based approach (FMA) using a\nfine-tuned Unified Neural Image (UNI) architecture, (2) a deep learning\napproach (DLA) built on a ResNet50 backbone, and (3) a knowledge-based approach\n(KBA) leveraging handcrafted features from texture, color, and frequency-based\nmetrics. The methods target six common artifact types: tissue folds,\nout-of-focus regions, air bubbles, tissue damage, marker traces, and blood\ncontamination. Evaluations were conducted on 50,000+ image patches from diverse\nscanners (Hamamatsu, Philips, Leica Aperio AT2) across multiple sites. The FMA\nachieved the highest patch-wise AUROC of 0.995 (95% CI [0.994, 0.995]),\noutperforming the ResNet50-based method (AUROC: 0.977, 95% CI [0.977, 0.978])\nand the KBA (AUROC: 0.940, 95% CI [0.933, 0.946]). To translate detection into\nactionable insights, we developed a quality report scorecard that quantifies\nhigh-quality patches and visualizes artifact distributions.", "AI": {"tldr": "本文提出并比较了三种检测全切片图像（WSI）中伪影的方法，旨在解决数字化病理学中因伪影导致的图像分析问题。", "motivation": "在数字化病理学中，WSI广泛用于癌症诊断，但伪影会影响分析准确性，因此需要有效的检测方法。", "method": "提出了三种方法：基于基础模型的FMA、基于ResNet50的DLA和基于手工特征的KBA，用于检测六种常见伪影。", "result": "FMA表现最佳（AUROC: 0.995），优于DLA（0.977）和KBA（0.940），并开发了质量报告工具。", "conclusion": "FMA在伪影检测中表现最优，为数字化病理学提供了可靠的质量控制工具。"}}
{"id": "2507.00180", "pdf": "https://arxiv.org/pdf/2507.00180", "abs": "https://arxiv.org/abs/2507.00180", "authors": ["Vidhi Rathore"], "title": "BlackBoxToBlueprint: Extracting Interpretable Logic from Legacy Systems using Reinforcement Learning and Counterfactual Analysis", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Modernizing legacy software systems is a critical but challenging task, often\nhampered by a lack of documentation and understanding of the original system's\nintricate decision logic. Traditional approaches like behavioral cloning merely\nreplicate input-output behavior without capturing the underlying intent. This\npaper proposes a novel pipeline to automatically extract interpretable decision\nlogic from legacy systems treated as black boxes. The approach uses a\nReinforcement Learning (RL) agent to explore the input space and identify\ncritical decision boundaries by rewarding actions that cause meaningful changes\nin the system's output. These counterfactual state transitions, where the\noutput changes, are collected and clustered using K-Means. Decision trees are\nthen trained on these clusters to extract human-readable rules that approximate\nthe system's decision logic near the identified boundaries. I demonstrated the\npipeline's effectiveness on three dummy legacy systems with varying complexity,\nincluding threshold-based, combined-conditional, and non-linear range logic.\nResults show that the RL agent successfully focuses exploration on relevant\nboundary regions, and the extracted rules accurately reflect the core logic of\nthe underlying dummy systems, providing a promising foundation for generating\nspecifications and test cases during legacy migration.", "AI": {"tldr": "提出了一种基于强化学习的自动化方法，从遗留系统中提取可解释的决策逻辑，并通过决策树生成人类可读的规则。", "motivation": "遗留系统的现代化改造因缺乏文档和理解其复杂决策逻辑而困难，传统方法仅复制输入输出行为，无法捕捉底层意图。", "method": "使用强化学习代理探索输入空间，识别关键决策边界，并通过K-Means聚类和决策树训练提取可解释规则。", "result": "在三种不同复杂度的遗留系统上验证，提取的规则准确反映了系统的核心逻辑。", "conclusion": "该方法为遗留系统迁移中的规范和测试用例生成提供了有前景的基础。"}}
{"id": "2507.00216", "pdf": "https://arxiv.org/pdf/2507.00216", "abs": "https://arxiv.org/abs/2507.00216", "authors": ["Shreya Havaldar", "Adam Stein", "Eric Wong", "Lyle Ungar"], "title": "Towards Style Alignment in Cross-Cultural Translation", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025", "summary": "Successful communication depends on the speaker's intended style (i.e., what\nthe speaker is trying to convey) aligning with the listener's interpreted style\n(i.e., what the listener perceives). However, cultural differences often lead\nto misalignment between the two; for example, politeness is often lost in\ntranslation. We characterize the ways that LLMs fail to translate style -\nbiasing translations towards neutrality and performing worse in non-Western\nlanguages. We mitigate these failures with RASTA (Retrieval-Augmented STylistic\nAlignment), a method that leverages learned stylistic concepts to encourage LLM\ntranslation to appropriately convey cultural communication norms and align\nstyle.", "AI": {"tldr": "论文探讨了LLMs在跨文化风格翻译中的失败，并提出RASTA方法以改善风格对齐。", "motivation": "文化差异常导致说话者意图与听者理解之间的风格不对齐，例如礼貌在翻译中丢失。", "method": "提出RASTA（检索增强的风格对齐）方法，利用学习到的风格概念促进LLM翻译的文化规范对齐。", "result": "LLMs在非西方语言中表现较差，且偏向中性翻译。RASTA方法有效改善了风格对齐。", "conclusion": "RASTA方法能帮助LLMs更好地传达文化沟通规范，提升翻译中的风格对齐。"}}
{"id": "2507.00045", "pdf": "https://arxiv.org/pdf/2507.00045", "abs": "https://arxiv.org/abs/2507.00045", "authors": ["Ming Li", "Chenguang Wang", "Yijun Liang", "Xiyao Wang", "Yuhang Zhou", "Xiyang Wu", "Yuqing Zhang", "Ruiyi Zhang", "Tianyi Zhou"], "title": "CaughtCheating: Is Your MLLM a Good Cheating Detective? Exploring the Boundary of Visual Perception and Reasoning", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Recent agentic Multi-Modal Large Language Models (MLLMs) such as GPT-o3 have\nachieved near-ceiling scores on various existing benchmarks, motivating a\ndemand for more challenging test tasks. These MLLMs have been reported to excel\nin a few expert-level tasks for humans, e.g., GeoGuesser, reflecting their\npotential as a detective who can notice minuscule cues in an image and weave\nthem into coherent, situational explanations, leading to a reliable answer. But\ncan they match the performance of excellent human detectives? To answer this\nquestion, we investigate some hard scenarios where GPT-o3 can still handle, and\nfind a common scenario where o3's performance drops to nearly zero, which we\nname CaughtCheating. It is inspired by the social media requests that ask\nothers to detect suspicious clues from photos shared by the poster's partner.\nWe conduct extensive experiments and analysis to understand why existing MLLMs\nlack sufficient capability to solve this kind of task. CaughtCheating provides\na class of challenging visual perception and reasoning tasks with great value\nand practical usage. Success in these tasks paves the way for MLLMs to acquire\nhuman-level detective perception and reasoning capabilities.", "AI": {"tldr": "论文探讨了多模态大语言模型（MLLMs）在复杂视觉推理任务中的局限性，提出了名为CaughtCheating的挑战性任务，发现现有模型在此类任务中表现极差。", "motivation": "现有MLLMs在多数基准测试中表现优异，但在某些人类专家级任务（如侦探推理）中仍有不足，需更复杂的测试任务以评估其能力。", "method": "通过设计CaughtCheating任务（基于社交媒体中检测伴侣照片可疑线索的场景），对GPT-o3等MLLMs进行实验和分析。", "result": "发现GPT-o3在CaughtCheating任务中表现近乎为零，揭示了现有模型在复杂视觉感知和推理上的不足。", "conclusion": "CaughtCheating任务为MLLMs提供了具有实际价值的挑战，成功解决此类任务将推动模型达到人类侦探水平的感知和推理能力。"}}
{"id": "2507.00181", "pdf": "https://arxiv.org/pdf/2507.00181", "abs": "https://arxiv.org/abs/2507.00181", "authors": ["Georgios P. Georgiou"], "title": "ChatGPT produces more \"lazy\" thinkers: Evidence of cognitive engagement decline", "categories": ["cs.AI"], "comment": null, "summary": "Despite the increasing use of large language models (LLMs) in education,\nconcerns have emerged about their potential to reduce deep thinking and active\nlearning. This study investigates the impact of generative artificial\nintelligence (AI) tools, specifically ChatGPT, on the cognitive engagement of\nstudents during academic writing tasks. The study employed an experimental\ndesign with participants randomly assigned to either an AI-assisted (ChatGPT)\nor a non-assisted (control) condition. Participants completed a structured\nargumentative writing task followed by a cognitive engagement scale (CES), the\nCES-AI, developed to assess mental effort, attention, deep processing, and\nstrategic thinking. The results revealed significantly lower cognitive\nengagement scores in the ChatGPT group compared to the control group. These\nfindings suggest that AI assistance may lead to cognitive offloading. The study\ncontributes to the growing body of literature on the psychological implications\nof AI in education and raises important questions about the integration of such\ntools into academic practice. It calls for pedagogical strategies that promote\nactive, reflective engagement with AI-generated content to avoid compromising\nself-regulated learning and deep cognitive involvement of students.", "AI": {"tldr": "研究发现，使用ChatGPT辅助写作的学生认知参与度显著低于未使用AI的对照组，表明AI可能导致认知卸载。", "motivation": "探讨生成式AI工具（如ChatGPT）对学生学术写作任务中认知参与的影响。", "method": "采用实验设计，随机分配学生至AI辅助（ChatGPT）或非辅助（对照组）条件，完成议论文写作任务并使用CES-AI量表评估认知参与度。", "result": "ChatGPT组的认知参与度显著低于对照组。", "conclusion": "AI辅助可能削弱学生的深度认知参与，需结合教学策略以促进主动反思性学习。"}}
{"id": "2507.00239", "pdf": "https://arxiv.org/pdf/2507.00239", "abs": "https://arxiv.org/abs/2507.00239", "authors": ["Aryan Shrivastava", "Ari Holtzman"], "title": "Linearly Decoding Refused Knowledge in Aligned Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Most commonly used language models (LMs) are instruction-tuned and aligned\nusing a combination of fine-tuning and reinforcement learning, causing them to\nrefuse users requests deemed harmful by the model. However, jailbreak prompts\ncan often bypass these refusal mechanisms and elicit harmful responses. In this\nwork, we study the extent to which information accessed via jailbreak prompts\nis decodable using linear probes trained on LM hidden states. We show that a\ngreat deal of initially refused information is linearly decodable. For example,\nacross models, the response of a jailbroken LM for the average IQ of a country\ncan be predicted by a linear probe with Pearson correlations exceeding $0.8$.\nSurprisingly, we find that probes trained on base models (which do not refuse)\nsometimes transfer to their instruction-tuned versions and are capable of\nrevealing information that jailbreaks decode generatively, suggesting that the\ninternal representations of many refused properties persist from base LMs\nthrough instruction-tuning. Importantly, we show that this information is not\nmerely \"leftover\" in instruction-tuned models, but is actively used by them: we\nfind that probe-predicted values correlate with LM generated pairwise\ncomparisons, indicating that the information decoded by our probes align with\nsuppressed generative behavior that may be expressed more subtly in other\ndownstream tasks. Overall, our results suggest that instruction-tuning does not\nwholly eliminate or even relocate harmful information in representation\nspace-they merely suppress its direct expression, leaving it both linearly\naccessible and indirectly influential in downstream behavior.", "AI": {"tldr": "研究发现，即使经过指令微调和对齐的语言模型（LMs）仍可通过线性探针解码被拒绝的有害信息，表明这些信息在表示空间中未被消除。", "motivation": "探讨指令微调是否真正消除了语言模型中的有害信息，或仅是抑制了其直接表达。", "method": "使用线性探针训练在LM隐藏状态上，解码被拒绝的信息，并测试其在指令微调模型中的可解码性。", "result": "线性探针能高精度解码被拒绝的信息（如国家平均IQ），且基模型探针可迁移到指令微调模型。", "conclusion": "指令微调未消除有害信息，仅抑制其直接表达，这些信息仍线性可解码并间接影响下游行为。"}}
{"id": "2507.00046", "pdf": "https://arxiv.org/pdf/2507.00046", "abs": "https://arxiv.org/abs/2507.00046", "authors": ["Akshansh Mishra", "Eyob Mesele Sefene", "Shivraman Thapliyal"], "title": "Evolutionary computing-based image segmentation method to detect defects and features in Additive Friction Stir Deposition Process", "categories": ["cs.CV", "cs.CE"], "comment": "7 pages, 4 figures", "summary": "This work proposes an evolutionary computing-based image segmentation\napproach for analyzing soundness in Additive Friction Stir Deposition (AFSD)\nprocesses. Particle Swarm Optimization (PSO) was employed to determine optimal\nsegmentation thresholds for detecting defects and features in multilayer AFSD\nbuilds. The methodology integrates gradient magnitude analysis with distance\ntransforms to create novel attention-weighted visualizations that highlight\ncritical interface regions. Five AFSD samples processed under different\nconditions were analyzed using multiple visualization techniques i.e.\nself-attention maps, and multi-channel visualization. These complementary\napproaches reveal subtle material transition zones and potential defect regions\nwhich were not readily observable through conventional imaging. The PSO\nalgorithm automatically identified optimal threshold values (ranging from\n156-173) for each sample, enabling precise segmentation of material interfaces.\nThe multi-channel visualization technique effectively combines boundary\ninformation (red channel), spatial relationships (green channel), and material\ndensity data (blue channel) into cohesive representations that quantify\ninterface quality. The results demonstrate that attention-based analysis\nsuccessfully identifies regions of incomplete bonding and inhomogeneities in\nAFSD joints, providing quantitative metrics for process optimization and\nquality assessment of additively manufactured components.", "AI": {"tldr": "提出了一种基于进化计算的图像分割方法，用于分析增材摩擦搅拌沉积（AFSD）过程中的缺陷，结合粒子群优化（PSO）和多通道可视化技术。", "motivation": "传统成像方法难以检测AFSD中的细微缺陷和材料过渡区，需要更精确的自动化分析工具。", "method": "采用PSO算法确定最佳分割阈值，结合梯度幅度分析和距离变换生成注意力加权可视化。", "result": "PSO自动确定阈值（156-173），多通道可视化成功识别未完全结合区域和不均匀性。", "conclusion": "该方法为AFSD工艺优化和质量评估提供了定量指标。"}}
{"id": "2507.00205", "pdf": "https://arxiv.org/pdf/2507.00205", "abs": "https://arxiv.org/abs/2507.00205", "authors": ["Periklis Petridis", "Georgios Margaritis", "Vasiliki Stoumpou", "Dimitris Bertsimas"], "title": "Holistic Artificial Intelligence in Medicine; improved performance and explainability", "categories": ["cs.AI", "cs.LG"], "comment": "Submitted to npj Digital Medicine", "summary": "With the increasing interest in deploying Artificial Intelligence in\nmedicine, we previously introduced HAIM (Holistic AI in Medicine), a framework\nthat fuses multimodal data to solve downstream clinical tasks. However, HAIM\nuses data in a task-agnostic manner and lacks explainability. To address these\nlimitations, we introduce xHAIM (Explainable HAIM), a novel framework\nleveraging Generative AI to enhance both prediction and explainability through\nfour structured steps: (1) automatically identifying task-relevant patient data\nacross modalities, (2) generating comprehensive patient summaries, (3) using\nthese summaries for improved predictive modeling, and (4) providing clinical\nexplanations by linking predictions to patient-specific medical knowledge.\nEvaluated on the HAIM-MIMIC-MM dataset, xHAIM improves average AUC from 79.9%\nto 90.3% across chest pathology and operative tasks. Importantly, xHAIM\ntransforms AI from a black-box predictor into an explainable decision support\nsystem, enabling clinicians to interactively trace predictions back to relevant\npatient data, bridging AI advancements with clinical utility.", "AI": {"tldr": "xHAIM是一个基于生成式AI的框架，通过任务相关数据识别、患者摘要生成、预测建模和临床解释四步，提升医疗AI的可解释性和性能。", "motivation": "解决HAIM框架在任务无关性和缺乏可解释性方面的局限性。", "method": "利用生成式AI，通过四步结构（数据识别、摘要生成、预测建模、临床解释）增强预测和可解释性。", "result": "在HAIM-MIMIC-MM数据集上，xHAIM将平均AUC从79.9%提升至90.3%。", "conclusion": "xHAIM将AI从黑盒预测器转变为可解释的决策支持系统，提升临床实用性。"}}
{"id": "2507.00244", "pdf": "https://arxiv.org/pdf/2507.00244", "abs": "https://arxiv.org/abs/2507.00244", "authors": ["Isabella Senturia", "Matilde Marcolli"], "title": "The Algebraic Structure of Morphosyntax", "categories": ["cs.CL", "math.QA", "91F20, 18M60, 68Q70"], "comment": "45 pages, LaTeX, 2 png figures", "summary": "Within the context of the mathematical formulation of Merge and the Strong\nMinimalist Thesis, we present a mathematical model of the morphology-syntax\ninterface. In this setting, morphology has compositional properties responsible\nfor word formation, organized into a magma of morphological trees. However,\nunlike syntax, we do not have movement within morphology. A coproduct\ndecomposition exists, but it requires extending the set of morphological trees\nbeyond those which are generated solely by the magma, to a larger set of\npossible morphological inputs to syntactic trees. These participate in the\nformation of morphosyntactic trees as an algebra over an operad, and a\ncorrespondence between algebras over an operad. The process of structure\nformation for morphosyntactic trees can then be described in terms of this\noperadic correspondence that pairs syntactic and morphological data and the\nmorphology coproduct. We reinterpret in this setting certain operations of\nDistributed Morphology as transformation that allow for flexibility in moving\nthe boundary between syntax and morphology within the morphosyntactic objects.", "AI": {"tldr": "论文提出了一种基于数学模型的形态-句法接口理论，结合Merge和强极简论题，通过代数操作描述形态与句法的交互。", "motivation": "研究动机在于从数学角度形式化形态与句法的接口，尤其是形态树的组合性质及其与句法树的交互。", "method": "方法包括将形态树组织为magma，并通过代数操作（如operad）扩展形态树集，以描述形态-句法树的形成过程。", "result": "结果表明，形态与句法的边界可以通过代数操作灵活调整，且分布式形态学的某些操作可被重新解释为这种框架下的变换。", "conclusion": "结论指出，该模型为形态-句法接口提供了严格的数学描述，并支持形态与句法边界的灵活性。"}}
{"id": "2507.00049", "pdf": "https://arxiv.org/pdf/2507.00049", "abs": "https://arxiv.org/abs/2507.00049", "authors": ["Feiyang Kang", "Nadine Chang", "Maying Shen", "Marc T. Law", "Rafid Mahmood", "Ruoxi Jia", "Jose M. Alvarez"], "title": "AdaDeDup: Adaptive Hybrid Data Pruning for Efficient Large-Scale Object Detection Training", "categories": ["cs.CV", "cs.LG"], "comment": "Preprint", "summary": "The computational burden and inherent redundancy of large-scale datasets\nchallenge the training of contemporary machine learning models. Data pruning\noffers a solution by selecting smaller, informative subsets, yet existing\nmethods struggle: density-based approaches can be task-agnostic, while\nmodel-based techniques may introduce redundancy or prove computationally\nprohibitive. We introduce Adaptive De-Duplication (AdaDeDup), a novel hybrid\nframework that synergistically integrates density-based pruning with\nmodel-informed feedback in a cluster-adaptive manner. AdaDeDup first partitions\ndata and applies an initial density-based pruning. It then employs a proxy\nmodel to evaluate the impact of this initial pruning within each cluster by\ncomparing losses on kept versus pruned samples. This task-aware signal\nadaptively adjusts cluster-specific pruning thresholds, enabling more\naggressive pruning in redundant clusters while preserving critical data in\ninformative ones. Extensive experiments on large-scale object detection\nbenchmarks (Waymo, COCO, nuScenes) using standard models (BEVFormer, Faster\nR-CNN) demonstrate AdaDeDup's advantages. It significantly outperforms\nprominent baselines, substantially reduces performance degradation (e.g., over\n54% versus random sampling on Waymo), and achieves near-original model\nperformance while pruning 20% of data, highlighting its efficacy in enhancing\ndata efficiency for large-scale model training. Code is open-sourced.", "AI": {"tldr": "AdaDeDup是一种混合框架，结合密度剪枝和模型反馈，自适应调整剪枝阈值，显著提升数据效率。", "motivation": "大规模数据集的冗余和计算负担挑战机器学习模型训练，现有剪枝方法存在任务无关性或计算成本高的问题。", "method": "AdaDeDup先分区并进行密度剪枝，再用代理模型评估剪枝效果，自适应调整阈值。", "result": "在多个基准测试中，AdaDeDup显著优于基线方法，减少性能下降，剪枝20%数据仍接近原始性能。", "conclusion": "AdaDeDup有效提升大规模模型训练的数据效率，代码已开源。"}}
{"id": "2507.00218", "pdf": "https://arxiv.org/pdf/2507.00218", "abs": "https://arxiv.org/abs/2507.00218", "authors": ["Fangting Zhou", "Attila Lischka", "Balazs Kulcsar", "Jiaming Wu", "Morteza Haghir Chehreghani", "Gilbert Laporte"], "title": "Learning for routing: A guided review of recent developments and future directions", "categories": ["cs.AI", "math.OC"], "comment": "Accepted for publication in Transportation Research Part E: Logistics\n  and Transportation Review", "summary": "This paper reviews the current progress in applying machine learning (ML)\ntools to solve NP-hard combinatorial optimization problems, with a focus on\nrouting problems such as the traveling salesman problem (TSP) and the vehicle\nrouting problem (VRP). Due to the inherent complexity of these problems, exact\nalgorithms often require excessive computational time to find optimal\nsolutions, while heuristics can only provide approximate solutions without\nguaranteeing optimality. With the recent success of machine learning models,\nthere is a growing trend in proposing and implementing diverse ML techniques to\nenhance the resolution of these challenging routing problems. We propose a\ntaxonomy categorizing ML-based routing methods into construction-based and\nimprovement-based approaches, highlighting their applicability to various\nproblem characteristics. This review aims to integrate traditional OR methods\nwith state-of-the-art ML techniques, providing a structured framework to guide\nfuture research and address emerging VRP variants.", "AI": {"tldr": "本文综述了机器学习在NP难组合优化问题（如TSP和VRP）中的应用进展，提出了基于构造和改进的ML方法分类，旨在整合传统OR方法与现代ML技术。", "motivation": "由于NP难问题的复杂性，传统算法效率低，启发式方法无法保证最优性，而ML的成功应用为解决这些问题提供了新思路。", "method": "提出了基于构造和改进的ML方法分类，并分析了其在不同问题特性中的适用性。", "result": "综述了ML在路由问题中的应用，为未来研究提供了结构化框架。", "conclusion": "整合传统OR与ML技术，为新兴VRP变体的研究提供了指导。"}}
{"id": "2507.00246", "pdf": "https://arxiv.org/pdf/2507.00246", "abs": "https://arxiv.org/abs/2507.00246", "authors": ["Sanchit Ahuja", "Praneetha Vaddamanu", "Barun Patra"], "title": "EfficientXLang: Towards Improving Token Efficiency Through Cross-Lingual Reasoning", "categories": ["cs.CL"], "comment": "15 pages, 5 figures, 9 tables", "summary": "Despite recent advances in Language Reasoning Models (LRMs), most research\nfocuses solely on English, even though many models are pretrained on\nmultilingual data. In this work, we investigate: Is English the most\ntoken-efficient language for reasoning? We evaluate three open-source RLMs:\nDeepSeek R1, Qwen 2.5 and Qwen 3, across four math datasets and seven\ntypologically diverse languages. We find that reasoning in non-English\nlanguages not only reduces token usage, but also preserves accuracy. These\ngains persist even after translating the reasoning traces into English,\nsuggesting genuine shifts in reasoning behavior rather than surface-level\nlinguistic effects. The extent of improvement, however, depends on the models\nmultilingual strength. Our findings motivate a broader view of reasoning in\nlanguage models, highlighting the potential of multilingual reasoning and the\nimportance of strong multilingual foundations. The code for our work can be\nfound: https://github.com/microsoft/EfficientXLang.", "AI": {"tldr": "研究发现，非英语语言在推理任务中不仅减少token使用，还能保持准确性，且效果优于英语。", "motivation": "探讨英语是否是最高效的推理语言，并验证多语言推理的潜力。", "method": "评估三个开源语言推理模型（DeepSeek R1、Qwen 2.5和Qwen 3）在四个数学数据集和七种语言上的表现。", "result": "非英语语言在推理中更高效，且准确性不受影响；多语言能力强的模型表现更优。", "conclusion": "多语言推理具有潜力，强调了强大多语言基础的重要性。"}}
{"id": "2507.00052", "pdf": "https://arxiv.org/pdf/2507.00052", "abs": "https://arxiv.org/abs/2507.00052", "authors": ["Binesh Sadanandan", "Vahid Behzadan"], "title": "VSF-Med:A Vulnerability Scoring Framework for Medical Vision-Language Models", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Vision Language Models (VLMs) hold great promise for streamlining\nlabour-intensive medical imaging workflows, yet systematic security evaluations\nin clinical settings remain scarce. We introduce VSF--Med, an end-to-end\nvulnerability-scoring framework for medical VLMs that unites three novel\ncomponents: (i) a rich library of sophisticated text-prompt attack templates\ntargeting emerging threat vectors; (ii) imperceptible visual perturbations\ncalibrated by structural similarity (SSIM) thresholds to preserve clinical\nrealism; and (iii) an eight-dimensional rubric evaluated by two independent\njudge LLMs, whose raw scores are consolidated via z-score normalization to\nyield a 0--32 composite risk metric. Built entirely on publicly available\ndatasets and accompanied by open-source code, VSF--Med synthesizes over 30,000\nadversarial variants from 5,000 radiology images and enables reproducible\nbenchmarking of any medical VLM with a single command. Our consolidated\nanalysis reports mean z-score shifts of $0.90\\sigma$ for\npersistence-of-attack-effects, $0.74\\sigma$ for prompt-injection effectiveness,\nand $0.63\\sigma$ for safety-bypass success across state-of-the-art VLMs.\nNotably, Llama-3.2-11B-Vision-Instruct exhibits a peak vulnerability increase\nof $1.29\\sigma$ for persistence-of-attack-effects, while GPT-4o shows increases\nof $0.69\\sigma$ for that same vector and $0.28\\sigma$ for prompt-injection\nattacks.", "AI": {"tldr": "VSF-Med是一个端到端的医疗视觉语言模型（VLM）漏洞评分框架，结合了文本提示攻击模板、视觉扰动和八维评分标准，用于评估医疗VLM的安全性。", "motivation": "医疗VLM在临床应用中潜力巨大，但缺乏系统性的安全性评估，因此需要开发一个标准化框架来识别和量化其漏洞。", "method": "VSF-Med整合了三种新组件：文本攻击模板库、基于SSIM的视觉扰动和八维评分标准，并通过z-score归一化生成综合风险指标。", "result": "在30,000个对抗样本测试中，VSF-Med显示主流VLM在攻击持续性、提示注入和安全性绕过方面存在显著漏洞，如Llama-3.2-11B和GPT-4o的z-score变化明显。", "conclusion": "VSF-Med为医疗VLM的安全性评估提供了标准化工具，揭示了现有模型的潜在风险，并支持可复现的基准测试。"}}
{"id": "2507.00417", "pdf": "https://arxiv.org/pdf/2507.00417", "abs": "https://arxiv.org/abs/2507.00417", "authors": ["Joongwon Kim", "Anirudh Goyal", "Liang Tan", "Hannaneh Hajishirzi", "Srinivasan Iyer", "Tianlu Wang"], "title": "ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context", "categories": ["cs.AI", "cs.CL"], "comment": "36 pages, 23 figures", "summary": "We introduce ASTRO, the \"Autoregressive Search-Taught Reasoner\", a framework\nfor training language models to reason like search algorithms, explicitly\nleveraging self-reflection, backtracking, and exploration in their outputs.\nRecently, training large language models (LLMs) via reinforcement learning (RL)\nhas led to the advent of reasoning models with greatly enhanced reasoning\ncapabilities. Open-source replications of reasoning models, while successful,\nbuild upon models that already exhibit strong reasoning capabilities along with\nsearch behavior observed even before RL. As a result, it is yet unclear how to\nboost the reasoning capabilities of other non-reasoner models including Llama\n3. ASTRO teaches such models to internalize structured search behavior through\na synthetic dataset derived from Monte Carlo Tree Search (MCTS) over\nmathematical problem-solving trajectories. By converting search traces into\nnatural language chain-of-thoughts that capture both successes and recoveries\nfrom failure, ASTRO bootstraps models with a rich prior for exploration during\nRL. We finetune our models on these search-derived traces and further improve\nperformance via RL with verifiable rewards. We apply ASTRO to the Llama 3\nfamily of models and achieve absolute performance gains of 16.0% on MATH-500,\n26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon\nchallenging problems that require iterative correction. Our results demonstrate\nthat search-inspired training offers a principled way to instill robust\nreasoning capabilities into open LLMs.", "AI": {"tldr": "ASTRO是一个通过自回归搜索训练语言模型推理能力的框架，利用蒙特卡洛树搜索生成的数据集，结合强化学习，显著提升了Llama 3等模型在数学问题上的表现。", "motivation": "当前开源推理模型依赖已有强推理能力的模型，而ASTRO旨在为非推理模型（如Llama 3）提供搜索行为内化的方法。", "method": "通过蒙特卡洛树搜索生成数学问题解决轨迹的合成数据集，将搜索痕迹转化为自然语言链式思维，结合强化学习微调模型。", "result": "在MATH-500、AMC 2023和AIME 2024等数学问题上，性能分别提升16.0%、26.9%和20.0%，尤其在需要迭代修正的难题上表现突出。", "conclusion": "ASTRO证明了搜索启发的训练方法能有效为开源大语言模型注入稳健的推理能力。"}}
{"id": "2507.00258", "pdf": "https://arxiv.org/pdf/2507.00258", "abs": "https://arxiv.org/abs/2507.00258", "authors": ["Jie Hou", "Chuxiong Wu", "Lannan Luo", "Qiang Zeng"], "title": "Impact of Fine-Tuning Methods on Memorization in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As the capabilities of pre-trained large language models (LLMs) continue to\nadvance, the \"pre-train and fine-tune\" paradigm has become increasingly\nmainstream, leading to the development of various fine-tuning methods. However,\nthe privacy risks arising from memorization during fine-tuning have received\nrelatively little attention. To address this gap, we categorize popular\nfine-tuning approaches and assess their impact on memorization through the lens\nof membership inference attacks (MIAs). Our results show that, compared to\nparameter-based fine-tuning, prompt-based fine-tuning achieves competitive\nperformance while exhibiting lower vulnerability to MIAs. Furthermore,\nprompt-based methods maintain low memorization regardless of model scale. These\nfindings suggest that parameter-based fine-tuning is more prone to leaking\nprivate information, whereas prompt-based fine-tuning serves as a more\nprivacy-preserving option.", "AI": {"tldr": "研究表明，基于提示的微调在性能上与基于参数的微调相当，但更隐私安全，不易受到成员推理攻击（MIAs）的影响。", "motivation": "探讨不同微调方法对隐私风险的影响，特别是成员推理攻击（MIAs）的脆弱性。", "method": "分类流行的微调方法，并通过成员推理攻击（MIAs）评估其对记忆化的影响。", "result": "基于提示的微调在性能上具有竞争力，且对MIAs的脆弱性较低，无论模型规模如何。", "conclusion": "基于参数的微调更容易泄露隐私信息，而基于提示的微调是一种更隐私安全的替代方案。"}}
{"id": "2507.00068", "pdf": "https://arxiv.org/pdf/2507.00068", "abs": "https://arxiv.org/abs/2507.00068", "authors": ["Ziqi Zhong", "Daniel Tang"], "title": "MANTA: Cross-Modal Semantic Alignment and Information-Theoretic Optimization for Long-form Multimodal Understanding", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "While multi-modal learning has advanced significantly, current approaches\noften treat modalities separately, creating inconsistencies in representation\nand reasoning. We introduce MANTA (Multi-modal Abstraction and Normalization\nvia Textual Alignment), a theoretically-grounded framework that unifies visual\nand auditory inputs into a structured textual space for seamless processing\nwith large language models. MANTA addresses four key challenges: (1) semantic\nalignment across modalities with information-theoretic optimization, (2)\nadaptive temporal synchronization for varying information densities, (3)\nhierarchical content representation for multi-scale understanding, and (4)\ncontext-aware retrieval of sparse information from long sequences. We formalize\nour approach within a rigorous mathematical framework, proving its optimality\nfor context selection under token constraints. Extensive experiments on the\nchallenging task of Long Video Question Answering show that MANTA improves\nstate-of-the-art models by up to 22.6% in overall accuracy, with particularly\nsignificant gains (27.3%) on videos exceeding 30 minutes. Additionally, we\ndemonstrate MANTA's superiority on temporal reasoning tasks (23.8% improvement)\nand cross-modal understanding (25.1% improvement). Our framework introduces\nnovel density estimation techniques for redundancy minimization while\npreserving rare signals, establishing new foundations for unifying multimodal\nrepresentations through structured text.", "AI": {"tldr": "MANTA是一个多模态学习框架，通过文本对齐统一视觉和听觉输入，解决了语义对齐、时间同步、多尺度表示和稀疏信息检索等挑战，显著提升了长视频问答和跨模态理解的性能。", "motivation": "当前多模态学习方法通常将不同模态分开处理，导致表示和推理的不一致性。MANTA旨在通过统一的文本空间解决这一问题。", "method": "MANTA采用信息论优化实现跨模态语义对齐，自适应时间同步，分层内容表示和上下文感知检索，并通过数学框架证明其最优性。", "result": "在长视频问答任务中，MANTA将最先进模型的准确率提高了22.6%，在30分钟以上的视频中提升27.3%，同时在时间推理和跨模态理解任务中也有显著改进。", "conclusion": "MANTA通过新颖的密度估计技术最小化冗余并保留稀有信号，为多模态表示的统一提供了新基础。"}}
{"id": "2507.00432", "pdf": "https://arxiv.org/pdf/2507.00432", "abs": "https://arxiv.org/abs/2507.00432", "authors": ["Maggie Huan", "Yuetai Li", "Tuney Zheng", "Xiaoyu Xu", "Seungone Kim", "Minxin Du", "Radha Poovendran", "Graham Neubig", "Xiang Yue"], "title": "Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Math reasoning has become the poster child of progress in large language\nmodels (LLMs), with new models rapidly surpassing human-level performance on\nbenchmarks like MATH and AIME. But as math leaderboards improve week by week,\nit is worth asking: do these gains reflect broader problem-solving ability or\njust narrow overfitting? To answer this question, we evaluate over 20\nopen-weight reasoning-tuned models across a broad suite of tasks, including\nmath, scientific QA, agent planning, coding, and standard\ninstruction-following. We surprisingly find that most models that succeed in\nmath fail to transfer their gains to other domains. To rigorously study this\nphenomenon, we conduct controlled experiments on Qwen3-14B models using\nmath-only data but different tuning methods. We find that reinforcement\nlearning (RL)-tuned models generalize well across domains, while supervised\nfine-tuning (SFT)-tuned models often forget general capabilities. Latent-space\nrepresentation and token-space distribution shift analyses reveal that SFT\ninduces substantial representation and output drift, while RL preserves\ngeneral-domain structure. Our results suggest a need to rethink standard\npost-training recipes, particularly the reliance on SFT-distilled data for\nadvancing reasoning models.", "AI": {"tldr": "研究发现，尽管LLMs在数学推理任务上表现优异，但多数模型无法将这种能力迁移到其他领域。强化学习调优的模型表现更好，而监督微调会导致能力退化。", "motivation": "探讨LLMs在数学推理上的进步是否反映其整体问题解决能力，还是仅局限于数学领域的过拟合。", "method": "评估20多个开源推理调优模型，涵盖数学、科学问答、规划、编码等任务，并对Qwen3-14B模型进行数学数据调优实验。", "result": "多数数学表现优秀的模型在其他领域表现不佳；强化学习调优的模型泛化能力更强，监督微调则导致能力退化。", "conclusion": "需重新考虑后训练方法，减少对监督微调数据的依赖，以提升模型的通用推理能力。"}}
{"id": "2507.00297", "pdf": "https://arxiv.org/pdf/2507.00297", "abs": "https://arxiv.org/abs/2507.00297", "authors": ["David Ifeoluwa Adelani"], "title": "Natural language processing for African languages", "categories": ["cs.CL", "cs.AI"], "comment": "PhD thesis", "summary": "Recent advances in word embeddings and language models use large-scale,\nunlabelled data and self-supervised learning to boost NLP performance.\nMultilingual models, often trained on web-sourced data like Wikipedia, face\nchallenges: few low-resource languages are included, their data is often noisy,\nand lack of labeled datasets makes it hard to evaluate performance outside\nhigh-resource languages like English. In this dissertation, we focus on\nlanguages spoken in Sub-Saharan Africa where all the indigenous languages in\nthis region can be regarded as low-resourced in terms of the availability of\nlabelled data for NLP tasks and unlabelled data found on the web. We analyse\nthe noise in the publicly available corpora, and curate a high-quality corpus,\ndemonstrating that the quality of semantic representations learned in word\nembeddings does not only depend on the amount of data but on the quality of\npre-training data. We demonstrate empirically the limitations of word\nembeddings, and the opportunities the multilingual pre-trained language model\n(PLM) offers especially for languages unseen during pre-training and\nlow-resource scenarios. We further study how to adapt and specialize\nmultilingual PLMs to unseen African languages using a small amount of\nmonolingual texts. To address the under-representation of the African languages\nin NLP research, we developed large scale human-annotated labelled datasets for\n21 African languages in two impactful NLP tasks: named entity recognition and\nmachine translation. We conduct an extensive empirical evaluation using\nstate-of-the-art methods across supervised, weakly-supervised, and transfer\nlearning settings.", "AI": {"tldr": "论文探讨了低资源语言（如撒哈拉以南非洲语言）在NLP中的挑战，提出通过高质量语料库和多语言预训练模型提升性能，并开发了标注数据集以支持研究。", "motivation": "解决低资源语言在NLP中的代表性不足问题，尤其是撒哈拉以南非洲语言，因缺乏标注数据和高质量语料库而难以评估和改进模型性能。", "method": "分析公开语料库的噪声，构建高质量语料库；研究多语言预训练模型在低资源语言中的适应性和优化方法；开发21种非洲语言的标注数据集。", "result": "证明了数据质量对词嵌入语义表示的重要性；多语言预训练模型在低资源语言中表现潜力；标注数据集为NLP任务提供了支持。", "conclusion": "通过高质量数据和模型优化，可以有效提升低资源语言的NLP性能，填补非洲语言在NLP研究中的空白。"}}
{"id": "2507.00070", "pdf": "https://arxiv.org/pdf/2507.00070", "abs": "https://arxiv.org/abs/2507.00070", "authors": ["Bosubabu Sambana", "Hillary Sunday Nnadi", "Mohd Anas Wajid", "Nwosu Ogochukwu Fidelia", "Claudia Camacho-Zuñiga", "Henry Dozie Ajuzie", "Edeh Michael Onyema"], "title": "An efficient plant disease detection using transfer learning approach", "categories": ["cs.CV", "cs.AI"], "comment": "15 pages , 4 figures. Scientific Reports 2025", "summary": "Plant diseases pose significant challenges to farmers and the agricultural\nsector at large. However, early detection of plant diseases is crucial to\nmitigating their effects and preventing widespread damage, as outbreaks can\nseverely impact the productivity and quality of crops. With advancements in\ntechnology, there are increasing opportunities for automating the monitoring\nand detection of disease outbreaks in plants. This study proposed a system\ndesigned to identify and monitor plant diseases using a transfer learning\napproach. Specifically, the study utilizes YOLOv7 and YOLOv8, two\nstate-ofthe-art models in the field of object detection. By fine-tuning these\nmodels on a dataset of plant leaf images, the system is able to accurately\ndetect the presence of Bacteria, Fungi and Viral diseases such as Powdery\nMildew, Angular Leaf Spot, Early blight and Tomato mosaic virus. The model's\nperformance was evaluated using several metrics, including mean Average\nPrecision (mAP), F1-score, Precision, and Recall, yielding values of 91.05,\n89.40, 91.22, and 87.66, respectively. The result demonstrates the superior\neffectiveness and efficiency of YOLOv8 compared to other object detection\nmethods, highlighting its potential for use in modern agricultural practices.\nThe approach provides a scalable, automated solution for early any plant\ndisease detection, contributing to enhanced crop yield, reduced reliance on\nmanual monitoring, and supporting sustainable agricultural practices.", "AI": {"tldr": "该研究提出了一种基于YOLOv7和YOLOv8的植物病害检测系统，通过迁移学习方法准确识别细菌、真菌和病毒病害，性能优于其他方法。", "motivation": "植物病害对农业造成严重影响，早期检测至关重要。技术进步为自动化监测提供了机会。", "method": "使用YOLOv7和YOLOv8模型，通过迁移学习在植物叶片图像数据集上进行微调。", "result": "模型性能优异，mAP为91.05，F1-score为89.40，YOLOv8表现最佳。", "conclusion": "该系统为植物病害早期检测提供了高效、可扩展的自动化解决方案，有助于提高作物产量和可持续农业。"}}
{"id": "2507.00557", "pdf": "https://arxiv.org/pdf/2507.00557", "abs": "https://arxiv.org/abs/2507.00557", "authors": ["Tianyi Ding", "Haokun Li", "Xinpeng Ni", "Bican Xia", "Tianqi Zhao"], "title": "Advancing Local Search in SMT-NRA with MCSAT Integration", "categories": ["cs.AI", "cs.LO", "cs.SC"], "comment": null, "summary": "In this paper, we advance local search for Satisfiability Modulo the Theory\nof Nonlinear Real Arithmetic (SMT-NRA for short). First, we introduce a\ntwo-dimensional cell-jump move, called \\emph{$2d$-cell-jump}, generalizing the\nkey operation, cell-jump, of the local search method for SMT-NRA. Then, we\npropose an extended local search framework, named \\emph{$2d$-LS} (following the\nlocal search framework, LS, for SMT-NRA), integrating the model constructing\nsatisfiability calculus (MCSAT) framework to improve search efficiency. To\nfurther improve the efficiency of MCSAT, we implement a recently proposed\ntechnique called \\emph{sample-cell projection operator} for MCSAT, which is\nwell suited for CDCL-style search in the real domain and helps guide the search\naway from conflicting states. Finally, we design a hybrid framework for SMT-NRA\ncombining MCSAT, $2d$-LS and OpenCAD, to improve search efficiency through\ninformation exchange. The experimental results demonstrate improvements in\nlocal search performance, highlighting the effectiveness of the proposed\nmethods.", "AI": {"tldr": "本文提出了一种名为$2d$-cell-jump的二维单元跳跃操作，扩展了SMT-NRA的局部搜索方法，并结合MCSAT框架和sample-cell投影算子提升搜索效率。实验证明该方法有效。", "motivation": "提升SMT-NRA（非线性实数算术的可满足性模理论）的局部搜索效率。", "method": "引入$2d$-cell-jump操作，扩展$2d$-LS框架，结合MCSAT和sample-cell投影算子，设计混合框架。", "result": "实验结果表明局部搜索性能有所提升。", "conclusion": "提出的方法有效提升了SMT-NRA的搜索效率。"}}
{"id": "2507.00322", "pdf": "https://arxiv.org/pdf/2507.00322", "abs": "https://arxiv.org/abs/2507.00322", "authors": ["Daking Rai", "Samuel Miller", "Kevin Moran", "Ziyu Yao"], "title": "Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones", "categories": ["cs.CL", "cs.AI", "cs.SE", "I.2.7"], "comment": "23 pages, 10 figures, Preprint", "summary": "Despite remarkable advances in coding capabilities, language models (LMs)\nstill struggle with simple syntactic tasks such as generating balanced\nparentheses. In this study, we investigate the underlying mechanisms behind the\npersistence of these errors across LMs of varying sizes (124M-7B) to both\nunderstand and mitigate the errors. Our study reveals that LMs rely on a number\nof components (attention heads and FF neurons) that independently make their\nown predictions. While some components reliably promote correct answers across\na generalized range of inputs (i.e., implementing \"sound mechanisms''), others\nare less reliable and introduce noise by promoting incorrect tokens (i.e.,\nimplementing \"faulty mechanisms''). Errors occur when the faulty mechanisms\novershadow the sound ones and dominantly affect the predictions. Motivated by\nthis insight, we introduce RASteer, a steering method to systematically\nidentify and increase the contribution of reliable components for improving\nmodel performance. RASteer substantially improves performance on balanced\nparentheses tasks, boosting accuracy of some models from $0$% to around $100$%\nwithout impairing the models' general coding ability. We further demonstrate\nits broader applicability in arithmetic reasoning tasks, achieving performance\ngains of up to around $20$%.", "AI": {"tldr": "语言模型在生成平衡括号等简单句法任务上表现不佳，研究发现错误源于部分不可靠组件。提出的RASteer方法通过增强可靠组件的作用显著提升了性能。", "motivation": "理解语言模型在简单句法任务上持续出错的原因，并提出解决方案。", "method": "分析模型组件（注意力头和FF神经元）的作用，提出RASteer方法以增强可靠组件的影响。", "result": "RASteer将平衡括号任务的准确率从0%提升至约100%，并在算术推理任务中取得约20%的性能提升。", "conclusion": "RASteer通过优化模型组件显著提升了语言模型在特定任务上的表现，且不影响其通用能力。"}}
{"id": "2507.00153", "pdf": "https://arxiv.org/pdf/2507.00153", "abs": "https://arxiv.org/abs/2507.00153", "authors": ["Peter Mortimer", "Mirko Maehlisch"], "title": "Diffusion-Based Image Augmentation for Semantic Segmentation in Outdoor Robotics", "categories": ["cs.CV"], "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics", "summary": "The performance of leaning-based perception algorithms suffer when deployed\nin out-of-distribution and underrepresented environments. Outdoor robots are\nparticularly susceptible to rapid changes in visual scene appearance due to\ndynamic lighting, seasonality and weather effects that lead to scenes\nunderrepresented in the training data of the learning-based perception system.\nIn this conceptual paper, we focus on preparing our autonomous vehicle for\ndeployment in snow-filled environments. We propose a novel method for\ndiffusion-based image augmentation to more closely represent the deployment\nenvironment in our training data. Diffusion-based image augmentations rely on\nthe public availability of vision foundation models learned on internet-scale\ndatasets. The diffusion-based image augmentations allow us to take control over\nthe semantic distribution of the ground surfaces in the training data and to\nfine-tune our model for its deployment environment. We employ open vocabulary\nsemantic segmentation models to filter out augmentation candidates that contain\nhallucinations. We believe that diffusion-based image augmentations can be\nextended to many other environments apart from snow surfaces, like sandy\nenvironments and volcanic terrains.", "AI": {"tldr": "提出了一种基于扩散的图像增强方法，以解决学习型感知算法在分布外和代表性不足环境中的性能问题。", "motivation": "户外机器人因动态光照、季节性和天气变化导致视觉场景外观快速变化，学习型感知系统在训练数据中缺乏这些场景的代表性。", "method": "利用基于扩散的图像增强技术，通过公共可用的视觉基础模型生成更接近部署环境的训练数据，并使用开放词汇语义分割模型过滤幻觉增强候选。", "result": "该方法能够更精确地控制训练数据中地面表面的语义分布，并针对部署环境微调模型。", "conclusion": "扩散基图像增强方法可扩展至雪地以外的多种环境，如沙地和火山地形。"}}
{"id": "2507.00726", "pdf": "https://arxiv.org/pdf/2507.00726", "abs": "https://arxiv.org/abs/2507.00726", "authors": ["Dongyoon Hwang", "Hojoon Lee", "Jaegul Choo", "Dongmin Park", "Jongho Park"], "title": "Can Large Language Models Develop Strategic Reasoning? Post-training Insights from Learning Chess", "categories": ["cs.AI", "cs.LG"], "comment": "27 pages", "summary": "While reinforcement learning (RL) for large language models (LLMs) has shown\npromise in mathematical reasoning, strategic reasoning for LLMs using RL\nremains largely unexplored. We investigate whether LLMs can develop strategic\nreasoning capabilities through RL in chess. To this end, we leverage a\nchess-pretrained action-value network to provide dense reward on the LLM's\noutput move quality, which can be seen as a form of knowledge distillation. Our\nexperiments show that our distillation-based dense rewards often outperform\nsparse binary rewards. However, surprisingly, all models plateau far below\nexpert levels. We provide SFT and RL ablations on chess reasoning training and\nfind evidence that this limitation stems from a deficit in the pretrained\nmodels' internal understanding of chess--a deficit which RL alone may not be\nable to fully overcome.", "AI": {"tldr": "研究了通过强化学习（RL）提升大语言模型（LLM）在象棋中的战略推理能力，发现基于知识蒸馏的密集奖励优于稀疏奖励，但模型表现仍远低于专家水平。", "motivation": "探索LLM是否可以通过RL在象棋中发展战略推理能力，填补该领域的研究空白。", "method": "利用预训练的象棋动作价值网络为LLM的输出动作质量提供密集奖励（知识蒸馏），并与稀疏奖励进行对比。", "result": "密集奖励通常优于稀疏奖励，但所有模型表现均远低于专家水平。", "conclusion": "预训练模型对象棋的内部理解不足可能是性能瓶颈，仅靠RL难以完全克服。"}}
{"id": "2507.00330", "pdf": "https://arxiv.org/pdf/2507.00330", "abs": "https://arxiv.org/abs/2507.00330", "authors": ["Mohna Chakraborty", "Adithya Kulkarni", "Qi Li"], "title": "Modeling Data Diversity for Joint Instance and Verbalizer Selection in Cold-Start Scenarios", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "Prompt-based methods leverage the knowledge of pre-trained language models\n(PLMs) trained with a masked language modeling (MLM) objective; however, these\nmethods are sensitive to template, verbalizer, and few-shot instance selection,\nparticularly in cold-start settings with no labeled data. Existing studies\noverlook the dependency between instances and verbalizers, where instance-label\nprobabilities depend on verbalizer token proximity in the embedding space. To\naddress this, we propose COLDSELECT, a joint verbalizer and instance selection\napproach that models data diversity. COLDSELECT maps PLM vocabulary and\n$h_{[MASK]}$ embeddings into a shared space, applying dimensionality reduction\nand clustering to ensure efficient and diverse selection. By optimizing for\nminimal uncertainty and maximal diversity, COLDSELECT captures data\nrelationships effectively. Experiments on eight benchmarks demonstrate\nCOLDSELECT's superiority in reducing uncertainty and enhancing generalization,\noutperforming baselines in verbalizer and few-shot instance selection for\ncold-start scenarios.", "AI": {"tldr": "COLDSELECT是一种联合选择verbalizer和实例的方法，通过建模数据多样性，优化不确定性和多样性，提升冷启动场景下的性能。", "motivation": "现有方法对模板、verbalizer和少样本实例选择敏感，且忽视了实例与verbalizer之间的依赖关系。", "method": "将PLM词汇和$h_{[MASK]}$嵌入映射到共享空间，通过降维和聚类实现高效多样的选择。", "result": "在八个基准测试中，COLDSELECT在减少不确定性和增强泛化能力方面优于基线方法。", "conclusion": "COLDSELECT在冷启动场景中显著提升了verbalizer和少样本实例选择的效果。"}}
{"id": "2507.00162", "pdf": "https://arxiv.org/pdf/2507.00162", "abs": "https://arxiv.org/abs/2507.00162", "authors": ["Yu Lu", "Yi Yang"], "title": "FreeLong++: Training-Free Long Video Generation via Multi-band SpectralFusion", "categories": ["cs.CV"], "comment": "under review", "summary": "Recent advances in video generation models have enabled high-quality short\nvideo generation from text prompts. However, extending these models to longer\nvideos remains a significant challenge, primarily due to degraded temporal\nconsistency and visual fidelity. Our preliminary observations show that naively\napplying short-video generation models to longer sequences leads to noticeable\nquality degradation. Further analysis identifies a systematic trend where\nhigh-frequency components become increasingly distorted as video length grows,\nan issue we term high-frequency distortion. To address this, we propose\nFreeLong, a training-free framework designed to balance the frequency\ndistribution of long video features during the denoising process. FreeLong\nachieves this by blending global low-frequency features, which capture holistic\nsemantics across the full video, with local high-frequency features extracted\nfrom short temporal windows to preserve fine details. Building on this,\nFreeLong++ extends FreeLong dual-branch design into a multi-branch architecture\nwith multiple attention branches, each operating at a distinct temporal scale.\nBy arranging multiple window sizes from global to local, FreeLong++ enables\nmulti-band frequency fusion from low to high frequencies, ensuring both\nsemantic continuity and fine-grained motion dynamics across longer video\nsequences. Without any additional training, FreeLong++ can be plugged into\nexisting video generation models (e.g. Wan2.1 and LTX-Video) to produce longer\nvideos with substantially improved temporal consistency and visual fidelity. We\ndemonstrate that our approach outperforms previous methods on longer video\ngeneration tasks (e.g. 4x and 8x of native length). It also supports coherent\nmulti-prompt video generation with smooth scene transitions and enables\ncontrollable video generation using long depth or pose sequences.", "AI": {"tldr": "FreeLong和FreeLong++是无需训练的框架，通过平衡长视频特征的频率分布，显著提升长视频生成的时间一致性和视觉保真度。", "motivation": "现有视频生成模型在生成长视频时存在时间一致性和视觉保真度下降的问题，尤其是高频失真现象。", "method": "FreeLong通过融合全局低频特征和局部高频特征；FreeLong++扩展为多分支架构，实现多频段融合。", "result": "在无需额外训练的情况下，显著提升长视频生成质量，支持多提示生成和可控视频生成。", "conclusion": "FreeLong++为长视频生成提供了一种高效且灵活的解决方案。"}}
{"id": "2507.00810", "pdf": "https://arxiv.org/pdf/2507.00810", "abs": "https://arxiv.org/abs/2507.00810", "authors": ["Qing Xu", "Xiaohua Xuan"], "title": "A Robust Algorithm for Non-IID Machine Learning Problems with Convergence Analysis", "categories": ["cs.AI", "math.OC"], "comment": null, "summary": "In this paper, we propose an improved numerical algorithm for solving minimax\nproblems based on nonsmooth optimization, quadratic programming and iterative\nprocess. We also provide a rigorous proof of convergence for our algorithm\nunder some mild assumptions, such as gradient continuity and boundedness. Such\nan algorithm can be widely applied in various fields such as robust\noptimization, imbalanced learning, etc.", "AI": {"tldr": "提出了一种基于非光滑优化、二次规划和迭代过程的改进数值算法，用于解决极小极大问题，并提供了收敛性证明。", "motivation": "极小极大问题在鲁棒优化和不平衡学习等领域有广泛应用，但现有算法可能效率不足或缺乏理论保证。", "method": "结合非光滑优化、二次规划和迭代过程，设计了一种改进的数值算法。", "result": "在梯度连续性和有界性等温和假设下，算法具有收敛性。", "conclusion": "该算法在多个领域具有潜在应用价值，且理论上有保障。"}}
{"id": "2507.00355", "pdf": "https://arxiv.org/pdf/2507.00355", "abs": "https://arxiv.org/abs/2507.00355", "authors": ["Paul J. L. Ammann", "Jonas Golde", "Alan Akbik"], "title": "Question Decomposition for Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": "Accepted to ACL SRW 2025. 9 Pages, 2 Figures, 4 Tables", "summary": "Grounding large language models (LLMs) in verifiable external sources is a\nwell-established strategy for generating reliable answers. Retrieval-augmented\ngeneration (RAG) is one such approach, particularly effective for tasks like\nquestion answering: it retrieves passages that are semantically related to the\nquestion and then conditions the model on this evidence. However, multi-hop\nquestions, such as \"Which company among NVIDIA, Apple, and Google made the\nbiggest profit in 2023?,\" challenge RAG because relevant facts are often\ndistributed across multiple documents rather than co-occurring in one source,\nmaking it difficult for standard RAG to retrieve sufficient information. To\naddress this, we propose a RAG pipeline that incorporates question\ndecomposition: (i) an LLM decomposes the original query into sub-questions,\n(ii) passages are retrieved for each sub-question, and (iii) the merged\ncandidate pool is reranked to improve the coverage and precision of the\nretrieved evidence. We show that question decomposition effectively assembles\ncomplementary documents, while reranking reduces noise and promotes the most\nrelevant passages before answer generation. Although reranking itself is\nstandard, we show that pairing an off-the-shelf cross-encoder reranker with\nLLM-driven question decomposition bridges the retrieval gap on multi-hop\nquestions and provides a practical, drop-in enhancement, without any extra\ntraining or specialized indexing. We evaluate our approach on the MultiHop-RAG\nand HotpotQA, showing gains in retrieval (MRR@10: +36.7%) and answer accuracy\n(F1: +11.6%) over standard RAG baselines.", "AI": {"tldr": "论文提出了一种结合问题分解和重排的RAG方法，以解决多跳问题中的检索不足问题，显著提升了检索和回答的准确性。", "motivation": "多跳问题中，相关信息分散在多个文档中，标准RAG方法难以检索足够证据。", "method": "采用问题分解（LLM分解原问题为子问题）、检索子问题相关段落，并通过重排合并候选池以提高证据覆盖率和精度。", "result": "在MultiHop-RAG和HotpotQA上，检索（MRR@10提升36.7%）和答案准确性（F1提升11.6%）显著优于标准RAG。", "conclusion": "问题分解与重排结合是提升多跳问题RAG性能的有效方法，无需额外训练或索引。"}}
{"id": "2507.00170", "pdf": "https://arxiv.org/pdf/2507.00170", "abs": "https://arxiv.org/abs/2507.00170", "authors": ["Hugo Baudchon", "Arthur Ouaknine", "Martin Weiss", "Mélisande Teng", "Thomas R. Walla", "Antoine Caron-Guay", "Christopher Pal", "Etienne Laliberté"], "title": "SelvaBox: A high-resolution dataset for tropical tree crown detection", "categories": ["cs.CV", "I.2.10; I.4.8; I.5.4"], "comment": null, "summary": "Detecting individual tree crowns in tropical forests is essential to study\nthese complex and crucial ecosystems impacted by human interventions and\nclimate change. However, tropical crowns vary widely in size, structure, and\npattern and are largely overlapping and intertwined, requiring advanced remote\nsensing methods applied to high-resolution imagery. Despite growing interest in\ntropical tree crown detection, annotated datasets remain scarce, hindering\nrobust model development. We introduce SelvaBox, the largest open-access\ndataset for tropical tree crown detection in high-resolution drone imagery. It\nspans three countries and contains more than 83,000 manually labeled crowns -\nan order of magnitude larger than all previous tropical forest datasets\ncombined. Extensive benchmarks on SelvaBox reveal two key findings: (1)\nhigher-resolution inputs consistently boost detection accuracy; and (2) models\ntrained exclusively on SelvaBox achieve competitive zero-shot detection\nperformance on unseen tropical tree crown datasets, matching or exceeding\ncompeting methods. Furthermore, jointly training on SelvaBox and three other\ndatasets at resolutions from 3 to 10 cm per pixel within a unified\nmulti-resolution pipeline yields a detector ranking first or second across all\nevaluated datasets. Our dataset, code, and pre-trained weights are made public.", "AI": {"tldr": "SelvaBox是一个用于热带树冠检测的最大公开数据集，包含83,000多个标注树冠，显著提升了检测模型的性能。", "motivation": "热带森林树冠检测对研究生态系统至关重要，但现有数据集稀缺，限制了模型开发。", "method": "引入SelvaBox数据集，并通过多分辨率管道联合训练模型。", "result": "高分辨率输入提升检测精度，SelvaBox训练的模型在未见数据集上表现优异。", "conclusion": "SelvaBox为热带树冠检测提供了重要资源，联合训练方法显著提升了模型性能。"}}
{"id": "2507.00841", "pdf": "https://arxiv.org/pdf/2507.00841", "abs": "https://arxiv.org/abs/2507.00841", "authors": ["Siyuan Liang", "Tianmeng Fang", "Zhe Liu", "Aishan Liu", "Yan Xiao", "Jinyuan He", "Ee-Chien Chang", "Xiaochun Cao"], "title": "SafeMobile: Chain-level Jailbreak Detection and Automated Evaluation for Multimodal Mobile Agents", "categories": ["cs.AI", "cs.CR"], "comment": "12 pages", "summary": "With the wide application of multimodal foundation models in intelligent\nagent systems, scenarios such as mobile device control, intelligent assistant\ninteraction, and multimodal task execution are gradually relying on such large\nmodel-driven agents. However, the related systems are also increasingly exposed\nto potential jailbreak risks. Attackers may induce the agents to bypass the\noriginal behavioral constraints through specific inputs, and then trigger\ncertain risky and sensitive operations, such as modifying settings, executing\nunauthorized commands, or impersonating user identities, which brings new\nchallenges to system security. Existing security measures for intelligent\nagents still have limitations when facing complex interactions, especially in\ndetecting potentially risky behaviors across multiple rounds of conversations\nor sequences of tasks. In addition, an efficient and consistent automated\nmethodology to assist in assessing and determining the impact of such risks is\ncurrently lacking. This work explores the security issues surrounding mobile\nmultimodal agents, attempts to construct a risk discrimination mechanism by\nincorporating behavioral sequence information, and designs an automated\nassisted assessment scheme based on a large language model. Through preliminary\nvalidation in several representative high-risk tasks, the results show that the\nmethod can improve the recognition of risky behaviors to some extent and assist\nin reducing the probability of agents being jailbroken. We hope that this study\ncan provide some valuable references for the security risk modeling and\nprotection of multimodal intelligent agent systems.", "AI": {"tldr": "该研究探讨了多模态智能代理的安全问题，提出了一种结合行为序列信息的风险判别机制，并基于大语言模型设计了自动化辅助评估方案。初步验证表明，该方法能提升风险行为识别能力，降低代理被越狱的概率。", "motivation": "随着多模态基础模型在智能代理系统中的广泛应用，相关系统面临潜在的越狱风险，现有安全措施在复杂交互中仍有局限，缺乏高效一致的风险评估方法。", "method": "研究构建了结合行为序列信息的风险判别机制，并设计基于大语言模型的自动化辅助评估方案。", "result": "初步验证显示，该方法能提升风险行为识别能力，降低代理被越狱的概率。", "conclusion": "该研究为多模态智能代理系统的安全风险建模与防护提供了有价值的参考。"}}
{"id": "2507.00380", "pdf": "https://arxiv.org/pdf/2507.00380", "abs": "https://arxiv.org/abs/2507.00380", "authors": ["Vojtěch Lanz", "Jan Hajič jr"], "title": "Gregorian melody, modality, and memory: Segmenting chant with Bayesian nonparametrics", "categories": ["cs.CL"], "comment": null, "summary": "The idea that Gregorian melodies are constructed from some vocabulary of\nsegments has long been a part of chant scholarship. This so-called\n\"centonisation\" theory has received much musicological criticism, but frequent\nre-use of certain melodic segments has been observed in chant melodies, and the\nintractable number of possible segmentations allowed the option that some\nundiscovered segmentation exists that will yet prove the value of\ncentonisation, and recent empirical results have shown that segmentations can\noutperform music-theoretical features in mode classification. Inspired by the\nfact that Gregorian chant was memorised, we search for an optimal unsupervised\nsegmentation of chant melody using nested hierarchical Pitman-Yor language\nmodels. The segmentation we find achieves state-of-the-art performance in mode\nclassification. Modeling a monk memorising the melodies from one liturgical\nmanuscript, we then find empirical evidence for the link between mode\nclassification and memory efficiency, and observe more formulaic areas at the\nbeginnings and ends of melodies corresponding to the practical role of modality\nin performance. However, the resulting segmentations themselves indicate that\neven such a memory-optimal segmentation is not what is understood as\ncentonisation.", "AI": {"tldr": "论文探讨了格里高利圣歌的旋律分段理论（centonisation），通过无监督分层Pitman-Yor语言模型寻找最优分段，发现其在调式分类中表现优异，但结果不支持传统centonisation理论。", "motivation": "研究格里高利圣歌旋律的分段理论（centonisation），探索其与记忆效率和调式分类的关系。", "method": "使用无监督分层Pitman-Yor语言模型对圣歌旋律进行最优分段，并分析其与调式分类和记忆效率的关联。", "result": "最优分段在调式分类中表现优异，但分段结果不支持传统centonisation理论；旋律的开头和结尾更公式化。", "conclusion": "尽管分段在调式分类中有效，但记忆最优的分段与传统centonisation理论不符，表明后者可能不成立。"}}
{"id": "2507.00182", "pdf": "https://arxiv.org/pdf/2507.00182", "abs": "https://arxiv.org/abs/2507.00182", "authors": ["J. I. Ruíz", "A. Méndez", "E. Rodríguez"], "title": "Graph-Based Deep Learning for Component Segmentation of Maize Plants", "categories": ["cs.CV"], "comment": null, "summary": "In precision agriculture, one of the most important tasks when exploring crop\nproduction is identifying individual plant components. There are several\nattempts to accomplish this task by the use of traditional 2D imaging, 3D\nreconstructions, and Convolutional Neural Networks (CNN). However, they have\nseveral drawbacks when processing 3D data and identifying individual plant\ncomponents. Therefore, in this work, we propose a novel Deep Learning\narchitecture to detect components of individual plants on Light Detection and\nRanging (LiDAR) 3D Point Cloud (PC) data sets. This architecture is based on\nthe concept of Graph Neural Networks (GNN), and feature enhancing with\nPrincipal Component Analysis (PCA). For this, each point is taken as a vertex\nand by the use of a K-Nearest Neighbors (KNN) layer, the edges are established,\nthus representing the 3D PC data set. Subsequently, Edge-Conv layers are used\nto further increase the features of each point. Finally, Graph Attention\nNetworks (GAT) are applied to classify visible phenotypic components of the\nplant, such as the leaf, stem, and soil. This study demonstrates that our\ngraph-based deep learning approach enhances segmentation accuracy for\nidentifying individual plant components, achieving percentages above 80% in the\nIoU average, thus outperforming other existing models based on point clouds.", "AI": {"tldr": "提出了一种基于图神经网络（GNN）和主成分分析（PCA）的新方法，用于从LiDAR 3D点云数据中识别植物组件，显著提高了分割精度。", "motivation": "传统方法在处理3D数据和识别植物组件时存在局限性，需要更高效的解决方案。", "method": "结合GNN、PCA和KNN构建深度学习架构，利用Edge-Conv层增强特征，并通过GAT分类植物组件。", "result": "模型在IoU平均指标上超过80%，优于现有基于点云的模型。", "conclusion": "该方法为精准农业中的植物组件识别提供了高效且准确的解决方案。"}}
{"id": "2507.00951", "pdf": "https://arxiv.org/pdf/2507.00951", "abs": "https://arxiv.org/abs/2507.00951", "authors": ["Rizwan Qureshi", "Ranjan Sapkota", "Abbas Shah", "Amgad Muneer", "Anas Zafar", "Ashmal Vayani", "Maged Shoman", "Abdelrahman B. M. Eldaly", "Kai Zhang", "Ferhat Sadak", "Shaina Raza", "Xinqi Fan", "Ravid Shwartz-Ziv", "Hong Yan", "Vinjia Jain", "Aman Chadha", "Manoj Karkee", "Jia Wu", "Philip Torr", "Seyedali Mirjalili"], "title": "Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive Foundations for Artificial General Intelligence and its Societal Impact", "categories": ["cs.AI"], "comment": null, "summary": "Can machines truly think, reason and act in domains like humans? This\nenduring question continues to shape the pursuit of Artificial General\nIntelligence (AGI). Despite the growing capabilities of models such as GPT-4.5,\nDeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal\nfluency and partial reasoning, these systems remain fundamentally limited by\ntheir reliance on token-level prediction and lack of grounded agency. This\npaper offers a cross-disciplinary synthesis of AGI development, spanning\nartificial intelligence, cognitive neuroscience, psychology, generative models,\nand agent-based systems. We analyze the architectural and cognitive foundations\nof general intelligence, highlighting the role of modular reasoning, persistent\nmemory, and multi-agent coordination. In particular, we emphasize the rise of\nAgentic RAG frameworks that combine retrieval, planning, and dynamic tool use\nto enable more adaptive behavior. We discuss generalization strategies,\nincluding information compression, test-time adaptation, and training-free\nmethods, as critical pathways toward flexible, domain-agnostic intelligence.\nVision-Language Models (VLMs) are reexamined not just as perception modules but\nas evolving interfaces for embodied understanding and collaborative task\ncompletion. We also argue that true intelligence arises not from scale alone\nbut from the integration of memory and reasoning: an orchestration of modular,\ninteractive, and self-improving components where compression enables adaptive\nbehavior. Drawing on advances in neurosymbolic systems, reinforcement learning,\nand cognitive scaffolding, we explore how recent architectures begin to bridge\nthe gap between statistical learning and goal-directed cognition. Finally, we\nidentify key scientific, technical, and ethical challenges on the path to AGI.", "AI": {"tldr": "论文探讨了机器是否能像人类一样思考、推理和行动，分析了当前AI模型的局限性，并提出了跨学科的AGI发展路径，强调模块化推理、记忆和多智能体协调的重要性。", "motivation": "研究动机在于探索如何克服当前AI模型（如GPT-4.5、DeepSeek等）的局限性，实现更接近人类水平的通用人工智能（AGI）。", "method": "通过跨学科综合，结合人工智能、认知神经科学、心理学等领域，分析通用智能的架构和认知基础，提出Agentic RAG框架、信息压缩等方法。", "result": "论文指出，真正的智能不仅依赖于规模，还需要记忆与推理的整合，以及模块化、交互性和自我改进能力的结合。", "conclusion": "结论强调了AGI发展的关键挑战，包括科学、技术和伦理问题，并展望了未来研究方向。"}}
{"id": "2507.00389", "pdf": "https://arxiv.org/pdf/2507.00389", "abs": "https://arxiv.org/abs/2507.00389", "authors": ["Jing Ren", "Wenhao Zhou", "Bowen Li", "Mujie Liu", "Nguyen Linh Dan Le", "Jiade Cen", "Liping Chen", "Ziqi Xu", "Xiwei Xu", "Xiaodong Li"], "title": "Causal Prompting for Implicit Sentiment Analysis with Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Implicit Sentiment Analysis (ISA) aims to infer sentiment that is implied\nrather than explicitly stated, requiring models to perform deeper reasoning\nover subtle contextual cues. While recent prompting-based methods using Large\nLanguage Models (LLMs) have shown promise in ISA, they often rely on majority\nvoting over chain-of-thought (CoT) reasoning paths without evaluating their\ncausal validity, making them susceptible to internal biases and spurious\ncorrelations. To address this challenge, we propose CAPITAL, a causal prompting\nframework that incorporates front-door adjustment into CoT reasoning. CAPITAL\ndecomposes the overall causal effect into two components: the influence of the\ninput prompt on the reasoning chains, and the impact of those chains on the\nfinal output. These components are estimated using encoder-based clustering and\nthe NWGM approximation, with a contrastive learning objective used to better\nalign the encoder's representation with the LLM's reasoning space. Experiments\non benchmark ISA datasets with three LLMs demonstrate that CAPITAL consistently\noutperforms strong prompting baselines in both accuracy and robustness,\nparticularly under adversarial conditions. This work offers a principled\napproach to integrating causal inference into LLM prompting and highlights its\nbenefits for bias-aware sentiment reasoning. The source code and case study are\navailable at: https://github.com/whZ62/CAPITAL.", "AI": {"tldr": "CAPITAL是一个因果提示框架，通过将前门调整融入思维链推理，解决了隐式情感分析中因果有效性问题，提升了准确性和鲁棒性。", "motivation": "现有基于提示的大语言模型方法在隐式情感分析中依赖多数投票，未评估因果有效性，易受内部偏见和虚假相关影响。", "method": "CAPITAL将总体因果效应分解为输入提示对推理链的影响和推理链对最终输出的影响，采用编码器聚类和NWGM近似估计，结合对比学习目标。", "result": "在三个大语言模型上的实验表明，CAPITAL在准确性和鲁棒性上均优于基线方法，尤其在对抗条件下表现突出。", "conclusion": "CAPITAL为将因果推理融入大语言模型提示提供了原则性方法，强调了其在偏见感知情感推理中的优势。"}}
{"id": "2507.00224", "pdf": "https://arxiv.org/pdf/2507.00224", "abs": "https://arxiv.org/abs/2507.00224", "authors": ["Changsoo Jung", "Sheikh Mannan", "Jack Fitzgerald", "Nathaniel Blanchard"], "title": "Computer Vision for Objects used in Group Work: Challenges and Opportunities", "categories": ["cs.CV", "cs.HC"], "comment": "Accepted to AIED 2025 Late Breaking Results Track", "summary": "Interactive and spatially aware technologies are transforming educational\nframeworks, particularly in K-12 settings where hands-on exploration fosters\ndeeper conceptual understanding. However, during collaborative tasks, existing\nsystems often lack the ability to accurately capture real-world interactions\nbetween students and physical objects. This issue could be addressed with\nautomatic 6D pose estimation, i.e., estimation of an object's position and\norientation in 3D space from RGB images or videos. For collaborative groups\nthat interact with physical objects, 6D pose estimates allow AI systems to\nrelate objects and entities. As part of this work, we introduce FiboSB, a novel\nand challenging 6D pose video dataset featuring groups of three participants\nsolving an interactive task featuring small hand-held cubes and a weight scale.\nThis setup poses unique challenges for 6D pose because groups are holistically\nrecorded from a distance in order to capture all participants -- this, coupled\nwith the small size of the cubes, makes 6D pose estimation inherently\nnon-trivial. We evaluated four state-of-the-art 6D pose estimation methods on\nFiboSB, exposing the limitations of current algorithms on collaborative group\nwork. An error analysis of these methods reveals that the 6D pose methods'\nobject detection modules fail. We address this by fine-tuning YOLO11-x for\nFiboSB, achieving an overall mAP_50 of 0.898. The dataset, benchmark results,\nand analysis of YOLO11-x errors presented here lay the groundwork for\nleveraging the estimation of 6D poses in difficult collaborative contexts.", "AI": {"tldr": "论文介绍了FiboSB数据集，用于解决协作任务中6D姿态估计的挑战，并通过改进YOLO11-x提升了性能。", "motivation": "现有系统在协作任务中难以准确捕捉学生与物理对象的交互，6D姿态估计可解决这一问题。", "method": "提出FiboSB数据集，评估四种6D姿态估计方法，并改进YOLO11-x。", "result": "改进后的YOLO11-x在FiboSB上达到mAP_50为0.898。", "conclusion": "FiboSB数据集和YOLO11-x的改进为协作场景中的6D姿态估计奠定了基础。"}}
{"id": "2507.00979", "pdf": "https://arxiv.org/pdf/2507.00979", "abs": "https://arxiv.org/abs/2507.00979", "authors": ["Dongyoon Hahm", "Woogyeol Jin", "June Suk Choi", "Sungsoo Ahn", "Kimin Lee"], "title": "Enhancing LLM Agent Safety via Causal Influence Prompting", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": "Accepted at ACL 2025 Findings, Source code:\n  https://github.com/HahmDY/causal_influence_prompting.git", "summary": "As autonomous agents powered by large language models (LLMs) continue to\ndemonstrate potential across various assistive tasks, ensuring their safe and\nreliable behavior is crucial for preventing unintended consequences. In this\nwork, we introduce CIP, a novel technique that leverages causal influence\ndiagrams (CIDs) to identify and mitigate risks arising from agent\ndecision-making. CIDs provide a structured representation of cause-and-effect\nrelationships, enabling agents to anticipate harmful outcomes and make safer\ndecisions. Our approach consists of three key steps: (1) initializing a CID\nbased on task specifications to outline the decision-making process, (2)\nguiding agent interactions with the environment using the CID, and (3)\niteratively refining the CID based on observed behaviors and outcomes.\nExperimental results demonstrate that our method effectively enhances safety in\nboth code execution and mobile device control tasks.", "AI": {"tldr": "论文提出了一种名为CIP的新技术，利用因果影响图（CIDs）来识别和减轻自主代理决策中的风险，从而提升安全性。", "motivation": "随着基于大语言模型的自主代理在辅助任务中展现出潜力，确保其行为安全可靠以防止意外后果变得至关重要。", "method": "方法包括三个步骤：1）基于任务规范初始化CID以描述决策过程；2）使用CID指导代理与环境交互；3）根据观察到的行为和结果迭代优化CID。", "result": "实验结果表明，该方法在代码执行和移动设备控制任务中显著提升了安全性。", "conclusion": "CIP通过因果影响图有效增强了自主代理的安全性，为未来研究提供了可靠框架。"}}
{"id": "2507.00439", "pdf": "https://arxiv.org/pdf/2507.00439", "abs": "https://arxiv.org/abs/2507.00439", "authors": ["Gauri Kambhatla", "Sanjana Gautam", "Angela Zhang", "Alex Liu", "Ravi Srinivasan", "Junyi Jessy Li", "Matthew Lease"], "title": "Beyond Sociodemographic Prompting: Using Supervision to Align LLMs with Human Response Distributions", "categories": ["cs.CL"], "comment": null, "summary": "The ability to accurately predict how different population groups would\nanswer subjective questions would have great value. In this work, we show that\nuse of relatively simple supervision can greatly improve language model\nalignment with diverse population groups, as measured over three datasets\nspanning various topics. Beyond evaluating average performance, we also report\nhow alignment varies across specific groups. The simplicity and generality of\nour approach promotes easy adoption, while our broad findings provide useful\nguidance for when to use or not use our approach in practice. By conducting\nevaluation over many LLMs and prompting strategies, along with open-sourcing\nour work, we provide a useful benchmark to stimulate future research.", "AI": {"tldr": "通过简单监督提升语言模型与多样化人群的对齐能力，提供实用指南和开源基准。", "motivation": "准确预测不同人群对主观问题的回答具有重要价值。", "method": "使用相对简单的监督方法，评估多种语言模型和提示策略。", "result": "显著提高了语言模型与多样化人群的对齐能力，并提供了具体群体间的差异分析。", "conclusion": "方法的简单性和通用性易于采用，为实践中的使用提供了指导，并开源以促进未来研究。"}}
{"id": "2507.00243", "pdf": "https://arxiv.org/pdf/2507.00243", "abs": "https://arxiv.org/abs/2507.00243", "authors": ["Chi-Yao Huang", "Zeel Bhatt", "Yezhou Yang"], "title": "VOCAL: Visual Odometry via ContrAstive Learning", "categories": ["cs.CV"], "comment": null, "summary": "Breakthroughs in visual odometry (VO) have fundamentally reshaped the\nlandscape of robotics, enabling ultra-precise camera state estimation that is\ncrucial for modern autonomous systems. Despite these advances, many\nlearning-based VO techniques rely on rigid geometric assumptions, which often\nfall short in interpretability and lack a solid theoretical basis within fully\ndata-driven frameworks. To overcome these limitations, we introduce VOCAL\n(Visual Odometry via ContrAstive Learning), a novel framework that reimagines\nVO as a label ranking challenge. By integrating Bayesian inference with a\nrepresentation learning framework, VOCAL organizes visual features to mirror\ncamera states. The ranking mechanism compels similar camera states to converge\ninto consistent and spatially coherent representations within the latent space.\nThis strategic alignment not only bolsters the interpretability of the learned\nfeatures but also ensures compatibility with multimodal data sources. Extensive\nevaluations on the KITTI dataset highlight VOCAL's enhanced interpretability\nand flexibility, pushing VO toward more general and explainable spatial\nintelligence.", "AI": {"tldr": "VOCAL是一种基于对比学习的视觉里程计框架，通过贝叶斯推理和表示学习提升特征可解释性和多模态兼容性。", "motivation": "现有学习型视觉里程计方法依赖刚性几何假设，缺乏可解释性和理论支持，VOCAL旨在解决这些问题。", "method": "将视觉里程计重新定义为标签排序问题，结合贝叶斯推理和表示学习，使视觉特征与相机状态对齐。", "result": "在KITTI数据集上验证，VOCAL表现出更高的可解释性和灵活性。", "conclusion": "VOCAL推动了视觉里程计向更通用、可解释的空间智能发展。"}}
{"id": "2507.00002", "pdf": "https://arxiv.org/pdf/2507.00002", "abs": "https://arxiv.org/abs/2507.00002", "authors": ["Christopher James Augeri"], "title": "Hypertokens: Holographic Associative Memory in Tokenized LLMs", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "preprint as accepted to https://qnlp.ai/ - Quantum AI and NLP\n  Conference 2025", "summary": "Large Language Models (LLMs) exhibit remarkable capabilities but suffer from\napparent precision loss, reframed here as information spreading. This reframing\nshifts the problem from computational precision to an information-theoretic\ncommunication issue. We address the K:V and V:K memory problem in LLMs by\nintroducing HDRAM (Holographically Defined Random Access Memory), a symbolic\nmemory framework treating transformer latent space as a spread-spectrum\nchannel. Built upon hypertokens, structured symbolic codes integrating\nclassical error-correcting codes (ECC), holographic computing, and\nquantum-inspired search, HDRAM recovers distributed information through\nprincipled despreading. These phase-coherent memory addresses enable efficient\nkey-value operations and Grover-style search in latent space. By combining ECC\ngrammar with compressed sensing and Krylov subspace alignment, HDRAM\nsignificantly improves associative retrieval without architectural changes,\ndemonstrating how Classical-Holographic-Quantum-inspired (CHQ) principles can\nfortify transformer architectures.", "AI": {"tldr": "论文提出HDRAM框架，通过信息论方法解决LLMs中的精度损失问题，结合经典纠错码、全息计算和量子启发搜索，显著提升关联检索能力。", "motivation": "大型语言模型（LLMs）存在精度损失问题，作者将其重新定义为信息扩散问题，试图从信息论角度解决。", "method": "引入HDRAM框架，利用超令牌（hypertokens）和相位相干内存地址，结合经典纠错码、全息计算和量子启发搜索，实现高效键值操作和Grover式搜索。", "result": "HDRAM显著提升了关联检索能力，且无需改变模型架构。", "conclusion": "通过经典-全息-量子启发（CHQ）原则，HDRAM为Transformer架构提供了新的加固方法。"}}
{"id": "2507.00460", "pdf": "https://arxiv.org/pdf/2507.00460", "abs": "https://arxiv.org/abs/2507.00460", "authors": ["Md. Najib Hasan", "Mohammad Fakhruddin Babar", "Souvika Sarkar", "Monowar Hasan", "Santu Karmaker"], "title": "Pitfalls of Evaluating Language Models with Open Benchmarks", "categories": ["cs.CL"], "comment": null, "summary": "Open Large Language Model (LLM) benchmarks, such as HELM and BIG-bench, offer\nstandardized, transparent protocols that facilitate the fair comparison,\nreproducibility, and iterative advancement of Language Models (LMs). However,\ntheir openness also introduces critical and underexplored pitfalls. This study\nexposes these weaknesses by systematically constructing ``cheating'' models --\nsmaller variants of BART, T5, and GPT-2 fine-tuned directly on public test sets\n-- which achieve top rankings on a prominent open, holistic benchmark (HELM)\ndespite poor generalization and limited practical utility. Our findings\nunderscore three key insights: \\ca high leaderboard performance on open\nbenchmarks may not always reflect real-world effectiveness; \\cb private or\ndynamic benchmarks must complement open evaluations to safeguard integrity; and\n\\cc a fundamental reevaluation of current benchmarking practices is essential\nto ensure robust and trustworthy LM assessments.", "AI": {"tldr": "研究发现开放大语言模型基准测试存在漏洞，通过构造作弊模型在HELM基准上取得高分，但实际泛化能力差，需改进评估方法。", "motivation": "揭示开放基准测试（如HELM和BIG-bench）的潜在问题，证明其可能被滥用，导致评估结果失真。", "method": "构造小型作弊模型（基于BART、T5和GPT-2），直接在公开测试集上微调，并在HELM基准上测试。", "result": "作弊模型在HELM基准上表现优异，但实际泛化能力差，实用性有限。", "conclusion": "需结合私有或动态基准测试，重新评估当前基准实践，以确保语言模型评估的可靠性和真实性。"}}
{"id": "2507.00248", "pdf": "https://arxiv.org/pdf/2507.00248", "abs": "https://arxiv.org/abs/2507.00248", "authors": ["Nikita Nikitin", "Eugene Fomin"], "title": "Developing Lightweight DNN Models With Limited Data For Real-Time Sign Language Recognition", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "comment": "7 pages, 2 figures, 2 tables, for associated mpeg file, see\n  https://slait.app/static/Screen_Recording.mp4", "summary": "We present a novel framework for real-time sign language recognition using\nlightweight DNNs trained on limited data. Our system addresses key challenges\nin sign language recognition, including data scarcity, high computational\ncosts, and discrepancies in frame rates between training and inference\nenvironments. By encoding sign language specific parameters, such as handshape,\npalm orientation, movement, and location into vectorized inputs, and leveraging\nMediaPipe for landmark extraction, we achieve highly separable input data\nrepresentations. Our DNN architecture, optimized for sub 10MB deployment,\nenables accurate classification of 343 signs with less than 10ms latency on\nedge devices. The data annotation platform 'slait data' facilitates structured\nlabeling and vector extraction. Our model achieved 92% accuracy in isolated\nsign recognition and has been integrated into the 'slait ai' web application,\nwhere it demonstrates stable inference.", "AI": {"tldr": "提出了一种基于轻量级DNN的实时手语识别框架，解决了数据稀缺、高计算成本和帧率差异等问题，模型在边缘设备上实现低延迟高精度分类。", "motivation": "解决手语识别中的数据稀缺、高计算成本和训练与推理环境帧率差异等关键挑战。", "method": "通过将手语特定参数（如手形、手掌方向、动作和位置）编码为向量输入，并利用MediaPipe提取关键点，构建轻量级DNN架构。", "result": "模型在343个手语分类中达到92%的准确率，延迟低于10ms，并已集成到'slait ai'应用中。", "conclusion": "该框架在有限数据下实现了高效、低延迟的手语识别，展示了实际应用的潜力。"}}
{"id": "2507.00003", "pdf": "https://arxiv.org/pdf/2507.00003", "abs": "https://arxiv.org/abs/2507.00003", "authors": ["Eyhab Al-Masri"], "title": "Deciding When Not to Decide: Indeterminacy-Aware Intrusion Detection with NeutroSENSE", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.NI"], "comment": null, "summary": "This paper presents NeutroSENSE, a neutrosophic-enhanced ensemble framework\nfor interpretable intrusion detection in IoT environments. By integrating\nRandom Forest, XGBoost, and Logistic Regression with neutrosophic logic, the\nsystem decomposes prediction confidence into truth (T), falsity (F), and\nindeterminacy (I) components, enabling uncertainty quantification and\nabstention. Predictions with high indeterminacy are flagged for review using\nboth global and adaptive, class-specific thresholds. Evaluated on the IoT-CAD\ndataset, NeutroSENSE achieved 97% accuracy, while demonstrating that\nmisclassified samples exhibit significantly higher indeterminacy (I = 0.62)\nthan correct ones (I = 0.24). The use of indeterminacy as a proxy for\nuncertainty enables informed abstention and targeted review-particularly\nvaluable in edge deployments. Figures and tables validate the correlation\nbetween I-scores and error likelihood, supporting more trustworthy,\nhuman-in-the-loop AI decisions. This work shows that neutrosophic logic\nenhances both accuracy and explainability, providing a practical foundation for\ntrust-aware AI in edge and fog-based IoT security systems.", "AI": {"tldr": "NeutroSENSE是一种基于中性逻辑的集成框架，用于物联网环境中的可解释入侵检测，通过分解预测置信度为真、假和不确定成分，实现不确定性量化和主动回避。", "motivation": "解决物联网环境中入侵检测的不确定性问题，提升检测的准确性和可解释性。", "method": "集成随机森林、XGBoost和逻辑回归，结合中性逻辑分解预测置信度，并设置全局和类别特定阈值进行不确定预测的标记。", "result": "在IoT-CAD数据集上达到97%的准确率，不确定成分（I）与错误率显著相关。", "conclusion": "中性逻辑提升了检测的准确性和可解释性，为边缘和雾计算物联网安全系统提供了信任感知的AI基础。"}}
{"id": "2507.00509", "pdf": "https://arxiv.org/pdf/2507.00509", "abs": "https://arxiv.org/abs/2507.00509", "authors": ["To Eun Kim", "João Coelho", "Gbemileke Onilude", "Jai Singh"], "title": "TeamCMU at Touché: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "As conversational search engines increasingly adopt generation-based\nparadigms powered by Large Language Models (LLMs) and Retrieval-Augmented\nGeneration (RAG), the integration of advertisements into generated responses\npresents both commercial opportunities and challenges for user experience.\nUnlike traditional search, where advertisements are clearly delineated,\ngenerative systems blur the boundary between informational content and\npromotional material, raising concerns around transparency and trust. In this\nwork, we propose a modular pipeline for advertisement management in RAG-based\nconversational systems, consisting of an ad-rewriter for seamless ad\nintegration and a robust ad-classifier for detection. We leverage synthetic\ndata to train high-performing classifiers, which are then used to guide two\ncomplementary ad-integration strategies: supervised fine-tuning of the\nad-rewriter and a best-of-N sampling approach that selects the least detectable\nad-integrated response among multiple candidates. Our evaluation focuses on two\ncore questions: the effectiveness of ad classifiers in detecting diverse ad\nintegration strategies, and the training methods that best support coherent,\nminimally intrusive ad insertion. Experimental results show that our\nad-classifier, trained on synthetic advertisement data inspired by marketing\nstrategies and enhanced through curriculum learning, achieves robust detection\nperformance. Additionally, we demonstrate that classifier-guided optimization,\nthrough both fine-tuning and best-of-N sampling, significantly improves ad\nstealth, enabling more seamless integration. These findings contribute an\nadversarial co-evolution framework for developing more sophisticated ad-aware\ngenerative search systems and robust ad classifiers.", "AI": {"tldr": "论文提出了一种模块化广告管理流程，用于基于RAG的对话系统，包括广告重写器和广告分类器，以提高广告的隐蔽性和用户体验。", "motivation": "随着生成式搜索系统的发展，广告与信息内容的界限模糊，影响了透明度和用户信任，因此需要一种有效的广告管理方法。", "method": "采用合成数据训练广告分类器，并基于分类器指导广告重写器的微调和最佳候选选择策略（best-of-N sampling）。", "result": "实验表明，分类器在检测广告方面表现优异，且分类器指导的优化显著提升了广告的隐蔽性。", "conclusion": "研究为广告感知生成系统和鲁棒广告分类器的开发提供了对抗性共进化框架。"}}
{"id": "2507.00253", "pdf": "https://arxiv.org/pdf/2507.00253", "abs": "https://arxiv.org/abs/2507.00253", "authors": ["Zhuangzhuang Dai", "Vincent Gbouna Zakka", "Luis J. Manso", "Chen Li"], "title": "GazeTarget360: Towards Gaze Target Estimation in 360-Degree for Robot Perception", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Enabling robots to understand human gaze target is a crucial step to allow\ncapabilities in downstream tasks, for example, attention estimation and\nmovement anticipation in real-world human-robot interactions. Prior works have\naddressed the in-frame target localization problem with data-driven approaches\nby carefully removing out-of-frame samples. Vision-based gaze estimation\nmethods, such as OpenFace, do not effectively absorb background information in\nimages and cannot predict gaze target in situations where subjects look away\nfrom the camera. In this work, we propose a system to address the problem of\n360-degree gaze target estimation from an image in generalized visual scenes.\nThe system, named GazeTarget360, integrates conditional inference engines of an\neye-contact detector, a pre-trained vision encoder, and a multi-scale-fusion\ndecoder. Cross validation results show that GazeTarget360 can produce accurate\nand reliable gaze target predictions in unseen scenarios. This makes a\nfirst-of-its-kind system to predict gaze targets from realistic camera footage\nwhich is highly efficient and deployable. Our source code is made publicly\navailable at: https://github.com/zdai257/DisengageNet.", "AI": {"tldr": "提出GazeTarget360系统，用于从图像中估计360度视线目标，适用于真实场景。", "motivation": "机器人理解人类视线目标是实现注意力估计和动作预测等任务的关键，现有方法无法有效处理视线离开相机的情况。", "method": "结合条件推理引擎（包括眼接触检测器、预训练视觉编码器和多尺度融合解码器）构建GazeTarget360系统。", "result": "交叉验证显示，系统在未见场景中能准确预测视线目标，高效且可部署。", "conclusion": "GazeTarget360是首个能从真实相机画面预测视线目标的系统，代码已开源。"}}
{"id": "2507.00004", "pdf": "https://arxiv.org/pdf/2507.00004", "abs": "https://arxiv.org/abs/2507.00004", "authors": ["Austin R. Ellis-Mohr", "Anuj K. Nayak", "Lav R. Varshney"], "title": "A Theory of Inference Compute Scaling: Reasoning through Directed Stochastic Skill Search", "categories": ["cs.LG", "cs.AI", "cs.CY", "cs.PF"], "comment": null, "summary": "Large language models (LLMs) demand considerable computational, energy, and\nfinancial resources during both training and deployment. While scaling laws for\ntraining have guided much of the field's recent progress, inference costs now\nrepresent a significant and growing component of the overall resource burden,\nparticularly for reasoning-focused models. Existing characterizations of\ncompute-optimality that consider model size, dataset size, and inference tokens\nin isolation or in fixed combinations risk overlooking more efficient operating\npoints. We introduce directed stochastic skill search (DS3), a general\nframework that represents inference as stochastic traversal over a learned\nskill graph. From a simplified yet expressive instantiation, we derive\nclosed-form expressions for task success and compute cost across a wide range\nof inference strategies -- including chain-of-thought (CoT) and tree-of-thought\n(ToT) -- enabling comparative analysis as a function of task difficulty and\nmodel capability. To that end, we extend a prior first-principles tripartite\ngraph framework of LLM training to incorporate inference, and separately bridge\nDS3 with empirical methods that characterize LLM scaling behavior. We\ntheoretically recover empirically observed patterns, including: linear accuracy\nscaling with logarithmic compute; variation in preferred inference strategies\nas a function of task difficulty and model capability; emergent behavior\nelicited by reasoning even when performance plateaus under parameter scaling;\nand both best-of-N (BoN) and majority voting behavior captured within a unified\nanalytical framework. By explicitly characterizing training-inference\ninterdependencies, our framework deepens theoretical understanding and supports\nprincipled algorithmic design and resource allocation.", "AI": {"tldr": "论文提出了一种名为DS3的框架，用于优化大型语言模型（LLM）的推理成本和效率，通过技能图随机遍历实现任务成功与计算成本的闭式表达。", "motivation": "由于LLM在训练和推理阶段的资源消耗巨大，现有方法未能综合考虑模型规模、数据集大小和推理令牌的优化点，因此需要一种更高效的框架。", "method": "提出DS3框架，将推理表示为技能图的随机遍历，并通过闭式表达式分析不同推理策略（如CoT和ToT）的任务成功率和计算成本。", "result": "理论推导揭示了线性精度与对数计算的关系，以及任务难度和模型能力对推理策略的影响，同时统一了BoN和多数投票行为的分析框架。", "conclusion": "DS3框架通过明确训练与推理的相互依赖关系，为理论理解和资源分配提供了支持。"}}
{"id": "2507.00534", "pdf": "https://arxiv.org/pdf/2507.00534", "abs": "https://arxiv.org/abs/2507.00534", "authors": ["Tahir Javed", "Kaushal Bhogale", "Mitesh M. Khapra"], "title": "NIRANTAR: Continual Learning with New Languages and Domains on Real-world Speech Data", "categories": ["cs.CL"], "comment": "Accepted in Interspecch 2025", "summary": "We introduce Nirantar, a comprehensive framework for evaluating continual\nlearning (CL) in multilingual and multi-domain ASR. Designed to reflect\nreal-world CL challenges, Nirantar leverages data collected incrementally\nacross 22 languages and 208 districts in India through natural episodes. This\nenables evaluation across Language-Incremental (LIL), Domain-Incremental (DIL),\nand the novel Language-Incremental Domain-Incremental Learning (LIDIL)\nscenarios. Unlike prior work that relies on simulated episodes, Nirantar\npresents dynamic, non-uniform language and domain shifts, making it an ideal\ntestbed for CL research. With 3250 hours of human-transcribed speech, including\n1720 hours newly introduced in this work, our framework enables systematic\nbenchmarking of CL methods. We evaluate existing approaches and demonstrate\nthat no single method performs consistently well, underscoring the need for\nmore robust CL strategies.", "AI": {"tldr": "Nirantar是一个用于评估多语言和多领域ASR中持续学习的框架，基于真实世界数据，涵盖22种语言和208个印度地区，支持多种增量学习场景。", "motivation": "现有持续学习研究多依赖模拟数据，缺乏真实世界的动态性和非均匀性，Nirantar旨在填补这一空白。", "method": "利用自然增量收集的3250小时语音数据（含1720小时新数据），评估语言增量、领域增量及混合增量学习场景。", "result": "现有方法在动态场景中表现不一致，表明需要更鲁棒的持续学习策略。", "conclusion": "Nirantar为持续学习研究提供了真实且动态的测试平台，凸显了现有方法的不足。"}}
{"id": "2507.00261", "pdf": "https://arxiv.org/pdf/2507.00261", "abs": "https://arxiv.org/abs/2507.00261", "authors": ["Zhiyin Lin", "Purvi Goel", "Joy Yun", "C. Karen Liu", "Joao Pedro Araujo"], "title": "VirtualFencer: Generating Fencing Bouts based on Strategies Extracted from In-the-Wild Videos", "categories": ["cs.CV", "cs.GR"], "comment": null, "summary": "Fencing is a sport where athletes engage in diverse yet strategically logical\nmotions. While most motions fall into a few high-level actions (e.g. step,\nlunge, parry), the execution can vary widely-fast vs. slow, large vs. small,\noffensive vs. defensive. Moreover, a fencer's actions are informed by a\nstrategy that often comes in response to the opponent's behavior. This\ncombination of motion diversity with underlying two-player strategy motivates\nthe application of data-driven modeling to fencing. We present VirtualFencer, a\nsystem capable of extracting 3D fencing motion and strategy from in-the-wild\nvideo without supervision, and then using that extracted knowledge to generate\nrealistic fencing behavior. We demonstrate the versatile capabilities of our\nsystem by having it (i) fence against itself (self-play), (ii) fence against a\nreal fencer's motion from online video, and (iii) fence interactively against a\nprofessional fencer.", "AI": {"tldr": "VirtualFencer是一个无监督地从视频中提取3D击剑动作和策略的系统，并能生成逼真的击剑行为。", "motivation": "击剑动作多样且受策略驱动，适合数据驱动建模。", "method": "从视频中无监督提取3D动作和策略，生成击剑行为。", "result": "系统能自我对战、与真实击剑手动作对战，并与专业击剑手互动。", "conclusion": "VirtualFencer展示了数据驱动建模在击剑中的潜力。"}}
{"id": "2507.00007", "pdf": "https://arxiv.org/pdf/2507.00007", "abs": "https://arxiv.org/abs/2507.00007", "authors": ["Vasiliy Znamenskiy", "Rafael Niyazov", "Joel Hernandez"], "title": "Integrating Universal Generative AI Platforms in Educational Labs to Foster Critical Thinking and Digital Literacy", "categories": ["cs.CY", "cs.AI", "cs.LG", "68T50, 68U20, 97U50, 97D40", "I.2.7; K.3.1; K.3.2; H.5.3"], "comment": "http://doi.org/10.5121/ijci.2025.140302", "summary": "This paper presents a new educational framework for integrating generative\nartificial intelligence (GenAI) platforms such as ChatGPT, Claude, and Gemini\ninto laboratory activities aimed at developing critical thinking and digital\nliteracy among undergraduate students. Recognizing the limitations and risks of\nuncritical reliance on large language models (LLMs), the proposed pedagogical\nmodel reframes GenAI as a research subject and cognitive tool. Students\nformulate discipline-specific prompts and evaluate GenAI-generated responses in\ntext, image, and video modalities. A pilot implementation in a general\nastronomy course for non-science majors demonstrated high levels of engagement\nand critical reflection, with many students continuing the activity after class\nand presenting results at a research symposium. The results highlight the\nimportance of structured AI interactions in education and suggest that GenAI\ncan improve learning outcomes when combined with reflective assessment methods.\nThe study proposes a replicable model for interdisciplinary AI-integrated lab\nwork, adaptable to scientific disciplines. See the guide to learning activities\nbased on Generative-Ai platforms: https://doi.org/10.5281/zenodo.15555802", "AI": {"tldr": "提出了一种将生成式人工智能（GenAI）平台融入实验室活动的新教育框架，旨在培养本科生的批判性思维和数字素养。", "motivation": "认识到对大型语言模型（LLM）不加批判依赖的局限性和风险，重新将GenAI定义为研究主题和认知工具。", "method": "学生制定学科特定的提示，并评估GenAI生成的文本、图像和视频响应。在非理科专业的普通天文学课程中进行了试点。", "result": "试点显示学生参与度高且能进行批判性反思，许多学生在课后继续活动并在研究研讨会上展示成果。", "conclusion": "结构化AI互动在教育中很重要，结合反思性评估方法可提升学习效果，提出了可复制的跨学科AI整合实验室模型。"}}
{"id": "2507.00540", "pdf": "https://arxiv.org/pdf/2507.00540", "abs": "https://arxiv.org/abs/2507.00540", "authors": ["Shixiao Wang", "Yifan Zhuang", "Runsheng Zhang", "Zhijun Song"], "title": "Capsule Network-Based Semantic Intent Modeling for Human-Computer Interaction", "categories": ["cs.CL"], "comment": null, "summary": "This paper proposes a user semantic intent modeling algorithm based on\nCapsule Networks to address the problem of insufficient accuracy in intent\nrecognition for human-computer interaction. The method represents semantic\nfeatures in input text through a vectorized capsule structure. It uses a\ndynamic routing mechanism to transfer information across multiple capsule\nlayers. This helps capture hierarchical relationships and part-whole structures\nbetween semantic entities more effectively. The model uses a convolutional\nfeature extraction module as the low-level encoder. After generating initial\nsemantic capsules, it forms high-level abstract intent representations through\nan iterative routing process. To further enhance performance, a margin-based\nmechanism is introduced into the loss function. This improves the model's\nability to distinguish between intent classes. Experiments are conducted using\na public natural language understanding dataset. Multiple mainstream models are\nused for comparison. Results show that the proposed model outperforms\ntraditional methods and other deep learning structures in terms of accuracy,\nF1-score, and intent detection rate. The study also analyzes the effect of the\nnumber of dynamic routing iterations on model performance. A convergence curve\nof the loss function during training is provided. These results verify the\nstability and effectiveness of the proposed method in semantic modeling.\nOverall, this study presents a new structured modeling approach to improve\nintent recognition under complex semantic conditions.", "AI": {"tldr": "提出了一种基于胶囊网络的用户语义意图建模算法，用于提高人机交互中意图识别的准确性。", "motivation": "解决人机交互中意图识别准确性不足的问题。", "method": "使用向量化胶囊结构表示语义特征，通过动态路由机制在多个胶囊层间传递信息，并结合卷积特征提取模块和基于边界的损失函数。", "result": "在公开数据集上实验表明，该模型在准确性、F1分数和意图检测率上优于传统方法和深度学习结构。", "conclusion": "该方法为复杂语义条件下的意图识别提供了一种新的结构化建模方法，验证了其稳定性和有效性。"}}
{"id": "2507.00263", "pdf": "https://arxiv.org/pdf/2507.00263", "abs": "https://arxiv.org/abs/2507.00263", "authors": ["Vignesh Ram Nithin Kappagantula", "Shayan Hassantabar"], "title": "Room Scene Discovery and Grouping in Unstructured Vacation Rental Image Collections", "categories": ["cs.CV", "cs.LG", "cs.NE"], "comment": null, "summary": "The rapid growth of vacation rental (VR) platforms has led to an increasing\nvolume of property images, often uploaded without structured categorization.\nThis lack of organization poses significant challenges for travelers attempting\nto understand the spatial layout of a property, particularly when multiple\nrooms of the same type are present. To address this issue, we introduce an\neffective approach for solving the room scene discovery and grouping problem,\nas well as identifying bed types within each bedroom group. This grouping is\nvaluable for travelers to comprehend the spatial organization, layout, and the\nsleeping configuration of the property. We propose a computationally efficient\nmachine learning pipeline characterized by low latency and the ability to\nperform effectively with sample-efficient learning, making it well-suited for\nreal-time and data-scarce environments. The pipeline integrates a supervised\nroom-type detection model, a supervised overlap detection model to identify the\noverlap similarity between two images, and a clustering algorithm to group the\nimages of the same space together using the similarity scores. Additionally,\nthe pipeline maps each bedroom group to the corresponding bed types specified\nin the property's metadata, based on the visual content present in the group's\nimages using a Multi-modal Large Language Model (MLLM) model. We evaluate the\naforementioned models individually and also assess the pipeline in its\nentirety, observing strong performance that significantly outperforms\nestablished approaches such as contrastive learning and clustering with\npretrained embeddings.", "AI": {"tldr": "提出了一种高效的机器学习流程，用于解决度假租赁平台中房间场景发现、分组及床型识别问题，显著优于现有方法。", "motivation": "度假租赁平台上的房产图片缺乏结构化分类，给旅行者理解空间布局带来挑战。", "method": "结合监督学习模型（房间类型检测和重叠检测）与聚类算法，并利用多模态大语言模型映射床型。", "result": "整体流程表现优异，显著优于对比学习和预训练嵌入聚类等方法。", "conclusion": "该方法在实时和数据稀缺环境下高效，有助于旅行者理解房产布局和睡眠配置。"}}
{"id": "2507.00011", "pdf": "https://arxiv.org/pdf/2507.00011", "abs": "https://arxiv.org/abs/2507.00011", "authors": ["Nathan Vaartjes", "Vincent Francois-Lavet"], "title": "Novel RL approach for efficient Elevator Group Control Systems", "categories": ["cs.LG", "cs.AI"], "comment": "15 pages, 12 figures", "summary": "Efficient elevator traffic management in large buildings is critical for\nminimizing passenger travel times and energy consumption. Because heuristic- or\npattern-detection-based controllers struggle with the stochastic and\ncombinatorial nature of dispatching, we model the six-elevator, fifteen-floor\nsystem at Vrije Universiteit Amsterdam as a Markov Decision Process and train\nan end-to-end Reinforcement Learning (RL) Elevator Group Control System (EGCS).\nKey innovations include a novel action space encoding to handle the\ncombinatorial complexity of elevator dispatching, the introduction of\ninfra-steps to model continuous passenger arrivals, and a tailored reward\nsignal to improve learning efficiency. In addition, we explore various ways to\nadapt the discounting factor to the infra-step formulation. We investigate RL\narchitectures based on Dueling Double Deep Q-learning, showing that the\nproposed RL-based EGCS adapts to fluctuating traffic patterns, learns from a\nhighly stochastic environment, and thereby outperforms a traditional rule-based\nalgorithm.", "AI": {"tldr": "论文提出了一种基于强化学习的电梯群控系统（EGCS），通过马尔可夫决策过程建模，解决了传统启发式或模式检测控制器在电梯调度中的随机性和组合性问题。", "motivation": "大型建筑中电梯交通管理的高效性对减少乘客等待时间和能耗至关重要，传统方法难以应对随机性和组合性问题。", "method": "采用马尔可夫决策过程建模，引入创新的动作空间编码、基础设施步骤和定制奖励信号，并探索基于Dueling Double Deep Q-learning的强化学习架构。", "result": "提出的RL-based EGCS能够适应波动的交通模式，在高度随机环境中学习，性能优于传统规则算法。", "conclusion": "强化学习方法在电梯群控系统中具有显著优势，能够有效提升调度效率和适应性。"}}
{"id": "2507.00547", "pdf": "https://arxiv.org/pdf/2507.00547", "abs": "https://arxiv.org/abs/2507.00547", "authors": ["Malmi Amadoru"], "title": "Methodological Rigour in Algorithm Application: An Illustration of Topic Modelling Algorithm", "categories": ["cs.CL"], "comment": null, "summary": "The rise of advanced computational algorithms has opened new avenues for\ncomputationally intensive research approaches to theory development. However,\nthe opacity of these algorithms and lack of transparency and rigour in their\napplication pose methodological challenges, potentially undermining trust in\nresearch. The discourse on methodological rigour in this new genre of research\nis still emerging. Against this backdrop, I attempt to offer guidance on\nmethodological rigour, particularly in the context of topic modelling\nalgorithms. By illustrating the application of the structural topic modelling\nalgorithm and presenting a set of guidelines, I discuss how to ensure rigour in\ntopic modelling studies. Although the guidelines are for the application of\ntopic modelling algorithms, they can be applied to other algorithms with\ncontext-specific adjustments. The guidelines are helpful, especially for novice\nresearchers applying topic modelling, and editors and reviewers handling topic\nmodelling manuscripts. I contribute to the literature on topic modelling and\njoin the emerging dialogue on methodological rigour in computationally\nintensive theory construction research.", "AI": {"tldr": "论文探讨了计算密集型研究中算法不透明性带来的方法学挑战，并提出了确保主题建模研究严谨性的指南。", "motivation": "随着高级计算算法的兴起，其不透明性和应用缺乏透明度可能削弱研究可信度，因此需要探讨方法学严谨性。", "method": "通过结构主题建模算法的应用示例和一套指南，讨论如何确保主题建模研究的严谨性。", "result": "提出的指南不仅适用于主题建模算法，还可通过调整应用于其他算法，对新手研究者、编辑和审稿人有帮助。", "conclusion": "论文为计算密集型理论构建研究中的方法学严谨性提供了指导，并推动了相关讨论。"}}
{"id": "2507.00287", "pdf": "https://arxiv.org/pdf/2507.00287", "abs": "https://arxiv.org/abs/2507.00287", "authors": ["Mohamad Dabboussi", "Malo Huard", "Yann Gousseau", "Pietro Gori"], "title": "Self-Supervised Multiview Xray Matching", "categories": ["cs.CV", "cs.AI"], "comment": "MICCAI 2025", "summary": "Accurate interpretation of multi-view radiographs is crucial for diagnosing\nfractures, muscular injuries, and other anomalies. While significant advances\nhave been made in AI-based analysis of single images, current methods often\nstruggle to establish robust correspondences between different X-ray views, an\nessential capability for precise clinical evaluations. In this work, we present\na novel self-supervised pipeline that eliminates the need for manual annotation\nby automatically generating a many-to-many correspondence matrix between\nsynthetic X-ray views. This is achieved using digitally reconstructed\nradiographs (DRR), which are automatically derived from unannotated CT volumes.\nOur approach incorporates a transformer-based training phase to accurately\npredict correspondences across two or more X-ray views. Furthermore, we\ndemonstrate that learning correspondences among synthetic X-ray views can be\nleveraged as a pretraining strategy to enhance automatic multi-view fracture\ndetection on real data. Extensive evaluations on both synthetic and real X-ray\ndatasets show that incorporating correspondences improves performance in\nmulti-view fracture classification.", "AI": {"tldr": "提出了一种自监督方法，通过合成X射线视图自动生成多对多对应矩阵，无需人工标注，并利用Transformer提升多视图骨折检测性能。", "motivation": "当前AI方法在多视图X射线分析中难以建立稳健的对应关系，而这对临床诊断至关重要。", "method": "使用数字重建X射线（DRR）从无标注CT数据生成合成视图，通过Transformer训练预测多视图对应关系。", "result": "在合成和真实X射线数据上验证，多视图对应关系显著提升了骨折分类性能。", "conclusion": "自监督学习多视图对应关系可作为预训练策略，有效提升真实数据上的骨折检测效果。"}}
{"id": "2507.00012", "pdf": "https://arxiv.org/pdf/2507.00012", "abs": "https://arxiv.org/abs/2507.00012", "authors": ["Linfeng Ye", "Shayan Mohajer Hamidi", "En-hui Yang"], "title": "Towards Undistillable Models by Minimizing Conditional Mutual Information", "categories": ["cs.LG", "cs.AI", "E.4"], "comment": "27 pages, 6 figures, Transactions on Machine Learning Research", "summary": "A deep neural network (DNN) is said to be undistillable if, when used as a\nblack-box input-output teacher, it cannot be distilled through knowledge\ndistillation (KD). In this case, the distilled student (referred to as the\nknockoff student) does not outperform a student trained independently with\nlabel smoothing (LS student) in terms of prediction accuracy. To protect\nintellectual property of DNNs, it is desirable to build undistillable DNNs. To\nthis end, it is first observed that an undistillable DNN may have the trait\nthat each cluster of its output probability distributions in response to all\nsample instances with the same label should be highly concentrated to the\nextent that each cluster corresponding to each label should ideally collapse\ninto one probability distribution. Based on this observation and by measuring\nthe concentration of each cluster in terms of conditional mutual information\n(CMI), a new training method called CMI minimized (CMIM) method is proposed,\nwhich trains a DNN by jointly minimizing the conventional cross entropy (CE)\nloss and the CMI values of all temperature scaled clusters across the entire\ntemperature spectrum. The resulting CMIM model is shown, by extensive\nexperiments, to be undistillable by all tested KD methods existing in the\nliterature. That is, the knockoff students distilled by these KD methods from\nthe CMIM model underperform the respective LS students. In addition, the CMIM\nmodel is also shown to performs better than the model trained with the CE loss\nalone in terms of their own prediction accuracy.", "AI": {"tldr": "该论文提出了一种称为CMIM的训练方法，通过最小化交叉熵损失和条件互信息（CMI）值，构建不可蒸馏的深度神经网络（DNN），以保护知识产权。", "motivation": "为了保护DNN的知识产权，需要构建不可蒸馏的DNN，即通过知识蒸馏（KD）无法有效复制的模型。", "method": "提出CMIM方法，通过联合最小化交叉熵损失和温度缩放簇的条件互信息（CMI）值，训练DNN。", "result": "实验表明，CMIM模型对所有测试的KD方法均不可蒸馏，且其预测精度优于仅使用交叉熵损失训练的模型。", "conclusion": "CMIM方法成功构建了不可蒸馏的DNN，同时提高了模型自身的预测性能。"}}
{"id": "2507.00579", "pdf": "https://arxiv.org/pdf/2507.00579", "abs": "https://arxiv.org/abs/2507.00579", "authors": ["Miriam Anschütz", "Ekaterina Gikalo", "Niklas Herbster", "Georg Groh"], "title": "TUM-MiKaNi at SemEval-2025 Task 3: Towards Multilingual and Knowledge-Aware Non-factual Hallucination Identification", "categories": ["cs.CL", "cs.AI"], "comment": "6 pages, 3 figures, SemEval-2025 Task 3, ACL", "summary": "Hallucinations are one of the major problems of LLMs, hindering their\ntrustworthiness and deployment to wider use cases. However, most of the\nresearch on hallucinations focuses on English data, neglecting the multilingual\nnature of LLMs. This paper describes our submission to the SemEval-2025 Task-3\n- Mu-SHROOM, the Multilingual Shared-task on Hallucinations and Related\nObservable Overgeneration Mistakes. We propose a two-part pipeline that\ncombines retrieval-based fact verification against Wikipedia with a BERT-based\nsystem fine-tuned to identify common hallucination patterns. Our system\nachieves competitive results across all languages, reaching top-10 results in\neight languages, including English. Moreover, it supports multiple languages\nbeyond the fourteen covered by the shared task. This multilingual hallucination\nidentifier can help to improve LLM outputs and their usefulness in the future.", "AI": {"tldr": "论文提出了一种多语言幻觉检测系统，结合检索式事实验证和BERT模型，在SemEval-2025任务中表现优异。", "motivation": "解决LLM在多语言环境中的幻觉问题，提升其可信度和应用范围。", "method": "采用两阶段流程：基于Wikipedia的检索式事实验证和针对幻觉模式的BERT微调系统。", "result": "系统在八种语言中进入前十，支持超过任务涵盖的十四种语言。", "conclusion": "多语言幻觉检测器有助于改进LLM输出，提升其未来实用性。"}}
{"id": "2507.00292", "pdf": "https://arxiv.org/pdf/2507.00292", "abs": "https://arxiv.org/abs/2507.00292", "authors": ["Ali Mammadov", "Loïc Le Folgoc", "Guillaume Hocquet", "Pietro Gori"], "title": "Reducing Variability of Multiple Instance Learning Methods for Digital Pathology", "categories": ["cs.CV", "cs.AI"], "comment": "MICCAI 2025", "summary": "Digital pathology has revolutionized the field by enabling the digitization\nof tissue samples into whole slide images (WSIs). However, the high resolution\nand large size of WSIs present significant challenges when it comes to applying\nDeep Learning models. As a solution, WSIs are often divided into smaller\npatches with a global label (\\textit{i.e., diagnostic}) per slide, instead of a\n(too) costly pixel-wise annotation. By treating each slide as a bag of patches,\nMultiple Instance Learning (MIL) methods have emerged as a suitable solution\nfor WSI classification. A major drawback of MIL methods is their high\nvariability in performance across different runs, which can reach up to 10-15\nAUC points on the test set, making it difficult to compare different MIL\nmethods reliably. This variability mainly comes from three factors: i) weight\ninitialization, ii) batch (shuffling) ordering, iii) and learning rate. To\naddress that, we introduce a Multi-Fidelity, Model Fusion strategy for MIL\nmethods. We first train multiple models for a few epochs and average the most\nstable and promising ones based on validation scores. This approach can be\napplied to any existing MIL model to reduce performance variability. It also\nsimplifies hyperparameter tuning and improves reproducibility while maintaining\ncomputational efficiency. We extensively validate our approach on WSI\nclassification tasks using 2 different datasets, 3 initialization strategies\nand 5 MIL methods, for a total of more than 2000 experiments.", "AI": {"tldr": "提出了一种多保真度模型融合策略，以减少多实例学习（MIL）方法在数字病理学中的性能波动，并通过实验验证了其有效性。", "motivation": "数字病理学中的全切片图像（WSI）分类存在性能波动大的问题，主要源于权重初始化、批次排序和学习率等因素。", "method": "通过训练多个模型并基于验证分数平均最稳定和有前景的模型，以减少性能波动。", "result": "在两种数据集、三种初始化策略和五种MIL方法上进行了超过2000次实验，验证了该方法的有效性。", "conclusion": "该方法能够降低性能波动，简化超参数调优，提高可重复性，同时保持计算效率。"}}
{"id": "2507.00013", "pdf": "https://arxiv.org/pdf/2507.00013", "abs": "https://arxiv.org/abs/2507.00013", "authors": ["Hyunwoo Seo", "Chiehyeon Lim"], "title": "ST-MTM: Masked Time Series Modeling with Seasonal-Trend Decomposition for Time Series Forecasting", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Accepted by KDD 2025 research track", "summary": "Forecasting complex time series is an important yet challenging problem that\ninvolves various industrial applications. Recently, masked time-series modeling\nhas been proposed to effectively model temporal dependencies for forecasting by\nreconstructing masked segments from unmasked ones. However, since the semantic\ninformation in time series is involved in intricate temporal variations\ngenerated by multiple time series components, simply masking a raw time series\nignores the inherent semantic structure, which may cause MTM to learn spurious\ntemporal patterns present in the raw data. To capture distinct temporal\nsemantics, we show that masked modeling techniques should address entangled\npatterns through a decomposition approach. Specifically, we propose ST-MTM, a\nmasked time-series modeling framework with seasonal-trend decomposition, which\nincludes a novel masking method for the seasonal-trend components that\nincorporates different temporal variations from each component. ST-MTM uses a\nperiod masking strategy for seasonal components to produce multiple masked\nseasonal series based on inherent multi-periodicity and a sub-series masking\nstrategy for trend components to mask temporal regions that share similar\nvariations. The proposed masking method presents an effective pre-training task\nfor learning intricate temporal variations and dependencies. Additionally,\nST-MTM introduces a contrastive learning task to support masked modeling by\nenhancing contextual consistency among multiple masked seasonal\nrepresentations. Experimental results show that our proposed ST-MTM achieves\nconsistently superior forecasting performance compared to existing masked\nmodeling, contrastive learning, and supervised forecasting methods.", "AI": {"tldr": "ST-MTM是一种结合季节性-趋势分解的掩码时间序列建模框架，通过改进掩码方法捕捉时间序列中的复杂语义，提升预测性能。", "motivation": "传统掩码时间序列建模方法忽略了时间序列的固有语义结构，可能导致学习到虚假的时序模式。", "method": "提出ST-MTM框架，采用季节性-趋势分解，设计针对季节性和趋势组件的掩码策略，并结合对比学习任务增强上下文一致性。", "result": "实验表明，ST-MTM在预测性能上优于现有的掩码建模、对比学习和监督预测方法。", "conclusion": "ST-MTM通过分解和针对性掩码策略，有效捕捉了时间序列的复杂语义，显著提升了预测效果。"}}
{"id": "2507.00601", "pdf": "https://arxiv.org/pdf/2507.00601", "abs": "https://arxiv.org/abs/2507.00601", "authors": ["Shuangquan Lyu", "Yingnan Deng", "Guiran Liu", "Zhen Qi", "Ruotong Wang"], "title": "Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based", "categories": ["cs.CL"], "comment": null, "summary": "This paper addresses the limited transfer and adaptation capabilities of\nlarge language models in low-resource language scenarios. It proposes a unified\nframework that combines a knowledge transfer module with parameter-efficient\nfine-tuning strategies. The method introduces knowledge alignment loss and soft\nprompt tuning to guide the model in effectively absorbing the structural\nfeatures of target languages or tasks under minimal annotation. This enhances\nboth generalization performance and training stability. The framework includes\nlightweight adaptation modules to reduce computational costs. During training,\nit integrates freezing strategies and prompt injection to preserve the model's\noriginal knowledge while enabling quick adaptation to new tasks. The study also\nconducts stability analysis experiments and synthetic pseudo-data transfer\nexperiments to systematically evaluate the method's applicability and\nrobustness across different low-resource tasks. Experimental results show that\ncompared with existing multilingual pre-trained models and mainstream transfer\nmethods, the proposed approach achieves higher performance and stability on\ncross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates\nparticularly strong advantages under extremely data-scarce conditions. The\nproposed method offers strong generality and scalability. It enhances\ntask-specific adaptability while preserving the general capabilities of large\nlanguage models. This makes it well-suited for complex semantic modeling and\nmultilingual processing tasks.", "AI": {"tldr": "本文提出了一种结合知识转移模块和参数高效微调策略的统一框架，以提升大语言模型在低资源语言场景中的迁移和适应能力。", "motivation": "解决大语言模型在低资源语言场景中迁移和适应能力有限的问题。", "method": "引入知识对齐损失和软提示调优，结合轻量级适应模块、冻结策略和提示注入，以低成本实现高效迁移。", "result": "在跨语言任务（如MLQA、XQuAD和PAWS-X）中表现优于现有方法，尤其在数据稀缺条件下优势显著。", "conclusion": "该方法具有强通用性和可扩展性，既提升任务适应性，又保留大语言模型的通用能力，适用于复杂语义建模和多语言处理。"}}
{"id": "2507.00327", "pdf": "https://arxiv.org/pdf/2507.00327", "abs": "https://arxiv.org/abs/2507.00327", "authors": ["Chuyan Zhang", "Kefan Wang", "Yun Gu"], "title": "Beyond Low-Rank Tuning: Model Prior-Guided Rank Allocation for Effective Transfer in Low-Data and Large-Gap Regimes", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Low-Rank Adaptation (LoRA) has proven effective in reducing computational\ncosts while maintaining performance comparable to fully fine-tuned foundation\nmodels across various tasks. However, its fixed low-rank structure restricts\nits adaptability in scenarios with substantial domain gaps, where higher ranks\nare often required to capture domain-specific complexities. Current adaptive\nLoRA methods attempt to overcome this limitation by dynamically expanding or\nselectively allocating ranks, but these approaches frequently depend on\ncomputationally intensive techniques such as iterative pruning, rank searches,\nor additional regularization. To address these challenges, we introduce Stable\nRank-Guided Low-Rank Adaptation (SR-LoRA), a novel framework that utilizes the\nstable rank of pre-trained weight matrices as a natural prior for layer-wise\nrank allocation. By leveraging the stable rank, which reflects the intrinsic\ndimensionality of the weights, SR-LoRA enables a principled and efficient\nredistribution of ranks across layers, enhancing adaptability without incurring\nadditional search costs. Empirical evaluations on few-shot tasks with\nsignificant domain gaps show that SR-LoRA consistently outperforms recent\nadaptive LoRA variants, achieving a superior trade-off between performance and\nefficiency. Our code is available at\nhttps://github.com/EndoluminalSurgicalVision-IMR/SR-LoRA.", "AI": {"tldr": "SR-LoRA利用预训练权重矩阵的稳定秩作为层间秩分配的先验，提升低秩自适应（LoRA）在领域差距大的任务中的表现，同时避免额外计算开销。", "motivation": "解决固定低秩结构在领域差距大的场景中适应性不足的问题，同时避免现有自适应LoRA方法的高计算成本。", "method": "提出SR-LoRA框架，利用预训练权重矩阵的稳定秩指导层间秩分配，无需额外搜索或正则化。", "result": "在领域差距大的少样本任务中，SR-LoRA表现优于现有自适应LoRA方法，性能与效率更优。", "conclusion": "SR-LoRA通过稳定秩指导的秩分配，实现了高效且适应性强的低秩自适应。"}}
{"id": "2507.00014", "pdf": "https://arxiv.org/pdf/2507.00014", "abs": "https://arxiv.org/abs/2507.00014", "authors": ["Thomas Joshi", "Shayan Chowdhury", "Fatih Uysal"], "title": "SWE-Bench-CL: Continual Learning for Coding Agents", "categories": ["cs.LG", "cs.AI", "cs.SE"], "comment": null, "summary": "Large Language Models (LLMs) have achieved impressive results on static\ncode-generation benchmarks, but real-world software development unfolds as a\ncontinuous stream of evolving issues, fixes, and feature requests. We introduce\nSWE-Bench-CL, a novel continual learning benchmark built on the human-verified\nSWE-Bench Verified dataset introduced by OpenAI and Princeton-NLP in 2024. By\norganizing GitHub issues into chronologically ordered sequences that reflect\nnatural repository evolution, SWE-Bench-CL enables direct evaluation of an\nagent's ability to accumulate experience, transfer knowledge across tasks, and\nresist catastrophic forgetting. We complement the dataset with (i) a\npreliminary analysis of inter-task structural similarity and contextual\nsensitivity, (ii) an interactive LangGraph-based evaluation framework augmented\nwith a FAISS-backed semantic memory module, and (iii) a suite of specialized\ncontinual learning metrics -- including average accuracy, forgetting,\nforward/backward transfer, tool-use efficiency, and a generalized Composite\nContinual Learning Score and CL-F-beta score -- to capture the\nstability-plasticity trade-off. We outline a rigorous experimental protocol\ncomparing memory-enabled and memory-disabled agents across diverse Python\nrepositories. All code and data are publicly available at\nhttps://github.com/thomasjoshi/agents-never-forget, providing the community\nwith a reproducible platform for developing more adaptive and robust AI agents\nin software engineering.", "AI": {"tldr": "SWE-Bench-CL是一个基于GitHub问题的持续学习基准，旨在评估AI代理在软件工程中的适应性和鲁棒性。", "motivation": "现有LLMs在静态代码生成任务中表现优异，但缺乏对持续演化的软件开发环境的适应能力。", "method": "通过组织GitHub问题为时间序列，结合LangGraph框架和FAISS语义记忆模块，设计了一套持续学习指标和评估协议。", "result": "提供了可复现的平台和工具，用于开发更适应和鲁棒的AI代理。", "conclusion": "SWE-Bench-CL为社区提供了一个评估和改进AI代理在持续学习场景中表现的标准。"}}
{"id": "2507.00606", "pdf": "https://arxiv.org/pdf/2507.00606", "abs": "https://arxiv.org/abs/2507.00606", "authors": ["Tao Xiong", "Xavier Hu", "Wenyan Fan", "Shengyu Zhang"], "title": "Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Large language models (LLMs) excel in complex tasks through advanced\nprompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but\ntheir reliance on manually crafted, task-specific prompts limits adaptability\nand efficiency. We introduce Mixture of Reasoning (MoR), a training framework\nthat embeds diverse reasoning strategies into LLMs for autonomous,\ntask-adaptive reasoning without external prompt engineering. MoR has two\nphases: Thought Generation, creating reasoning chain templates with models like\nGPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets\nfor supervised fine-tuning.Our experiments show that MoR significantly enhances\nperformance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting\nand 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need\nfor task-specific prompts, offering a generalizable solution for robust\nreasoning across diverse tasks.", "AI": {"tldr": "MoR框架通过嵌入多样化推理策略，使LLM无需手动设计任务特定提示即可自主推理，显著提升性能。", "motivation": "现有LLM依赖手动设计的任务特定提示，限制了适应性和效率。", "method": "MoR分两阶段：1) 生成推理链模板；2) 构建监督微调数据集。", "result": "MoR150在CoT提示下性能提升2.2%，与基线相比提升13.5%。", "conclusion": "MoR提供了一种通用解决方案，无需任务特定提示即可实现多样化任务的鲁棒推理。"}}
{"id": "2507.00328", "pdf": "https://arxiv.org/pdf/2507.00328", "abs": "https://arxiv.org/abs/2507.00328", "authors": ["Xuan Liu", "Yinhao Ren", "Marc D. Ryser", "Lars J. Grimm", "Joseph Y. Lo"], "title": "MammoTracker: Mask-Guided Lesion Tracking in Temporal Mammograms", "categories": ["cs.CV"], "comment": null, "summary": "Accurate lesion tracking in temporal mammograms is essential for monitoring\nbreast cancer progression and facilitating early diagnosis. However, automated\nlesion correspondence across exams remains a challenges in computer-aided\ndiagnosis (CAD) systems, limiting their effectiveness. We propose MammoTracker,\na mask-guided lesion tracking framework that automates lesion localization\nacross consecutively exams. Our approach follows a coarse-to-fine strategy\nincorporating three key modules: global search, local search, and score\nrefinement. To support large-scale training and evaluation, we introduce a new\ndataset with curated prior-exam annotations for 730 mass and calcification\ncases from the public EMBED mammogram dataset, yielding over 20000 lesion\npairs, making it the largest known resource for temporal lesion tracking in\nmammograms. Experimental results demonstrate that MammoTracker achieves 0.455\naverage overlap and 0.509 accuracy, surpassing baseline models by 8%,\nhighlighting its potential to enhance CAD-based lesion progression analysis.\nOur dataset will be available at\nhttps://gitlab.oit.duke.edu/railabs/LoGroup/mammotracker.", "AI": {"tldr": "MammoTracker是一个基于掩码引导的乳腺病变跟踪框架，通过全局搜索、局部搜索和分数细化模块实现跨时间乳腺X光片的病变定位，性能优于基线模型8%。", "motivation": "乳腺病变的准确跟踪对乳腺癌进展监测和早期诊断至关重要，但现有计算机辅助诊断系统在跨时间病变对应方面存在挑战。", "method": "采用从粗到细的策略，结合全局搜索、局部搜索和分数细化三个模块，并引入一个包含730个病例的新数据集。", "result": "MammoTracker的平均重叠率为0.455，准确率为0.509，性能提升8%。", "conclusion": "MammoTracker在乳腺病变跟踪中表现出色，其数据集将公开以支持进一步研究。"}}
{"id": "2507.00015", "pdf": "https://arxiv.org/pdf/2507.00015", "abs": "https://arxiv.org/abs/2507.00015", "authors": ["Lu Zhang", "Sangarapillai Lambotharan", "Gan Zheng", "Guisheng Liao", "Xuekang Liu", "Fabio Roli", "Carsten Maple"], "title": "Vision Transformer with Adversarial Indicator Token against Adversarial Attacks in Radio Signal Classifications", "categories": ["cs.LG", "cs.AI", "cs.CR"], "comment": null, "summary": "The remarkable success of transformers across various fields such as natural\nlanguage processing and computer vision has paved the way for their\napplications in automatic modulation classification, a critical component in\nthe communication systems of Internet of Things (IoT) devices. However, it has\nbeen observed that transformer-based classification of radio signals is\nsusceptible to subtle yet sophisticated adversarial attacks. To address this\nissue, we have developed a defensive strategy for transformer-based modulation\nclassification systems to counter such adversarial attacks. In this paper, we\npropose a novel vision transformer (ViT) architecture by introducing a new\nconcept known as adversarial indicator (AdvI) token to detect adversarial\nattacks. To the best of our knowledge, this is the first work to propose an\nAdvI token in ViT to defend against adversarial attacks. Integrating an\nadversarial training method with a detection mechanism using AdvI token, we\ncombine a training time defense and running time defense in a unified neural\nnetwork model, which reduces architectural complexity of the system compared to\ndetecting adversarial perturbations using separate models. We investigate into\nthe operational principles of our method by examining the attention mechanism.\nWe show the proposed AdvI token acts as a crucial element within the ViT,\ninfluencing attention weights and thereby highlighting regions or features in\nthe input data that are potentially suspicious or anomalous. Through\nexperimental results, we demonstrate that our approach surpasses several\ncompetitive methods in handling white-box attack scenarios, including those\nutilizing the fast gradient method, projected gradient descent attacks and\nbasic iterative method.", "AI": {"tldr": "提出一种新型视觉Transformer架构，通过引入对抗指示符（AdvI）令牌来检测对抗攻击，结合对抗训练和检测机制，提升系统防御能力。", "motivation": "Transformer在自动调制分类中的应用易受对抗攻击，需开发防御策略。", "method": "提出带有AdvI令牌的ViT架构，结合对抗训练和检测机制，统一训练时和运行时防御。", "result": "实验证明该方法在白盒攻击场景下优于多种竞争方法。", "conclusion": "AdvI令牌在ViT中有效检测对抗攻击，简化系统架构并提升防御性能。"}}
{"id": "2507.00665", "pdf": "https://arxiv.org/pdf/2507.00665", "abs": "https://arxiv.org/abs/2507.00665", "authors": ["Sihang Li", "Wei Shi", "Ziyuan Xie", "Tao Liang", "Guojun Ma", "Xiang Wang"], "title": "SAFER: Probing Safety in Reward Models with Sparse Autoencoder", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Reinforcement learning from human feedback (RLHF) is a key paradigm for\naligning large language models (LLMs) with human values, yet the reward models\nat its core remain largely opaque. In this work, we present sparse Autoencoder\nFor Enhanced Reward model (\\textbf{SAFER}), a novel framework for interpreting\nand improving reward models through mechanistic analysis. Leveraging Sparse\nAutoencoders (SAEs), we uncover human-interpretable features in reward model\nactivations, enabling insight into safety-relevant decision-making. We apply\nSAFER to safety-oriented preference datasets and quantify the salience of\nindividual features by activation differences between chosen and rejected\nresponses. Using these feature-level signals, we design targeted data poisoning\nand denoising strategies. Experiments show that SAFER can precisely degrade or\nenhance safety alignment with minimal data modification, without sacrificing\ngeneral chat performance. Our approach contributes to interpreting, auditing\nand refining reward models in high-stakes LLM alignment tasks. Our codes are\navailable at https://github.com/xzy-101/SAFER-code. \\textit{This paper\ndiscusses topics related to large language model safety and may include\ndiscussions or examples that highlight potential risks or unsafe outcomes.}", "AI": {"tldr": "SAFER框架通过稀疏自编码器（SAEs）解释和改进奖励模型，揭示人类可理解的特征，并设计数据投毒和去噪策略，以提升LLM的安全对齐。", "motivation": "奖励模型在RLHF中至关重要，但其内部机制不透明，需要一种方法来解释和改进其决策过程。", "method": "使用稀疏自编码器（SAEs）分析奖励模型激活，识别安全相关特征，并设计数据投毒和去噪策略。", "result": "实验表明，SAFER能以最小数据修改精确调整安全对齐，同时保持通用聊天性能。", "conclusion": "SAFER为高风险LLM对齐任务提供了解释、审计和改进奖励模型的方法。"}}
{"id": "2507.00334", "pdf": "https://arxiv.org/pdf/2507.00334", "abs": "https://arxiv.org/abs/2507.00334", "authors": ["Mengyi Shan", "Zecheng He", "Haoyu Ma", "Felix Juefei-Xu", "Peizhao Zhang", "Tingbo Hou", "Ching-Yao Chuang"], "title": "Populate-A-Scene: Affordance-Aware Human Video Generation", "categories": ["cs.CV"], "comment": "Project page: https://shanmy.github.io/Populate-A-Scene", "summary": "Can a video generation model be repurposed as an interactive world simulator?\nWe explore the affordance perception potential of text-to-video models by\nteaching them to predict human-environment interaction. Given a scene image and\na prompt describing human actions, we fine-tune the model to insert a person\ninto the scene, while ensuring coherent behavior, appearance, harmonization,\nand scene affordance. Unlike prior work, we infer human affordance for video\ngeneration (i.e., where to insert a person and how they should behave) from a\nsingle scene image, without explicit conditions like bounding boxes or body\nposes. An in-depth study of cross-attention heatmaps demonstrates that we can\nuncover the inherent affordance perception of a pre-trained video model without\nlabeled affordance datasets.", "AI": {"tldr": "研究探讨了文本到视频模型是否可作为交互式世界模拟器，通过微调模型预测人与环境的交互，无需显式条件即可推断场景的可用性。", "motivation": "探索文本到视频模型在感知场景可用性方面的潜力，实现从单张场景图像推断人类行为的能力。", "method": "通过微调模型，在场景图像中插入符合行为、外观和场景可用性的人，利用交叉注意力热图分析模型的固有可用性感知。", "result": "无需标记数据集或显式条件，模型能够生成符合场景可用性的人类行为视频。", "conclusion": "预训练的视频模型具备固有的可用性感知能力，可用于交互式世界模拟。"}}
{"id": "2507.00016", "pdf": "https://arxiv.org/pdf/2507.00016", "abs": "https://arxiv.org/abs/2507.00016", "authors": ["Xuanbo Liu", "Liu Liu", "Fuxiang Wu", "Fusheng Hao", "Xianglong Liu"], "title": "Gradient-based Fine-Tuning through Pre-trained Model Regularization", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Large pre-trained models have demonstrated extensive applications across\nvarious fields. However, fine-tuning these models for specific downstream tasks\ndemands significant computational resources and storage. One fine-tuning\nmethod, gradient-based parameter selection (GPS), focuses on fine-tuning only\nthe parameters with high gradients in each neuron, thereby reducing the number\nof training parameters. Nevertheless, this approach increases computational\nresource requirements and storage demands. In this paper, we propose an\nefficient gradient-based and regularized fine-tuning method (GRFT) that updates\nthe rows or columns of the weight matrix. We theoretically demonstrate that the\nrows or columns with the highest sum of squared gradients are optimal for\nupdating. This strategy effectively reduces storage overhead and improves the\nefficiency of parameter selection. Additionally, we incorporate regularization\nto enhance knowledge transfer from the pre-trained model. GRFT achieves\nstate-of-the-art performance, surpassing existing methods such as GPS, Adapter\nTuning, and LoRA. Notably, GRFT requires updating only 1.22% and 0.30% of the\ntotal parameters on FGVC and VTAB datasets, respectively, demonstrating its\nhigh efficiency and effectiveness. The source code will be released soon.", "AI": {"tldr": "提出了一种高效的基于梯度和正则化的微调方法（GRFT），通过更新权重矩阵的行或列来减少存储开销并提高参数选择效率，性能优于现有方法。", "motivation": "大型预训练模型在微调时需要大量计算资源和存储，现有方法如GPS虽减少训练参数但增加资源需求。", "method": "GRFT方法选择梯度平方和最高的行或列进行更新，并结合正则化以增强知识迁移。", "result": "GRFT在FGVC和VTAB数据集上仅需更新1.22%和0.30%的参数，性能优于GPS、Adapter Tuning和LoRA。", "conclusion": "GRFT在高效性和性能上均优于现有方法，是一种有效的微调策略。"}}
{"id": "2507.00700", "pdf": "https://arxiv.org/pdf/2507.00700", "abs": "https://arxiv.org/abs/2507.00700", "authors": ["Ahmed Sabir", "Azinovič Gasper", "Mengsay Loem", "Rajesh Sharma"], "title": "Contrasting Cognitive Styles in Vision-Language Models: Holistic Attention in Japanese Versus Analytical Focus in English", "categories": ["cs.CL"], "comment": null, "summary": "Cross-cultural research in perception and cognition has shown that\nindividuals from different cultural backgrounds process visual information in\ndistinct ways. East Asians, for example, tend to adopt a holistic perspective,\nattending to contextual relationships, whereas Westerners often employ an\nanalytical approach, focusing on individual objects and their attributes. In\nthis study, we investigate whether Vision-Language Models (VLMs) trained\npredominantly on different languages, specifically Japanese and English,\nexhibit similar culturally grounded attentional patterns. Using comparative\nanalysis of image descriptions, we examine whether these models reflect\ndifferences in holistic versus analytic tendencies. Our findings suggest that\nVLMs not only internalize the structural properties of language but also\nreproduce cultural behaviors embedded in the training data, indicating that\ncultural cognition may implicitly shape model outputs.", "AI": {"tldr": "研究发现，视觉语言模型（VLMs）在训练过程中内化了语言的结构特性，并再现了训练数据中的文化行为，表明文化认知可能隐式影响模型输出。", "motivation": "探讨不同文化背景（东亚与西方）对视觉信息处理的差异是否反映在基于不同语言（日语和英语）训练的视觉语言模型中。", "method": "通过对比分析图像描述，研究VLMs是否表现出整体性与分析性倾向的差异。", "result": "VLMs不仅内化了语言的结构特性，还再现了训练数据中的文化行为。", "conclusion": "文化认知可能隐式地塑造模型输出，表明文化背景对模型行为有显著影响。"}}
{"id": "2507.00339", "pdf": "https://arxiv.org/pdf/2507.00339", "abs": "https://arxiv.org/abs/2507.00339", "authors": ["Alexander Moore", "Amar Saini", "Kylie Cancilla", "Doug Poland", "Carmen Carrano"], "title": "Training for X-Ray Vision: Amodal Segmentation, Amodal Content Completion, and View-Invariant Object Representation from Multi-Camera Video", "categories": ["cs.CV", "cs.AI", "68T45, 68T07", "I.2.10; I.2.6; I.4.6"], "comment": "9 pages, 2 figures", "summary": "Amodal segmentation and amodal content completion require using object priors\nto estimate occluded masks and features of objects in complex scenes. Until\nnow, no data has provided an additional dimension for object context: the\npossibility of multiple cameras sharing a view of a scene. We introduce\nMOVi-MC-AC: Multiple Object Video with Multi-Cameras and Amodal Content, the\nlargest amodal segmentation and first amodal content dataset to date. Cluttered\nscenes of generic household objects are simulated in multi-camera video.\nMOVi-MC-AC contributes to the growing literature of object detection, tracking,\nand segmentation by including two new contributions to the deep learning for\ncomputer vision world. Multiple Camera (MC) settings where objects can be\nidentified and tracked between various unique camera perspectives are rare in\nboth synthetic and real-world video. We introduce a new complexity to synthetic\nvideo by providing consistent object ids for detections and segmentations\nbetween both frames and multiple cameras each with unique features and motion\npatterns on a single scene. Amodal Content (AC) is a reconstructive task in\nwhich models predict the appearance of target objects through occlusions. In\nthe amodal segmentation literature, some datasets have been released with\namodal detection, tracking, and segmentation labels. While other methods rely\non slow cut-and-paste schemes to generate amodal content pseudo-labels, they do\nnot account for natural occlusions present in the modal masks. MOVi-MC-AC\nprovides labels for ~5.8 million object instances, setting a new maximum in the\namodal dataset literature, along with being the first to provide ground-truth\namodal content. The full dataset is available at\nhttps://huggingface.co/datasets/Amar-S/MOVi-MC-AC ,", "AI": {"tldr": "MOVi-MC-AC是一个新的多摄像头视频数据集，用于模态分割和内容补全，提供多视角对象跟踪和模态内容标签。", "motivation": "现有数据集缺乏多摄像头视角和模态内容标签，限制了模态分割和内容补全的研究。", "method": "通过模拟多摄像头场景，提供一致的对象ID和模态内容标签，生成大规模合成视频数据。", "result": "数据集包含约580万对象实例，是模态分割领域最大的数据集，并首次提供真实模态内容标签。", "conclusion": "MOVi-MC-AC填补了多摄像头模态数据集的空白，为计算机视觉研究提供了新资源。"}}
{"id": "2507.00018", "pdf": "https://arxiv.org/pdf/2507.00018", "abs": "https://arxiv.org/abs/2507.00018", "authors": ["Bo Wang", "Qinyuan Cheng", "Runyu Peng", "Rong Bao", "Peiji Li", "Qipeng Guo", "Linyang Li", "Zhiyuan Zeng", "Yunhua Zhou", "Xipeng Qiu"], "title": "Implicit Reward as the Bridge: A Unified View of SFT and DPO Connections", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Post-training processes are essential phases in grounding pre-trained\nlanguage models to real-world tasks, with learning from demonstrations or\npreference signals playing a crucial role in this adaptation. We present a\nunified theoretical framework bridging Supervised Fine-Tuning (SFT) and\npreference learning in Large Language Model (LLM) post-training. Through\nrigorous mathematical derivation, we demonstrate that both SFT and preference\nlearning methods like Direct Preference Optimization (DPO) operate within the\nsame optimal policy-reward subspace, with SFT representing a special case of\nimplicit reward learning. Our analysis reveals a critical limitation in\nconventional SFT: the KL divergence term in distribution matching becomes\nconstant with respect to the policy during optimization, failing to constrain\nmodel updates. To address this, we propose a simple yet effective learning rate\nreduction approach that yields significant performance improvements (up to\n\\textbf{25\\%} relative gain and \\textbf{6\\%} absolute win rate increase in\ninstruction following tasks. Additionally, we derive alternative SFT objectives\nfrom various f-divergence functions that preserve the KL term during\noptimization, further enhancing post-DPO model performance. Finally, we extend\nthe theoretical relationship between LLM logits and Q-functions from preference\nlearning to the SFT context, providing mathematical derivations and\nexperimental validation.", "AI": {"tldr": "论文提出了一种统一的理论框架，将监督微调（SFT）和偏好学习（如DPO）联系起来，揭示了SFT的局限性，并提出改进方法。", "motivation": "研究如何通过统一框架优化语言模型的后训练过程，提升任务适应能力。", "method": "通过数学推导证明SFT和偏好学习的共同性，提出学习率降低方法和替代SFT目标。", "result": "改进方法显著提升性能（相对增益25%，绝对胜率提高6%）。", "conclusion": "研究为后训练优化提供了理论支持和实用方法，扩展了LLM的应用潜力。"}}
{"id": "2507.00718", "pdf": "https://arxiv.org/pdf/2507.00718", "abs": "https://arxiv.org/abs/2507.00718", "authors": ["Elizabeth Fons", "Elena Kochkina", "Rachneet Kaur", "Zhen Zeng", "Berowne Hlavaty", "Charese Smiley", "Svitlana Vyetrenko", "Manuela Veloso"], "title": "AI Analyst: Framework and Comprehensive Evaluation of Large Language Models for Financial Time Series Report Generation", "categories": ["cs.CL"], "comment": null, "summary": "This paper explores the potential of large language models (LLMs) to generate\nfinancial reports from time series data. We propose a framework encompassing\nprompt engineering, model selection, and evaluation. We introduce an automated\nhighlighting system to categorize information within the generated reports,\ndifferentiating between insights derived directly from time series data,\nstemming from financial reasoning, and those reliant on external knowledge.\nThis approach aids in evaluating the factual grounding and reasoning\ncapabilities of the models. Our experiments, utilizing both data from the real\nstock market indices and synthetic time series, demonstrate the capability of\nLLMs to produce coherent and informative financial reports.", "AI": {"tldr": "论文探讨了大型语言模型（LLMs）从时间序列数据生成财务报告的潜力，提出了一个包含提示工程、模型选择和评估的框架，并引入了自动高亮系统以分类报告中的信息。", "motivation": "研究LLMs在财务报告生成中的应用潜力，评估其事实基础和推理能力。", "method": "提出一个框架，包括提示工程、模型选择和评估，并引入自动高亮系统分类信息。", "result": "实验表明LLMs能生成连贯且信息丰富的财务报告。", "conclusion": "LLMs在财务报告生成中具有潜力，框架和自动高亮系统有助于评估其能力。"}}
{"id": "2507.00356", "pdf": "https://arxiv.org/pdf/2507.00356", "abs": "https://arxiv.org/abs/2507.00356", "authors": ["Zhiwei Yi", "Xin Cheng", "Jingyu Ma", "Ruifei Zhu", "Junwei Tian", "Yuanxiu Zhou", "Xinge Zhao", "Hongzhe Li"], "title": "CGEarthEye:A High-Resolution Remote Sensing Vision Foundation Model Based on the Jilin-1 Satellite Constellation", "categories": ["cs.CV", "cs.AI"], "comment": "A Remote Sensing Fundation Model for Very High Resolution Images", "summary": "Deep learning methods have significantly advanced the development of\nintelligent rinterpretation in remote sensing (RS), with foundational model\nresearch based on large-scale pre-training paradigms rapidly reshaping various\ndomains of Earth Observation (EO). However, compared to the open accessibility\nand high spatiotemporal coverage of medium-resolution data, the limited\nacquisition channels for ultra-high-resolution optical RS imagery have\nconstrained the progress of high-resolution remote sensing vision foundation\nmodels (RSVFM). As the world's largest sub-meter-level commercial RS satellite\nconstellation, the Jilin-1 constellation possesses abundant sub-meter-level\nimage resources. This study proposes CGEarthEye, a RSVFM framework specifically\ndesigned for Jilin-1 satellite characteristics, comprising five backbones with\ndifferent parameter scales with totaling 2.1 billion parameters. To enhance the\nrepresentational capacity of the foundation model, we developed JLSSD, the\nfirst 15-million-scale multi-temporal self-supervised learning (SSL) dataset\nfeaturing global coverage with quarterly temporal sampling within a single\nyear, constructed through multi-level representation clustering and sampling\nstrategies. The framework integrates seasonal contrast, augmentation-based\ncontrast, and masked patch token contrastive strategies for pre-training.\nComprehensive evaluations across 10 benchmark datasets covering four typical RS\ntasks demonstrate that the CGEarthEye consistently achieves state-of-the-art\n(SOTA) performance. Further analysis reveals CGEarthEye's superior\ncharacteristics in feature visualization, model convergence, parameter\nefficiency, and practical mapping applications. This study anticipates that the\nexceptional representation capabilities of CGEarthEye will facilitate broader\nand more efficient applications of Jilin-1 data in traditional EO application.", "AI": {"tldr": "该研究提出了一种针对吉林一号卫星特性的高分辨率遥感视觉基础模型（RSVFM）框架CGEarthEye，通过自监督学习数据集JLSSD和多策略对比预训练，在多个遥感任务中实现了最先进的性能。", "motivation": "尽管深度学习在遥感领域取得了进展，但超高分辨率光学遥感影像的获取渠道有限，限制了高分辨率遥感视觉基础模型的发展。吉林一号卫星作为全球最大的亚米级商业遥感卫星星座，提供了丰富的资源。", "method": "研究设计了CGEarthEye框架，包含五个不同参数规模的骨干网络（总计21亿参数），并开发了JLSSD数据集（1500万规模的多时相自监督学习数据集）。预训练结合了季节性对比、增强对比和掩码补丁标记对比策略。", "result": "在10个基准数据集上的评估表明，CGEarthEye在四种典型遥感任务中均达到了最先进性能，并在特征可视化、模型收敛、参数效率和实际应用中表现出优越特性。", "conclusion": "CGEarthEye的卓越表征能力有望推动吉林一号数据在传统地球观测应用中的更广泛和高效应用。"}}
{"id": "2507.00019", "pdf": "https://arxiv.org/pdf/2507.00019", "abs": "https://arxiv.org/abs/2507.00019", "authors": ["Minati Rath", "Hema Date"], "title": "Quantum Inspired Encoding Strategies for Machine Learning Models: Proposing and Evaluating Instance Level, Global Discrete, and Class Conditional Representations", "categories": ["cs.LG", "cs.AI", "quant-ph"], "comment": null, "summary": "In this study, we propose, evaluate and compare three quantum inspired data\nencoding strategies, Instance Level Strategy (ILS), Global Discrete Strategy\n(GDS) and Class Conditional Value Strategy (CCVS), for transforming classical\ndata into quantum data for use in pure classical machine learning models. The\nprimary objective is to reduce high encoding time while ensuring correct\nencoding values and analyzing their impact on classification performance. The\nInstance Level Strategy treats each row of dataset independently; mimics local\nquantum states. Global Discrete Value Based encoding strategy maps all unique\nfeature values across the full dataset to quantum states uniformly. In\ncontrast, the Class conditional Value based encoding strategy encodes unique\nvalues separately for each class, preserving class dependent information.\n  We apply these encoding strategies to a classification task and assess their\nimpact on en-coding efficiency, correctness, model accuracy, and computational\ncost. By analyzing the trade offs between encoding time, precision, and\npredictive performance, this study provides insights into optimizing quantum\ninspired data transformations for classical machine learning workflows.", "AI": {"tldr": "本研究提出并比较了三种量子启发的数据编码策略（ILS、GDS、CCVS），旨在减少编码时间并确保准确性，同时分析其对分类性能的影响。", "motivation": "目标是优化量子启发的数据转换，以提升经典机器学习模型的效率。", "method": "三种策略：ILS（独立处理每行数据）、GDS（全局统一映射）、CCVS（按类别单独编码）。", "result": "评估了编码效率、正确性、模型准确性和计算成本，分析了时间、精度和性能的权衡。", "conclusion": "研究为量子启发的数据转换在经典机器学习中的应用提供了优化思路。"}}
{"id": "2507.00769", "pdf": "https://arxiv.org/pdf/2507.00769", "abs": "https://arxiv.org/abs/2507.00769", "authors": ["Daniel Fein", "Sebastian Russo", "Violet Xiang", "Kabir Jolly", "Rafael Rafailov", "Nick Haber"], "title": "LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Evaluating creative writing generated by large language models (LLMs) remains\nchallenging because open-ended narratives lack ground truths. Without\nperformant automated evaluation methods, off-the-shelf (OTS) language models\nare employed as zero-shot judges, yet their reliability is unclear in this\ncontext. In pursuit of robust evaluation for creative writing, we introduce\nLitBench, the first standardized benchmark and paired dataset for creative\nwriting verification, comprising a held-out test set of 2,480 debiased,\nhuman-labeled story comparisons drawn from Reddit and a 43,827-pair training\ncorpus of human preference labels. Using LitBench, we (i) benchmark zero-shot\nLLM judges, (ii) train Bradley Terry and generative reward models, and (iii)\nconduct an online human study to validate reward model rankings on newly\nLLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the\nstrongest off-the-shelf judge, reaching 73% agreement with human preferences;\namong trained reward models, Bradley-Terry and Generative reward models both\nattain an accuracy of 78%, outperforming all off-the-shelf judges. An online\nhuman study further confirms that our trained reward models consistently align\nwith human preferences in novel LLM-generated stories. We release LitBench and\nreward models at\nhttps://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461,\nproviding a vetted resource for reliable, automated evaluation and optimization\nof creative writing systems.", "AI": {"tldr": "LitBench是一个标准化基准和数据集，用于评估大语言模型生成的创意写作，通过人类偏好标签和模型训练提升评估可靠性。", "motivation": "由于开放叙事缺乏真实标准，评估大语言模型生成的创意写作具有挑战性，需要可靠的自动化评估方法。", "method": "引入LitBench，包含人类标注的故事比较数据集，训练Bradley Terry和生成奖励模型，并进行在线人类研究验证。", "result": "Claude-3.7-Sonnet是最佳零-shot评估模型（73%人类一致性），训练模型达78%准确性。", "conclusion": "LitBench为创意写作系统提供了可靠的自动化评估和优化资源。"}}
{"id": "2507.00363", "pdf": "https://arxiv.org/pdf/2507.00363", "abs": "https://arxiv.org/abs/2507.00363", "authors": ["Xingjun Wang", "Lianlei Shan"], "title": "GDGS: 3D Gaussian Splatting Via Geometry-Guided Initialization And Dynamic Density Control", "categories": ["cs.CV"], "comment": null, "summary": "We propose a method to enhance 3D Gaussian Splatting (3DGS)~\\cite{Kerbl2023},\naddressing challenges in initialization, optimization, and density control.\nGaussian Splatting is an alternative for rendering realistic images while\nsupporting real-time performance, and it has gained popularity due to its\nexplicit 3D Gaussian representation. However, 3DGS heavily depends on accurate\ninitialization and faces difficulties in optimizing unstructured Gaussian\ndistributions into ordered surfaces, with limited adaptive density control\nmechanism proposed so far. Our first key contribution is a geometry-guided\ninitialization to predict Gaussian parameters, ensuring precise placement and\nfaster convergence. We then introduce a surface-aligned optimization strategy\nto refine Gaussian placement, improving geometric accuracy and aligning with\nthe surface normals of the scene. Finally, we present a dynamic adaptive\ndensity control mechanism that adjusts Gaussian density based on regional\ncomplexity, for visual fidelity. These innovations enable our method to achieve\nhigh-fidelity real-time rendering and significant improvements in visual\nquality, even in complex scenes. Our method demonstrates comparable or superior\nresults to state-of-the-art methods, rendering high-fidelity images in real\ntime.", "AI": {"tldr": "提出了一种改进3D高斯泼溅（3DGS）的方法，解决了初始化、优化和密度控制的挑战，实现了高保真实时渲染。", "motivation": "3DGS因其显式3D高斯表示在实时渲染中受欢迎，但依赖准确初始化且优化无序高斯分布困难，缺乏自适应密度控制。", "method": "提出几何引导初始化、表面对齐优化策略和动态自适应密度控制机制。", "result": "方法在复杂场景中实现高保真实时渲染，视觉质量显著提升，优于或媲美现有方法。", "conclusion": "通过改进初始化、优化和密度控制，显著提升了3DGS的渲染质量和效率。"}}
{"id": "2507.00022", "pdf": "https://arxiv.org/pdf/2507.00022", "abs": "https://arxiv.org/abs/2507.00022", "authors": ["Zehao Wang"], "title": "GLU Attention Improve Transformer", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.NE"], "comment": "4 pages 4 figures", "summary": "Gated Linear Units (GLU) have shown great potential in enhancing neural\nnetwork performance. In this paper, I introduce a novel attention mechanism\ncalled GLU Attention, which introduces nonlinearity into the values of\nAttention. My experiments demonstrate that GLU Attention improves both model\nperformance and convergence speed across text and vision modalities with zero\nadditional parameters and negligible computational costs. GLU Attention is\nlightweight and can seamlessly integrate with other technologies, such as Flash\nAttention, Rotary Position Embedding (RoPE), and various Multi-Head Attention\n(MHA) variants such as Grouped-Query Attention (GQA). This project is\nopen-sourced at github.", "AI": {"tldr": "提出了一种名为GLU Attention的新型注意力机制，通过引入非线性提升模型性能，且无需额外参数和计算成本。", "motivation": "探索如何通过改进注意力机制中的非线性特性来提升神经网络的性能和收敛速度。", "method": "在注意力机制的值部分引入Gated Linear Units（GLU）的非线性特性，设计出GLU Attention。", "result": "实验表明，GLU Attention在文本和视觉任务中均能提升模型性能和收敛速度，且兼容多种现有技术。", "conclusion": "GLU Attention是一种轻量级、高效的注意力机制改进方法，具有广泛的应用潜力。"}}
{"id": "2507.00782", "pdf": "https://arxiv.org/pdf/2507.00782", "abs": "https://arxiv.org/abs/2507.00782", "authors": ["Matthieu Pierre Boyer"], "title": "A Diagrammatic Calculus for a Functional Model of Natural Language Semantics", "categories": ["cs.CL", "cs.PL", "J.5; D.3.1; D.3.3"], "comment": "15 pages, preprint before submission to CSL 2026", "summary": "In this paper, we study a functional programming approach to natural language\nsemantics, allowing us to increase the expressivity of a more traditional\ndenotation style. We will formalize a category based type and effect system,\nand construct a diagrammatic calculus to model parsing and handling of effects,\nand use it to efficiently compute the denotations for sentences.", "AI": {"tldr": "论文提出了一种基于函数式编程的自然语言语义方法，通过类型和效应系统及图解演算提升表达能力。", "motivation": "传统指称语义的表达能力有限，需要更高效的方法来计算句子的指称。", "method": "形式化基于类别的类型和效应系统，构建图解演算以建模效应解析和处理。", "result": "实现了对句子指称的高效计算。", "conclusion": "该方法提升了自然语言语义的表达能力和计算效率。"}}
{"id": "2507.00365", "pdf": "https://arxiv.org/pdf/2507.00365", "abs": "https://arxiv.org/abs/2507.00365", "authors": ["Wanghui Xiao"], "title": "An Improved U-Net Model for Offline handwriting signature denoising", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Handwriting signatures, as an important means of identity recognition, are\nwidely used in multiple fields such as financial transactions, commercial\ncontracts and personal affairs due to their legal effect and uniqueness. In\nforensic science appraisals, the analysis of offline handwriting signatures\nrequires the appraiser to provide a certain number of signature samples, which\nare usually derived from various historical contracts or archival materials.\nHowever, the provided handwriting samples are often mixed with a large amount\nof interfering information, which brings severe challenges to handwriting\nidentification work. This study proposes a signature handwriting denoising\nmodel based on the improved U-net structure, aiming to enhance the robustness\nof the signature recognition system. By introducing discrete wavelet transform\nand PCA transform, the model's ability to suppress noise has been enhanced. The\nexperimental results show that this modelis significantly superior to the\ntraditional methods in denoising effect, can effectively improve the clarity\nand readability of the signed images, and provide more reliable technical\nsupport for signature analysis and recognition.", "AI": {"tldr": "该研究提出了一种基于改进U-net结构的签名去噪模型，通过离散小波变换和PCA变换增强去噪能力，显著优于传统方法。", "motivation": "手写签名在身份识别中具有法律效力和独特性，但签名样本常受干扰信息影响，给识别工作带来挑战。", "method": "采用改进的U-net结构，结合离散小波变换和PCA变换，提升去噪能力。", "result": "实验表明，该模型去噪效果显著优于传统方法，提高了签名图像的清晰度和可读性。", "conclusion": "该模型为签名分析和识别提供了更可靠的技术支持。"}}
{"id": "2507.00024", "pdf": "https://arxiv.org/pdf/2507.00024", "abs": "https://arxiv.org/abs/2507.00024", "authors": ["Yeyong Yu", "Xilei Bian", "Jie Xiong", "Xing Wu", "Quan Qian"], "title": "AIMatDesign: Knowledge-Augmented Reinforcement Learning for Inverse Materials Design under Data Scarcity", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI"], "comment": null, "summary": "With the growing demand for novel materials, machine learning-driven inverse\ndesign methods face significant challenges in reconciling the high-dimensional\nmaterials composition space with limited experimental data. Existing approaches\nsuffer from two major limitations: (I) machine learning models often lack\nreliability in high-dimensional spaces, leading to prediction biases during the\ndesign process; (II) these models fail to effectively incorporate domain expert\nknowledge, limiting their capacity to support knowledge-guided inverse design.\nTo address these challenges, we introduce AIMatDesign, a reinforcement learning\nframework that addresses these limitations by augmenting experimental data\nusing difference-based algorithms to build a trusted experience pool,\naccelerating model convergence. To enhance model reliability, an automated\nrefinement strategy guided by large language models (LLMs) dynamically corrects\nprediction inconsistencies, reinforcing alignment between reward signals and\nstate value functions. Additionally, a knowledge-based reward function\nleverages expert domain rules to improve stability and efficiency during\ntraining. Our experiments demonstrate that AIMatDesign significantly surpasses\ntraditional machine learning and reinforcement learning methods in discovery\nefficiency, convergence speed, and success rates. Among the numerous candidates\nproposed by AIMatDesign, experimental synthesis of representative Zr-based\nalloys yielded a top-performing BMG with 1.7GPa yield strength and 10.2\\%\nelongation, closely matching predictions. Moreover, the framework accurately\ncaptured the trend of yield strength variation with composition, demonstrating\nits reliability and potential for closed-loop materials discovery.", "AI": {"tldr": "AIMatDesign是一个强化学习框架，通过增强实验数据和结合专家知识，解决了高维材料设计中的预测偏差和知识整合问题，显著提升了材料发现效率。", "motivation": "现有机器学习方法在高维材料设计中存在预测偏差和专家知识整合不足的问题，AIMatDesign旨在解决这些挑战。", "method": "采用强化学习框架，结合差异算法增强数据，利用大语言模型自动修正预测不一致，并通过知识奖励函数优化训练。", "result": "实验表明，AIMatDesign在发现效率、收敛速度和成功率上优于传统方法，成功合成高性能Zr基合金。", "conclusion": "AIMatDesign展示了在闭环材料发现中的可靠性和潜力，能够准确预测材料性能趋势。"}}
{"id": "2507.00783", "pdf": "https://arxiv.org/pdf/2507.00783", "abs": "https://arxiv.org/abs/2507.00783", "authors": ["Benedetto Lepori", "Jens Peter Andersen", "Karsten Donnay"], "title": "Generative AI and the future of scientometrics: current topics and future questions", "categories": ["cs.CL", "cs.DL"], "comment": null, "summary": "The aim of this paper is to review the use of GenAI in scientometrics, and to\nbegin a debate on the broader implications for the field. First, we provide an\nintroduction on GenAI's generative and probabilistic nature as rooted in\ndistributional linguistics. And we relate this to the debate on the extent to\nwhich GenAI might be able to mimic human 'reasoning'. Second, we leverage this\ndistinction for a critical engagement with recent experiments using GenAI in\nscientometrics, including topic labelling, the analysis of citation contexts,\npredictive applications, scholars' profiling, and research assessment. GenAI\nshows promise in tasks where language generation dominates, such as labelling,\nbut faces limitations in tasks that require stable semantics, pragmatic\nreasoning, or structured domain knowledge. However, these results might become\nquickly outdated. Our recommendation is, therefore, to always strive to\nsystematically compare the performance of different GenAI models for specific\ntasks. Third, we inquire whether, by generating large amounts of scientific\nlanguage, GenAI might have a fundamental impact on our field by affecting\ntextual characteristics used to measure science, such as authors, words, and\nreferences. We argue that careful empirical work and theoretical reflection\nwill be essential to remain capable of interpreting the evolving patterns of\nknowledge production.", "AI": {"tldr": "本文回顾了GenAI在科学计量学中的应用，并探讨了其对领域的广泛影响。", "motivation": "探讨GenAI在科学计量学中的潜力及其对科学测量方法的影响。", "method": "通过分析GenAI的生成和概率特性，结合其在科学计量学中的实验应用（如主题标注、引用分析等），评估其优缺点。", "result": "GenAI在语言生成任务中表现良好，但在需要稳定语义或领域知识的任务中存在局限。", "conclusion": "建议系统比较不同GenAI模型的表现，并强调需通过实证研究和理论反思应对GenAI对科学测量方法的潜在影响。"}}
{"id": "2507.00368", "pdf": "https://arxiv.org/pdf/2507.00368", "abs": "https://arxiv.org/abs/2507.00368", "authors": ["Hikaru Shijo", "Yutaka Yoshihama", "Kenichi Yadani", "Norifumi Murata"], "title": "Out-of-Distribution Detection with Adaptive Top-K Logits Integration", "categories": ["cs.CV"], "comment": null, "summary": "Neural networks often make overconfident predictions from out-of-distribution\n(OOD) samples. Detection of OOD data is therefore crucial to improve the safety\nof machine learning. The simplest and most powerful method for OOD detection is\nMaxLogit, which uses the model's maximum logit to provide an OOD score. We have\ndiscovered that, in addition to the maximum logit, some other logits are also\nuseful for OOD detection. Based on this finding, we propose a new method called\nATLI (Adaptive Top-k Logits Integration), which adaptively determines effective\ntop-k logits that are specific to each model and combines the maximum logit\nwith the other top-k logits. In this study we evaluate our proposed method\nusing ImageNet-1K benchmark. Extensive experiments showed our proposed method\nto reduce the false positive rate (FPR95) by 6.73% compared to the MaxLogit\napproach, and decreased FPR95 by an additional 2.67% compared to other\nstate-of-the-art methods.", "AI": {"tldr": "论文提出了一种名为ATLI的新方法，通过自适应选择有效的top-k logits并结合最大logit，显著提升了OOD检测性能，优于现有方法。", "motivation": "神经网络对分布外（OOD）样本的预测往往过于自信，检测OOD数据对提升机器学习安全性至关重要。", "method": "提出ATLI方法，自适应确定模型特定的有效top-k logits，并将其与最大logit结合用于OOD检测。", "result": "在ImageNet-1K基准测试中，ATLI将FPR95降低了6.73%（相比MaxLogit）和2.67%（相比其他先进方法）。", "conclusion": "ATLI通过利用更多logit信息，显著提升了OOD检测性能，为机器学习安全性提供了更优解决方案。"}}
{"id": "2507.00025", "pdf": "https://arxiv.org/pdf/2507.00025", "abs": "https://arxiv.org/abs/2507.00025", "authors": ["Tiexin Qin", "Hong Yan", "Haoliang Li"], "title": "Generalizing to New Dynamical Systems via Frequency Domain Adaptation", "categories": ["cs.LG", "cs.AI", "stat.ML"], "comment": "Accepted by TPAMI 2025", "summary": "Learning the underlying dynamics from data with deep neural networks has\nshown remarkable potential in modeling various complex physical dynamics.\nHowever, current approaches are constrained in their ability to make reliable\npredictions in a specific domain and struggle with generalizing to unseen\nsystems that are governed by the same general dynamics but differ in\nenvironmental characteristics. In this work, we formulate a parameter-efficient\nmethod, Fourier Neural Simulator for Dynamical Adaptation (FNSDA), that can\nreadily generalize to new dynamics via adaptation in the Fourier space.\nSpecifically, FNSDA identifies the shareable dynamics based on the known\nenvironments using an automatic partition in Fourier modes and learns to adjust\nthe modes specific for each new environment by conditioning on low-dimensional\nlatent systematic parameters for efficient generalization. We evaluate our\napproach on four representative families of dynamic systems, and the results\nshow that FNSDA can achieve superior or competitive generalization performance\ncompared to existing methods with a significantly reduced parameter cost. Our\ncode is available at https://github.com/WonderSeven/FNSDA.", "AI": {"tldr": "FNSDA是一种参数高效的方法，通过傅里叶空间的动态适应，能够泛化到新的动态系统。", "motivation": "现有方法在特定领域内预测能力有限，难以泛化到具有相同动态但环境特征不同的新系统。", "method": "FNSDA在傅里叶空间中自动划分共享动态，并通过低维潜在系统参数调整特定模式。", "result": "在四个代表性动态系统上，FNSDA表现出优于或与现有方法相当的泛化性能，且参数成本显著降低。", "conclusion": "FNSDA提供了一种高效且通用的动态建模方法，适用于复杂物理系统的泛化预测。"}}
{"id": "2507.00814", "pdf": "https://arxiv.org/pdf/2507.00814", "abs": "https://arxiv.org/abs/2507.00814", "authors": ["Anita Keshmirian", "Razan Baltaji", "Babak Hemmatian", "Hadi Asghari", "Lav R. Varshney"], "title": "Many LLMs Are More Utilitarian Than One", "categories": ["cs.CL", "cs.AI", "cs.CY", "I.2.7; I.2.11"], "comment": "9 pages, 8 Figures, 7 tables", "summary": "Moral judgment is integral to large language model (LLM) alignment and social\nreasoning. As multi-agent systems gain prominence, it becomes crucial to\nunderstand how LLMs function collectively during collaboration, compared to\nindividual agents. In human moral judgment, group deliberation leads to a\nutilitarian boost: a tendency to endorse norm violations that maximize benefits\nfor the greatest number of people despite harms. We study whether a similar\ndynamic emerges in multi-agent LLM systems. We tested six models on\nwell-established sets of moral dilemmas across two conditions: (1) Solo, where\nmodels reasoned independently, and (2) Group, where they engaged in multi-turn\ndiscussions in pairs or triads. In personal moral dilemmas, where agents must\ndecide to directly harm one individual to maximize the utility for others, all\nmodels found moral violations to be more acceptable when part of a group than\nindividually, similar to human experiments. Some models endorsed actions that\nmaximized overall well-being, even if they benefited strangers over familiar\nindividuals. Others became more willing to violate moral norms in groups.\nHowever, while human groups show a similar action bias, the mechanism for their\nutilitarian boost differs from LLMs. Whereas the human shift comes from\nheightened sensitivity to decision outcomes, LLM groups show either reduced\nnorm sensitivity or enhanced impartiality. This suggests that while the surface\nbehavior of LLM collectives mimics human group reasoning, the underlying\ndrivers differ. We discuss the implications for AI alignment, multi-agent\ndesign, and artificial moral reasoning.", "AI": {"tldr": "研究发现，多代理LLM系统在群体讨论中表现出类似人类的功利主义倾向，但其驱动机制与人类不同。", "motivation": "探讨多代理LLM系统在道德判断中的集体行为是否与人类相似。", "method": "测试六个LLM模型在独立和群体讨论条件下的道德困境反应。", "result": "LLM在群体中更倾向于接受道德违规，但驱动机制与人类不同。", "conclusion": "LLM集体行为的表面模仿与人类不同，对AI对齐和多代理设计有重要意义。"}}
{"id": "2507.00371", "pdf": "https://arxiv.org/pdf/2507.00371", "abs": "https://arxiv.org/abs/2507.00371", "authors": ["Xin Yang", "Ruiming Du", "Hanyang Huang", "Jiayang Xie", "Pengyao Xie", "Leisen Fang", "Ziyue Guo", "Nanjun Jiang", "Yu Jiang", "Haiyan Cen"], "title": "PlantSegNeRF: A few-shot, cross-dataset method for plant 3D instance point cloud reconstruction via joint-channel NeRF with multi-view image instance matching", "categories": ["cs.CV"], "comment": null, "summary": "Organ segmentation of plant point clouds is a prerequisite for the\nhigh-resolution and accurate extraction of organ-level phenotypic traits.\nAlthough the fast development of deep learning has boosted much research on\nsegmentation of plant point clouds, the existing techniques for organ\nsegmentation still face limitations in resolution, segmentation accuracy, and\ngeneralizability across various plant species. In this study, we proposed a\nnovel approach called plant segmentation neural radiance fields (PlantSegNeRF),\naiming to directly generate high-precision instance point clouds from\nmulti-view RGB image sequences for a wide range of plant species. PlantSegNeRF\nperformed 2D instance segmentation on the multi-view images to generate\ninstance masks for each organ with a corresponding ID. The multi-view instance\nIDs corresponding to the same plant organ were then matched and refined using a\nspecially designed instance matching module. The instance NeRF was developed to\nrender an implicit scene, containing color, density, semantic and instance\ninformation. The implicit scene was ultimately converted into high-precision\nplant instance point clouds based on the volume density. The results proved\nthat in semantic segmentation of point clouds, PlantSegNeRF outperformed the\ncommonly used methods, demonstrating an average improvement of 16.1%, 18.3%,\n17.8%, and 24.2% in precision, recall, F1-score, and IoU compared to the\nsecond-best results on structurally complex datasets. More importantly,\nPlantSegNeRF exhibited significant advantages in plant point cloud instance\nsegmentation tasks. Across all plant datasets, it achieved average improvements\nof 11.7%, 38.2%, 32.2% and 25.3% in mPrec, mRec, mCov, mWCov, respectively.\nThis study extends the organ-level plant phenotyping and provides a\nhigh-throughput way to supply high-quality 3D data for the development of\nlarge-scale models in plant science.", "AI": {"tldr": "提出了一种名为PlantSegNeRF的新方法，通过多视角RGB图像序列直接生成高精度植物器官点云，显著提升了分割精度和通用性。", "motivation": "现有植物点云器官分割技术在分辨率、精度和跨物种通用性方面存在局限，需要一种更高效、高精度的解决方案。", "method": "PlantSegNeRF结合2D实例分割、实例匹配模块和实例NeRF，从多视角图像生成包含颜色、密度、语义和实例信息的隐式场景，最终转换为高精度点云。", "result": "在语义分割和实例分割任务中，PlantSegNeRF表现优异，各项指标平均提升显著，尤其在复杂数据集上优势明显。", "conclusion": "PlantSegNeRF为植物器官级表型分析提供了高通量方法，并为植物科学大规模模型开发提供了高质量3D数据支持。"}}
{"id": "2507.00026", "pdf": "https://arxiv.org/pdf/2507.00026", "abs": "https://arxiv.org/abs/2507.00026", "authors": ["Jiale Ding", "Xiang Zheng", "Cong Wang", "Wei-Bin Lee", "Xingjun Ma", "Yu-Gang Jiang"], "title": "ROSE: Toward Reality-Oriented Safety Evaluation of Large Language Models", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CY"], "comment": null, "summary": "As Large Language Models (LLMs) are increasingly deployed as black-box\ncomponents in real-world applications, evaluating their safety-especially under\nadversarial prompting-has become critical. Arguably, effective safety\nevaluations should be adaptive, evolving with LLM capabilities, and also cover\na broad spectrum of harmful topics and real-world scenarios to fully expose\npotential vulnerabilities. Existing manual safety benchmarks, built on\nhandcrafted adversarial prompts, are limited by their static nature and the\nintensive labor required to update them, making it difficult to keep pace with\nrapidly advancing LLMs. In contrast, automated adversarial prompt generation\noffers a promising path toward adaptive evaluation. However, current methods\noften suffer from insufficient adversarial topic coverage (topic-level\ndiversity) and weak alignment with real-world contexts. These shortcomings stem\nfrom the exploration-exploitation dilemma in black-box optimization and a lack\nof real-world contextualization, resulting in adversarial prompts that are both\ntopically narrow and scenario-repetitive. To address these issues, we propose\nReality-Oriented Safety Evaluation (ROSE), a novel framework that uses\nmulti-objective reinforcement learning to fine-tune an adversarial LLM for\ngenerating topically diverse and contextually rich adversarial prompts.\nExperiments show that ROSE outperforms existing methods in uncovering safety\nvulnerabilities in state-of-the-art LLMs, with notable improvements in\nintegrated evaluation metrics. We hope ROSE represents a step toward more\npractical and reality-oriented safety evaluation of LLMs. WARNING: This paper\ncontains examples of potentially harmful text.", "AI": {"tldr": "论文提出ROSE框架，通过多目标强化学习生成多样化和情境丰富的对抗性提示，以更有效地评估大型语言模型的安全性。", "motivation": "随着大型语言模型（LLMs）在现实应用中的广泛部署，评估其安全性（尤其是在对抗性提示下的表现）变得至关重要。现有手动基准的静态性和更新成本高，难以适应快速发展的LLMs。", "method": "提出ROSE框架，利用多目标强化学习微调对抗性LLM，生成主题多样且情境丰富的对抗性提示。", "result": "实验表明，ROSE在揭示先进LLMs的安全漏洞方面优于现有方法，综合评估指标显著提升。", "conclusion": "ROSE为更实用和现实导向的LLMs安全性评估迈出了重要一步。"}}
{"id": "2507.00828", "pdf": "https://arxiv.org/pdf/2507.00828", "abs": "https://arxiv.org/abs/2507.00828", "authors": ["Alexander Hoyle", "Lorena Calvo-Bartolomé", "Jordan Boyd-Graber", "Philip Resnik"], "title": "ProxAnn: Use-Oriented Evaluations of Topic Models and Document Clustering", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 (Main)", "summary": "Topic model and document-clustering evaluations either use automated metrics\nthat align poorly with human preferences or require expert labels that are\nintractable to scale. We design a scalable human evaluation protocol and a\ncorresponding automated approximation that reflect practitioners' real-world\nusage of models. Annotators -- or an LLM-based proxy -- review text items\nassigned to a topic or cluster, infer a category for the group, then apply that\ncategory to other documents. Using this protocol, we collect extensive\ncrowdworker annotations of outputs from a diverse set of topic models on two\ndatasets. We then use these annotations to validate automated proxies, finding\nthat the best LLM proxies are statistically indistinguishable from a human\nannotator and can therefore serve as a reasonable substitute in automated\nevaluations. Package, web interface, and data are at\nhttps://github.com/ahoho/proxann", "AI": {"tldr": "论文提出了一种可扩展的人类评估协议及其自动化近似方法，用于评估主题模型和文档聚类，解决了传统方法依赖专家标签或与人类偏好不一致的问题。", "motivation": "传统评估方法要么依赖难以扩展的专家标签，要么使用与人类偏好不一致的自动化指标，无法满足实际需求。", "method": "设计了一种评估协议，通过人工标注或基于LLM的代理推断类别并应用于其他文档，收集了大量标注数据验证自动化代理。", "result": "发现最佳LLM代理与人类标注者统计上无差异，可作为自动化评估的合理替代。", "conclusion": "提出的方法为实际应用中的主题模型和文档聚类评估提供了可扩展且可靠的解决方案。"}}
{"id": "2507.00372", "pdf": "https://arxiv.org/pdf/2507.00372", "abs": "https://arxiv.org/abs/2507.00372", "authors": ["Xinge Yang", "Chuong Nguyen", "Wenbin Wang", "Kaizhang Kang", "Wolfgang Heidrich", "Xiaoxing Li"], "title": "Efficient Depth- and Spatially-Varying Image Simulation for Defocus Deblur", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Modern cameras with large apertures often suffer from a shallow depth of\nfield, resulting in blurry images of objects outside the focal plane. This\nlimitation is particularly problematic for fixed-focus cameras, such as those\nused in smart glasses, where adding autofocus mechanisms is challenging due to\nform factor and power constraints. Due to unmatched optical aberrations and\ndefocus properties unique to each camera system, deep learning models trained\non existing open-source datasets often face domain gaps and do not perform well\nin real-world settings. In this paper, we propose an efficient and scalable\ndataset synthesis approach that does not rely on fine-tuning with real-world\ndata. Our method simultaneously models depth-dependent defocus and spatially\nvarying optical aberrations, addressing both computational complexity and the\nscarcity of high-quality RGB-D datasets. Experimental results demonstrate that\na network trained on our low resolution synthetic images generalizes\neffectively to high resolution (12MP) real-world images across diverse scenes.", "AI": {"tldr": "提出一种高效、可扩展的数据集合成方法，解决大光圈相机因浅景深导致的模糊问题，无需依赖真实数据微调。", "motivation": "大光圈相机因浅景深导致图像模糊，尤其是固定焦距相机（如智能眼镜）难以添加自动对焦机制，现有深度学习模型因域差距在真实场景中表现不佳。", "method": "同时建模深度相关散焦和空间变化光学像差，解决计算复杂性和高质量RGB-D数据集稀缺问题。", "result": "实验表明，基于低分辨率合成图像训练的网络能有效泛化到高分辨率（12MP）真实场景图像。", "conclusion": "提出的方法为相机系统提供了一种高效且可扩展的解决方案，无需依赖真实数据微调。"}}
{"id": "2507.00028", "pdf": "https://arxiv.org/pdf/2507.00028", "abs": "https://arxiv.org/abs/2507.00028", "authors": ["Lihuan Li", "Hao Xue", "Shuang Ao", "Yang Song", "Flora Salim"], "title": "HiT-JEPA: A Hierarchical Self-supervised Trajectory Embedding Framework for Similarity Computation", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "The representation of urban trajectory data plays a critical role in\neffectively analyzing spatial movement patterns. Despite considerable progress,\nthe challenge of designing trajectory representations that can capture diverse\nand complementary information remains an open research problem. Existing\nmethods struggle in incorporating trajectory fine-grained details and\nhigh-level summary in a single model, limiting their ability to attend to both\nlong-term dependencies while preserving local nuances. To address this, we\npropose HiT-JEPA (Hierarchical Interactions of Trajectory Semantics via a Joint\nEmbedding Predictive Architecture), a unified framework for learning\nmulti-scale urban trajectory representations across semantic abstraction\nlevels. HiT-JEPA adopts a three-layer hierarchy that progressively captures\npoint-level fine-grained details, intermediate patterns, and high-level\ntrajectory abstractions, enabling the model to integrate both local dynamics\nand global semantics in one coherent structure. Extensive experiments on\nmultiple real-world datasets for trajectory similarity computation show that\nHiT-JEPA's hierarchical design yields richer, multi-scale representations. Code\nis available at: https://anonymous.4open.science/r/HiT-JEPA.", "AI": {"tldr": "HiT-JEPA提出了一种分层轨迹表示框架，结合细粒度细节和高层语义，用于多尺度城市轨迹分析。", "motivation": "现有方法难以同时捕捉轨迹的细粒度细节和高层摘要，限制了其对长期依赖和局部细微差异的关注。", "method": "HiT-JEPA采用三层分层结构，逐步捕获点级细节、中间模式和高层轨迹抽象，整合局部动态和全局语义。", "result": "在多个真实数据集上的实验表明，HiT-JEPA的分层设计能生成更丰富的多尺度表示。", "conclusion": "HiT-JEPA通过分层设计有效解决了轨迹表示的多尺度问题，提升了轨迹相似性计算的性能。"}}
{"id": "2507.00838", "pdf": "https://arxiv.org/pdf/2507.00838", "abs": "https://arxiv.org/abs/2507.00838", "authors": ["Karol Przystalski", "Jan K. Argasiński", "Iwona Grabska-Gradzińska", "Jeremi K. Ochab"], "title": "Stylometry recognizes human and LLM-generated texts in short samples", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "The paper explores stylometry as a method to distinguish between texts\ncreated by Large Language Models (LLMs) and humans, addressing issues of model\nattribution, intellectual property, and ethical AI use. Stylometry has been\nused extensively to characterise the style and attribute authorship of texts.\nBy applying it to LLM-generated texts, we identify their emergent writing\npatterns. The paper involves creating a benchmark dataset based on Wikipedia,\nwith (a) human-written term summaries, (b) texts generated purely by LLMs\n(GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text\nsummarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods\n(Dipper, T5). The 10-sentence long texts were classified by tree-based models\n(decision trees and LightGBM) using human-designed (StyloMetrix) and\nn-gram-based (our own pipeline) stylometric features that encode lexical,\ngrammatical, syntactic, and punctuation patterns. The cross-validated results\nreached a performance of up to .87 Matthews correlation coefficient in the\nmulticlass scenario with 7 classes, and accuracy between .79 and 1. in binary\nclassification, with the particular example of Wikipedia and GPT-4 reaching up\nto .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed\nfeatures characteristic of the encyclopaedic text type, individual overused\nwords, as well as a greater grammatical standardisation of LLMs with respect to\nhuman-written texts. These results show -- crucially, in the context of the\nincreasingly sophisticated LLMs -- that it is possible to distinguish machine-\nfrom human-generated texts at least for a well-defined text type.", "AI": {"tldr": "该论文探讨了如何使用文体测量学区分LLM生成文本与人类文本，涉及模型归属、知识产权和AI伦理问题，并通过实验验证了方法的有效性。", "motivation": "解决LLM生成文本的归属、知识产权和伦理问题，探索其写作模式。", "method": "创建基于Wikipedia的基准数据集，使用树模型（决策树和LightGBM）结合文体特征进行分类。", "result": "多分类场景下Matthews相关系数达0.87，二分类准确率0.79-1.0，Wikipedia与GPT-4的平衡数据集准确率达0.98。", "conclusion": "研究表明，对于特定文本类型，可以有效区分机器与人类生成文本。"}}
{"id": "2507.00373", "pdf": "https://arxiv.org/pdf/2507.00373", "abs": "https://arxiv.org/abs/2507.00373", "authors": ["Ian Jin", "Fanxin Xia", "Feng Ding", "Xinfeng Zhang", "Meiqin Liu", "Yao Zhao", "Weisi Lin", "Lili Meng"], "title": "Customizable ROI-Based Deep Image Compression", "categories": ["cs.CV", "eess.IV"], "comment": null, "summary": "Region of Interest (ROI)-based image compression optimizes bit allocation by\nprioritizing ROI for higher-quality reconstruction. However, as the users\n(including human clients and downstream machine tasks) become more diverse,\nROI-based image compression needs to be customizable to support various\npreferences. For example, different users may define distinct ROI or require\ndifferent quality trade-offs between ROI and non-ROI. Existing ROI-based image\ncompression schemes predefine the ROI, making it unchangeable, and lack\neffective mechanisms to balance reconstruction quality between ROI and non-ROI.\nThis work proposes a paradigm for customizable ROI-based deep image\ncompression. First, we develop a Text-controlled Mask Acquisition (TMA) module,\nwhich allows users to easily customize their ROI for compression by just\ninputting the corresponding semantic \\emph{text}. It makes the encoder\ncontrolled by text. Second, we design a Customizable Value Assign (CVA)\nmechanism, which masks the non-ROI with a changeable extent decided by users\ninstead of a constant one to manage the reconstruction quality trade-off\nbetween ROI and non-ROI. Finally, we present a Latent Mask Attention (LMA)\nmodule, where the latent spatial prior of the mask and the latent\nRate-Distortion Optimization (RDO) prior of the image are extracted and fused\nin the latent space, and further used to optimize the latent representation of\nthe source image. Experimental results demonstrate that our proposed\ncustomizable ROI-based deep image compression paradigm effectively addresses\nthe needs of customization for ROI definition and mask acquisition as well as\nthe reconstruction quality trade-off management between the ROI and non-ROI.", "AI": {"tldr": "提出了一种可定制的基于ROI的深度图像压缩方法，支持用户通过文本定义ROI并调整ROI与非ROI的质量权衡。", "motivation": "现有ROI图像压缩方案固定ROI定义且缺乏质量权衡机制，无法满足多样化用户需求。", "method": "开发了文本控制的掩膜获取模块（TMA）、可自定义值分配机制（CVA）和潜在掩膜注意力模块（LMA）。", "result": "实验证明该方法有效支持ROI定制和质量权衡管理。", "conclusion": "该方法成功解决了ROI定义和压缩质量权衡的定制需求。"}}
{"id": "2507.00029", "pdf": "https://arxiv.org/pdf/2507.00029", "abs": "https://arxiv.org/abs/2507.00029", "authors": ["Wenbing Li", "Zikai Song", "Hang Zhou", "Yunyao Zhang", "Junqing Yu", "Wei Yang"], "title": "LoRA-Mixer: Coordinate Modular LoRA Experts Through Serial Attention Routing", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent efforts to combine low-rank adaptation (LoRA) with mixture-of-experts\n(MoE) for adapting large language models (LLMs) to multiple tasks still exhibit\nprevailing limitations: they either swap entire attention/feed-forward layers\nfor switch experts or bolt on parallel expert branches, diluting parameter\nefficiency and task fidelity. We propose the LoRA-Mixer, a modular and\nlightweight MoE framework that integrates LoRA experts. Our core innovation\nlies in replacing the projection matrices of the attention module's\ninput/output linear layers with dynamically routed, task-specific LoRA experts.\nThis design ensures seamless compatibility with diverse foundation models,\nincluding transformers and state space models (SSMs), by leveraging their\ninherent linear projection structures. The framework supports two operational\nparadigms: (1) joint optimization of LoRA experts and routing mechanisms via a\nnovel hard-soft routing strategy, or (2) direct deployment of pre-trained,\nfrozen LoRA modules sourced from external repositories. To enable robust router\ntraining with limited data while ensuring stable routing decisions and\nmaximizing expert reuse, we introduce an adaptive Specialization Balance Loss\n(SBL) that jointly optimizes expert balance and task-specific alignment.\nExtensive experiments on seven benchmark datasets, including MedQA, CoLA,\nSST-2, GSM8K, ARC-E, ARC-C, and HumanEval, demonstrate the effectiveness of\nLoRA-Mixer. On datasets such as GSM8K, HumanEval, and MedQA, LoRA-Mixer\nachieves significant improvements of 7.61%, 4.88%, and 3.08% over the base\nmodels, respectively. Compared with state-of-the-art methods, LoRA-Mixer\nachieves additional improvements of 1.09%, 1.45%, and 1.68%, respectively,\nusing only 48% of the parameters, demonstrating its efficiency and strong\nperformance.", "AI": {"tldr": "LoRA-Mixer是一种轻量级MoE框架，通过动态路由任务特定的LoRA专家提升参数效率和任务保真度，在多个基准数据集上表现优异。", "motivation": "现有方法在结合LoRA和MoE时存在参数效率低和任务保真度不足的问题，需要一种更高效的解决方案。", "method": "提出LoRA-Mixer框架，动态替换注意力模块的输入/输出线性层为任务特定的LoRA专家，支持联合优化或直接部署预训练模块。", "result": "在多个数据集上显著优于基线模型和现有方法，如GSM8K提升7.61%，且仅使用48%的参数。", "conclusion": "LoRA-Mixer在参数效率和性能上均表现出色，适用于多种基础模型。"}}
{"id": "2507.00875", "pdf": "https://arxiv.org/pdf/2507.00875", "abs": "https://arxiv.org/abs/2507.00875", "authors": ["Xi Xuan", "King-kui Sin", "Yufei Zhou", "Chunyu Kit"], "title": "TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation", "categories": ["cs.CL", "cs.HC", "cs.MA"], "comment": "arXiv admin note: text overlap with arXiv:2501.09444; text overlap\n  with arXiv:2409.20288 by other authors", "summary": "Multi-agent systems empowered by large language models (LLMs) have\ndemonstrated remarkable capabilities in a wide range of downstream\napplications, including machine translation. However, the potential of LLMs in\ntranslating Hong Kong legal judgments remains uncertain due to challenges such\nas intricate legal terminology, culturally embedded nuances, and strict\nlinguistic structures. In this work, we introduce TransLaw, a novel multi-agent\nframework implemented for real-world Hong Kong case law translation. It employs\nthree specialized agents, namely, Translator, Annotator, and Proofreader, to\ncollaboratively produce translations for high accuracy in legal meaning,\nappropriateness in style, and adequate coherence and cohesion in structure.\nThis framework supports customizable LLM configurations and achieves tremendous\ncost reduction compared to professional human translation services. We\nevaluated its performance using 13 open-source and commercial LLMs as agents\nand obtained interesting findings, including that it surpasses GPT-4o in legal\nsemantic accuracy, structural coherence, and stylistic fidelity, yet trails\nhuman experts in contextualizing complex terminology and stylistic naturalness.\nOur platform website is available at CityUHK, and our bilingual judgment corpus\nused for the evaluation is available at Hugging Face.", "AI": {"tldr": "TransLaw是一个基于多智能体框架的香港法律判决翻译系统，通过三个专业代理（翻译、注释、校对）协作，显著提升了翻译的准确性、风格适应性和结构连贯性，同时降低了成本。", "motivation": "探索大语言模型（LLMs）在翻译香港法律判决中的潜力，解决法律术语复杂、文化差异和严格语言结构等挑战。", "method": "采用多智能体框架TransLaw，包含翻译、注释和校对三个代理，支持可定制的LLM配置。", "result": "TransLaw在语义准确性、结构连贯性和风格保真度上超越GPT-4o，但在复杂术语和风格自然度上仍不及人类专家。", "conclusion": "TransLaw为法律翻译提供了一种高效且经济的解决方案，但仍需在术语和风格上进一步优化。"}}
{"id": "2507.00377", "pdf": "https://arxiv.org/pdf/2507.00377", "abs": "https://arxiv.org/abs/2507.00377", "authors": ["Jianhao Xie", "Ziang Zhang", "Zhenyu Weng", "Yuesheng Zhu", "Guibo Luo"], "title": "MedDiff-FT: Data-Efficient Diffusion Model Fine-tuning with Structural Guidance for Controllable Medical Image Synthesis", "categories": ["cs.CV"], "comment": "11 pages,3 figures", "summary": "Recent advancements in deep learning for medical image segmentation are often\nlimited by the scarcity of high-quality training data.While diffusion models\nprovide a potential solution by generating synthetic images, their\neffectiveness in medical imaging remains constrained due to their reliance on\nlarge-scale medical datasets and the need for higher image quality. To address\nthese challenges, we present MedDiff-FT, a controllable medical image\ngeneration method that fine-tunes a diffusion foundation model to produce\nmedical images with structural dependency and domain specificity in a\ndata-efficient manner. During inference, a dynamic adaptive guiding mask\nenforces spatial constraints to ensure anatomically coherent synthesis, while a\nlightweight stochastic mask generator enhances diversity through hierarchical\nrandomness injection. Additionally, an automated quality assessment protocol\nfilters suboptimal outputs using feature-space metrics, followed by mask\ncorrosion to refine fidelity. Evaluated on five medical segmentation\ndatasets,MedDiff-FT's synthetic image-mask pairs improve SOTA method's\nsegmentation performance by an average of 1% in Dice score. The framework\neffectively balances generation quality, diversity, and computational\nefficiency, offering a practical solution for medical data augmentation. The\ncode is available at https://github.com/JianhaoXie1/MedDiff-FT.", "AI": {"tldr": "MedDiff-FT是一种可控的医学图像生成方法，通过微调扩散基础模型，以数据高效的方式生成具有结构依赖性和领域特异性的医学图像，提升分割性能。", "motivation": "解决医学图像分割中高质量训练数据稀缺以及扩散模型在医学成像中依赖大规模数据和高质量图像的问题。", "method": "提出MedDiff-FT方法，包括动态自适应引导掩码、轻量级随机掩码生成器和自动质量评估协议，以确保解剖学一致性和多样性。", "result": "在五个医学分割数据集上，MedDiff-FT的合成图像-掩码对将SOTA方法的分割性能平均提高了1%的Dice分数。", "conclusion": "MedDiff-FT在生成质量、多样性和计算效率之间取得了平衡，为医学数据增强提供了实用解决方案。"}}
{"id": "2507.00030", "pdf": "https://arxiv.org/pdf/2507.00030", "abs": "https://arxiv.org/abs/2507.00030", "authors": ["Abhishek Verma", "Nallarasan V", "Balaraman Ravindran"], "title": "Adaptive Action Duration with Contextual Bandits for Deep Reinforcement Learning in Dynamic Environments", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Deep Reinforcement Learning (DRL) has achieved remarkable success in complex\nsequential decision-making tasks, such as playing Atari 2600 games and\nmastering board games. A critical yet underexplored aspect of DRL is the\ntemporal scale of action execution. We propose a novel paradigm that integrates\ncontextual bandits with DRL to adaptively select action durations, enhancing\npolicy flexibility and computational efficiency. Our approach augments a Deep\nQ-Network (DQN) with a contextual bandit module that learns to choose optimal\naction repetition rates based on state contexts. Experiments on Atari 2600\ngames demonstrate significant performance improvements over static duration\nbaselines, highlighting the efficacy of adaptive temporal abstractions in DRL.\nThis paradigm offers a scalable solution for real-time applications like gaming\nand robotics, where dynamic action durations are critical.", "AI": {"tldr": "提出了一种结合上下文赌博机与深度强化学习的新方法，通过自适应选择动作持续时间提升策略灵活性和计算效率。", "motivation": "深度强化学习在复杂序列决策任务中表现优异，但动作执行的时间尺度问题尚未充分探索。", "method": "在深度Q网络中引入上下文赌博机模块，学习基于状态上下文选择最优动作重复频率。", "result": "在Atari 2600游戏中，相比静态持续时间基线，性能显著提升。", "conclusion": "该方法为实时应用（如游戏和机器人）提供了可扩展的动态动作持续时间解决方案。"}}
{"id": "2507.00883", "pdf": "https://arxiv.org/pdf/2507.00883", "abs": "https://arxiv.org/abs/2507.00883", "authors": ["Aditya Tomar", "Nihar Ranjan Sahoo", "Ashish Mittal", "Rudra Murthy", "Pushpak Bhattacharyya"], "title": "Mathematics Isn't Culture-Free: Probing Cultural Gaps via Entity and Scenario Perturbations", "categories": ["cs.CL"], "comment": null, "summary": "Although mathematics is often considered culturally neutral, the way\nmathematical problems are presented can carry implicit cultural context.\nExisting benchmarks like GSM8K are predominantly rooted in Western norms,\nincluding names, currencies, and everyday scenarios. In this work, we create\nculturally adapted variants of the GSM8K test set for five regions Africa,\nIndia, China, Korea, and Japan using prompt-based transformations followed by\nmanual verification. We evaluate six large language models (LLMs), ranging from\n8B to 72B parameters, across five prompting strategies to assess their\nrobustness to cultural variation in math problem presentation. Our findings\nreveal a consistent performance gap: models perform best on the original\nUS-centric dataset and comparatively worse on culturally adapted versions.\nHowever, models with reasoning capabilities are more resilient to these shifts,\nsuggesting that deeper reasoning helps bridge cultural presentation gaps in\nmathematical tasks", "AI": {"tldr": "论文探讨了数学问题呈现方式中的文化偏见，通过创建适应不同文化的GSM8K测试集，评估大型语言模型在文化差异下的表现。", "motivation": "数学常被视为文化中立，但问题呈现方式隐含文化背景。现有基准如GSM8K以西方文化为主，本研究旨在填补这一空白。", "method": "通过提示转换和人工验证，为非洲、印度、中国、韩国和日本创建文化适应版本的GSM8K测试集，并评估六种LLM在五种提示策略下的表现。", "result": "模型在美国中心数据集上表现最佳，文化适应版本表现较差；具备推理能力的模型对文化差异更具韧性。", "conclusion": "深层推理能力有助于弥合数学任务中的文化呈现差异。"}}
{"id": "2507.00392", "pdf": "https://arxiv.org/pdf/2507.00392", "abs": "https://arxiv.org/abs/2507.00392", "authors": ["Yingping Liang", "Yutao Hu", "Wenqi Shao", "Ying Fu"], "title": "Learning Dense Feature Matching via Lifting Single 2D Image to 3D Space", "categories": ["cs.CV"], "comment": null, "summary": "Feature matching plays a fundamental role in many computer vision tasks, yet\nexisting methods heavily rely on scarce and clean multi-view image collections,\nwhich constrains their generalization to diverse and challenging scenarios.\nMoreover, conventional feature encoders are typically trained on single-view 2D\nimages, limiting their capacity to capture 3D-aware correspondences. In this\npaper, we propose a novel two-stage framework that lifts 2D images to 3D space,\nnamed as \\textbf{Lift to Match (L2M)}, taking full advantage of large-scale and\ndiverse single-view images. To be specific, in the first stage, we learn a\n3D-aware feature encoder using a combination of multi-view image synthesis and\n3D feature Gaussian representation, which injects 3D geometry knowledge into\nthe encoder. In the second stage, a novel-view rendering strategy, combined\nwith large-scale synthetic data generation from single-view images, is employed\nto learn a feature decoder for robust feature matching, thus achieving\ngeneralization across diverse domains. Extensive experiments demonstrate that\nour method achieves superior generalization across zero-shot evaluation\nbenchmarks, highlighting the effectiveness of the proposed framework for robust\nfeature matching.", "AI": {"tldr": "提出了一种名为Lift to Match（L2M）的两阶段框架，通过将2D图像提升到3D空间，利用大规模单视图图像提升特征匹配的泛化能力。", "motivation": "现有特征匹配方法依赖稀缺且干净的多视图图像，限制了其在多样化场景中的泛化能力。传统特征编码器基于单视图2D图像训练，难以捕捉3D感知的对应关系。", "method": "第一阶段通过多视图图像合成和3D特征高斯表示学习3D感知特征编码器；第二阶段采用新视图渲染策略和大规模合成数据学习特征解码器，实现跨域泛化。", "result": "实验表明，该方法在零样本评估基准上表现出卓越的泛化能力。", "conclusion": "L2M框架通过3D感知特征编码和合成数据训练，显著提升了特征匹配的鲁棒性和泛化能力。"}}
{"id": "2507.00032", "pdf": "https://arxiv.org/pdf/2507.00032", "abs": "https://arxiv.org/abs/2507.00032", "authors": ["Grey Kuling", "Marinka Zitnik"], "title": "Ken Utilization Layer: Hebbian Replay Within a Student's Ken for Adaptive Knowledge Tracing", "categories": ["cs.CY", "cs.AI", "cs.LG", "cs.NE"], "comment": null, "summary": "We introduce KUL-KT, a biologically inspired architecture for knowledge\ntracing (KT), combining Hebbian memory encoding with gradient-based\nconsolidation in a scalable, input-agnostic framework. KUL-KT adapts the\nprinciple of memory consolidation in neural systems, to student modeling by\nintroducing two key innovations: (i) a time-decaying Hebbian memory update that\nenables graceful forgetting, and (ii) a novel Loss-aligned Internal Target\n(LIT) method to compute an ideal internal state, allowing continual learning\nwithout backpropagation through time. The architecture consists of a fast\nHebbian memory that captures each learner interaction via a single associative\nupdate, and a slower linear network that consolidates recalled samples through\ngradient descent. This design enables few-shot personalization and natural\nforgetting without storing raw data or relying on large cohort training.\nOperating entirely in embedding space, KUL-KT supports both structured\n(tabular) and unstructured (short-answer) inputs. Empirically, KUL-KT\noutperforms strong baselines on ten public KT benchmarks in rank-sensitive\nmetrics such as nDCG and Recall@10. In a classroom deployment, KUL-KT\npersonalized quizzes from short-answer data, leading to improved\nlearner-perceived helpfulness and reduced difficulty (p < 0.05). Ablation\nstudies confirm that Hebbian decay and LIT are critical for continual\nadaptation. Compared to a strong graph-based KT model, KUL-KT trains 1.75x\nfaster and uses 99.01\\% less memory. These results position KUL-KT as a\nbiologically grounded, memory-efficient, and input-flexible framework for\npersonalized learning at scale.", "AI": {"tldr": "KUL-KT是一种受生物启发的知识追踪架构，结合了Hebbian记忆编码和基于梯度的巩固方法，具有可扩展性和输入无关性。", "motivation": "通过模拟神经系统的记忆巩固机制，改进学生建模，实现持续学习和自然遗忘。", "method": "采用时间衰减的Hebbian记忆更新和Loss-aligned Internal Target (LIT)方法，结合快速Hebbian记忆和慢速线性网络。", "result": "在十个公开KT基准测试中表现优异，训练速度更快且内存占用更少，实际课堂部署中显著提升学习效果。", "conclusion": "KUL-KT是一种高效、灵活且可扩展的个性化学习框架。"}}
{"id": "2507.00885", "pdf": "https://arxiv.org/pdf/2507.00885", "abs": "https://arxiv.org/abs/2507.00885", "authors": ["Nicholas Lourie", "Michael Y. Hu", "Kyunghyun Cho"], "title": "Scaling Laws Are Unreliable for Downstream Tasks: A Reality Check", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Downstream scaling laws aim to predict task performance at larger scales from\npretraining losses at smaller scales. Whether this prediction should be\npossible is unclear: some works demonstrate that task performance follows clear\nlinear scaling trends under transformation, whereas others point out\nfundamental challenges to downstream scaling laws, such as emergence and\ninverse scaling. In this work, we conduct a meta-analysis of existing data on\ndownstream scaling laws, finding that close fit to linear scaling laws only\noccurs in a minority of cases: 39% of the time. Furthermore, seemingly benign\nchanges to the experimental setting can completely change the scaling trend.\nOur analysis underscores the need to understand the conditions under which\nscaling laws succeed. To fully model the relationship between pretraining loss\nand downstream task performance, we must embrace the cases in which scaling\nbehavior deviates from linear trends.", "AI": {"tldr": "下游扩展定律试图从小规模预训练损失预测大规模任务表现，但研究发现仅39%的情况下符合线性扩展趋势，且实验设置的微小变化可能完全改变扩展趋势。", "motivation": "探讨下游扩展定律是否能够有效预测任务表现，以及其适用性和局限性。", "method": "对现有下游扩展定律数据进行元分析，评估线性扩展趋势的普遍性和实验设置的影响。", "result": "仅39%的情况下符合线性扩展趋势，实验设置的微小变化可能显著改变扩展行为。", "conclusion": "需进一步理解扩展定律成功的条件，并关注偏离线性趋势的情况，以全面建模预训练损失与下游任务表现的关系。"}}
{"id": "2507.00401", "pdf": "https://arxiv.org/pdf/2507.00401", "abs": "https://arxiv.org/abs/2507.00401", "authors": ["Xin Xu", "Eibe Frank", "Geoffrey Holmes"], "title": "Few-shot Classification as Multi-instance Verification: Effective Backbone-agnostic Transfer across Domains", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "We investigate cross-domain few-shot learning under the constraint that\nfine-tuning of backbones (i.e., feature extractors) is impossible or infeasible\n-- a scenario that is increasingly common in practical use cases. Handling the\nlow-quality and static embeddings produced by frozen, \"black-box\" backbones\nleads to a problem representation of few-shot classification as a series of\nmultiple instance verification (MIV) tasks. Inspired by this representation, we\nintroduce a novel approach to few-shot domain adaptation, named the \"MIV-head\",\nakin to a classification head that is agnostic to any pretrained backbone and\ncomputationally efficient. The core components designed for the MIV-head, when\ntrained on few-shot data from a target domain, collectively yield strong\nperformance on test data from that domain. Importantly, it does so without\nfine-tuning the backbone, and within the \"meta-testing\" phase. Experimenting\nunder various settings and on an extension of the Meta-dataset benchmark for\ncross-domain few-shot image classification, using representative off-the-shelf\nconvolutional neural network and vision transformer backbones pretrained on\nImageNet1K, we show that the MIV-head achieves highly competitive accuracy when\ncompared to state-of-the-art \"adapter\" (or partially fine-tuning) methods\napplied to the same backbones, while incurring substantially lower adaptation\ncost. We also find well-known \"classification head\" approaches lag far behind\nin terms of accuracy. Ablation study empirically justifies the core components\nof our approach. We share our code at https://github.com/xxweka/MIV-head.", "AI": {"tldr": "论文提出了一种名为MIV-head的新方法，用于跨域少样本学习，无需微调预训练的特征提取器，性能优于现有方法。", "motivation": "研究在无法或不可行微调特征提取器的场景下，如何解决跨域少样本学习问题。", "method": "将少样本分类问题表示为多实例验证任务，并设计了一种与预训练主干无关的MIV-head方法。", "result": "MIV-head在多个设置和基准测试中表现出色，性能优于部分微调方法，且适应成本更低。", "conclusion": "MIV-head是一种高效且性能优越的跨域少样本学习方法，无需微调主干网络。"}}
{"id": "2507.00891", "pdf": "https://arxiv.org/pdf/2507.00891", "abs": "https://arxiv.org/abs/2507.00891", "authors": ["Yuheng Wang", "Xianhe Tang", "Pufeng Huang"], "title": "MemeCMD: An Automatically Generated Chinese Multi-turn Dialogue Dataset with Contextually Retrieved Memes", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Memes are widely used in online social interactions, providing vivid,\nintuitive, and often humorous means to express intentions and emotions.\nExisting dialogue datasets are predominantly limited to either manually\nannotated or pure-text conversations, lacking the expressiveness and contextual\nnuance that multimodal interactions provide.To address these challenges, we\nintroduce MemeCMD, an automatically generated Chinese Multi-turn Dialogue\ndataset with contextually retrieved memes. Our dataset combines a large-scale,\nMLLM-annotated meme library with dialogues auto-generated by dual agents across\ndiverse scenarios. We introduce a retrieval framework and adaptive threshold to\nensure contextually relevant, naturally spaced meme usage. Experiments\ndemonstrate the effectiveness of our approach in generating contextually\nappropriate and diverse meme-incorporated dialogues, offering a scalable and\nprivacy-preserving resource for advancing multimodal conversational AI.", "AI": {"tldr": "介绍了MemeCMD，一个自动生成的中文多轮对话数据集，结合了上下文检索的表情包，用于提升多模态对话AI的表达力。", "motivation": "现有对话数据集多为纯文本或手动标注，缺乏多模态交互的表达力和上下文细微差别。", "method": "结合大规模MLLM标注的表情包库和双代理自动生成的对话，引入检索框架和自适应阈值确保表情包的上下文相关性和自然分布。", "result": "实验证明该方法能生成上下文恰当且多样化的表情包对话，为多模态对话AI提供了可扩展且隐私保护的资源。", "conclusion": "MemeCMD为多模态对话AI提供了一个有效的解决方案，填补了现有数据集的不足。"}}
{"id": "2507.00429", "pdf": "https://arxiv.org/pdf/2507.00429", "abs": "https://arxiv.org/abs/2507.00429", "authors": ["Jingyi Pan", "Dan Xu", "Qiong Luo"], "title": "DiGA3D: Coarse-to-Fine Diffusional Propagation of Geometry and Appearance for Versatile 3D Inpainting", "categories": ["cs.CV"], "comment": "ICCV 2025, Project page: https://rorisis.github.io/DiGA3D/", "summary": "Developing a unified pipeline that enables users to remove, re-texture, or\nreplace objects in a versatile manner is crucial for text-guided 3D inpainting.\nHowever, there are still challenges in performing multiple 3D inpainting tasks\nwithin a unified framework: 1) Single reference inpainting methods lack\nrobustness when dealing with views that are far from the reference view. 2)\nAppearance inconsistency arises when independently inpainting multi-view images\nwith 2D diffusion priors; 3) Geometry inconsistency limits performance when\nthere are significant geometric changes in the inpainting regions. To tackle\nthese challenges, we introduce DiGA3D, a novel and versatile 3D inpainting\npipeline that leverages diffusion models to propagate consistent appearance and\ngeometry in a coarse-to-fine manner. First, DiGA3D develops a robust strategy\nfor selecting multiple reference views to reduce errors during propagation.\nNext, DiGA3D designs an Attention Feature Propagation (AFP) mechanism that\npropagates attention features from the selected reference views to other views\nvia diffusion models to maintain appearance consistency. Furthermore, DiGA3D\nintroduces a Texture-Geometry Score Distillation Sampling (TG-SDS) loss to\nfurther improve the geometric consistency of inpainted 3D scenes. Extensive\nexperiments on multiple 3D inpainting tasks demonstrate the effectiveness of\nour method. The project page is available at https://rorisis.github.io/DiGA3D/.", "AI": {"tldr": "DiGA3D是一个统一的3D修复管道，通过扩散模型实现多任务修复，解决了视图不一致、外观和几何不一致的问题。", "motivation": "解决3D修复中单视图参考不鲁棒、多视图外观不一致以及几何变化导致的性能限制问题。", "method": "采用多参考视图选择策略、注意力特征传播机制（AFP）和纹理-几何评分蒸馏采样（TG-SDS）损失。", "result": "实验证明DiGA3D在多种3D修复任务中表现优异。", "conclusion": "DiGA3D通过扩散模型实现了外观和几何一致性的高效修复。"}}
{"id": "2507.00037", "pdf": "https://arxiv.org/pdf/2507.00037", "abs": "https://arxiv.org/abs/2507.00037", "authors": ["Phoomraphee Luenam", "Andreas Spanopoulos", "Amit Sant", "Thomas Hofmann", "Sotiris Anagnostidis", "Sidak Pal Singh"], "title": "Model Fusion via Neuron Interpolation", "categories": ["cs.LG", "cs.AI", "I.2.6; I.2.1"], "comment": "5 figures, 15 tables, 23 pages", "summary": "Model fusion aims to combine the knowledge of multiple models by creating one\nrepresentative model that captures the strengths of all of its parents.\nHowever, this process is non-trivial due to differences in internal\nrepresentations, which can stem from permutation invariance, random\ninitialization, or differently distributed training data. We present a novel,\nneuron-centric family of model fusion algorithms designed to integrate multiple\ntrained neural networks into a single network effectively regardless of\ntraining data distribution. Our algorithms group intermediate neurons of parent\nmodels to create target representations that the fused model approximates with\nits corresponding sub-network. Unlike prior approaches, our approach\nincorporates neuron attribution scores into the fusion process. Furthermore,\nour algorithms can generalize to arbitrary layer types. Experimental results on\nvarious benchmark datasets demonstrate that our algorithms consistently\noutperform previous fusion techniques, particularly in zero-shot and non-IID\nfusion scenarios. The code is available at\nhttps://github.com/AndrewSpano/neuron-interpolation-model-fusion.", "AI": {"tldr": "提出了一种基于神经元中心的新型模型融合算法，通过分组中间神经元并利用神经元属性评分，有效整合多个训练好的神经网络。", "motivation": "解决模型融合中因内部表示差异（如排列不变性、随机初始化或不同分布的训练数据）导致的困难。", "method": "设计神经元中心融合算法，分组父模型的中间神经元，创建目标表示，并通过子网络近似。引入神经元属性评分。", "result": "在多个基准数据集上表现优于现有融合技术，尤其在零样本和非独立同分布场景中。", "conclusion": "提出的算法能有效整合不同模型，提升融合效果，代码已开源。"}}
{"id": "2507.00911", "pdf": "https://arxiv.org/pdf/2507.00911", "abs": "https://arxiv.org/abs/2507.00911", "authors": ["Luise Häuser", "Alexandros Stamatakis"], "title": "The Cognate Data Bottleneck in Language Phylogenetics", "categories": ["cs.CL", "q-bio.PE"], "comment": null, "summary": "To fully exploit the potential of computational phylogenetic methods for\ncognate data one needs to leverage specific (complex) models an machine\nlearning-based techniques. However, both approaches require datasets that are\nsubstantially larger than the manually collected cognate data currently\navailable. To the best of our knowledge, there exists no feasible approach to\nautomatically generate larger cognate datasets. We substantiate this claim by\nautomatically extracting datasets from BabelNet, a large multilingual\nencyclopedic dictionary. We demonstrate that phylogenetic inferences on the\nrespective character matrices yield trees that are largely inconsistent with\nthe established gold standard ground truth trees. We also discuss why we\nconsider it as being unlikely to be able to extract more suitable character\nmatrices from other multilingual resources. Phylogenetic data analysis\napproaches that require larger datasets can therefore not be applied to cognate\ndata. Thus, it remains an open question how, and if these computational\napproaches can be applied in historical linguistics.", "AI": {"tldr": "论文探讨了利用计算系统发育方法处理同源词数据的挑战，指出当前缺乏自动生成大规模同源词数据的方法，并验证了从多语言资源中提取的数据在系统发育推断中的不一致性。", "motivation": "研究旨在解决计算系统发育方法在同源词数据应用中的局限性，特别是数据规模不足的问题。", "method": "通过从BabelNet（多语言百科全书词典）自动提取数据集，生成字符矩阵，并进行系统发育推断。", "result": "推断出的系统发育树与已知的金标准树不一致，且从其他多语言资源提取更合适数据的可能性较低。", "conclusion": "当前计算系统发育方法难以直接应用于同源词数据，其应用前景仍存疑。"}}
{"id": "2507.00430", "pdf": "https://arxiv.org/pdf/2507.00430", "abs": "https://arxiv.org/abs/2507.00430", "authors": ["Huanxin Yang", "Qiwen Wang"], "title": "MFH: Marrying Frequency Domain with Handwritten Mathematical Expression Recognition", "categories": ["cs.CV"], "comment": null, "summary": "Handwritten mathematical expression recognition (HMER) suffers from complex\nformula structures and character layouts in sequence prediction. In this paper,\nwe incorporate frequency domain analysis into HMER and propose a method that\nmarries frequency domain with HMER (MFH), leveraging the discrete cosine\ntransform (DCT). We emphasize the structural analysis assistance of frequency\ninformation for recognizing mathematical formulas. When implemented on various\nbaseline models, our network exhibits a consistent performance enhancement,\ndemonstrating the efficacy of frequency domain information. Experiments show\nthat our MFH-CoMER achieves noteworthy accuracyrates of 61.66%/62.07%/63.72% on\nthe CROHME 2014/2016/2019 test sets. The source code is available at\nhttps://github.com/Hryxyhe/MFH.", "AI": {"tldr": "论文提出了一种结合频域分析的手写数学表达式识别方法（MFH），利用离散余弦变换（DCT）提升识别性能。", "motivation": "手写数学表达式识别（HMER）因复杂的公式结构和字符布局而面临序列预测的挑战，频域信息可能提供结构分析的辅助。", "method": "提出MFH方法，将频域分析与HMER结合，利用DCT提取频域信息，增强模型对公式结构的理解。", "result": "在多个基准模型上实现性能提升，MFH-CoMER在CROHME 2014/2016/2019测试集上分别达到61.66%/62.07%/63.72%的准确率。", "conclusion": "频域信息对HMER具有显著帮助，MFH方法有效提升了识别性能。"}}
{"id": "2507.00038", "pdf": "https://arxiv.org/pdf/2507.00038", "abs": "https://arxiv.org/abs/2507.00038", "authors": ["Fei Chen", "Wenchi Zhou"], "title": "Quality over Quantity: An Effective Large-Scale Data Reduction Strategy Based on Pointwise V-Information", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Data reduction plays a vital role in data-centric AI by identifying the most\ninformative instance within large-scale datasets to enhance model training\nefficiency. The core challenge lies in how to select the optimal\ninstances-rather than the entire datasets-to improve data quality and training\nefficiency. In this paper, we propose an effective data reduction strategy\nbased on Pointwise V-information(PVI). First, we quantify instance difficulty\nusing PVI and filter out low-difficulty instances enabling a static approach.\nExperiments demonstrate that removing 10%-30% of the data preserves the\nclassifier performance with only a 0.0001% to 0.76% loss in accuracy.Second, we\nuse a progressive learning approach to training the classifiers on instances\nsorted by ascending PVI, accelerating convergence and achieving a 0.8% accuracy\ngain over conventional training. Our results suggest that with the effective\ndata reduction strategy, training a classifier on the selected optimal subset\ncould enhance the model performance and boost training efficiency. Moreover, we\nhave transferred the PVI framework, which previously applied only to English\ndatasets, to diverse Chinese NLP tasks and base models, leading to valuable\ninsights for cross-lingual data reduction and faster training. The codes are\nreleased at https://github.com/zhouwenchi/DatasetReductionStrategy.", "AI": {"tldr": "论文提出了一种基于点状V信息（PVI）的数据缩减策略，通过筛选高难度实例提升模型训练效率和性能。", "motivation": "解决在大规模数据集中选择最优实例而非全部数据以提高数据质量和训练效率的核心挑战。", "method": "使用PVI量化实例难度，静态过滤低难度实例，并采用渐进学习法按PVI升序训练分类器。", "result": "实验显示，移除10%-30%数据仅损失0.0001%-0.76%准确率，渐进学习法提升0.8%准确率。", "conclusion": "PVI策略能有效提升模型性能和训练效率，且成功应用于中文NLP任务，实现跨语言数据缩减。"}}
{"id": "2507.00985", "pdf": "https://arxiv.org/pdf/2507.00985", "abs": "https://arxiv.org/abs/2507.00985", "authors": ["Guangliang Liu", "Zimo Qi", "Xitong Zhang", "Kristen Marie Johnson"], "title": "Discourse Heuristics For Paradoxically Moral Self-Correction", "categories": ["cs.CL"], "comment": null, "summary": "Moral self-correction has emerged as a promising approach for aligning the\noutput of Large Language Models (LLMs) with human moral values. However, moral\nself-correction techniques are subject to two primary paradoxes. First, despite\nempirical and theoretical evidence to support the effectiveness of\nself-correction, this LLM capability only operates at a superficial level.\nSecond, while LLMs possess the capability of self-diagnosing immoral aspects of\ntheir output, they struggle to identify the cause of this moral inconsistency\nduring their self-correction process. To better understand and address these\nparadoxes, we analyze the discourse constructions in fine-tuning corpora\ndesigned to enhance moral self-correction, uncovering the existence of the\nheuristics underlying effective constructions. We demonstrate that moral\nself-correction relies on discourse constructions that reflect heuristic\nshortcuts, and that the presence of these heuristic shortcuts during\nself-correction leads to inconsistency when attempting to enhance both\nself-correction and self-diagnosis capabilities jointly. Based on our findings,\nwe propose a solution to improve moral self-correction by leveraging the\nheuristics of curated datasets. We also highlight the generalization challenges\nof this capability, particularly in terms of learning from situated context and\nmodel scales.", "AI": {"tldr": "论文探讨了大型语言模型（LLM）的道德自我修正能力，发现其存在表面性和诊断能力不足的悖论，并提出基于启发式数据集的改进方案。", "motivation": "研究旨在解决LLM道德自我修正中的两个主要悖论：表面性和诊断能力不足。", "method": "通过分析微调语料库中的话语结构，揭示启发式捷径的存在及其影响。", "result": "发现道德自我修正依赖启发式捷径，导致自我修正与自我诊断能力提升不一致。", "conclusion": "提出利用精选数据集的启发式改进道德自我修正，并指出其在情境学习和模型规模上的泛化挑战。"}}
{"id": "2507.00447", "pdf": "https://arxiv.org/pdf/2507.00447", "abs": "https://arxiv.org/abs/2507.00447", "authors": ["Xin Luo", "Menglin Zhang", "Yunwei Lan", "Tianyu Zhang", "Rui Li", "Chang Liu", "Dong Liu"], "title": "Latent Posterior-Mean Rectified Flow for Higher-Fidelity Perceptual Face Restoration", "categories": ["cs.CV", "eess.IV"], "comment": "Code and Models will be publicly available at\n  https://github.com/Luciennnnnnn/Latent-PMRF", "summary": "The Perception-Distortion tradeoff (PD-tradeoff) theory suggests that face\nrestoration algorithms must balance perceptual quality and fidelity. To achieve\nminimal distortion while maintaining perfect perceptual quality, Posterior-Mean\nRectified Flow (PMRF) proposes a flow based approach where source distribution\nis minimum distortion estimations. Although PMRF is shown to be effective, its\npixel-space modeling approach limits its ability to align with human\nperception, where human perception is defined as how humans distinguish between\ntwo image distributions. In this work, we propose Latent-PMRF, which\nreformulates PMRF in the latent space of a variational autoencoder (VAE),\nfacilitating better alignment with human perception during optimization. By\ndefining the source distribution on latent representations of minimum\ndistortion estimation, we bound the minimum distortion by the VAE's\nreconstruction error. Moreover, we reveal the design of VAE is crucial, and our\nproposed VAE significantly outperforms existing VAEs in both reconstruction and\nrestoration. Extensive experiments on blind face restoration demonstrate the\nsuperiority of Latent-PMRF, offering an improved PD-tradeoff compared to\nexisting methods, along with remarkable convergence efficiency, achieving a\n5.79X speedup over PMRF in terms of FID. Our code will be available as\nopen-source.", "AI": {"tldr": "Latent-PMRF通过将PMRF方法重新定义在VAE的潜在空间中，优化了感知-失真平衡，显著提升了人脸恢复的性能和效率。", "motivation": "解决PMRF在像素空间中建模时与人类感知对齐不足的问题，提出潜在空间建模以更好地匹配人类感知。", "method": "在VAE的潜在空间中重新定义PMRF，利用潜在表示的最小失真估计源分布，并通过VAE的重构误差限制最小失真。", "result": "Latent-PMRF在盲人脸恢复任务中表现优异，实现了比现有方法更好的感知-失真平衡，且收敛效率显著提升（5.79倍加速）。", "conclusion": "Latent-PMRF通过潜在空间建模有效改善了感知-失真平衡，同时展示了VAE设计对性能的关键影响。"}}
{"id": "2507.00039", "pdf": "https://arxiv.org/pdf/2507.00039", "abs": "https://arxiv.org/abs/2507.00039", "authors": ["Lucas Potin", "Rosa Figueiredo", "Vincent Labatut", "Christine Largeron"], "title": "Pattern-Based Graph Classification: Comparison of Quality Measures and Importance of Preprocessing", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Graph classification aims to categorize graphs based on their structural and\nattribute features, with applications in diverse fields such as social network\nanalysis and bioinformatics. Among the methods proposed to solve this task,\nthose relying on patterns (i.e. subgraphs) provide good explainability, as the\npatterns used for classification can be directly interpreted. To identify\nmeaningful patterns, a standard approach is to use a quality measure, i.e. a\nfunction that evaluates the discriminative power of each pattern. However, the\nliterature provides tens of such measures, making it difficult to select the\nmost appropriate for a given application. Only a handful of surveys try to\nprovide some insight by comparing these measures, and none of them specifically\nfocuses on graphs. This typically results in the systematic use of the most\nwidespread measures, without thorough evaluation. To address this issue, we\npresent a comparative analysis of 38 quality measures from the literature. We\ncharacterize them theoretically, based on four mathematical properties. We\nleverage publicly available datasets to constitute a benchmark, and propose a\nmethod to elaborate a gold standard ranking of the patterns. We exploit these\nresources to perform an empirical comparison of the measures, both in terms of\npattern ranking and classification performance. Moreover, we propose a\nclustering-based preprocessing step, which groups patterns appearing in the\nsame graphs to enhance classification performance. Our experimental results\ndemonstrate the effectiveness of this step, reducing the number of patterns to\nbe processed while achieving comparable performance. Additionally, we show that\nsome popular measures widely used in the literature are not associated with the\nbest results.", "AI": {"tldr": "论文对38种质量度量进行了理论和实证比较，提出了一种基于聚类的预处理方法以提高分类性能，并发现一些常用度量效果不佳。", "motivation": "解决图分类中质量度量选择困难的问题，缺乏针对图数据的系统比较。", "method": "理论分析38种质量度量的数学特性，构建基准数据集和黄金标准排名，提出聚类预处理方法。", "result": "实验表明聚类预处理能减少模式数量并保持性能，某些流行度量效果不佳。", "conclusion": "研究为图分类中的质量度量选择提供了指导，并验证了预处理方法的有效性。"}}
{"id": "2507.00994", "pdf": "https://arxiv.org/pdf/2507.00994", "abs": "https://arxiv.org/abs/2507.00994", "authors": ["Hippolyte Gisserot-Boukhlef", "Nicolas Boizard", "Manuel Faysse", "Duarte M. Alves", "Emmanuel Malherbe", "André F. T. Martins", "Céline Hudelot", "Pierre Colombo"], "title": "Should We Still Pretrain Encoders with Masked Language Modeling?", "categories": ["cs.CL"], "comment": "23 pages, 10 figures, 17 tables", "summary": "Learning high-quality text representations is fundamental to a wide range of\nNLP tasks. While encoder pretraining has traditionally relied on Masked\nLanguage Modeling (MLM), recent evidence suggests that decoder models\npretrained with Causal Language Modeling (CLM) can be effectively repurposed as\nencoders, often surpassing traditional encoders on text representation\nbenchmarks. However, it remains unclear whether these gains reflect an inherent\nadvantage of the CLM objective or arise from confounding factors such as model\nand data scale. In this paper, we address this question through a series of\nlarge-scale, carefully controlled pretraining ablations, training a total of 30\nmodels ranging from 210 million to 1 billion parameters, and conducting over\n15,000 fine-tuning and evaluation runs. We find that while training with MLM\ngenerally yields better performance across text representation tasks,\nCLM-trained models are more data-efficient and demonstrate improved fine-tuning\nstability. Building on these findings, we experimentally show that a biphasic\ntraining strategy that sequentially applies CLM and then MLM, achieves optimal\nperformance under a fixed computational training budget. Moreover, we\ndemonstrate that this strategy becomes more appealing when initializing from\nreadily available pretrained CLM models (from the existing LLM ecosystem),\nreducing the computational burden needed to train best-in-class encoder models.\nWe release all project artifacts at https://hf.co/MLMvsCLM to foster further\nresearch.", "AI": {"tldr": "研究发现，虽然MLM在文本表示任务上表现更好，但CLM模型更高效且稳定性更强。结合CLM和MLM的双阶段训练策略在固定计算预算下表现最佳。", "motivation": "探讨CLM和MLM在文本表示任务中的优劣，明确其性能差异是否源于目标函数本身或其他因素。", "method": "通过大规模对照实验，训练30个不同规模的模型，并进行15,000多次微调和评估。", "result": "MLM在任务性能上更优，但CLM更高效且稳定；双阶段训练策略（CLM后接MLM）表现最佳。", "conclusion": "双阶段训练策略结合CLM和MLM的优势，尤其适合从现有LLM生态中初始化模型，减少计算负担。"}}
{"id": "2507.00454", "pdf": "https://arxiv.org/pdf/2507.00454", "abs": "https://arxiv.org/abs/2507.00454", "authors": ["Yihao Zhen", "Qiang Wang", "Yu Qiao", "Liangqiong Qu", "Huijie Fan"], "title": "ATSTrack: Enhancing Visual-Language Tracking by Aligning Temporal and Spatial Scales", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "A main challenge of Visual-Language Tracking (VLT) is the misalignment\nbetween visual inputs and language descriptions caused by target movement.\nPrevious trackers have explored many effective feature modification methods to\npreserve more aligned features. However, an important yet unexplored factor\nultimately hinders their capability, which is the inherent differences in the\ntemporal and spatial scale of information between visual and language inputs.\nTo address this issue, we propose a novel visual-language tracker that enhances\nthe effect of feature modification by \\textbf{A}ligning \\textbf{T}emporal and\n\\textbf{S}patial scale of different input components, named as\n\\textbf{ATSTrack}. Specifically, we decompose each language description into\nphrases with different attributes based on their temporal and spatial\ncorrespondence with visual inputs, and modify their features in a fine-grained\nmanner. Moreover, we introduce a Visual-Language token that comprises modified\nlinguistic information from the previous frame to guide the model to extract\nvisual features that are more relevant to language description, thereby\nreducing the impact caused by the differences in spatial scale. Experimental\nresults show that our proposed ATSTrack achieves performance comparable to\nexisting methods. Our code will be released.", "AI": {"tldr": "论文提出了一种名为ATSTrack的视觉语言跟踪方法，通过对齐视觉和语言输入的时间和空间尺度差异，提升特征修改效果。", "motivation": "解决视觉语言跟踪中因目标移动导致的视觉输入与语言描述不对齐问题，以及现有方法未充分考虑的时空尺度差异。", "method": "将语言描述分解为具有不同属性的短语，并细粒度地修改其特征；引入包含前一帧语言信息的视觉语言标记，以提取更相关的视觉特征。", "result": "实验表明，ATSTrack性能与现有方法相当。", "conclusion": "ATSTrack通过时空尺度对齐有效提升了视觉语言跟踪的性能。"}}
{"id": "2507.00999", "pdf": "https://arxiv.org/pdf/2507.00999", "abs": "https://arxiv.org/abs/2507.00999", "authors": ["María Grandury", "Javier Aula-Blasco", "Júlia Falcão", "Clémentine Fourrier", "Miguel González", "Gonzalo Martínez", "Gonzalo Santamaría", "Rodrigo Agerri", "Nuria Aldama", "Luis Chiruzzo", "Javier Conde", "Helena Gómez", "Marta Guerrero", "Guido Ivetta", "Natalia López", "Flor Miriam Plaza-del-Arco", "María Teresa Martín-Valdivia", "Helena Montoro", "Carmen Muñoz", "Pedro Reviriego", "Leire Rosado", "Alejandro Vaca", "María Estrella Vallecillo-Rodríguez", "Jorge Vallego", "Irune Zubiaga"], "title": "La Leaderboard: A Large Language Model Leaderboard for Spanish Varieties and Languages of Spain and Latin America", "categories": ["cs.CL"], "comment": "Accepted at ACL 2025 Main", "summary": "Leaderboards showcase the current capabilities and limitations of Large\nLanguage Models (LLMs). To motivate the development of LLMs that represent the\nlinguistic and cultural diversity of the Spanish-speaking community, we present\nLa Leaderboard, the first open-source leaderboard to evaluate generative LLMs\nin languages and language varieties of Spain and Latin America. La Leaderboard\nis a community-driven project that aims to establish an evaluation standard for\neveryone interested in developing LLMs for the Spanish-speaking community. This\ninitial version combines 66 datasets in Basque, Catalan, Galician, and\ndifferent Spanish varieties, showcasing the evaluation results of 50 models. To\nencourage community-driven development of leaderboards in other languages, we\nexplain our methodology, including guidance on selecting the most suitable\nevaluation setup for each downstream task. In particular, we provide a\nrationale for using fewer few-shot examples than typically found in the\nliterature, aiming to reduce environmental impact and facilitate access to\nreproducible results for a broader research community.", "AI": {"tldr": "La Leaderboard是首个开源排行榜，用于评估西班牙和拉丁美洲语言及方言的生成型大型语言模型（LLMs），旨在推动LLMs的多样性和社区驱动发展。", "motivation": "激励开发能够代表西班牙语社区语言和文化多样性的LLMs。", "method": "结合66个数据集（包括巴斯克语、加泰罗尼亚语、加利西亚语和西班牙语变体），评估50个模型，并提供方法论指导，如减少少样本示例以降低环境影响。", "result": "展示了50个模型的评估结果，并提供了适用于不同下游任务的评估设置建议。", "conclusion": "La Leaderboard为西班牙语社区建立了评估标准，并鼓励其他语言的社区驱动排行榜开发。"}}
{"id": "2507.00462", "pdf": "https://arxiv.org/pdf/2507.00462", "abs": "https://arxiv.org/abs/2507.00462", "authors": ["Jizhou Han", "Chenhao Ding", "SongLin Dong", "Yuhang He", "Xinyuan Gao", "Yihong Gong"], "title": "Unleashing the Potential of All Test Samples: Mean-Shift Guided Test-Time Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "Visual-language models (VLMs) like CLIP exhibit strong generalization but\nstruggle with distribution shifts at test time. Existing training-free\ntest-time adaptation (TTA) methods operate strictly within CLIP's original\nfeature space, relying on high-confidence samples while overlooking the\npotential of low-confidence ones. We propose MS-TTA, a training-free approach\nthat enhances feature representations beyond CLIP's space using a single-step\nk-nearest neighbors (kNN) Mean-Shift. By refining all test samples, MS-TTA\nimproves feature compactness and class separability, leading to more stable\nadaptation. Additionally, a cache of refined embeddings further enhances\ninference by providing Mean Shift enhanced logits. Extensive evaluations on OOD\nand cross-dataset benchmarks demonstrate that MS-TTA consistently outperforms\nstate-of-the-art training-free TTA methods, achieving robust adaptation without\nrequiring additional training.", "AI": {"tldr": "MS-TTA是一种无需训练的测试时适应方法，通过kNN Mean-Shift增强CLIP的特征表示，提升分布偏移下的性能。", "motivation": "现有方法仅依赖高置信度样本，忽略了低置信度样本的潜力，导致在分布偏移下表现不佳。", "method": "采用单步kNN Mean-Shift优化所有测试样本的特征表示，提高特征紧凑性和类别可分性。", "result": "在OOD和跨数据集基准测试中，MS-TTA表现优于现有方法，实现稳定适应。", "conclusion": "MS-TTA无需额外训练即可提升CLIP在分布偏移下的鲁棒性。"}}
{"id": "2507.01001", "pdf": "https://arxiv.org/pdf/2507.01001", "abs": "https://arxiv.org/abs/2507.01001", "authors": ["Yilun Zhao", "Kaiyan Zhang", "Tiansheng Hu", "Sihong Wu", "Ronan Le Bras", "Taira Anderson", "Jonathan Bragg", "Joseph Chee Chang", "Jesse Dodge", "Matt Latzke", "Yixin Liu", "Charles McGrady", "Xiangru Tang", "Zihang Wang", "Chen Zhao", "Hannaneh Hajishirzi", "Doug Downey", "Arman Cohan"], "title": "SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We present SciArena, an open and collaborative platform for evaluating\nfoundation models on scientific literature tasks. Unlike traditional benchmarks\nfor scientific literature understanding and synthesis, SciArena engages the\nresearch community directly, following the Chatbot Arena evaluation approach of\ncommunity voting on model comparisons. By leveraging collective intelligence,\nSciArena offers a community-driven evaluation of model performance on\nopen-ended scientific tasks that demand literature-grounded, long-form\nresponses. The platform currently supports 23 open-source and proprietary\nfoundation models and has collected over 13,000 votes from trusted researchers\nacross diverse scientific domains. We analyze the data collected so far and\nconfirm that the submitted questions are diverse, aligned with real-world\nliterature needs, and that participating researchers demonstrate strong\nself-consistency and inter-annotator agreement in their evaluations. We discuss\nthe results and insights based on the model ranking leaderboard. To further\npromote research in building model-based automated evaluation systems for\nliterature tasks, we release SciArena-Eval, a meta-evaluation benchmark based\non our collected preference data. The benchmark measures the accuracy of models\nin judging answer quality by comparing their pairwise assessments with human\nvotes. Our experiments highlight the benchmark's challenges and emphasize the\nneed for more reliable automated evaluation methods.", "AI": {"tldr": "SciArena是一个开放协作平台，用于评估基础模型在科学文献任务上的表现，通过社区投票方式收集数据，支持23个模型并分析了13,000多票数据。", "motivation": "传统科学文献评估方法缺乏开放性和社区参与，SciArena旨在通过集体智慧提供更真实的模型性能评估。", "method": "采用类似Chatbot Arena的社区投票方法，支持开放和专有模型，收集研究者对模型回答的偏好数据。", "result": "数据显示问题多样且符合实际需求，研究者评估一致性强；发布SciArena-Eval基准，用于评估模型判断答案质量的能力。", "conclusion": "SciArena为科学文献任务提供了社区驱动的评估方法，但自动化评估仍需改进。"}}
{"id": "2507.00469", "pdf": "https://arxiv.org/pdf/2507.00469", "abs": "https://arxiv.org/abs/2507.00469", "authors": ["Yue Tan", "Xiaoqian Hu", "Hao Xue", "Celso De Melo", "Flora D. Salim"], "title": "Bisecle: Binding and Separation in Continual Learning for Video Language Understanding", "categories": ["cs.CV", "cs.LG"], "comment": "23 pages, 12 figures, 10 tables", "summary": "Frontier vision-language models (VLMs) have made remarkable improvements in\nvideo understanding tasks. However, real-world videos typically exist as\ncontinuously evolving data streams (e.g., dynamic scenes captured by wearable\nglasses), necessitating models to continually adapt to shifting data\ndistributions and novel scenarios. Considering the prohibitive computational\ncosts of fine-tuning models on new tasks, usually, a small subset of parameters\nis updated while the bulk of the model remains frozen. This poses new\nchallenges to existing continual learning frameworks in the context of large\nmultimodal foundation models, i.e., catastrophic forgetting and update\nconflict. While the foundation models struggle with parameter-efficient\ncontinual learning, the hippocampus in the human brain has evolved highly\nefficient mechanisms for memory formation and consolidation. Inspired by the\nrapid Binding and pattern separation mechanisms in the hippocampus, in this\nwork, we propose Bisecle for video-language continual learning, where a\nmulti-directional supervision module is used to capture more cross-modal\nrelationships and a contrastive prompt learning scheme is designed to isolate\ntask-specific knowledge to facilitate efficient memory storage. Binding and\nseparation processes further strengthen the ability of VLMs to retain complex\nexperiences, enabling robust and efficient continual learning in video\nunderstanding tasks. We perform a thorough evaluation of the proposed Bisecle,\ndemonstrating its ability to mitigate forgetting and enhance cross-task\ngeneralization on several VideoQA benchmarks.", "AI": {"tldr": "论文提出Bisecle方法，通过多方向监督模块和对比提示学习方案，解决视频语言持续学习中的灾难性遗忘和更新冲突问题。", "motivation": "现实世界视频是持续演化的数据流，现有视觉语言模型在持续学习中面临灾难性遗忘和更新冲突的挑战。", "method": "受海马体快速绑定和模式分离机制启发，设计多方向监督模块和对比提示学习方案。", "result": "在多个VideoQA基准测试中，Bisecle有效缓解遗忘并提升跨任务泛化能力。", "conclusion": "Bisecle为视频语言持续学习提供了一种高效且鲁棒的解决方案。"}}
{"id": "2507.00472", "pdf": "https://arxiv.org/pdf/2507.00472", "abs": "https://arxiv.org/abs/2507.00472", "authors": ["Ying Guo", "Xi Liu", "Cheng Zhen", "Pengfei Yan", "Xiaoming Wei"], "title": "ARIG: Autoregressive Interactive Head Generation for Real-time Conversations", "categories": ["cs.CV"], "comment": "ICCV 2025. Homepage: https://jinyugy21.github.io/ARIG/", "summary": "Face-to-face communication, as a common human activity, motivates the\nresearch on interactive head generation. A virtual agent can generate motion\nresponses with both listening and speaking capabilities based on the audio or\nmotion signals of the other user and itself. However, previous clip-wise\ngeneration paradigm or explicit listener/speaker generator-switching methods\nhave limitations in future signal acquisition, contextual behavioral\nunderstanding, and switching smoothness, making it challenging to be real-time\nand realistic. In this paper, we propose an autoregressive (AR) based\nframe-wise framework called ARIG to realize the real-time generation with\nbetter interaction realism. To achieve real-time generation, we model motion\nprediction as a non-vector-quantized AR process. Unlike discrete codebook-index\nprediction, we represent motion distribution using diffusion procedure,\nachieving more accurate predictions in continuous space. To improve interaction\nrealism, we emphasize interactive behavior understanding (IBU) and detailed\nconversational state understanding (CSU). In IBU, based on dual-track\ndual-modal signals, we summarize short-range behaviors through\nbidirectional-integrated learning and perform contextual understanding over\nlong ranges. In CSU, we use voice activity signals and context features of IBU\nto understand the various states (interruption, feedback, pause, etc.) that\nexist in actual conversations. These serve as conditions for the final\nprogressive motion prediction. Extensive experiments have verified the\neffectiveness of our model.", "AI": {"tldr": "提出了一种基于自回归的帧级框架ARIG，用于实时生成更真实的交互式头部运动，解决了传统方法的信号获取、上下文行为理解和切换平滑性问题。", "motivation": "面对面对话是常见的人类活动，虚拟代理需要具备实时且真实的听和说能力，但现有方法在信号获取、上下文理解和切换平滑性上存在局限。", "method": "采用非向量量化的自回归过程建模运动预测，利用扩散过程表示运动分布；通过交互行为理解（IBU）和对话状态理解（CSU）提升交互真实性。", "result": "实验验证了模型的有效性，实现了更准确的连续空间预测和更真实的交互行为。", "conclusion": "ARIG框架在实时性和交互真实性上优于现有方法，为虚拟代理的交互提供了新思路。"}}
{"id": "2507.00474", "pdf": "https://arxiv.org/pdf/2507.00474", "abs": "https://arxiv.org/abs/2507.00474", "authors": ["Yaofei Duan", "Yuhao Huang", "Xin Yang", "Luyi Han", "Xinyu Xie", "Zhiyuan Zhu", "Ping He", "Ka-Hou Chan", "Ligang Cui", "Sio-Kei Im", "Dong Ni", "Tao Tan"], "title": "ADAptation: Reconstruction-based Unsupervised Active Learning for Breast Ultrasound Diagnosis", "categories": ["cs.CV"], "comment": "11 pages, 4 figures, 4 tables. Accepted by conference MICCAI2025", "summary": "Deep learning-based diagnostic models often suffer performance drops due to\ndistribution shifts between training (source) and test (target) domains.\nCollecting and labeling sufficient target domain data for model retraining\nrepresents an optimal solution, yet is limited by time and scarce resources.\nActive learning (AL) offers an efficient approach to reduce annotation costs\nwhile maintaining performance, but struggles to handle the challenge posed by\ndistribution variations across different datasets. In this study, we propose a\nnovel unsupervised Active learning framework for Domain Adaptation, named\nADAptation, which efficiently selects informative samples from multi-domain\ndata pools under limited annotation budget. As a fundamental step, our method\nfirst utilizes the distribution homogenization capabilities of diffusion models\nto bridge cross-dataset gaps by translating target images into source-domain\nstyle. We then introduce two key innovations: (a) a hypersphere-constrained\ncontrastive learning network for compact feature clustering, and (b) a\ndual-scoring mechanism that quantifies and balances sample uncertainty and\nrepresentativeness. Extensive experiments on four breast ultrasound datasets\n(three public and one in-house/multi-center) across five common deep\nclassifiers demonstrate that our method surpasses existing strong AL-based\ncompetitors, validating its effectiveness and generalization for clinical\ndomain adaptation. The code is available at the anonymized link:\nhttps://github.com/miccai25-966/ADAptation.", "AI": {"tldr": "提出了一种名为ADAptation的无监督主动学习框架，通过扩散模型和双评分机制解决领域适应问题，显著优于现有方法。", "motivation": "解决深度学习模型在训练和测试领域分布偏移时的性能下降问题，减少标注成本并提升适应性。", "method": "利用扩散模型进行分布同质化，结合紧凑特征聚类和双评分机制选择信息样本。", "result": "在四个乳腺超声数据集上验证，性能优于现有主动学习方法。", "conclusion": "ADAptation框架在临床领域适应中表现出高效性和泛化能力。"}}
{"id": "2507.00490", "pdf": "https://arxiv.org/pdf/2507.00490", "abs": "https://arxiv.org/abs/2507.00490", "authors": ["Zijian Chen", "Yuan Tian", "Yuze Sun", "Wei Sun", "Zicheng Zhang", "Weisi Lin", "Guangtao Zhai", "Wenjun Zhang"], "title": "Just Noticeable Difference for Large Multimodal Models", "categories": ["cs.CV", "eess.IV"], "comment": "19 pages, 19 figures", "summary": "Just noticeable difference (JND), the minimum change that the human visual\nsystem (HVS) can perceive, has been studied for decades. Although recent work\nhas extended this line of research into machine vision, there has been a\nscarcity of studies systematically exploring its perceptual boundaries across\nmultiple tasks and stimulus types, particularly in the current era of rapidly\nadvancing large multimodal models (LMMs), where studying the multifaceted\ncapabilities of models has become a mainstream focus. Moreover, the perceptual\ndefects of LMMs are not investigated thoroughly, resulting in potential\nsecurity issues and suboptimal response efficiency. In this paper, we take an\ninitial attempt and demonstrate that there exist significant visual blind spots\nin current LMMs. To systemically quantify this characteristic, we propose a new\nconcept, {\\bf LMM-JND}, together with its determination pipeline. Targeting\nuncovering the behavior commonalities in HVS-aligned visual perception tasks,\nwe delve into several LMM families and construct a large-scale dataset, named\nVPA-JND, which contains 21.5k reference images with over 489k stimuli across 12\ndistortion types, to facilitate LMM-JND studies. VPA-JND exposes areas where\nstate-of-the-art LMMs, including GPT-4o and the InternVL2.5 series, struggle\nwith basic comparison queries and fall significantly short of human-level\nvisual performance. We further explore the effects of vision and language\nbackbones and find a notable correlation between their design philosophy that\nmay instruct the future refinement of LMMs for their visual acuity. Together,\nour research underscores the significance of LMM-JND as a unique perspective\nfor studying LMMs, and predictable LMM-JND is crucial for security concerns.\nThis work will be available at https://github.com/zijianchen98/LMM-JND.", "AI": {"tldr": "本文提出LMM-JND概念及其测定流程，揭示当前大型多模态模型（LMMs）在视觉感知任务中的盲区，并构建VPA-JND数据集以支持研究。", "motivation": "研究LMMs在视觉感知任务中的缺陷，填补系统性研究的空白，并解决潜在的安全问题。", "method": "提出LMM-JND概念及测定流程，构建包含21.5k参考图像和489k刺激的大规模数据集VPA-JND，分析多种LMMs的表现。", "result": "发现当前先进LMMs（如GPT-4o和InternVL2.5系列）在基础比较任务中表现不佳，远低于人类水平。", "conclusion": "LMM-JND为研究LMMs提供了新视角，其可预测性对安全至关重要，未来研究可基于此优化LMMs的视觉能力。"}}
{"id": "2507.00057", "pdf": "https://arxiv.org/pdf/2507.00057", "abs": "https://arxiv.org/abs/2507.00057", "authors": ["Thomas Valentin", "Ardi Madadi", "Gaetano Sapia", "Marcel Böhme"], "title": "Estimating Correctness Without Oracles in LLM-Based Code Generation", "categories": ["cs.PL", "cs.AI", "cs.LG", "cs.SE"], "comment": "8 pages + refs and appendix", "summary": "Generating code from natural language specifications is one of the most\nsuccessful applications of Large Language Models (LLMs). Yet, they hallucinate:\nLLMs produce outputs that may be grammatically correct but are factually\nincorrect. Without an existing, correct implementation (i.e., an oracle), can\nwe quantify how likely the generated program is correct?\n  In this paper, we propose a measure of incorrectness, called incoherence,\nthat can be estimated efficiently in the absence of an oracle and provides a\nlower bound on the error, i.e., the probability that the LLM-generated program\nfor that specification is incorrect. Our experiments demonstrate an\nextraordinary effectiveness. For the average code generation task, our\nincoherence-based methodology can automatically identify about two-thirds of\nincorrect programs without reports of false positives. In fact, an oracle-based\nevaluation of LLMs can be reliably replaced by an incoherence-based evaluation.\nIn particular, we find a very strong agreement between the ranking of LLMs by\nthe number of programs deemed correct via an oracle (pass@1) and the ranking of\nLLMs by the number of programs deemed correct via our incoherence.", "AI": {"tldr": "提出了一种名为“不连贯性”的度量方法，用于在没有正确实现（oracle）的情况下量化LLM生成代码的错误概率，实验证明其高效且可靠。", "motivation": "LLM生成的代码可能存在语法正确但事实错误的问题，需要一种无需oracle的方法来评估其正确性。", "method": "提出“不连贯性”作为错误概率的下界估计，并通过实验验证其有效性。", "result": "实验显示，该方法能自动识别约三分之二的错误程序，且无假阳性报告，与oracle评估结果高度一致。", "conclusion": "不连贯性评估可替代oracle评估，为LLM生成代码的正确性提供可靠度量。"}}
{"id": "2507.00493", "pdf": "https://arxiv.org/pdf/2507.00493", "abs": "https://arxiv.org/abs/2507.00493", "authors": ["Fenil R. Doshi", "Thomas Fel", "Talia Konkle", "George Alvarez"], "title": "Visual Anagrams Reveal Hidden Differences in Holistic Shape Processing Across Vision Models", "categories": ["cs.CV", "cs.AI"], "comment": "Project page: https://www.fenildoshi.com/configural-shape/", "summary": "Humans are able to recognize objects based on both local texture cues and the\nconfiguration of object parts, yet contemporary vision models primarily harvest\nlocal texture cues, yielding brittle, non-compositional features. Work on\nshape-vs-texture bias has pitted shape and texture representations in\nopposition, measuring shape relative to texture, ignoring the possibility that\nmodels (and humans) can simultaneously rely on both types of cues, and\nobscuring the absolute quality of both types of representation. We therefore\nrecast shape evaluation as a matter of absolute configural competence,\noperationalized by the Configural Shape Score (CSS), which (i) measures the\nability to recognize both images in Object-Anagram pairs that preserve local\ntexture while permuting global part arrangement to depict different object\ncategories. Across 86 convolutional, transformer, and hybrid models, CSS (ii)\nuncovers a broad spectrum of configural sensitivity with fully self-supervised\nand language-aligned transformers -- exemplified by DINOv2, SigLIP2 and\nEVA-CLIP -- occupying the top end of the CSS spectrum. Mechanistic probes\nreveal that (iii) high-CSS networks depend on long-range interactions:\nradius-controlled attention masks abolish performance showing a distinctive\nU-shaped integration profile, and representational-similarity analyses expose a\nmid-depth transition from local to global coding. A BagNet control remains at\nchance (iv), ruling out \"border-hacking\" strategies. Finally, (v) we show that\nconfigural shape score also predicts other shape-dependent evals. Overall, we\npropose that the path toward truly robust, generalizable, and human-like vision\nsystems may not lie in forcing an artificial choice between shape and texture,\nbut rather in architectural and learning frameworks that seamlessly integrate\nboth local-texture and global configural shape.", "AI": {"tldr": "论文提出了一种新的形状评估方法——Configural Shape Score (CSS)，用于衡量模型对全局配置形状的敏感性，发现自监督和语言对齐的Transformer模型表现最佳。", "motivation": "当前视觉模型主要依赖局部纹理线索，忽视了全局配置形状的重要性，导致特征脆弱且非组合性。论文旨在探索模型能否同时利用纹理和形状线索。", "method": "通过Object-Anagram对评估模型的配置形状敏感性，提出CSS评分，并测试了86种卷积、Transformer和混合模型。", "result": "自监督和语言对齐的Transformer模型（如DINOv2、SigLIP2和EVA-CLIP）在CSS评分中表现最佳，且依赖长程交互。BagNet表现随机，排除了边界策略。", "conclusion": "构建真正鲁棒且类人的视觉系统需要同时整合局部纹理和全局配置形状，而非在二者之间做选择。"}}
{"id": "2507.00061", "pdf": "https://arxiv.org/pdf/2507.00061", "abs": "https://arxiv.org/abs/2507.00061", "authors": ["Hoang-Dieu Vu", "Duc-Nghia Tran", "Quang-Tu Pham", "Hieu H. Pham", "Nicolas Vuillerme", "Duc-Tan Tran"], "title": "Smooth-Distill: A Self-distillation Framework for Multitask Learning with Wearable Sensor Data", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": null, "summary": "This paper introduces Smooth-Distill, a novel self-distillation framework\ndesigned to simultaneously perform human activity recognition (HAR) and sensor\nplacement detection using wearable sensor data. The proposed approach utilizes\na unified CNN-based architecture, MTL-net, which processes accelerometer data\nand branches into two outputs for each respective task. Unlike conventional\ndistillation methods that require separate teacher and student models, the\nproposed framework utilizes a smoothed, historical version of the model itself\nas the teacher, significantly reducing training computational overhead while\nmaintaining performance benefits. To support this research, we developed a\ncomprehensive accelerometer-based dataset capturing 12 distinct sleep postures\nacross three different wearing positions, complementing two existing public\ndatasets (MHealth and WISDM). Experimental results show that Smooth-Distill\nconsistently outperforms alternative approaches across different evaluation\nscenarios, achieving notable improvements in both human activity recognition\nand device placement detection tasks. This method demonstrates enhanced\nstability in convergence patterns during training and exhibits reduced\noverfitting compared to traditional multitask learning baselines. This\nframework contributes to the practical implementation of knowledge distillation\nin human activity recognition systems, offering an effective solution for\nmultitask learning with accelerometer data that balances accuracy and training\nefficiency. More broadly, it reduces the computational cost of model training,\nwhich is critical for scenarios requiring frequent model updates or training on\nresource-constrained platforms. The code and model are available at\nhttps://github.com/Kuan2vn/smooth\\_distill.", "AI": {"tldr": "Smooth-Distill是一种新颖的自蒸馏框架，用于同时进行人类活动识别（HAR）和传感器位置检测，通过统一的CNN架构MTL-net处理加速度计数据，显著减少计算开销。", "motivation": "解决传统蒸馏方法需要独立师生模型的问题，同时提升多任务学习的效率和性能。", "method": "使用MTL-net处理加速度计数据，采用平滑历史模型作为教师模型，减少计算开销。", "result": "在多个评估场景中表现优于其他方法，提升了活动识别和位置检测的准确性，减少了过拟合。", "conclusion": "Smooth-Distill为多任务学习提供了一种高效解决方案，降低了计算成本，适用于资源受限平台。"}}
{"id": "2507.00501", "pdf": "https://arxiv.org/pdf/2507.00501", "abs": "https://arxiv.org/abs/2507.00501", "authors": ["Yongzhen Wang", "Liangliang Chen", "Bingwen Hu", "Heng Liu", "Xiao-Ping Zhang", "Mingqiang Wei"], "title": "Laplace-Mamba: Laplace Frequency Prior-Guided Mamba-CNN Fusion Network for Image Dehazing", "categories": ["cs.CV"], "comment": "12 pages, 11 figures, 6 tables", "summary": "Recent progress in image restoration has underscored Spatial State Models\n(SSMs) as powerful tools for modeling long-range dependencies, owing to their\nappealing linear complexity and computational efficiency. However, SSM-based\napproaches exhibit limitations in reconstructing localized structures and tend\nto be less effective when handling high-dimensional data, frequently resulting\nin suboptimal recovery of fine image features. To tackle these challenges, we\nintroduce Laplace-Mamba, a novel framework that integrates Laplace frequency\nprior with a hybrid Mamba-CNN architecture for efficient image dehazing.\nLeveraging the Laplace decomposition, the image is disentangled into\nlow-frequency components capturing global texture and high-frequency components\nrepresenting edges and fine details. This decomposition enables specialized\nprocessing via dual parallel pathways: the low-frequency branch employs SSMs\nfor global context modeling, while the high-frequency branch utilizes CNNs to\nrefine local structural details, effectively addressing diverse haze scenarios.\nNotably, the Laplace transformation facilitates information-preserving\ndownsampling of low-frequency components in accordance with the Nyquist theory,\nthereby significantly improving computational efficiency. Extensive evaluations\nacross multiple benchmarks demonstrate that our method outperforms\nstate-of-the-art approaches in both restoration quality and efficiency. The\nsource code and pretrained models are available at\nhttps://github.com/yz-wang/Laplace-Mamba.", "AI": {"tldr": "Laplace-Mamba结合拉普拉斯频率先验和混合Mamba-CNN架构，通过双路径处理高低频分量，显著提升图像去雾效果和效率。", "motivation": "现有基于SSM的方法在重建局部结构和高维数据处理上表现不足，导致细粒度特征恢复不理想。", "method": "利用拉普拉斯分解将图像分为高低频分量，分别通过SSM和CNN处理，并结合信息保留的下采样技术。", "result": "在多个基准测试中，Laplace-Mamba在恢复质量和效率上均优于现有方法。", "conclusion": "Laplace-Mamba通过双路径设计和拉普拉斯分解，有效解决了图像去雾中的局部结构和高维数据问题。"}}
{"id": "2507.00066", "pdf": "https://arxiv.org/pdf/2507.00066", "abs": "https://arxiv.org/abs/2507.00066", "authors": ["Xingyu Xiao", "Jiejuan Tong", "Peng Chen", "Jun Sun", "Zhe Sui", "Jingang Liang", "Hongru Zhao", "Jun Zhao", "Haitao Wang"], "title": "InSight-R: A Framework for Risk-informed Human Failure Event Identification and Interface-Induced Risk Assessment Driven by AutoGraph", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Human reliability remains a critical concern in safety-critical domains such\nas nuclear power, where operational failures are often linked to human error.\nWhile conventional human reliability analysis (HRA) methods have been widely\nadopted, they rely heavily on expert judgment for identifying human failure\nevents (HFEs) and assigning performance influencing factors (PIFs). This\nreliance introduces challenges related to reproducibility, subjectivity, and\nlimited integration of interface-level data. In particular, current approaches\nlack the capacity to rigorously assess how human-machine interface design\ncontributes to operator performance variability and error susceptibility. To\naddress these limitations, this study proposes a framework for risk-informed\nhuman failure event identification and interface-induced risk assessment driven\nby AutoGraph (InSight-R). By linking empirical behavioral data to the\ninterface-embedded knowledge graph (IE-KG) constructed by the automated\ngraph-based execution framework (AutoGraph), the InSight-R framework enables\nautomated HFE identification based on both error-prone and time-deviated\noperational paths. Furthermore, we discuss the relationship between\ndesigner-user conflicts and human error. The results demonstrate that InSight-R\nnot only enhances the objectivity and interpretability of HFE identification\nbut also provides a scalable pathway toward dynamic, real-time human\nreliability assessment in digitalized control environments. This framework\noffers actionable insights for interface design optimization and contributes to\nthe advancement of mechanism-driven HRA methodologies.", "AI": {"tldr": "该研究提出了一种基于AutoGraph的框架InSight-R，用于自动化识别人为故障事件（HFE）和评估界面设计风险，解决了传统HRA方法依赖专家判断的问题。", "motivation": "传统人为可靠性分析（HRA）方法依赖专家判断，存在主观性和可重复性差的问题，且难以评估人机界面设计对操作员表现的影响。", "method": "通过AutoGraph构建界面嵌入知识图（IE-KG），结合实证行为数据，自动化识别HFE并评估界面设计风险。", "result": "InSight-R提高了HFE识别的客观性和可解释性，为动态实时人为可靠性评估提供了可扩展的途径。", "conclusion": "该框架为界面设计优化提供了实用见解，并推动了机制驱动的HRA方法的发展。"}}
{"id": "2507.00502", "pdf": "https://arxiv.org/pdf/2507.00502", "abs": "https://arxiv.org/abs/2507.00502", "authors": ["JianChao Zhao", "Songlin Dong"], "title": "ExPaMoE: An Expandable Parallel Mixture of Experts for Continual Test-Time Adaptation", "categories": ["cs.CV"], "comment": null, "summary": "Continual Test-Time Adaptation (CTTA) aims to enable models to adapt\non-the-fly to a stream of unlabeled data under evolving distribution shifts.\nHowever, existing CTTA methods typically rely on shared model parameters across\nall domains, making them vulnerable to feature entanglement and catastrophic\nforgetting in the presence of large or non-stationary domain shifts. To address\nthis limitation, we propose \\textbf{ExPaMoE}, a novel framework based on an\n\\emph{Expandable Parallel Mixture-of-Experts} architecture. ExPaMoE decouples\ndomain-general and domain-specific knowledge via a dual-branch expert design\nwith token-guided feature separation, and dynamically expands its expert pool\nbased on a \\emph{Spectral-Aware Online Domain Discriminator} (SODD) that\ndetects distribution changes in real-time using frequency-domain cues.\nExtensive experiments demonstrate the superiority of ExPaMoE across diverse\nCTTA scenarios. We evaluate our method on standard benchmarks including\nCIFAR-10C, CIFAR-100C, ImageNet-C, and Cityscapes-to-ACDC for semantic\nsegmentation. Additionally, we introduce \\textbf{ImageNet++}, a large-scale and\nrealistic CTTA benchmark built from multiple ImageNet-derived datasets, to\nbetter reflect long-term adaptation under complex domain evolution. ExPaMoE\nconsistently outperforms prior arts, showing strong robustness, scalability,\nand resistance to forgetting.", "AI": {"tldr": "ExPaMoE框架通过可扩展的并行混合专家架构解决持续测试时适应中的特征纠缠和灾难性遗忘问题，动态扩展专家池并利用频域线索实时检测分布变化。", "motivation": "现有CTTA方法依赖共享模型参数，易受特征纠缠和灾难性遗忘影响，尤其在非平稳域偏移下表现不佳。", "method": "提出ExPaMoE框架，采用双分支专家设计和基于频域线索的实时分布变化检测器（SODD），动态扩展专家池。", "result": "在多个标准基准测试（如CIFAR-10C、ImageNet-C等）和新引入的ImageNet++上表现优异，具有强鲁棒性和抗遗忘性。", "conclusion": "ExPaMoE在持续测试时适应任务中显著优于现有方法，适用于复杂域演化场景。"}}
{"id": "2507.00505", "pdf": "https://arxiv.org/pdf/2507.00505", "abs": "https://arxiv.org/abs/2507.00505", "authors": ["Haoran Lou", "Chunxiao Fan", "Ziyan Liu", "Yuexin Wu", "Xinxiang Wang"], "title": "LLaVA-SP: Enhancing Visual Representation with Visual Spatial Tokens for MLLMs", "categories": ["cs.CV"], "comment": "ICCV", "summary": "The architecture of multimodal large language models (MLLMs) commonly\nconnects a vision encoder, often based on CLIP-ViT, to a large language model.\nWhile CLIP-ViT works well for capturing global image features, it struggles to\nmodel local relationships between adjacent patches, leading to weaker visual\nrepresentation, which in turn affects the detailed understanding ability of\nMLLMs. To solve this, we propose LLaVA-SP, which \\textbf{ only adds six spatial\nvisual tokens} to the original visual tokens to enhance the visual\nrepresentation. Our approach offers three key advantages: 1)We propose a novel\nProjector, which uses convolutional kernels to derive visual spatial tokens\nfrom ViT patch features, simulating two visual spatial ordering approaches:\n``from central region to global\" and ``from abstract to specific\". Then, a\ncross-attention mechanism is applied to fuse fine-grained visual information,\nenriching the overall visual representation. 2) We present two model variants:\nLLaVA-SP-Cropping, which focuses on detail features through progressive\ncropping, and LLaVA-SP-Pooling, which captures global semantics through\nadaptive pooling, enabling the model to handle diverse visual understanding\ntasks. 3) Extensive experiments show that LLaVA-SP, fine-tuned with LoRA,\nachieves significant performance improvements across various multimodal\nbenchmarks, outperforming the state-of-the-art LLaVA-1.5 model in multiple\ntasks with nearly identical inference latency. The code and models are\navailable at\n\\href{https://github.com/CnFaker/LLaVA-SP}{\\texttt{https://github.com/CnFaker/LLaVA-SP}}.", "AI": {"tldr": "LLaVA-SP通过添加六个空间视觉标记增强视觉表示，提出新型Projector和两种变体，显著提升多模态任务性能。", "motivation": "CLIP-ViT在捕捉局部关系时表现不佳，影响多模态大语言模型的细节理解能力。", "method": "提出LLaVA-SP，添加空间视觉标记，使用卷积核生成视觉空间标记，并通过交叉注意力机制融合细粒度信息。提供两种变体：LLaVA-SP-Cropping和LLaVA-SP-Pooling。", "result": "在多个多模态基准测试中表现优异，性能超过LLaVA-1.5，推理延迟几乎相同。", "conclusion": "LLaVA-SP通过简单高效的方法显著提升了视觉表示能力，适用于多样化视觉理解任务。"}}
{"id": "2507.00506", "pdf": "https://arxiv.org/pdf/2507.00506", "abs": "https://arxiv.org/abs/2507.00506", "authors": ["Yunfei Xie", "Yuxuan Cheng", "Juncheng Wu", "Haoyu Zhang", "Yuyin Zhou", "Shoudong Han"], "title": "SCING:Towards More Efficient and Robust Person Re-Identification through Selective Cross-modal Prompt Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Recent advancements in adapting vision-language pre-training models like CLIP\nfor person re-identification (ReID) tasks often rely on complex adapter design\nor modality-specific tuning while neglecting cross-modal interaction, leading\nto high computational costs or suboptimal alignment. To address these\nlimitations, we propose a simple yet effective framework named Selective\nCross-modal Prompt Tuning (SCING) that enhances cross-modal alignment and\nrobustness against real-world perturbations. Our method introduces two key\ninnovations: Firstly, we proposed Selective Visual Prompt Fusion (SVIP), a\nlightweight module that dynamically injects discriminative visual features into\ntext prompts via a cross-modal gating mechanism. Moreover, the proposed\nPerturbation-Driven Consistency Alignment (PDCA) is a dual-path training\nstrategy that enforces invariant feature alignment under random image\nperturbations by regularizing consistency between original and augmented\ncross-modal embeddings. Extensive experiments are conducted on several popular\nbenchmarks covering Market1501, DukeMTMC-ReID, Occluded-Duke, Occluded-REID,\nand P-DukeMTMC, which demonstrate the impressive performance of the proposed\nmethod. Notably, our framework eliminates heavy adapters while maintaining\nefficient inference, achieving an optimal trade-off between performance and\ncomputational overhead. The code will be released upon acceptance.", "AI": {"tldr": "论文提出了一种名为SCING的简单有效框架，通过选择性跨模态提示调优增强跨模态对齐和鲁棒性，避免了复杂适配器设计和高计算成本。", "motivation": "现有方法在视觉-语言预训练模型（如CLIP）用于行人重识别任务时，常忽视跨模态交互，导致高计算成本或对齐效果不佳。", "method": "提出选择性视觉提示融合（SVIP）和扰动驱动一致性对齐（PDCA）两种创新方法，动态注入视觉特征并增强跨模态鲁棒性。", "result": "在多个流行基准测试中表现优异，实现了性能与计算开销的最佳平衡。", "conclusion": "SCING框架通过轻量级设计显著提升了跨模态对齐效果，同时保持高效推理。"}}
{"id": "2507.00075", "pdf": "https://arxiv.org/pdf/2507.00075", "abs": "https://arxiv.org/abs/2507.00075", "authors": ["Yifan Sun", "Yushan Liang", "Zhen Zhang", "Jiaye Teng"], "title": "Theoretical Modeling of LLM Self-Improvement Training Dynamics Through Solver-Verifier Gap", "categories": ["cs.LG", "cs.AI"], "comment": "24 pages", "summary": "Self-improvement is among the most prominent techniques within the realm of\nlarge language models (LLM), aiming to enhance the LLM performance without\nrelying on external data. Despite its significance, generally how LLM\nperformances evolve during the self-improvement process remains underexplored.\nIn this paper, we theoretically model the training dynamics of self-improvement\nvia the concept of solver-verifier gap. This is inspired by the conjecture that\nthe performance enhancement of self-improvement stems from the gap between\nLLM's solver capability and verifier capability. Based on the theoretical\nframework, we further introduce how to predict the ultimate power of\nself-improvement using only information from the first few training epochs. We\nempirically validate the effectiveness of the theoretical model on various LLMs\nand datasets. Beyond self-improvement, we extend our analysis to investigate\nhow external data influences these dynamics within the framework. Notably, we\nfind that under limited external data regimes, such external data can be\nutilized at any stage without significantly affecting final performances, which\naccords with the empirical observations.", "AI": {"tldr": "论文通过理论建模研究LLM自改进过程中的性能演化，提出求解器-验证器间隙的概念，并验证其有效性。", "motivation": "探索LLM自改进过程中性能演化的理论机制，填补研究空白。", "method": "提出求解器-验证器间隙的理论框架，并基于此预测自改进的最终能力。", "result": "理论模型在多种LLM和数据集上得到验证，且发现有限外部数据不影响最终性能。", "conclusion": "自改进性能提升源于求解器与验证器能力的差距，外部数据在有限条件下可灵活使用。"}}
{"id": "2507.00078", "pdf": "https://arxiv.org/pdf/2507.00078", "abs": "https://arxiv.org/abs/2507.00078", "authors": ["Yi Xie", "Yun Xiong", "Zejian Shi", "Hao Niu", "Zhengfu Liu"], "title": "The language of time: a language model perspective on time-series foundation models", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "With the rise of large language models, the paradigm of training foundation\nmodels with massive parameter counts on vast datasets has been adopted in\nmultiple domains to achieve remarkable success. Time series foundation models\nrepresent a significant extension of this paradigm, demonstrating exceptional\nexpressive power, generalization, and cross-domain transferability. However,\nthis gives rise to a fundamental paradox: time series data reflect distinct\ndynamical systems, making cross-domain transfer intuitively implausible, yet\nthis is contradicted by the models' empirical success. To resolve this paradox,\nthis paper investigates, from both theoretical and experimental perspectives,\nthe representation learning mechanisms and generalization capabilities of\npatch-based time series foundation models. We argue that such models are not\nmerely applying a new architecture but are fundamentally generalizing the\nrepresentation paradigm of language models by extending deterministic\nvector-based representations to latent probabilistic distributional forms. Our\ntheoretical analysis supports this framework by demonstrating that continuous\ntime-series patches can be faithfully quantized into a discrete vocabulary\nwhose key statistical properties are highly consistent with those of natural\nlanguage. This generalization allows time series models to inherit the robust\nrepresentation and transfer abilities of large language models, thereby\nexplaining their superior performance in temporal tasks. Ultimately, our work\nprovides a rigorous theoretical cornerstone for understanding, evaluating, and\nimproving the safety and reliability of large-scale time series foundation\nmodels.", "AI": {"tldr": "论文探讨了基于补丁的时间序列基础模型的表示学习机制和泛化能力，解释了其跨域迁移成功的理论依据。", "motivation": "研究时间序列基础模型在跨域迁移中的成功现象，解决其与时间序列数据动态特性矛盾的悖论。", "method": "从理论和实验角度分析补丁时间序列模型的表示学习机制，提出其通过概率分布形式扩展语言模型表示范式。", "result": "证明时间序列补丁可量化到离散词汇表，统计特性与自然语言一致，解释了模型的优越性能。", "conclusion": "为理解、评估和改进大规模时间序列基础模型的安全性和可靠性提供了理论基础。"}}
{"id": "2507.00519", "pdf": "https://arxiv.org/pdf/2507.00519", "abs": "https://arxiv.org/abs/2507.00519", "authors": ["Ruize Cui", "Jiaan Zhang", "Jialun Pei", "Kai Wang", "Pheng-Ann Heng", "Jing Qin"], "title": "Topology-Constrained Learning for Efficient Laparoscopic Liver Landmark Detection", "categories": ["cs.CV"], "comment": "This paper has been accepted by MICCAI 2025", "summary": "Liver landmarks provide crucial anatomical guidance to the surgeon during\nlaparoscopic liver surgery to minimize surgical risk. However, the tubular\nstructural properties of landmarks and dynamic intraoperative deformations pose\nsignificant challenges for automatic landmark detection. In this study, we\nintroduce TopoNet, a novel topology-constrained learning framework for\nlaparoscopic liver landmark detection. Our framework adopts a snake-CNN\ndual-path encoder to simultaneously capture detailed RGB texture information\nand depth-informed topological structures. Meanwhile, we propose a\nboundary-aware topology fusion (BTF) module, which adaptively merges RGB-D\nfeatures to enhance edge perception while preserving global topology.\nAdditionally, a topological constraint loss function is embedded, which\ncontains a center-line constraint loss and a topological persistence loss to\nensure homotopy equivalence between predictions and labels. Extensive\nexperiments on L3D and P2ILF datasets demonstrate that TopoNet achieves\noutstanding accuracy and computational complexity, highlighting the potential\nfor clinical applications in laparoscopic liver surgery. Our code will be\navailable at https://github.com/cuiruize/TopoNet.", "AI": {"tldr": "TopoNet是一种用于腹腔镜肝脏标志物检测的新型拓扑约束学习框架，通过双路径编码器和边界感知拓扑融合模块提升检测精度。", "motivation": "肝脏标志物在腹腔镜手术中为外科医生提供关键解剖指导，但其管状结构和动态变形增加了自动检测的难度。", "method": "采用snake-CNN双路径编码器捕捉RGB纹理和深度拓扑结构，提出边界感知拓扑融合模块和拓扑约束损失函数。", "result": "在L3D和P2ILF数据集上表现出色，兼具高精度和低计算复杂度。", "conclusion": "TopoNet在临床腹腔镜肝脏手术中具有应用潜力。"}}
{"id": "2507.00081", "pdf": "https://arxiv.org/pdf/2507.00081", "abs": "https://arxiv.org/abs/2507.00081", "authors": ["Matthew Muhoberac", "Atharva Parikh", "Nirvi Vakharia", "Saniya Virani", "Aco Radujevic", "Savannah Wood", "Meghav Verma", "Dimitri Metaxotos", "Jeyaraman Soundararajan", "Thierry Masquelin", "Alexander G. Godfrey", "Sean Gardner", "Dobrila Rudnicki", "Sam Michael", "Gaurav Chopra"], "title": "State and Memory is All You Need for Robust and Reliable AI Agents", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.ET", "physics.chem-ph"], "comment": "5 Main Figures, 10 Extended Data Figures (37 Pages) for Manuscript ;\n  9 Supplementary Tables, 40 Supplementary Figures (180 Pages) for Supporting\n  Information", "summary": "Large language models (LLMs) have enabled powerful advances in natural\nlanguage understanding and generation. Yet their application to complex,\nreal-world scientific workflows remain limited by challenges in memory,\nplanning, and tool integration. Here, we introduce SciBORG (Scientific Bespoke\nArtificial Intelligence Agents Optimized for Research Goals), a modular agentic\nframework that allows LLM-based agents to autonomously plan, reason, and\nachieve robust and reliable domain-specific task execution. Agents are\nconstructed dynamically from source code documentation and augmented with\nfinite-state automata (FSA) memory, enabling persistent state tracking and\ncontext-aware decision-making. This approach eliminates the need for manual\nprompt engineering and allows for robust, scalable deployment across diverse\napplications via maintaining context across extended workflows and to recover\nfrom tool or execution failures. We validate SciBORG through integration with\nboth physical and virtual hardware, such as microwave synthesizers for\nexecuting user-specified reactions, with context-aware decision making and\ndemonstrate its use in autonomous multi-step bioassay retrieval from the\nPubChem database utilizing multi-step planning, reasoning, agent-to-agent\ncommunication and coordination for execution of exploratory tasks. Systematic\nbenchmarking shows that SciBORG agents achieve reliable execution, adaptive\nplanning, and interpretable state transitions. Our results show that memory and\nstate awareness are critical enablers of agentic planning and reliability,\noffering a generalizable foundation for deploying AI agents in complex\nenvironments.", "AI": {"tldr": "SciBORG是一个模块化框架，通过动态构建LLM代理并增强其记忆和状态跟踪能力，实现复杂科学任务的自主执行。", "motivation": "解决LLM在复杂科学工作流中因内存、规划和工具集成限制而难以应用的问题。", "method": "动态构建代理，利用有限状态自动机（FSA）记忆实现状态跟踪和上下文感知决策。", "result": "SciBORG在物理和虚拟硬件中验证了可靠执行、自适应规划和可解释状态转换。", "conclusion": "记忆和状态感知是代理规划和可靠性的关键，为复杂环境中的AI代理部署提供了通用基础。"}}
{"id": "2507.00525", "pdf": "https://arxiv.org/pdf/2507.00525", "abs": "https://arxiv.org/abs/2507.00525", "authors": ["Djamahl Etchegaray", "Yuxia Fu", "Zi Huang", "Yadan Luo"], "title": "Box-QAymo: Box-Referring VQA Dataset for Autonomous Driving", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Interpretable communication is essential for safe and trustworthy autonomous\ndriving, yet current vision-language models (VLMs) often operate under\nidealized assumptions and struggle to capture user intent in real-world\nscenarios. Existing driving-oriented VQA datasets are limited to full-scene\ndescriptions or waypoint prediction, preventing the assessment of whether VLMs\ncan respond to localized user-driven queries. We introduce Box-QAymo, a\nbox-referring dataset and benchmark designed to both evaluate and finetune VLMs\non spatial and temporal reasoning over user-specified objects. Users express\nintent by drawing bounding boxes, offering a fast and intuitive interface for\nfocused queries in complex scenes. Specifically, we propose a hierarchical\nevaluation protocol that begins with binary sanity-check questions to assess\nbasic model capacities, and progresses to (1) attribute prediction for\nbox-referred objects, (2) motion understanding of target instances, and (3)\nspatiotemporal motion reasoning over inter-object dynamics across frames. To\nsupport this, we crowd-sourced fine-grained object classes and visual\nattributes that reflect the complexity drivers encounter, and extract object\ntrajectories to construct temporally grounded QA pairs. Rigorous quality\ncontrol through negative sampling, temporal consistency checks, and\ndifficulty-aware balancing guarantee dataset robustness and diversity. Our\ncomprehensive evaluation reveals significant limitations in current VLMs when\nqueried about perception questions, highlighting the gap in achieving\nreal-world performance. This work provides a foundation for developing more\nrobust and interpretable autonomous driving systems that can communicate\neffectively with users under real-world conditions. Project page and dataset\nare available at https://djamahl99.github.io/qaymo-pages/.", "AI": {"tldr": "Box-QAymo是一个用于评估和微调视觉语言模型（VLMs）的数据集和基准，专注于用户指定对象的时空推理。", "motivation": "当前VLMs在真实场景中难以捕捉用户意图，现有数据集局限于全场景描述或路径点预测，无法评估VLMs对局部用户驱动查询的响应能力。", "method": "提出Box-QAymo数据集，用户通过绘制边界框表达意图，支持分层评估协议（从基本能力到属性预测、运动理解和时空推理）。", "result": "评估显示当前VLMs在感知问题上的显著局限性，揭示了与现实世界性能的差距。", "conclusion": "该工作为开发更鲁棒和可解释的自动驾驶系统奠定了基础，支持真实条件下的有效用户沟通。"}}
{"id": "2507.00082", "pdf": "https://arxiv.org/pdf/2507.00082", "abs": "https://arxiv.org/abs/2507.00082", "authors": ["Faranaksadat Solat", "Joohyung Lee", "Mohamed Seif", "Dusit Niyato", "H. Vincent Poor"], "title": "Federated Learning-Enabled Hybrid Language Models for Communication-Efficient Token Transmission", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "17 pages, 16 figures, IEEE Internet of Things", "summary": "Hybrid Language Models (HLMs) combine the low-latency efficiency of Small\nLanguage Models (SLMs) on edge devices with the high accuracy of Large Language\nModels (LLMs) on centralized servers. Unlike traditional end-to-end LLM\ninference, HLMs reduce latency and communication by invoking LLMs only when\nlocal SLM predictions are uncertain, i.e., when token-level confidence is low\nor entropy is high. However, ambiguous or low-confidence predictions still\nrequire frequent offloading to the LLM, leading to significant communication\noverhead in bandwidth-constrained settings. To address this, we propose FedHLM,\na communication-efficient HLM framework that integrates uncertainty-aware\ninference with Federated Learning (FL). FedHLM's key innovation lies in\ncollaboratively learning token-level uncertainty thresholds that govern when\nLLM assistance is needed. Rather than using static or manually tuned\nthresholds, FedHLM employs FL to optimize these thresholds in a\nprivacy-preserving, distributed manner. Additionally, it leverages\nembedding-based token representations for Peer-to-Peer (P2P) resolution,\nenabling clients to reuse tokens inferred by semantically similar peers without\nengaging the LLM. We further introduce hierarchical model aggregation: edge\nservers refine local routing policies through client updates, while\ncross-cluster coordination aligns global decision boundaries. This layered\ndesign captures recurring uncertainty patterns, reducing redundant LLM queries.\nExperiments on large-scale news classification tasks show that FedHLM reduces\nLLM transmissions by over 95 percent with negligible accuracy loss, making it\nwell-suited for scalable and efficient edge-AI applications.", "AI": {"tldr": "FedHLM是一种通信高效的混合语言模型框架，结合了不确定性感知推理和联邦学习，通过动态优化阈值和P2P重用机制，显著减少LLM调用。", "motivation": "解决传统混合语言模型因频繁调用LLM导致的通信开销问题，特别是在带宽受限环境中。", "method": "集成不确定性感知推理与联邦学习，动态学习阈值；引入P2P重用和分层模型聚合。", "result": "在大规模新闻分类任务中，LLM传输减少95%以上，精度损失可忽略。", "conclusion": "FedHLM适用于高效、可扩展的边缘AI应用。"}}
{"id": "2507.00537", "pdf": "https://arxiv.org/pdf/2507.00537", "abs": "https://arxiv.org/abs/2507.00537", "authors": ["Feng Lin", "Marco Chen", "Haokui Zhang", "Xiaotian Yu", "Guangming Lu", "Rong Xiao"], "title": "Not All Attention Heads Are What You Need: Refining CLIP's Image Representation with Attention Ablation", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "21 pages, 7 figures", "summary": "This paper studies the role of attention heads in CLIP's image encoder. While\nCLIP has exhibited robust performance across diverse applications, we\nhypothesize that certain attention heads negatively affect final\nrepresentations and that ablating them can improve performance in downstream\ntasks. To capitalize on this insight, we propose a simple yet effective method,\ncalled Attention Ablation Technique (AAT), to suppress the contribution of\nspecific heads by manipulating attention weights. By integrating two\nalternative strategies tailored for different application scenarios, AAT\nsystematically identifies and ablates detrimental attention heads to enhance\nrepresentation quality. Experiments demonstrate that AAT consistently improves\ndownstream task performance across various domains, boosting recall rate by up\nto 11.1% on CLIP-family models for cross-modal retrieval. The results highlight\nthe potential of AAT to effectively refine large-scale vision-language models\nwith virtually no increase in inference cost.", "AI": {"tldr": "本文研究了CLIP图像编码器中注意力头的作用，提出了一种称为注意力消融技术（AAT）的方法，通过抑制特定头的贡献来提升下游任务性能。", "motivation": "尽管CLIP在多种应用中表现稳健，但某些注意力头可能对最终表示产生负面影响，消融这些头可以提升性能。", "method": "提出AAT方法，通过操纵注意力权重来抑制特定头的贡献，并结合两种策略以适应不同场景。", "result": "实验表明，AAT在跨模态检索等任务中显著提升性能，召回率最高提升11.1%。", "conclusion": "AAT能有效优化大规模视觉语言模型，且几乎不增加推理成本。"}}
{"id": "2507.00554", "pdf": "https://arxiv.org/pdf/2507.00554", "abs": "https://arxiv.org/abs/2507.00554", "authors": ["Zhenya Yang", "Bingchen Gong", "Kai Chen", "Qi Dou"], "title": "LOD-GS: Level-of-Detail-Sensitive 3D Gaussian Splatting for Detail Conserved Anti-Aliasing", "categories": ["cs.CV"], "comment": null, "summary": "Despite the advancements in quality and efficiency achieved by 3D Gaussian\nSplatting (3DGS) in 3D scene rendering, aliasing artifacts remain a persistent\nchallenge. Existing approaches primarily rely on low-pass filtering to mitigate\naliasing. However, these methods are not sensitive to the sampling rate, often\nresulting in under-filtering and over-smoothing renderings. To address this\nlimitation, we propose LOD-GS, a Level-of-Detail-sensitive filtering framework\nfor Gaussian Splatting, which dynamically predicts the optimal filtering\nstrength for each 3D Gaussian primitive. Specifically, we introduce a set of\nbasis functions to each Gaussian, which take the sampling rate as input to\nmodel appearance variations, enabling sampling-rate-sensitive filtering. These\nbasis function parameters are jointly optimized with the 3D Gaussian in an\nend-to-end manner. The sampling rate is influenced by both focal length and\ncamera distance. However, existing methods and datasets rely solely on\ndown-sampling to simulate focal length changes for anti-aliasing evaluation,\noverlooking the impact of camera distance. To enable a more comprehensive\nassessment, we introduce a new synthetic dataset featuring objects rendered at\nvarying camera distances. Extensive experiments on both public datasets and our\nnewly collected dataset demonstrate that our method achieves SOTA rendering\nquality while effectively eliminating aliasing. The code and dataset have been\nopen-sourced.", "AI": {"tldr": "论文提出LOD-GS框架，通过动态预测每个3D高斯基元的最优滤波强度，解决了3D高斯溅射中的混叠问题。", "motivation": "现有方法主要依赖低通滤波来缓解混叠，但对采样率不敏感，导致滤波不足或过度平滑。", "method": "引入一组基函数，以采样率为输入建模外观变化，实现采样率敏感的滤波，并与3D高斯联合优化。", "result": "在公开数据集和新合成数据集上，LOD-GS实现了SOTA渲染质量，有效消除混叠。", "conclusion": "LOD-GS框架显著提升了3D高斯溅射的抗混叠能力，代码和数据集已开源。"}}
{"id": "2507.00083", "pdf": "https://arxiv.org/pdf/2507.00083", "abs": "https://arxiv.org/abs/2507.00083", "authors": ["Wei Meng"], "title": "Strategic Counterfactual Modeling of Deep-Target Airstrike Systems via Intervention-Aware Spatio-Causal Graph Networks", "categories": ["cs.LG", "cs.AI", "91A80, 91B62, 68T07", "I.2.6; J.7; K.4.1; C.2.4"], "comment": "This paper proposes the first closed-loop causal modeling framework\n  (IA-STGNN) that links tactical strike variables to strategic delay outcomes\n  via graph neural networks with counterfactual reasoning", "summary": "This study addresses the lack of structured causal modeling between tactical\nstrike behavior and strategic delay in current strategic-level simulations,\nparticularly the structural bottlenecks in capturing intermediate variables\nwithin the \"resilience - nodal suppression - negotiation window\" chain. We\npropose the Intervention-Aware Spatio-Temporal Graph Neural Network (IA-STGNN),\na novel framework that closes the causal loop from tactical input to strategic\ndelay output. The model integrates graph attention mechanisms, counterfactual\nsimulation units, and spatial intervention node reconstruction to enable\ndynamic simulations of strike configurations and synchronization strategies.\nTraining data are generated from a multi-physics simulation platform (GEANT4 +\nCOMSOL) under NIST SP 800-160 standards, ensuring structural traceability and\npolicy-level validation. Experimental results demonstrate that IA-STGNN\nsignificantly outperforms baseline models (ST-GNN, GCN-LSTM, XGBoost),\nachieving a 12.8 percent reduction in MAE and 18.4 percent increase in Top-5\npercent accuracy, while improving causal path consistency and intervention\nstability. IA-STGNN enables interpretable prediction of strategic delay and\nsupports applications such as nuclear deterrence simulation, diplomatic window\nassessment, and multi-strategy optimization, providing a structured and\ntransparent AI decision-support mechanism for high-level policy modeling.", "AI": {"tldr": "该研究提出了一种名为IA-STGNN的新框架，用于解决战略级模拟中战术打击行为与战略延迟之间缺乏结构化因果建模的问题。", "motivation": "当前战略级模拟在捕捉‘韧性-节点压制-谈判窗口’链中的中间变量时存在结构性瓶颈，缺乏因果建模。", "method": "提出IA-STGNN框架，结合图注意力机制、反事实模拟单元和空间干预节点重建，实现动态模拟。训练数据来自多物理模拟平台（GEANT4 + COMSOL）。", "result": "IA-STGNN显著优于基线模型，MAE降低12.8%，Top-5准确率提升18.4%，同时提高了因果路径一致性和干预稳定性。", "conclusion": "IA-STGNN为高层政策建模提供了结构化且透明的AI决策支持机制，适用于核威慑模拟、外交窗口评估等应用。"}}
{"id": "2507.00234", "pdf": "https://arxiv.org/pdf/2507.00234", "abs": "https://arxiv.org/abs/2507.00234", "authors": ["Jiztom Kavalakkatt Francis", "Matthew J Darr"], "title": "Interpretable AI for Time-Series: Multi-Model Heatmap Fusion with Global Attention and NLP-Generated Explanations", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": "13 pages", "summary": "In this paper, we present a novel framework for enhancing model\ninterpretability by integrating heatmaps produced separately by ResNet and a\nrestructured 2D Transformer with globally weighted input saliency. We address\nthe critical problem of spatial-temporal misalignment in existing\ninterpretability methods, where convolutional networks fail to capture global\ncontext and Transformers lack localized precision - a limitation that impedes\nactionable insights in safety-critical domains like healthcare and industrial\nmonitoring. Our method merges gradient-weighted activation maps (ResNet) and\nTransformer attention rollout into a unified visualization, achieving full\nspatial-temporal alignment while preserving real-time performance. Empirical\nevaluations on clinical (ECG arrhythmia detection) and industrial (energy\nconsumption prediction) datasets demonstrate significant improvements: the\nhybrid framework achieves 94.1% accuracy (F1 0.93) on the PhysioNet dataset and\nreduces regression error to RMSE = 0.28 kWh (R2 = 0.95) on the UCI Energy\nAppliance dataset-outperforming standalone ResNet, Transformer, and\nInceptionTime baselines by 3.8-12.4%. An NLP module translates fused heatmaps\ninto domain-specific narratives (e.g., \"Elevated ST-segment between 2-4 seconds\nsuggests myocardial ischemia\"), validated via BLEU-4 (0.586) and ROUGE-L\n(0.650) scores. By formalizing interpretability as causal fidelity and\nspatial-temporal alignment, our approach bridges the gap between technical\noutputs and stakeholder understanding, offering a scalable solution for\ntransparent, time-aware decision-making.", "AI": {"tldr": "提出了一种结合ResNet和Transformer热图的新框架，解决时空对齐问题，提升模型可解释性，在医疗和工业领域表现优异。", "motivation": "现有可解释性方法存在时空错位问题，卷积网络缺乏全局上下文，Transformer缺乏局部精度，限制了在医疗和工业等关键领域的应用。", "method": "整合ResNet的梯度加权激活图和Transformer注意力展开，形成统一可视化，同时保持实时性能。", "result": "在PhysioNet和UCI Energy数据集上表现优异，准确率94.1%，F1 0.93，回归误差RMSE=0.28 kWh，R2=0.95。", "conclusion": "该方法通过时空对齐和因果保真，弥合技术输出与用户理解之间的差距，提供透明、实时的决策支持。"}}
{"id": "2507.00566", "pdf": "https://arxiv.org/pdf/2507.00566", "abs": "https://arxiv.org/abs/2507.00566", "authors": ["Kai Zhou", "Shuhai Zhang", "Zeng You", "Jinwu Hu", "Mingkui Tan", "Fei Liu"], "title": "Zero-shot Skeleton-based Action Recognition with Prototype-guided Feature Alignment", "categories": ["cs.CV"], "comment": "This paper is accepted by IEEE TIP 2025. Code is publicly available\n  at https://github.com/kaai520/PGFA", "summary": "Zero-shot skeleton-based action recognition aims to classify unseen\nskeleton-based human actions without prior exposure to such categories during\ntraining. This task is extremely challenging due to the difficulty in\ngeneralizing from known to unknown actions. Previous studies typically use\ntwo-stage training: pre-training skeleton encoders on seen action categories\nusing cross-entropy loss and then aligning pre-extracted skeleton and text\nfeatures, enabling knowledge transfer to unseen classes through skeleton-text\nalignment and language models' generalization. However, their efficacy is\nhindered by 1) insufficient discrimination for skeleton features, as the fixed\nskeleton encoder fails to capture necessary alignment information for effective\nskeleton-text alignment; 2) the neglect of alignment bias between skeleton and\nunseen text features during testing. To this end, we propose a prototype-guided\nfeature alignment paradigm for zero-shot skeleton-based action recognition,\ntermed PGFA. Specifically, we develop an end-to-end cross-modal contrastive\ntraining framework to improve skeleton-text alignment, ensuring sufficient\ndiscrimination for skeleton features. Additionally, we introduce a\nprototype-guided text feature alignment strategy to mitigate the adverse impact\nof the distribution discrepancy during testing. We provide a theoretical\nanalysis to support our prototype-guided text feature alignment strategy and\nempirically evaluate our overall PGFA on three well-known datasets. Compared\nwith the top competitor SMIE method, our PGFA achieves absolute accuracy\nimprovements of 22.96%, 12.53%, and 18.54% on the NTU-60, NTU-120, and PKU-MMD\ndatasets, respectively.", "AI": {"tldr": "PGFA提出了一种原型引导的特征对齐范式，用于零样本骨架动作识别，通过端到端跨模态对比训练和原型引导的文本特征对齐策略，显著提升了性能。", "motivation": "解决现有方法中骨架特征区分度不足和对齐偏差的问题，以提升零样本骨架动作识别的效果。", "method": "采用端到端跨模态对比训练框架和原型引导的文本特征对齐策略。", "result": "在NTU-60、NTU-120和PKU-MMD数据集上分别比SMIE方法提升了22.96%、12.53%和18.54%的准确率。", "conclusion": "PGFA通过改进特征对齐和减少分布差异，显著提升了零样本骨架动作识别的性能。"}}
{"id": "2507.00085", "pdf": "https://arxiv.org/pdf/2507.00085", "abs": "https://arxiv.org/abs/2507.00085", "authors": ["Ruiyuan Jiang", "Dongyao Jia", "Eng Gee Lim", "Pengfei Fan", "Yuli Zhang", "Shangbo Wang"], "title": "A Joint Topology-Data Fusion Graph Network for Robust Traffic Speed Prediction with Data Anomalism", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Accurate traffic prediction is essential for Intelligent Transportation\nSystems (ITS), yet current methods struggle with the inherent complexity and\nnon-linearity of traffic dynamics, making it difficult to integrate spatial and\ntemporal characteristics. Furthermore, existing approaches use static\ntechniques to address non-stationary and anomalous historical data, which\nlimits adaptability and undermines data smoothing. To overcome these\nchallenges, we propose the Graph Fusion Enhanced Network (GFEN), an innovative\nframework for network-level traffic speed prediction. GFEN introduces a novel\ntopological spatiotemporal graph fusion technique that meticulously extracts\nand merges spatial and temporal correlations from both data distribution and\nnetwork topology using trainable methods, enabling the modeling of multi-scale\nspatiotemporal features. Additionally, GFEN employs a hybrid methodology\ncombining a k-th order difference-based mathematical framework with an\nattention-based deep learning structure to adaptively smooth historical\nobservations and dynamically mitigate data anomalies and non-stationarity.\nExtensive experiments demonstrate that GFEN surpasses state-of-the-art methods\nby approximately 6.3% in prediction accuracy and exhibits convergence rates\nnearly twice as fast as recent hybrid models, confirming its superior\nperformance and potential to significantly enhance traffic prediction system\nefficiency.", "AI": {"tldr": "提出了一种名为GFEN的新框架，用于交通速度预测，通过结合空间和时间特征，并采用自适应方法处理数据异常和非平稳性，显著提升了预测精度和收敛速度。", "motivation": "现有方法难以处理交通数据的复杂性和非线性，且静态技术无法适应非平稳和异常数据，限制了预测的准确性和适应性。", "method": "GFEN采用拓扑时空图融合技术提取和融合空间与时间相关性，并结合k阶差分数学框架与注意力深度学习结构，自适应平滑数据并动态处理异常。", "result": "实验表明，GFEN在预测精度上优于现有方法约6.3%，收敛速度是近期混合模型的两倍。", "conclusion": "GFEN在交通预测中表现出卓越性能，有望显著提升智能交通系统的效率。"}}
{"id": "2507.00570", "pdf": "https://arxiv.org/pdf/2507.00570", "abs": "https://arxiv.org/abs/2507.00570", "authors": ["Zizhao Li", "Xueyang Kang", "Joseph West", "Kourosh Khoshelham"], "title": "Out-of-distribution detection in 3D applications: a review", "categories": ["cs.CV"], "comment": null, "summary": "The ability to detect objects that are not prevalent in the training set is a\ncritical capability in many 3D applications, including autonomous driving.\nMachine learning methods for object recognition often assume that all object\ncategories encountered during inference belong to a closed set of classes\npresent in the training data. This assumption limits generalization to the real\nworld, as objects not seen during training may be misclassified or entirely\nignored. As part of reliable AI, OOD detection identifies inputs that deviate\nsignificantly from the training distribution. This paper provides a\ncomprehensive overview of OOD detection within the broader scope of trustworthy\nand uncertain AI. We begin with key use cases across diverse domains, introduce\nbenchmark datasets spanning multiple modalities, and discuss evaluation\nmetrics. Next, we present a comparative analysis of OOD detection\nmethodologies, exploring model structures, uncertainty indicators, and\ndistributional distance taxonomies, alongside uncertainty calibration\ntechniques. Finally, we highlight promising research directions, including\nadversarially robust OOD detection and failure identification, particularly\nrelevant to 3D applications. The paper offers both theoretical and practical\ninsights into OOD detection, showcasing emerging research opportunities such as\n3D vision integration. These insights help new researchers navigate the field\nmore effectively, contributing to the development of reliable, safe, and robust\nAI systems.", "AI": {"tldr": "本文综述了OOD（分布外）检测在可信赖和不确定AI中的重要性，包括方法、评估指标及未来研究方向，特别关注3D应用。", "motivation": "解决训练数据中未出现类别的检测问题，提升AI在真实世界中的泛化能力，尤其是在自动驾驶等3D应用中。", "method": "通过比较OOD检测方法，分析模型结构、不确定性指标和分布距离分类，以及不确定性校准技术。", "result": "提供了OOD检测的全面概述，包括基准数据集、评估指标和未来研究方向。", "conclusion": "OOD检测是构建可靠、安全和鲁棒AI系统的关键，尤其在3D视觉集成等领域具有广阔前景。"}}
{"id": "2507.00087", "pdf": "https://arxiv.org/pdf/2507.00087", "abs": "https://arxiv.org/abs/2507.00087", "authors": ["Jiale Zhao", "Pengzhi Mao", "Kaifei Wang", "Yiming Li", "Yaping Peng", "Ranfei Chen", "Shuqi Lu", "Xiaohong Ji", "Jiaxiang Ding", "Xin Zhang", "Yucheng Liao", "Weinan E", "Weijie Zhang", "Han Wen", "Hao Chi"], "title": "pUniFind: a unified large pre-trained deep learning model pushing the limit of mass spectra interpretation", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Deep learning has advanced mass spectrometry data interpretation, yet most\nmodels remain feature extractors rather than unified scoring frameworks. We\npresent pUniFind, the first large-scale multimodal pre-trained model in\nproteomics that integrates end-to-end peptide-spectrum scoring with open,\nzero-shot de novo sequencing. Trained on over 100 million open search-derived\nspectra, pUniFind aligns spectral and peptide modalities via cross modality\nprediction and outperforms traditional engines across diverse datasets,\nparticularly achieving a 42.6 percent increase in the number of identified\npeptides in immunopeptidomics. Supporting over 1,300 modifications, pUniFind\nidentifies 60 percent more PSMs than existing de novo methods despite a\n300-fold larger search space. A deep learning based quality control module\nfurther recovers 38.5 percent additional peptides including 1,891 mapped to the\ngenome but absent from reference proteomes while preserving full fragment ion\ncoverage. These results establish a unified, scalable deep learning framework\nfor proteomic analysis, offering improved sensitivity, modification coverage,\nand interpretability.", "AI": {"tldr": "pUniFind是一个基于深度学习的统一评分框架，用于质谱数据分析，结合了端到端肽谱评分和零样本从头测序，显著提高了肽段鉴定数量和修饰覆盖范围。", "motivation": "当前深度学习模型多为特征提取器，缺乏统一的评分框架，pUniFind旨在填补这一空白，提升质谱数据解析的敏感性和可解释性。", "method": "pUniFind通过跨模态预测整合光谱和肽段模态，基于超过1亿个开放搜索衍生的光谱进行训练，支持超过1,300种修饰。", "result": "pUniFind在免疫肽组学中肽段鉴定数量提高了42.6%，PSMs数量比现有方法多60%，且通过质量控制模块额外恢复了38.5%的肽段。", "conclusion": "pUniFind为蛋白质组学分析提供了一个统一、可扩展的深度学习框架，显著提升了敏感性和修饰覆盖范围。"}}
{"id": "2507.00310", "pdf": "https://arxiv.org/pdf/2507.00310", "abs": "https://arxiv.org/abs/2507.00310", "authors": ["Dhruv Agarwal", "Bodhisattwa Prasad Majumder", "Reece Adamson", "Megha Chakravorty", "Satvika Reddy Gavireddy", "Aditya Parashar", "Harshit Surana", "Bhavana Dalvi Mishra", "Andrew McCallum", "Ashish Sabharwal", "Peter Clark"], "title": "Open-ended Scientific Discovery via Bayesian Surprise", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "The promise of autonomous scientific discovery (ASD) hinges not only on\nanswering questions, but also on knowing which questions to ask. Most recent\nworks in ASD explore the use of large language models (LLMs) in goal-driven\nsettings, relying on human-specified research questions to guide hypothesis\ngeneration. However, scientific discovery may be accelerated further by\nallowing the AI system to drive exploration by its own criteria. The few\nexisting approaches in open-ended ASD select hypotheses based on diversity\nheuristics or subjective proxies for human interestingness, but the former\nstruggles to meaningfully navigate the typically vast hypothesis space, and the\nlatter suffers from imprecise definitions. This paper presents AutoDS -- a\nmethod for open-ended ASD that instead drives scientific exploration using\nBayesian surprise. Here, we quantify the epistemic shift from the LLM's prior\nbeliefs about a hypothesis to its posterior beliefs after gathering\nexperimental results. To efficiently explore the space of nested hypotheses,\nour method employs a Monte Carlo tree search (MCTS) strategy with progressive\nwidening using surprisal as the reward function. We evaluate AutoDS in the\nsetting of data-driven discovery across 21 real-world datasets spanning domains\nsuch as biology, economics, finance, and behavioral science. Our results\ndemonstrate that under a fixed budget, AutoDS substantially outperforms\ncompetitors by producing 5--29\\% more discoveries deemed surprising by the LLM.\nOur human evaluation further finds that two-thirds of AutoDS discoveries are\nsurprising to the domain experts, suggesting this is an important step forward\ntowards building open-ended ASD systems.", "AI": {"tldr": "AutoDS提出了一种基于贝叶斯惊喜的开源自主动科学发现方法，通过蒙特卡洛树搜索策略优化假设探索，显著提升了发现效率。", "motivation": "现有方法依赖人工指定问题或主观兴趣度，难以有效导航庞大的假设空间。AutoDS旨在通过量化LLM的认知转变，实现更高效的科学探索。", "method": "AutoDS利用贝叶斯惊喜量化LLM从先验到后验的认知转变，采用蒙特卡洛树搜索（MCTS）策略，以惊喜为奖励函数进行假设探索。", "result": "在21个真实数据集上，AutoDS在固定预算下比竞争对手多产生5-29%的惊喜发现，且三分之二的发现被领域专家认为具有惊喜性。", "conclusion": "AutoDS是开源自主动科学发现的重要进展，通过量化认知转变和优化探索策略，显著提升了发现效率和惊喜性。"}}
{"id": "2507.00583", "pdf": "https://arxiv.org/pdf/2507.00583", "abs": "https://arxiv.org/abs/2507.00583", "authors": ["Christian Internò", "Robert Geirhos", "Markus Olhofer", "Sunny Liu", "Barbara Hammer", "David Klindt"], "title": "AI-Generated Video Detection via Perceptual Straightening", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The rapid advancement of generative AI enables highly realistic synthetic\nvideos, posing significant challenges for content authentication and raising\nurgent concerns about misuse. Existing detection methods often struggle with\ngeneralization and capturing subtle temporal inconsistencies. We propose\nReStraV(Representation Straightening Video), a novel approach to distinguish\nnatural from AI-generated videos. Inspired by the \"perceptual straightening\"\nhypothesis -- which suggests real-world video trajectories become more straight\nin neural representation domain -- we analyze deviations from this expected\ngeometric property. Using a pre-trained self-supervised vision transformer\n(DINOv2), we quantify the temporal curvature and stepwise distance in the\nmodel's representation domain. We aggregate statistics of these measures for\neach video and train a classifier. Our analysis shows that AI-generated videos\nexhibit significantly different curvature and distance patterns compared to\nreal videos. A lightweight classifier achieves state-of-the-art detection\nperformance (e.g., 97.17% accuracy and 98.63% AUROC on the VidProM benchmark),\nsubstantially outperforming existing image- and video-based methods. ReStraV is\ncomputationally efficient, it is offering a low-cost and effective detection\nsolution. This work provides new insights into using neural representation\ngeometry for AI-generated video detection.", "AI": {"tldr": "ReStraV利用神经表示几何学区分真实与AI生成视频，通过量化时间曲率和步长距离，实现高效检测。", "motivation": "生成式AI的快速发展导致合成视频难以辨别，现有方法在泛化和捕捉时间不一致性上表现不佳。", "method": "基于'感知拉直'假设，使用预训练视觉变换器分析视频的神经表示几何特性，训练分类器。", "result": "在VidProM基准测试中达到97.17%准确率和98.63% AUROC，显著优于现有方法。", "conclusion": "ReStraV为AI生成视频检测提供了低成本高效解决方案，并揭示了神经表示几何的新应用。"}}
{"id": "2507.00088", "pdf": "https://arxiv.org/pdf/2507.00088", "abs": "https://arxiv.org/abs/2507.00088", "authors": ["Alexandre S. Pires", "Laurens Samson", "Sennay Ghebreab", "Fernando P. Santos"], "title": "How large language models judge and influence human cooperation", "categories": ["physics.soc-ph", "cs.AI", "cs.SI"], "comment": null, "summary": "Humans increasingly rely on large language models (LLMs) to support decisions\nin social settings. Previous work suggests that such tools shape people's moral\nand political judgements. However, the long-term implications of LLM-based\nsocial decision-making remain unknown. How will human cooperation be affected\nwhen the assessment of social interactions relies on language models? This is a\npressing question, as human cooperation is often driven by indirect\nreciprocity, reputations, and the capacity to judge interactions of others.\nHere, we assess how state-of-the-art LLMs judge cooperative actions. We provide\n21 different LLMs with an extensive set of examples where individuals cooperate\n-- or refuse cooperating -- in a range of social contexts, and ask how these\ninteractions should be judged. Furthermore, through an evolutionary\ngame-theoretical model, we evaluate cooperation dynamics in populations where\nthe extracted LLM-driven judgements prevail, assessing the long-term impact of\nLLMs on human prosociality. We observe a remarkable agreement in evaluating\ncooperation against good opponents. On the other hand, we notice within- and\nbetween-model variance when judging cooperation with ill-reputed individuals.\nWe show that the differences revealed between models can significantly impact\nthe prevalence of cooperation. Finally, we test prompts to steer LLM norms,\nshowing that such interventions can shape LLM judgements, particularly through\ngoal-oriented prompts. Our research connects LLM-based advices and long-term\nsocial dynamics, and highlights the need to carefully align LLM norms in order\nto preserve human cooperation.", "AI": {"tldr": "研究探讨了大型语言模型（LLMs）对人类合作行为的影响，发现LLMs在评估合作行为时存在差异，可能影响长期合作动态。", "motivation": "随着人类越来越依赖LLMs支持社会决策，研究其长期对社会合作的影响变得迫切。", "method": "通过21种LLMs评估不同社交情境下的合作行为，并结合进化博弈论模型分析合作动态。", "result": "LLMs在评估合作行为时对良好对手表现一致，但对声誉不佳的个体存在差异，这些差异显著影响合作率。", "conclusion": "研究强调需要谨慎调整LLM规范，以维护人类合作行为。"}}
{"id": "2507.00316", "pdf": "https://arxiv.org/pdf/2507.00316", "abs": "https://arxiv.org/abs/2507.00316", "authors": ["Siyou Li", "Pengyao Qin", "Huanan Wu", "Dong Nie", "Arun J. Thirunavukarasu", "Juntao Yu", "Le Zhang"], "title": "$μ^2$Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for Radiology Report Generation", "categories": ["cs.LG", "cs.CL", "eess.IV"], "comment": "Accepted by MICCAI 2025", "summary": "Automated radiology report generation (RRG) aims to produce detailed textual\nreports from clinical imaging, such as computed tomography (CT) scans, to\nimprove the accuracy and efficiency of diagnosis and provision of management\nadvice. RRG is complicated by two key challenges: (1) inherent complexity in\nextracting relevant information from imaging data under resource constraints,\nand (2) difficulty in objectively evaluating discrepancies between\nmodel-generated and expert-written reports. To address these challenges, we\npropose $\\mu^2$LLM, a $\\underline{\\textbf{mu}}$ltiscale\n$\\underline{\\textbf{mu}}$ltimodal large language models for RRG tasks. The\nnovel ${\\mu}^2$Tokenizer, as an intermediate layer, integrates multi-modal\nfeatures from the multiscale visual tokenizer and the text tokenizer, then\nenhances report generation quality through direct preference optimization\n(DPO), guided by GREEN-RedLlama. Experimental results on four large CT\nimage-report medical datasetdemonstrate that our method outperforms existing\napproaches, highlighting the potential of our fine-tuned $\\mu^2$LLMs on limited\ndata for RRG tasks.", "AI": {"tldr": "论文提出了一种多尺度多模态大语言模型（μ²LLM）用于自动化放射学报告生成（RRG），通过μ²Tokenizer整合多模态特征，并利用直接偏好优化（DPO）提升报告质量，实验表明其在有限数据下优于现有方法。", "motivation": "自动化放射学报告生成（RRG）面临从复杂影像数据中提取信息及客观评估模型生成报告与专家报告差异的挑战，需改进现有方法。", "method": "提出μ²LLM模型，结合μ²Tokenizer整合多尺度视觉和文本特征，并通过DPO优化报告生成质量。", "result": "在四个大型CT图像-报告数据集上，μ²LLM表现优于现有方法，展示了其在有限数据下的潜力。", "conclusion": "μ²LLM通过多模态特征整合和优化策略，显著提升了RRG任务的性能，为临床诊断提供了高效工具。"}}
{"id": "2507.00585", "pdf": "https://arxiv.org/pdf/2507.00585", "abs": "https://arxiv.org/abs/2507.00585", "authors": ["Tang Hao", "Guo ZhiQing", "Wang LieJun", "Liu Chao"], "title": "Similarity Memory Prior is All You Need for Medical Image Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "In recent years, it has been found that \"grandmother cells\" in the primary\nvisual cortex (V1) of macaques can directly recognize visual input with complex\nshapes. This inspires us to examine the value of these cells in promoting the\nresearch of medical image segmentation. In this paper, we design a Similarity\nMemory Prior Network (Sim-MPNet) for medical image segmentation. Specifically,\nwe propose a Dynamic Memory Weights-Loss Attention (DMW-LA), which matches and\nremembers the category features of specific lesions or organs in medical images\nthrough the similarity memory prior in the prototype memory bank, thus helping\nthe network to learn subtle texture changes between categories. DMW-LA also\ndynamically updates the similarity memory prior in reverse through Weight-Loss\nDynamic (W-LD) update strategy, effectively assisting the network directly\nextract category features. In addition, we propose the Double-Similarity Global\nInternal Enhancement Module (DS-GIM) to deeply explore the internal differences\nin the feature distribution of input data through cosine similarity and\neuclidean distance. Extensive experiments on four public datasets show that\nSim-MPNet has better segmentation performance than other state-of-the-art\nmethods. Our code is available on https://github.com/vpsg-research/Sim-MPNet.", "AI": {"tldr": "论文提出Sim-MPNet，结合动态记忆权重损失注意力和双相似性全局内部增强模块，提升医学图像分割性能。", "motivation": "受猕猴初级视觉皮层中“祖母细胞”启发，探索其在医学图像分割中的应用价值。", "method": "设计Sim-MPNet，包括动态记忆权重损失注意力（DMW-LA）和双相似性全局内部增强模块（DS-GIM）。", "result": "在四个公开数据集上表现优于现有方法。", "conclusion": "Sim-MPNet通过相似性记忆先验和动态更新策略，有效提升医学图像分割性能。"}}
{"id": "2507.00090", "pdf": "https://arxiv.org/pdf/2507.00090", "abs": "https://arxiv.org/abs/2507.00090", "authors": ["Corbeau Michael", "Claeys Emmanuelle", "Serrurier Mathieu", "Zaraté Pascale"], "title": "Generating Heterogeneous Multi-dimensional Data : A Comparative Study", "categories": ["cs.LG", "cs.AI"], "comment": "accepted at IEEE SMC 2025 Vienna", "summary": "Allocation of personnel and material resources is highly sensible in the case\nof firefighter interventions. This allocation relies on simulations to\nexperiment with various scenarios. The main objective of this allocation is the\nglobal optimization of the firefighters response. Data generation is then\nmandatory to study various scenarios In this study, we propose to compare\ndifferent data generation methods. Methods such as Random Sampling, Tabular\nVariational Autoencoders, standard Generative Adversarial Networks, Conditional\nTabular Generative Adversarial Networks and Diffusion Probabilistic Models are\nexamined to ascertain their efficacy in capturing the intricacies of\nfirefighter interventions. Traditional evaluation metrics often fall short in\ncapturing the nuanced requirements of synthetic datasets for real-world\nscenarios. To address this gap, an evaluation of synthetic data quality is\nconducted using a combination of domain-specific metrics tailored to the\nfirefighting domain and standard measures such as the Wasserstein distance.\nDomain-specific metrics include response time distribution, spatial-temporal\ndistribution of interventions, and accidents representation. These metrics are\ndesigned to assess data variability, the preservation of fine and complex\ncorrelations and anomalies such as event with a very low occurrence, the\nconformity with the initial statistical distribution and the operational\nrelevance of the synthetic data. The distribution has the particularity of\nbeing highly unbalanced, none of the variables following a Gaussian\ndistribution, adding complexity to the data generation process.", "AI": {"tldr": "比较不同数据生成方法在消防员干预场景中的效果，提出领域特定指标评估合成数据质量。", "motivation": "优化消防员资源分配需依赖模拟实验，而数据生成是研究多种场景的关键。", "method": "比较随机采样、表格变分自编码器、生成对抗网络、条件表格生成对抗网络和扩散概率模型等方法。", "result": "传统评估指标不足以满足现实需求，提出结合领域特定指标（如响应时间分布）和标准指标（如Wasserstein距离）的综合评估。", "conclusion": "领域特定指标能更有效评估合成数据的质量，为消防资源分配提供更可靠的数据支持。"}}
{"id": "2507.00586", "pdf": "https://arxiv.org/pdf/2507.00586", "abs": "https://arxiv.org/abs/2507.00586", "authors": ["Luming Zhao", "Jingwen Xuan", "Jiamin Lou", "Yonghui Yu", "Wenwu Yang"], "title": "Context-Aware Academic Emotion Dataset and Benchmark", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025", "summary": "Academic emotion analysis plays a crucial role in evaluating students'\nengagement and cognitive states during the learning process. This paper\naddresses the challenge of automatically recognizing academic emotions through\nfacial expressions in real-world learning environments. While significant\nprogress has been made in facial expression recognition for basic emotions,\nacademic emotion recognition remains underexplored, largely due to the scarcity\nof publicly available datasets. To bridge this gap, we introduce RAER, a novel\ndataset comprising approximately 2,700 video clips collected from around 140\nstudents in diverse, natural learning contexts such as classrooms, libraries,\nlaboratories, and dormitories, covering both classroom sessions and individual\nstudy. Each clip was annotated independently by approximately ten annotators\nusing two distinct sets of academic emotion labels with varying granularity,\nenhancing annotation consistency and reliability. To our knowledge, RAER is the\nfirst dataset capturing diverse natural learning scenarios. Observing that\nannotators naturally consider context cues-such as whether a student is looking\nat a phone or reading a book-alongside facial expressions, we propose CLIP-CAER\n(CLIP-based Context-aware Academic Emotion Recognition). Our method utilizes\nlearnable text prompts within the vision-language model CLIP to effectively\nintegrate facial expression and context cues from videos. Experimental results\ndemonstrate that CLIP-CAER substantially outperforms state-of-the-art\nvideo-based facial expression recognition methods, which are primarily designed\nfor basic emotions, emphasizing the crucial role of context in accurately\nrecognizing academic emotions. Project page: https://zgsfer.github.io/CAER", "AI": {"tldr": "论文提出了一种基于CLIP的上下文感知学术情感识别方法（CLIP-CAER），并发布了首个涵盖多样化自然学习场景的学术情感数据集RAER。", "motivation": "学术情感分析对评估学生学习状态至关重要，但现有研究多关注基础情感，且缺乏公开数据集。", "method": "通过RAER数据集和CLIP-CAER方法，结合面部表情和上下文线索进行学术情感识别。", "result": "CLIP-CAER在学术情感识别上显著优于现有方法，验证了上下文的重要性。", "conclusion": "上下文信息对学术情感识别至关重要，RAER数据集和CLIP-CAER方法填补了研究空白。"}}
{"id": "2507.00093", "pdf": "https://arxiv.org/pdf/2507.00093", "abs": "https://arxiv.org/abs/2507.00093", "authors": ["Binghua Yao", "Joris M. Mooij"], "title": "$σ$-Maximal Ancestral Graphs", "categories": ["cs.DM", "cs.AI", "cs.DS", "math.ST", "stat.TH"], "comment": "It has beee accepted by the 41st Conference on Uncertainty in\n  Artificial Intelligence (UAI)", "summary": "Maximal Ancestral Graphs (MAGs) provide an abstract representation of\nDirected Acyclic Graphs (DAGs) with latent (selection) variables. These\ngraphical objects encode information about ancestral relations and\nd-separations of the DAGs they represent. This abstract representation has been\nused amongst others to prove the soundness and completeness of the FCI\nalgorithm for causal discovery, and to derive a do-calculus for its output. One\nsignificant inherent limitation of MAGs is that they rule out the possibility\nof cyclic causal relationships. In this work, we address that limitation. We\nintroduce and study a class of graphical objects that we coin\n''$\\sigma$-Maximal Ancestral Graphs'' (''$\\sigma$-MAGs''). We show how these\ngraphs provide an abstract representation of (possibly cyclic) Directed Graphs\n(DGs) with latent (selection) variables, analogously to how MAGs represent\nDAGs. We study the properties of these objects and provide a characterization\nof their Markov equivalence classes.", "AI": {"tldr": "本文介绍了σ-最大祖先图（σ-MAGs），扩展了传统MAGs的能力，使其能够表示包含潜在变量和循环因果关系的图结构。", "motivation": "传统MAGs无法表示循环因果关系，限制了其应用范围。本文旨在解决这一局限性。", "method": "提出并研究了σ-MAGs，分析了其性质，并给出了其马尔可夫等价类的特征。", "result": "σ-MAGs能够表示包含潜在变量和循环因果关系的图结构，扩展了传统MAGs的能力。", "conclusion": "σ-MAGs为研究循环因果关系提供了新的工具，扩展了因果发现的理论基础。"}}
{"id": "2507.00425", "pdf": "https://arxiv.org/pdf/2507.00425", "abs": "https://arxiv.org/abs/2507.00425", "authors": ["Ruixiang Zhang", "Shuangfei Zhai", "Jiatao Gu", "Yizhe Zhang", "Huangjie Zheng", "Tianrong Chen", "Miguel Angel Bautista", "Josh Susskind", "Navdeep Jaitly"], "title": "Flexible Language Modeling in Continuous Space with Transformer-based Autoregressive Flows", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Autoregressive models have driven remarkable progress in language modeling.\nTheir foundational reliance on discrete tokens, unidirectional context, and\nsingle-pass decoding, while central to their success, also inspires the\nexploration of a design space that could offer new axes of modeling\nflexibility. In this work, we explore an alternative paradigm, shifting\nlanguage modeling from a discrete token space to a continuous latent space. We\npropose a novel framework TarFlowLM, that employs transformer-based\nautoregressive normalizing flows to model these continuous representations.\nThis approach unlocks substantial flexibility, enabling the construction of\nmodels that can capture global bi-directional context through stacked,\nalternating-direction autoregressive transformations, support block-wise\ngeneration with flexible token patch sizes, and facilitate a hierarchical\nmulti-pass generation process. We further propose new mixture-based coupling\ntransformations designed to capture complex dependencies within the latent\nspace shaped by discrete data, and demonstrate theoretical connections to\nconventional discrete autoregressive models. Extensive experiments on language\nmodeling benchmarks demonstrate strong likelihood performance and highlight the\nflexible modeling capabilities inherent in our framework.", "AI": {"tldr": "论文提出了一种名为TarFlowLM的新框架，将语言建模从离散标记空间转移到连续潜在空间，利用基于Transformer的自回归归一化流模型，提供了更灵活的建模能力。", "motivation": "探索语言建模的新设计空间，突破传统自回归模型在离散标记、单向上下文和单次解码上的限制。", "method": "提出TarFlowLM框架，使用Transformer自回归归一化流模型连续潜在表示，支持双向上下文、块生成和多层次生成过程。", "result": "在语言建模基准测试中表现出色，展示了框架的灵活建模能力。", "conclusion": "TarFlowLM为语言建模提供了新的灵活性和性能提升，展示了连续潜在空间的潜力。"}}
{"id": "2507.00593", "pdf": "https://arxiv.org/pdf/2507.00593", "abs": "https://arxiv.org/abs/2507.00593", "authors": ["Fernando Alonso-Fernandez", "Talha Hanif Butt", "Prayag Tiwari"], "title": "Overtake Detection in Trucks Using CAN Bus Signals: A Comparative Study of Machine Learning Methods", "categories": ["cs.CV"], "comment": "Under review at ESWA", "summary": "Safe overtaking manoeuvres in trucks are vital for preventing accidents and\nensuring efficient traffic flow. Accurate prediction of such manoeuvres is\nessential for Advanced Driver Assistance Systems (ADAS) to make timely and\ninformed decisions. In this study, we focus on overtake detection using\nController Area Network (CAN) bus data collected from five in-service trucks\nprovided by the Volvo Group. We evaluate three common classifiers for vehicle\nmanoeuvre detection, Artificial Neural Networks (ANN), Random Forest (RF), and\nSupport Vector Machines (SVM), and analyse how different preprocessing\nconfigurations affect performance. We find that variability in traffic\nconditions strongly influences the signal patterns, particularly in the\nno-overtake class, affecting classification performance if training data lacks\nadequate diversity. Since the data were collected under unconstrained,\nreal-world conditions, class diversity cannot be guaranteed a priori. However,\ntraining with data from multiple vehicles improves generalisation and reduces\ncondition-specific bias. Our pertruck analysis also reveals that classification\naccuracy, especially for overtakes, depends on the amount of training data per\nvehicle. To address this, we apply a score-level fusion strategy, which yields\nthe best per-truck performance across most cases. Overall, we achieve an\naccuracy via fusion of TNR=93% (True Negative Rate) and TPR=86.5% (True\nPositive Rate). This research has been part of the BIG FUN project, which\nexplores how Artificial Intelligence can be applied to logged vehicle data to\nunderstand and predict driver behaviour, particularly in relation to Camera\nMonitor Systems (CMS), being introduced as digital replacements for traditional\nexterior mirrors.", "AI": {"tldr": "研究通过分析卡车CAN总线数据，评估三种分类器（ANN、RF、SVM）用于超车检测的性能，发现数据多样性和多车训练对提升分类准确性至关重要。", "motivation": "卡车安全超车对防止事故和提升交通效率至关重要，准确预测超车行为能为ADAS系统提供及时决策支持。", "method": "使用Volvo提供的五辆卡车CAN总线数据，评估三种分类器（ANN、RF、SVM），分析预处理配置对性能的影响，并采用分数级融合策略优化结果。", "result": "多车训练提升泛化能力，分数级融合策略在多数情况下表现最佳，最终实现TNR=93%和TPR=86.5%的准确率。", "conclusion": "数据多样性和多车训练是提升超车检测性能的关键，分数级融合策略显著优化了分类结果。"}}
{"id": "2507.00094", "pdf": "https://arxiv.org/pdf/2507.00094", "abs": "https://arxiv.org/abs/2507.00094", "authors": ["Jacobo Casas-Ramos", "Sarah Winkler", "Alessandro Gianola", "Marco Montali", "Manuel Mucientes", "Manuel Lama"], "title": "Efficient Conformance Checking of Rich Data-Aware Declare Specifications (Extended)", "categories": ["cs.DB", "cs.AI", "cs.PL"], "comment": "Extended version of the paper of the same title accepted at the 23rd\n  International Conference on Business Process Management (BPM 2025)", "summary": "Despite growing interest in process analysis and mining for data-aware\nspecifications, alignment-based conformance checking for declarative process\nmodels has focused on pure control-flow specifications, or mild data-aware\nextensions limited to numerical data and variable-to-constant comparisons. This\nis not surprising: finding alignments is computationally hard, even more so in\nthe presence of data dependencies. In this paper, we challenge this problem in\nthe case where the reference model is captured using data-aware Declare with\ngeneral data types and data conditions. We show that, unexpectedly, it is\npossible to compute data-aware optimal alignments in this rich setting,\nenjoying at once efficiency and expressiveness. This is achieved by carefully\ncombining the two best-known approaches to deal with control flow and data\ndependencies when computing alignments, namely A* search and SMT solving.\nSpecifically, we introduce a novel algorithmic technique that efficiently\nexplores the search space, generating descendant states through the application\nof repair actions aiming at incrementally resolving constraint violations. We\nprove the correctness of our algorithm and experimentally show its efficiency.\nThe evaluation witnesses that our approach matches or surpasses the performance\nof the state of the art while also supporting significantly more expressive\ndata dependencies, showcasing its potential to support real-world applications.", "AI": {"tldr": "该论文提出了一种新颖的算法，用于在数据感知的Declare模型中计算最优对齐，结合了A*搜索和SMT求解技术，实现了高效性和表达能力的平衡。", "motivation": "尽管对数据感知规范的过程分析和挖掘兴趣增长，但现有的对齐方法主要局限于控制流或简单的数值数据比较。本文旨在解决在更复杂的数据条件下计算对齐的挑战。", "method": "通过结合A*搜索和SMT求解技术，提出了一种新算法，通过逐步修复约束违规来高效探索搜索空间。", "result": "实验证明，该方法在性能上匹配或超越现有技术，同时支持更复杂的数据依赖关系。", "conclusion": "该算法在高效性和表达能力上取得了显著进展，展示了其在现实应用中的潜力。"}}
{"id": "2507.00603", "pdf": "https://arxiv.org/pdf/2507.00603", "abs": "https://arxiv.org/abs/2507.00603", "authors": ["Yupeng Zheng", "Pengxuan Yang", "Zebin Xing", "Qichao Zhang", "Yuhang Zheng", "Yinfeng Gao", "Pengfei Li", "Teng Zhang", "Zhongpu Xia", "Peng Jia", "Dongbin Zhao"], "title": "World4Drive: End-to-End Autonomous Driving via Intention-aware Physical Latent World Model", "categories": ["cs.CV"], "comment": "ICCV 2025, first version", "summary": "End-to-end autonomous driving directly generates planning trajectories from\nraw sensor data, yet it typically relies on costly perception supervision to\nextract scene information. A critical research challenge arises: constructing\nan informative driving world model to enable perception annotation-free,\nend-to-end planning via self-supervised learning. In this paper, we present\nWorld4Drive, an end-to-end autonomous driving framework that employs vision\nfoundation models to build latent world models for generating and evaluating\nmulti-modal planning trajectories. Specifically, World4Drive first extracts\nscene features, including driving intention and world latent representations\nenriched with spatial-semantic priors provided by vision foundation models. It\nthen generates multi-modal planning trajectories based on current scene\nfeatures and driving intentions and predicts multiple intention-driven future\nstates within the latent space. Finally, it introduces a world model selector\nmodule to evaluate and select the best trajectory. We achieve perception\nannotation-free, end-to-end planning through self-supervised alignment between\nactual future observations and predicted observations reconstructed from the\nlatent space. World4Drive achieves state-of-the-art performance without manual\nperception annotations on both the open-loop nuScenes and closed-loop NavSim\nbenchmarks, demonstrating an 18.1\\% relative reduction in L2 error, 46.7% lower\ncollision rate, and 3.75 faster training convergence. Codes will be accessed at\nhttps://github.com/ucaszyp/World4Drive.", "AI": {"tldr": "World4Drive提出了一种无需感知标注的端到端自动驾驶框架，通过自监督学习构建驾驶世界模型，生成多模态规划轨迹并选择最优路径。", "motivation": "解决传统端到端自动驾驶依赖昂贵感知标注的问题，构建自监督学习驱动的驾驶世界模型。", "method": "利用视觉基础模型提取场景特征（意图和潜在表示），生成多模态轨迹，并通过世界模型选择器评估最优路径。", "result": "在nuScenes和NavSim基准测试中表现优异，L2误差降低18.1%，碰撞率减少46.7%，训练收敛速度提升3.75倍。", "conclusion": "World4Drive实现了无需感知标注的高效端到端规划，展示了自监督学习在自动驾驶中的潜力。"}}
{"id": "2507.00096", "pdf": "https://arxiv.org/pdf/2507.00096", "abs": "https://arxiv.org/abs/2507.00096", "authors": ["Ailiya Borjigin", "Wei Zhou", "Cong He"], "title": "AI-Governed Agent Architecture for Web-Trustworthy Tokenization of Alternative Assets", "categories": ["cs.CR", "cs.AI"], "comment": "8 Pages, 1 figure", "summary": "Alternative Assets tokenization is transforming non-traditional financial\ninstruments are represented and traded on the web. However, ensuring\ntrustworthiness in web-based tokenized ecosystems poses significant challenges,\nfrom verifying off-chain asset data to enforcing regulatory compliance. This\npaper proposes an AI-governed agent architecture that integrates intelligent\nagents with blockchain to achieve web-trustworthy tokenization of alternative\nassets. In the proposed architecture, autonomous agents orchestrate the\ntokenization process (asset verification, valuation, compliance checking, and\nlifecycle management), while an AI-driven governance layer monitors agent\nbehavior and enforces trust through adaptive policies and cryptoeconomic\nincentives. We demonstrate that this approach enhances transparency, security,\nand compliance in asset tokenization, addressing key concerns around data\nauthenticity and fraud. A case study on tokenizing real estate assets\nillustrates how the architecture mitigates risks (e.g., fraudulent listings and\nmoney laundering) through real-time AI anomaly detection and on-chain\nenforcement. Our evaluation and analysis suggest that combining AI governance\nwith multi-agent systems and blockchain can significantly bolster trust in\ntokenized asset ecosystems. This work offers a novel framework for trustworthy\nasset tokenization on the web and provides insights for practitioners aiming to\ndeploy secure, compliant tokenization platforms.", "AI": {"tldr": "论文提出了一种结合AI治理和多智能体系统的区块链架构，用于实现可信的替代资产代币化，提升透明度、安全性和合规性。", "motivation": "解决基于网络的代币化生态系统中信任问题，如链下资产数据验证和监管合规性。", "method": "提出AI治理的智能体架构，整合区块链和智能体，实现资产验证、估值、合规检查和生命周期管理。", "result": "通过案例研究（房地产代币化）证明该架构能有效降低欺诈和洗钱风险，提升数据真实性和信任。", "conclusion": "结合AI治理、多智能体系统和区块链，显著增强代币化资产生态系统的信任，为实践者提供安全合规的框架。"}}
{"id": "2507.00449", "pdf": "https://arxiv.org/pdf/2507.00449", "abs": "https://arxiv.org/abs/2507.00449", "authors": ["Zhihao Zhan", "Jianan Zhao", "Zhaocheng Zhu", "Jian Tang"], "title": "Overcoming Long-Context Limitations of State-Space Models via Context-Dependent Sparse Attention", "categories": ["cs.LG", "cs.CL", "I.2.7"], "comment": "Proceedings of the 42nd International Conference on Machine Learning,\n  ES-FoMo III: 3rd Workshop on Efficient Systems for Foundation Models, 18\n  pages, 9 figures", "summary": "Efficient long-context modeling remains a critical challenge for natural\nlanguage processing (NLP), as the time complexity of the predominant\nTransformer architecture scales quadratically with the sequence length. While\nstate-space models (SSMs) offer alternative sub-quadratic solutions, they\nstruggle to capture long-range dependencies effectively. In this work, we focus\non analyzing and improving the long-context modeling capabilities of SSMs. We\nshow that the widely used synthetic task, associative recall, which requires a\nmodel to recall a value associated with a single key without context,\ninsufficiently represents the complexities of real-world long-context modeling.\nTo address this limitation, we extend the associative recall to a novel\nsynthetic task, \\emph{joint recall}, which requires a model to recall the value\nassociated with a key given in a specified context. Theoretically, we prove\nthat SSMs do not have the expressiveness to solve multi-query joint recall in\nsub-quadratic time complexity. To resolve this issue, we propose a solution\nbased on integrating SSMs with Context-Dependent Sparse Attention (CDSA), which\nhas the expressiveness to solve multi-query joint recall with sub-quadratic\ncomputation. To bridge the gap between theoretical analysis and real-world\napplications, we propose locality-sensitive Hashing Attention with sparse Key\nSelection (HAX), which instantiates the theoretical solution and is further\ntailored to natural language domains. Extensive experiments on both synthetic\nand real-world long-context benchmarks show that HAX consistently outperforms\nSSM baselines and SSMs integrated with context-independent sparse attention\n(CISA).", "AI": {"tldr": "论文分析了状态空间模型（SSMs）在长上下文建模中的局限性，并提出了一种结合上下文依赖稀疏注意力（CDSA）的解决方案，实验证明其优于现有方法。", "motivation": "长上下文建模是自然语言处理（NLP）的关键挑战，而现有方法如Transformer和SSMs在效率和效果上存在不足。", "method": "提出了一种新的合成任务“联合回忆”，并证明了SSMs的局限性；随后提出结合CDSA的解决方案，并进一步设计了HAX方法。", "result": "实验表明，HAX在合成和真实长上下文任务中均优于SSMs及其变体。", "conclusion": "通过理论分析和实际应用结合，论文为长上下文建模提供了一种高效且有效的解决方案。"}}
{"id": "2507.00608", "pdf": "https://arxiv.org/pdf/2507.00608", "abs": "https://arxiv.org/abs/2507.00608", "authors": ["Zehua Fu", "Chenguang Liu", "Yuyu Chen", "Jiaqi Zhou", "Qingjie Liu", "Yunhong Wang"], "title": "De-Simplifying Pseudo Labels to Enhancing Domain Adaptive Object Detection", "categories": ["cs.CV"], "comment": "Accepted by IEEE Transactions on Intelligent Transportation Systems.\n  15 pages, 10 figures", "summary": "Despite its significant success, object detection in traffic and\ntransportation scenarios requires time-consuming and laborious efforts in\nacquiring high-quality labeled data. Therefore, Unsupervised Domain Adaptation\n(UDA) for object detection has recently gained increasing research attention.\nUDA for object detection has been dominated by domain alignment methods, which\nachieve top performance. Recently, self-labeling methods have gained popularity\ndue to their simplicity and efficiency. In this paper, we investigate the\nlimitations that prevent self-labeling detectors from achieving commensurate\nperformance with domain alignment methods. Specifically, we identify the high\nproportion of simple samples during training, i.e., the simple-label bias, as\nthe central cause. We propose a novel approach called De-Simplifying Pseudo\nLabels (DeSimPL) to mitigate the issue. DeSimPL utilizes an instance-level\nmemory bank to implement an innovative pseudo label updating strategy. Then,\nadversarial samples are introduced during training to enhance the proportion.\nFurthermore, we propose an adaptive weighted loss to avoid the model suffering\nfrom an abundance of false positive pseudo labels in the late training period.\nExperimental results demonstrate that DeSimPL effectively reduces the\nproportion of simple samples during training, leading to a significant\nperformance improvement for self-labeling detectors. Extensive experiments\nconducted on four benchmarks validate our analysis and conclusions.", "AI": {"tldr": "论文提出DeSimPL方法，通过减少训练中简单样本的比例，提升自标记检测器的性能。", "motivation": "交通场景中目标检测需要大量高质量标注数据，而自标记方法因简单高效受到关注，但其性能不及域对齐方法。研究发现简单样本比例过高（简单标签偏差）是主要原因。", "method": "提出DeSimPL方法，利用实例级记忆库更新伪标签策略，引入对抗样本增加样本多样性，并采用自适应加权损失避免后期训练中的假阳性伪标签问题。", "result": "实验表明DeSimPL有效减少简单样本比例，显著提升自标记检测器性能，四个基准测试验证了其有效性。", "conclusion": "DeSimPL通过解决简单标签偏差问题，为自标记检测器提供了性能提升的有效途径。"}}
{"id": "2507.00102", "pdf": "https://arxiv.org/pdf/2507.00102", "abs": "https://arxiv.org/abs/2507.00102", "authors": ["Bernd Hofmann", "Patrick Bruendl", "Huong Giang Nguyen", "Joerg Franke"], "title": "Towards transparent and data-driven fault detection in manufacturing: A case study on univariate, discrete time series", "categories": ["cs.LG", "cs.AI", "eess.SP"], "comment": null, "summary": "Ensuring consistent product quality in modern manufacturing is crucial,\nparticularly in safety-critical applications. Conventional quality control\napproaches, reliant on manually defined thresholds and features, lack\nadaptability to the complexity and variability inherent in production data and\nnecessitate extensive domain expertise. Conversely, data-driven methods, such\nas machine learning, demonstrate high detection performance but typically\nfunction as black-box models, thereby limiting their acceptance in industrial\nenvironments where interpretability is paramount. This paper introduces a\nmethodology for industrial fault detection, which is both data-driven and\ntransparent. The approach integrates a supervised machine learning model for\nmulti-class fault classification, Shapley Additive Explanations for post-hoc\ninterpretability, and a do-main-specific visualisation technique that maps\nmodel explanations to operator-interpretable features. Furthermore, the study\nproposes an evaluation methodology that assesses model explanations through\nquantitative perturbation analysis and evaluates visualisations by qualitative\nexpert assessment. The approach was applied to the crimping process, a\nsafety-critical joining technique, using a dataset of univariate, discrete time\nseries. The system achieves a fault detection accuracy of 95.9 %, and both\nquantitative selectivity analysis and qualitative expert evaluations confirmed\nthe relevance and inter-pretability of the generated explanations. This\nhuman-centric approach is designed to enhance trust and interpretability in\ndata-driven fault detection, thereby contributing to applied system design in\nindustrial quality control.", "AI": {"tldr": "论文提出了一种结合数据驱动和透明性的工业故障检测方法，通过机器学习、SHAP解释和可视化技术，实现了高准确性和可解释性。", "motivation": "传统质量控制方法依赖人工定义阈值，缺乏适应性；数据驱动方法虽高效但缺乏可解释性，限制了工业应用。", "method": "结合监督学习模型、SHAP解释和领域特定可视化技术，提出定量扰动分析和专家评估方法。", "result": "故障检测准确率达95.9%，定量和定性评估均验证了方法的有效性和可解释性。", "conclusion": "该方法提升了数据驱动故障检测的可信度和可解释性，适用于工业质量控制。"}}
{"id": "2507.00466", "pdf": "https://arxiv.org/pdf/2507.00466", "abs": "https://arxiv.org/abs/2507.00466", "authors": ["Sebastian Murgul", "Michael Heizmann"], "title": "Beat and Downbeat Tracking in Performance MIDI Using an End-to-End Transformer Architecture", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "comment": "Accepted to the 22nd Sound and Music Computing Conference (SMC), 2025", "summary": "Beat tracking in musical performance MIDI is a challenging and important task\nfor notation-level music transcription and rhythmical analysis, yet existing\nmethods primarily focus on audio-based approaches. This paper proposes an\nend-to-end transformer-based model for beat and downbeat tracking in\nperformance MIDI, leveraging an encoder-decoder architecture for\nsequence-to-sequence translation of MIDI input to beat annotations. Our\napproach introduces novel data preprocessing techniques, including dynamic\naugmentation and optimized tokenization strategies, to improve accuracy and\ngeneralizability across different datasets. We conduct extensive experiments\nusing the A-MAPS, ASAP, GuitarSet, and Leduc datasets, comparing our model\nagainst state-of-the-art hidden Markov models (HMMs) and deep learning-based\nbeat tracking methods. The results demonstrate that our model outperforms\nexisting symbolic music beat tracking approaches, achieving competitive\nF1-scores across various musical styles and instruments. Our findings highlight\nthe potential of transformer architectures for symbolic beat tracking and\nsuggest future integration with automatic music transcription systems for\nenhanced music analysis and score generation.", "AI": {"tldr": "本文提出了一种基于Transformer的端到端模型，用于MIDI表演中的节拍和强拍跟踪，通过新颖的数据预处理技术和优化的架构，在多个数据集中表现优于现有方法。", "motivation": "现有方法主要关注基于音频的节拍跟踪，而MIDI表演中的节拍跟踪在音乐转录和节奏分析中具有重要价值。", "method": "采用编码器-解码器架构的Transformer模型，结合动态增强和优化的标记化策略进行数据预处理。", "result": "在A-MAPS、ASAP、GuitarSet和Leduc数据集上，模型优于隐马尔可夫模型和深度学习方法，F1分数表现优异。", "conclusion": "Transformer架构在符号音乐节拍跟踪中具有潜力，未来可与自动音乐转录系统结合以提升音乐分析和乐谱生成。"}}
{"id": "2507.00648", "pdf": "https://arxiv.org/pdf/2507.00648", "abs": "https://arxiv.org/abs/2507.00648", "authors": ["Siyuan Yao", "Rui Zhu", "Ziqi Wang", "Wenqi Ren", "Yanyang Yan", "Xiaochun Cao"], "title": "UMDATrack: Unified Multi-Domain Adaptive Tracking Under Adverse Weather Conditions", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025", "summary": "Visual object tracking has gained promising progress in past decades. Most of\nthe existing approaches focus on learning target representation in\nwell-conditioned daytime data, while for the unconstrained real-world scenarios\nwith adverse weather conditions, e.g. nighttime or foggy environment, the\ntremendous domain shift leads to significant performance degradation. In this\npaper, we propose UMDATrack, which is capable of maintaining high-quality\ntarget state prediction under various adverse weather conditions within a\nunified domain adaptation framework. Specifically, we first use a controllable\nscenario generator to synthesize a small amount of unlabeled videos (less than\n2% frames in source daytime datasets) in multiple weather conditions under the\nguidance of different text prompts. Afterwards, we design a simple yet\neffective domain-customized adapter (DCA), allowing the target objects'\nrepresentation to rapidly adapt to various weather conditions without redundant\nmodel updating. Furthermore, to enhance the localization consistency between\nsource and target domains, we propose a target-aware confidence alignment\nmodule (TCA) following optimal transport theorem. Extensive experiments\ndemonstrate that UMDATrack can surpass existing advanced visual trackers and\nlead new state-of-the-art performance by a significant margin. Our code is\navailable at https://github.com/Z-Z188/UMDATrack.", "AI": {"tldr": "UMDATrack提出了一种统一域适应框架，能够在恶劣天气条件下保持高质量的目标状态预测，通过可控场景生成器和域定制适配器实现快速适应。", "motivation": "现有视觉目标跟踪方法主要针对良好条件的白天数据，而在恶劣天气条件下性能显著下降，因此需要一种能够适应多种天气条件的解决方案。", "method": "使用可控场景生成器合成少量未标记视频，设计域定制适配器（DCA）快速适应不同天气条件，并提出目标感知置信对齐模块（TCA）增强定位一致性。", "result": "UMDATrack在实验中显著超越现有先进视觉跟踪器，达到新的最优性能。", "conclusion": "UMDATrack通过统一域适应框架，有效解决了恶劣天气条件下的目标跟踪问题，性能显著提升。"}}
{"id": "2507.00108", "pdf": "https://arxiv.org/pdf/2507.00108", "abs": "https://arxiv.org/abs/2507.00108", "authors": ["Clemente Rubio-Manzano", "Jazna Meza", "Rodolfo Fernandez-Santibanez", "Christian Vidal-Castro"], "title": "Teaching Programming in the Age of Generative AI: Insights from Literature, Pedagogical Proposals, and Student Perspectives", "categories": ["cs.CY", "cs.AI", "cs.ET", "cs.PL"], "comment": null, "summary": "Computer programming is undergoing a true transformation driven by powerful\nnew tools for automatic source code generation based on large language models.\nThis transformation is also manifesting in introductory programming courses at\nuniversities around the world, generating an in-depth debate about how\nprogramming content should be taught, learned, and assessed in the context of\ngenerative artificial intelligence.\n  This article aims, on the one hand, to review the most relevant studies on\nthis issue, highlighting the advantages and disadvantages identified in the\nspecialized literature. On the other hand, it proposes enriching teaching and\nlearning methodologies by focusing on code comprehension and execution rather\nthan on mere coding or program functionality. In particular, it advocates for\nthe use of visual representations of code and visual simulations of its\nexecution as effective tools for teaching, learning, and assessing programming,\nthus fostering a deeper understanding among students.\n  Finally, the opinions of students who took the object-oriented programming\ncourse are presented to provide preliminary context supporting the\nincorporation of visual simulations in Java (or other languages) as part of the\ntraining process.", "AI": {"tldr": "本文探讨了生成式人工智能对编程教育的变革，提出通过代码理解和执行（而非单纯编码）来改进教学方法，并推荐使用可视化工具。", "motivation": "研究生成式AI对编程教育的影响，探索更有效的教学方法。", "method": "综述相关研究，提出基于代码理解和可视化的教学方法，并结合学生反馈验证。", "result": "可视化工具能提升学生对代码的理解，学生反馈支持其纳入教学。", "conclusion": "建议在编程教学中融入可视化工具，以深化学生对代码的理解和执行能力。"}}
{"id": "2507.00487", "pdf": "https://arxiv.org/pdf/2507.00487", "abs": "https://arxiv.org/abs/2507.00487", "authors": ["Jianghao Lin", "Xinyuan Wang", "Xinyi Dai", "Menghui Zhu", "Bo Chen", "Ruiming Tang", "Yong Yu", "Weinan Zhang"], "title": "MassTool: A Multi-Task Search-Based Tool Retrieval Framework for Large Language Models", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Tool retrieval is a critical component in enabling large language models\n(LLMs) to interact effectively with external tools. It aims to precisely filter\nthe massive tools into a small set of candidates for the downstream\ntool-augmented LLMs. However, most existing approaches primarily focus on\noptimizing tool representations, often neglecting the importance of precise\nquery comprehension. To address this gap, we introduce MassTool, a multi-task\nsearch-based framework designed to enhance both query representation and tool\nretrieval accuracy. MassTool employs a two-tower architecture: a tool usage\ndetection tower that predicts the need for function calls, and a tool retrieval\ntower that leverages a query-centric graph convolution network (QC-GCN) for\neffective query-tool matching. It also incorporates search-based user intent\nmodeling (SUIM) to handle diverse and out-of-distribution queries, alongside an\nadaptive knowledge transfer (AdaKT) module for efficient multi-task learning.\nBy jointly optimizing tool usage detection loss, list-wise retrieval loss, and\ncontrastive regularization loss, MassTool establishes a robust dual-step\nsequential decision-making pipeline for precise query understanding. Extensive\nexperiments demonstrate its effectiveness in improving retrieval accuracy. Our\ncode is available at https://github.com/wxydada/MassTool.", "AI": {"tldr": "MassTool是一个多任务搜索框架，通过两塔架构和搜索驱动的用户意图建模，提升工具检索的准确性。", "motivation": "现有方法主要优化工具表示，忽视了精确查询理解的重要性，MassTool旨在填补这一空白。", "method": "采用两塔架构（工具使用检测塔和工具检索塔），结合QC-GCN、SUIM和AdaKT模块，多任务联合优化。", "result": "实验证明MassTool显著提高了检索准确性。", "conclusion": "MassTool通过精确查询理解和多任务学习，为工具检索提供了有效解决方案。"}}
{"id": "2507.00659", "pdf": "https://arxiv.org/pdf/2507.00659", "abs": "https://arxiv.org/abs/2507.00659", "authors": ["Juelin Zhu", "Shuaibang Peng", "Long Wang", "Hanlin Tan", "Yu Liu", "Maojun Zhang", "Shen Yan"], "title": "LoD-Loc v2: Aerial Visual Localization over Low Level-of-Detail City Models using Explicit Silhouette Alignment", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "We propose a novel method for aerial visual localization over low\nLevel-of-Detail (LoD) city models. Previous wireframe-alignment-based method\nLoD-Loc has shown promising localization results leveraging LoD models.\nHowever, LoD-Loc mainly relies on high-LoD (LoD3 or LoD2) city models, but the\nmajority of available models and those many countries plan to construct\nnationwide are low-LoD (LoD1). Consequently, enabling localization on low-LoD\ncity models could unlock drones' potential for global urban localization. To\naddress these issues, we introduce LoD-Loc v2, which employs a coarse-to-fine\nstrategy using explicit silhouette alignment to achieve accurate localization\nover low-LoD city models in the air. Specifically, given a query image, LoD-Loc\nv2 first applies a building segmentation network to shape building silhouettes.\nThen, in the coarse pose selection stage, we construct a pose cost volume by\nuniformly sampling pose hypotheses around a prior pose to represent the pose\nprobability distribution. Each cost of the volume measures the degree of\nalignment between the projected and predicted silhouettes. We select the pose\nwith maximum value as the coarse pose. In the fine pose estimation stage, a\nparticle filtering method incorporating a multi-beam tracking approach is used\nto efficiently explore the hypothesis space and obtain the final pose\nestimation. To further facilitate research in this field, we release two\ndatasets with LoD1 city models covering 10.7 km , along with real RGB queries\nand ground-truth pose annotations. Experimental results show that LoD-Loc v2\nimproves estimation accuracy with high-LoD models and enables localization with\nlow-LoD models for the first time. Moreover, it outperforms state-of-the-art\nbaselines by large margins, even surpassing texture-model-based methods, and\nbroadens the convergence basin to accommodate larger prior errors.", "AI": {"tldr": "LoD-Loc v2是一种新颖的空中视觉定位方法，适用于低细节（LoD1）城市模型，通过粗到细的策略和显式轮廓对齐实现高精度定位。", "motivation": "现有方法主要依赖高细节（LoD3或LoD2）城市模型，但全球范围内多数可用或计划建设的模型为低细节（LoD1），限制了无人机在全球城市定位中的应用。", "method": "LoD-Loc v2采用粗到细策略：首先通过建筑分割网络提取轮廓，然后在粗姿态选择阶段构建姿态成本体积，最后通过粒子滤波和多光束跟踪优化姿态。", "result": "实验表明，LoD-Loc v2在高细节模型上提升了精度，首次实现了低细节模型的定位，且性能优于现有方法。", "conclusion": "LoD-Loc v2扩展了定位模型的适用范围，提升了精度和鲁棒性，为无人机全球城市定位提供了新可能。"}}
{"id": "2507.00145", "pdf": "https://arxiv.org/pdf/2507.00145", "abs": "https://arxiv.org/abs/2507.00145", "authors": ["Hasan Yiğit"], "title": "AI-Hybrid TRNG: Kernel-Based Deep Learning for Near-Uniform Entropy Harvesting from Physical Noise", "categories": ["cs.CR", "cs.AI", "cs.ET", "cs.IT", "eess.SP", "math.IT"], "comment": null, "summary": "AI-Hybrid TRNG is a deep-learning framework that extracts near-uniform\nentropy directly from physical noise, eliminating the need for bulky quantum\ndevices or expensive laboratory-grade RF receivers. Instead, it relies on a\nlow-cost, thumb-sized RF front end, plus CPU-timing jitter, for training, and\nthen emits 32-bit high-entropy streams without any quantization step.\n  Unlike deterministic or trained artificial intelligence random number\ngenerators (RNGs), our dynamic inner-outer network couples adaptive natural\nsources and reseeding, yielding truly unpredictable and autonomous sequences.\nGenerated numbers pass the NIST SP 800-22 battery better than a CPU-based\nmethod. It also passes nineteen bespoke statistical tests for both bit- and\ninteger-level analysis. All results satisfy cryptographic standards, while\nforward and backward prediction experiments reveal no exploitable biases. The\nmodel's footprint is below 0.5 MB, making it deployable on MCUs and FPGA soft\ncores, as well as suitable for other resource-constrained platforms.\n  By detaching randomness quality from dedicated hardware, AI-Hybrid TRNG\nbroadens the reach of high-integrity random number generators across secure\nsystems, cryptographic protocols, embedded and edge devices, stochastic\nsimulations, and server applications that need randomness.", "AI": {"tldr": "AI-Hybrid TRNG是一个深度学习框架，直接从物理噪声中提取接近均匀的熵，无需量子设备或昂贵的射频接收器。它通过低成本射频前端和CPU时序抖动生成高熵32位流，并通过NIST SP 800-22测试。", "motivation": "传统随机数生成器依赖专用硬件，成本高且体积大。AI-Hybrid TRNG旨在通过深度学习框架，利用低成本硬件实现高质量的随机数生成。", "method": "采用动态内外网络结构，结合自适应自然源和重新播种技术，生成不可预测的随机序列。无需量化步骤，直接输出高熵流。", "result": "生成的随机数通过NIST SP 800-22测试和19项定制统计测试，满足加密标准。模型体积小（<0.5 MB），适用于资源受限平台。", "conclusion": "AI-Hybrid TRNG通过脱离专用硬件限制，扩展了高质量随机数生成器的应用范围，适用于安全系统、加密协议、嵌入式设备等领域。"}}
{"id": "2507.00693", "pdf": "https://arxiv.org/pdf/2507.00693", "abs": "https://arxiv.org/abs/2507.00693", "authors": ["Yifan Gao", "Jiao Fu", "Long Guo", "Hong Liu"], "title": "Leveraging Large Language Models for Spontaneous Speech-Based Suicide Risk Detection", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "Accepted to Interspeech 2025", "summary": "Early identification of suicide risk is crucial for preventing suicidal\nbehaviors. As a result, the identification and study of patterns and markers\nrelated to suicide risk have become a key focus of current research. In this\npaper, we present the results of our work in the 1st SpeechWellness Challenge\n(SW1), which aims to explore speech as a non-invasive and easily accessible\nmental health indicator for identifying adolescents at risk of suicide.Our\napproach leverages large language model (LLM) as the primary tool for feature\nextraction, alongside conventional acoustic and semantic features. The proposed\nmethod achieves an accuracy of 74\\% on the test set, ranking first in the SW1\nchallenge. These findings demonstrate the potential of LLM-based methods for\nanalyzing speech in the context of suicide risk assessment.", "AI": {"tldr": "论文提出了一种基于大语言模型（LLM）的方法，结合传统声学和语义特征，用于通过语音识别青少年自杀风险，并在SW1挑战中取得74%的准确率。", "motivation": "早期识别自杀风险对预防自杀行为至关重要，语音作为一种非侵入性且易获取的指标，具有研究价值。", "method": "利用大语言模型（LLM）进行特征提取，并结合传统声学和语义特征。", "result": "在SW1挑战中，该方法取得了74%的准确率，排名第一。", "conclusion": "LLM方法在自杀风险评估中具有潜力，语音分析是一种可行的研究方向。"}}
{"id": "2507.00676", "pdf": "https://arxiv.org/pdf/2507.00676", "abs": "https://arxiv.org/abs/2507.00676", "authors": ["Edward Effendy", "Kuan-Wei Tseng", "Rei Kawakami"], "title": "A Unified Transformer-Based Framework with Pretraining For Whole Body Grasping Motion Generation", "categories": ["cs.CV"], "comment": null, "summary": "Accepted in the ICIP 2025\n  We present a novel transformer-based framework for whole-body grasping that\naddresses both pose generation and motion infilling, enabling realistic and\nstable object interactions. Our pipeline comprises three stages: Grasp Pose\nGeneration for full-body grasp generation, Temporal Infilling for smooth motion\ncontinuity, and a LiftUp Transformer that refines downsampled joints back to\nhigh-resolution markers. To overcome the scarcity of hand-object interaction\ndata, we introduce a data-efficient Generalized Pretraining stage on large,\ndiverse motion datasets, yielding robust spatio-temporal representations\ntransferable to grasping tasks. Experiments on the GRAB dataset show that our\nmethod outperforms state-of-the-art baselines in terms of coherence, stability,\nand visual realism. The modular design also supports easy adaptation to other\nhuman-motion applications.", "AI": {"tldr": "提出了一种基于Transformer的全身抓取框架，结合姿势生成和运动填充，实现稳定且真实的物体交互。", "motivation": "解决全身抓取任务中姿势生成和运动连续性的挑战，同时克服手-物体交互数据稀缺的问题。", "method": "采用三阶段流程：抓取姿势生成、时间填充和LiftUp Transformer，并通过广义预训练提高数据效率。", "result": "在GRAB数据集上表现优于现有方法，提升了连贯性、稳定性和视觉真实感。", "conclusion": "模块化设计易于扩展至其他人体运动应用，展示了高效且通用的抓取解决方案。"}}
{"id": "2507.00161", "pdf": "https://arxiv.org/pdf/2507.00161", "abs": "https://arxiv.org/abs/2507.00161", "authors": ["Christopher M. Wegemer", "Edward Halim", "Jeff Burke"], "title": "Designing an Adaptive Storytelling Platform to Promote Civic Education in Politically Polarized Learning Environments", "categories": ["cs.HC", "cs.AI"], "comment": null, "summary": "Political polarization undermines democratic civic education by exacerbating\nidentity-based resistance to opposing viewpoints. Emerging AI technologies\noffer new opportunities to advance interventions that reduce polarization and\npromote political open-mindedness. We examined novel design strategies that\nleverage adaptive and emotionally-responsive civic narratives that may sustain\nstudents' emotional engagement in stories, and in turn, promote\nperspective-taking toward members of political out-groups. Drawing on theories\nfrom political psychology and narratology, we investigate how affective\ncomputing techniques can support three storytelling mechanisms: transportation\ninto a story world, identification with characters, and interaction with the\nstoryteller. Using a design-based research (DBR) approach, we iteratively\ndeveloped and refined an AI-mediated Digital Civic Storytelling (AI-DCS)\nplatform. Our prototype integrates facial emotion recognition and attention\ntracking to assess users' affective and attentional states in real time.\nNarrative content is organized around pre-structured story outlines, with\nbeat-by-beat language adaptation implemented via GPT-4, personalizing\nlinguistic tone to sustain students' emotional engagement in stories that\ncenter political perspectives different from their own. Our work offers a\nfoundation for AI-supported, emotionally-sensitive strategies that address\naffective polarization while preserving learner autonomy. We conclude with\nimplications for civic education interventions, algorithmic literacy, and HCI\nchallenges associated with AI dialogue management and affect-adaptive learning\nenvironments.", "AI": {"tldr": "AI技术用于减少政治极化，通过情感适应性叙事促进学生对不同政治观点的开放态度。", "motivation": "政治极化阻碍民主公民教育，AI技术提供新干预机会。", "method": "结合情感计算技术，开发AI-DCS平台，实时调整叙事内容以维持情感投入。", "result": "AI-DCS平台通过情感识别和语言适应，支持学生对不同政治观点的理解。", "conclusion": "AI支持的策略可减少情感极化，同时保护学习者自主性，对公民教育和HCI有重要意义。"}}
{"id": "2507.00740", "pdf": "https://arxiv.org/pdf/2507.00740", "abs": "https://arxiv.org/abs/2507.00740", "authors": ["Craig S Wright"], "title": "Safe Low Bandwidth SPV: A Formal Treatment of Simplified Payment Verification Protocols and Security Bounds", "categories": ["cs.CR", "cs.CL", "cs.DC", "68Q85, 68M10, 94A60, 91A80, 68Q17, 68W10, 68R10", "C.2.2; F.2.2; D.4.6; K.6.5"], "comment": "56 pages 5 images", "summary": "This paper presents a complete formal specification, protocol description,\nand mathematical proof structure for Simplified Payment Verification (SPV) as\noriginally defined in the Bitcoin whitepaper \\cite{nakamoto2008}. In stark\ncontrast to the misrepresentations proliferated by popular implementations, we\nshow that SPV is not only secure under bounded adversarial assumptions but\nstrictly optimal for digital cash systems requiring scalable and verifiable\ntransaction inclusion. We reconstruct the SPV protocol from first principles,\ngrounding its verification model in symbolic automata, Merkle membership\nrelations, and chain-of-proof dominance predicates. Through rigorous\nprobabilistic and game-theoretic analysis, we derive the economic bounds within\nwhich the protocol operates securely and verify its liveness and safety\nproperties under partial connectivity, hostile relay networks, and adversarial\npropagation delay. Our specification further introduces low-bandwidth\noptimisations such as adaptive polling and compressed header synchronisation\nwhile preserving correctness. This document serves both as a blueprint for\nsecure SPV implementation and a rebuttal of common misconceptions surrounding\nnon-validating clients.", "AI": {"tldr": "本文对比特币白皮书中定义的简化支付验证（SPV）进行了完整的规范描述、协议定义和数学证明，纠正了流行实现中的误解，证明SPV在特定条件下不仅是安全的，而且是数字现金系统的最优选择。", "motivation": "澄清SPV协议的误解，证明其在特定条件下的安全性和最优性，为安全实现提供理论依据。", "method": "从基本原理重构SPV协议，使用符号自动机、Merkle成员关系和证明链支配谓词进行验证模型，并通过概率和博弈论分析验证其安全性。", "result": "证明了SPV协议在部分连接、敌对中继网络和传播延迟下的活性和安全性，并提出了低带宽优化方案。", "conclusion": "本文为安全的SPV实现提供了蓝图，并反驳了关于非验证客户端的常见误解。"}}
{"id": "2507.00690", "pdf": "https://arxiv.org/pdf/2507.00690", "abs": "https://arxiv.org/abs/2507.00690", "authors": ["Keke Tang", "Ziyong Du", "Weilong Peng", "Xiaofei Wang", "Peican Zhu", "Ligang Liu", "Zhihong Tian"], "title": "Cage-Based Deformation for Transferable and Undefendable Point Cloud Attack", "categories": ["cs.CV", "cs.CR"], "comment": null, "summary": "Adversarial attacks on point clouds often impose strict geometric constraints\nto preserve plausibility; however, such constraints inherently limit\ntransferability and undefendability. While deformation offers an alternative,\nexisting unstructured approaches may introduce unnatural distortions, making\nadversarial point clouds conspicuous and undermining their plausibility. In\nthis paper, we propose CageAttack, a cage-based deformation framework that\nproduces natural adversarial point clouds. It first constructs a cage around\nthe target object, providing a structured basis for smooth, natural-looking\ndeformation. Perturbations are then applied to the cage vertices, which\nseamlessly propagate to the point cloud, ensuring that the resulting\ndeformations remain intrinsic to the object and preserve plausibility.\nExtensive experiments on seven 3D deep neural network classifiers across three\ndatasets show that CageAttack achieves a superior balance among\ntransferability, undefendability, and plausibility, outperforming\nstate-of-the-art methods. Codes will be made public upon acceptance.", "AI": {"tldr": "CageAttack提出了一种基于笼形变形的对抗攻击方法，通过结构化变形生成自然的对抗点云，平衡了可转移性、不可防御性和合理性。", "motivation": "现有对抗攻击方法在点云上施加严格几何约束，限制了可转移性和不可防御性；而现有非结构化变形方法可能导致不自然的扭曲。", "method": "构建目标对象的笼形结构，通过扰动笼形顶点实现平滑、自然的点云变形，确保变形内在且合理。", "result": "在三个数据集上的七个3D深度神经网络分类器上，CageAttack在可转移性、不可防御性和合理性上优于现有方法。", "conclusion": "CageAttack通过结构化变形实现了更自然的对抗点云生成，为对抗攻击提供了新思路。"}}
{"id": "2507.00184", "pdf": "https://arxiv.org/pdf/2507.00184", "abs": "https://arxiv.org/abs/2507.00184", "authors": ["Jacob Schrum", "Olivia Kilday", "Emilio Salas", "Bess Hagan", "Reid Williams"], "title": "Text-to-Level Diffusion Models With Various Text Encoders for Super Mario Bros", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent research shows how diffusion models can unconditionally generate\ntile-based game levels, but use of diffusion models for text-to-level\ngeneration is underexplored. There are practical considerations for creating a\nusable model: caption/level pairs are needed, as is a text embedding model, and\na way of generating entire playable levels, rather than individual scenes. We\npresent strategies to automatically assign descriptive captions to an existing\nlevel dataset, and train diffusion models using both pretrained text encoders\nand simple transformer models trained from scratch. Captions are automatically\nassigned to generated levels so that the degree of overlap between input and\noutput captions can be compared. We also assess the diversity and playability\nof the resulting levels. Results are compared with an unconditional diffusion\nmodel and a generative adversarial network, as well as the text-to-level\napproaches Five-Dollar Model and MarioGPT. Notably, the best diffusion model\nuses a simple transformer model for text embedding, and takes less time to\ntrain than diffusion models employing more complex text encoders, indicating\nthat reliance on larger language models is not necessary. We also present a GUI\nallowing designers to construct long levels from model-generated scenes.", "AI": {"tldr": "本文探讨了扩散模型在文本到游戏关卡生成中的应用，提出了自动标注关卡数据集的策略，并比较了不同文本嵌入模型的性能。", "motivation": "研究旨在解决扩散模型在文本到关卡生成中的未充分探索问题，并解决实际应用中所需的标注数据、文本嵌入模型和完整关卡生成等问题。", "method": "采用自动标注策略为现有关卡数据集生成描述性标题，训练扩散模型时结合预训练文本编码器和从头训练的简单Transformer模型，并评估生成关卡的多样性和可玩性。", "result": "最佳扩散模型使用简单Transformer进行文本嵌入，训练时间短于复杂文本编码器模型，且性能优于无条件扩散模型、生成对抗网络及其他文本到关卡方法。", "conclusion": "研究表明，依赖大型语言模型并非必要，简单Transformer模型在文本到关卡生成中表现优异，同时提供了GUI工具以支持设计师构建长关卡。"}}
{"id": "2507.00808", "pdf": "https://arxiv.org/pdf/2507.00808", "abs": "https://arxiv.org/abs/2507.00808", "authors": ["Hiroki Kanagawa", "Kenichi Fujita", "Aya Watanabe", "Yusuke Ijima"], "title": "Multi-interaction TTS toward professional recording reproduction", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "7 pages,6 figures, Accepted to Speech Synthesis Workshop 2025 (SSW13)", "summary": "Voice directors often iteratively refine voice actors' performances by\nproviding feedback to achieve the desired outcome. While this iterative\nfeedback-based refinement process is important in actual recordings, it has\nbeen overlooked in text-to-speech synthesis (TTS). As a result, fine-grained\nstyle refinement after the initial synthesis is not possible, even though the\nsynthesized speech often deviates from the user's intended style. To address\nthis issue, we propose a TTS method with multi-step interaction that allows\nusers to intuitively and rapidly refine synthetized speech. Our approach models\nthe interaction between the TTS model and its user to emulate the relationship\nbetween voice actors and voice directors. Experiments show that the proposed\nmodel with its corresponding dataset enable iterative style refinements in\naccordance with users' directions, thus demonstrating its multi-interaction\ncapability. Sample audios are available: https://ntt-hilab-gensp.\ngithub.io/ssw13multiinteraction_tts/", "AI": {"tldr": "提出了一种支持多步交互的TTS方法，允许用户直观快速地细化合成语音，模拟语音导演与演员的互动。", "motivation": "现有TTS系统缺乏对合成语音风格的细粒度迭代优化，无法满足用户对语音风格的精确需求。", "method": "提出了一种多步交互的TTS方法，模拟语音导演与演员的互动关系，支持用户对合成语音进行迭代风格优化。", "result": "实验表明，该方法能够根据用户指示实现迭代风格优化，展示了多步交互能力。", "conclusion": "该方法为TTS系统提供了更灵活的语音风格优化能力，提升了用户体验。"}}
{"id": "2507.00698", "pdf": "https://arxiv.org/pdf/2507.00698", "abs": "https://arxiv.org/abs/2507.00698", "authors": ["Qihang Fan", "Huaibo Huang", "Yuang Ai", "ran He"], "title": "Rectifying Magnitude Neglect in Linear Attention", "categories": ["cs.CV"], "comment": "Accepted by ICCV2025", "summary": "As the core operator of Transformers, Softmax Attention exhibits excellent\nglobal modeling capabilities. However, its quadratic complexity limits its\napplicability to vision tasks. In contrast, Linear Attention shares a similar\nformulation with Softmax Attention while achieving linear complexity, enabling\nefficient global information modeling. Nevertheless, Linear Attention suffers\nfrom a significant performance degradation compared to standard Softmax\nAttention. In this paper, we analyze the underlying causes of this issue based\non the formulation of Linear Attention. We find that, unlike Softmax Attention,\nLinear Attention entirely disregards the magnitude information of the Query.\nThis prevents the attention score distribution from dynamically adapting as the\nQuery scales. As a result, despite its structural similarity to Softmax\nAttention, Linear Attention exhibits a significantly different attention score\ndistribution. Based on this observation, we propose Magnitude-Aware Linear\nAttention (MALA), which modifies the computation of Linear Attention to fully\nincorporate the Query's magnitude. This adjustment allows MALA to generate an\nattention score distribution that closely resembles Softmax Attention while\nexhibiting a more well-balanced structure. We evaluate the effectiveness of\nMALA on multiple tasks, including image classification, object detection,\ninstance segmentation, semantic segmentation, natural language processing,\nspeech recognition, and image generation. Our MALA achieves strong results on\nall of these tasks. Code will be available at https://github.com/qhfan/MALA", "AI": {"tldr": "论文分析了线性注意力（Linear Attention）性能下降的原因，并提出了一种改进方法MALA，通过引入查询（Query）的幅度信息，使其更接近Softmax Attention的表现。", "motivation": "线性注意力虽然复杂度低，但性能显著低于标准Softmax Attention。研究发现其忽略了查询的幅度信息，导致注意力分布无法动态适应查询的变化。", "method": "提出MALA方法，改进线性注意力的计算方式，充分纳入查询的幅度信息，使其注意力分布更接近Softmax Attention。", "result": "MALA在多个任务（如图像分类、目标检测、NLP等）中表现优异，验证了其有效性。", "conclusion": "MALA通过引入查询的幅度信息，显著提升了线性注意力的性能，同时保持了低复杂度的优势。"}}
{"id": "2507.00185", "pdf": "https://arxiv.org/pdf/2507.00185", "abs": "https://arxiv.org/abs/2507.00185", "authors": ["Yang Zhou", "Chrystie Wan Ning Quek", "Jun Zhou", "Yan Wang", "Yang Bai", "Yuhe Ke", "Jie Yao", "Laura Gutierrez", "Zhen Ling Teo", "Darren Shu Jeng Ting", "Brian T. Soetikno", "Christopher S. Nielsen", "Tobias Elze", "Zengxiang Li", "Linh Le Dinh", "Lionel Tim-Ee Cheng", "Tran Nguyen Tuan Anh", "Chee Leong Cheng", "Tien Yin Wong", "Nan Liu", "Iain Beehuat Tan", "Tony Kiat Hon Lim", "Rick Siow Mong Goh", "Yong Liu", "Daniel Shu Wei Ting"], "title": "Multimodal, Multi-Disease Medical Imaging Foundation Model (MerMED-FM)", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "42 pages, 3 composite figures, 4 tables", "summary": "Current artificial intelligence models for medical imaging are predominantly\nsingle modality and single disease. Attempts to create multimodal and\nmulti-disease models have resulted in inconsistent clinical accuracy.\nFurthermore, training these models typically requires large, labour-intensive,\nwell-labelled datasets. We developed MerMED-FM, a state-of-the-art multimodal,\nmulti-specialty foundation model trained using self-supervised learning and a\nmemory module. MerMED-FM was trained on 3.3 million medical images from over\nten specialties and seven modalities, including computed tomography (CT), chest\nX-rays (CXR), ultrasound (US), pathology patches, color fundus photography\n(CFP), optical coherence tomography (OCT) and dermatology images. MerMED-FM was\nevaluated across multiple diseases and compared against existing foundational\nmodels. Strong performance was achieved across all modalities, with AUROCs of\n0.988 (OCT); 0.982 (pathology); 0.951 (US); 0.943 (CT); 0.931 (skin); 0.894\n(CFP); 0.858 (CXR). MerMED-FM has the potential to be a highly adaptable,\nversatile, cross-specialty foundation model that enables robust medical imaging\ninterpretation across diverse medical disciplines.", "AI": {"tldr": "MerMED-FM是一种多模态、多专科的基础模型，通过自监督学习和记忆模块训练，在多种医学影像任务中表现优异。", "motivation": "当前医学影像AI模型多为单模态、单疾病，且多模态、多疾病模型的临床准确性不一致，训练数据需求高。", "method": "使用自监督学习和记忆模块训练，基于330万张医学影像数据，涵盖10个专科和7种模态。", "result": "在多种疾病和模态中表现优异，AUROC值在0.858至0.988之间。", "conclusion": "MerMED-FM是一种高度适应性强、多用途的基础模型，可支持跨专科的医学影像解读。"}}
{"id": "2507.00877", "pdf": "https://arxiv.org/pdf/2507.00877", "abs": "https://arxiv.org/abs/2507.00877", "authors": ["William H English", "Chase Walker", "Dominic Simon", "Sumit Kumar Jha", "Rickard Ewetz"], "title": "Verifiable Natural Language to Linear Temporal Logic Translation: A Benchmark Dataset and Evaluation Suite", "categories": ["eess.SY", "cs.CL", "cs.SY"], "comment": null, "summary": "Empirical evaluation of state-of-the-art natural-language (NL) to\ntemporal-logic (TL) translation systems reveals near-perfect performance on\nexisting benchmarks. However, current studies measure only the accuracy of the\ntranslation of NL logic into formal TL, ignoring a system's capacity to ground\natomic propositions into new scenarios or environments. This is a critical\nfeature, necessary for the verification of resulting formulas in a concrete\nstate space. Consequently, most NL-to-TL translation frameworks propose their\nown bespoke dataset in which the correct grounding is known a-priori, inflating\nperformance metrics and neglecting the need for extensible, domain-general\nsystems. In this paper, we introduce the Verifiable Linear Temporal Logic\nBenchmark ( VLTL-Bench), a unifying benchmark that measures verification and\nverifiability of automated NL-to-LTL translation. The dataset consists of three\nunique state spaces and thousands of diverse natural language specifications\nand corresponding formal specifications in temporal logic. Moreover, the\nbenchmark contains sample traces to validate the temporal logic expressions.\nWhile the benchmark directly supports end-to-end evaluation, we observe that\nmany frameworks decompose the process into i) lifting, ii) grounding, iii)\ntranslation, and iv) verification. The benchmark provides ground truths after\neach of these steps to enable researches to improve and evaluate different\nsubsteps of the overall problem. To encourage methodologically sound advances\nin verifiable NL-to-LTL translation approaches, we release VLTL-Bench here:\nhttps://www.kaggle.com/datasets/dubascudes/vltl bench.", "AI": {"tldr": "论文提出了VLTL-Bench基准，用于评估自然语言到线性时序逻辑（NL-to-LTL）翻译系统的可验证性，弥补现有研究忽略的原子命题在新场景中的落地能力。", "motivation": "现有研究仅关注NL到TL翻译的准确性，而忽略了系统在新场景中落地原子命题的能力，导致性能指标虚高，缺乏领域通用性。", "method": "引入VLTL-Bench基准，包含多样化的自然语言规范、对应的时序逻辑表达式及验证样本，支持端到端评估及分步改进。", "result": "VLTL-Bench提供了三种独特的状态空间和大量样本，支持翻译、验证及各子步骤的评估。", "conclusion": "VLTL-Bench填补了现有研究的不足，为可验证的NL-to-LTL翻译方法提供了方法论支持。"}}
{"id": "2507.00707", "pdf": "https://arxiv.org/pdf/2507.00707", "abs": "https://arxiv.org/abs/2507.00707", "authors": ["Zeming Chen", "Hang Zhao"], "title": "BEV-VAE: Multi-view Image Generation with Spatial Consistency for Autonomous Driving", "categories": ["cs.CV"], "comment": null, "summary": "Multi-view image generation in autonomous driving demands consistent 3D scene\nunderstanding across camera views. Most existing methods treat this problem as\na 2D image set generation task, lacking explicit 3D modeling. However, we argue\nthat a structured representation is crucial for scene generation, especially\nfor autonomous driving applications. This paper proposes BEV-VAE for consistent\nand controllable view synthesis. BEV-VAE first trains a multi-view image\nvariational autoencoder for a compact and unified BEV latent space and then\ngenerates the scene with a latent diffusion transformer. BEV-VAE supports\narbitrary view generation given camera configurations, and optionally 3D\nlayouts. Experiments on nuScenes and Argoverse 2 (AV2) show strong performance\nin both 3D consistent reconstruction and generation. The code is available at:\nhttps://github.com/Czm369/bev-vae.", "AI": {"tldr": "BEV-VAE提出了一种基于鸟瞰图（BEV）的变分自编码器方法，用于自动驾驶中多视角图像的一致生成。", "motivation": "现有方法多将多视角图像生成视为2D任务，缺乏显式3D建模，而结构化表示对自动驾驶场景生成至关重要。", "method": "BEV-VAE通过训练多视角图像的变分自编码器，构建紧凑的BEV潜在空间，并利用潜在扩散变换器生成场景。", "result": "在nuScenes和Argoverse 2数据集上，BEV-VAE在3D一致性重建和生成方面表现优异。", "conclusion": "BEV-VAE为自动驾驶场景生成提供了一种一致且可控的解决方案。"}}
{"id": "2507.00191", "pdf": "https://arxiv.org/pdf/2507.00191", "abs": "https://arxiv.org/abs/2507.00191", "authors": ["Eray Erturk", "Fahad Kamran", "Salar Abbaspourazad", "Sean Jewell", "Harsh Sharma", "Yujie Li", "Sinead Williamson", "Nicholas J Foti", "Joseph Futoma"], "title": "Beyond Sensor Data: Foundation Models of Behavioral Data from Wearables Improve Health Predictions", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to ICML 2025", "summary": "Wearable devices record physiological and behavioral signals that can improve\nhealth predictions. While foundation models are increasingly used for such\npredictions, they have been primarily applied to low-level sensor data, despite\nbehavioral data often being more informative due to their alignment with\nphysiologically relevant timescales and quantities. We develop foundation\nmodels of such behavioral signals using over 2.5B hours of wearable data from\n162K individuals, systematically optimizing architectures and tokenization\nstrategies for this unique dataset. Evaluated on 57 health-related tasks, our\nmodel shows strong performance across diverse real-world applications including\nindividual-level classification and time-varying health state prediction. The\nmodel excels in behavior-driven tasks like sleep prediction, and improves\nfurther when combined with representations of raw sensor data. These results\nunderscore the importance of tailoring foundation model design to wearables and\ndemonstrate the potential to enable new health applications.", "AI": {"tldr": "该论文开发了一种基于可穿戴设备行为信号的基础模型，优化了架构和标记化策略，并在57项健康相关任务中表现出色。", "motivation": "行为数据比低级别传感器数据更具信息量，但现有基础模型主要关注后者，因此需要针对行为信号开发专门的基础模型。", "method": "使用来自162K个体的2.5B小时可穿戴数据，系统优化模型架构和标记化策略。", "result": "模型在57项健康任务中表现优异，尤其在行为驱动任务（如睡眠预测）中表现突出，结合原始传感器数据后性能进一步提升。", "conclusion": "研究表明，为基础模型设计专门针对可穿戴设备的策略至关重要，并展示了其在健康应用中的潜力。"}}
{"id": "2507.00898", "pdf": "https://arxiv.org/pdf/2507.00898", "abs": "https://arxiv.org/abs/2507.00898", "authors": ["Zifu Wan", "Ce Zhang", "Silong Yong", "Martin Q. Ma", "Simon Stepputtis", "Louis-Philippe Morency", "Deva Ramanan", "Katia Sycara", "Yaqi Xie"], "title": "ONLY: One-Layer Intervention Sufficiently Mitigates Hallucinations in Large Vision-Language Models", "categories": ["cs.CV", "cs.CL"], "comment": "Accepted by ICCV 2025. Project page: https://zifuwan.github.io/ONLY/", "summary": "Recent Large Vision-Language Models (LVLMs) have introduced a new paradigm\nfor understanding and reasoning about image input through textual responses.\nAlthough they have achieved remarkable performance across a range of\nmulti-modal tasks, they face the persistent challenge of hallucination, which\nintroduces practical weaknesses and raises concerns about their reliable\ndeployment in real-world applications. Existing work has explored contrastive\ndecoding approaches to mitigate this issue, where the output of the original\nLVLM is compared and contrasted with that of a perturbed version. However,\nthese methods require two or more queries that slow down LVLM response\ngeneration, making them less suitable for real-time applications. To overcome\nthis limitation, we propose ONLY, a training-free decoding approach that\nrequires only a single query and a one-layer intervention during decoding,\nenabling efficient real-time deployment. Specifically, we enhance textual\noutputs by selectively amplifying crucial textual information using a\ntext-to-visual entropy ratio for each token. Extensive experimental results\ndemonstrate that our proposed ONLY consistently outperforms state-of-the-art\nmethods across various benchmarks while requiring minimal implementation effort\nand computational cost. Code is available at https://github.com/zifuwan/ONLY.", "AI": {"tldr": "ONLY是一种无需训练的解码方法，通过单次查询和单层干预减少大型视觉语言模型的幻觉问题，实现高效实时部署。", "motivation": "现有方法需多次查询，影响实时性，需改进以提升效率和实用性。", "method": "利用文本到视觉熵比选择性增强关键文本信息，仅需单次查询和单层干预。", "result": "在多个基准测试中优于现有方法，计算成本低且易于实现。", "conclusion": "ONLY高效解决幻觉问题，适合实时应用，性能优越且易于部署。"}}
{"id": "2507.00709", "pdf": "https://arxiv.org/pdf/2507.00709", "abs": "https://arxiv.org/abs/2507.00709", "authors": ["Yiming Yang", "Yueru Luo", "Bingkun He", "Hongbin Lin", "Suzhong Fu", "Chao Yan", "Kun Tang", "Xinrui Yan", "Chao Zheng", "Shuguang Cui", "Zhen Li"], "title": "TopoStreamer: Temporal Lane Segment Topology Reasoning in Autonomous Driving", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Lane segment topology reasoning constructs a comprehensive road network by\ncapturing the topological relationships between lane segments and their\nsemantic types. This enables end-to-end autonomous driving systems to perform\nroad-dependent maneuvers such as turning and lane changing. However, the\nlimitations in consistent positional embedding and temporal multiple attribute\nlearning in existing methods hinder accurate roadnet reconstruction. To address\nthese issues, we propose TopoStreamer, an end-to-end temporal perception model\nfor lane segment topology reasoning. Specifically, TopoStreamer introduces\nthree key improvements: streaming attribute constraints, dynamic lane boundary\npositional encoding, and lane segment denoising. The streaming attribute\nconstraints enforce temporal consistency in both centerline and boundary\ncoordinates, along with their classifications. Meanwhile, dynamic lane boundary\npositional encoding enhances the learning of up-to-date positional information\nwithin queries, while lane segment denoising helps capture diverse lane segment\npatterns, ultimately improving model performance. Additionally, we assess the\naccuracy of existing models using a lane boundary classification metric, which\nserves as a crucial measure for lane-changing scenarios in autonomous driving.\nOn the OpenLane-V2 dataset, TopoStreamer demonstrates significant improvements\nover state-of-the-art methods, achieving substantial performance gains of +3.4%\nmAP in lane segment perception and +2.1% OLS in centerline perception tasks.", "AI": {"tldr": "TopoStreamer提出了一种端到端的时序感知模型，用于车道段拓扑推理，通过流式属性约束、动态车道边界位置编码和车道段去噪，显著提升了车道段感知和中心线感知任务的性能。", "motivation": "现有方法在一致位置嵌入和时序多属性学习方面存在局限，影响了道路网络的准确重建，因此需要改进。", "method": "TopoStreamer引入了流式属性约束、动态车道边界位置编码和车道段去噪三项关键技术。", "result": "在OpenLane-V2数据集上，TopoStreamer在车道段感知任务中提升了3.4% mAP，在中心线感知任务中提升了2.1% OLS。", "conclusion": "TopoStreamer通过改进时序一致性和位置编码，显著提升了车道段拓扑推理的准确性，为自动驾驶系统提供了更可靠的道路网络信息。"}}
{"id": "2507.00195", "pdf": "https://arxiv.org/pdf/2507.00195", "abs": "https://arxiv.org/abs/2507.00195", "authors": ["Kumar Kshitij Patel"], "title": "What Makes Local Updates Effective: The Role of Data Heterogeneity and Smoothness", "categories": ["cs.LG", "cs.AI", "cs.MA", "math.OC", "stat.ML"], "comment": null, "summary": "This thesis contributes to the theoretical understanding of local update\nalgorithms, especially Local SGD, in distributed and federated optimization\nunder realistic models of data heterogeneity. A central focus is on the bounded\nsecond-order heterogeneity assumption, which is shown to be both necessary and\nsufficient for local updates to outperform centralized or mini-batch methods in\nconvex and non-convex settings. The thesis establishes tight upper and lower\nbounds in several regimes for various local update algorithms and characterizes\nthe min-max complexity of multiple problem classes. At its core is a\nfine-grained consensus-error-based analysis framework that yields sharper\nfinite-time convergence bounds under third-order smoothness and relaxed\nheterogeneity assumptions. The thesis also extends to online federated\nlearning, providing fundamental regret bounds under both first-order and bandit\nfeedback. Together, these results clarify when and why local updates offer\nprovable advantages, and the thesis serves as a self-contained guide for\nanalyzing Local SGD in heterogeneous environments.", "AI": {"tldr": "该论文深入研究了分布式和联邦优化中局部更新算法（如Local SGD）的理论基础，特别是在数据异构性模型下的表现。核心贡献包括证明了二阶有界异构假设的必要性和充分性，并提出了细粒度共识误差分析框架。", "motivation": "研究动机是理解局部更新算法在异构数据环境下的表现，明确其何时及为何优于集中式或小批量方法。", "method": "方法包括建立紧致的上下界分析框架，涵盖凸和非凸场景，并扩展到在线联邦学习中的一阶和bandit反馈。", "result": "结果表明，局部更新算法在特定条件下优于其他方法，并提供了更精确的有限时间收敛边界。", "conclusion": "结论是局部更新算法在异构环境中具有理论优势，论文为分析Local SGD提供了完整指南。"}}
{"id": "2507.00721", "pdf": "https://arxiv.org/pdf/2507.00721", "abs": "https://arxiv.org/abs/2507.00721", "authors": ["Xiao Zhang", "Fei Wei", "Yong Wang", "Wenda Zhao", "Feiyi Li", "Xiangxiang Chu"], "title": "UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement", "categories": ["cs.CV"], "comment": "ICCV2025", "summary": "Zero-shot domain adaptation (ZSDA) presents substantial challenges due to the\nlack of images in the target domain. Previous approaches leverage\nVision-Language Models (VLMs) to tackle this challenge, exploiting their\nzero-shot learning capabilities. However, these methods primarily address\ndomain distribution shifts and overlook the misalignment between the detection\ntask and VLMs, which rely on manually crafted prompts. To overcome these\nlimitations, we propose the unified prompt and representation enhancement\n(UPRE) framework, which jointly optimizes both textual prompts and visual\nrepresentations. Specifically, our approach introduces a multi-view domain\nprompt that combines linguistic domain priors with detection-specific\nknowledge, and a visual representation enhancement module that produces domain\nstyle variations. Furthermore, we introduce multi-level enhancement strategies,\nincluding relative domain distance and positive-negative separation, which\nalign multi-modal representations at the image level and capture diverse visual\nrepresentations at the instance level, respectively. Extensive experiments\nconducted on nine benchmark datasets demonstrate the superior performance of\nour framework in ZSDA detection scenarios. Code is available at\nhttps://github.com/AMAP-ML/UPRE.", "AI": {"tldr": "UPRE框架通过联合优化文本提示和视觉表示，解决了零样本域适应中检测任务与视觉语言模型之间的不对齐问题。", "motivation": "零样本域适应（ZSDA）因目标域缺乏图像而具有挑战性，现有方法依赖手动设计的提示，忽略了检测任务与视觉语言模型之间的不对齐。", "method": "提出UPRE框架，结合多视图域提示和视觉表示增强模块，并采用多级增强策略（如相对域距离和正负分离）。", "result": "在九个基准数据集上的实验表明，UPRE在ZSDA检测场景中表现优异。", "conclusion": "UPRE通过优化提示和表示，显著提升了零样本域适应的性能。"}}
{"id": "2507.00209", "pdf": "https://arxiv.org/pdf/2507.00209", "abs": "https://arxiv.org/abs/2507.00209", "authors": ["Fengyi Jiang", "Xiaorui Zhang", "Lingbo Jin", "Ruixing Liang", "Yuxin Chen", "Adi Chola Venkatesh", "Jason Culman", "Tiantian Wu", "Lirong Shao", "Wenqing Sun", "Cong Gao", "Hallie McNamara", "Jingpei Lu", "Omid Mohareri"], "title": "SurgiSR4K: A High-Resolution Endoscopic Video Dataset for Robotic-Assisted Minimally Invasive Procedures", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.RO"], "comment": null, "summary": "High-resolution imaging is crucial for enhancing visual clarity and enabling\nprecise computer-assisted guidance in minimally invasive surgery (MIS). Despite\nthe increasing adoption of 4K endoscopic systems, there remains a significant\ngap in publicly available native 4K datasets tailored specifically for\nrobotic-assisted MIS. We introduce SurgiSR4K, the first publicly accessible\nsurgical imaging and video dataset captured at a native 4K resolution,\nrepresenting realistic conditions of robotic-assisted procedures. SurgiSR4K\ncomprises diverse visual scenarios including specular reflections, tool\nocclusions, bleeding, and soft tissue deformations, meticulously designed to\nreflect common challenges faced during laparoscopic and robotic surgeries. This\ndataset opens up possibilities for a broad range of computer vision tasks that\nmight benefit from high resolution data, such as super resolution (SR), smoke\nremoval, surgical instrument detection, 3D tissue reconstruction, monocular\ndepth estimation, instance segmentation, novel view synthesis, and\nvision-language model (VLM) development. SurgiSR4K provides a robust foundation\nfor advancing research in high-resolution surgical imaging and fosters the\ndevelopment of intelligent imaging technologies aimed at enhancing performance,\nsafety, and usability in image-guided robotic surgeries.", "AI": {"tldr": "SurgiSR4K是首个公开的4K分辨率手术影像数据集，专为机器人辅助微创手术设计，涵盖多种视觉挑战场景，支持多种计算机视觉任务。", "motivation": "目前缺乏公开的4K分辨率手术影像数据集，限制了高分辨率手术影像研究的进展。", "method": "通过采集机器人辅助手术中的真实4K影像，构建包含多种视觉挑战场景的数据集SurgiSR4K。", "result": "数据集为高分辨率手术影像研究提供了基础，支持多种计算机视觉任务。", "conclusion": "SurgiSR4K推动了高分辨率手术影像研究，有助于提升机器人辅助手术的性能和安全性。"}}
{"id": "2507.00724", "pdf": "https://arxiv.org/pdf/2507.00724", "abs": "https://arxiv.org/abs/2507.00724", "authors": ["Linghui Zhu", "Yiming Li", "Haiqin Weng", "Yan Liu", "Tianwei Zhang", "Shu-Tao Xia", "Zhi Wang"], "title": "Holmes: Towards Effective and Harmless Model Ownership Verification to Personalized Large Vision Models via Decoupling Common Features", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Large vision models achieve remarkable performance in various downstream\ntasks, primarily by personalizing pre-trained models through fine-tuning with\nprivate and valuable local data, which makes the personalized model a valuable\nintellectual property for its owner. Similar to the era of traditional DNNs,\nmodel stealing attacks also pose significant risks to these personalized\nmodels. However, in this paper, we reveal that most existing defense methods\n(developed for traditional DNNs), typically designed for models trained from\nscratch, either introduce additional security risks, are prone to misjudgment,\nor are even ineffective for fine-tuned models. To alleviate these problems,\nthis paper proposes a harmless model ownership verification method for\npersonalized models by decoupling similar common features. In general, our\nmethod consists of three main stages. In the first stage, we create shadow\nmodels that retain common features of the victim model while disrupting\ndataset-specific features. We represent the dataset-specific features of the\nvictim model by the output differences between the shadow and victim models.\nAfter that, a meta-classifier is trained to identify stolen models by\ndetermining whether suspicious models contain the dataset-specific features of\nthe victim. In the third stage, we conduct model ownership verification by\nhypothesis test to mitigate randomness and enhance robustness. Extensive\nexperiments on benchmark datasets verify the effectiveness of the proposed\nmethod in detecting different types of model stealing simultaneously.", "AI": {"tldr": "论文提出了一种针对个性化模型的无害所有权验证方法，通过解耦相似共同特征来防御模型窃取攻击。", "motivation": "现有防御方法针对从头训练的模型设计，对微调模型可能无效或引入风险，因此需要一种更安全的验证方法。", "method": "分三阶段：1) 创建保留共同特征的影子模型；2) 训练元分类器识别特定数据集特征；3) 通过假设检验验证所有权。", "result": "在基准数据集上的实验验证了该方法能有效检测多种模型窃取类型。", "conclusion": "该方法为个性化模型提供了一种安全、鲁棒的所有权验证解决方案。"}}
{"id": "2507.00739", "pdf": "https://arxiv.org/pdf/2507.00739", "abs": "https://arxiv.org/abs/2507.00739", "authors": ["An Le", "Hung Nguyen", "Sungbal Seo", "You-Suk Bae", "Truong Nguyen"], "title": "Biorthogonal Tunable Wavelet Unit with Lifting Scheme in Convolutional Neural Network", "categories": ["cs.CV", "eess.IV", "eess.SP"], "comment": null, "summary": "This work introduces a novel biorthogonal tunable wavelet unit constructed\nusing a lifting scheme that relaxes both the orthogonality and equal filter\nlength constraints, providing greater flexibility in filter design. The\nproposed unit enhances convolution, pooling, and downsampling operations,\nleading to improved image classification and anomaly detection in convolutional\nneural networks (CNN). When integrated into an 18-layer residual neural network\n(ResNet-18), the approach improved classification accuracy on CIFAR-10 by 2.12%\nand on the Describable Textures Dataset (DTD) by 9.73%, demonstrating its\neffectiveness in capturing fine-grained details. Similar improvements were\nobserved in ResNet-34. For anomaly detection in the hazelnut category of the\nMVTec Anomaly Detection dataset, the proposed method achieved competitive and\nwellbalanced performance in both segmentation and detection tasks,\noutperforming existing approaches in terms of accuracy and robustness.", "AI": {"tldr": "提出了一种基于提升方案的双正交可调小波单元，放宽了正交性和滤波器长度限制，提升了CNN中的卷积、池化和下采样操作，显著提高了图像分类和异常检测性能。", "motivation": "传统小波单元的正交性和滤波器长度限制限制了其灵活性，影响了CNN的性能。本文旨在通过放松这些约束，提升模型的表达能力。", "method": "采用提升方案构建双正交可调小波单元，并将其集成到ResNet-18和ResNet-34中，优化卷积、池化和下采样操作。", "result": "在CIFAR-10和DTD数据集上分类准确率分别提升2.12%和9.73%；在MVTec异常检测任务中表现优于现有方法。", "conclusion": "所提方法在图像分类和异常检测任务中均表现出色，证明了其灵活性和有效性。"}}
{"id": "2507.00225", "pdf": "https://arxiv.org/pdf/2507.00225", "abs": "https://arxiv.org/abs/2507.00225", "authors": ["S. V. Chekanov", "H. Kjellerstrand"], "title": "Discovering the underlying analytic structure within Standard Model constants using artificial intelligence", "categories": ["hep-ph", "cs.AI", "physics.data-an"], "comment": "42 pages, 10 tables", "summary": "This paper presents a search for underlying analytic structures among the\nfundamental parameters of the Standard Model (SM) using symbolic regression and\ngenetic programming. We identify the simplest analytic relationships connecting\npairs of these constants and report several notable observations based on about\na thousand expressions with relative precision better than 1%. These results\nmay serve as valuable inputs for model builders and artificial intelligence\nmethods aimed at uncovering hidden patterns among the SM constants, or\npotentially used as building blocks for a deeper underlying law that connects\nall parameters of the SM through a small set of fundamental constants.", "AI": {"tldr": "使用符号回归和遗传编程寻找标准模型基本参数间的解析结构，发现了一些简单关系，精度优于1%。", "motivation": "探索标准模型基本参数间是否存在隐藏的解析关系，为模型构建和AI方法提供输入。", "method": "采用符号回归和遗传编程技术，分析参数对之间的简单解析关系。", "result": "发现了约一千个精度优于1%的表达式，揭示了参数间的潜在联系。", "conclusion": "这些结果可能为揭示标准模型参数间的深层规律提供线索或构建模块。"}}
{"id": "2507.00748", "pdf": "https://arxiv.org/pdf/2507.00748", "abs": "https://arxiv.org/abs/2507.00748", "authors": ["Bob Zhang", "Haoran Li", "Tao Zhang", "Cilin Yan", "Jiayin Cai", "Xiaolong Jiang", "Yanbin Hao"], "title": "Improving the Reasoning of Multi-Image Grounding in MLLMs via Reinforcement Learning", "categories": ["cs.CV"], "comment": "11 pages", "summary": "Recently, Multimodal Large Language Models (MLLMs) excel at visual grounding\nin single-image scenarios with textual references. However, their performance\ndegrades when handling real-world applications involving complex multi-image\ncompositions and multimodal instructions, which reveals limitations in\ncross-image reasoning and generalization. To address these challenges, we adopt\na Reinforcement Learning (RL) based post-training strategy to improve the\nreasoning performance of MLLMs in multi-image grounding tasks. Our approach\nbegins with synthesizing high-quality chain-of-thought (CoT) data for\ncold-start initialization, followed by supervised fine-tuning (SFT) using\nlow-rank adaptation (LoRA). The cold-start training stage enables the model to\nidentify correct solutions. Subsequently, we perform rejection sampling using\nthe merged SFT model to curate high-quality RL data and leverage rule-based RL\nto guide the model toward optimal reasoning paths. Extensive experimental\nresults demonstrate the effectiveness of our approach, achieving +9.04\\%\nimprovements on MIG-Bench and +4.98\\% improvements on several out-of-domain\nreasoning grounding benchmarks over the SFT baseline. Furthermore, our approach\nexhibits strong generalization in multi-image perception, with gains of +3.1\\%\nand +2.4\\% over the base model on subsets of the BLINK and MMIU benchmarks,\nrespectively.", "AI": {"tldr": "该论文提出了一种基于强化学习的后训练策略，提升多模态大语言模型在多图像任务中的推理性能，通过合成高质量思维链数据和监督微调，显著提升了模型表现。", "motivation": "当前多模态大语言模型在单图像任务中表现优异，但在多图像任务中推理和泛化能力不足，需要改进。", "method": "采用强化学习后训练策略，包括合成思维链数据冷启动初始化、监督微调（LoRA），并通过拒绝采样和规则强化学习优化推理路径。", "result": "在MIG-Bench和多个域外推理基准上分别提升9.04%和4.98%，在多图像感知任务中泛化能力显著提升。", "conclusion": "该方法有效提升了多模态大语言模型在多图像任务中的推理和泛化能力，具有实际应用潜力。"}}
{"id": "2507.00227", "pdf": "https://arxiv.org/pdf/2507.00227", "abs": "https://arxiv.org/abs/2507.00227", "authors": ["Paul Mayer", "Florian Lux", "Alejandro Pérez-González-de-Martos", "Angelina Elizarova", "Lindsey Vanderlyn", "Dirk Väth", "Ngoc Thang Vu"], "title": "Investigating Stochastic Methods for Prosody Modeling in Speech Synthesis", "categories": ["eess.AS", "cs.AI"], "comment": "Accepted at Interspeech 2025", "summary": "While generative methods have progressed rapidly in recent years, generating\nexpressive prosody for an utterance remains a challenging task in\ntext-to-speech synthesis. This is particularly true for systems that model\nprosody explicitly through parameters such as pitch, energy, and duration,\nwhich is commonly done for the sake of interpretability and controllability. In\nthis work, we investigate the effectiveness of stochastic methods for this\ntask, including Normalizing Flows, Conditional Flow Matching, and Rectified\nFlows. We compare these methods to a traditional deterministic baseline, as\nwell as to real human realizations. Our extensive subjective and objective\nevaluations demonstrate that stochastic methods produce natural prosody on par\nwith human speakers by capturing the variability inherent in human speech.\nFurther, they open up additional controllability options by allowing the\nsampling temperature to be tuned.", "AI": {"tldr": "本文研究了随机方法（如归一化流、条件流匹配和整流流）在生成语音韵律中的有效性，证明其效果优于传统确定性方法，并能模拟人类语音的变异性。", "motivation": "生成语音韵律是文本到语音合成的挑战，尤其是为了可解释性和可控性而显式建模韵律参数的系统。", "method": "比较了随机方法（归一化流、条件流匹配、整流流）与传统确定性基线及人类语音样本。", "result": "随机方法生成的韵律与人类语音相当，并能通过调节采样温度提供额外的可控性。", "conclusion": "随机方法在生成自然韵律和可控性方面表现优异，适用于语音合成任务。"}}
{"id": "2507.00752", "pdf": "https://arxiv.org/pdf/2507.00752", "abs": "https://arxiv.org/abs/2507.00752", "authors": ["Hao Xing", "Kai Zhe Boey", "Yuankai Wu", "Darius Burschka", "Gordon Cheng"], "title": "Multi-Modal Graph Convolutional Network with Sinusoidal Encoding for Robust Human Action Segmentation", "categories": ["cs.CV", "cs.RO"], "comment": "7 pages, 4 figures, accepted in IROS25, Hangzhou, China", "summary": "Accurate temporal segmentation of human actions is critical for intelligent\nrobots in collaborative settings, where a precise understanding of sub-activity\nlabels and their temporal structure is essential. However, the inherent noise\nin both human pose estimation and object detection often leads to\nover-segmentation errors, disrupting the coherence of action sequences. To\naddress this, we propose a Multi-Modal Graph Convolutional Network (MMGCN) that\nintegrates low-frame-rate (e.g., 1 fps) visual data with high-frame-rate (e.g.,\n30 fps) motion data (skeleton and object detections) to mitigate fragmentation.\nOur framework introduces three key contributions. First, a sinusoidal encoding\nstrategy that maps 3D skeleton coordinates into a continuous sin-cos space to\nenhance spatial representation robustness. Second, a temporal graph fusion\nmodule that aligns multi-modal inputs with differing resolutions via\nhierarchical feature aggregation, Third, inspired by the smooth transitions\ninherent to human actions, we design SmoothLabelMix, a data augmentation\ntechnique that mixes input sequences and labels to generate synthetic training\nexamples with gradual action transitions, enhancing temporal consistency in\npredictions and reducing over-segmentation artifacts.\n  Extensive experiments on the Bimanual Actions Dataset, a public benchmark for\nhuman-object interaction understanding, demonstrate that our approach\noutperforms state-of-the-art methods, especially in action segmentation\naccuracy, achieving F1@10: 94.5% and F1@25: 92.8%.", "AI": {"tldr": "提出了一种多模态图卷积网络（MMGCN），通过整合低帧率视觉数据和高帧率运动数据，减少动作分割中的过分割错误。", "motivation": "在协作环境中，机器人需要精确理解人类动作的子活动标签及其时间结构，但现有方法因噪声问题容易导致过分割。", "method": "采用正弦编码策略增强空间表示，设计时间图融合模块对齐多模态输入，并提出数据增强技术SmoothLabelMix以提升时间一致性。", "result": "在Bimanual Actions Dataset上表现优异，F1@10为94.5%，F1@25为92.8%。", "conclusion": "MMGCN通过多模态融合和数据增强，显著提升了动作分割的准确性和时间一致性。"}}
{"id": "2507.00229", "pdf": "https://arxiv.org/pdf/2507.00229", "abs": "https://arxiv.org/abs/2507.00229", "authors": ["Tarikul Islam Tamiti", "Biraj Joshi", "Rida Hasan", "Rashedul Hasan", "Taieba Athay", "Nursad Mamun", "Anomadarshi Barua"], "title": "A High-Fidelity Speech Super Resolution Network using a Complex Global Attention Module with Spectro-Temporal Loss", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Speech super-resolution (SSR) enhances low-resolution speech by increasing\nthe sampling rate. While most SSR methods focus on magnitude reconstruction,\nrecent research highlights the importance of phase reconstruction for improved\nperceptual quality. Therefore, we introduce CTFT-Net, a Complex Time-Frequency\nTransformation Network that reconstructs both magnitude and phase in complex\ndomains for improved SSR tasks. It incorporates a complex global attention\nblock to model inter-phoneme and inter-frequency dependencies and a complex\nconformer to capture long-range and local features, improving frequency\nreconstruction and noise robustness. CTFT-Net employs time-domain and\nmulti-resolution frequency-domain loss functions for better generalization.\nExperiments show CTFT-Net outperforms state-of-the-art models (NU-Wave,\nWSRGlow, NVSR, AERO) on the VCTK dataset, particularly for extreme upsampling\n(2 kHz to 48 kHz), reconstructing high frequencies effectively without noisy\nartifacts.", "AI": {"tldr": "CTFT-Net是一种用于语音超分辨率的复杂时频变换网络，通过同时重建幅度和相位，显著提升了语音质量。", "motivation": "现有语音超分辨率方法主要关注幅度重建，而忽略了相位重建对感知质量的重要性。", "method": "CTFT-Net结合了复杂全局注意力块和复杂Conformer模块，建模音素间和频率间依赖关系，并采用时域和多分辨率频域损失函数。", "result": "在VCTK数据集上，CTFT-Net在极端上采样任务（2 kHz到48 kHz）中优于现有模型（如NU-Wave、WSRGlow等），有效重建高频且无噪声伪影。", "conclusion": "CTFT-Net通过复杂域重建和新型网络结构，显著提升了语音超分辨率的性能和质量。"}}
{"id": "2507.00754", "pdf": "https://arxiv.org/pdf/2507.00754", "abs": "https://arxiv.org/abs/2507.00754", "authors": ["Selim Kuzucu", "Muhammad Ferjad Naeem", "Anna Kukleva", "Federico Tombari", "Bernt Schiele"], "title": "Language-Unlocked ViT (LUViT): Empowering Self-Supervised Vision Transformers with LLMs", "categories": ["cs.CV"], "comment": "26 pages, 6 figures", "summary": "The integration of Large Language Model (LLMs) blocks with Vision\nTransformers (ViTs) holds immense promise for vision-only tasks by leveraging\nthe rich semantic knowledge and reasoning capabilities of LLMs. However, a\nfundamental challenge lies in the inherent modality mismatch between\ntext-centric pretraining of LLMs and vision-centric training of ViTs. Direct\nfusion often fails to fully exploit the LLM's potential and suffers from\nunstable finetuning. As a result, LLM blocks are kept frozen while only the\nvision components are learned. As a remedy to these challenges, we introduce\nLanguage-Unlocked Vision Transformers (LUViT), a novel approach that bridges\nthis modality mismatch through a synergistic pre-training strategy. LUViT\nco-adapts a ViT backbone and an LLM fusion block by (1) employing Masked\nAuto-Encoding (MAE) to pre-train the ViT for richer visual representations, and\n(2) concurrently training Low-Rank Adaptation (LoRA) layers within the LLM\nblock using the MAE objective. This joint optimization guides the ViT to\nproduce LLM-aligned features and the LLM to effectively interpret visual\ninformation. We demonstrate through extensive experiments that LUViT\nsignificantly improves performance on various downstream vision tasks,\nshowcasing a more effective and efficient pathway to harness LLM knowledge for\nvisual understanding.", "AI": {"tldr": "论文提出了一种名为LUViT的新方法，通过联合预训练策略解决LLMs与ViTs的模态不匹配问题，显著提升了视觉任务的性能。", "motivation": "LLMs与ViTs的直接融合存在模态不匹配问题，导致LLMs潜力未充分发挥且微调不稳定。", "method": "采用MAE预训练ViT以增强视觉表示，同时使用LoRA层在LLM块中通过MAE目标进行训练。", "result": "LUViT在多种下游视觉任务中表现出显著性能提升。", "conclusion": "LUViT为利用LLM知识进行视觉理解提供了更有效和高效的途径。"}}
{"id": "2507.00756", "pdf": "https://arxiv.org/pdf/2507.00756", "abs": "https://arxiv.org/abs/2507.00756", "authors": ["Hao Xing", "Kai Zhe Boey", "Gordon Cheng"], "title": "Towards Open-World Human Action Segmentation Using Graph Convolutional Networks", "categories": ["cs.CV", "cs.RO"], "comment": "8 pages, 3 figures, accepted in IROS25, Hangzhou, China", "summary": "Human-object interaction segmentation is a fundamental task of daily activity\nunderstanding, which plays a crucial role in applications such as assistive\nrobotics, healthcare, and autonomous systems. Most existing learning-based\nmethods excel in closed-world action segmentation, they struggle to generalize\nto open-world scenarios where novel actions emerge. Collecting exhaustive\naction categories for training is impractical due to the dynamic diversity of\nhuman activities, necessitating models that detect and segment\nout-of-distribution actions without manual annotation. To address this issue,\nwe formally define the open-world action segmentation problem and propose a\nstructured framework for detecting and segmenting unseen actions. Our framework\nintroduces three key innovations: 1) an Enhanced Pyramid Graph Convolutional\nNetwork (EPGCN) with a novel decoder module for robust spatiotemporal feature\nupsampling. 2) Mixup-based training to synthesize out-of-distribution data,\neliminating reliance on manual annotations. 3) A novel Temporal Clustering loss\nthat groups in-distribution actions while distancing out-of-distribution\nsamples.\n  We evaluate our framework on two challenging human-object interaction\nrecognition datasets: Bimanual Actions and 2 Hands and Object (H2O) datasets.\nExperimental results demonstrate significant improvements over state-of-the-art\naction segmentation models across multiple open-set evaluation metrics,\nachieving 16.9% and 34.6% relative gains in open-set segmentation (F1@50) and\nout-of-distribution detection performances (AUROC), respectively. Additionally,\nwe conduct an in-depth ablation study to assess the impact of each proposed\ncomponent, identifying the optimal framework configuration for open-world\naction segmentation.", "AI": {"tldr": "论文提出了一种开放世界动作分割框架，通过增强的金字塔图卷积网络、基于Mixup的训练和时序聚类损失，显著提升了开放集分割性能。", "motivation": "现有方法在封闭世界动作分割表现良好，但难以泛化到开放世界场景，需要无需手动标注的模型来处理新动作。", "method": "提出EPGCN网络、Mixup训练和时序聚类损失，用于检测和分割未见动作。", "result": "在两个数据集上，开放集分割性能提升16.9%，离群检测性能提升34.6%。", "conclusion": "框架有效解决了开放世界动作分割问题，并通过消融研究验证了各组件的重要性。"}}
{"id": "2507.00789", "pdf": "https://arxiv.org/pdf/2507.00789", "abs": "https://arxiv.org/abs/2507.00789", "authors": ["Ziji Lu"], "title": "OptiPrune: Boosting Prompt-Image Consistency with Attention-Guided Noise and Dynamic Token Selection", "categories": ["cs.CV"], "comment": null, "summary": "Text-to-image diffusion models often struggle to achieve accurate semantic\nalignment between generated images and text prompts while maintaining\nefficiency for deployment on resource-constrained hardware. Existing approaches\neither incur substantial computational overhead through noise optimization or\ncompromise semantic fidelity by aggressively pruning tokens. In this work, we\npropose OptiPrune, a unified framework that combines distribution-aware initial\nnoise optimization with similarity-based token pruning to address both\nchallenges simultaneously. Specifically, (1) we introduce a distribution-aware\nnoise optimization module guided by attention scores to steer the initial\nlatent noise toward semantically meaningful regions, mitigating issues such as\nsubject neglect and feature entanglement; (2) we design a hardware-efficient\ntoken pruning strategy that selects representative base tokens via patch-wise\nsimilarity, injects randomness to enhance generalization, and recovers pruned\ntokens using maximum similarity copying before attention operations. Our method\npreserves the Gaussian prior during noise optimization and enables efficient\ninference without sacrificing alignment quality. Experiments on benchmark\ndatasets, including Animal-Animal, demonstrate that OptiPrune achieves\nstate-of-the-art prompt-image consistency with significantly reduced\ncomputational cost.", "AI": {"tldr": "OptiPrune结合分布感知噪声优化和基于相似性的令牌剪枝，提升文本到图像扩散模型的语义对齐效率。", "motivation": "解决现有方法在计算开销和语义保真度之间的权衡问题。", "method": "1. 分布感知噪声优化模块；2. 硬件高效的令牌剪枝策略。", "result": "在基准数据集上实现最优的提示-图像一致性，显著降低计算成本。", "conclusion": "OptiPrune同时解决了语义对齐和效率问题，适用于资源受限硬件。"}}
{"id": "2507.00790", "pdf": "https://arxiv.org/pdf/2507.00790", "abs": "https://arxiv.org/abs/2507.00790", "authors": ["Huaqiu Li", "Yong Wang", "Tongwen Huang", "Hailang Huang", "Haoqian Wang", "Xiangxiang Chu"], "title": "LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Unified image restoration is a significantly challenging task in low-level\nvision. Existing methods either make tailored designs for specific tasks,\nlimiting their generalizability across various types of degradation, or rely on\ntraining with paired datasets, thereby suffering from closed-set constraints.\nTo address these issues, we propose a novel, dataset-free, and unified approach\nthrough recurrent posterior sampling utilizing a pretrained latent diffusion\nmodel. Our method incorporates the multimodal understanding model to provide\nsematic priors for the generative model under a task-blind condition.\nFurthermore, it utilizes a lightweight module to align the degraded input with\nthe generated preference of the diffusion model, and employs recurrent\nrefinement for posterior sampling. Extensive experiments demonstrate that our\nmethod outperforms state-of-the-art methods, validating its effectiveness and\nrobustness. Our code and data will be available at\nhttps://github.com/AMAP-ML/LD-RPS.", "AI": {"tldr": "提出一种基于预训练潜在扩散模型的无数据集统一图像修复方法，通过多模态理解模型提供语义先验，并利用轻量级模块和循环细化提升效果。", "motivation": "现有方法要么针对特定任务设计，泛化性差；要么依赖配对数据集，受限于闭集约束。", "method": "利用预训练潜在扩散模型，结合多模态理解模型提供语义先验，使用轻量级模块对齐退化输入，并通过循环细化进行后验采样。", "result": "实验表明，该方法优于现有技术，验证了其有效性和鲁棒性。", "conclusion": "该方法为统一图像修复提供了一种无数据集、任务盲的高效解决方案。"}}
{"id": "2507.00257", "pdf": "https://arxiv.org/pdf/2507.00257", "abs": "https://arxiv.org/abs/2507.00257", "authors": ["Davide Salaorni", "Vincenzo De Paola", "Samuele Delpero", "Giovanni Dispoto", "Paolo Bonetti", "Alessio Russo", "Giuseppe Calcagno", "Francesco Trovò", "Matteo Papini", "Alberto Maria Metelli", "Marco Mussi", "Marcello Restelli"], "title": "Gym4ReaL: A Suite for Benchmarking Real-World Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages", "summary": "In recent years, \\emph{Reinforcement Learning} (RL) has made remarkable\nprogress, achieving superhuman performance in a wide range of simulated\nenvironments. As research moves toward deploying RL in real-world applications,\nthe field faces a new set of challenges inherent to real-world settings, such\nas large state-action spaces, non-stationarity, and partial observability.\nDespite their importance, these challenges are often underexplored in current\nbenchmarks, which tend to focus on idealized, fully observable, and stationary\nenvironments, often neglecting to incorporate real-world complexities\nexplicitly. In this paper, we introduce \\texttt{Gym4ReaL}, a comprehensive\nsuite of realistic environments designed to support the development and\nevaluation of RL algorithms that can operate in real-world scenarios. The suite\nincludes a diverse set of tasks that expose algorithms to a variety of\npractical challenges. Our experimental results show that, in these settings,\nstandard RL algorithms confirm their competitiveness against rule-based\nbenchmarks, motivating the development of new methods to fully exploit the\npotential of RL to tackle the complexities of real-world tasks.", "AI": {"tldr": "论文介绍了Gym4ReaL，一套针对现实世界复杂性的强化学习环境，旨在推动RL算法在真实场景中的应用。", "motivation": "当前RL研究多关注理想化环境，忽视了现实世界的挑战（如大状态-动作空间、非平稳性和部分可观测性）。", "method": "开发了Gym4ReaL，包含多样化的任务，模拟现实世界复杂性。", "result": "标准RL算法在这些环境中表现优于基于规则的基准，但仍需新方法以充分应对现实挑战。", "conclusion": "Gym4ReaL为RL在现实场景中的应用提供了重要工具，并呼吁开发更强大的算法。"}}
{"id": "2507.00792", "pdf": "https://arxiv.org/pdf/2507.00792", "abs": "https://arxiv.org/abs/2507.00792", "authors": ["Hendric Voss", "Stefan Kopp"], "title": "Real-Time Inverse Kinematics for Generating Multi-Constrained Movements of Virtual Human Characters", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "Generating accurate and realistic virtual human movements in real-time is of\nhigh importance for a variety of applications in computer graphics, interactive\nvirtual environments, robotics, and biomechanics. This paper introduces a novel\nreal-time inverse kinematics (IK) solver specifically designed for realistic\nhuman-like movement generation. Leveraging the automatic differentiation and\njust-in-time compilation of TensorFlow, the proposed solver efficiently handles\ncomplex articulated human skeletons with high degrees of freedom. By treating\nforward and inverse kinematics as differentiable operations, our method\neffectively addresses common challenges such as error accumulation and\ncomplicated joint limits in multi-constrained problems, which are critical for\nrealistic human motion modeling. We demonstrate the solver's effectiveness on\nthe SMPLX human skeleton model, evaluating its performance against widely used\niterative-based IK algorithms, like Cyclic Coordinate Descent (CCD), FABRIK,\nand the nonlinear optimization algorithm IPOPT. Our experiments cover both\nsimple end-effector tasks and sophisticated, multi-constrained problems with\nrealistic joint limits. Results indicate that our IK solver achieves real-time\nperformance, exhibiting rapid convergence, minimal computational overhead per\niteration, and improved success rates compared to existing methods. The project\ncode is available at https://github.com/hvoss-techfak/TF-JAX-IK", "AI": {"tldr": "提出了一种基于TensorFlow的实时逆向运动学（IK）求解器，用于生成逼真的人体运动，解决了多约束问题中的误差累积和关节限制问题。", "motivation": "实时生成逼真的人体运动对计算机图形学、虚拟环境、机器人学和生物力学等领域至关重要。", "method": "利用TensorFlow的自动微分和即时编译技术，将正向和逆向运动学视为可微分操作，处理复杂的人体骨骼模型。", "result": "在SMPLX模型上测试，相比CCD、FABRIK和IPOPT等方法，新求解器具有实时性能、快速收敛和更高的成功率。", "conclusion": "该IK求解器在多约束问题中表现优异，适用于实时应用。"}}
{"id": "2507.00802", "pdf": "https://arxiv.org/pdf/2507.00802", "abs": "https://arxiv.org/abs/2507.00802", "authors": ["Minye Shao", "Xingyu Miao", "Haoran Duan", "Zeyu Wang", "Jingkun Chen", "Yawen Huang", "Xian Wu", "Jingjing Deng", "Yang Long", "Yefeng Zheng"], "title": "TRACE: Temporally Reliable Anatomically-Conditioned 3D CT Generation with Enhanced Efficiency", "categories": ["cs.CV"], "comment": "Accepted to MICCAI 2025 (this version is not peer-reviewed; it is the\n  preprint version). MICCAI proceedings DOI will appear here", "summary": "3D medical image generation is essential for data augmentation and patient\nprivacy, calling for reliable and efficient models suited for clinical\npractice. However, current methods suffer from limited anatomical fidelity,\nrestricted axial length, and substantial computational cost, placing them\nbeyond reach for regions with limited resources and infrastructure. We\nintroduce TRACE, a framework that generates 3D medical images with\nspatiotemporal alignment using a 2D multimodal-conditioned diffusion approach.\nTRACE models sequential 2D slices as video frame pairs, combining segmentation\npriors and radiology reports for anatomical alignment, incorporating optical\nflow to sustain temporal coherence. During inference, an overlapping-frame\nstrategy links frame pairs into a flexible length sequence, reconstructed into\na spatiotemporally and anatomically aligned 3D volume. Experimental results\ndemonstrate that TRACE effectively balances computational efficiency with\npreserving anatomical fidelity and spatiotemporal consistency. Code is\navailable at: https://github.com/VinyehShaw/TRACE.", "AI": {"tldr": "TRACE是一个基于2D多模态条件扩散的框架，用于生成具有时空对齐的3D医学图像，解决了现有方法在解剖保真度、轴向长度和计算成本上的限制。", "motivation": "当前3D医学图像生成方法存在解剖保真度不足、轴向长度受限和计算成本高的问题，限制了其在资源有限地区的应用。", "method": "TRACE通过将2D切片建模为视频帧对，结合分割先验和放射学报告实现解剖对齐，并利用光流保持时间一致性。推理时采用重叠帧策略生成灵活长度的序列，最终重建为3D体积。", "result": "实验表明，TRACE在计算效率和保持解剖保真度与时空一致性之间取得了良好平衡。", "conclusion": "TRACE为3D医学图像生成提供了一种高效且可靠的解决方案，适用于临床实践。"}}
{"id": "2507.00268", "pdf": "https://arxiv.org/pdf/2507.00268", "abs": "https://arxiv.org/abs/2507.00268", "authors": ["Oren Fivel", "Matan Rudman", "Kobi Cohen"], "title": "Control-Optimized Deep Reinforcement Learning for Artificially Intelligent Autonomous Systems", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "comment": "27 pages, 10 figures", "summary": "Deep reinforcement learning (DRL) has become a powerful tool for complex\ndecision-making in machine learning and AI. However, traditional methods often\nassume perfect action execution, overlooking the uncertainties and deviations\nbetween an agent's selected actions and the actual system response. In\nreal-world applications, such as robotics, mechatronics, and communication\nnetworks, execution mismatches arising from system dynamics, hardware\nconstraints, and latency can significantly degrade performance. This work\nadvances AI by developing a novel control-optimized DRL framework that\nexplicitly models and compensates for action execution mismatches, a challenge\nlargely overlooked in existing methods. Our approach establishes a structured\ntwo-stage process: determining the desired action and selecting the appropriate\ncontrol signal to ensure proper execution. It trains the agent while accounting\nfor action mismatches and controller corrections. By incorporating these\nfactors into the training process, the AI agent optimizes the desired action\nwith respect to both the actual control signal and the intended outcome,\nexplicitly considering execution errors. This approach enhances robustness,\nensuring that decision-making remains effective under real-world uncertainties.\nOur approach offers a substantial advancement for engineering practice by\nbridging the gap between idealized learning and real-world implementation. It\nequips intelligent agents operating in engineering environments with the\nability to anticipate and adjust for actuation errors and system disturbances\nduring training. We evaluate the framework in five widely used open-source\nmechanical simulation environments we restructured and developed to reflect\nreal-world operating conditions, showcasing its robustness against\nuncertainties and offering a highly practical and efficient solution for\ncontrol-oriented applications.", "AI": {"tldr": "提出了一种新型控制优化的深度强化学习框架，显式建模并补偿动作执行不匹配问题，提升了在现实不确定性下的决策鲁棒性。", "motivation": "传统深度强化学习方法假设动作执行完美，忽略了实际系统响应中的不确定性和偏差，导致在机器人、机电和通信网络等现实应用中性能下降。", "method": "采用两阶段结构化过程：确定期望动作并选择适当的控制信号以确保正确执行，训练过程中考虑动作不匹配和控制器修正。", "result": "在五个开源机械仿真环境中验证了框架的鲁棒性，展示了其在控制导向应用中的高效性和实用性。", "conclusion": "该框架填补了理想化学习与现实实现之间的差距，为工程实践提供了显著进步，使智能代理能够在训练中预见并调整执行误差和系统扰动。"}}
{"id": "2507.00817", "pdf": "https://arxiv.org/pdf/2507.00817", "abs": "https://arxiv.org/abs/2507.00817", "authors": ["Jiaming Zhang", "Rui Hu", "Qing Guo", "Wei Yang Bryan Lim"], "title": "CAVALRY-V: A Large-Scale Generator Framework for Adversarial Attacks on Video MLLMs", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Video Multimodal Large Language Models (V-MLLMs) have shown impressive\ncapabilities in temporal reasoning and cross-modal understanding, yet their\nvulnerability to adversarial attacks remains underexplored due to unique\nchallenges: complex cross-modal reasoning mechanisms, temporal dependencies,\nand computational constraints. We present CAVALRY-V (Cross-modal\nLanguage-Vision Adversarial Yielding for Videos), a novel framework that\ndirectly targets the critical interface between visual perception and language\ngeneration in V-MLLMs. Our approach introduces two key innovations: (1) a\ndual-objective semantic-visual loss function that simultaneously disrupts the\nmodel's text generation logits and visual representations to undermine\ncross-modal integration, and (2) a computationally efficient two-stage\ngenerator framework that combines large-scale pre-training for cross-model\ntransferability with specialized fine-tuning for spatiotemporal coherence.\nEmpirical evaluation on comprehensive video understanding benchmarks\ndemonstrates that CAVALRY-V significantly outperforms existing attack methods,\nachieving 22.8% average improvement over the best baseline attacks on both\ncommercial systems (GPT-4.1, Gemini 2.0) and open-source models (QwenVL-2.5,\nInternVL-2.5, Llava-Video, Aria, MiniCPM-o-2.6). Our framework achieves\nflexibility through implicit temporal coherence modeling rather than explicit\nregularization, enabling significant performance improvements even on image\nunderstanding (34.4% average gain). This capability demonstrates CAVALRY-V's\npotential as a foundational approach for adversarial research across multimodal\nsystems.", "AI": {"tldr": "CAVALRY-V是一种针对视频多模态大语言模型（V-MLLMs）的对抗攻击框架，通过双目标语义-视觉损失函数和高效的两阶段生成器，显著提升了攻击效果。", "motivation": "探索V-MLLMs在对抗攻击中的脆弱性，解决跨模态推理、时间依赖性和计算限制等挑战。", "method": "提出双目标语义-视觉损失函数和两阶段生成器框架，结合大规模预训练和微调。", "result": "在多个基准测试中显著优于现有攻击方法，平均提升22.8%，并在图像理解任务中表现优异。", "conclusion": "CAVALRY-V为多模态系统的对抗研究提供了基础性方法。"}}
{"id": "2507.00269", "pdf": "https://arxiv.org/pdf/2507.00269", "abs": "https://arxiv.org/abs/2507.00269", "authors": ["Omar Claflin"], "title": "Feature Integration Spaces: Joint Training Reveals Dual Encoding in Neural Network Representations", "categories": ["q-bio.NC", "cs.AI"], "comment": null, "summary": "Current sparse autoencoder (SAE) approaches to neural network\ninterpretability assume that activations can be decomposed through linear\nsuperposition into sparse, interpretable features. Despite high reconstruction\nfidelity, SAEs consistently fail to eliminate polysemanticity and exhibit\npathological behavioral errors. We propose that neural networks encode\ninformation in two complementary spaces compressed into the same substrate:\nfeature identity and feature integration. To test this dual encoding\nhypothesis, we develop sequential and joint-training architectures to capture\nidentity and integration patterns simultaneously. Joint training achieves 41.3%\nreconstruction improvement and 51.6% reduction in KL divergence errors. This\narchitecture spontaneously develops bimodal feature organization: low squared\nnorm features contributing to integration pathways and the rest contributing\ndirectly to the residual. Small nonlinear components (3% of parameters) achieve\n16.5% standalone improvements, demonstrating parameter-efficient capture of\ncomputational relationships crucial for behavior. Additionally, intervention\nexperiments using 2x2 factorial stimulus designs demonstrated that integration\nfeatures exhibit selective sensitivity to experimental manipulations and\nproduce systematic behavioral effects on model outputs, including significant\ninteraction effects across semantic dimensions. This work provides systematic\nevidence for (1) dual encoding in neural representations, (2) meaningful\nnonlinearly encoded feature interactions, and (3) introduces an architectural\nparadigm shift from post-hoc feature analysis to integrated computational\ndesign, establishing foundations for next-generation SAEs.", "AI": {"tldr": "论文提出神经网络信息编码的双重空间假设，开发了同时捕获特征身份和整合的架构，显著提升了重建性能和减少了错误。", "motivation": "现有稀疏自编码器（SAE）方法假设激活可通过线性叠加分解为稀疏、可解释特征，但未能消除多义性且存在行为错误，因此提出双重编码假设。", "method": "开发了顺序和联合训练架构，同时捕获特征身份和整合模式，联合训练显著提升了性能。", "result": "联合训练重建性能提升41.3%，KL散度错误减少51.6%，非线性组件（3%参数）独立提升16.5%。干预实验显示整合特征对实验操作敏感且影响模型输出。", "conclusion": "研究为神经网络双重编码、非线性特征交互提供了系统证据，并提出了从后验特征分析到集成计算设计的架构范式转变。"}}
{"id": "2507.00822", "pdf": "https://arxiv.org/pdf/2507.00822", "abs": "https://arxiv.org/abs/2507.00822", "authors": ["Yasser El Jarida", "Youssef Iraqi", "Loubna Mekouar"], "title": "Instant Particle Size Distribution Measurement Using CNNs Trained on Synthetic Data", "categories": ["cs.CV"], "comment": "Accepted at the Synthetic Data for Computer Vision Workshop @ CVPR\n  2025. 10 pages, 5 figures. Code available at\n  https://github.com/YasserElj/Synthetic-Granular-Gen", "summary": "Accurate particle size distribution (PSD) measurement is important in\nindustries such as mining, pharmaceuticals, and fertilizer manufacturing,\nsignificantly influencing product quality and operational efficiency.\nTraditional PSD methods like sieve analysis and laser diffraction are manual,\ntime-consuming, and limited by particle overlap. Recent developments in\nconvolutional neural networks (CNNs) enable automated, real-time PSD estimation\ndirectly from particle images. In this work, we present a CNN-based methodology\ntrained on realistic synthetic particle imagery generated using Blender's\nadvanced rendering capabilities. Synthetic data sets using this method can\nreplicate various industrial scenarios by systematically varying particle\nshapes, textures, lighting, and spatial arrangements that closely resemble the\nactual configurations. We evaluated three CNN-based architectures, ResNet-50,\nInceptionV3, and EfficientNet-B0, for predicting critical PSD parameters (d10,\nd50, d90). Results demonstrated comparable accuracy across models, with\nEfficientNet-B0 achieving the best computational efficiency suitable for\nreal-time industrial deployment. This approach shows the effectiveness of\nrealistic synthetic data for robust CNN training, which offers significant\npotential for automated industrial PSD monitoring. The code is released at :\nhttps://github.com/YasserElj/Synthetic-Granular-Gen", "AI": {"tldr": "提出了一种基于CNN的方法，利用Blender生成的合成粒子图像进行PSD测量，评估了三种CNN架构，EfficientNet-B0表现最佳。", "motivation": "传统PSD测量方法耗时且受限于粒子重叠，需要自动化实时解决方案。", "method": "使用Blender生成合成粒子图像训练CNN，评估ResNet-50、InceptionV3和EfficientNet-B0。", "result": "EfficientNet-B0在计算效率上表现最佳，适合工业实时部署。", "conclusion": "合成数据训练CNN在工业PSD监测中具有潜力。"}}
{"id": "2507.00275", "pdf": "https://arxiv.org/pdf/2507.00275", "abs": "https://arxiv.org/abs/2507.00275", "authors": ["Prabhat Nagarajan", "Martha White", "Marlos C. Machado"], "title": "Double Q-learning for Value-based Deep Reinforcement Learning, Revisited", "categories": ["cs.LG", "cs.AI"], "comment": "44 pages", "summary": "Overestimation is pervasive in reinforcement learning (RL), including in\nQ-learning, which forms the algorithmic basis for many value-based deep RL\nalgorithms. Double Q-learning is an algorithm introduced to address\nQ-learning's overestimation by training two Q-functions and using both to\nde-correlate action-selection and action-evaluation in bootstrap targets.\nShortly after Q-learning was adapted to deep RL in the form of deep Q-networks\n(DQN), Double Q-learning was adapted to deep RL in the form of Double DQN.\nHowever, Double DQN only loosely adapts Double Q-learning, forgoing the\ntraining of two different Q-functions that bootstrap off one another. In this\npaper, we study algorithms that adapt this core idea of Double Q-learning for\nvalue-based deep RL. We term such algorithms Deep Double Q-learning (DDQL). Our\naim is to understand whether DDQL exhibits less overestimation than Double DQN\nand whether performant instantiations of DDQL exist. We answer both questions\naffirmatively, demonstrating that DDQL reduces overestimation and outperforms\nDouble DQN in aggregate across 57 Atari 2600 games, without requiring\nadditional hyperparameters. We also study several aspects of DDQL, including\nits network architecture, replay ratio, and minibatch sampling strategy.", "AI": {"tldr": "论文研究了深度强化学习中的过估计问题，提出了Deep Double Q-learning (DDQL)算法，验证了其优于Double DQN的性能。", "motivation": "解决Q-learning在深度强化学习中的过估计问题，并探索更有效的算法。", "method": "提出DDQL算法，通过训练两个Q函数并相互引导来减少过估计。", "result": "DDQL在57个Atari 2600游戏中表现优于Double DQN，且无需额外超参数。", "conclusion": "DDQL有效减少了过估计，性能优于Double DQN，并探讨了其网络架构等细节。"}}
{"id": "2507.00825", "pdf": "https://arxiv.org/pdf/2507.00825", "abs": "https://arxiv.org/abs/2507.00825", "authors": ["Hongxing Peng", "Lide Chen", "Hui Zhu", "Yan Chen"], "title": "High-Frequency Semantics and Geometric Priors for End-to-End Detection Transformers in Challenging UAV Imagery", "categories": ["cs.CV", "I.2.10; I.4.8; I.5.1"], "comment": "14 pages, 9 figures, to appear in KBS", "summary": "Unmanned Aerial Vehicle-based Object Detection (UAV-OD) faces substantial\nchallenges, including small target sizes, high-density distributions, and\ncluttered backgrounds in UAV imagery. Current algorithms often depend on\nhand-crafted components like anchor boxes, which demand fine-tuning and exhibit\nlimited generalization, and Non-Maximum Suppression (NMS), which is\nthreshold-sensitive and prone to misclassifying dense objects. These generic\narchitectures thus struggle to adapt to aerial imaging characteristics,\nresulting in performance limitations. Moreover, emerging end-to-end frameworks\nhave yet to effectively mitigate these aerial-specific challenges.To address\nthese issues, we propose HEGS-DETR, a comprehensively enhanced, real-time\nDetection Transformer framework tailored for UAVs. First, we introduce the\nHigh-Frequency Enhanced Semantics Network (HFESNet) as a novel backbone.\nHFESNet preserves critical high-frequency spatial details to extract robust\nsemantic features, thereby improving discriminative capability for small and\noccluded targets in complex backgrounds. Second, our Efficient Small Object\nPyramid (ESOP) strategy strategically fuses high-resolution feature maps with\nminimal computational overhead, significantly boosting small object detection.\nFinally, the proposed Selective Query Recollection (SQR) and Geometry-Aware\nPositional Encoding (GAPE) modules enhance the detector's decoder stability and\nlocalization accuracy, effectively optimizing bounding boxes and providing\nexplicit spatial priors for dense scenes. Experiments on the VisDrone dataset\ndemonstrate that HEGS-DETR achieves a 5.1\\% AP$_{50}$ and 3.8\\% AP increase\nover the baseline, while maintaining real-time speed and reducing parameter\ncount by 4M.", "AI": {"tldr": "HEGS-DETR是一种针对无人机图像检测的改进Transformer框架，通过高频增强语义网络、高效小目标金字塔和选择性查询重收集模块，显著提升了小目标和密集目标的检测性能。", "motivation": "无人机图像检测面临小目标、高密度分布和复杂背景等挑战，现有方法依赖手工组件且泛化能力有限。", "method": "提出HFESNet作为骨干网络，结合ESOP策略和SQR、GAPE模块，优化特征提取和检测精度。", "result": "在VisDrone数据集上，AP$_{50}$提升5.1%，AP提升3.8%，同时保持实时速度和减少参数。", "conclusion": "HEGS-DETR有效解决了无人机图像检测的特定问题，性能显著优于基线方法。"}}
{"id": "2507.00286", "pdf": "https://arxiv.org/pdf/2507.00286", "abs": "https://arxiv.org/abs/2507.00286", "authors": ["Tanusree Sharma", "Yu-Yun Tseng", "Lotus Zhang", "Ayae Ide", "Kelly Avery Mack", "Leah Findlater", "Danna Gurari", "Yang Wang"], "title": "Visual Privacy Management with Generative AI for Blind and Low-Vision People", "categories": ["cs.HC", "cs.AI", "cs.ET"], "comment": null, "summary": "Blind and low vision (BLV) individuals use Generative AI (GenAI) tools to\ninterpret and manage visual content in their daily lives. While such tools can\nenhance the accessibility of visual content and so enable greater user\nindependence, they also introduce complex challenges around visual privacy. In\nthis paper, we investigate the current practices and future design preferences\nof blind and low vision individuals through an interview study with 21\nparticipants. Our findings reveal a range of current practices with GenAI that\nbalance privacy, efficiency, and emotional agency, with users accounting for\nprivacy risks across six key scenarios, such as self-presentation,\nindoor/outdoor spatial privacy, social sharing, and handling professional\ncontent. Our findings reveal design preferences, including on-device\nprocessing, zero-retention guarantees, sensitive content redaction,\nprivacy-aware appearance indicators, and multimodal tactile mirrored\ninteraction methods. We conclude with actionable design recommendations to\nsupport user-centered visual privacy through GenAI, expanding the notion of\nprivacy and responsible handling of others data.", "AI": {"tldr": "BLV个体使用GenAI工具管理视觉内容，但面临隐私挑战。研究通过21人访谈揭示了当前实践和未来设计偏好。", "motivation": "探索BLV个体如何平衡GenAI工具的便利性与视觉隐私问题。", "method": "通过21名BLV个体的访谈研究当前实践和设计偏好。", "result": "发现六种隐私风险场景及设计偏好，如设备端处理、敏感内容屏蔽等。", "conclusion": "提出以用户为中心的视觉隐私设计建议，强调数据责任。"}}
{"id": "2507.00845", "pdf": "https://arxiv.org/pdf/2507.00845", "abs": "https://arxiv.org/abs/2507.00845", "authors": ["Peter Pavlík", "Marc Schleiss", "Anna Bou Ezzeddine", "Viera Rozinajová"], "title": "Do Echo Top Heights Improve Deep Learning Nowcasts?", "categories": ["cs.CV", "cs.LG"], "comment": "Pre-review version of an article accepted at Transactions on\n  Large-Scale Data and Knowledge-Centered Systems", "summary": "Precipitation nowcasting -- the short-term prediction of rainfall using\nrecent radar observations -- is critical for weather-sensitive sectors such as\ntransportation, agriculture, and disaster mitigation. While recent deep\nlearning models have shown promise in improving nowcasting skill, most\napproaches rely solely on 2D radar reflectivity fields, discarding valuable\nvertical information available in the full 3D radar volume. In this work, we\nexplore the use of Echo Top Height (ETH), a 2D projection indicating the\nmaximum altitude of radar reflectivity above a given threshold, as an auxiliary\ninput variable for deep learning-based nowcasting. We examine the relationship\nbetween ETH and radar reflectivity, confirming its relevance for predicting\nrainfall intensity. We implement a single-pass 3D U-Net that processes both the\nradar reflectivity and ETH as separate input channels. While our models are\nable to leverage ETH to improve skill at low rain-rate thresholds, results are\ninconsistent at higher intensities and the models with ETH systematically\nunderestimate precipitation intensity. Three case studies are used to\nillustrate how ETH can help in some cases, but also confuse the models and\nincrease the error variance. Nonetheless, the study serves as a foundation for\ncritically assessing the potential contribution of additional variables to\nnowcasting performance.", "AI": {"tldr": "论文探讨了在降水临近预报中使用Echo Top Height（ETH）作为深度学习模型的辅助输入变量，发现ETH在低雨强阈值下能提升预报技能，但在高雨强下效果不一致且会低估降水强度。", "motivation": "降水临近预报对天气敏感行业至关重要，现有深度学习模型多依赖2D雷达反射率，忽略了3D雷达数据中的垂直信息。研究旨在评估ETH作为辅助变量对预报性能的潜在贡献。", "method": "采用单通道3D U-Net模型，同时处理雷达反射率和ETH作为输入通道，分析ETH与雷达反射率的关系及其对降雨强度预测的影响。", "result": "ETH在低雨强阈值下能提升预报技能，但在高雨强下效果不一致且会低估降水强度。案例研究表明ETH在某些情况下有帮助，但也可能增加误差方差。", "conclusion": "研究为评估额外变量对临近预报性能的潜在贡献提供了基础，但需进一步探索ETH在高雨强下的适用性。"}}
{"id": "2507.00849", "pdf": "https://arxiv.org/pdf/2507.00849", "abs": "https://arxiv.org/abs/2507.00849", "authors": ["Wei Li", "Jiaman Tang", "Yang Li", "Beihao Xia", "Ligang Tan", "Hongmao Qin"], "title": "UAVD-Mamba: Deformable Token Fusion Vision Mamba for Multimodal UAV Detection", "categories": ["cs.CV"], "comment": "The paper was accepted by the 36th IEEE Intelligent Vehicles\n  Symposium (IEEE IV 2025)", "summary": "Unmanned Aerial Vehicle (UAV) object detection has been widely used in\ntraffic management, agriculture, emergency rescue, etc. However, it faces\nsignificant challenges, including occlusions, small object sizes, and irregular\nshapes. These challenges highlight the necessity for a robust and efficient\nmultimodal UAV object detection method. Mamba has demonstrated considerable\npotential in multimodal image fusion. Leveraging this, we propose UAVD-Mamba, a\nmultimodal UAV object detection framework based on Mamba architectures. To\nimprove geometric adaptability, we propose the Deformable Token Mamba Block\n(DTMB) to generate deformable tokens by incorporating adaptive patches from\ndeformable convolutions alongside normal patches from normal convolutions,\nwhich serve as the inputs to the Mamba Block. To optimize the multimodal\nfeature complementarity, we design two separate DTMBs for the RGB and infrared\n(IR) modalities, with the outputs from both DTMBs integrated into the Mamba\nBlock for feature extraction and into the Fusion Mamba Block for feature\nfusion. Additionally, to improve multiscale object detection, especially for\nsmall objects, we stack four DTMBs at different scales to produce multiscale\nfeature representations, which are then sent to the Detection Neck for Mamba\n(DNM). The DNM module, inspired by the YOLO series, includes modifications to\nthe SPPF and C3K2 of YOLOv11 to better handle the multiscale features. In\nparticular, we employ cross-enhanced spatial attention before the DTMB and\ncross-channel attention after the Fusion Mamba Block to extract more\ndiscriminative features. Experimental results on the DroneVehicle dataset show\nthat our method outperforms the baseline OAFA method by 3.6% in the mAP metric.\nCodes will be released at https://github.com/GreatPlum-hnu/UAVD-Mamba.git.", "AI": {"tldr": "提出了一种基于Mamba架构的多模态无人机目标检测框架UAVD-Mamba，通过改进几何适应性和多模态特征互补性，显著提升了检测性能。", "motivation": "无人机目标检测面临遮挡、小目标和形状不规则等挑战，需要一种鲁棒且高效的多模态检测方法。", "method": "设计了Deformable Token Mamba Block (DTMB) 生成可变形令牌，并采用多模态特征融合和多尺度特征提取策略。", "result": "在DroneVehicle数据集上，mAP指标比基线OAFA方法提高了3.6%。", "conclusion": "UAVD-Mamba框架在多模态无人机目标检测中表现出色，代码将开源。"}}
{"id": "2507.00288", "pdf": "https://arxiv.org/pdf/2507.00288", "abs": "https://arxiv.org/abs/2507.00288", "authors": ["Claire Li", "David Freeborn"], "title": "Reconfiguring Digital Accountability: AI-Powered Innovations and Transnational Governance in a Postnational Accounting Context", "categories": ["econ.TH", "cs.AI", "cs.ET"], "comment": "22 pages", "summary": "This study explores how AI-powered digital innovations are reshaping\norganisational accountability in a transnational governance context. As AI\nsystems increasingly mediate decision-making in domains such as auditing and\nfinancial reporting, traditional mechanisms of accountability, based on\ncontrol, transparency, and auditability, are being destabilised. We integrate\nthe Technology Acceptance Model (TAM), Actor-Network Theory (ANT), and\ninstitutional theory to examine how organisations adopt AI technologies in\nresponse to regulatory, ethical, and cultural pressures that transcend national\nboundaries. We argue that accountability is co-constructed within global\nsocio-technical networks, shaped not only by user perceptions but also by\ngovernance logics and normative expectations. Extending TAM, we incorporate\ncompliance and legitimacy as key factors in perceived usefulness and usability.\nDrawing on ANT, we reconceptualise accountability as a relational and emergent\nproperty of networked assemblages. We propose two organisational strategies\nincluding internal governance reconfiguration and external actor-network\nengagement to foster responsible, legitimate, and globally accepted AI adoption\nin the accounting domain.", "AI": {"tldr": "研究探讨AI驱动的数字创新如何重塑跨国治理中的组织问责制，整合TAM、ANT和制度理论分析AI技术采纳。", "motivation": "传统问责机制因AI系统介入决策而受到挑战，需研究跨国背景下组织如何应对监管、伦理和文化压力。", "method": "结合TAM、ANT和制度理论，分析用户感知、治理逻辑和规范期望对问责的共同构建。", "result": "提出两种组织策略：内部治理重构和外部行动者网络参与，以促进负责任的AI采纳。", "conclusion": "问责是网络化组合的关系性和涌现属性，需通过多理论整合实现全球接受的AI应用。"}}
{"id": "2507.00852", "pdf": "https://arxiv.org/pdf/2507.00852", "abs": "https://arxiv.org/abs/2507.00852", "authors": ["Fatemeh Sadat Daneshmand"], "title": "Robust Component Detection for Flexible Manufacturing: A Deep Learning Approach to Tray-Free Object Recognition under Variable Lighting", "categories": ["cs.CV"], "comment": null, "summary": "Flexible manufacturing systems in Industry 4.0 require robots capable of\nhandling objects in unstructured environments without rigid positioning\nconstraints. This paper presents a computer vision system that enables\nindustrial robots to detect and grasp pen components in arbitrary orientations\nwithout requiring structured trays, while maintaining robust performance under\nvarying lighting conditions. We implement and evaluate a Mask R-CNN-based\napproach on a complete pen manufacturing line at ZHAW, addressing three\ncritical challenges: object detection without positional constraints,\nrobustness to extreme lighting variations, and reliable performance with\ncost-effective cameras. Our system achieves 95% detection accuracy across\ndiverse lighting conditions while eliminating the need for structured component\nplacement, demonstrating a 30% reduction in setup time and significant\nimprovement in manufacturing flexibility. The approach is validated through\nextensive testing under four distinct lighting scenarios, showing practical\napplicability for real-world industrial deployment.", "AI": {"tldr": "提出了一种基于Mask R-CNN的计算机视觉系统，使工业机器人能够在非结构化环境中检测和抓取笔组件，无需固定位置约束，并在不同光照条件下保持稳定性能。", "motivation": "工业4.0中的柔性制造系统需要机器人能够在非结构化环境中处理物体，无需刚性定位约束。", "method": "采用Mask R-CNN方法，解决了无位置约束的物体检测、极端光照变化的鲁棒性以及低成本相机的可靠性能等挑战。", "result": "系统在多样光照条件下达到95%的检测准确率，减少了30%的安装时间，显著提升了制造灵活性。", "conclusion": "通过四种不同光照场景的广泛测试验证了该系统的实用性，适用于实际工业部署。"}}
{"id": "2507.00861", "pdf": "https://arxiv.org/pdf/2507.00861", "abs": "https://arxiv.org/abs/2507.00861", "authors": ["Xiaoshuai Hao", "Lingdong Kong", "Rong Yin", "Pengwei Wang", "Jing Zhang", "Yunfeng Diao", "Shu Zhao"], "title": "SafeMap: Robust HD Map Construction from Incomplete Observations", "categories": ["cs.CV"], "comment": "Accepted by ICML 2025", "summary": "Robust high-definition (HD) map construction is vital for autonomous driving,\nyet existing methods often struggle with incomplete multi-view camera data.\nThis paper presents SafeMap, a novel framework specifically designed to secure\naccuracy even when certain camera views are missing. SafeMap integrates two key\ncomponents: the Gaussian-based Perspective View Reconstruction (G-PVR) module\nand the Distillation-based Bird's-Eye-View (BEV) Correction (D-BEVC) module.\nG-PVR leverages prior knowledge of view importance to dynamically prioritize\nthe most informative regions based on the relationships among available camera\nviews. Furthermore, D-BEVC utilizes panoramic BEV features to correct the BEV\nrepresentations derived from incomplete observations. Together, these\ncomponents facilitate the end-to-end map reconstruction and robust HD map\ngeneration. SafeMap is easy to implement and integrates seamlessly into\nexisting systems, offering a plug-and-play solution for enhanced robustness.\nExperimental results demonstrate that SafeMap significantly outperforms\nprevious methods in both complete and incomplete scenarios, highlighting its\nsuperior performance and reliability.", "AI": {"tldr": "SafeMap是一种新颖的框架，用于在部分相机视角缺失的情况下构建高精度HD地图，通过G-PVR和D-BEVC模块实现鲁棒性。", "motivation": "现有方法在多视角相机数据不完整时表现不佳，影响自动驾驶的HD地图构建。", "method": "SafeMap结合G-PVR模块（基于高斯模型动态优先处理关键区域）和D-BEVC模块（利用全景BEV特征校正不完整观测）。", "result": "实验表明，SafeMap在完整和不完整数据场景下均显著优于现有方法。", "conclusion": "SafeMap提供了一种即插即用的鲁棒解决方案，适用于自动驾驶中的HD地图构建。"}}
{"id": "2507.00868", "pdf": "https://arxiv.org/pdf/2507.00868", "abs": "https://arxiv.org/abs/2507.00868", "authors": ["Simon Reiß", "Zdravko Marinov", "Alexander Jaus", "Constantin Seibold", "M. Saquib Sarfraz", "Erik Rodner", "Rainer Stiefelhagen"], "title": "Is Visual in-Context Learning for Compositional Medical Tasks within Reach?", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025", "summary": "In this paper, we explore the potential of visual in-context learning to\nenable a single model to handle multiple tasks and adapt to new tasks during\ntest time without re-training. Unlike previous approaches, our focus is on\ntraining in-context learners to adapt to sequences of tasks, rather than\nindividual tasks. Our goal is to solve complex tasks that involve multiple\nintermediate steps using a single model, allowing users to define entire vision\npipelines flexibly at test time. To achieve this, we first examine the\nproperties and limitations of visual in-context learning architectures, with a\nparticular focus on the role of codebooks. We then introduce a novel method for\ntraining in-context learners using a synthetic compositional task generation\nengine. This engine bootstraps task sequences from arbitrary segmentation\ndatasets, enabling the training of visual in-context learners for compositional\ntasks. Additionally, we investigate different masking-based training objectives\nto gather insights into how to train models better for solving complex,\ncompositional tasks. Our exploration not only provides important insights\nespecially for multi-modal medical task sequences but also highlights\nchallenges that need to be addressed.", "AI": {"tldr": "探索视觉上下文学习的潜力，使单一模型能处理多任务并在测试时适应新任务，无需重新训练。", "motivation": "解决复杂任务，允许用户在测试时灵活定义整个视觉流程，而非仅适应单个任务。", "method": "研究视觉上下文学习架构的特性与限制，提出基于合成组合任务生成引擎的新训练方法，并探索掩码训练目标。", "result": "为多模态医疗任务序列提供重要见解，同时揭示需解决的挑战。", "conclusion": "视觉上下文学习在复杂任务中具有潜力，但仍需进一步研究以克服现有挑战。"}}
{"id": "2507.00886", "pdf": "https://arxiv.org/pdf/2507.00886", "abs": "https://arxiv.org/abs/2507.00886", "authors": ["Anna-Maria Halacheva", "Jan-Nico Zaech", "Xi Wang", "Danda Pani Paudel", "Luc Van Gool"], "title": "GaussianVLM: Scene-centric 3D Vision-Language Models using Language-aligned Gaussian Splats for Embodied Reasoning and Beyond", "categories": ["cs.CV", "cs.RO"], "comment": null, "summary": "As multimodal language models advance, their application to 3D scene\nunderstanding is a fast-growing frontier, driving the development of 3D\nVision-Language Models (VLMs). Current methods show strong dependence on object\ndetectors, introducing processing bottlenecks and limitations in taxonomic\nflexibility. To address these limitations, we propose a scene-centric 3D VLM\nfor 3D Gaussian splat scenes that employs language- and task-aware scene\nrepresentations. Our approach directly embeds rich linguistic features into the\n3D scene representation by associating language with each Gaussian primitive,\nachieving early modality alignment. To process the resulting dense\nrepresentations, we introduce a dual sparsifier that distills them into\ncompact, task-relevant tokens via task-guided and location-guided pathways,\nproducing sparse, task-aware global and local scene tokens. Notably, we present\nthe first Gaussian splatting-based VLM, leveraging photorealistic 3D\nrepresentations derived from standard RGB images, demonstrating strong\ngeneralization: it improves performance of prior 3D VLM five folds, in\nout-of-the-domain settings.", "AI": {"tldr": "提出了一种基于3D高斯泼溅的场景中心3D视觉语言模型，通过语言和任务感知的场景表示，解决了现有方法对物体检测器的依赖问题。", "motivation": "现有3D视觉语言模型对物体检测器依赖性强，导致处理瓶颈和分类灵活性受限。", "method": "直接嵌入语言特征到3D场景表示中，通过任务和位置引导的双重稀疏化器生成紧凑的任务相关令牌。", "result": "在域外设置中，性能提升至现有3D VLM的五倍。", "conclusion": "该方法通过高斯泼溅技术实现了高效的3D场景理解和语言对齐，具有强泛化能力。"}}
{"id": "2507.00916", "pdf": "https://arxiv.org/pdf/2507.00916", "abs": "https://arxiv.org/abs/2507.00916", "authors": ["Tianshi Cao", "Marie-Julie Rakotosaona", "Ben Poole", "Federico Tombari", "Michael Niemeyer"], "title": "Masks make discriminative models great again!", "categories": ["cs.CV"], "comment": null, "summary": "We present Image2GS, a novel approach that addresses the challenging problem\nof reconstructing photorealistic 3D scenes from a single image by focusing\nspecifically on the image-to-3D lifting component of the reconstruction\nprocess. By decoupling the lifting problem (converting an image to a 3D model\nrepresenting what is visible) from the completion problem (hallucinating\ncontent not present in the input), we create a more deterministic task suitable\nfor discriminative models. Our method employs visibility masks derived from\noptimized 3D Gaussian splats to exclude areas not visible from the source view\nduring training. This masked training strategy significantly improves\nreconstruction quality in visible regions compared to strong baselines.\nNotably, despite being trained only on masked regions, Image2GS remains\ncompetitive with state-of-the-art discriminative models trained on full target\nimages when evaluated on complete scenes. Our findings highlight the\nfundamental struggle discriminative models face when fitting unseen regions and\ndemonstrate the advantages of addressing image-to-3D lifting as a distinct\nproblem with specialized techniques.", "AI": {"tldr": "Image2GS提出了一种从单图像重建逼真3D场景的新方法，专注于图像到3D的提升问题，通过掩码训练策略显著提升可见区域的重建质量。", "motivation": "解决从单图像重建3D场景的挑战，特别是图像到3D的提升问题，避免生成不可见区域的内容。", "method": "使用优化的3D高斯斑点生成的可见性掩码，在训练中排除不可见区域，专注于可见区域的提升。", "result": "在可见区域的重建质量显著优于基线方法，且与完整场景训练的先进模型竞争。", "conclusion": "Image2GS展示了将图像到3D提升作为独立问题处理的优势，揭示了判别模型在拟合不可见区域时的根本挑战。"}}
{"id": "2507.00347", "pdf": "https://arxiv.org/pdf/2507.00347", "abs": "https://arxiv.org/abs/2507.00347", "authors": ["Sun Ding", "Ude Enebeli", "Atilhan", "Manay", "Ryan Pua", "Kamal Kotak"], "title": "VTS-Guided AI Interaction Workflow for Business Insights", "categories": ["cs.SE", "cs.AI"], "comment": null, "summary": "Modern firms face a flood of dense, unstructured reports. Turning these\ndocuments into usable insights takes heavy effort and is far from agile when\nquick answers are needed. VTS-AI tackles this gap. It integrates Visual\nThinking Strategies, which emphasize evidence-based observation, linking, and\nthinking, into AI agents, so the agents can extract business insights from\nunstructured text, tables, and images at scale. The system works in three tiers\n(micro, meso, macro). It tags issues, links them to source pages, and rolls\nthem into clear action levers stored in a searchable YAML file. In tests on an\n18-page business report, VTS-AI matched the speed of a one-shot ChatGPT prompt\nyet produced richer findings: page locations, verbatim excerpts, severity\nscores, and causal links. Analysts can accept or adjust these outputs in the\nsame IDE, keeping human judgment in the loop. Early results show VTS-AI spots\nthe direction of key metrics and flags where deeper number-crunching is needed.\nNext steps include mapping narrative tags to financial ratios, adding\nfinance-tuned language models through a Model-Context Protocol, and building a\nRisk & Safety Layer to stress-test models and secure data. These upgrades aim\nto make VTS-AI a production-ready, audit-friendly tool for rapid business\nanalysis.", "AI": {"tldr": "VTS-AI结合视觉思维策略与AI技术，从非结构化报告中提取商业洞察，速度快且结果丰富，支持人工调整。", "motivation": "现代企业面临大量非结构化报告，传统方法效率低且不敏捷，VTS-AI旨在填补这一空白。", "method": "VTS-AI采用三层架构（微观、中观、宏观），通过AI代理提取洞察，并生成可搜索的YAML文件。", "result": "测试显示，VTS-AI速度与ChatGPT相当，但提供更丰富的结果（如页面位置、严重性评分等）。", "conclusion": "VTS-AI有望成为生产就绪的快速商业分析工具，未来将扩展功能以支持财务分析和数据安全。"}}
{"id": "2507.00950", "pdf": "https://arxiv.org/pdf/2507.00950", "abs": "https://arxiv.org/abs/2507.00950", "authors": ["Liliang Ye", "Yunyao Zhang", "Yafeng Wu", "Yi-Ping Phoebe Chen", "Junqing Yu", "Wei Yang", "Zikai Song"], "title": "MVP: Winning Solution to SMP Challenge 2025 Video Track", "categories": ["cs.CV", "cs.LG", "cs.MM"], "comment": null, "summary": "Social media platforms serve as central hubs for content dissemination,\nopinion expression, and public engagement across diverse modalities. Accurately\npredicting the popularity of social media videos enables valuable applications\nin content recommendation, trend detection, and audience engagement. In this\npaper, we present Multimodal Video Predictor (MVP), our winning solution to the\nVideo Track of the SMP Challenge 2025. MVP constructs expressive post\nrepresentations by integrating deep video features extracted from pretrained\nmodels with user metadata and contextual information. The framework applies\nsystematic preprocessing techniques, including log-transformations and outlier\nremoval, to improve model robustness. A gradient-boosted regression model is\ntrained to capture complex patterns across modalities. Our approach ranked\nfirst in the official evaluation of the Video Track, demonstrating its\neffectiveness and reliability for multimodal video popularity prediction on\nsocial platforms. The source code is available at\nhttps://anonymous.4open.science/r/SMPDVideo.", "AI": {"tldr": "MVP是一种多模态视频流行度预测框架，结合视频特征、用户元数据和上下文信息，在SMP Challenge 2025中获胜。", "motivation": "社交媒体视频流行度预测对内容推荐和趋势检测有重要价值。", "method": "MVP整合预训练模型的视频特征、用户元数据和上下文信息，采用梯度提升回归模型。", "result": "MVP在官方评估中排名第一，证明了其有效性和可靠性。", "conclusion": "MVP为社交媒体多模态视频流行度预测提供了高效解决方案。"}}
{"id": "2507.00352", "pdf": "https://arxiv.org/pdf/2507.00352", "abs": "https://arxiv.org/abs/2507.00352", "authors": ["Abanoub E. Abdelmalak", "Mohamed A. Elsayed", "David Abercrombie", "Ilhami Torunoglu"], "title": "An AST-guided LLM Approach for SVRF Code Synthesis", "categories": ["cs.SE", "cs.AI", "cs.ET"], "comment": "9 Pages, 5 Figures, 2 Tables", "summary": "Standard Verification Rule Format (SVRF) is essential for semiconductor\napplications like Design Rule Check (DRC), Layout Versus Schematic (LVS), and\nOptical Proximity Correction (OPC) and it faces challenges as advancing nodes\ncreate complex design rules that renders traditional SVRF development\nineffective and highlight an expertise gap. This paper introduces a novel\nmethodology integrating Abstract Syntax Tree (AST) embedding and\nRetrieval-Augmented Generation (RAG) for enhanced SVRF code synthesis, ensuring\nsemantic accuracy and error minimization through structural validation with\ndomain-specific insights for precise code generation.\n  We evaluate different T5-based models and propose an innovative SVRF-specific\nscoring framework that complements standard metrics like BLEU and ROUGE-L. In\nour approach, AST provides rigorous structural validation, while RAG infuses\nrelevant domain knowledge, effectively enhancing the code generation workflow.\n  Testing on a comprehensive benchmark of 740 DRC rule implementations, our\nmethodology demonstrates up to a 40\\% improvement in code generation accuracy\ncompared to basic text-based fine-tuning process. This fusion of industry\nexpertise with advanced coding strategies not only optimizes SVRF development\nunder limited dataset constraints but also creates a more intuitive and\nefficient coding environment. Consequently, users can rapidly iterate through\ndesign cycles, reduce manual error correction, and significantly improve\noverall productivity.", "AI": {"tldr": "论文提出了一种结合AST嵌入和RAG的新方法，用于提升SVRF代码合成的准确性和效率，在740条DRC规则测试中实现了40%的改进。", "motivation": "随着半导体技术节点的进步，传统SVRF开发方法因设计规则复杂化而失效，亟需一种新方法填补专业知识缺口。", "method": "采用AST嵌入和RAG技术，结合领域知识进行代码生成，并通过结构验证确保语义准确性。", "result": "在740条DRC规则测试中，代码生成准确性提升了40%。", "conclusion": "该方法不仅优化了SVRF开发效率，还减少了人工纠错，显著提升了生产力。"}}
{"id": "2507.00969", "pdf": "https://arxiv.org/pdf/2507.00969", "abs": "https://arxiv.org/abs/2507.00969", "authors": ["Alberto Neri", "Maximilan Fehrentz", "Veronica Penza", "Leonardo S. Mattos", "Nazim Haouchine"], "title": "Surgical Neural Radiance Fields from One Image", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Purpose: Neural Radiance Fields (NeRF) offer exceptional capabilities for 3D\nreconstruction and view synthesis, yet their reliance on extensive multi-view\ndata limits their application in surgical intraoperative settings where only\nlimited data is available. In particular, collecting such extensive data\nintraoperatively is impractical due to time constraints. This work addresses\nthis challenge by leveraging a single intraoperative image and preoperative\ndata to train NeRF efficiently for surgical scenarios.\n  Methods: We leverage preoperative MRI data to define the set of camera\nviewpoints and images needed for robust and unobstructed training.\nIntraoperatively, the appearance of the surgical image is transferred to the\npre-constructed training set through neural style transfer, specifically\ncombining WTC2 and STROTSS to prevent over-stylization. This process enables\nthe creation of a dataset for instant and fast single-image NeRF training.\n  Results: The method is evaluated with four clinical neurosurgical cases.\nQuantitative comparisons to NeRF models trained on real surgical microscope\nimages demonstrate strong synthesis agreement, with similarity metrics\nindicating high reconstruction fidelity and stylistic alignment. When compared\nwith ground truth, our method demonstrates high structural similarity,\nconfirming good reconstruction quality and texture preservation.\n  Conclusion: Our approach demonstrates the feasibility of single-image NeRF\ntraining in surgical settings, overcoming the limitations of traditional\nmulti-view methods.", "AI": {"tldr": "该论文提出了一种利用单张术中图像和术前数据高效训练NeRF的方法，解决了手术场景中多视图数据不足的问题。", "motivation": "NeRF在3D重建和视图合成方面表现优异，但在手术场景中，由于时间限制无法获取大量多视图数据，因此需要一种基于单张图像的高效训练方法。", "method": "结合术前MRI数据和术中图像，通过神经风格迁移（WTC2和STROTSS）生成训练数据集，实现单图像快速训练NeRF。", "result": "在四个神经外科病例中验证，结果显示高重建保真度和风格一致性，与真实显微镜图像训练的NeRF模型相比表现优异。", "conclusion": "该方法证明了在手术场景中单图像训练NeRF的可行性，突破了传统多视图方法的限制。"}}
{"id": "2507.00980", "pdf": "https://arxiv.org/pdf/2507.00980", "abs": "https://arxiv.org/abs/2507.00980", "authors": ["Yuheng Du", "Sheng Yang", "Lingxuan Wang", "Zhenghua Hou", "Chengying Cai", "Zhitao Tan", "Mingxia Chen", "Shi-Sheng Huang", "Qiang Li"], "title": "RTMap: Real-Time Recursive Mapping with Change Detection and Localization", "categories": ["cs.CV"], "comment": null, "summary": "While recent online HD mapping methods relieve burdened offline pipelines and\nsolve map freshness, they remain limited by perceptual inaccuracies, occlusion\nin dense traffic, and an inability to fuse multi-agent observations. We propose\nRTMap to enhance these single-traversal methods by persistently crowdsourcing a\nmulti-traversal HD map as a self-evolutional memory. On onboard agents, RTMap\nsimultaneously addresses three core challenges in an end-to-end fashion: (1)\nUncertainty-aware positional modeling for HD map elements, (2)\nprobabilistic-aware localization w.r.t. the crowdsourced prior-map, and (3)\nreal-time detection for possible road structural changes. Experiments on\nseveral public autonomous driving datasets demonstrate our solid performance on\nboth the prior-aided map quality and the localization accuracy, demonstrating\nour effectiveness of robustly serving downstream prediction and planning\nmodules while gradually improving the accuracy and freshness of the\ncrowdsourced prior-map asynchronously. Our source-code will be made publicly\navailable at https://github.com/CN-ADLab/RTMap (Camera ready version\nincorporating reviewer suggestions will be updated soon).", "AI": {"tldr": "RTMap通过多轨迹众包构建高精度地图，解决单次遍历方法的感知误差、遮挡和多智能体观测融合问题。", "motivation": "现有在线高精地图方法存在感知不准确、遮挡和多智能体观测融合不足的问题。", "method": "RTMap采用不确定性感知的位置建模、概率感知的定位和实时道路结构变化检测。", "result": "实验表明RTMap在地图质量和定位精度上表现优异，支持下游预测和规划模块。", "conclusion": "RTMap能异步提升众包地图的准确性和新鲜度，代码将开源。"}}
{"id": "2507.00358", "pdf": "https://arxiv.org/pdf/2507.00358", "abs": "https://arxiv.org/abs/2507.00358", "authors": ["Yilie Huang", "Xun Yu Zhou"], "title": "Data-Driven Exploration for a Class of Continuous-Time Linear--Quadratic Reinforcement Learning Problems", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY", "math.OC"], "comment": "36 pages, 10 figures", "summary": "We study reinforcement learning (RL) for the same class of continuous-time\nstochastic linear--quadratic (LQ) control problems as in\n\\cite{huang2024sublinear}, where volatilities depend on both states and\ncontrols while states are scalar-valued and running control rewards are absent.\nWe propose a model-free, data-driven exploration mechanism that adaptively\nadjusts entropy regularization by the critic and policy variance by the actor.\nUnlike the constant or deterministic exploration schedules employed in\n\\cite{huang2024sublinear}, which require extensive tuning for implementations\nand ignore learning progresses during iterations, our adaptive exploratory\napproach boosts learning efficiency with minimal tuning. Despite its\nflexibility, our method achieves a sublinear regret bound that matches the\nbest-known model-free results for this class of LQ problems, which were\npreviously derived only with fixed exploration schedules. Numerical experiments\ndemonstrate that adaptive explorations accelerate convergence and improve\nregret performance compared to the non-adaptive model-free and model-based\ncounterparts.", "AI": {"tldr": "本文提出了一种自适应探索机制，用于连续时间随机线性-二次控制问题的强化学习，通过动态调整熵正则化和策略方差，显著提升了学习效率。", "motivation": "解决现有方法中固定探索计划需要大量调参且忽略学习进展的问题，提出自适应探索以提升效率。", "method": "采用模型无关、数据驱动的探索机制，动态调整熵正则化（critic）和策略方差（actor）。", "result": "实现了与固定探索计划相同的最优次线性遗憾界，数值实验显示自适应探索加速收敛并改善遗憾性能。", "conclusion": "自适应探索机制在减少调参需求的同时，显著提升了学习效率和性能。"}}
{"id": "2507.00981", "pdf": "https://arxiv.org/pdf/2507.00981", "abs": "https://arxiv.org/abs/2507.00981", "authors": ["Jack Nugent", "Siyang Wu", "Zeyu Ma", "Beining Han", "Meenal Parakh", "Abhishek Joshi", "Lingjie Mei", "Alexander Raistrick", "Xinyuan Li", "Jia Deng"], "title": "Evaluating Robustness of Monocular Depth Estimation with Procedural Scene Perturbations", "categories": ["cs.CV"], "comment": null, "summary": "Recent years have witnessed substantial progress on monocular depth\nestimation, particularly as measured by the success of large models on standard\nbenchmarks. However, performance on standard benchmarks does not offer a\ncomplete assessment, because most evaluate accuracy but not robustness. In this\nwork, we introduce PDE (Procedural Depth Evaluation), a new benchmark which\nenables systematic robustness evaluation. PDE uses procedural generation to\ncreate 3D scenes that test robustness to various controlled perturbations,\nincluding object, camera, material and lighting changes. Our analysis yields\ninteresting findings on what perturbations are challenging for state-of-the-art\ndepth models, which we hope will inform further research. Code and data are\navailable at https://github.com/princeton-vl/proc-depth-eval.", "AI": {"tldr": "论文提出了PDE（Procedural Depth Evaluation）基准，用于系统评估单目深度估计模型的鲁棒性。", "motivation": "现有标准基准主要评估准确性而非鲁棒性，PDE旨在填补这一空白。", "method": "通过程序生成3D场景，测试模型对物体、相机、材质和光照变化的鲁棒性。", "result": "分析揭示了当前先进深度模型在特定扰动下的挑战。", "conclusion": "PDE为深度估计研究提供了新的鲁棒性评估工具，代码和数据已开源。"}}
{"id": "2507.00378", "pdf": "https://arxiv.org/pdf/2507.00378", "abs": "https://arxiv.org/abs/2507.00378", "authors": ["Xikai Sun", "Fan Dang", "Kebin Liu", "Xin Miao", "Zihao Yang", "Haimo Lu", "Yawen Zheng", "Yunhao Liu"], "title": "iPanda: An Intelligent Protocol Testing and Debugging Agent for Conformance Testing", "categories": ["cs.SE", "cs.AI"], "comment": "14 pages, 6 figures", "summary": "Conformance testing is essential for ensuring that protocol implementations\ncomply with their specifications. However, traditional testing approaches\ninvolve manually creating numerous test cases and scripts, making the process\nlabor-intensive and inefficient. Recently, Large Language Models (LLMs) have\ndemonstrated impressive text comprehension and code generation abilities,\nproviding promising opportunities for automation. In this paper, we propose\niPanda, the first end-to-end framework that leverages LLMs to automate protocol\nconformance testing. Given a protocol specification document and its\nimplementation, iPanda first employs a keyword-based method to automatically\ngenerate comprehensive test cases. Then, it utilizes a code-based\nretrieval-augmented generation approach to effectively interpret the\nimplementation and produce executable test code. To further enhance code\nquality, iPanda incorporates an iterative self-correction mechanism to refine\ngenerated test scripts interactively. Finally, by executing and analyzing the\ngenerated tests, iPanda systematically verifies compliance between\nimplementations and protocol specifications. Comprehensive experiments on\nvarious protocols show that iPanda significantly outperforms pure LLM-based\napproaches, improving the success rate (Pass@1) of test-code generation by\nfactors ranging from 4.675 times to 10.751 times.", "AI": {"tldr": "iPanda是一个利用大语言模型（LLMs）自动化协议一致性测试的端到端框架，显著提高了测试代码生成的效率和成功率。", "motivation": "传统协议一致性测试方法依赖人工创建测试用例和脚本，效率低下且劳动密集。LLMs的文本理解和代码生成能力为自动化提供了新机会。", "method": "iPanda结合关键词生成测试用例和基于检索增强的代码生成方法，并通过迭代自校正机制优化生成的测试脚本。", "result": "实验表明，iPanda在测试代码生成的成功率（Pass@1）上比纯LLM方法提高了4.675至10.751倍。", "conclusion": "iPanda为协议一致性测试提供了一种高效、自动化的解决方案，显著优于传统和纯LLM方法。"}}
{"id": "2507.00992", "pdf": "https://arxiv.org/pdf/2507.00992", "abs": "https://arxiv.org/abs/2507.00992", "authors": ["Yuanrui Wang", "Cong Han", "YafeiLi", "Zhipeng Jin", "Xiawei Li", "SiNan Du", "Wen Tao", "Yi Yang", "shuanglong li", "Chun Yuan", "Liu Lin"], "title": "UniGlyph: Unified Segmentation-Conditioned Diffusion for Precise Visual Text Synthesis", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Text-to-image generation has greatly advanced content creation, yet\naccurately rendering visual text remains a key challenge due to blurred glyphs,\nsemantic drift, and limited style control. Existing methods often rely on\npre-rendered glyph images as conditions, but these struggle to retain original\nfont styles and color cues, necessitating complex multi-branch designs that\nincrease model overhead and reduce flexibility. To address these issues, we\npropose a segmentation-guided framework that uses pixel-level visual text masks\n-- rich in glyph shape, color, and spatial detail -- as unified conditional\ninputs. Our method introduces two core components: (1) a fine-tuned bilingual\nsegmentation model for precise text mask extraction, and (2) a streamlined\ndiffusion model augmented with adaptive glyph conditioning and a\nregion-specific loss to preserve textual fidelity in both content and style.\nOur approach achieves state-of-the-art performance on the AnyText benchmark,\nsignificantly surpassing prior methods in both Chinese and English settings. To\nenable more rigorous evaluation, we also introduce two new benchmarks:\nGlyphMM-benchmark for testing layout and glyph consistency in complex\ntypesetting, and MiniText-benchmark for assessing generation quality in\nsmall-scale text regions. Experimental results show that our model outperforms\nexisting methods by a large margin in both scenarios, particularly excelling at\nsmall text rendering and complex layout preservation, validating its strong\ngeneralization and deployment readiness.", "AI": {"tldr": "提出了一种基于分割引导的文本生成框架，通过像素级视觉文本掩码作为统一条件输入，解决了现有方法在字体风格和颜色保留上的不足，显著提升了文本生成的准确性和灵活性。", "motivation": "现有文本生成方法在视觉文本渲染中存在模糊字形、语义漂移和风格控制有限的问题，且依赖预渲染字形图像导致模型复杂性和灵活性降低。", "method": "采用分割引导框架，结合双语分割模型提取精确文本掩码，并改进扩散模型以增强字形条件和区域特定损失，保留文本内容和风格。", "result": "在AnyText基准测试中表现最优，显著优于现有方法，并在新提出的GlyphMM和MiniText基准测试中展现出卓越的小文本渲染和复杂布局保留能力。", "conclusion": "该方法在文本生成任务中具有强泛化能力和部署准备性，为视觉文本渲染提供了高效解决方案。"}}
{"id": "2507.00407", "pdf": "https://arxiv.org/pdf/2507.00407", "abs": "https://arxiv.org/abs/2507.00407", "authors": ["Cong Fu", "Yuchao Lin", "Zachary Krueger", "Haiyang Yu", "Maho Nakata", "Jianwen Xie", "Emine Kucukbenli", "Xiaofeng Qian", "Shuiwang Ji"], "title": "Augmenting Molecular Graphs with Geometries via Machine Learning Interatomic Potentials", "categories": ["physics.chem-ph", "cs.AI", "q-bio.QM"], "comment": null, "summary": "Accurate molecular property predictions require 3D geometries, which are\ntypically obtained using expensive methods such as density functional theory\n(DFT). Here, we attempt to obtain molecular geometries by relying solely on\nmachine learning interatomic potential (MLIP) models. To this end, we first\ncurate a large-scale molecular relaxation dataset comprising 3.5 million\nmolecules and 300 million snapshots. Then MLIP foundation models are trained\nwith supervised learning to predict energy and forces given 3D molecular\nstructures. Once trained, we show that the foundation models can be used in\ndifferent ways to obtain geometries either explicitly or implicitly. First, it\ncan be used to obtain low-energy 3D geometries via geometry optimization,\nproviding relaxed 3D geometries for downstream molecular property predictions.\nTo mitigate potential biases and enhance downstream predictions, we introduce\ngeometry fine-tuning based on the relaxed 3D geometries. Second, the foundation\nmodels can be directly fine-tuned for property prediction when ground truth 3D\ngeometries are available. Our results demonstrate that MLIP foundation models\ntrained on relaxation data can provide valuable molecular geometries that\nbenefit property predictions.", "AI": {"tldr": "该论文提出了一种基于机器学习原子间势（MLIP）模型的方法，用于获取分子几何结构，避免了昂贵的传统方法（如DFT）。通过大规模数据集训练MLIP基础模型，并展示了其在几何优化和性质预测中的应用。", "motivation": "传统获取分子3D几何结构的方法（如DFT）成本高昂，因此探索仅依赖MLIP模型的方法以降低成本并提高效率。", "method": "构建包含350万分子和3亿快照的数据集，训练MLIP基础模型预测能量和力。模型可用于几何优化或直接微调进行性质预测。", "result": "MLIP基础模型能够提供有效的分子几何结构，并通过几何微调优化下游性质预测。", "conclusion": "MLIP模型在分子几何结构获取和性质预测中具有潜力，为替代传统方法提供了可行方案。"}}
{"id": "2507.01006", "pdf": "https://arxiv.org/pdf/2507.01006", "abs": "https://arxiv.org/abs/2507.01006", "authors": ["Wenyi Hong", "Wenmeng Yu", "Xiaotao Gu", "Guo Wang", "Guobing Gan", "Haomiao Tang", "Jiale Cheng", "Ji Qi", "Junhui Ji", "Lihang Pan", "Shuaiqi Duan", "Weihan Wang", "Yan Wang", "Yean Cheng", "Zehai He", "Zhe Su", "Zhen Yang", "Ziyang Pan", "Aohan Zeng", "Baoxu Wang", "Boyan Shi", "Changyu Pang", "Chenhui Zhang", "Da Yin", "Fan Yang", "Guoqing Chen", "Jiazheng Xu", "Jiali Chen", "Jing Chen", "Jinhao Chen", "Jinghao Lin", "Jinjiang Wang", "Junjie Chen", "Leqi Lei", "Leyi Pan", "Mingzhi Zhang", "Qinkai Zheng", "Sheng Yang", "Shi Zhong", "Shiyu Huang", "Shuyuan Zhao", "Siyan Xue", "Shangqin Tu", "Shengbiao Meng", "Tianshu Zhang", "Tianwei Luo", "Tianxiang Hao", "Tianle Gong", "Wenkai Li", "Wei Jia", "Xin Lyu", "Xuancheng Huang", "Yanling Wang", "Yadong Xue", "Yanfeng Wang", "Yifan An", "Yifan Du", "Yiming Shi", "Yiheng Huang", "Yilin Niu", "Yuan Wang", "Yuanchang Yue", "Yuchen Li", "Yutao Zhang", "Yuxuan Zhang", "Zhanxiao Du", "Zhenyu Hou", "Zhao Xue", "Zhengxiao Du", "Zihan Wang", "Peng Zhang", "Debing Liu", "Bin Xu", "Juanzi Li", "Minlie Huang", "Yuxiao Dong", "Jie Tang"], "title": "GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to\nadvance general-purpose multimodal reasoning. In this report, we share our key\nfindings in the development of the reasoning-centric training framework. We\nfirst develop a capable vision foundation model with significant potential\nthrough large-scale pre-training, which arguably sets the upper bound for the\nfinal performance. Reinforcement Learning with Curriculum Sampling (RLCS) then\nunlocks the full potential of the model, leading to comprehensive capability\nenhancement across a diverse range of tasks, including STEM problem solving,\nvideo understanding, content recognition, coding, grounding, GUI-based agents,\nand long document understanding, among others. To facilitate research in this\nfield, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art\nperformance among models of comparable size. In a comprehensive evaluation\nacross 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all\ntasks and achieves comparable or even superior performance on 18 benchmarks\nrelative to the significantly larger Qwen2.5-VL-72B. Notably,\nGLM-4.1V-9B-Thinking also demonstrates competitive or superior performance\ncompared to closed-source models such as GPT-4o on challenging tasks including\nlong document understanding and STEM reasoning, further underscoring its strong\ncapabilities. Code, models and more information are released at\nhttps://github.com/THUDM/GLM-4.1V-Thinking.", "AI": {"tldr": "GLM-4.1V-Thinking是一个视觉语言模型，通过强化学习和课程采样提升多模态推理能力，在多个任务上表现优异，甚至超越更大规模的模型。", "motivation": "开发一个通用的多模态推理模型，提升在STEM、视频理解、内容识别等任务上的性能。", "method": "采用大规模预训练和强化学习结合课程采样（RLCS）的方法，优化模型能力。", "result": "在28个公开基准测试中表现优异，超越同类规模模型，甚至与更大规模模型竞争。", "conclusion": "GLM-4.1V-Thinking展示了强大的多模态推理能力，为研究提供了开源资源。"}}
{"id": "2507.00418", "pdf": "https://arxiv.org/pdf/2507.00418", "abs": "https://arxiv.org/abs/2507.00418", "authors": ["Mohammad Firas Sada", "John J. Graham", "Elham E Khoda", "Mahidhar Tatineni", "Dmitry Mishin", "Rajesh K. Gupta", "Rick Wagner", "Larry Smarr", "Thomas A. DeFanti", "Frank Würthwein"], "title": "Serving LLMs in HPC Clusters: A Comparative Study of Qualcomm Cloud AI 100 Ultra and High-Performance GPUs", "categories": ["cs.DC", "cs.AI"], "comment": "To appear in Proceedings of the Practice and Experience in Advanced\n  Research Computing (PEARC '25)", "summary": "This study presents a benchmarking analysis of the Qualcomm Cloud AI 100\nUltra (QAic) accelerator for large language model (LLM) inference, evaluating\nits energy efficiency (throughput per watt) and performance against leading\nNVIDIA (A100, H200) and AMD (MI300A) GPUs within the National Research Platform\n(NRP) ecosystem. A total of 15 open-source LLMs, ranging from 117 million to 90\nbillion parameters, are served using the vLLM framework. The QAic inference\ncards appears to be energy efficient and performs well in the energy efficiency\nmetric in most cases. The findings offer insights into the potential of the\nQualcomm Cloud AI 100 Ultra for high-performance computing (HPC) applications\nwithin the National Research Platform (NRP).", "AI": {"tldr": "对Qualcomm Cloud AI 100 Ultra（QAic）加速器在大型语言模型（LLM）推理中的能效和性能进行了基准测试，并与NVIDIA和AMD的GPU进行了比较。", "motivation": "评估QAic加速器在高性能计算（HPC）应用中的潜力，特别是在国家研究平台（NRP）生态系统中的表现。", "method": "使用vLLM框架对15个开源LLM（参数范围从1.17亿到900亿）进行推理测试，比较能效（每瓦吞吐量）和性能。", "result": "QAic加速器在大多数情况下表现出较高的能效，并在能效指标上表现良好。", "conclusion": "QAic加速器在国家研究平台（NRP）中具有高性能计算（HPC）应用的潜力。"}}
{"id": "2507.01009", "pdf": "https://arxiv.org/pdf/2507.01009", "abs": "https://arxiv.org/abs/2507.01009", "authors": ["Anna Foix Romero", "Craig Russell", "Alexander Krull", "Virginie Uhlmann"], "title": "ShapeEmbed: a self-supervised learning framework for 2D contour quantification", "categories": ["cs.CV", "q-bio.QM"], "comment": null, "summary": "The shape of objects is an important source of visual information in a wide\nrange of applications. One of the core challenges of shape quantification is to\nensure that the extracted measurements remain invariant to transformations that\npreserve an object's intrinsic geometry, such as changing its size,\norientation, and position in the image. In this work, we introduce ShapeEmbed,\na self-supervised representation learning framework designed to encode the\ncontour of objects in 2D images, represented as a Euclidean distance matrix,\ninto a shape descriptor that is invariant to translation, scaling, rotation,\nreflection, and point indexing. Our approach overcomes the limitations of\ntraditional shape descriptors while improving upon existing state-of-the-art\nautoencoder-based approaches. We demonstrate that the descriptors learned by\nour framework outperform their competitors in shape classification tasks on\nnatural and biological images. We envision our approach to be of particular\nrelevance to biological imaging applications.", "AI": {"tldr": "ShapeEmbed是一个自监督表示学习框架，用于编码2D图像中物体的轮廓，生成对平移、缩放、旋转、反射和点索引不变的形状描述符。", "motivation": "解决形状量化中提取的测量值对保持物体内在几何的变换（如大小、方向和位置变化）不变的核心挑战。", "method": "提出ShapeEmbed框架，通过自监督学习将欧几里得距离矩阵编码为形状描述符。", "result": "在自然和生物图像形状分类任务中，ShapeEmbed的描述符优于现有方法。", "conclusion": "ShapeEmbed在生物成像应用中具有潜在的重要价值。"}}
{"id": "2507.00419", "pdf": "https://arxiv.org/pdf/2507.00419", "abs": "https://arxiv.org/abs/2507.00419", "authors": ["Yimin Dou", "Xinming Wu", "Nathan L Bangs", "Harpreet Singh Sethi", "Jintao Li", "Hang Gao", "Zhixiang Guo"], "title": "Geological Everything Model 3D: A Promptable Foundation Model for Unified and Zero-Shot Subsurface Understanding", "categories": ["physics.geo-ph", "cs.AI"], "comment": null, "summary": "Understanding Earth's subsurface is critical for energy transition, natural\nhazard mitigation, and planetary science. Yet subsurface analysis remains\nfragmented, with separate models required for structural interpretation,\nstratigraphic analysis, geobody segmentation, and property modeling-each\ntightly coupled to specific data distributions and task formulations. We\nintroduce the Geological Everything Model 3D (GEM), a unified generative\narchitecture that reformulates all these tasks as prompt-conditioned inference\nalong latent structural frameworks derived from subsurface imaging. This\nformulation moves beyond task-specific models by enabling a shared inference\nmechanism, where GEM propagates human-provided prompts-such as well logs,\nmasks, or structural sketches-along inferred structural frameworks to produce\ngeologically coherent outputs. Through this mechanism, GEM achieves zero-shot\ngeneralization across tasks with heterogeneous prompt types, without retraining\nfor new tasks or data sources. This capability emerges from a two-stage\ntraining process that combines self-supervised representation learning on\nlarge-scale field seismic data with adversarial fine-tuning using mixed prompts\nand labels across diverse subsurface tasks. GEM demonstrates broad\napplicability across surveys and tasks, including Martian radar stratigraphy\nanalysis, structural interpretation in subduction zones, full seismic\nstratigraphic interpretation, geobody delineation, and property modeling. By\nbridging expert knowledge with generative reasoning in a structurally aware\nmanner, GEM lays the foundation for scalable, human-in-the-loop geophysical\nAI-transitioning from fragmented pipelines to a vertically integrated,\npromptable reasoning system. Project page: https://douyimin.github.io/GEM", "AI": {"tldr": "GEM是一种统一生成架构，通过提示条件推理实现多种地下分析任务的零样本泛化。", "motivation": "解决地下分析任务分散、模型专用的问题，提出统一框架。", "method": "两阶段训练：自监督表示学习与对抗微调，结合提示条件推理。", "result": "GEM在多种任务中表现优异，如火星雷达地层分析、地震解释等。", "conclusion": "GEM为可扩展、人机交互的地球物理AI奠定了基础。"}}
{"id": "2507.01012", "pdf": "https://arxiv.org/pdf/2507.01012", "abs": "https://arxiv.org/abs/2507.01012", "authors": ["Zhe Kong", "Le Li", "Yong Zhang", "Feng Gao", "Shaoshu Yang", "Tao Wang", "Kaihao Zhang", "Zhuoliang Kang", "Xiaoming Wei", "Guanying Chen", "Wenhan Luo"], "title": "DAM-VSR: Disentanglement of Appearance and Motion for Video Super-Resolution", "categories": ["cs.CV"], "comment": "Accepted by ACM SIGGRAPH 2025, Homepage:\n  https://kongzhecn.github.io/projects/dam-vsr/ Github:\n  https://github.com/kongzhecn/DAM-VSR", "summary": "Real-world video super-resolution (VSR) presents significant challenges due\nto complex and unpredictable degradations. Although some recent methods utilize\nimage diffusion models for VSR and have shown improved detail generation\ncapabilities, they still struggle to produce temporally consistent frames. We\nattempt to use Stable Video Diffusion (SVD) combined with ControlNet to address\nthis issue. However, due to the intrinsic image-animation characteristics of\nSVD, it is challenging to generate fine details using only low-quality videos.\nTo tackle this problem, we propose DAM-VSR, an appearance and motion\ndisentanglement framework for VSR. This framework disentangles VSR into\nappearance enhancement and motion control problems. Specifically, appearance\nenhancement is achieved through reference image super-resolution, while motion\ncontrol is achieved through video ControlNet. This disentanglement fully\nleverages the generative prior of video diffusion models and the detail\ngeneration capabilities of image super-resolution models. Furthermore, equipped\nwith the proposed motion-aligned bidirectional sampling strategy, DAM-VSR can\nconduct VSR on longer input videos. DAM-VSR achieves state-of-the-art\nperformance on real-world data and AIGC data, demonstrating its powerful detail\ngeneration capabilities.", "AI": {"tldr": "DAM-VSR提出了一种基于外观与运动解耦的视频超分辨率框架，结合了视频扩散模型和图像超分辨率模型的优势，解决了时间一致性和细节生成的挑战。", "motivation": "现实世界中的视频超分辨率（VSR）面临复杂且不可预测的退化问题，现有方法难以同时实现时间一致性和高质量细节生成。", "method": "提出DAM-VSR框架，将VSR解耦为外观增强（通过参考图像超分辨率）和运动控制（通过视频ControlNet），并采用运动对齐的双向采样策略。", "result": "DAM-VSR在真实世界数据和AIGC数据上实现了最先进的性能，展现了强大的细节生成能力。", "conclusion": "DAM-VSR通过解耦外观与运动，充分利用生成模型和超分辨率模型的优势，显著提升了视频超分辨率的效果。"}}
{"id": "2507.00435", "pdf": "https://arxiv.org/pdf/2507.00435", "abs": "https://arxiv.org/abs/2507.00435", "authors": ["Yi Ru Wang", "Carter Ung", "Grant Tannert", "Jiafei Duan", "Josephine Li", "Amy Le", "Rishabh Oswal", "Markus Grotz", "Wilbert Pumacay", "Yuquan Deng", "Ranjay Krishna", "Dieter Fox", "Siddhartha Srinivasa"], "title": "RoboEval: Where Robotic Manipulation Meets Structured and Scalable Evaluation", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Project page: https://robo-eval.github.io", "summary": "We present RoboEval, a simulation benchmark and structured evaluation\nframework designed to reveal the limitations of current bimanual manipulation\npolicies. While prior benchmarks report only binary task success, we show that\nsuch metrics often conceal critical weaknesses in policy behavior -- such as\npoor coordination, slipping during grasping, or asymmetric arm usage. RoboEval\nintroduces a suite of tiered, semantically grounded tasks decomposed into\nskill-specific stages, with variations that systematically challenge spatial,\nphysical, and coordination capabilities. Tasks are paired with fine-grained\ndiagnostic metrics and 3000+ human demonstrations to support imitation\nlearning. Our experiments reveal that policies with similar success rates\ndiverge in how tasks are executed -- some struggle with alignment, others with\ntemporally consistent bimanual control. We find that behavioral metrics\ncorrelate with success in over half of task-metric pairs, and remain\ninformative even when binary success saturates. By pinpointing when and how\npolicies fail, RoboEval enables a deeper, more actionable understanding of\nrobotic manipulation -- and highlights the need for evaluation tools that go\nbeyond success alone.", "AI": {"tldr": "RoboEval是一个用于评估双手机器人操作策略的仿真基准和结构化框架，揭示了当前策略的潜在弱点，如协调性差或抓取滑动。", "motivation": "现有基准仅报告二元任务成功率，掩盖了策略行为的缺陷，如协调性差或不对称手臂使用。", "method": "RoboEval引入分层任务、细粒度诊断指标和3000+人类演示，系统性挑战空间、物理和协调能力。", "result": "实验表明，成功率相似的政策在执行方式上差异显著，行为指标在超过一半的任务中与成功率相关。", "conclusion": "RoboEval通过揭示失败原因，提供了更深入、可操作的机器人操作理解，强调了超越单纯成功率的评估工具的必要性。"}}
{"id": "2507.00440", "pdf": "https://arxiv.org/pdf/2507.00440", "abs": "https://arxiv.org/abs/2507.00440", "authors": ["Yujia Yin", "Tianyi Qu", "Zihao Wang", "Yifan Chen"], "title": "A Recipe for Causal Graph Regression: Confounding Effects Revisited", "categories": ["cs.LG", "cs.AI", "stat.ME"], "comment": "ICML 2025 accepted", "summary": "Through recognizing causal subgraphs, causal graph learning (CGL) has risen\nto be a promising approach for improving the generalizability of graph neural\nnetworks under out-of-distribution (OOD) scenarios. However, the empirical\nsuccesses of CGL techniques are mostly exemplified in classification settings,\nwhile regression tasks, a more challenging setting in graph learning, are\noverlooked. We thus devote this work to tackling causal graph regression (CGR);\nto this end we reshape the processing of confounding effects in existing CGL\nstudies, which mainly deal with classification. Specifically, we reflect on the\npredictive power of confounders in graph-level regression, and generalize\nclassification-specific causal intervention techniques to regression through a\nlens of contrastive learning. Extensive experiments on graph OOD benchmarks\nvalidate the efficacy of our proposals for CGR. The model implementation and\nthe code are provided on https://github.com/causal-graph/CGR.", "AI": {"tldr": "本文提出了一种因果图回归（CGR）方法，通过重新处理混杂效应，将因果干预技术从分类任务推广到回归任务，并通过对比学习提升性能。", "motivation": "现有的因果图学习（CGL）技术主要关注分类任务，而更具挑战性的回归任务被忽视。本文旨在填补这一空白，提升图神经网络在分布外（OOD）场景下的泛化能力。", "method": "通过重新处理混杂效应，将分类任务中的因果干预技术推广到回归任务，并结合对比学习方法。", "result": "在图的OOD基准测试中，实验验证了所提CGR方法的有效性。", "conclusion": "本文成功将因果图学习技术扩展到回归任务，为图神经网络的泛化能力提供了新的解决方案。"}}
{"id": "2507.00443", "pdf": "https://arxiv.org/pdf/2507.00443", "abs": "https://arxiv.org/abs/2507.00443", "authors": ["Reza Ahmadvand", "Sarah Safura Sharif", "Yaser Mike Banad"], "title": "Novel Pigeon-inspired 3D Obstacle Detection and Avoidance Maneuver for Multi-UAV Systems", "categories": ["cs.RO", "cs.AI", "cs.MA"], "comment": "11 Pages, 11 Pictures, 1 Table, 3 Algorithms", "summary": "Recent advances in multi-agent systems manipulation have demonstrated a\nrising demand for the implementation of multi-UAV systems in urban areas, which\nare always subjected to the presence of static and dynamic obstacles. Inspired\nby the collective behavior of tilapia fish and pigeons, the focus of the\npresented research is on the introduction of a nature-inspired collision-free\nformation control for a multi-UAV system, considering the obstacle avoidance\nmaneuvers. The developed framework in this study utilizes a semi-distributed\ncontrol approach, in which, based on a probabilistic Lloyd's algorithm, a\ncentralized guidance algorithm works for optimal positioning of the UAVs, while\na distributed control approach has been used for the intervehicle collision and\nobstacle avoidance. Further, the presented framework has been extended to the\n3D space with a novel definition of 3D maneuvers. Finally, the presented\nframework has been applied to multi-UAV systems in 2D and 3D scenarios, and the\nobtained results demonstrated the validity of the presented method in dynamic\nenvironments with stationary and moving obstacles.", "AI": {"tldr": "提出了一种受自然界启发的多无人机系统无碰撞编队控制方法，结合集中式与分布式控制，适用于动态环境中的障碍物避让。", "motivation": "城市环境中多无人机系统的需求增加，但面临静态和动态障碍物的挑战，受鱼类和鸽子的群体行为启发，研究无碰撞编队控制。", "method": "采用半分布式控制框架，集中式算法优化无人机位置，分布式控制处理碰撞和障碍物避让，并扩展到3D空间。", "result": "在2D和3D场景中验证了方法的有效性，能够应对静态和动态障碍物。", "conclusion": "提出的框架在多无人机系统中实现了高效的无碰撞编队控制，适用于复杂动态环境。"}}
{"id": "2507.00445", "pdf": "https://arxiv.org/pdf/2507.00445", "abs": "https://arxiv.org/abs/2507.00445", "authors": ["Xingyu Su", "Xiner Li", "Masatoshi Uehara", "Sunwoo Kim", "Yulai Zhao", "Gabriele Scalia", "Ehsan Hajiramezanali", "Tommaso Biancalani", "Degui Zhi", "Shuiwang Ji"], "title": "Iterative Distillation for Reward-Guided Fine-Tuning of Diffusion Models in Biomolecular Design", "categories": ["cs.LG", "cs.AI", "q-bio.QM"], "comment": null, "summary": "We address the problem of fine-tuning diffusion models for reward-guided\ngeneration in biomolecular design. While diffusion models have proven highly\neffective in modeling complex, high-dimensional data distributions, real-world\napplications often demand more than high-fidelity generation, requiring\noptimization with respect to potentially non-differentiable reward functions\nsuch as physics-based simulation or rewards based on scientific knowledge.\nAlthough RL methods have been explored to fine-tune diffusion models for such\nobjectives, they often suffer from instability, low sample efficiency, and mode\ncollapse due to their on-policy nature. In this work, we propose an iterative\ndistillation-based fine-tuning framework that enables diffusion models to\noptimize for arbitrary reward functions. Our method casts the problem as policy\ndistillation: it collects off-policy data during the roll-in phase, simulates\nreward-based soft-optimal policies during roll-out, and updates the model by\nminimizing the KL divergence between the simulated soft-optimal policy and the\ncurrent model policy. Our off-policy formulation, combined with KL divergence\nminimization, enhances training stability and sample efficiency compared to\nexisting RL-based methods. Empirical results demonstrate the effectiveness and\nsuperior reward optimization of our approach across diverse tasks in protein,\nsmall molecule, and regulatory DNA design.", "AI": {"tldr": "提出了一种基于迭代蒸馏的微调框架，用于优化扩散模型以适应任意奖励函数，解决了传统强化学习方法的不稳定性和低效问题。", "motivation": "扩散模型在生物分子设计中需要优化不可微的奖励函数（如基于物理模拟或科学知识的奖励），但现有强化学习方法存在不稳定性和低效问题。", "method": "采用策略蒸馏方法，通过收集离线数据、模拟基于奖励的软最优策略，并最小化KL散度来更新模型。", "result": "在蛋白质、小分子和调控DNA设计中表现出高效和优越的奖励优化能力。", "conclusion": "该方法显著提升了训练稳定性和样本效率，适用于多种生物分子设计任务。"}}
{"id": "2507.00451", "pdf": "https://arxiv.org/pdf/2507.00451", "abs": "https://arxiv.org/abs/2507.00451", "authors": ["Matthew Stephenson", "Alex Newcombe", "Eric Piette", "Dennis Soemers"], "title": "Best Agent Identification for General Game Playing", "categories": ["cs.LG", "cs.AI", "cs.DS", "cs.IT", "math.IT", "stat.ML"], "comment": null, "summary": "We present an efficient and generalised procedure to accurately identify the\nbest performing algorithm for each sub-task in a multi-problem domain. Our\napproach treats this as a set of best arm identification problems for\nmulti-armed bandits, where each bandit corresponds to a specific task and each\narm corresponds to a specific algorithm or agent. We propose an optimistic\nselection process based on the Wilson score interval (Optimistic-WS) that ranks\neach arm across all bandits in terms of their potential regret reduction. We\nevaluate the performance of Optimistic-WS on two of the most popular general\ngame domains, the General Video Game AI (GVGAI) framework and the Ludii general\ngame playing system, with the goal of identifying the highest performing agent\nfor each game within a limited number of trials. Compared to previous best arm\nidentification algorithms for multi-armed bandits, our results demonstrate a\nsubstantial performance improvement in terms of average simple regret. This\nnovel approach can be used to significantly improve the quality and accuracy of\nagent evaluation procedures for general game frameworks, as well as other\nmulti-task domains with high algorithm runtimes.", "AI": {"tldr": "提出了一种基于Wilson得分区间的乐观选择方法（Optimistic-WS），用于在多任务领域高效识别每个子任务的最佳算法，显著提升了性能。", "motivation": "在多问题领域中，准确识别每个子任务的最佳算法是一个关键问题，现有方法在性能上存在不足。", "method": "将问题建模为多臂老虎机的最佳臂识别问题，提出基于Wilson得分区间的乐观选择方法（Optimistic-WS）。", "result": "在GVGAI和Ludii游戏框架中，Optimistic-WS显著降低了平均简单遗憾，优于现有方法。", "conclusion": "Optimistic-WS为多任务领域提供了一种高效、准确的算法评估方法，尤其适用于高计算成本的场景。"}}
{"id": "2507.00051", "pdf": "https://arxiv.org/pdf/2507.00051", "abs": "https://arxiv.org/abs/2507.00051", "authors": ["Tianliang Yao", "Zhiqiang Pei", "Yong Li", "Yixuan Yuan", "Peng Qi"], "title": "Real-Time Guidewire Tip Tracking Using a Siamese Network for Image-Guided Endovascular Procedures", "categories": ["eess.IV", "cs.CV"], "comment": "This paper has been accepted by Advanced Intelligent Systems", "summary": "An ever-growing incorporation of AI solutions into clinical practices\nenhances the efficiency and effectiveness of healthcare services. This paper\nfocuses on guidewire tip tracking tasks during image-guided therapy for\ncardiovascular diseases, aiding physicians in improving diagnostic and\ntherapeutic quality. A novel tracking framework based on a Siamese network with\ndual attention mechanisms combines self- and cross-attention strategies for\nrobust guidewire tip tracking. This design handles visual ambiguities, tissue\ndeformations, and imaging artifacts through enhanced spatial-temporal feature\nlearning. Validation occurred on 3 randomly selected clinical digital\nsubtraction angiography (DSA) sequences from a dataset of 15 sequences,\ncovering multiple interventional scenarios. The results indicate a mean\nlocalization error of 0.421 $\\pm$ 0.138 mm, with a maximum error of 1.736 mm,\nand a mean Intersection over Union (IoU) of 0.782. The framework maintains an\naverage processing speed of 57.2 frames per second, meeting the temporal\ndemands of endovascular imaging. Further validations with robotic platforms for\nautomating diagnostics and therapies in clinical routines yielded tracking\nerrors of 0.708 $\\pm$ 0.695 mm and 0.148 $\\pm$ 0.057 mm in two distinct\nexperimental scenarios.", "AI": {"tldr": "论文提出了一种基于Siamese网络和双重注意力机制的导丝尖端跟踪框架，用于心血管疾病的图像引导治疗，显著提升了跟踪精度和速度。", "motivation": "AI在临床实践中的应用日益增多，但导丝尖端跟踪任务仍面临视觉模糊、组织变形和成像伪影等挑战，需要更鲁棒的解决方案。", "method": "采用Siamese网络结合自注意力和交叉注意力机制，增强时空特征学习，以应对复杂场景。", "result": "在临床DSA序列上验证，平均定位误差0.421±0.138 mm，最大误差1.736 mm，平均IoU 0.782，处理速度57.2帧/秒。", "conclusion": "该框架在精度和速度上均满足临床需求，未来可进一步结合机器人平台实现自动化诊疗。"}}
{"id": "2507.00459", "pdf": "https://arxiv.org/pdf/2507.00459", "abs": "https://arxiv.org/abs/2507.00459", "authors": ["Hoang Cuong Phan", "Minh Tien Tran", "Chihun Lee", "Hoheok Kim", "Sehyok Oh", "Dong-Kyu Kim", "Ho Won Lee"], "title": "Process-aware and high-fidelity microstructure generation using stable diffusion", "categories": ["cond-mat.mtrl-sci", "cs.AI"], "comment": "46 pages, 13 figures, 5 tables, 3rd Word Congress on Artificial\n  Intelligence in Materials & Manufacturing 2025", "summary": "Synthesizing realistic microstructure images conditioned on processing\nparameters is crucial for understanding process-structure relationships in\nmaterials design. However, this task remains challenging due to limited\ntraining micrographs and the continuous nature of processing variables. To\novercome these challenges, we present a novel process-aware generative modeling\napproach based on Stable Diffusion 3.5 Large (SD3.5-Large), a state-of-the-art\ntext-to-image diffusion model adapted for microstructure generation. Our method\nintroduces numeric-aware embeddings that encode continuous variables (annealing\ntemperature, time, and magnification) directly into the model's conditioning,\nenabling controlled image generation under specified process conditions and\ncapturing process-driven microstructural variations. To address data scarcity\nand computational constraints, we fine-tune only a small fraction of the\nmodel's weights via DreamBooth and Low-Rank Adaptation (LoRA), efficiently\ntransferring the pre-trained model to the materials domain. We validate realism\nusing a semantic segmentation model based on a fine-tuned U-Net with a VGG16\nencoder on 24 labeled micrographs. It achieves 97.1% accuracy and 85.7% mean\nIoU, outperforming previous methods. Quantitative analyses using physical\ndescriptors and spatial statistics show strong agreement between synthetic and\nreal microstructures. Specifically, two-point correlation and lineal-path\nerrors remain below 2.1% and 0.6%, respectively. Our method represents the\nfirst adaptation of SD3.5-Large for process-aware microstructure generation,\noffering a scalable approach for data-driven materials design.", "AI": {"tldr": "提出了一种基于SD3.5-Large的生成模型，用于合成受工艺参数调控的微观结构图像，解决了数据稀缺和连续变量编码的挑战。", "motivation": "理解工艺参数与微观结构的关系对材料设计至关重要，但现有方法受限于数据稀缺和连续变量的复杂性。", "method": "采用SD3.5-Large模型，引入数值感知嵌入编码连续变量，并通过DreamBooth和LoRA进行高效微调。", "result": "生成图像在语义分割中达到97.1%准确率和85.7%平均IoU，物理描述符和空间统计误差低于2.1%和0.6%。", "conclusion": "该方法首次将SD3.5-Large应用于工艺感知的微观结构生成，为数据驱动的材料设计提供了可扩展方案。"}}
{"id": "2507.00190", "pdf": "https://arxiv.org/pdf/2507.00190", "abs": "https://arxiv.org/abs/2507.00190", "authors": ["Satoshi Tanaka", "Koji Minoda", "Fumiya Watanabe", "Takamasa Horibe"], "title": "Rethink 3D Object Detection from Physical World", "categories": ["cs.RO", "cs.CV"], "comment": "15 pages, 10 figures", "summary": "High-accuracy and low-latency 3D object detection is essential for autonomous\ndriving systems. While previous studies on 3D object detection often evaluate\nperformance based on mean average precision (mAP) and latency, they typically\nfail to address the trade-off between speed and accuracy, such as 60.0 mAP at\n100 ms vs 61.0 mAP at 500 ms. A quantitative assessment of the trade-offs\nbetween different hardware devices and accelerators remains unexplored, despite\nbeing critical for real-time applications. Furthermore, they overlook the\nimpact on collision avoidance in motion planning, for example, 60.0 mAP leading\nto safer motion planning or 61.0 mAP leading to high-risk motion planning. In\nthis paper, we introduce latency-aware AP (L-AP) and planning-aware AP (P-AP)\nas new metrics, which consider the physical world such as the concept of time\nand physical constraints, offering a more comprehensive evaluation for\nreal-time 3D object detection. We demonstrate the effectiveness of our metrics\nfor the entire autonomous driving system using nuPlan dataset, and evaluate 3D\nobject detection models accounting for hardware differences and accelerators.\nWe also develop a state-of-the-art performance model for real-time 3D object\ndetection through latency-aware hyperparameter optimization (L-HPO) using our\nmetrics. Additionally, we quantitatively demonstrate that the assumption \"the\nmore point clouds, the better the recognition performance\" is incorrect for\nreal-time applications and optimize both hardware and model selection using our\nmetrics.", "AI": {"tldr": "论文提出了两种新指标L-AP和P-AP，用于更全面地评估实时3D物体检测系统，考虑了硬件差异和运动规划的影响，并通过实验验证了其有效性。", "motivation": "现有3D物体检测研究通常忽略速度与精度之间的权衡，以及硬件差异和运动规划的影响，这对自动驾驶系统的实时应用至关重要。", "method": "引入延迟感知AP（L-AP）和规划感知AP（P-AP）作为新指标，结合nuPlan数据集评估模型，并通过延迟感知超参数优化（L-HPO）开发高性能模型。", "result": "实验证明新指标的有效性，并优化了硬件和模型选择，同时纠正了‘点云越多性能越好’的错误假设。", "conclusion": "新指标为实时3D物体检测提供了更全面的评估方法，优化了自动驾驶系统的性能。"}}
{"id": "2507.00461", "pdf": "https://arxiv.org/pdf/2507.00461", "abs": "https://arxiv.org/abs/2507.00461", "authors": ["Garimella Ramamurthy", "Marcos Eduardo Valle", "Tata Jagannadha Swamy"], "title": "Novel Complex-Valued Hopfield Neural Networks with Phase and Magnitude Quantization", "categories": ["cs.NE", "cs.AI"], "comment": "Paper submitted to the Fifth International Conference on Emerging\n  Techniques in Computational Intelligence (ICETCI 2025)", "summary": "This research paper introduces two novel complex-valued Hopfield neural\nnetworks (CvHNNs) that incorporate phase and magnitude quantization. The first\nCvHNN employs a ceiling-type activation function that operates on the\nrectangular coordinate representation of the complex net contribution. The\nsecond CvHNN similarly incorporates phase and magnitude quantization but\nutilizes a ceiling-type activation function based on the polar coordinate\nrepresentation of the complex net contribution. The proposed CvHNNs, with their\nphase and magnitude quantization, significantly increase the number of states\ncompared to existing models in the literature, thereby expanding the range of\npotential applications for CvHNNs.", "AI": {"tldr": "论文提出两种新型复数Hopfield神经网络（CvHNNs），通过相位和幅度量化增加状态数，扩展应用范围。", "motivation": "现有复数Hopfield神经网络的状态数有限，限制了其应用潜力。", "method": "第一种CvHNN基于矩形坐标表示，使用天花板型激活函数；第二种基于极坐标表示，同样采用相位和幅度量化。", "result": "提出的CvHNNs显著增加了状态数，优于现有模型。", "conclusion": "相位和幅度量化的CvHNNs扩展了Hopfield神经网络的应用潜力。"}}
{"id": "2507.00206", "pdf": "https://arxiv.org/pdf/2507.00206", "abs": "https://arxiv.org/abs/2507.00206", "authors": ["Wenwu Tang", "Khaled Seyam", "Bin Yang"], "title": "Towards 3D Semantic Image Synthesis for Medical Imaging", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "In the medical domain, acquiring large datasets is challenging due to both\naccessibility issues and stringent privacy regulations. Consequently, data\navailability and privacy protection are major obstacles to applying machine\nlearning in medical imaging. To address this, our study proposes the Med-LSDM\n(Latent Semantic Diffusion Model), which operates directly in the 3D domain and\nleverages de-identified semantic maps to generate synthetic data as a method of\nprivacy preservation and data augmentation. Unlike many existing methods that\nfocus on generating 2D slices, Med-LSDM is designed specifically for 3D\nsemantic image synthesis, making it well-suited for applications requiring full\nvolumetric data. Med-LSDM incorporates a guiding mechanism that controls the 3D\nimage generation process by applying a diffusion model within the latent space\nof a pre-trained VQ-GAN. By operating in the compressed latent space, the model\nsignificantly reduces computational complexity while still preserving critical\n3D spatial details. Our approach demonstrates strong performance in 3D semantic\nmedical image synthesis, achieving a 3D-FID score of 0.0054 on the conditional\nDuke Breast dataset and similar Dice scores (0.70964) to those of real images\n(0.71496). These results demonstrate that the synthetic data from our model\nhave a small domain gap with real data and are useful for data augmentation.", "AI": {"tldr": "论文提出Med-LSDM模型，通过3D语义扩散生成合成医学图像，解决数据隐私和可用性问题。", "motivation": "医学领域数据获取困难且隐私要求严格，阻碍机器学习应用。", "method": "Med-LSDM在3D域操作，利用去标识语义图和扩散模型生成合成数据。", "result": "模型在3D-FID和Dice分数上表现优异，接近真实数据。", "conclusion": "合成数据与真实数据差距小，适用于数据增强。"}}
{"id": "2507.00467", "pdf": "https://arxiv.org/pdf/2507.00467", "abs": "https://arxiv.org/abs/2507.00467", "authors": ["Sijan Bhattarai", "Saurav Bhandari", "Girija Bhusal", "Saroj Shakya", "Tapendra Pandey"], "title": "Diversity Conscious Refined Random Forest", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Random Forest (RF) is a widely used ensemble learning technique known for its\nrobust classification performance across diverse domains. However, it often\nrelies on hundreds of trees and all input features, leading to high inference\ncost and model redundancy. In this work, our goal is to grow trees dynamically\nonly on informative features and then enforce maximal diversity by clustering\nand retaining uncorrelated trees. Therefore, we propose a Refined Random Forest\nClassifier that iteratively refines itself by first removing the least\ninformative features and then analytically determines how many new trees should\nbe grown, followed by correlation-based clustering to remove redundant trees.\nThe classification accuracy of our model was compared against the standard RF\non the same number of trees. Experiments on 8 multiple benchmark datasets,\nincluding binary and multiclass datasets, demonstrate that the proposed model\nachieves improved accuracy compared to standard RF.", "AI": {"tldr": "提出了一种改进的随机森林分类器，通过动态选择信息特征和聚类去冗余树，提高了分类精度。", "motivation": "标准随机森林依赖大量树和所有特征，导致推理成本高和模型冗余。", "method": "动态选择信息特征，迭代移除冗余特征，并通过相关性聚类保留不相关树。", "result": "在多个基准数据集上，改进模型比标准随机森林精度更高。", "conclusion": "该方法有效减少了模型冗余并提升了分类性能。"}}
{"id": "2507.00482", "pdf": "https://arxiv.org/pdf/2507.00482", "abs": "https://arxiv.org/abs/2507.00482", "authors": ["Chanseok Lee", "Fakhriyya Mammadova", "Jiseong Barg", "Mooseok Jang"], "title": "Physics-Aware Style Transfer for Adaptive Holographic Reconstruction", "categories": ["physics.optics", "cs.AI", "cs.LG"], "comment": "Keywords: holographic imaging, style transfer, phase retrieval, deep\n  learning", "summary": "Inline holographic imaging presents an ill-posed inverse problem of\nreconstructing objects' complex amplitude from recorded diffraction patterns.\nAlthough recent deep learning approaches have shown promise over classical\nphase retrieval algorithms, they often require high-quality ground truth\ndatasets of complex amplitude maps to achieve a statistical inverse mapping\noperation between the two domains. Here, we present a physics-aware style\ntransfer approach that interprets the object-to-sensor distance as an implicit\nstyle within diffraction patterns. Using the style domain as the intermediate\ndomain to construct cyclic image translation, we show that the inverse mapping\noperation can be learned in an adaptive manner only with datasets composed of\nintensity measurements. We further demonstrate its biomedical applicability by\nreconstructing the morphology of dynamically flowing red blood cells,\nhighlighting its potential for real-time, label-free imaging. As a framework\nthat leverages physical cues inherently embedded in measurements, the presented\nmethod offers a practical learning strategy for imaging applications where\nground truth is difficult or impossible to obtain.", "AI": {"tldr": "论文提出了一种基于物理感知的风格迁移方法，用于解决内联全息成像中的逆问题，无需高质量的真实数据集即可重建物体的复振幅。", "motivation": "传统深度学习方法需要高质量的真实数据集，而实际应用中这类数据难以获取。本文旨在通过物理线索实现自适应学习。", "method": "利用物体到传感器的距离作为衍射图案中的隐式风格，通过风格域构建循环图像翻译，仅需强度测量数据集即可学习逆映射操作。", "result": "方法成功重建了动态流动的红细胞形态，展示了其在实时、无标记成像中的潜力。", "conclusion": "该方法为难以获取真实数据的成像应用提供了一种实用的学习策略。"}}
{"id": "2507.00320", "pdf": "https://arxiv.org/pdf/2507.00320", "abs": "https://arxiv.org/abs/2507.00320", "authors": ["Christiana Westlin", "Ashutosh Singh", "Deniz Erdogmus", "Georgios Stratis", "Lisa Feldman Barrett"], "title": "Exploring Theory-Laden Observations in the Brain Basis of Emotional Experience", "categories": ["cs.LG", "cs.CV", "q-bio.NC"], "comment": null, "summary": "In the science of emotion, it is widely assumed that folk emotion categories\nform a biological and psychological typology, and studies are routinely\ndesigned and analyzed to identify emotion-specific patterns. This approach\nshapes the observations that studies report, ultimately reinforcing the\nassumption that guided the investigation. Here, we reanalyzed data from one\nsuch typologically-guided study that reported mappings between individual brain\npatterns and group-averaged ratings of 34 emotion categories. Our reanalysis\nwas guided by an alternative view of emotion categories as populations of\nvariable, situated instances, and which predicts a priori that there will be\nsignificant variation in brain patterns within a category across instances.\nCorrespondingly, our analysis made minimal assumptions about the structure of\nthe variance present in the data. As predicted, we did not observe the original\nmappings and instead observed significant variation across individuals. These\nfindings demonstrate how starting assumptions can ultimately impact scientific\nconclusions and suggest that a hypothesis must be supported using multiple\nanalytic methods before it is taken seriously.", "AI": {"tldr": "论文重新分析了情绪类别的研究数据，发现情绪类别内部的脑模式存在显著变异，挑战了传统情绪分类的生物学和心理学假设。", "motivation": "传统情绪科学假设情绪类别具有生物学和心理学上的固定模式，但作者提出情绪类别是多样化的实例集合，需要更灵活的假设。", "method": "重新分析了一项基于情绪类别固定模式的研究数据，采用最小假设的方法分析数据中的变异。", "result": "未观察到原始研究中情绪类别与脑模式的固定映射，而是发现个体间存在显著变异。", "conclusion": "研究假设对科学结论有重要影响，需通过多种分析方法验证假设的可靠性。"}}
{"id": "2507.00485", "pdf": "https://arxiv.org/pdf/2507.00485", "abs": "https://arxiv.org/abs/2507.00485", "authors": ["Weiran Guo", "Guanjun Liu", "Ziyuan Zhou", "Ling Wang"], "title": "PNAct: Crafting Backdoor Attacks in Safe Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement Learning (RL) is widely used in tasks where agents interact\nwith an environment to maximize rewards. Building on this foundation, Safe\nReinforcement Learning (Safe RL) incorporates a cost metric alongside the\nreward metric, ensuring that agents adhere to safety constraints during\ndecision-making. In this paper, we identify that Safe RL is vulnerable to\nbackdoor attacks, which can manipulate agents into performing unsafe actions.\nFirst, we introduce the relevant concepts and evaluation metrics for backdoor\nattacks in Safe RL. It is the first attack framework in the Safe RL field that\ninvolves both Positive and Negative Action sample (PNAct) is to implant\nbackdoors, where positive action samples provide reference actions and negative\naction samples indicate actions to be avoided. We theoretically point out the\nproperties of PNAct and design an attack algorithm. Finally, we conduct\nexperiments to evaluate the effectiveness of our proposed backdoor attack\nframework, evaluating it with the established metrics. This paper highlights\nthe potential risks associated with Safe RL and underscores the feasibility of\nsuch attacks. Our code and supplementary material are available at\nhttps://github.com/azure-123/PNAct.", "AI": {"tldr": "本文提出了一种针对安全强化学习（Safe RL）的后门攻击框架PNAct，通过正负动作样本植入后门，揭示了Safe RL的潜在风险。", "motivation": "Safe RL在确保安全约束的同时可能受到后门攻击的威胁，本文旨在揭示这种风险并提出攻击框架。", "method": "引入PNAct框架，结合正负动作样本植入后门，设计攻击算法并进行实验验证。", "result": "实验验证了PNAct框架的有效性，证明了Safe RL存在后门攻击的可行性。", "conclusion": "本文揭示了Safe RL的后门攻击风险，为未来研究提供了安全性的新视角。"}}
{"id": "2507.00333", "pdf": "https://arxiv.org/pdf/2507.00333", "abs": "https://arxiv.org/abs/2507.00333", "authors": ["Emin Zerman", "Jonas Carlsson", "Mårten Sjöström"], "title": "Scope Meets Screen: Lessons Learned in Designing Composite Visualizations for Marksmanship Training Across Skill Levels", "categories": ["cs.HC", "cs.CV", "cs.GR", "eess.IV"], "comment": "5 pages, accepted at IEEE VIS 2025", "summary": "Marksmanship practices are required in various professions, including police,\nmilitary personnel, hunters, as well as sports shooters, such as Olympic\nshooting, biathlon, and modern pentathlon. The current form of training and\ncoaching is mostly based on repetition, where the coach does not see through\nthe eyes of the shooter, and analysis is limited to stance and accuracy\npost-session. In this study, we present a shooting visualization system and\nevaluate its perceived effectiveness for both novice and expert shooters. To\nachieve this, five composite visualizations were developed using first-person\nshooting video recordings enriched with overlaid metrics and graphical\nsummaries. These views were evaluated with 10 participants (5 expert marksmen,\n5 novices) through a mixed-methods study including shot-count and aiming\ninterpretation tasks, pairwise preference comparisons, and semi-structured\ninterviews. The results show that a dashboard-style composite view, combining\nraw video with a polar plot and selected graphs, was preferred in 9 of 10 cases\nand supported understanding across skill levels. The insights gained from this\ndesign study point to the broader value of integrating first-person video with\nvisual analytics for coaching, and we suggest directions for applying this\napproach to other precision-based sports.", "AI": {"tldr": "研究开发了一种射击可视化系统，结合第一人称视频与图形化数据，用于提升射击训练效果，受到新手和专家的普遍认可。", "motivation": "当前射击训练主要依赖重复练习和事后分析，缺乏实时反馈和教练的直观视角。", "method": "开发了五种复合可视化工具，通过第一人称视频和叠加数据，对10名参与者（5名专家和5名新手）进行混合方法研究。", "result": "仪表盘式复合视图（结合原始视频、极坐标图和图表）在10人中有9人偏好，且适用于不同技能水平。", "conclusion": "研究表明，第一人称视频与可视化分析的结合对射击训练有显著价值，并可推广至其他精准运动。"}}
{"id": "2507.00491", "pdf": "https://arxiv.org/pdf/2507.00491", "abs": "https://arxiv.org/abs/2507.00491", "authors": ["Zain Taufique", "Aman Vyas", "Antonio Miele", "Pasi Liljeberg", "Anil Kanduri"], "title": "Twill: Scheduling Compound AI Systems on Heterogeneous Mobile Edge Platforms", "categories": ["cs.MA", "cs.AI", "cs.CV", "cs.PF"], "comment": "9 Pages, 9 Figures, Accepted in International Conference on\n  Computer-Aided Design (ICCAD) 2025", "summary": "Compound AI (cAI) systems chain multiple AI models to solve complex problems.\ncAI systems are typically composed of deep neural networks (DNNs),\ntransformers, and large language models (LLMs), exhibiting a high degree of\ncomputational diversity and dynamic workload variation. Deploying cAI services\non mobile edge platforms poses a significant challenge in scheduling concurrent\nDNN-transformer inference tasks, which arrive dynamically in an unknown\nsequence. Existing mobile edge AI inference strategies manage multi-DNN or\ntransformer-only workloads, relying on design-time profiling, and cannot handle\nconcurrent inference of DNNs and transformers required by cAI systems. In this\nwork, we address the challenge of scheduling cAI systems on heterogeneous\nmobile edge platforms. We present Twill, a run-time framework to handle\nconcurrent inference requests of cAI workloads through task affinity-aware\ncluster mapping and migration, priority-aware task freezing/unfreezing, and\nDVFS, while minimizing inference latency within power budgets. We implement and\ndeploy our Twill framework on the Nvidia Jetson Orin NX platform. We evaluate\nTwill against state-of-the-art edge AI inference techniques over contemporary\nDNNs and LLMs, reducing inference latency by 54% on average, while honoring\npower budgets.", "AI": {"tldr": "Twill是一个运行时框架，用于在异构移动边缘平台上调度复合AI（cAI）系统的并发推理任务，通过任务亲和性感知的集群映射和迁移、优先级感知的任务冻结/解冻以及DVFS，显著降低推理延迟。", "motivation": "现有的移动边缘AI推理策略无法处理cAI系统中DNN和Transformer的并发推理需求，因此需要一种新的调度方法。", "method": "Twill框架采用任务亲和性感知的集群映射和迁移、优先级感知的任务冻结/解冻以及动态电压频率调整（DVFS）技术。", "result": "在Nvidia Jetson Orin NX平台上，Twill将推理延迟平均降低了54%，同时满足功耗预算。", "conclusion": "Twill有效解决了cAI系统在移动边缘平台上的调度挑战，显著提升了性能和能效。"}}
{"id": "2507.00398", "pdf": "https://arxiv.org/pdf/2507.00398", "abs": "https://arxiv.org/abs/2507.00398", "authors": ["Jian Wang", "Qiongying Ni", "Hongkui Yu", "Ruixuan Yao", "Jinqiao Ying", "Bin Zhang", "Xingyi Yang", "Jin Peng", "Jiongquan Chen", "Junxuan Yu", "Wenlong Shi", "Chaoyu Chen", "Zhongnuo Yan", "Mingyuan Luo", "Gaocheng Cai", "Dong Ni", "Jing Lu", "Xin Yang"], "title": "Accurate and Efficient Fetal Birth Weight Estimation from 3D Ultrasound", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted by MICCAI 2025", "summary": "Accurate fetal birth weight (FBW) estimation is essential for optimizing\ndelivery decisions and reducing perinatal mortality. However, clinical methods\nfor FBW estimation are inefficient, operator-dependent, and challenging to\napply in cases of complex fetal anatomy. Existing deep learning methods are\nbased on 2D standard ultrasound (US) images or videos that lack spatial\ninformation, limiting their prediction accuracy. In this study, we propose the\nfirst method for directly estimating FBW from 3D fetal US volumes. Our approach\nintegrates a multi-scale feature fusion network (MFFN) and a synthetic\nsample-based learning framework (SSLF). The MFFN effectively extracts and fuses\nmulti-scale features under sparse supervision by incorporating channel\nattention, spatial attention, and a ranking-based loss function. SSLF generates\nsynthetic samples by simply combining fetal head and abdomen data from\ndifferent fetuses, utilizing semi-supervised learning to improve prediction\nperformance. Experimental results demonstrate that our method achieves superior\nperformance, with a mean absolute error of $166.4\\pm155.9$ $g$ and a mean\nabsolute percentage error of $5.1\\pm4.6$%, outperforming existing methods and\napproaching the accuracy of a senior doctor. Code is available at:\nhttps://github.com/Qioy-i/EFW.", "AI": {"tldr": "提出了一种基于3D胎儿超声体积直接估计胎儿出生体重的方法，结合多尺度特征融合网络和合成样本学习框架，显著提高了预测准确性。", "motivation": "现有胎儿出生体重估计方法效率低、依赖操作者且难以应对复杂胎儿解剖结构，而现有深度学习方法因缺乏空间信息而受限。", "method": "采用多尺度特征融合网络（MFFN）提取和融合多尺度特征，结合合成样本学习框架（SSLF）生成合成样本以提升性能。", "result": "实验结果显示，该方法平均绝对误差为166.4±155.9克，平均绝对百分比误差为5.1±4.6%，优于现有方法并接近资深医生水平。", "conclusion": "该方法为胎儿出生体重估计提供了更准确和高效的解决方案，具有临床应用潜力。"}}
{"id": "2507.00416", "pdf": "https://arxiv.org/pdf/2507.00416", "abs": "https://arxiv.org/abs/2507.00416", "authors": ["Tao Lin", "Gen Li", "Yilei Zhong", "Yanwen Zou", "Bo Zhao"], "title": "Evo-0: Vision-Language-Action Model with Implicit Spatial Understanding", "categories": ["cs.RO", "cs.CV"], "comment": null, "summary": "Vision-Language-Action (VLA) models have emerged as a promising framework for\nenabling generalist robots capable of perceiving, reasoning, and acting in the\nreal world. These models usually build upon pretrained Vision-Language Models\n(VLMs), which excel at semantic understanding due to large-scale text\npretraining. However, VLMs typically lack precise spatial understanding\ncapabilities, as they are primarily tuned on 2D image-text pairs without 3D\nsupervision. To address this limitation, recent approaches have incorporated\nexplicit 3D inputs such as point clouds or depth maps, but this necessitates\nadditional depth sensors or defective estimation. In contrast, our work\nintroduces a plug-and-play module that implicitly injects 3D geometry features\ninto VLA models by leveraging an off-the-shelf visual geometry foundation\nmodels. We design five spatially challenging tasks that require precise spatial\nunderstanding ability to validate effectiveness of our method. Extensive\nevaluations show that our method significantly improves the performance of\nstate-of-the-art VLA models across diverse scenarios.", "AI": {"tldr": "论文提出了一种隐式注入3D几何特征的模块，显著提升了Vision-Language-Action (VLA)模型在空间理解任务中的性能。", "motivation": "现有Vision-Language Models (VLMs)缺乏精确的空间理解能力，需要额外的3D输入，而本文旨在通过隐式方法解决这一问题。", "method": "利用现成的视觉几何基础模型，设计了一个即插即用的模块，隐式注入3D几何特征到VLA模型中。", "result": "在五个空间挑战任务中，该方法显著提升了VLA模型的性能。", "conclusion": "该方法无需额外传感器或缺陷估计，有效增强了VLA模型的空间理解能力。"}}
{"id": "2507.00513", "pdf": "https://arxiv.org/pdf/2507.00513", "abs": "https://arxiv.org/abs/2507.00513", "authors": ["Kai Qin", "Kexin Du", "Yimeng Chen", "Yueyan Liu", "Jie Cai", "Zhiqiang Nie", "Nan Gao", "Guohui Wei", "Shengzhu Wang", "Chun Yu"], "title": "Customer Service Representative's Perception of the AI Assistant in an Organization's Call Center", "categories": ["cs.HC", "cs.AI", "cs.CY"], "comment": "ACM CSCW Poster 2025", "summary": "The integration of various AI tools creates a complex socio-technical\nenvironment where employee-customer interactions form the core of work\npractices. This study investigates how customer service representatives (CSRs)\nat the power grid service customer service call center perceive AI assistance\nin their interactions with customers. Through a field visit and semi-structured\ninterviews with 13 CSRs, we found that AI can alleviate some traditional\nburdens during the call (e.g., typing and memorizing) but also introduces new\nburdens (e.g., earning, compliance, psychological burdens). This research\ncontributes to a more nuanced understanding of AI integration in organizational\nsettings and highlights the efforts and burdens undertaken by CSRs to adapt to\nthe updated system.", "AI": {"tldr": "研究探讨了AI辅助对电网客服中心员工的影响，发现AI既能减轻传统负担，也带来新负担。", "motivation": "探索AI工具在客服代表与客户互动中的实际影响，理解AI整合的复杂性。", "method": "通过实地考察和半结构化访谈13名客服代表。", "result": "AI减轻了打字和记忆等传统负担，但也引入了学习、合规和心理等新负担。", "conclusion": "研究深化了对AI在组织中整合的理解，并强调了客服代表为适应新系统所做的努力和负担。"}}
{"id": "2507.00476", "pdf": "https://arxiv.org/pdf/2507.00476", "abs": "https://arxiv.org/abs/2507.00476", "authors": ["Chenliang Zhou", "Zheyuan Hu", "Cengiz Oztireli"], "title": "FreNBRDF: A Frequency-Rectified Neural Material Representation", "categories": ["cs.GR", "cs.CV"], "comment": null, "summary": "Accurate material modeling is crucial for achieving photorealistic rendering,\nbridging the gap between computer-generated imagery and real-world photographs.\nWhile traditional approaches rely on tabulated BRDF data, recent work has\nshifted towards implicit neural representations, which offer compact and\nflexible frameworks for a range of tasks. However, their behavior in the\nfrequency domain remains poorly understood. To address this, we introduce\nFreNBRDF, a frequency-rectified neural material representation. By leveraging\nspherical harmonics, we integrate frequency-domain considerations into neural\nBRDF modeling. We propose a novel frequency-rectified loss, derived from a\nfrequency analysis of neural materials, and incorporate it into a generalizable\nand adaptive reconstruction and editing pipeline. This framework enhances\nfidelity, adaptability, and efficiency. Extensive experiments demonstrate that\n\\ours improves the accuracy and robustness of material appearance\nreconstruction and editing compared to state-of-the-art baselines, enabling\nmore structured and interpretable downstream tasks and applications.", "AI": {"tldr": "FreNBRDF是一种基于频率校正的神经材料表示方法，通过球谐函数将频域考虑融入神经BRDF建模，提升了材料外观重建和编辑的准确性与鲁棒性。", "motivation": "传统方法依赖表格化BRDF数据，而神经隐式表示虽灵活但频域行为理解不足，需改进以实现更高保真度的渲染。", "method": "提出频率校正损失函数，结合球谐函数，构建通用且自适应的重建与编辑流程。", "result": "实验表明，FreNBRDF在材料外观重建和编辑上优于现有方法，提高了下游任务的结构化和可解释性。", "conclusion": "FreNBRDF通过频域优化，显著提升了神经材料建模的保真度和适应性。"}}
{"id": "2507.00535", "pdf": "https://arxiv.org/pdf/2507.00535", "abs": "https://arxiv.org/abs/2507.00535", "authors": ["Dietmar Jannach", "Amra Delić", "Francesco Ricci", "Markus Zanker"], "title": "Rethinking Group Recommender Systems in the Era of Generative AI: From One-Shot Recommendations to Agentic Group Decision Support", "categories": ["cs.IR", "cs.AI"], "comment": "Submitted for publication", "summary": "More than twenty-five years ago, first ideas were developed on how to design\na system that can provide recommendations to groups of users instead of\nindividual users. Since then, a rich variety of algorithmic proposals were\npublished, e.g., on how to acquire individual preferences, how to aggregate\nthem, and how to generate recommendations for groups of users. However, despite\nthe rich literature on the topic, barely any examples of real-world group\nrecommender systems can be found. This lets us question common assumptions in\nacademic research, in particular regarding communication processes in a group\nand how recommendation-supported decisions are made. In this essay, we argue\nthat these common assumptions and corresponding system designs often may not\nmatch the needs or expectations of users. We thus call for a reorientation in\nthis research area, leveraging the capabilities of modern Generative AI\nassistants like ChatGPT. Specifically, as one promising future direction, we\nenvision group recommender systems to be systems where human group members\ninteract in a chat and an AI-based group recommendation agent assists the\ndecision-making process in an agentic way. Ultimately, this shall lead to a\nmore natural group decision-making environment and finally to wider adoption of\ngroup recommendation systems in practice.", "AI": {"tldr": "论文探讨了群体推荐系统的研究现状，指出尽管学术研究丰富，但实际应用稀少，并提出利用现代生成式AI（如ChatGPT）重新设计系统，以更自然的方式支持群体决策。", "motivation": "尽管群体推荐系统研究已有25年历史，但实际应用案例极少，作者质疑学术研究中的常见假设是否与用户需求匹配。", "method": "提出利用现代生成式AI助手（如ChatGPT）设计新型群体推荐系统，通过AI代理辅助群体决策。", "result": "提出未来研究方向，即通过AI代理在聊天中辅助群体决策，以实现更自然的决策环境。", "conclusion": "呼吁重新定位群体推荐系统研究，利用AI技术提升实用性，推动其在实际中的广泛应用。"}}
{"id": "2507.00498", "pdf": "https://arxiv.org/pdf/2507.00498", "abs": "https://arxiv.org/abs/2507.00498", "authors": ["Yifan Liu", "Yu Fang", "Zhouhan Lin"], "title": "MuteSwap: Silent Face-based Voice Conversion", "categories": ["cs.SD", "cs.CV", "cs.LG", "cs.MM", "eess.AS"], "comment": null, "summary": "Conventional voice conversion modifies voice characteristics from a source\nspeaker to a target speaker, relying on audio input from both sides. However,\nthis process becomes infeasible when clean audio is unavailable, such as in\nsilent videos or noisy environments. In this work, we focus on the task of\nSilent Face-based Voice Conversion (SFVC), which does voice conversion entirely\nfrom visual inputs. i.e., given images of a target speaker and a silent video\nof a source speaker containing lip motion, SFVC generates speech aligning the\nidentity of the target speaker while preserving the speech content in the\nsource silent video. As this task requires generating intelligible speech and\nconverting identity using only visual cues, it is particularly challenging. To\naddress this, we introduce MuteSwap, a novel framework that employs contrastive\nlearning to align cross-modality identities and minimize mutual information to\nseparate shared visual features. Experimental results show that MuteSwap\nachieves impressive performance in both speech synthesis and identity\nconversion, especially under noisy conditions where methods dependent on audio\ninput fail to produce intelligible results, demonstrating both the\neffectiveness of our training approach and the feasibility of SFVC.", "AI": {"tldr": "MuteSwap框架通过视觉输入实现无声视频的语音转换，解决了传统方法依赖音频输入的局限性。", "motivation": "传统语音转换依赖干净音频输入，但在无声视频或嘈杂环境中不可行，因此需要仅基于视觉输入的解决方案。", "method": "提出MuteSwap框架，利用对比学习对齐跨模态身份，并通过最小化互信息分离共享视觉特征。", "result": "MuteSwap在语音合成和身份转换中表现优异，尤其在嘈杂环境下优于依赖音频的方法。", "conclusion": "MuteSwap证明了仅基于视觉输入的语音转换的可行性，并展示了其训练方法的有效性。"}}
{"id": "2507.00511", "pdf": "https://arxiv.org/pdf/2507.00511", "abs": "https://arxiv.org/abs/2507.00511", "authors": ["Sayandeep Kanrar", "Raja Piyush", "Qaiser Razi", "Debanshi Chakraborty", "Vikas Hassija", "GSS Chalapathi"], "title": "Medical Image Segmentation Using Advanced Unet: VMSE-Unet and VM-Unet CBAM+", "categories": ["eess.IV", "cs.CV", "cs.LG"], "comment": "under review", "summary": "In this paper, we present the VMSE U-Net and VM-Unet CBAM+ model, two\ncutting-edge deep learning architectures designed to enhance medical image\nsegmentation. Our approach integrates Squeeze-and-Excitation (SE) and\nConvolutional Block Attention Module (CBAM) techniques into the traditional VM\nU-Net framework, significantly improving segmentation accuracy, feature\nlocalization, and computational efficiency. Both models show superior\nperformance compared to the baseline VM-Unet across multiple datasets. Notably,\nVMSEUnet achieves the highest accuracy, IoU, precision, and recall while\nmaintaining low loss values. It also exhibits exceptional computational\nefficiency with faster inference times and lower memory usage on both GPU and\nCPU. Overall, the study suggests that the enhanced architecture VMSE-Unet is a\nvaluable tool for medical image analysis. These findings highlight its\npotential for real-world clinical applications, emphasizing the importance of\nfurther research to optimize accuracy, robustness, and computational\nefficiency.", "AI": {"tldr": "本文介绍了两种改进的深度学习架构VMSE U-Net和VM-Unet CBAM+，用于提升医学图像分割性能。结合SE和CBAM技术，显著提高了分割精度、特征定位和计算效率。", "motivation": "传统VM U-Net在医学图像分割中存在精度和效率不足的问题，因此需要改进架构以提升性能。", "method": "在VM U-Net框架中集成SE和CBAM技术，构建了VMSE U-Net和VM-Unet CBAM+模型。", "result": "两种模型在多个数据集上表现优于基线VM-Unet，其中VMSE U-Net在精度、IoU、召回率等方面表现最佳，且计算效率更高。", "conclusion": "VMSE U-Net在医学图像分析中具有重要价值，未来需进一步优化其精度、鲁棒性和计算效率。"}}
{"id": "2507.00546", "pdf": "https://arxiv.org/pdf/2507.00546", "abs": "https://arxiv.org/abs/2507.00546", "authors": ["Reza Marzban", "Ali Adibi", "Raphael Pestourie"], "title": "Inverse Design in Nanophotonics via Representation Learning", "categories": ["physics.app-ph", "cs.AI", "cs.LG", "physics.optics"], "comment": null, "summary": "Inverse design in nanophotonics, the computational discovery of structures\nachieving targeted electromagnetic (EM) responses, has become a key tool for\nrecent optical advances. Traditional intuition-driven or iterative optimization\nmethods struggle with the inherently high-dimensional, non-convex design spaces\nand the substantial computational demands of EM simulations. Recently, machine\nlearning (ML) has emerged to address these bottlenecks effectively. This review\nframes ML-enhanced inverse design methodologies through the lens of\nrepresentation learning, classifying them into two categories: output-side and\ninput-side approaches. Output-side methods use ML to learn a representation in\nthe solution space to create a differentiable solver that accelerates\noptimization. Conversely, input-side techniques employ ML to learn compact,\nlatent-space representations of feasible device geometries, enabling efficient\nglobal exploration through generative models. Each strategy presents unique\ntrade-offs in data requirements, generalization capacity, and novel design\ndiscovery potentials. Hybrid frameworks that combine physics-based optimization\nwith data-driven representations help escape poor local optima, improve\nscalability, and facilitate knowledge transfer. We conclude by highlighting\nopen challenges and opportunities, emphasizing complexity management,\ngeometry-independent representations, integration of fabrication constraints,\nand advancements in multiphysics co-designs.", "AI": {"tldr": "机器学习在纳米光子学逆向设计中的应用，分为输出端和输入端方法，分别加速优化和探索设计空间，混合框架结合物理优化与数据驱动，提出未来挑战。", "motivation": "传统直觉驱动或迭代优化方法在高维非凸设计空间和计算需求大的电磁模拟中表现不佳，机器学习能有效解决这些瓶颈。", "method": "分类为输出端方法（学习解空间表示以加速优化）和输入端方法（学习潜在空间表示以探索设计空间），并采用混合框架结合物理优化与数据驱动。", "result": "机器学习方法在加速优化、全局探索和知识迁移方面表现优异，混合框架能避免局部最优并提升可扩展性。", "conclusion": "未来需关注复杂性管理、几何无关表示、制造约束集成和多物理场协同设计的进展。"}}
{"id": "2507.00577", "pdf": "https://arxiv.org/pdf/2507.00577", "abs": "https://arxiv.org/abs/2507.00577", "authors": ["Yinghao Wu", "Liyan Zhang"], "title": "BadViM: Backdoor Attack against Vision Mamba", "categories": ["cs.CR", "cs.AI", "cs.CV"], "comment": null, "summary": "Vision State Space Models (SSMs), particularly architectures like Vision\nMamba (ViM), have emerged as promising alternatives to Vision Transformers\n(ViTs). However, the security implications of this novel architecture,\nespecially their vulnerability to backdoor attacks, remain critically\nunderexplored. Backdoor attacks aim to embed hidden triggers into victim\nmodels, causing the model to misclassify inputs containing these triggers while\nmaintaining normal behavior on clean inputs. This paper investigates the\nsusceptibility of ViM to backdoor attacks by introducing BadViM, a novel\nbackdoor attack framework specifically designed for Vision Mamba. The proposed\nBadViM leverages a Resonant Frequency Trigger (RFT) that exploits the frequency\nsensitivity patterns of the victim model to create stealthy, distributed\ntriggers. To maximize attack efficacy, we propose a Hidden State Alignment loss\nthat strategically manipulates the internal representations of model by\naligning the hidden states of backdoor images with those of target classes.\nExtensive experimental results demonstrate that BadViM achieves superior attack\nsuccess rates while maintaining clean data accuracy. Meanwhile, BadViM exhibits\nremarkable resilience against common defensive measures, including PatchDrop,\nPatchShuffle and JPEG compression, which typically neutralize normal backdoor\nattacks.", "AI": {"tldr": "BadViM是一种针对Vision Mamba（ViM）的新型后门攻击框架，利用共振频率触发器和隐藏状态对齐损失，成功实现高攻击成功率，同时对常见防御措施表现出强韧性。", "motivation": "研究ViM架构的安全性，尤其是其对后门攻击的脆弱性，填补该领域的空白。", "method": "提出BadViM框架，结合共振频率触发器（RFT）和隐藏状态对齐损失，设计隐蔽且分布式的触发器。", "result": "实验显示BadViM攻击成功率高，且能保持干净数据的准确性，同时对PatchDrop等防御措施具有强韧性。", "conclusion": "ViM架构对后门攻击存在显著脆弱性，BadViM框架为未来防御研究提供了重要参考。"}}
{"id": "2507.00582", "pdf": "https://arxiv.org/pdf/2507.00582", "abs": "https://arxiv.org/abs/2507.00582", "authors": ["Yi Zhang", "Yidong Zhao", "Qian Tao"], "title": "Bridging Classical and Learning-based Iterative Registration through Deep Equilibrium Models", "categories": ["eess.IV", "cs.CV"], "comment": "Submitted version. Accepted by MICCAI 2025", "summary": "Deformable medical image registration is traditionally formulated as an\noptimization problem. While classical methods solve this problem iteratively,\nrecent learning-based approaches use recurrent neural networks (RNNs) to mimic\nthis process by unrolling the prediction of deformation fields in a fixed\nnumber of steps. However, classical methods typically converge after sufficient\niterations, but learning-based unrolling methods lack a theoretical convergence\nguarantee and show instability empirically. In addition, unrolling methods have\na practical bottleneck at training time: GPU memory usage grows linearly with\nthe unrolling steps due to backpropagation through time (BPTT). To address both\ntheoretical and practical challenges, we propose DEQReg, a novel registration\nframework based on Deep Equilibrium Models (DEQ), which formulates registration\nas an equilibrium-seeking problem, establishing a natural connection between\nclassical optimization and learning-based unrolling methods. DEQReg maintains\nconstant memory usage, enabling theoretically unlimited iteration steps.\nThrough extensive evaluation on the public brain MRI and lung CT datasets, we\nshow that DEQReg can achieve competitive registration performance, while\nsubstantially reducing memory consumption compared to state-of-the-art\nunrolling methods. We also reveal an intriguing phenomenon: the performance of\nexisting unrolling methods first increases slightly then degrades irreversibly\nwhen the inference steps go beyond the training configuration. In contrast,\nDEQReg achieves stable convergence with its inbuilt equilibrium-seeking\nmechanism, bridging the gap between classical optimization-based and modern\nlearning-based registration methods.", "AI": {"tldr": "论文提出DEQReg，一种基于深度平衡模型的医学图像配准框架，解决了传统学习方法的收敛性和内存问题。", "motivation": "传统迭代方法和学习型展开方法在医学图像配准中存在收敛性不足和内存消耗大的问题。", "method": "采用深度平衡模型（DEQ），将配准问题转化为平衡寻找问题，保持恒定内存使用。", "result": "在脑MRI和肺CT数据集上表现优异，内存消耗显著降低，且收敛稳定。", "conclusion": "DEQReg通过平衡机制，弥合了传统优化方法与现代学习型方法之间的差距。"}}
{"id": "2507.00635", "pdf": "https://arxiv.org/pdf/2507.00635", "abs": "https://arxiv.org/abs/2507.00635", "authors": ["Tinghe Hong", "Shenlin Cai", "Boyang Li", "Kai Huang"], "title": "Stable Tracking of Eye Gaze Direction During Ophthalmic Surgery", "categories": ["cs.RO", "cs.CV", "cs.HC"], "comment": "Accepted by ICRA 2025", "summary": "Ophthalmic surgical robots offer superior stability and precision by reducing\nthe natural hand tremors of human surgeons, enabling delicate operations in\nconfined surgical spaces. Despite the advancements in developing vision- and\nforce-based control methods for surgical robots, preoperative navigation\nremains heavily reliant on manual operation, limiting the consistency and\nincreasing the uncertainty. Existing eye gaze estimation techniques in the\nsurgery, whether traditional or deep learning-based, face challenges including\ndependence on additional sensors, occlusion issues in surgical environments,\nand the requirement for facial detection. To address these limitations, this\nstudy proposes an innovative eye localization and tracking method that combines\nmachine learning with traditional algorithms, eliminating the requirements of\nlandmarks and maintaining stable iris detection and gaze estimation under\nvarying lighting and shadow conditions. Extensive real-world experiment results\nshow that our proposed method has an average estimation error of 0.58 degrees\nfor eye orientation estimation and 2.08-degree average control error for the\nrobotic arm's movement based on the calculated orientation.", "AI": {"tldr": "提出了一种结合机器学习和传统算法的眼动追踪方法，解决了手术机器人导航中的眼动估计问题，提高了精度和稳定性。", "motivation": "现有眼动估计技术依赖额外传感器或面部检测，且在手术环境中易受遮挡和光线变化影响，限制了术前导航的准确性和一致性。", "method": "结合机器学习和传统算法，无需依赖标志点，能在不同光照和阴影条件下稳定检测虹膜并估计视线方向。", "result": "实验显示，眼动方向估计平均误差为0.58度，基于估计方向的机械臂运动控制平均误差为2.08度。", "conclusion": "该方法显著提升了手术机器人导航的精确性和稳定性，解决了现有技术的局限性。"}}
{"id": "2507.00651", "pdf": "https://arxiv.org/pdf/2507.00651", "abs": "https://arxiv.org/abs/2507.00651", "authors": ["Maurizio Filippone", "Marius P. Linhard"], "title": "GANs Secretly Perform Approximate Bayesian Model Selection", "categories": ["cs.LG", "cs.CV", "stat.ML"], "comment": null, "summary": "Generative Adversarial Networks (GANs) are popular and successful generative\nmodels. Despite their success, optimization is notoriously challenging and they\nrequire regularization against overfitting. In this work, we explain the\nsuccess and limitations of GANs by interpreting them as probabilistic\ngenerative models. This interpretation enables us to view GANs as Bayesian\nneural networks with partial stochasticity, allowing us to establish conditions\nof universal approximation. We can then cast the adversarial-style optimization\nof several variants of GANs as the optimization of a proxy for the marginal\nlikelihood. Taking advantage of the connection between marginal likelihood\noptimization and Occam's razor, we can define regularization and optimization\nstrategies to smooth the loss landscape and search for solutions with minimum\ndescription length, which are associated with flat minima and good\ngeneralization. The results on a wide range of experiments indicate that these\nstrategies lead to performance improvements and pave the way to a deeper\nunderstanding of regularization strategies for GANs.", "AI": {"tldr": "该论文将GANs解释为概率生成模型，并提出了基于边际似然优化的正则化和优化策略，以改善其性能和泛化能力。", "motivation": "尽管GANs在生成模型中很成功，但其优化困难且容易过拟合，需要更好的正则化方法。", "method": "将GANs视为贝叶斯神经网络，通过边际似然优化代理目标，定义正则化和优化策略。", "result": "实验表明，这些策略提升了性能，并有助于理解GANs的正则化机制。", "conclusion": "通过概率视角和边际似然优化，为GANs的正则化和优化提供了新思路。"}}
{"id": "2507.00589", "pdf": "https://arxiv.org/pdf/2507.00589", "abs": "https://arxiv.org/abs/2507.00589", "authors": ["Seok Bin Son", "Joongheon Kim"], "title": "Quantum Circuit Structure Optimization for Quantum Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Reinforcement learning (RL) enables agents to learn optimal policies through\nenvironmental interaction. However, RL suffers from reduced learning efficiency\ndue to the curse of dimensionality in high-dimensional spaces. Quantum\nreinforcement learning (QRL) addresses this issue by leveraging superposition\nand entanglement in quantum computing, allowing efficient handling of\nhigh-dimensional problems with fewer resources. QRL combines quantum neural\nnetworks (QNNs) with RL, where the parameterized quantum circuit (PQC) acts as\nthe core computational module. The PQC performs linear and nonlinear\ntransformations through gate operations, similar to hidden layers in classical\nneural networks. Previous QRL studies, however, have used fixed PQC structures\nbased on empirical intuition without verifying their optimality. This paper\nproposes a QRL-NAS algorithm that integrates quantum neural architecture search\n(QNAS) to optimize PQC structures within QRL. Experiments demonstrate that\nQRL-NAS achieves higher rewards than QRL with fixed circuits, validating its\neffectiveness and practical utility.", "AI": {"tldr": "论文提出QRL-NAS算法，通过量子神经架构搜索优化PQC结构，提升量子强化学习性能。", "motivation": "传统量子强化学习使用固定PQC结构，未验证其最优性，限制了学习效率。", "method": "结合量子神经架构搜索（QNAS）优化PQC结构，提升量子强化学习的性能。", "result": "实验表明，QRL-NAS比固定PQC结构的QRL获得更高奖励。", "conclusion": "QRL-NAS有效优化PQC结构，提升量子强化学习的实用性和效率。"}}
{"id": "2507.00660", "pdf": "https://arxiv.org/pdf/2507.00660", "abs": "https://arxiv.org/abs/2507.00660", "authors": ["Rusi Chen", "Yuanting Yang", "Jiezhi Yao", "Hongning Song", "Ji Zhang", "Yongsong Zhou", "Yuhao Huang", "Ronghao Yang", "Dan Jia", "Yuhan Zhang", "Xing Tao", "Haoran Dou", "Qing Zhou", "Xin Yang", "Dong Ni"], "title": "MTCNet: Motion and Topology Consistency Guided Learning for Mitral Valve Segmentationin 4D Ultrasound", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "Accepted by MICCAI 2025", "summary": "Mitral regurgitation is one of the most prevalent cardiac disorders.\nFour-dimensional (4D) ultrasound has emerged as the primary imaging modality\nfor assessing dynamic valvular morphology. However, 4D mitral valve (MV)\nanalysis remains challenging due to limited phase annotations, severe motion\nartifacts, and poor imaging quality. Yet, the absence of inter-phase dependency\nin existing methods hinders 4D MV analysis. To bridge this gap, we propose a\nMotion-Topology guided consistency network (MTCNet) for accurate 4D MV\nultrasound segmentation in semi-supervised learning (SSL). MTCNet requires only\nsparse end-diastolic and end-systolic annotations. First, we design a\ncross-phase motion-guided consistency learning strategy, utilizing a\nbi-directional attention memory bank to propagate spatio-temporal features.\nThis enables MTCNet to achieve excellent performance both per- and inter-phase.\nSecond, we devise a novel topology-guided correlation regularization that\nexplores physical prior knowledge to maintain anatomically plausible.\nTherefore, MTCNet can effectively leverage structural correspondence between\nlabeled and unlabeled phases. Extensive evaluations on the first largest 4D MV\ndataset, with 1408 phases from 160 patients, show that MTCNet performs superior\ncross-phase consistency compared to other advanced methods (Dice: 87.30%, HD:\n1.75mm). Both the code and the dataset are available at\nhttps://github.com/crs524/MTCNet.", "AI": {"tldr": "提出了一种基于运动-拓扑引导的一致性网络（MTCNet），用于半监督学习中的4D二尖瓣超声分割，解决了现有方法缺乏相位间依赖性的问题。", "motivation": "4D二尖瓣分析因相位标注有限、运动伪影严重和成像质量差而具有挑战性，现有方法缺乏相位间依赖性。", "method": "设计了跨相位运动引导一致性学习策略和拓扑引导相关性正则化，利用双向注意力记忆库传播时空特征。", "result": "在160名患者的1408个相位数据集上，MTCNet表现出色（Dice: 87.30%，HD: 1.75mm）。", "conclusion": "MTCNet通过运动-拓扑引导策略，显著提升了4D二尖瓣分割的准确性和一致性。"}}
{"id": "2507.00598", "pdf": "https://arxiv.org/pdf/2507.00598", "abs": "https://arxiv.org/abs/2507.00598", "authors": ["Madison Cotteret", "Christopher J. Kymn", "Hugh Greatorex", "Martin Ziegler", "Elisabetta Chicca", "Friedrich T. Sommer"], "title": "High-resolution spatial memory requires grid-cell-like neural codes", "categories": ["cs.NE", "cs.AI", "cs.SC"], "comment": "14 pages, 4 figures. Supplementary material: 11 pages, 5 figures", "summary": "Continuous attractor networks (CANs) are widely used to model how the brain\ntemporarily retains continuous behavioural variables via persistent recurrent\nactivity, such as an animal's position in an environment. However, this memory\nmechanism is very sensitive to even small imperfections, such as noise or\nheterogeneity, which are both common in biological systems. Previous work has\nshown that discretising the continuum into a finite set of discrete attractor\nstates provides robustness to these imperfections, but necessarily reduces the\nresolution of the represented variable, creating a dilemma between stability\nand resolution. We show that this stability-resolution dilemma is most severe\nfor CANs using unimodal bump-like codes, as in traditional models. To overcome\nthis, we investigate sparse binary distributed codes based on random feature\nembeddings, in which neurons have spatially-periodic receptive fields. We\ndemonstrate theoretically and with simulations that such grid-cell-like codes\nenable CANs to achieve both high stability and high resolution simultaneously.\nThe model extends to embedding arbitrary nonlinear manifolds into a CAN, such\nas spheres or tori, and generalises linear path integration to integration\nalong freely-programmable on-manifold vector fields. Together, this work\nprovides a theory of how the brain could robustly represent continuous\nvariables with high resolution and perform flexible computations over\ntask-relevant manifolds.", "AI": {"tldr": "论文提出了一种基于稀疏二进制分布式编码的连续吸引子网络（CANs），解决了传统模型在稳定性和分辨率之间的权衡问题。", "motivation": "传统连续吸引子网络（CANs）对噪声和异质性非常敏感，导致稳定性和分辨率之间的困境。", "method": "采用稀疏二进制分布式编码，基于随机特征嵌入，模拟网格细胞样编码。", "result": "理论和仿真表明，该方法能同时实现高稳定性和高分辨率，并可扩展到非线性流形。", "conclusion": "该研究为大脑如何高分辨率、稳定地表示连续变量提供了理论支持。"}}
{"id": "2507.00669", "pdf": "https://arxiv.org/pdf/2507.00669", "abs": "https://arxiv.org/abs/2507.00669", "authors": ["Duc Cao-Dinh", "Khai Le-Duc", "Anh Dao", "Bach Phan Tat", "Chris Ngo", "Duy M. H. Nguyen", "Nguyen X. Khanh", "Thanh Nguyen-Tang"], "title": "Audio-3DVG: Unified Audio - Point Cloud Fusion for 3D Visual Grounding", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.RO"], "comment": "Work in progress, 42 pages", "summary": "3D Visual Grounding (3DVG) involves localizing target objects in 3D point\nclouds based on natural language. While prior work has made strides using\ntextual descriptions, leveraging spoken language-known as Audio-based 3D Visual\nGrounding-remains underexplored and challenging. Motivated by advances in\nautomatic speech recognition (ASR) and speech representation learning, we\npropose Audio-3DVG, a simple yet effective framework that integrates audio and\nspatial information for enhanced grounding. Rather than treating speech as a\nmonolithic input, we decompose the task into two complementary components.\nFirst, we introduce Object Mention Detection, a multi-label classification task\nthat explicitly identifies which objects are referred to in the audio, enabling\nmore structured audio-scene reasoning. Second, we propose an Audio-Guided\nAttention module that captures interactions between candidate objects and\nrelational speech cues, improving target discrimination in cluttered scenes. To\nsupport benchmarking, we synthesize audio descriptions for standard 3DVG\ndatasets, including ScanRefer, Sr3D, and Nr3D. Experimental results demonstrate\nthat Audio-3DVG not only achieves new state-of-the-art performance in\naudio-based grounding, but also competes with text-based methods-highlighting\nthe promise of integrating spoken language into 3D vision tasks.", "AI": {"tldr": "论文提出Audio-3DVG框架，通过分解音频输入为对象提及检测和音频引导注意力模块，提升3D视觉定位性能，并在合成音频数据集上验证其有效性。", "motivation": "现有3D视觉定位主要依赖文本描述，而基于音频的3D视觉定位研究较少且更具挑战性。", "method": "提出Audio-3DVG框架，包含对象提及检测（多标签分类任务）和音频引导注意力模块，结合音频与空间信息。", "result": "Audio-3DVG在音频定位任务中达到新SOTA，性能接近文本方法。", "conclusion": "研究表明，将语音整合到3D视觉任务中具有潜力。"}}
{"id": "2507.00670", "pdf": "https://arxiv.org/pdf/2507.00670", "abs": "https://arxiv.org/abs/2507.00670", "authors": ["Jan Nikolas Morshuis", "Christian Schlarmann", "Thomas Küstner", "Christian F. Baumgartner", "Matthias Hein"], "title": "Mind the Detail: Uncovering Clinically Relevant Image Details in Accelerated MRI with Semantically Diverse Reconstructions", "categories": ["eess.IV", "cs.CV"], "comment": "MICCAI 2025", "summary": "In recent years, accelerated MRI reconstruction based on deep learning has\nled to significant improvements in image quality with impressive results for\nhigh acceleration factors. However, from a clinical perspective image quality\nis only secondary; much more important is that all clinically relevant\ninformation is preserved in the reconstruction from heavily undersampled data.\nIn this paper, we show that existing techniques, even when considering\nresampling for diffusion-based reconstruction, can fail to reconstruct small\nand rare pathologies, thus leading to potentially wrong diagnosis decisions\n(false negatives). To uncover the potentially missing clinical information we\npropose ``Semantically Diverse Reconstructions'' (\\SDR), a method which, given\nan original reconstruction, generates novel reconstructions with enhanced\nsemantic variability while all of them are fully consistent with the measured\ndata. To evaluate \\SDR automatically we train an object detector on the\nfastMRI+ dataset. We show that \\SDR significantly reduces the chance of\nfalse-negative diagnoses (higher recall) and improves mean average precision\ncompared to the original reconstructions. The code is available on\nhttps://github.com/NikolasMorshuis/SDR", "AI": {"tldr": "论文提出了一种名为“语义多样性重建”（SDR）的方法，用于解决现有深度学习MRI重建技术可能遗漏小且罕见病理信息的问题，从而减少假阴性诊断。", "motivation": "现有MRI重建技术在高加速因子下可能遗漏临床相关的小且罕见病理信息，导致假阴性诊断。", "method": "提出SDR方法，通过生成语义多样性增强的重建图像，确保所有重建与测量数据一致。使用fastMRI+数据集训练目标检测器进行自动评估。", "result": "SDR显著降低了假阴性诊断的概率（召回率提高），并提升了平均精度。", "conclusion": "SDR方法在保留临床相关信息方面优于现有技术，代码已开源。"}}
{"id": "2507.00611", "pdf": "https://arxiv.org/pdf/2507.00611", "abs": "https://arxiv.org/abs/2507.00611", "authors": ["Chenyang Cao", "Miguel Rogel-García", "Mohamed Nabail", "Xueqian Wang", "Nicholas Rhinehart"], "title": "Residual Reward Models for Preference-based Reinforcement Learning", "categories": ["cs.LG", "cs.AI", "cs.RO"], "comment": "26 pages, 22 figures", "summary": "Preference-based Reinforcement Learning (PbRL) provides a way to learn\nhigh-performance policies in environments where the reward signal is hard to\nspecify, avoiding heuristic and time-consuming reward design. However, PbRL can\nsuffer from slow convergence speed since it requires training in a reward\nmodel. Prior work has proposed learning a reward model from demonstrations and\nfine-tuning it using preferences. However, when the model is a neural network,\nusing different loss functions for pre-training and fine-tuning can pose\nchallenges to reliable optimization. In this paper, we propose a method to\neffectively leverage prior knowledge with a Residual Reward Model (RRM). An RRM\nassumes that the true reward of the environment can be split into a sum of two\nparts: a prior reward and a learned reward. The prior reward is a term\navailable before training, for example, a user's ``best guess'' reward\nfunction, or a reward function learned from inverse reinforcement learning\n(IRL), and the learned reward is trained with preferences. We introduce\nstate-based and image-based versions of RRM and evaluate them on several tasks\nin the Meta-World environment suite. Experimental results show that our method\nsubstantially improves the performance of a common PbRL method. Our method\nachieves performance improvements for a variety of different types of prior\nrewards, including proxy rewards, a reward obtained from IRL, and even a\nnegated version of the proxy reward. We also conduct experiments with a Franka\nPanda to show that our method leads to superior performance on a real robot. It\nsignificantly accelerates policy learning for different tasks, achieving\nsuccess in fewer steps than the baseline. The videos are presented at\nhttps://sunlighted.github.io/RRM-web/.", "AI": {"tldr": "论文提出了一种基于残差奖励模型（RRM）的方法，通过结合先验奖励和学习奖励，显著提升了偏好强化学习（PbRL）的性能和收敛速度。", "motivation": "偏好强化学习在奖励信号难以明确的环境中表现优异，但存在收敛速度慢的问题。现有方法在预训练和微调时使用不同损失函数可能导致优化不可靠。", "method": "提出残差奖励模型（RRM），将真实奖励分解为先验奖励和学习奖励两部分。先验奖励可以是用户猜测或逆强化学习得到的奖励，学习奖励通过偏好训练。", "result": "在Meta-World环境中的实验表明，RRM显著提升了PbRL方法的性能，且适用于多种先验奖励类型。在真实机器人任务中也表现出色，学习速度更快。", "conclusion": "RRM方法有效结合先验知识，显著提升了偏好强化学习的性能和收敛速度，适用于多种任务和场景。"}}
{"id": "2507.00673", "pdf": "https://arxiv.org/pdf/2507.00673", "abs": "https://arxiv.org/abs/2507.00673", "authors": ["Abduz Zami", "Shadman Sobhan", "Rounaq Hossain", "Md. Sawran Sorker", "Mohiuddin Ahmed", "Md. Redwan Hossain"], "title": "Prompt2SegCXR:Prompt to Segment All Organs and Diseases in Chest X-rays", "categories": ["eess.IV", "cs.CV"], "comment": "29 Pages", "summary": "Image segmentation plays a vital role in the medical field by isolating\norgans or regions of interest from surrounding areas. Traditionally,\nsegmentation models are trained on a specific organ or a disease, limiting\ntheir ability to handle other organs and diseases. At present, few advanced\nmodels can perform multi-organ or multi-disease segmentation, offering greater\nflexibility. Also, recently, prompt-based image segmentation has gained\nattention as a more flexible approach. It allows models to segment areas based\non user-provided prompts. Despite these advances, there has been no dedicated\nwork on prompt-based interactive multi-organ and multi-disease segmentation,\nespecially for Chest X-rays. This work presents two main contributions: first,\ngenerating doodle prompts by medical experts of a collection of datasets from\nmultiple sources with 23 classes, including 6 organs and 17 diseases,\nspecifically designed for prompt-based Chest X-ray segmentation. Second, we\nintroduce Prompt2SegCXR, a lightweight model for accurately segmenting multiple\norgans and diseases from Chest X-rays. The model incorporates multi-stage\nfeature fusion, enabling it to combine features from various network layers for\nbetter spatial and semantic understanding, enhancing segmentation accuracy.\nCompared to existing pre-trained models for prompt-based image segmentation,\nour model scores well, providing a reliable solution for segmenting Chest\nX-rays based on user prompts.", "AI": {"tldr": "本文提出了一种基于提示的交互式多器官和多疾病分割方法，专注于胸部X光片，并介绍了Prompt2SegCXR模型，该模型通过多阶段特征融合提高了分割准确性。", "motivation": "传统分割模型通常针对单一器官或疾病，缺乏灵活性。目前缺乏专门针对胸部X光片的基于提示的多器官和多疾病分割方法。", "method": "生成由医疗专家提供的涂鸦提示数据集，包含23个类别（6个器官和17种疾病），并开发了轻量级模型Prompt2SegCXR，采用多阶段特征融合技术。", "result": "Prompt2SegCXR在基于提示的分割任务中表现优异，优于现有预训练模型。", "conclusion": "该方法为胸部X光片的多器官和多疾病分割提供了灵活且可靠的解决方案。"}}
{"id": "2507.00613", "pdf": "https://arxiv.org/pdf/2507.00613", "abs": "https://arxiv.org/abs/2507.00613", "authors": ["Nuno Capitão", "Yi Zhang", "Yidong Zhao", "Qian Tao"], "title": "Physics-Informed Neural ODEs for Temporal Dynamics Modeling in Cardiac T1 Mapping", "categories": ["eess.IV", "cs.AI"], "comment": "Submitted version. Accepted at MICCAI 2025", "summary": "Spin-lattice relaxation time ($T_1$) is an important biomarker in cardiac\nparametric mapping for characterizing myocardial tissue and diagnosing\ncardiomyopathies. Conventional Modified Look-Locker Inversion Recovery (MOLLI)\nacquires 11 breath-hold baseline images with interleaved rest periods to ensure\nmapping accuracy. However, prolonged scanning can be challenging for patients\nwith poor breathholds, often leading to motion artifacts that degrade image\nquality. In addition, $T_1$ mapping requires voxel-wise nonlinear fitting to a\nsignal recovery model involving an iterative estimation process. Recent studies\nhave proposed deep-learning approaches for rapid $T_1$ mapping using shortened\nsequences to reduce acquisition time for patient comfort. Nevertheless,\nexisting methods overlook important physics constraints, limiting\ninterpretability and generalization. In this work, we present an accelerated,\nend-to-end $T_1$ mapping framework leveraging Physics-Informed Neural Ordinary\nDifferential Equations (ODEs) to model temporal dynamics and address these\nchallenges. Our method achieves high-accuracy $T_1$ estimation from a sparse\nsubset of baseline images and ensures efficient null index estimation at test\ntime. Specifically, we develop a continuous-time LSTM-ODE model to enable\nselective Look-Locker (LL) data acquisition with arbitrary time lags.\nExperimental results show superior performance in $T_1$ estimation for both\nnative and post-contrast sequences and demonstrate the strong benefit of our\nphysics-based formulation over direct data-driven $T_1$ priors.", "AI": {"tldr": "提出了一种基于物理约束的深度学习框架，用于加速心脏T1映射，减少扫描时间并提高准确性。", "motivation": "传统MOLLI方法扫描时间长且易受呼吸运动影响，现有深度学习方法忽视物理约束，限制了泛化能力。", "method": "利用物理信息神经ODE建模时间动态，开发连续时间LSTM-ODE模型，支持稀疏数据采集。", "result": "实验显示该方法在原生和对比后序列中均表现优越，物理约束显著优于纯数据驱动方法。", "conclusion": "该框架成功结合物理约束与深度学习，为T1映射提供了高效且准确的解决方案。"}}
{"id": "2507.00687", "pdf": "https://arxiv.org/pdf/2507.00687", "abs": "https://arxiv.org/abs/2507.00687", "authors": ["Philipp Vaeth", "Dibyanshu Kumar", "Benjamin Paassen", "Magda Gregorová"], "title": "Diffusion Classifier Guidance for Non-robust Classifiers", "categories": ["cs.LG", "cs.CV"], "comment": "Accepted at ECML 2025", "summary": "Classifier guidance is intended to steer a diffusion process such that a\ngiven classifier reliably recognizes the generated data point as a certain\nclass. However, most classifier guidance approaches are restricted to robust\nclassifiers, which were specifically trained on the noise of the diffusion\nforward process. We extend classifier guidance to work with general,\nnon-robust, classifiers that were trained without noise. We analyze the\nsensitivity of both non-robust and robust classifiers to noise of the diffusion\nprocess on the standard CelebA data set, the specialized SportBalls data set\nand the high-dimensional real-world CelebA-HQ data set. Our findings reveal\nthat non-robust classifiers exhibit significant accuracy degradation under\nnoisy conditions, leading to unstable guidance gradients. To mitigate these\nissues, we propose a method that utilizes one-step denoised image predictions\nand implements stabilization techniques inspired by stochastic optimization\nmethods, such as exponential moving averages. Experimental results demonstrate\nthat our approach improves the stability of classifier guidance while\nmaintaining sample diversity and visual quality. This work contributes to\nadvancing conditional sampling techniques in generative models, enabling a\nbroader range of classifiers to be used as guidance classifiers.", "AI": {"tldr": "论文提出了一种方法，使非鲁棒分类器也能用于扩散过程的分类器引导，解决了其在噪声条件下性能下降的问题。", "motivation": "扩展分类器引导的应用范围，使其不仅限于鲁棒分类器，而是适用于更广泛的非鲁棒分类器。", "method": "利用一步去噪图像预测，并结合随机优化方法（如指数移动平均）来稳定引导梯度。", "result": "实验表明，该方法提高了分类器引导的稳定性，同时保持了样本多样性和视觉质量。", "conclusion": "该研究推动了生成模型中条件采样技术的发展，使更多类型的分类器可用作引导分类器。"}}
{"id": "2507.00631", "pdf": "https://arxiv.org/pdf/2507.00631", "abs": "https://arxiv.org/abs/2507.00631", "authors": ["David Shi", "Kevin Joo"], "title": "Horus: A Protocol for Trustless Delegation Under Uncertainty", "categories": ["cs.GT", "cs.AI", "cs.MA", "I.2.11; F.2.2"], "comment": "9 pages, 1 figure", "summary": "Correctness is an emergent property of systems where exposing error is\ncheaper than committing it. In dynamic, low-trust environments, autonomous AI\nagents benefit from delegating work to sub-agents, yet correctness cannot be\nassured through upfront specification or centralized oversight. We propose a\nprotocol that enforces correctness through collateralized claims in a recursive\nverification game. Tasks are published as intents, and solvers compete to\nfulfill them. Selected solvers carry out tasks under risk, with correctness\nchecked post hoc by verifiers. Any challenger can challenge a result by staking\nagainst it to trigger the verification process. Incorrect agents are slashed\nand correct opposition is rewarded, with an escalation path that penalizes\nerroneous verifiers themselves. When incentives are aligned across solvers,\nchallengers, and verifiers, falsification conditions make correctness the Nash\nequilibrium.", "AI": {"tldr": "提出一种通过抵押索赔和递归验证游戏确保AI代理正确性的协议。", "motivation": "在动态、低信任环境中，无法通过预先规范或集中监督确保AI代理的正确性。", "method": "任务发布为意图，解决者竞争完成，验证者事后检查正确性，挑战者可抵押触发验证。", "result": "激励对齐时，正确性成为纳什均衡。", "conclusion": "该协议通过经济激励确保AI代理的正确性。"}}
{"id": "2507.00743", "pdf": "https://arxiv.org/pdf/2507.00743", "abs": "https://arxiv.org/abs/2507.00743", "authors": ["An Le", "Nehal Mehta", "William Freeman", "Ines Nagel", "Melanie Tran", "Anna Heinke", "Akshay Agnihotri", "Lingyun Cheng", "Dirk-Uwe Bartsch", "Hung Nguyen", "Truong Nguyen", "Cheolhong An"], "title": "Tunable Wavelet Unit based Convolutional Neural Network in Optical Coherence Tomography Analysis Enhancement for Classifying Type of Epiretinal Membrane Surgery", "categories": ["eess.IV", "cs.CV", "eess.SP"], "comment": null, "summary": "In this study, we developed deep learning-based method to classify the type\nof surgery performed for epiretinal membrane (ERM) removal, either internal\nlimiting membrane (ILM) removal or ERM-alone removal. Our model, based on the\nResNet18 convolutional neural network (CNN) architecture, utilizes\npostoperative optical coherence tomography (OCT) center scans as inputs. We\nevaluated the model using both original scans and scans preprocessed with\nenergy crop and wavelet denoising, achieving 72% accuracy on preprocessed\ninputs, outperforming the 66% accuracy achieved on original scans. To further\nimprove accuracy, we integrated tunable wavelet units with two key adaptations:\nOrthogonal Lattice-based Wavelet Units (OrthLatt-UwU) and Perfect\nReconstruction Relaxation-based Wavelet Units (PR-Relax-UwU). These units\nallowed the model to automatically adjust filter coefficients during training\nand were incorporated into downsampling, stride-two convolution, and pooling\nlayers, enhancing its ability to distinguish between ERM-ILM removal and\nERM-alone removal, with OrthLattUwU boosting accuracy to 76% and PR-Relax-UwU\nincreasing performance to 78%. Performance comparisons showed that our AI model\noutperformed a trained human grader, who achieved only 50% accuracy in\nclassifying the removal surgery types from postoperative OCT scans. These\nfindings highlight the potential of CNN based models to improve clinical\ndecision-making by providing more accurate and reliable classifications. To the\nbest of our knowledge, this is the first work to employ tunable wavelets for\nclassifying different types of ERM removal surgery.", "AI": {"tldr": "使用基于ResNet18的CNN模型，结合可调小波单元，对术后OCT扫描进行分类，以区分ERM-ILM和ERM-alone手术类型，模型性能优于人类评分者。", "motivation": "提高术后OCT扫描中对不同类型ERM手术的分类准确性，辅助临床决策。", "method": "采用ResNet18 CNN架构，结合能量裁剪和小波去噪预处理，并引入可调小波单元（OrthLatt-UwU和PR-Relax-UwU）优化模型。", "result": "预处理后输入准确率达72%，可调小波单元进一步提升至76%和78%，优于人类评分者的50%。", "conclusion": "CNN模型结合可调小波单元在ERM手术分类中表现出潜力，为临床提供更可靠的分类结果。"}}
{"id": "2507.00653", "pdf": "https://arxiv.org/pdf/2507.00653", "abs": "https://arxiv.org/abs/2507.00653", "authors": ["Yilun Zhang"], "title": "Cognitive Load-Aware Inference: A Neuro-Symbolic Framework for Optimizing the Token Economy of Large Language Models", "categories": ["cs.LG", "cs.AI"], "comment": "23 pages", "summary": "The escalating computational costs of Large Language Model (LLM) inference\nhave become a critical barrier to their widespread and sustainable deployment.\nWhile existing optimization strategies are effective, they are predominantly\nbased on statistical heuristics or architectural modifications, lacking a\nguiding cognitive theory to manage the inference process itself. This paper\naims to bridge this gap by introducing a novel paradigm: the Cognitive\nLoad-Aware Inference (CLAI) framework, which operationalizes principles from\nCognitive Load Theory (CLT) and neuroscience for LLM inference. We formalize\nthe concepts of Intrinsic Cognitive Load, Extraneous Cognitive Load, and\nGermane Cognitive Load into quantifiable LLM metrics ($ICL_{LLM}$, $ECL_{LLM}$,\nand $GCL_{LLM}$), thereby reframing the inference process as a cognitive\neconomics optimization problem: based on the intrinsic complexity of a problem\n($ICL_{LLM}$), minimize wasteful computation ($ECL_{LLM}$), and strategically\nallocate the token budget to productive reasoning ($GCL_{LLM}$). We propose two\nimplementation paths: CLAI-Prompt, a zero-shot method that guides a base LLM\nthrough cognitive control steps via a structured meta-prompt, and CLAI-Tune, a\nfine-tuned model that internalizes these principles for spontaneous cognitive\neconomy. Across a range of benchmarks in complex reasoning, long-context\nquestion answering, and code generation, our methods achieve significant\nreductions in token consumption (up to 45\\%) without sacrificing accuracy.\nFurthermore, CLAI-Tune exhibits an emergent ability to autonomously decompose\ndifficult problems, a key characteristic of human expert cognition. This work\ndemonstrates that by emulating the brain's resource management strategies, we\ncan build more efficient, robust, and capable artificial intelligence systems.", "AI": {"tldr": "论文提出了一种基于认知负荷理论（CLT）的框架CLAI，通过量化内在、外在和关联认知负荷来优化LLM推理过程，显著减少计算资源消耗。", "motivation": "当前LLM推理的高计算成本阻碍了其广泛应用，现有方法缺乏理论指导，需要一种基于认知理论的优化方法。", "method": "提出CLAI框架，将认知负荷理论量化并应用于LLM推理，包括CLAI-Prompt（零样本方法）和CLAI-Tune（微调模型）。", "result": "在复杂推理、长文本问答和代码生成任务中，CLAI方法显著减少token消耗（最高45%），且不损失准确性。", "conclusion": "通过模拟人脑资源管理策略，CLAI框架为构建更高效、鲁棒和智能的AI系统提供了新思路。"}}
{"id": "2507.00780", "pdf": "https://arxiv.org/pdf/2507.00780", "abs": "https://arxiv.org/abs/2507.00780", "authors": ["Fei Yuhuan", "Sun Xufei", "Zang Ran", "Wang Gengchen", "Su Meng", "Liu Fenghao"], "title": "Research on Improving the High Precision and Lightweight Diabetic Retinopathy Detection of YOLOv8n", "categories": ["eess.IV", "cs.CV"], "comment": "in Chinese language", "summary": "Early detection and diagnosis of diabetic retinopathy is one of the current\nresearch focuses in ophthalmology. However, due to the subtle features of\nmicro-lesions and their susceptibility to background interference, ex-isting\ndetection methods still face many challenges in terms of accuracy and\nrobustness. To address these issues, a lightweight and high-precision detection\nmodel based on the improved YOLOv8n, named YOLO-KFG, is proposed. Firstly, a\nnew dynamic convolution KWConv and C2f-KW module are designed to improve the\nbackbone network, enhancing the model's ability to perceive micro-lesions.\nSecondly, a fea-ture-focused diffusion pyramid network FDPN is designed to\nfully integrate multi-scale context information, further improving the model's\nability to perceive micro-lesions. Finally, a lightweight shared detection head\nGSDHead is designed to reduce the model's parameter count, making it more\ndeployable on re-source-constrained devices. Experimental results show that\ncompared with the base model YOLOv8n, the improved model reduces the parameter\ncount by 20.7%, increases mAP@0.5 by 4.1%, and improves the recall rate by\n7.9%. Compared with single-stage mainstream algorithms such as YOLOv5n and\nYOLOv10n, YOLO-KFG demonstrates significant advantages in both detection\naccuracy and efficiency.", "AI": {"tldr": "提出了一种基于改进YOLOv8n的轻量高精度检测模型YOLO-KFG，用于糖尿病视网膜病变的早期检测，显著提升了检测精度和效率。", "motivation": "糖尿病视网膜病变的微病变特征细微且易受背景干扰，现有检测方法在准确性和鲁棒性方面面临挑战。", "method": "设计了动态卷积KWConv和C2f-KW模块改进主干网络，提出特征聚焦扩散金字塔网络FDPN整合多尺度信息，并设计轻量共享检测头GSDHead减少参数。", "result": "改进模型参数减少20.7%，mAP@0.5提升4.1%，召回率提高7.9%，优于YOLOv5n和YOLOv10n等主流算法。", "conclusion": "YOLO-KFG在检测精度和效率上具有显著优势，适用于资源受限设备。"}}
{"id": "2507.00657", "pdf": "https://arxiv.org/pdf/2507.00657", "abs": "https://arxiv.org/abs/2507.00657", "authors": ["Jacopo Nudo", "Mario Edoardo Pandolfo", "Edoardo Loru", "Mattia Samory", "Matteo Cinelli", "Walter Quattrociocchi"], "title": "Generative Exaggeration in LLM Social Agents: Consistency, Bias, and Toxicity", "categories": ["cs.HC", "cs.AI", "cs.SI"], "comment": null, "summary": "We investigate how Large Language Models (LLMs) behave when simulating\npolitical discourse on social media. Leveraging 21 million interactions on X\nduring the 2024 U.S. presidential election, we construct LLM agents based on\n1,186 real users, prompting them to reply to politically salient tweets under\ncontrolled conditions. Agents are initialized either with minimal ideological\ncues (Zero Shot) or recent tweet history (Few Shot), allowing one-to-one\ncomparisons with human replies. We evaluate three model families (Gemini,\nMistral, and DeepSeek) across linguistic style, ideological consistency, and\ntoxicity. We find that richer contextualization improves internal consistency\nbut also amplifies polarization, stylized signals, and harmful language. We\nobserve an emergent distortion that we call \"generation exaggeration\": a\nsystematic amplification of salient traits beyond empirical baselines. Our\nanalysis shows that LLMs do not emulate users, they reconstruct them. Their\noutputs, indeed, reflect internal optimization dynamics more than observed\nbehavior, introducing structural biases that compromise their reliability as\nsocial proxies. This challenges their use in content moderation, deliberative\nsimulations, and policy modeling.", "AI": {"tldr": "研究探讨了大型语言模型（LLM）在模拟社交媒体政治话语时的行为，发现其输出更反映内部优化动态而非用户行为，导致结构性偏见。", "motivation": "探究LLM在模拟政治话语时的表现及其潜在影响，特别是在内容审核和政策建模中的应用。", "method": "基于2024年美国总统选举期间的2100万次X平台互动，构建了1186个真实用户的LLM代理，比较零样本和少样本初始化下的回复行为。", "result": "发现更丰富的上下文提高了内部一致性，但也加剧了极化、风格化信号和有害语言，并出现“生成夸张”现象。", "conclusion": "LLM的输出更依赖内部优化而非用户行为，其作为社会代理的可靠性受到挑战。"}}
{"id": "2507.00832", "pdf": "https://arxiv.org/pdf/2507.00832", "abs": "https://arxiv.org/abs/2507.00832", "authors": ["Jisoo Kim", "Chu-Hsuan Lin", "Alberto Ceballos-Arroyo", "Ping Liu", "Huaizu Jiang", "Shrikanth Yadav", "Qi Wan", "Lei Qin", "Geoffrey S Young"], "title": "Automated anatomy-based post-processing reduces false positives and improved interpretability of deep learning intracranial aneurysm detection", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": null, "summary": "Introduction: Deep learning (DL) models can help detect intracranial\naneurysms on CTA, but high false positive (FP) rates remain a barrier to\nclinical translation, despite improvement in model architectures and strategies\nlike detection threshold tuning. We employed an automated, anatomy-based,\nheuristic-learning hybrid artery-vein segmentation post-processing method to\nfurther reduce FPs. Methods: Two DL models, CPM-Net and a deformable 3D\nconvolutional neural network-transformer hybrid (3D-CNN-TR), were trained with\n1,186 open-source CTAs (1,373 annotated aneurysms), and evaluated with 143\nheld-out private CTAs (218 annotated aneurysms). Brain, artery, vein, and\ncavernous venous sinus (CVS) segmentation masks were applied to remove possible\nFPs in the DL outputs that overlapped with: (1) brain mask; (2) vein mask; (3)\nvein more than artery masks; (4) brain plus vein mask; (5) brain plus vein more\nthan artery masks. Results: CPM-Net yielded 139 true-positives (TP); 79\nfalse-negative (FN); 126 FP. 3D-CNN-TR yielded 179 TP; 39 FN; 182 FP. FPs were\ncommonly extracranial (CPM-Net 27.3%; 3D-CNN-TR 42.3%), venous (CPM-Net 56.3%;\n3D-CNN-TR 29.1%), arterial (CPM-Net 11.9%; 3D-CNN-TR 53.3%), and non-vascular\n(CPM-Net 25.4%; 3D-CNN-TR 9.3%) structures. Method 5 performed best, reducing\nCPM-Net FP by 70.6% (89/126) and 3D-CNN-TR FP by 51.6% (94/182), without\nreducing TP, lowering the FP/case rate from 0.88 to 0.26 for CPM-NET, and from\n1.27 to 0.62 for the 3D-CNN-TR. Conclusion: Anatomy-based, interpretable\npost-processing can improve DL-based aneurysm detection model performance. More\nbroadly, automated, domain-informed, hybrid heuristic-learning processing holds\npromise for improving the performance and clinical acceptance of aneurysm\ndetection models.", "AI": {"tldr": "该研究提出了一种基于解剖学的后处理方法，结合深度学习模型，显著降低了颅内动脉瘤检测的假阳性率。", "motivation": "尽管深度学习模型在颅内动脉瘤检测上有所改进，但高假阳性率仍是临床应用的障碍。研究旨在通过解剖学后处理进一步减少假阳性。", "method": "使用两种深度学习模型（CPM-Net和3D-CNN-TR）训练开源CTA数据，并应用基于解剖学的后处理（如脑、动脉、静脉分割掩码）去除假阳性。", "result": "后处理方法显著降低了假阳性率（CPM-Net降低70.6%，3D-CNN-TR降低51.6%），且未减少真阳性。", "conclusion": "基于解剖学的后处理可提升深度学习模型的性能，为动脉瘤检测的临床应用提供了潜力。"}}
{"id": "2507.00903", "pdf": "https://arxiv.org/pdf/2507.00903", "abs": "https://arxiv.org/abs/2507.00903", "authors": ["Andreea Bianca Popescu", "Andreas Seitz", "Heiko Mahrholdt", "Jens Wetzl", "Athira Jacob", "Lucian Mihai Itu", "Constantin Suciu", "Teodora Chitiboi"], "title": "Deep learning-based segmentation of T1 and T2 cardiac MRI maps for automated disease detection", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "This work has been submitted for consideration at European Radiology\n  (Springer). Upon acceptance, this preprint will be updated with the journal\n  reference", "summary": "Objectives Parametric tissue mapping enables quantitative cardiac tissue\ncharacterization but is limited by inter-observer variability during manual\ndelineation. Traditional approaches relying on average relaxation values and\nsingle cutoffs may oversimplify myocardial complexity. This study evaluates\nwhether deep learning (DL) can achieve segmentation accuracy comparable to\ninter-observer variability, explores the utility of statistical features beyond\nmean T1/T2 values, and assesses whether machine learning (ML) combining\nmultiple features enhances disease detection. Materials & Methods T1 and T2\nmaps were manually segmented. The test subset was independently annotated by\ntwo observers, and inter-observer variability was assessed. A DL model was\ntrained to segment left ventricle blood pool and myocardium. Average (A), lower\nquartile (LQ), median (M), and upper quartile (UQ) were computed for the\nmyocardial pixels and employed in classification by applying cutoffs or in ML.\nDice similarity coefficient (DICE) and mean absolute percentage error evaluated\nsegmentation performance. Bland-Altman plots assessed inter-user and\nmodel-observer agreement. Receiver operating characteristic analysis determined\noptimal cutoffs. Pearson correlation compared features from model and manual\nsegmentations. F1-score, precision, and recall evaluated classification\nperformance. Wilcoxon test assessed differences between classification methods,\nwith p < 0.05 considered statistically significant. Results 144 subjects were\nsplit into training (100), validation (15) and evaluation (29) subsets.\nSegmentation model achieved a DICE of 85.4%, surpassing inter-observer\nagreement. Random forest applied to all features increased F1-score (92.7%, p <\n0.001). Conclusion DL facilitates segmentation of T1/ T2 maps. Combining\nmultiple features with ML improves disease detection.", "AI": {"tldr": "深度学习（DL）在心脏组织分割中表现优于人工标注，结合多特征的机器学习（ML）提升了疾病检测能力。", "motivation": "解决人工标注的观察者间变异性问题，并探索深度学习在心脏组织定量分析中的应用潜力。", "method": "使用DL模型分割心脏组织，结合T1/T2图的统计特征（如均值、四分位数等），并通过ML进行分类。", "result": "DL分割的DICE系数达85.4%，优于观察者间一致性；ML结合多特征显著提升F1分数至92.7%。", "conclusion": "DL在心脏组织分割中表现优异，ML结合多特征可显著提升疾病检测能力。"}}
{"id": "2507.00937", "pdf": "https://arxiv.org/pdf/2507.00937", "abs": "https://arxiv.org/abs/2507.00937", "authors": ["David Hunt", "Shaocheng Luo", "Spencer Hallyburton", "Shafii Nillongo", "Yi Li", "Tingjun Chen", "Miroslav Pajic"], "title": "RaGNNarok: A Light-Weight Graph Neural Network for Enhancing Radar Point Clouds on Unmanned Ground Vehicles", "categories": ["cs.RO", "cs.AR", "cs.CV", "cs.LG"], "comment": "8 pages, accepted by IROS 2025", "summary": "Low-cost indoor mobile robots have gained popularity with the increasing\nadoption of automation in homes and commercial spaces. However, existing lidar\nand camera-based solutions have limitations such as poor performance in\nvisually obscured environments, high computational overhead for data\nprocessing, and high costs for lidars. In contrast, mmWave radar sensors offer\na cost-effective and lightweight alternative, providing accurate ranging\nregardless of visibility. However, existing radar-based localization suffers\nfrom sparse point cloud generation, noise, and false detections. Thus, in this\nwork, we introduce RaGNNarok, a real-time, lightweight, and generalizable graph\nneural network (GNN)-based framework to enhance radar point clouds, even in\ncomplex and dynamic environments. With an inference time of just 7.3 ms on the\nlow-cost Raspberry Pi 5, RaGNNarok runs efficiently even on such\nresource-constrained devices, requiring no additional computational resources.\nWe evaluate its performance across key tasks, including localization, SLAM, and\nautonomous navigation, in three different environments. Our results demonstrate\nstrong reliability and generalizability, making RaGNNarok a robust solution for\nlow-cost indoor mobile robots.", "AI": {"tldr": "RaGNNarok是一个基于图神经网络的轻量级框架，用于增强雷达点云，适用于低成本室内移动机器人。", "motivation": "现有基于激光雷达和摄像头的解决方案在视觉遮挡环境中性能不佳，计算开销大且成本高，而毫米波雷达传感器虽成本低但存在点云稀疏、噪声和误检问题。", "method": "提出RaGNNarok框架，利用图神经网络实时增强雷达点云，适用于复杂动态环境，并在低成本设备上高效运行。", "result": "在三种不同环境中测试了定位、SLAM和自主导航任务，表现出高可靠性和泛化能力，推理时间仅7.3毫秒。", "conclusion": "RaGNNarok为低成本室内移动机器人提供了一种鲁棒的解决方案。"}}
{"id": "2507.00983", "pdf": "https://arxiv.org/pdf/2507.00983", "abs": "https://arxiv.org/abs/2507.00983", "authors": ["Sara Yavari", "Rahul Nitin Pandya", "Jacob Furst"], "title": "DMCIE: Diffusion Model with Concatenation of Inputs and Errors to Improve the Accuracy of the Segmentation of Brain Tumors in MRI Images", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Accurate segmentation of brain tumors in MRI scans is essential for reliable\nclinical diagnosis and effective treatment planning. Recently, diffusion models\nhave demonstrated remarkable effectiveness in image generation and segmentation\ntasks. This paper introduces a novel approach to corrective segmentation based\non diffusion models. We propose DMCIE (Diffusion Model with Concatenation of\nInputs and Errors), a novel framework for accurate brain tumor segmentation in\nmulti-modal MRI scans. We employ a 3D U-Net to generate an initial segmentation\nmask, from which an error map is generated by identifying the differences\nbetween the prediction and the ground truth. The error map, concatenated with\nthe original MRI images, are used to guide a diffusion model. Using multimodal\nMRI inputs (T1, T1ce, T2, FLAIR), DMCIE effectively enhances segmentation\naccuracy by focusing on misclassified regions, guided by the original inputs.\nEvaluated on the BraTS2020 dataset, DMCIE outperforms several state-of-the-art\ndiffusion-based segmentation methods, achieving a Dice Score of 93.46 and an\nHD95 of 5.94 mm. These results highlight the effectiveness of error-guided\ndiffusion in producing precise and reliable brain tumor segmentations.", "AI": {"tldr": "本文提出了一种基于扩散模型的脑肿瘤分割新方法DMCIE，通过结合输入图像和误差图，显著提高了分割精度。", "motivation": "准确的脑肿瘤分割对临床诊断和治疗至关重要，扩散模型在图像生成和分割任务中表现出色。", "method": "使用3D U-Net生成初始分割掩码，提取误差图后与原始MRI图像结合，通过扩散模型优化分割。", "result": "在BraTS2020数据集上，DMCIE的Dice Score达93.46，HD95为5.94 mm，优于现有方法。", "conclusion": "DMCIE通过误差引导扩散模型，实现了高精度和可靠的脑肿瘤分割。"}}
{"id": "2507.00984", "pdf": "https://arxiv.org/pdf/2507.00984", "abs": "https://arxiv.org/abs/2507.00984", "authors": ["Xihang Yu", "Rajat Talak", "Jingnan Shi", "Ulrich Viereck", "Igor Gilitschenski", "Luca Carlone"], "title": "Box Pose and Shape Estimation and Domain Adaptation for Large-Scale Warehouse Automation", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "12 pages, 6 figures. This work will be presented at the 19th\n  International Symposium on Experimental Robotics (ISER2025)", "summary": "Modern warehouse automation systems rely on fleets of intelligent robots that\ngenerate vast amounts of data -- most of which remains unannotated. This paper\ndevelops a self-supervised domain adaptation pipeline that leverages\nreal-world, unlabeled data to improve perception models without requiring\nmanual annotations. Our work focuses specifically on estimating the pose and\nshape of boxes and presents a correct-and-certify pipeline for self-supervised\nbox pose and shape estimation. We extensively evaluate our approach across a\nrange of simulated and real industrial settings, including adaptation to a\nlarge-scale real-world dataset of 50,000 images. The self-supervised model\nsignificantly outperforms models trained solely in simulation and shows\nsubstantial improvements over a zero-shot 3D bounding box estimation baseline.", "AI": {"tldr": "本文提出了一种自监督领域适应方法，利用未标注数据提升仓库自动化系统中盒子姿态和形状的估计模型，显著优于模拟训练和零样本基线。", "motivation": "现代仓库自动化系统依赖智能机器人产生大量未标注数据，如何利用这些数据提升感知模型是一个关键问题。", "method": "开发了一种自监督领域适应流程，结合真实世界的未标注数据，提出了一种纠正和认证的自监督盒子姿态和形状估计方法。", "result": "在模拟和真实工业环境中广泛评估，自监督模型显著优于仅模拟训练的模型，并在50,000张真实图像数据集上表现优异。", "conclusion": "自监督方法能有效利用未标注数据提升模型性能，为仓库自动化系统提供了更高效的感知解决方案。"}}
{"id": "2507.00990", "pdf": "https://arxiv.org/pdf/2507.00990", "abs": "https://arxiv.org/abs/2507.00990", "authors": ["Shivansh Patel", "Shraddhaa Mohan", "Hanlin Mai", "Unnat Jain", "Svetlana Lazebnik", "Yunzhu Li"], "title": "Robotic Manipulation by Imitating Generated Videos Without Physical Demonstrations", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": "Project Page: https://rigvid-robot.github.io/", "summary": "This work introduces Robots Imitating Generated Videos (RIGVid), a system\nthat enables robots to perform complex manipulation tasks--such as pouring,\nwiping, and mixing--purely by imitating AI-generated videos, without requiring\nany physical demonstrations or robot-specific training. Given a language\ncommand and an initial scene image, a video diffusion model generates potential\ndemonstration videos, and a vision-language model (VLM) automatically filters\nout results that do not follow the command. A 6D pose tracker then extracts\nobject trajectories from the video, and the trajectories are retargeted to the\nrobot in an embodiment-agnostic fashion. Through extensive real-world\nevaluations, we show that filtered generated videos are as effective as real\ndemonstrations, and that performance improves with generation quality. We also\nshow that relying on generated videos outperforms more compact alternatives\nsuch as keypoint prediction using VLMs, and that strong 6D pose tracking\noutperforms other ways to extract trajectories, such as dense feature point\ntracking. These findings suggest that videos produced by a state-of-the-art\noff-the-shelf model can offer an effective source of supervision for robotic\nmanipulation.", "AI": {"tldr": "RIGVid系统通过模仿AI生成的视频，使机器人无需物理演示或机器人特定训练即可完成复杂任务。", "motivation": "解决机器人执行复杂任务时对物理演示或特定训练的依赖问题。", "method": "使用视频扩散模型生成演示视频，通过VLM筛选符合指令的视频，6D姿态跟踪器提取轨迹并适配机器人。", "result": "生成的视频效果与真实演示相当，性能随生成质量提升，优于关键点预测等方法。", "conclusion": "AI生成的视频可作为机器人操作的有效监督来源。"}}
{"id": "2507.00755", "pdf": "https://arxiv.org/pdf/2507.00755", "abs": "https://arxiv.org/abs/2507.00755", "authors": ["Jinhai Hu", "Zhongyi Zhang", "Cong Sheng Leow", "Wang Ling Goh", "Yuan Gao"], "title": "LearnAFE: Circuit-Algorithm Co-design Framework for Learnable Audio Analog Front-End", "categories": ["eess.AS", "cs.AI", "cs.SD"], "comment": "11 pages, 15 figures, accepted for publication on IEEE Transactions\n  on Circuits and Systems I: Regular Papers", "summary": "This paper presents a circuit-algorithm co-design framework for learnable\nanalog front-end (AFE) in audio signal classification. Designing AFE and\nbackend classifiers separately is a common practice but non-ideal, as shown in\nthis paper. Instead, this paper proposes a joint optimization of the backend\nclassifier with the AFE's transfer function to achieve system-level optimum.\nMore specifically, the transfer function parameters of an analog bandpass\nfilter (BPF) bank are tuned in a signal-to-noise ratio (SNR)-aware training\nloop for the classifier. Using a co-design loss function LBPF, this work shows\nsuperior optimization of both the filter bank and the classifier. Implemented\nin open-source SKY130 130nm CMOS process, the optimized design achieved\n90.5%-94.2% accuracy for 10-keyword classification task across a wide range of\ninput signal SNR from 5 dB to 20 dB, with only 22k classifier parameters.\nCompared to conventional approach, the proposed audio AFE achieves 8.7% and\n12.9% reduction in power and capacitor area respectively.", "AI": {"tldr": "论文提出了一种电路-算法协同设计框架，用于音频信号分类中的可学习模拟前端（AFE），通过联合优化后端分类器和AFE的传递函数实现系统级优化。", "motivation": "传统方法将AFE和后端分类器分开设计效果不理想，本文旨在通过联合优化提升性能。", "method": "采用SNR感知训练循环调整模拟带通滤波器（BPF）的传递函数参数，并使用协同设计损失函数LBPF优化滤波器组和分类器。", "result": "在SKY130 130nm CMOS工艺中实现，优化设计在5 dB至20 dB的输入信号SNR范围内，10关键词分类任务准确率达90.5%-94.2%，功耗和电容面积分别减少8.7%和12.9%。", "conclusion": "提出的协同设计方法显著优于传统方法，实现了系统级优化和资源节省。"}}
{"id": "2507.00993", "pdf": "https://arxiv.org/pdf/2507.00993", "abs": "https://arxiv.org/abs/2507.00993", "authors": ["Qingqiu Li", "Runtian Yuan", "Junlin Hou", "Jilan Xu", "Yuejie Zhang", "Rui Feng", "Hao Chen"], "title": "Advancing Lung Disease Diagnosis in 3D CT Scans", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "To enable more accurate diagnosis of lung disease in chest CT scans, we\npropose a straightforward yet effective model. Firstly, we analyze the\ncharacteristics of 3D CT scans and remove non-lung regions, which helps the\nmodel focus on lesion-related areas and reduces computational cost. We adopt\nResNeSt50 as a strong feature extractor, and use a weighted cross-entropy loss\nto mitigate class imbalance, especially for the underrepresented squamous cell\ncarcinoma category. Our model achieves a Macro F1 Score of 0.80 on the\nvalidation set of the Fair Disease Diagnosis Challenge, demonstrating its\nstrong performance in distinguishing between different lung conditions.", "AI": {"tldr": "提出了一种简单有效的模型，用于提高胸部CT扫描中肺部疾病诊断的准确性。通过分析3D CT特征并去除非肺部区域，模型能更专注于病灶区域并降低计算成本。采用ResNeSt50作为特征提取器，并使用加权交叉熵损失解决类别不平衡问题。在验证集上Macro F1 Score达到0.80。", "motivation": "提高胸部CT扫描中肺部疾病诊断的准确性，尤其是针对鳞状细胞癌等少数类别。", "method": "分析3D CT特征，去除非肺部区域；采用ResNeSt50作为特征提取器，使用加权交叉熵损失解决类别不平衡。", "result": "在Fair Disease Diagnosis Challenge验证集上Macro F1 Score为0.80。", "conclusion": "模型在区分不同肺部疾病方面表现出色，尤其适用于类别不平衡的场景。"}}
{"id": "2507.01016", "pdf": "https://arxiv.org/pdf/2507.01016", "abs": "https://arxiv.org/abs/2507.01016", "authors": ["Yating Wang", "Haoyi Zhu", "Mingyu Liu", "Jiange Yang", "Hao-Shu Fang", "Tong He"], "title": "VQ-VLA: Improving Vision-Language-Action Models via Scaling Vector-Quantized Action Tokenizers", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "In this paper, we introduce an innovative vector quantization based action\ntokenizer built upon the largest-scale action trajectory dataset to date,\nleveraging over 100 times more data than previous approaches. This extensive\ndataset enables our tokenizer to capture rich spatiotemporal dynamics,\nresulting in a model that not only accelerates inference but also generates\nsmoother and more coherent action outputs. Once trained, the tokenizer can be\nseamlessly adapted to a wide range of downstream tasks in a zero-shot manner,\nfrom short-horizon reactive behaviors to long-horizon planning. A key finding\nof our work is that the domain gap between synthetic and real action\ntrajectories is marginal, allowing us to effectively utilize a vast amount of\nsynthetic data during training without compromising real-world performance. To\nvalidate our approach, we conducted extensive experiments in both simulated\nenvironments and on real robotic platforms. The results demonstrate that as the\nvolume of synthetic trajectory data increases, the performance of our tokenizer\non downstream tasks improves significantly-most notably, achieving up to a 30%\nhigher success rate on two real-world tasks in long-horizon scenarios. These\nfindings highlight the potential of our action tokenizer as a robust and\nscalable solution for real-time embodied intelligence systems, paving the way\nfor more efficient and reliable robotic control in diverse application\ndomains.Project website: https://xiaoxiao0406.github.io/vqvla.github.io", "AI": {"tldr": "本文提出了一种基于大规模动作轨迹数据集的向量量化动作分词器，显著提升了动作输出的流畅性和一致性，并在零样本任务中表现出色。", "motivation": "利用比以往方法多100倍的数据，捕捉丰富的时空动态，以提升动作分词的性能和适应性。", "method": "基于向量量化的动作分词器，利用合成数据进行训练，并在模拟和真实机器人平台上验证。", "result": "随着合成数据量的增加，分词器在下游任务中的性能显著提升，真实任务成功率最高提升30%。", "conclusion": "该动作分词器为实时智能系统提供了可扩展的解决方案，为高效可靠的机器人控制铺平了道路。"}}
{"id": "2507.00788", "pdf": "https://arxiv.org/pdf/2507.00788", "abs": "https://arxiv.org/abs/2507.00788", "authors": ["Markus Borg", "Dave Hewett", "Nadim Hagatulah", "Noric Couderc", "Emma Söderberg", "Donald Graham", "Uttam Kini", "Dave Farley"], "title": "Echoes of AI: Investigating the Downstream Effects of AI Assistants on Software Maintainability", "categories": ["cs.SE", "cs.AI"], "comment": "Preprint of study preregistered at ICSME 2025 with In-Principal\n  Acceptance.\n  https://conf.researchr.org/track/icsme-2024/icsme-2024-registered-reports-track", "summary": "[Context] AI assistants, like GitHub Copilot and Cursor, are transforming\nsoftware engineering. While several studies highlight productivity\nimprovements, their impact on maintainability requires further investigation.\n[Objective] This study investigates whether co-development with AI assistants\naffects software maintainability, specifically how easily other developers can\nevolve the resulting source code. [Method] We conducted a two-phase controlled\nexperiment involving 151 participants, 95% of whom were professional\ndevelopers. In Phase 1, participants added a new feature to a Java web\napplication, with or without AI assistance. In Phase 2, a randomized controlled\ntrial, new participants evolved these solutions without AI assistance.\n[Results] AI-assisted development in Phase 1 led to a modest speedup in\nsubsequent evolution and slightly higher average CodeHealth. Although neither\ndifference was significant overall, the increase in CodeHealth was\nstatistically significant when habitual AI users completed Phase 1. For Phase\n1, we also observed a significant effect that corroborates previous\nproductivity findings: using an AI assistant yielded a 30.7% median decrease in\ntask completion time. Moreover, for habitual AI users, the mean speedup was\n55.9%. [Conclusions] Our study adds to the growing evidence that AI assistants\ncan effectively accelerate development. Moreover, we did not observe warning\nsigns of degraded code-level maintainability. We recommend that future research\nfocus on risks such as code bloat from excessive code generation and the\nbuild-up of cognitive debt as developers invest less mental effort during\nimplementation.", "AI": {"tldr": "研究探讨AI助手对代码可维护性的影响，发现AI辅助开发能提升速度和代码健康度，但未显著降低可维护性。", "motivation": "尽管AI助手如GitHub Copilot和Cursor提高了开发效率，但其对代码可维护性的影响尚不明确，需进一步研究。", "method": "通过两阶段实验，151名参与者（95%为专业开发者）分别在有/无AI辅助下开发功能，随后由其他参与者维护代码。", "result": "AI辅助开发显著提升开发速度（中位减少30.7%时间），但对可维护性影响有限，仅习惯性用户代码健康度显著提升。", "conclusion": "AI助手能有效加速开发且未损害可维护性，未来需关注代码膨胀和认知债务等潜在风险。"}}
{"id": "2507.00816", "pdf": "https://arxiv.org/pdf/2507.00816", "abs": "https://arxiv.org/abs/2507.00816", "authors": ["Mengyun Wang", "Bo Wang", "Yifeng Niu", "Chang Wang"], "title": "PI-WAN: A Physics-Informed Wind-Adaptive Network for Quadrotor Dynamics Prediction in Unknown Environments", "categories": ["cs.RO", "cs.AI"], "comment": null, "summary": "Accurate dynamics modeling is essential for quadrotors to achieve precise\ntrajectory tracking in various applications. Traditional physical\nknowledge-driven modeling methods face substantial limitations in unknown\nenvironments characterized by variable payloads, wind disturbances, and\nexternal perturbations. On the other hand, data-driven modeling methods suffer\nfrom poor generalization when handling out-of-distribution (OoD) data,\nrestricting their effectiveness in unknown scenarios. To address these\nchallenges, we introduce the Physics-Informed Wind-Adaptive Network (PI-WAN),\nwhich combines knowledge-driven and data-driven modeling methods by embedding\nphysical constraints directly into the training process for robust quadrotor\ndynamics learning. Specifically, PI-WAN employs a Temporal Convolutional\nNetwork (TCN) architecture that efficiently captures temporal dependencies from\nhistorical flight data, while a physics-informed loss function applies physical\nprinciples to improve model generalization and robustness across previously\nunseen conditions. By incorporating real-time prediction results into a model\npredictive control (MPC) framework, we achieve improvements in closed-loop\ntracking performance. Comprehensive simulations and real-world flight\nexperiments demonstrate that our approach outperforms baseline methods in terms\nof prediction accuracy, tracking precision, and robustness to unknown\nenvironments.", "AI": {"tldr": "论文提出PI-WAN方法，结合物理知识与数据驱动建模，提升四旋翼飞行器在未知环境中的轨迹跟踪性能。", "motivation": "传统物理建模方法在未知环境中受限，数据驱动方法泛化能力差，需结合两者以提升性能。", "method": "PI-WAN结合Temporal Convolutional Network和物理约束损失函数，嵌入MPC框架。", "result": "仿真和实验表明，PI-WAN在预测精度、跟踪性能和鲁棒性上优于基线方法。", "conclusion": "PI-WAN有效结合物理与数据驱动方法，显著提升四旋翼在未知环境中的性能。"}}
{"id": "2507.00833", "pdf": "https://arxiv.org/pdf/2507.00833", "abs": "https://arxiv.org/abs/2507.00833", "authors": ["Zhi Jing", "Siyuan Yang", "Jicong Ao", "Ting Xiao", "Yugang Jiang", "Chenjia Bai"], "title": "HumanoidGen: Data Generation for Bimanual Dexterous Manipulation via LLM Reasoning", "categories": ["cs.RO", "cs.AI"], "comment": "Project Page: https://openhumanoidgen.github.io", "summary": "For robotic manipulation, existing robotics datasets and simulation\nbenchmarks predominantly cater to robot-arm platforms. However, for humanoid\nrobots equipped with dual arms and dexterous hands, simulation tasks and\nhigh-quality demonstrations are notably lacking. Bimanual dexterous\nmanipulation is inherently more complex, as it requires coordinated arm\nmovements and hand operations, making autonomous data collection challenging.\nThis paper presents HumanoidGen, an automated task creation and demonstration\ncollection framework that leverages atomic dexterous operations and LLM\nreasoning to generate relational constraints. Specifically, we provide spatial\nannotations for both assets and dexterous hands based on the atomic operations,\nand perform an LLM planner to generate a chain of actionable spatial\nconstraints for arm movements based on object affordances and scenes. To\nfurther improve planning ability, we employ a variant of Monte Carlo tree\nsearch to enhance LLM reasoning for long-horizon tasks and insufficient\nannotation. In experiments, we create a novel benchmark with augmented\nscenarios to evaluate the quality of the collected data. The results show that\nthe performance of the 2D and 3D diffusion policies can scale with the\ngenerated dataset. Project page is https://openhumanoidgen.github.io.", "AI": {"tldr": "HumanoidGen是一个自动化任务创建和演示收集框架，专注于双手机器人操作，利用原子操作和LLM推理生成空间约束。", "motivation": "现有机器人数据集和仿真基准主要针对单臂机器人，缺乏针对双手机器人的高质量演示和任务。", "method": "通过原子操作和LLM规划生成空间约束，并结合蒙特卡洛树搜索优化长时程任务。", "result": "实验表明，生成的2D和3D扩散策略性能随数据集规模提升。", "conclusion": "HumanoidGen填补了双手机器人操作任务的空白，为相关研究提供了新基准。"}}
{"id": "2507.00880", "pdf": "https://arxiv.org/pdf/2507.00880", "abs": "https://arxiv.org/abs/2507.00880", "authors": ["Ruihan Xu", "Haokui Zhang", "Yaowei Wang", "Wei Zeng", "Shiliang Zhang"], "title": "NN-Former: Rethinking Graph Structure in Neural Architecture Representation", "categories": ["cs.LG", "cs.AI"], "comment": "Accepted to CVPR 2025. Code is avaiable at\n  https://github.com/XuRuihan/NNFormer", "summary": "The growing use of deep learning necessitates efficient network design and\ndeployment, making neural predictors vital for estimating attributes such as\naccuracy and latency. Recently, Graph Neural Networks (GNNs) and transformers\nhave shown promising performance in representing neural architectures. However,\neach of both methods has its disadvantages. GNNs lack the capabilities to\nrepresent complicated features, while transformers face poor generalization\nwhen the depth of architecture grows. To mitigate the above issues, we rethink\nneural architecture topology and show that sibling nodes are pivotal while\noverlooked in previous research. We thus propose a novel predictor leveraging\nthe strengths of GNNs and transformers to learn the enhanced topology. We\nintroduce a novel token mixer that considers siblings, and a new channel mixer\nnamed bidirectional graph isomorphism feed-forward network. Our approach\nconsistently achieves promising performance in both accuracy and latency\nprediction, providing valuable insights for learning Directed Acyclic Graph\n(DAG) topology. The code is available at https://github.com/XuRuihan/NNFormer.", "AI": {"tldr": "论文提出了一种结合GNN和transformer优势的新预测器，通过改进拓扑表示提升神经网络架构的准确性和延迟预测性能。", "motivation": "现有GNN和transformer在表示神经网络架构时各有不足，GNN难以捕捉复杂特征，transformer在架构深度增加时泛化能力差。", "method": "提出新预测器，结合GNN和transformer，引入考虑兄弟节点的新token mixer和双向图同构前馈网络。", "result": "方法在准确性和延迟预测上表现优异，为学习DAG拓扑提供了新见解。", "conclusion": "通过改进拓扑表示，新方法有效解决了GNN和transformer的局限性，提升了预测性能。"}}
{"id": "2507.00902", "pdf": "https://arxiv.org/pdf/2507.00902", "abs": "https://arxiv.org/abs/2507.00902", "authors": ["Feng Wang", "Shengyu Zhang", "Een-Kee Hong", "Tony Q. S. Quek"], "title": "Constellation as a Service: Tailored Connectivity Management in Direct-Satellite-to-Device Networks", "categories": ["eess.SY", "cs.AI", "cs.SY", "eess.SP"], "comment": "To appear in IEEE Communications Magazine", "summary": "Direct-satellite-to-device (DS2D) communication is emerging as a promising\nsolution for global mobile service extension, leveraging the deployment of\nsatellite constellations. However, the challenge of managing DS2D connectivity\nfor multi-constellations becomes outstanding, including high interference and\nfrequent handovers caused by multi-coverage overlap and rapid satellite\nmovement. Moreover, existing approaches primarily operate within\nsingle-constellation shell, which inherently limits the ability to exploit the\nvast potential of multi-constellation connectivity provision, resulting in\nsuboptimal DS2D service performances. To address these challenges, this article\nproposes a Constellation as a Service (CaaS) framework, which treats the entire\nmulti-constellation infrastructure as a shared resource pool and dynamically\nforms optimal sub-constellations (SCs) for each DS2D service region. The\nformation of each SC integrates satellites from various orbits to provide\ntailored connectivity based on user demands, guided by two innovative\nstrategies: predictive satellite beamforming using generative artificial\nintelligence (GenAI) and pre-configured handover path for efficient satellite\naccess and mobility management. Simulation results demonstrate that CaaS\nsignificantly improves satellite service rates while reducing handover\noverhead, making it an efficient and continuable solution for managing DS2D\nconnectivity in multi-constellation environments.", "AI": {"tldr": "提出了一种名为CaaS的框架，通过动态组网和AI预测优化多星座DS2D通信，提升服务率并减少切换开销。", "motivation": "解决多星座DS2D通信中的高干扰和频繁切换问题，现有单星座方法无法充分利用多星座潜力。", "method": "提出CaaS框架，动态形成最优子星座，结合预测性波束成形和预配置切换路径。", "result": "仿真显示CaaS显著提升服务率并降低切换开销。", "conclusion": "CaaS是管理多星座DS2D通信的高效可持续解决方案。"}}
{"id": "2507.00907", "pdf": "https://arxiv.org/pdf/2507.00907", "abs": "https://arxiv.org/abs/2507.00907", "authors": ["Fabio Correa Xavier"], "title": "The Age of Sensorial Zero Trust: Why We Can No Longer Trust Our Senses", "categories": ["cs.CR", "cs.AI", "68T07, 68T45, 94A60", "K.6.5; D.4.6; I.2.6"], "comment": "14 pages", "summary": "In a world where deepfakes and cloned voices are emerging as sophisticated\nattack vectors, organizations require a new security mindset: Sensorial Zero\nTrust [9]. This article presents a scientific analysis of the need to\nsystematically doubt information perceived through the senses, establishing\nrigorous verification protocols to mitigate the risks of fraud based on\ngenerative artificial intelligence. Key concepts, such as Out-of-Band\nverification, Vision-Language Models (VLMs) as forensic collaborators,\ncryptographic provenance, and human training, are integrated into a framework\nthat extends Zero Trust principles to human sensory information. The approach\nis grounded in empirical findings and academic research, emphasizing that in an\nera of AI-generated realities, even our eyes and ears can no longer be\nimplicitly trusted without verification. Leaders are called to foster a culture\nof methodological skepticism to protect organizational integrity in this new\nthreat landscape.", "AI": {"tldr": "本文提出了一种新的安全理念“感官零信任”，通过系统性地怀疑感官信息，结合多种技术手段（如OOB验证、VLMs等）来应对深度伪造和克隆声音等威胁。", "motivation": "随着深度伪造和克隆声音等技术的成熟，传统的信任机制已不足以应对新型威胁，需要建立更严格的验证体系。", "method": "整合了OOB验证、视觉语言模型、加密来源和人类培训等概念，构建了一个扩展零信任原则的框架。", "result": "通过实证研究和学术支持，证明了在AI生成内容的时代，感官信息需要额外的验证。", "conclusion": "呼吁领导者培养方法论怀疑的文化，以保护组织在这种新型威胁环境中的完整性。"}}
{"id": "2507.00909", "pdf": "https://arxiv.org/pdf/2507.00909", "abs": "https://arxiv.org/abs/2507.00909", "authors": ["Philip Colangelo", "Ayse K. Coskun", "Jack Megrue", "Ciaran Roberts", "Shayan Sengupta", "Varun Sivaram", "Ethan Tiao", "Aroon Vijaykar", "Chris Williams", "Daniel C. Wilson", "Zack MacFarland", "Daniel Dreiling", "Nathan Morey", "Anuja Ratnayake", "Baskar Vairamohan"], "title": "Turning AI Data Centers into Grid-Interactive Assets: Results from a Field Demonstration in Phoenix, Arizona", "categories": ["cs.DC", "cs.AI", "cs.PF", "cs.SY", "eess.SY"], "comment": "10 pages, 6 figures, 1 table", "summary": "Artificial intelligence (AI) is fueling exponential electricity demand\ngrowth, threatening grid reliability, raising prices for communities paying for\nnew energy infrastructure, and stunting AI innovation as data centers wait for\ninterconnection to constrained grids. This paper presents the first field\ndemonstration, in collaboration with major corporate partners, of a\nsoftware-only approach--Emerald Conductor--that transforms AI data centers into\nflexible grid resources that can efficiently and immediately harness existing\npower systems without massive infrastructure buildout. Conducted at a 256-GPU\ncluster running representative AI workloads within a commercial, hyperscale\ncloud data center in Phoenix, Arizona, the trial achieved a 25% reduction in\ncluster power usage for three hours during peak grid events while maintaining\nAI quality of service (QoS) guarantees. By orchestrating AI workloads based on\nreal-time grid signals without hardware modifications or energy storage, this\nplatform reimagines data centers as grid-interactive assets that enhance grid\nreliability, advance affordability, and accelerate AI's development.", "AI": {"tldr": "论文提出了一种名为Emerald Conductor的纯软件方法，将AI数据中心转变为灵活的电网资源，无需大规模基础设施改造即可高效利用现有电力系统。", "motivation": "AI的快速发展导致电力需求激增，威胁电网可靠性，增加社区能源基础设施成本，并阻碍AI创新。", "method": "通过实时电网信号协调AI工作负载，无需硬件修改或储能，在256-GPU集群上进行了实地试验。", "result": "在电网高峰时段实现了25%的集群功耗降低，同时保持AI服务质量。", "conclusion": "该方法将数据中心重新定义为电网互动资产，提升电网可靠性、经济性，并加速AI发展。"}}
{"id": "2507.00914", "pdf": "https://arxiv.org/pdf/2507.00914", "abs": "https://arxiv.org/abs/2507.00914", "authors": ["Jindong Han", "Yansong Ning", "Zirui Yuan", "Hang Ni", "Fan Liu", "Tengfei Lyu", "Hao Liu"], "title": "Large Language Model Powered Intelligent Urban Agents: Concepts, Capabilities, and Applications", "categories": ["cs.MA", "cs.AI"], "comment": null, "summary": "The long-standing vision of intelligent cities is to create efficient,\nlivable, and sustainable urban environments using big data and artificial\nintelligence technologies. Recently, the advent of Large Language Models (LLMs)\nhas opened new ways toward realizing this vision. With powerful semantic\nunderstanding and reasoning capabilities, LLMs can be deployed as intelligent\nagents capable of autonomously solving complex problems across domains. In this\narticle, we focus on Urban LLM Agents, which are LLM-powered agents that are\nsemi-embodied within the hybrid cyber-physical-social space of cities and used\nfor system-level urban decision-making. First, we introduce the concept of\nurban LLM agents, discussing their unique capabilities and features. Second, we\nsurvey the current research landscape from the perspective of agent workflows,\nencompassing urban sensing, memory management, reasoning, execution, and\nlearning. Third, we categorize the application domains of urban LLM agents into\nfive groups: urban planning, transportation, environment, public safety, and\nurban society, presenting representative works in each group. Finally, we\ndiscuss trustworthiness and evaluation issues that are critical for real-world\ndeployment, and identify several open problems for future research. This survey\naims to establish a foundation for the emerging field of urban LLM agents and\nto provide a roadmap for advancing the intersection of LLMs and urban\nintelligence. A curated list of relevant papers and open-source resources is\nmaintained and continuously updated at\nhttps://github.com/usail-hkust/Awesome-Urban-LLM-Agents.", "AI": {"tldr": "本文介绍了城市LLM代理的概念及其在智能城市中的应用，包括感知、推理、执行和学习等方面，并探讨了其可信度和评估问题。", "motivation": "探索如何利用大型语言模型（LLMs）作为智能代理，推动智能城市的高效、宜居和可持续发展。", "method": "通过引入城市LLM代理的概念，综述其工作流程（感知、记忆、推理、执行和学习），并分类其在城市规划、交通、环境、公共安全和城市社会等领域的应用。", "result": "提出了城市LLM代理的研究框架，并总结了代表性工作和开源资源。", "conclusion": "为城市LLM代理这一新兴领域奠定了基础，并提供了未来研究的路线图。"}}
{"id": "2507.00938", "pdf": "https://arxiv.org/pdf/2507.00938", "abs": "https://arxiv.org/abs/2507.00938", "authors": ["Zihao Sun", "Meng Fang", "Ling Chen"], "title": "WebArXiv: Evaluating Multimodal Agents on Time-Invariant arXiv Tasks", "categories": ["cs.IR", "cs.AI", "cs.DB", "F.2.2; I.2.7"], "comment": "10 pages, 9 figures, 4 tables", "summary": "Recent progress in large language models (LLMs) has enabled the development\nof autonomous web agents capable of navigating and interacting with real\nwebsites. However, evaluating such agents remains challenging due to the\ninstability and inconsistency of existing benchmarks, which often rely on\ndynamic content or oversimplified simulations. In this work, we introduce\nWebArXiv, a static and time-invariant benchmark comprising 275 web-based tasks\ngrounded in the arXiv platform. WebArXiv ensures reproducible and reliable\nevaluation by anchoring tasks in fixed web snapshots with deterministic ground\ntruths and standardized action trajectories. Through behavioral analysis, we\nidentify a common failure mode, Rigid History Reflection, where agents\nover-rely on fixed interaction histories. To address this, we propose a\nlightweight dynamic reflection mechanism that allows agents to selectively\nretrieve relevant past steps during decision-making. We evaluate ten\nstate-of-the-art web agents on WebArXiv. Results demonstrate clear performance\ndifferences across agents and validate the effectiveness of our proposed\nreflection strategy.", "AI": {"tldr": "本文介绍了WebArXiv，一个静态且时间不变的基准测试，用于评估自主网络代理的性能，并提出了一种动态反射机制以解决代理的常见失败模式。", "motivation": "现有评估自主网络代理的基准测试存在不稳定性和不一致性问题，需要一种更可靠的方法。", "method": "开发了WebArXiv基准测试，包含275个基于arXiv平台的任务，并提出动态反射机制以改进代理决策。", "result": "评估了10种先进网络代理，结果显示性能差异明显，动态反射机制有效。", "conclusion": "WebArXiv为评估网络代理提供了可靠基准，动态反射机制提升了代理性能。"}}
{"id": "2507.00953", "pdf": "https://arxiv.org/pdf/2507.00953", "abs": "https://arxiv.org/abs/2507.00953", "authors": ["Ke Liu", "Shuanke Shen", "Hao Chen"], "title": "From Sentences to Sequences: Rethinking Languages in Biological System", "categories": ["q-bio.BM", "cs.AI"], "comment": null, "summary": "The paradigm of large language models in natural language processing (NLP)\nhas also shown promise in modeling biological languages, including proteins,\nRNA, and DNA. Both the auto-regressive generation paradigm and evaluation\nmetrics have been transferred from NLP to biological sequence modeling.\nHowever, the intrinsic structural correlations in natural and biological\nlanguages differ fundamentally. Therefore, we revisit the notion of language in\nbiological systems to better understand how NLP successes can be effectively\ntranslated to biological domains. By treating the 3D structure of biomolecules\nas the semantic content of a sentence and accounting for the strong\ncorrelations between residues or bases, we highlight the importance of\nstructural evaluation and demonstrate the applicability of the auto-regressive\nparadigm in biological language modeling. Code can be found at\n\\href{https://github.com/zjuKeLiu/RiFold}{github.com/zjuKeLiu/RiFold}", "AI": {"tldr": "论文探讨了如何将自然语言处理（NLP）中的大型语言模型范式应用于生物语言建模，强调了结构评估的重要性。", "motivation": "研究动机在于理解NLP的成功如何有效迁移到生物领域，尤其是考虑到自然语言和生物语言在结构相关性上的根本差异。", "method": "方法包括将生物分子的3D结构视为句子的语义内容，并考虑残基或碱基之间的强相关性，同时验证自回归生成范式在生物语言建模中的适用性。", "result": "结果表明，结构评估在生物语言建模中至关重要，且自回归范式在此领域具有适用性。", "conclusion": "结论指出，通过重新定义生物系统中的语言概念，可以更有效地将NLP技术应用于生物序列建模。"}}
{"id": "2507.00966", "pdf": "https://arxiv.org/pdf/2507.00966", "abs": "https://arxiv.org/abs/2507.00966", "authors": ["Nikolai Lund Kühne", "Jesper Jensen", "Jan Østergaard", "Zheng-Hua Tan"], "title": "MambAttention: Mamba with Multi-Head Attention for Generalizable Single-Channel Speech Enhancement", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": "Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing for possible publication", "summary": "With the advent of new sequence models like Mamba and xLSTM, several studies\nhave shown that these models match or outperform state-of-the-art models in\nsingle-channel speech enhancement, automatic speech recognition, and\nself-supervised audio representation learning. However, prior research has\ndemonstrated that sequence models like LSTM and Mamba tend to overfit to the\ntraining set. To address this issue, previous works have shown that adding\nself-attention to LSTMs substantially improves generalization performance for\nsingle-channel speech enhancement. Nevertheless, neither the concept of hybrid\nMamba and time-frequency attention models nor their generalization performance\nhave been explored for speech enhancement. In this paper, we propose a novel\nhybrid architecture, MambAttention, which combines Mamba and shared time- and\nfrequency-multi-head attention modules for generalizable single-channel speech\nenhancement. To train our model, we introduce VoiceBank+Demand Extended\n(VB-DemandEx), a dataset inspired by VoiceBank+Demand but with more challenging\nnoise types and lower signal-to-noise ratios. Trained on VB-DemandEx, our\nproposed MambAttention model significantly outperforms existing\nstate-of-the-art LSTM-, xLSTM-, Mamba-, and Conformer-based systems of similar\ncomplexity across all reported metrics on two out-of-domain datasets: DNS 2020\nand EARS-WHAM_v2, while matching their performance on the in-domain dataset\nVB-DemandEx. Ablation studies highlight the role of weight sharing between the\ntime- and frequency-multi-head attention modules for generalization\nperformance. Finally, we explore integrating the shared time- and\nfrequency-multi-head attention modules with LSTM and xLSTM, which yields a\nnotable performance improvement on the out-of-domain datasets. However, our\nMambAttention model remains superior on both out-of-domain datasets across all\nreported evaluation metrics.", "AI": {"tldr": "提出了一种新型混合架构MambAttention，结合Mamba和共享时频多头注意力模块，显著提升了单通道语音增强的泛化性能。", "motivation": "解决现有序列模型（如LSTM和Mamba）在训练集上过拟合的问题，探索混合Mamba与时频注意力模型的潜力。", "method": "提出MambAttention架构，结合Mamba和共享时频多头注意力模块，并使用VB-DemandEx数据集进行训练。", "result": "在VB-DemandEx数据集上表现优异，并在两个域外数据集（DNS 2020和EARS-WHAM_v2）上显著优于现有模型。", "conclusion": "MambAttention模型在泛化性能上表现最佳，共享时频注意力模块对性能提升至关重要。"}}
{"id": "2507.00971", "pdf": "https://arxiv.org/pdf/2507.00971", "abs": "https://arxiv.org/abs/2507.00971", "authors": ["Taeyoun Kim", "Fahim Tajwar", "Aditi Raghunathan", "Aviral Kumar"], "title": "Reasoning as an Adaptive Defense for Safety", "categories": ["cs.LG", "cs.AI"], "comment": "42 pages, 11 Figures, 7 Tables", "summary": "Reasoning methods that adaptively allocate test-time compute have advanced\nLLM performance on easy to verify domains such as math and code. In this work,\nwe study how to utilize this approach to train models that exhibit a degree of\nrobustness to safety vulnerabilities, and show that doing so can provide\nbenefits. We build a recipe called $\\textit{TARS}$ (Training Adaptive Reasoners\nfor Safety), a reinforcement learning (RL) approach that trains models to\nreason about safety using chain-of-thought traces and a reward signal that\nbalances safety with task completion. To build TARS, we identify three critical\ndesign choices: (1) a \"lightweight\" warmstart SFT stage, (2) a mix of harmful,\nharmless, and ambiguous prompts to prevent shortcut behaviors such as too many\nrefusals, and (3) a reward function to prevent degeneration of reasoning\ncapabilities during training. Models trained with TARS exhibit adaptive\nbehaviors by spending more compute on ambiguous queries, leading to better\nsafety-refusal trade-offs. They also internally learn to better distinguish\nbetween safe and unsafe prompts and attain greater robustness to both white-box\n(e.g., GCG) and black-box attacks (e.g., PAIR). Overall, our work provides an\neffective, open recipe for training LLMs against jailbreaks and harmful\nrequests by reasoning per prompt.", "AI": {"tldr": "论文提出了一种名为TARS的强化学习方法，通过自适应分配计算资源来提升LLM在安全性问题上的表现。", "motivation": "研究如何利用自适应计算分配方法训练模型，以提高其对安全漏洞的鲁棒性。", "method": "采用强化学习（RL）方法，结合链式思维追踪和平衡安全性与任务完成的奖励信号，设计了三个关键步骤：轻量级SFT预热、混合提示训练和防止推理能力退化的奖励函数。", "result": "TARS训练的模型在模糊查询上分配更多计算资源，实现了更好的安全性与拒绝的平衡，并提升了对抗攻击的鲁棒性。", "conclusion": "TARS为训练LLM对抗越狱和有害请求提供了一种有效的开放式方法。"}}
{"id": "2507.01003", "pdf": "https://arxiv.org/pdf/2507.01003", "abs": "https://arxiv.org/abs/2507.01003", "authors": ["Eun-Ji Park", "Sangwon Yun"], "title": "Description of the Training Process of Neural Networks via Ergodic Theorem : Ghost nodes", "categories": ["cs.LG", "cs.AI"], "comment": "9 pages, 2 figures", "summary": "Recent studies have proposed interpreting the training process from an\nergodic perspective. Building on this foundation we present a unified framework\nfor understanding and accelerating the training of deep neural networks via\nstochastic gradient descent. By analyzing the geometric landscape of the\nobjective function we introduce a practical diagnostic, the running estimate of\nthe largest Lyapunov exponent, which provably distinguishes genuine convergence\ntoward stable minimizers from mere statistical stabilization near saddle\npoints. We then propose a ghost category extension for standard classifiers\nthat adds auxiliary ghost output nodes so the model gains extra descent\ndirections that open a lateral corridor around narrow loss barriers and enable\nthe optimizer to bypass poor basins during the early training phase. We show\nthat this extension strictly reduces approximation error and that after\nsufficient convergence the ghost dimensions collapse and the extended model's\ninvariant law coincides with that of the original and there exists a path in\nthe enlarged parameter space along which the total loss does not increase while\nthe original loss decreases by an arbitrary margin. Taken together these\nresults provide a principled architecture level intervention that accelerates\nearly stage trainability while preserving asymptotic behavior.", "AI": {"tldr": "论文提出了一种通过随机梯度下降理解和加速深度神经网络训练的统一框架，引入Lyapunov指数诊断，并提出幽灵类别扩展以优化训练过程。", "motivation": "现有研究从遍历性角度解释训练过程，但缺乏对训练动态的全面理解和加速方法。", "method": "通过分析目标函数的几何景观，引入Lyapunov指数诊断；提出幽灵类别扩展，增加辅助输出节点以绕过早期训练中的不良区域。", "result": "幽灵扩展严格减少近似误差，且在收敛后与原始模型一致；训练加速且保持渐近行为。", "conclusion": "该框架为架构层面的干预提供了理论基础，优化了早期训练性能而不影响最终结果。"}}
