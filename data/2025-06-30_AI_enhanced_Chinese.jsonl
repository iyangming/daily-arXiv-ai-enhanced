{"id": "2506.21669", "pdf": "https://arxiv.org/pdf/2506.21669", "abs": "https://arxiv.org/abs/2506.21669", "authors": ["Wanxin Tian", "Shijie Zhang", "Kevin Zhang", "Xiaowei Chi", "Yulin Luo", "Junyu Lu", "Chunkai Fan", "Qiang Zhou", "Yiming Zhao", "Ning Liu Siyu Lin", "Zhiyuan Qin", "Xiaozhu Ju", "Shanghang Zhang", "Jian Tang"], "title": "SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving Embodied Agents", "categories": ["cs.AI"], "comment": null, "summary": "Self-evolution, the ability of agents to autonomously improve their reasoning\nand behavior, is essential for the embodied domain with long-horizon,\nreal-world tasks. Despite current advancements in reinforcement fine-tuning\n(RFT) showing strong performance in enhancing reasoning in LLMs, its potential\nto enable self-evolving embodied intelligence with multi-modal interactions\nremains largely unexplored. Specifically, reinforcement fine-tuning faces two\nfundamental obstacles in embodied settings: (i) the lack of accessible\nintermediate rewards in multi-step reasoning tasks limits effective learning\nsignals, and (ii) reliance on hand-crafted reward functions restricts\ngeneralization to novel tasks and environments. To address these challenges, we\npresent Self-Evolving Embodied Agents-R1, SEEA-R1, the first RFT framework\ndesigned for enabling the self-evolving capabilities of embodied agents.\nSpecifically, to convert sparse delayed rewards into denser intermediate\nsignals that improve multi-step reasoning, we propose Tree-based group relative\npolicy optimization (Tree-GRPO), which integrates Monte Carlo Tree Search into\nGRPO. To generalize reward estimation across tasks and scenes, supporting\nautonomous adaptation and reward-driven self-evolution, we further introduce\nMulti-modal Generative Reward Model (MGRM). To holistically evaluate the\neffectiveness of SEEA-R1, we evaluate on the ALFWorld benchmark, surpassing\nstate-of-the-art methods with scores of 85.07% (textual) and 36.19%\n(multi-modal), outperforming prior models including GPT-4o. SEEA-R1 also\nachieves scores of 80.3% without environmental reward, surpassing all\nopen-source baselines and highlighting its scalability as a self-evolving\nembodied agent. Additional experiments and qualitative analysis further support\nthe potential of SEEA-R1 for future research in scalable embodied intelligence.", "AI": {"tldr": "SEEA-R1是一种强化微调框架，旨在提升具身智能体的自我进化能力，通过Tree-GRPO和MGRM解决多步推理和奖励泛化问题，在ALFWorld基准测试中表现优异。", "motivation": "当前强化微调在具身智能领域的潜力尚未充分探索，面临多步推理任务中奖励稀疏和手工奖励函数泛化能力不足的挑战。", "method": "提出SEEA-R1框架，结合Tree-GRPO（基于蒙特卡洛树搜索的GRPO）和MGRM（多模态生成奖励模型），优化多步推理和奖励泛化。", "result": "在ALFWorld基准测试中，SEEA-R1在文本和多模态任务中分别达到85.07%和36.19%的得分，超越GPT-4o等模型。", "conclusion": "SEEA-R1展示了具身智能体自我进化的潜力，为未来可扩展的具身智能研究提供了有力支持。"}}
{"id": "2506.21734", "pdf": "https://arxiv.org/pdf/2506.21734", "abs": "https://arxiv.org/abs/2506.21734", "authors": ["Guan Wang", "Jin Li", "Yuhao Sun", "Xing Chen", "Changling Liu", "Yue Wu", "Meng Lu", "Sen Song", "Yasin Abbasi Yadkori"], "title": "Hierarchical Reasoning Model", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Reasoning, the process of devising and executing complex goal-oriented action\nsequences, remains a critical challenge in AI. Current large language models\n(LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from\nbrittle task decomposition, extensive data requirements, and high latency.\nInspired by the hierarchical and multi-timescale processing in the human brain,\nwe propose the Hierarchical Reasoning Model (HRM), a novel recurrent\narchitecture that attains significant computational depth while maintaining\nboth training stability and efficiency. HRM executes sequential reasoning tasks\nin a single forward pass without explicit supervision of the intermediate\nprocess, through two interdependent recurrent modules: a high-level module\nresponsible for slow, abstract planning, and a low-level module handling rapid,\ndetailed computations. With only 27 million parameters, HRM achieves\nexceptional performance on complex reasoning tasks using only 1000 training\nsamples. The model operates without pre-training or CoT data, yet achieves\nnearly perfect performance on challenging tasks including complex Sudoku\npuzzles and optimal path finding in large mazes. Furthermore, HRM outperforms\nmuch larger models with significantly longer context windows on the Abstraction\nand Reasoning Corpus (ARC), a key benchmark for measuring artificial general\nintelligence capabilities. These results underscore HRM's potential as a\ntransformative advancement toward universal computation and general-purpose\nreasoning systems.", "AI": {"tldr": "HRM是一种新型递归架构，通过分层和多时间尺度处理实现高效推理，无需大量数据或显式监督，在少量训练样本下表现优异。", "motivation": "当前大型语言模型（LLMs）的Chain-of-Thought（CoT）技术存在任务分解脆弱、数据需求大和高延迟问题，需要更高效的推理方法。", "method": "HRM采用分层递归架构，包含高层模块（慢速抽象规划）和低层模块（快速详细计算），单次前向传递完成推理任务。", "result": "HRM仅用2700万参数和1000个训练样本，在复杂推理任务（如数独、迷宫路径规划）和ARC基准测试中表现优异，超越更大模型。", "conclusion": "HRM展示了通用计算和通用推理系统的潜力，是迈向人工通用智能的重要进展。"}}
{"id": "2506.21763", "pdf": "https://arxiv.org/pdf/2506.21763", "abs": "https://arxiv.org/abs/2506.21763", "authors": ["Xin Wang", "Jiyao Liu", "Yulong Xiao", "Junzhi Ning", "Lihao Liu", "Junjun He", "Botian Shi", "Kaicheng Yu"], "title": "THE-Tree: Can Tracing Historical Evolution Enhance Scientific Verification and Reasoning?", "categories": ["cs.AI"], "comment": null, "summary": "Large Language Models (LLMs) are accelerating scientific idea generation, but\nrigorously evaluating these numerous, often superficial, AI-generated\npropositions for novelty and factual accuracy is a critical bottleneck; manual\nverification is too slow.Existing validation methods are inadequate: LLMs as\nstandalone verifiers may hallucinate and lack domain knowledge (our findings\nshow ~60\\% unawareness of relevant papers in specific domains), while\ntraditional citation networks lack explicit causality and narrative surveys are\nunstructured.This underscores a core challenge: the absence of structured,\nverifiable, and causally-linked historical data of scientific evolution.To\naddress this,we introduce \\textbf{THE-Tree} (\\textbf{T}echnology\n\\textbf{H}istory \\textbf{E}volution Tree), a computational framework that\nconstructs such domain-specific evolution trees from scientific\nliterature.THE-Tree employs a search algorithm to explore evolutionary paths.\nDuring its node expansion, it utilizes a novel \"Think-Verbalize-Cite-Verify\"\nprocess: an LLM proposes potential advancements and cites supporting\nliterature. Critically, each proposed evolutionary link is then validated for\nlogical coherence and evidential support by a recovered natural language\ninference mechanism that interrogates the cited literature, ensuring that each\nstep is grounded.We construct and validate 88 THE-Trees across diverse domains\nand release a benchmark dataset including up to 71k fact verifications covering\n27k papers to foster further research.Experiments demonstrate that i) in graph\ncompletion, our THE-Tree improves hit@1 by 8\\% to 14\\% across multiple models\ncompared to traditional citation networks; ii) for predicting future scientific\ndevelopments, it improves hit@1 metric by nearly 10\\%; and iii) when combined\nwith other methods, it boosts the performance of evaluating important\nscientific papers by almost 100\\%.", "AI": {"tldr": "THE-Tree是一种计算框架，通过构建领域特定的技术演化树，解决大语言模型生成科学命题的验证问题，显著提升科学发展的预测和评估能力。", "motivation": "现有验证方法（如大语言模型独立验证或传统引用网络）无法有效评估AI生成科学命题的新颖性和准确性，缺乏结构化、可验证且因果关联的科学演化数据。", "method": "THE-Tree框架通过搜索算法构建演化树，采用“Think-Verbalize-Cite-Verify”流程：LLM提出潜在进展并引用文献，通过自然语言推理机制验证逻辑和证据支持。", "result": "实验显示，THE-Tree在图形补全、预测未来科学发展和评估重要论文方面显著优于传统方法，性能提升8%至100%。", "conclusion": "THE-Tree为解决科学命题验证瓶颈提供了有效工具，推动了科学演化的结构化研究。"}}
{"id": "2506.21784", "pdf": "https://arxiv.org/pdf/2506.21784", "abs": "https://arxiv.org/abs/2506.21784", "authors": ["Yifan Liu", "Xishun Liao", "Haoxuan Ma", "Jonathan Liu", "Rohan Jadhav", "Jiaqi Ma"], "title": "MobiVerse: Scaling Urban Mobility Simulation with Hybrid Lightweight Domain-Specific Generator and Large Language Models", "categories": ["cs.AI"], "comment": null, "summary": "Understanding and modeling human mobility patterns is crucial for effective\ntransportation planning and urban development. Despite significant advances in\nmobility research, there remains a critical gap in simulation platforms that\nallow for algorithm development, policy implementation, and comprehensive\nevaluation at scale. Traditional activity-based models require extensive data\ncollection and manual calibration, machine learning approaches struggle with\nadaptation to dynamic conditions, and treding agent-based Large Language Models\n(LLMs) implementations face computational constraints with large-scale\nsimulations. To address these challenges, we propose MobiVerse, a hybrid\nframework leverages the efficiency of lightweight domain-specific generator for\ngenerating base activity chains with the adaptability of LLMs for context-aware\nmodifications. A case study was conducted in Westwood, Los Angeles, where we\nefficiently generated and dynamically adjusted schedules for the whole\npopulation of approximately 53,000 agents on a standard PC. Our experiments\ndemonstrate that MobiVerse successfully enables agents to respond to\nenvironmental feedback, including road closures, large gathering events like\nfootball games, and congestion, through our hybrid framework. Its modular\ndesign facilitates testing various mobility algorithms at both transportation\nsystem and agent levels. Results show our approach maintains computational\nefficiency while enhancing behavioral realism. MobiVerse bridges the gap in\nmobility simulation by providing a customizable platform for mobility systems\nplanning and operations with benchmark algorithms. Code and videos are\navailable at https://github.com/ucla-mobility/MobiVerse.", "AI": {"tldr": "MobiVerse是一个混合框架，结合轻量级领域特定生成器和LLMs，用于大规模模拟人类移动模式，解决了传统方法的局限性。", "motivation": "现有移动模拟平台在算法开发、政策实施和大规模评估方面存在不足，传统方法数据需求高且适应性差。", "method": "提出MobiVerse，结合轻量级生成器生成基础活动链，利用LLMs进行上下文感知调整。", "result": "在洛杉矶Westwood的案例中，成功模拟了53,000个代理的动态调整，展示了环境反馈的适应性。", "conclusion": "MobiVerse在计算效率和行为真实性之间取得平衡，为移动系统规划提供了可定制平台。"}}
{"id": "2506.21555", "pdf": "https://arxiv.org/pdf/2506.21555", "abs": "https://arxiv.org/abs/2506.21555", "authors": ["Jiahong Li", "Yiwen Shao", "Jianheng Zhuo", "Chenda Li", "Liliang Tang", "Dong Yu", "Yanmin Qian"], "title": "Efficient Multilingual ASR Finetuning via LoRA Language Experts", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted in Interspeech 2025", "summary": "Recent advancements in deep learning have significantly enhanced multilingual\nautomatic speech recognition (ASR) due to the development of advanced model\narchitectures and available large-scale multilingual datasets. Despite that,\nmultilingual ASR still suffers from the curse of multilinguality in that\ndifferent languages tend to interfere with each other, making it difficult for\nthe ASR model to identify multiple languages effectively while sharing model\ncapacity across them. This paper proposes an efficient finetuning framework for\ncustomized multilingual ASR via prepared LoRA language experts based on\nWhisper. Through LoRA expert fusion or knowledge distillation, our approach\nachieves better recognition performance on target languages than standard\nfine-tuning methods. Experimental results demonstrate that the proposed models\nyield approximately 10\\% and 15\\% relative performance gains in language-aware\nand language-agnostic scenarios, respectively.", "AI": {"tldr": "提出了一种基于Whisper的高效微调框架，通过LoRA语言专家提升多语言ASR性能。", "motivation": "多语言ASR中不同语言相互干扰，影响识别效果。", "method": "采用LoRA专家融合或知识蒸馏的微调框架。", "result": "在语言感知和无语言感知场景下分别提升10%和15%的性能。", "conclusion": "该方法显著提高了多语言ASR的识别效果。"}}
{"id": "2506.21656", "pdf": "https://arxiv.org/pdf/2506.21656", "abs": "https://arxiv.org/abs/2506.21656", "authors": ["Yifan Shen", "Yuanzhe Liu", "Jingyuan Zhu", "Xu Cao", "Xiaofeng Zhang", "Yixiao He", "Wenming Ye", "James Matthew Rehg", "Ismini Lourentzou"], "title": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs", "categories": ["cs.CV", "cs.CL"], "comment": "29 pages", "summary": "Current Vision-Language Models (VLMs) struggle with fine-grained spatial\nreasoning, particularly when multi-step logic and precise spatial alignment are\nrequired. In this work, we introduce SpatialReasoner-R1, a vision-language\nreasoning model designed to address these limitations. To construct\nhigh-quality supervision for spatial reasoning, we design a Multi-Model Monte\nCarlo Tree Search (M3CTS) method that generates diverse, logically consistent\nLong Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose\nfine-grained Direct Preference Optimization (fDPO), which introduces\nsegment-specific preference granularity for descriptive grounding and logical\nreasoning, guided by a spatial reward mechanism that evaluates candidate\nresponses based on visual consistency, spatial grounding, and logical\ncoherence. Experimental results demonstrate that fDPO achieves an average\nimprovement of 4.1% over standard DPO across spatial quality tasks, and a 9.0%\ngain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a\nnew SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in\naverage accuracy, while maintaining competitive performance on general\nvision-language tasks.", "AI": {"tldr": "SpatialReasoner-R1模型通过M3CTS生成高质量监督数据，结合fDPO优化空间推理任务，显著提升性能。", "motivation": "现有视觉语言模型在细粒度空间推理和多步逻辑对齐方面表现不足。", "method": "提出M3CTS生成多样且逻辑一致的LongCoT轨迹，并设计fDPO优化空间奖励机制。", "result": "fDPO在空间质量任务上提升4.1%，数量任务提升9.0%；SpatialReasoner-R1在SPATIALRGPT-Bench上刷新SoTA。", "conclusion": "SpatialReasoner-R1在空间推理任务中表现优异，同时保持通用视觉语言任务的竞争力。"}}
{"id": "2506.21805", "pdf": "https://arxiv.org/pdf/2506.21805", "abs": "https://arxiv.org/abs/2506.21805", "authors": ["Nicolas Bougie", "Narimasa Watanabe"], "title": "CitySim: Modeling Urban Behaviors and City Dynamics with Large-Scale LLM-Driven Agent Simulation", "categories": ["cs.AI", "cs.CL"], "comment": null, "summary": "Modeling human behavior in urban environments is fundamental for social\nscience, behavioral studies, and urban planning. Prior work often rely on\nrigid, hand-crafted rules, limiting their ability to simulate nuanced\nintentions, plans, and adaptive behaviors. Addressing these challenges, we\nenvision an urban simulator (CitySim), capitalizing on breakthroughs in\nhuman-level intelligence exhibited by large language models. In CitySim, agents\ngenerate realistic daily schedules using a recursive value-driven approach that\nbalances mandatory activities, personal habits, and situational factors. To\nenable long-term, lifelike simulations, we endow agents with beliefs, long-term\ngoals, and spatial memory for navigation. CitySim exhibits closer alignment\nwith real humans than prior work, both at micro and macro levels. Additionally,\nwe conduct insightful experiments by modeling tens of thousands of agents and\nevaluating their collective behaviors under various real-world scenarios,\nincluding estimating crowd density, predicting place popularity, and assessing\nwell-being. Our results highlight CitySim as a scalable, flexible testbed for\nunderstanding and forecasting urban phenomena.", "AI": {"tldr": "CitySim利用大语言模型模拟人类行为，通过递归价值驱动方法生成真实日程，优于传统规则方法。", "motivation": "传统方法依赖固定规则，难以模拟复杂行为和意图，需要更灵活的模拟工具。", "method": "采用递归价值驱动方法，结合信念、长期目标和空间记忆，模拟个体和群体行为。", "result": "CitySim在微观和宏观层面更接近真实人类行为，并能模拟大规模场景。", "conclusion": "CitySim是理解和预测城市现象的可扩展、灵活平台。"}}
{"id": "2506.21556", "pdf": "https://arxiv.org/pdf/2506.21556", "abs": "https://arxiv.org/abs/2506.21556", "authors": ["Hyeongcheol Park", "MinHyuk Jang", "Ha Dam Baek", "Gyusam Chang", "Jiyoung Seo", "Jiwan Park", "Hogun Park", "Sangpil Kim"], "title": "VAT-KG: Knowledge-Intensive Multimodal Knowledge Graph Dataset for Retrieval-Augmented Generation", "categories": ["cs.CL"], "comment": "Project Page: https://vatkg.github.io/", "summary": "Multimodal Knowledge Graphs (MMKGs), which represent explicit knowledge\nacross multiple modalities, play a pivotal role by complementing the implicit\nknowledge of Multimodal Large Language Models (MLLMs) and enabling more\ngrounded reasoning via Retrieval Augmented Generation (RAG). However, existing\nMMKGs are generally limited in scope: they are often constructed by augmenting\npre-existing knowledge graphs, which restricts their knowledge, resulting in\noutdated or incomplete knowledge coverage, and they often support only a narrow\nrange of modalities, such as text and visual information. These limitations\nreduce their extensibility and applicability to a broad range of multimodal\ntasks, particularly as the field shifts toward richer modalities such as video\nand audio in recent MLLMs. Therefore, we propose the Visual-Audio-Text\nKnowledge Graph (VAT-KG), the first concept-centric and knowledge-intensive\nmultimodal knowledge graph that covers visual, audio, and text information,\nwhere each triplet is linked to multimodal data and enriched with detailed\ndescriptions of concepts. Specifically, our construction pipeline ensures\ncross-modal knowledge alignment between multimodal data and fine-grained\nsemantics through a series of stringent filtering and alignment steps, enabling\nthe automatic generation of MMKGs from any multimodal dataset. We further\nintroduce a novel multimodal RAG framework that retrieves detailed\nconcept-level knowledge in response to queries from arbitrary modalities.\nExperiments on question answering tasks across various modalities demonstrate\nthe effectiveness of VAT-KG in supporting MLLMs, highlighting its practical\nvalue in unifying and leveraging multimodal knowledge.", "AI": {"tldr": "论文提出了VAT-KG，首个涵盖视觉、音频和文本的多模态知识图谱，通过严格的过滤和对齐步骤自动生成，并引入多模态RAG框架，实验证明其在多模态任务中的有效性。", "motivation": "现有MMKGs知识覆盖有限且模态单一，无法满足多模态任务需求，特别是视频和音频等丰富模态。", "method": "提出VAT-KG，通过严格过滤和对齐步骤自动生成多模态知识图谱，并设计多模态RAG框架。", "result": "实验表明VAT-KG在多模态问答任务中有效支持MLLMs。", "conclusion": "VAT-KG在多模态知识统一和应用中具有实用价值。"}}
{"id": "2506.21681", "pdf": "https://arxiv.org/pdf/2506.21681", "abs": "https://arxiv.org/abs/2506.21681", "authors": ["Hakan Çapuk", "Andrew Bond", "Muhammed Burak Kızıl", "Emir Göçen", "Erkut Erdem", "Aykut Erdem"], "title": "TanDiT: Tangent-Plane Diffusion Transformer for High-Quality 360° Panorama Generation", "categories": ["cs.CV", "cs.LG"], "comment": null, "summary": "Recent advances in image generation have led to remarkable improvements in\nsynthesizing perspective images. However, these models still struggle with\npanoramic image generation due to unique challenges, including varying levels\nof geometric distortion and the requirement for seamless loop-consistency. To\naddress these issues while leveraging the strengths of the existing models, we\nintroduce TanDiT, a method that synthesizes panoramic scenes by generating\ngrids of tangent-plane images covering the entire 360$^\\circ$ view. Unlike\nprevious methods relying on multiple diffusion branches, TanDiT utilizes a\nunified diffusion model trained to produce these tangent-plane images\nsimultaneously within a single denoising iteration. Furthermore, we propose a\nmodel-agnostic post-processing step specifically designed to enhance global\ncoherence across the generated panoramas. To accurately assess panoramic image\nquality, we also present two specialized metrics, TangentIS and TangentFID, and\nprovide a comprehensive benchmark comprising captioned panoramic datasets and\nstandardized evaluation scripts. Extensive experiments demonstrate that our\nmethod generalizes effectively beyond its training data, robustly interprets\ndetailed and complex text prompts, and seamlessly integrates with various\ngenerative models to yield high-quality, diverse panoramic images.", "AI": {"tldr": "TanDiT是一种新方法，通过生成覆盖360度视图的切面图像网格来合成全景场景，解决了现有模型在全景图像生成中的几何失真和循环一致性挑战。", "motivation": "现有图像生成模型在全景图像生成中存在几何失真和循环一致性问题，需要一种新方法来克服这些挑战。", "method": "TanDiT使用统一的扩散模型在单次去噪迭代中同时生成切面图像，并提出模型无关的后处理步骤以增强全局一致性。", "result": "实验表明，TanDiT能有效泛化到训练数据之外，处理复杂文本提示，并与其他生成模型无缝集成，生成高质量全景图像。", "conclusion": "TanDiT通过创新方法解决了全景图像生成的挑战，并提供了专用评估指标和基准，为未来研究奠定了基础。"}}
{"id": "2506.21887", "pdf": "https://arxiv.org/pdf/2506.21887", "abs": "https://arxiv.org/abs/2506.21887", "authors": ["Edward Chen", "Sang T. Truong", "Natalie Dullerud", "Sanmi Koyejo", "Carlos Guestrin"], "title": "Interactive Multi-Objective Probabilistic Preference Learning with Soft and Hard Bounds", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "High-stakes decision-making involves navigating multiple competing objectives\nwith expensive evaluations. For instance, in brachytherapy, clinicians must\nbalance maximizing tumor coverage (e.g., an aspirational target or soft bound\nof >95% coverage) against strict organ dose limits (e.g., a non-negotiable hard\nbound of <601 cGy to the bladder), with each plan evaluation being\nresource-intensive. Selecting Pareto-optimal solutions that match implicit\npreferences is challenging, as exhaustive Pareto frontier exploration is\ncomputationally and cognitively prohibitive, necessitating interactive\nframeworks to guide users. While decision-makers (DMs) often possess domain\nknowledge to narrow the search via such soft-hard bounds, current methods often\nlack systematic approaches to iteratively refine these multi-faceted preference\nstructures. Critically, DMs must trust their final decision, confident they\nhaven't missed superior alternatives; this trust is paramount in\nhigh-consequence scenarios. We present Active-MoSH, an interactive local-global\nframework designed for this process. Its local component integrates soft-hard\nbounds with probabilistic preference learning, maintaining distributions over\nDM preferences and bounds for adaptive Pareto subset refinement. This is guided\nby an active sampling strategy optimizing exploration-exploitation while\nminimizing cognitive burden. To build DM trust, Active-MoSH's global component,\nT-MoSH, leverages multi-objective sensitivity analysis to identify potentially\noverlooked, high-value points beyond immediate feedback. We demonstrate\nActive-MoSH's performance benefits through diverse synthetic and real-world\napplications. A user study on AI-generated image selection further validates\nour hypotheses regarding the framework's ability to improve convergence,\nenhance DM trust, and provide expressive preference articulation, enabling more\neffective DMs.", "AI": {"tldr": "Active-MoSH是一个交互式框架，用于在高风险决策中平衡多目标优化，结合软硬边界和偏好学习，提升决策者信任和效率。", "motivation": "在高风险决策（如放射治疗）中，需平衡多个目标（如肿瘤覆盖率和器官剂量限制），但现有方法缺乏系统性偏好结构迭代优化，且决策者需信任最终选择。", "method": "提出Active-MoSH框架，包含局部组件（结合软硬边界和概率偏好学习）和全局组件（T-MoSH，通过多目标敏感性分析识别潜在高价值点）。", "result": "在合成和实际应用中验证了性能优势，用户研究表明其能加速收敛、增强决策者信任并提供更丰富的偏好表达。", "conclusion": "Active-MoSH通过交互式优化和信任构建机制，有效支持高风险多目标决策。"}}
{"id": "2506.21557", "pdf": "https://arxiv.org/pdf/2506.21557", "abs": "https://arxiv.org/abs/2506.21557", "authors": ["Kaiying Yan", "Moyang Liu", "Yukun Liu", "Ruibo Fu", "Zhengqi Wen", "Jianhua Tao", "Xuefei Liu"], "title": "Debunk and Infer: Multimodal Fake News Detection via Diffusion-Generated Evidence and LLM Reasoning", "categories": ["cs.CL"], "comment": null, "summary": "The rapid spread of fake news across multimedia platforms presents serious\nchallenges to information credibility. In this paper, we propose a\nDebunk-and-Infer framework for Fake News Detection(DIFND) that leverages\ndebunking knowledge to enhance both the performance and interpretability of\nfake news detection. DIFND integrates the generative strength of conditional\ndiffusion models with the collaborative reasoning capabilities of multimodal\nlarge language models (MLLMs). Specifically, debunk diffusion is employed to\ngenerate refuting or authenticating evidence based on the multimodal content of\nnews videos, enriching the evaluation process with diverse yet semantically\naligned synthetic samples. To improve inference, we propose a chain-of-debunk\nstrategy where a multi-agent MLLM system produces logic-grounded,\nmultimodal-aware reasoning content and final veracity judgment. By jointly\nmodeling multimodal features, generative debunking cues, and reasoning-rich\nverification within a unified architecture, DIFND achieves notable improvements\nin detection accuracy. Extensive experiments on the FakeSV and FVC datasets\nshow that DIFND not only outperforms existing approaches but also delivers\ntrustworthy decisions.", "AI": {"tldr": "论文提出了一种基于Debunk-and-Infer框架的假新闻检测方法（DIFND），结合条件扩散模型和多模态大语言模型（MLLMs），通过生成反驳证据和多代理推理提升检测性能和可解释性。", "motivation": "假新闻在多媒体平台上的快速传播对信息可信度构成严重威胁，需要一种既能提高检测准确性又能增强可解释性的方法。", "method": "DIFND框架结合条件扩散模型生成反驳或验证证据，并利用多代理MLLM系统进行逻辑推理和多模态感知，最终做出真实性判断。", "result": "在FakeSV和FVC数据集上的实验表明，DIFND在检测准确性和可信度上优于现有方法。", "conclusion": "DIFND通过整合多模态特征、生成反驳线索和推理验证，显著提升了假新闻检测的性能和可解释性。"}}
{"id": "2506.21710", "pdf": "https://arxiv.org/pdf/2506.21710", "abs": "https://arxiv.org/abs/2506.21710", "authors": ["Liangyu Zhong", "Fabio Rosenthal", "Joachim Sicking", "Fabian Hüger", "Thorsten Bagdonat", "Hanno Gottschalk", "Leo Schwinn"], "title": "FOCUS: Internal MLLM Representations for Efficient Fine-Grained Visual Question Answering", "categories": ["cs.CV"], "comment": "Preprint. Under review", "summary": "While Multimodal Large Language Models (MLLMs) offer strong perception and\nreasoning capabilities for image-text input, Visual Question Answering (VQA)\nfocusing on small image details still remains a challenge. Although visual\ncropping techniques seem promising, recent approaches have several limitations:\nthe need for task-specific fine-tuning, low efficiency due to uninformed\nexhaustive search, or incompatibility with efficient attention implementations.\nWe address these shortcomings by proposing a training-free visual cropping\nmethod, dubbed FOCUS, that leverages MLLM-internal representations to guide the\nsearch for the most relevant image region. This is accomplished in four steps:\nfirst, we identify the target object(s) in the VQA prompt; second, we compute\nan object relevance map using the key-value (KV) cache; third, we propose and\nrank relevant image regions based on the map; and finally, we perform the\nfine-grained VQA task using the top-ranked region. As a result of this informed\nsearch strategy, FOCUS achieves strong performance across four fine-grained VQA\ndatasets and two types of MLLMs. It outperforms three popular visual cropping\nmethods in both accuracy and efficiency, and matches the best-performing\nbaseline, ZoomEye, while requiring 3 - 6.5 x less compute.", "AI": {"tldr": "FOCUS是一种无需训练的视觉裁剪方法，利用MLLM内部表示指导搜索最相关图像区域，显著提升细粒度VQA任务的性能和效率。", "motivation": "尽管MLLM在图像-文本输入上表现出强大的感知和推理能力，但针对小图像细节的VQA任务仍具挑战性。现有视觉裁剪方法存在任务特定微调、低效搜索或与高效注意力实现不兼容等问题。", "method": "FOCUS通过四步实现：1)识别VQA提示中的目标对象；2)使用KV缓存计算对象相关性图；3)基于该图提出并排序相关图像区域；4)使用排名最高的区域执行细粒度VQA任务。", "result": "FOCUS在四个细粒度VQA数据集和两种MLLM上表现优异，优于三种流行视觉裁剪方法，并在计算效率上显著提升（3-6.5倍）。", "conclusion": "FOCUS通过智能搜索策略解决了现有视觉裁剪方法的局限性，为细粒度VQA任务提供了高效且高性能的解决方案。"}}
{"id": "2506.21996", "pdf": "https://arxiv.org/pdf/2506.21996", "abs": "https://arxiv.org/abs/2506.21996", "authors": ["Raphaël Boige", "Amine Boumaza", "Bruno Scherrer"], "title": "AlphaBeta is not as good as you think: a new probabilistic model to better analyze deterministic game-solving algorithms", "categories": ["cs.AI"], "comment": null, "summary": "Deterministic game-solving algorithms are conventionally analyzed in the\nlight of their average-case complexity against a distribution of random\ngame-trees, where leaf values are independently sampled from a fixed\ndistribution. This simplified model enables uncluttered mathematical analysis,\nrevealing two key properties: root value distributions asymptotically collapse\nto a single fixed value for finite-valued trees, and all reasonable algorithms\nachieve global optimality. However, these findings are artifacts of the model's\ndesign-its long criticized independence assumption strips games of structural\ncomplexity, producing trivial instances where no algorithm faces meaningful\nchallenges. To address this limitation, we introduce a new probabilistic model\nthat incrementally constructs game-trees using a fixed level-wise conditional\ndistribution. By enforcing ancestor dependency, a critical structural feature\nof real-world games, our framework generates problems with adjustable\ndifficulty while retaining some form of analytical tractability. For several\nalgorithms, including AlphaBeta and Scout, we derive recursive formulas\ncharacterizing their average-case complexities under this model. These allow us\nto rigorously compare algorithms on deep game-trees, where Monte-Carlo\nsimulations are no longer feasible. While asymptotically, all algorithms seem\nto converge to identical branching factor (a result analogous to those of\nindependence-based models), deep finite trees reveal stark differences:\nAlphaBeta incurs a significantly larger constant multiplicative factor compared\nto algorithms like Scout, leading to a substantial practical slowdown. Our\nframework sheds new light on classical game-solving algorithms, offering\nrigorous evidence and analytical tools to advance the understanding of these\nmethods under a more realistic, challenging, and yet tractable model.", "AI": {"tldr": "论文提出了一种新的概率模型，通过引入祖先依赖关系生成更具挑战性的游戏树，分析了AlphaBeta和Scout等算法的平均复杂度，揭示了实际性能差异。", "motivation": "传统模型因独立性假设简化了游戏结构，导致分析结果不具实际意义。新模型旨在通过引入结构复杂性，更真实地评估算法性能。", "method": "提出基于层级条件分布的概率模型，强制祖先依赖关系，生成可调难度的游戏树，并推导算法的递归复杂度公式。", "result": "在有限深度树中，AlphaBeta比Scout等算法有更大的常数乘数，导致实际性能显著下降。", "conclusion": "新模型为经典游戏求解算法提供了更真实的分析框架，揭示了实际性能差异，并提供了分析工具。"}}
{"id": "2506.21558", "pdf": "https://arxiv.org/pdf/2506.21558", "abs": "https://arxiv.org/abs/2506.21558", "authors": ["FutureSearch", ":", "Jack Wildman", "Nikos I. Bosse", "Daniel Hnyk", "Peter Mühlbacher", "Finn Hambly", "Jon Evans", "Dan Schwarz", "Lawrence Phillips"], "title": "Bench to the Future: A Pastcasting Benchmark for Forecasting Agents", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Forecasting is a challenging task that offers a clearly measurable way to\nstudy AI systems. Forecasting requires a large amount of research on the\ninternet, and evaluations require time for events to happen, making the\ndevelopment of forecasting benchmarks challenging. To date, no forecasting\nbenchmark provides a realistic, hermetic, and repeatable environment for LLM\nforecasters. We introduce Bench To the Future (BTF), a \"pastcasting\" benchmark\nwith hundreds of high-quality questions for which the resolution is already\nknown. Each question is accompanied by a large offline corpus of tens of\nthousands of relevant web pages, enabling a way to elicit realistic \"forecasts\"\non past events from LLMs. Results suggest that our pastcasting environment can\nproduce results comparable to those based on forecasts using the internet on\nat-the-time unresolved questions. We show results benchmarking agent and\nchain-of-thought forecasting approaches using several LLMs, including the\nrecently-released Claude 4 models, and demonstrate BTF's ability to track\nsteady forecasting capability progress over time. We intend this to be a living\nbenchmark, with new questions added continually to account for increasing\ntraining data cutoff dates. We invite researchers to contact us at\nhello@futuresearch.ai to utilize our benchmark or tooling for their own\nresearch.", "AI": {"tldr": "论文介绍了BTF（Bench To the Future）基准，用于评估LLM在已知历史事件上的预测能力，提供了一种可重复且现实的测试环境。", "motivation": "当前缺乏一个现实、封闭且可重复的预测基准来评估LLM的预测能力。", "method": "通过构建一个包含数百个已知结果问题的“pastcasting”基准，并配备大量相关网页语料库，模拟真实预测环境。", "result": "实验表明，该基准能产生与实时未解决问题预测相似的结果，并展示了不同LLM（如Claude 4）的表现。", "conclusion": "BTF是一个动态基准，将持续更新问题以跟踪LLM预测能力的进步，并欢迎研究者使用。"}}
{"id": "2506.21711", "pdf": "https://arxiv.org/pdf/2506.21711", "abs": "https://arxiv.org/abs/2506.21711", "authors": ["Aryan Thakre", "Omkar Nagwekar", "Vedang Talekar", "Aparna Santra Biswas"], "title": "CAST: Cross-Attentive Spatio-Temporal feature fusion for Deepfake detection", "categories": ["cs.CV"], "comment": "50 pages, 6 figures", "summary": "Deepfakes have emerged as a significant threat to digital media authenticity,\nincreasing the need for advanced detection techniques that can identify subtle\nand time-dependent manipulations. CNNs are effective at capturing spatial\nartifacts, and Transformers excel at modeling temporal inconsistencies.\nHowever, many existing CNN-Transformer models process spatial and temporal\nfeatures independently. In particular, attention-based methods often use\nseparate attention mechanisms for spatial and temporal features and combine\nthem using naive approaches like averaging, addition, or concatenation, which\nlimits the depth of spatio-temporal interaction. To address this challenge, we\npropose a unified CAST model that leverages cross-attention to effectively fuse\nspatial and temporal features in a more integrated manner. Our approach allows\ntemporal features to dynamically attend to relevant spatial regions, enhancing\nthe model's ability to detect fine-grained, time-evolving artifacts such as\nflickering eyes or warped lips. This design enables more precise localization\nand deeper contextual understanding, leading to improved performance across\ndiverse and challenging scenarios. We evaluate the performance of our model\nusing the FaceForensics++, Celeb-DF, and DeepfakeDetection datasets in both\nintra- and cross-dataset settings to affirm the superiority of our approach.\nOur model achieves strong performance with an AUC of 99.49 percent and an\naccuracy of 97.57 percent in intra-dataset evaluations. In cross-dataset\ntesting, it demonstrates impressive generalization by achieving a 93.31 percent\nAUC on the unseen DeepfakeDetection dataset. These results highlight the\neffectiveness of cross-attention-based feature fusion in enhancing the\nrobustness of deepfake video detection.", "AI": {"tldr": "提出了一种名为CAST的模型，利用交叉注意力机制融合时空特征，显著提升了深度伪造视频检测的性能。", "motivation": "深度伪造技术对数字媒体真实性构成威胁，现有CNN-Transformer模型在时空特征融合上存在局限性，需要更高效的检测方法。", "method": "提出CAST模型，通过交叉注意力机制动态融合时空特征，增强对细微时间演化伪造痕迹的检测能力。", "result": "在FaceForensics++、Celeb-DF和DeepfakeDetection数据集上表现优异，AUC达99.49%，准确率97.57%，跨数据集测试AUC为93.31%。", "conclusion": "交叉注意力机制显著提升了深度伪造检测的鲁棒性和泛化能力。"}}
{"id": "2506.22005", "pdf": "https://arxiv.org/pdf/2506.22005", "abs": "https://arxiv.org/abs/2506.22005", "authors": ["Naoto Onda", "Kazumi Kasaura", "Yuta Oriike", "Masaya Taniguchi", "Akiyoshi Sannai", "Sho Sonoda"], "title": "LeanConjecturer: Automatic Generation of Mathematical Conjectures for Theorem Proving", "categories": ["cs.AI"], "comment": "15 pages, 4 figures, 5 tables", "summary": "We introduce LeanConjecturer, a pipeline for automatically generating\nuniversity-level mathematical conjectures in Lean 4 using Large Language Models\n(LLMs). Our hybrid approach combines rule-based context extraction with\nLLM-based theorem statement generation, addressing the data scarcity challenge\nin formal theorem proving. Through iterative generation and evaluation,\nLeanConjecturer produced 12,289 conjectures from 40 Mathlib seed files, with\n3,776 identified as syntactically valid and non-trivial, that is, cannot be\nproven by \\texttt{aesop} tactic. We demonstrate the utility of these generated\nconjectures for reinforcement learning through Group Relative Policy\nOptimization (GRPO), showing that targeted training on domain-specific\nconjectures can enhance theorem proving capabilities. Our approach generates\n103.25 novel conjectures per seed file on average, providing a scalable\nsolution for creating training data for theorem proving systems. Our system\nsuccessfully verified several non-trivial theorems in topology, including\nproperties of semi-open, alpha-open, and pre-open sets, demonstrating its\npotential for mathematical discovery beyond simple variations of existing\nresults.", "AI": {"tldr": "LeanConjecturer是一个基于LLM的自动化数学猜想生成工具，结合规则和LLM生成定理，解决了形式化定理证明中的数据稀缺问题。", "motivation": "解决形式化定理证明中数据稀缺的挑战，为定理证明系统生成可扩展的训练数据。", "method": "结合规则上下文提取和LLM生成定理，通过迭代生成和评估，验证猜想的语法有效性和非平凡性。", "result": "从40个Mathlib种子文件中生成12,289个猜想，3,776个被验证为有效且非平凡，并成功应用于强化学习优化定理证明能力。", "conclusion": "LeanConjecturer展示了在数学发现中的潜力，能够生成新颖且有意义的猜想，为定理证明系统提供可扩展的数据支持。"}}
{"id": "2506.21559", "pdf": "https://arxiv.org/pdf/2506.21559", "abs": "https://arxiv.org/abs/2506.21559", "authors": ["Junze Chen", "Cheng Yang", "Shujie Li", "Zhiqiang Zhang", "Yawen Li", "Junping Du", "Chuan Shi"], "title": "GraphLAMA: Enabling Efficient Adaptation of Graph Language Models with Limited Annotations", "categories": ["cs.CL"], "comment": null, "summary": "Large language models (LLMs) have demonstrated their strong capabilities in\nvarious domains, and have been recently integrated for graph analysis as graph\nlanguage models (GLMs). With LLMs as the predictor, some GLMs can interpret\nunseen tasks described by natural language, and learn from a few examples in\nthe prompts without parameter tuning, known as in-context learning (ICL).\nAnother subset of GLMs utilizes abundant training labels to enhance model\nperformance, known as instruction tuning. However, we argue that ICL on graphs\nhas effectiveness issues due to fixed parameters and efficiency issues due to\nlong context. Meanwhile, the large amount of labeled data required for\ninstruction tuning can be difficult to obtain in real-world scenarios. To this\nend, we aim to introduce an extra parameter adaptation stage that can\nefficiently tailor GLMs to an unseen graph and task with only a few labeled\nexamples, in exchange for better prediction accuracy and faster inference\nspeed. For implementation, in this paper we propose GraphLAMA method, with its\nmodel backbone and learning schemes specialized for efficient tuning and\ninference. Specifically, for model backbone, we use a graph neural network\n(GNN) with several well-designed components to transform nodes into the\nrepresentation space of LLM tokens. Task instructions can then be represented\nas a mixture of node and language tokens. In the pre-training stage, model\nparameters except the LLM will be trained with different tasks to capture\ngeneral knowledge. In the adaptation stage, only a few pre-trained parameters\nwill be updated based on few-shot examples. Extensive experiments on\nfew/zero-shot node classification and summary generation show that our proposed\nGraphLAMA achieves state-of-the-art performance with 4.91% absolution\nimprovement in accuracy. Compared with ICL, our inference speed can be 10 times\nfaster under 5-shot setting.", "AI": {"tldr": "GraphLAMA方法通过引入参数适应阶段，解决了图语言模型（GLM）在上下文学习（ICL）和指令调优中的效率和效果问题，实现了更高的预测精度和更快的推理速度。", "motivation": "现有的GLMs在ICL中因固定参数和长上下文导致效率低下，而指令调优需要大量标注数据。GraphLAMA旨在通过少量标注样本高效适应新图任务。", "method": "提出GraphLAMA方法，结合GNN和LLM，通过预训练和参数适应阶段优化模型。预训练阶段学习通用知识，适应阶段仅更新少量参数。", "result": "在少样本/零样本节点分类和摘要生成任务中，GraphLAMA实现了4.91%的绝对准确率提升，推理速度比ICL快10倍。", "conclusion": "GraphLAMA通过高效参数适应，显著提升了GLMs的性能和效率，适用于实际场景中的少样本学习。"}}
{"id": "2506.21722", "pdf": "https://arxiv.org/pdf/2506.21722", "abs": "https://arxiv.org/abs/2506.21722", "authors": ["Xin Lu", "Xueyang Fu", "Jie Xiao", "Zihao Fan", "Yurui Zhu", "Zheng-Jun Zha"], "title": "Elucidating and Endowing the Diffusion Training Paradigm for General Image Restoration", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "While diffusion models demonstrate strong generative capabilities in image\nrestoration (IR) tasks, their complex architectures and iterative processes\nlimit their practical application compared to mainstream reconstruction-based\ngeneral ordinary IR networks. Existing approaches primarily focus on optimizing\nnetwork architecture and diffusion paths but overlook the integration of the\ndiffusion training paradigm within general ordinary IR frameworks. To address\nthese challenges, this paper elucidates key principles for adapting the\ndiffusion training paradigm to general IR training through systematic analysis\nof time-step dependencies, network hierarchies, noise-level relationships, and\nmulti-restoration task correlations, proposing a new IR framework supported by\ndiffusion-based training. To enable IR networks to simultaneously restore\nimages and model generative representations, we introduce a series of\nregularization strategies that align diffusion objectives with IR tasks,\nimproving generalization in single-task scenarios. Furthermore, recognizing\nthat diffusion-based generation exerts varying influences across different IR\ntasks, we develop an incremental training paradigm and task-specific adaptors,\nfurther enhancing performance in multi-task unified IR. Experiments demonstrate\nthat our method significantly improves the generalization of IR networks in\nsingle-task IR and achieves superior performance in multi-task unified IR.\nNotably, the proposed framework can be seamlessly integrated into existing\ngeneral IR architectures.", "AI": {"tldr": "本文提出了一种将扩散训练范式融入通用图像修复（IR）框架的方法，通过系统分析时间步依赖、网络层次、噪声级别关系和多任务相关性，优化了IR网络的泛化能力和性能。", "motivation": "扩散模型在图像修复任务中表现出强大的生成能力，但其复杂架构和迭代过程限制了实际应用。现有方法主要关注网络架构和扩散路径优化，而忽视了将扩散训练范式融入通用IR框架。", "method": "通过分析时间步依赖、网络层次等关键因素，提出了一种新的IR框架，并引入正则化策略和对齐扩散目标与IR任务的方法。此外，开发了增量训练范式和任务特定适配器以提升多任务性能。", "result": "实验表明，该方法显著提升了单任务IR的泛化能力，并在多任务统一IR中表现优异。", "conclusion": "所提出的框架可无缝集成到现有通用IR架构中，为扩散训练范式在IR任务中的应用提供了有效解决方案。"}}
{"id": "2506.22056", "pdf": "https://arxiv.org/pdf/2506.22056", "abs": "https://arxiv.org/abs/2506.22056", "authors": ["Xuan Zhang", "Ziyan Jiang", "Rui Meng", "Yifei Leng", "Zhenbang Xiao", "Zora Zhiruo Wang", "Yanyi Shang", "Dehan Kong"], "title": "Universal Retrieval for Multimodal Trajectory Modeling", "categories": ["cs.AI"], "comment": "18 pages, 3 figures, accepted by Workshop on Computer-use Agents @\n  ICML 2025", "summary": "Trajectory data, capturing human actions and environmental states across\nvarious modalities, holds significant potential for enhancing AI agent\ncapabilities, particularly in GUI environments. However, how to model the\nrepresentation of trajectory-level data presents a significant challenge that\nhas not been systematically addressed amid explosive trajectory data growth. In\nthis work, we introduce Multimodal Trajectory Retrieval, bridging the gap\nbetween universal retrieval and agent-centric trajectory modeling. We construct\nthe Unified Agent Trajectory Dataset (UATD) from annotated demonstrations and\nstates across diverse real-world scenarios. Based on this, we present\nGAE-Bench, a benchmark containing a large number of trajectory-based retrieval\npairs. In addition, we propose GAE-Retriever, a multimodal retrieval framework\nthat adopts vision-language models and incorporates optimized contrastive\nlearning through a token selection and the GradCache mechanism. Comprehensive\nevaluations across multiple datasets show that GAE-Retriever consistently\noutperforms strong baselines in retrieval recall, highlighting its\neffectiveness in advancing multimodal trajectory retrieval.", "AI": {"tldr": "论文提出了一种多模态轨迹检索方法（GAE-Retriever），通过构建统一代理轨迹数据集（UATD）和基准测试（GAE-Bench），解决了轨迹数据建模的挑战，并在检索召回率上优于基线方法。", "motivation": "轨迹数据在增强AI代理能力方面潜力巨大，但其建模方法尚未系统化，尤其是在GUI环境中。", "method": "构建UATD数据集和GAE-Bench基准，提出GAE-Retriever框架，结合视觉-语言模型和优化的对比学习（如token选择和GradCache机制）。", "result": "GAE-Retriever在多个数据集上均优于基线方法，显著提升了多模态轨迹检索的召回率。", "conclusion": "该方法有效推动了多模态轨迹检索的发展，为AI代理能力的提升提供了新思路。"}}
{"id": "2506.21560", "pdf": "https://arxiv.org/pdf/2506.21560", "abs": "https://arxiv.org/abs/2506.21560", "authors": ["Yifu Han", "Geo Zhang"], "title": "Reinforcement Learning Fine-Tuning of Language Model for Instruction Following and Math Reasoning", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This study investigates the effectiveness of reinforcement learning (RL)\nfine-tuning techniques on a compact language model (Qwen2.5-0.5B Base) for two\nchallenging tasks: instruction following and mathematical reasoning. We compare\nsupervised fine-tuning (SFT), Direct Preference Optimization (DPO) using\npreference-labeled data, and Reinforce Leave-One-Out (RLOO) with reward models.\nOur experiments show that RLOO with DeBERTa reward modeling achieves the best\nalignment, while DPO provides strong and consistent results. For math reasoing\ntasks, synthetic data augmentation and best-of-N sampling with an external\nverifier significantly improve accuracy, showing the potential of combining\nfine-tuning with inference-time tools. This study highlights key trade-offs and\npractical strategies for training lightweight, task-aligned small-scale\nlanguage models.", "AI": {"tldr": "研究比较了强化学习微调技术在小型语言模型上的效果，发现RLOO结合DeBERTa奖励模型表现最佳，DPO结果稳定，数学任务中数据增强和验证器显著提升准确性。", "motivation": "探索如何通过强化学习微调技术提升小型语言模型在指令跟随和数学推理任务中的表现。", "method": "比较了监督微调（SFT）、直接偏好优化（DPO）和强化学习留一法（RLOO）结合奖励模型的方法，并在数学任务中尝试数据增强和验证器。", "result": "RLOO结合DeBERTa奖励模型表现最佳，DPO结果稳定；数学任务中数据增强和验证器显著提升准确性。", "conclusion": "研究为轻量级任务对齐的小型语言模型训练提供了关键权衡和实用策略。"}}
{"id": "2506.21724", "pdf": "https://arxiv.org/pdf/2506.21724", "abs": "https://arxiv.org/abs/2506.21724", "authors": ["Remco F. Leijenaar", "Hamidreza Kasaei"], "title": "Asymmetric Dual Self-Distillation for 3D Self-Supervised Representation Learning", "categories": ["cs.CV"], "comment": "for associated source code, see\n  https://github.com/RFLeijenaar/AsymDSD", "summary": "Learning semantically meaningful representations from unstructured 3D point\nclouds remains a central challenge in computer vision, especially in the\nabsence of large-scale labeled datasets. While masked point modeling (MPM) is\nwidely used in self-supervised 3D learning, its reconstruction-based objective\ncan limit its ability to capture high-level semantics. We propose AsymDSD, an\nAsymmetric Dual Self-Distillation framework that unifies masked modeling and\ninvariance learning through prediction in the latent space rather than the\ninput space. AsymDSD builds on a joint embedding architecture and introduces\nseveral key design choices: an efficient asymmetric setup, disabling attention\nbetween masked queries to prevent shape leakage, multi-mask sampling, and a\npoint cloud adaptation of multi-crop. AsymDSD achieves state-of-the-art results\non ScanObjectNN (90.53%) and further improves to 93.72% when pretrained on 930k\nshapes, surpassing prior methods.", "AI": {"tldr": "AsymDSD是一种自监督3D学习框架，通过潜在空间预测结合掩码建模和不变性学习，显著提升语义表示能力。", "motivation": "解决无监督3D点云学习中高维语义信息捕获不足的问题。", "method": "采用非对称双自蒸馏框架，结合掩码建模和不变性学习，引入多种设计优化。", "result": "在ScanObjectNN上达到90.53%的准确率，预训练后提升至93.72%。", "conclusion": "AsymDSD在3D点云语义表示学习中表现优异，超越现有方法。"}}
{"id": "2506.22068", "pdf": "https://arxiv.org/pdf/2506.22068", "abs": "https://arxiv.org/abs/2506.22068", "authors": ["Shengyue Yao", "Runqing Guo", "Yangyang Qin", "Miangbing Meng", "Jipeng Cao", "Yilun Lin", "Yisheng Lv", "Fei-Yue Wang"], "title": "Query as Test: An Intelligent Driving Test and Data Storage Method for Integrated Cockpit-Vehicle-Road Scenarios", "categories": ["cs.AI"], "comment": "Submitted to IEEE Transaction on Vehicular Technology", "summary": "With the deep penetration of Artificial Intelligence (AI) in the\ntransportation sector, intelligent cockpits, autonomous driving, and\nintelligent road networks are developing at an unprecedented pace. However, the\ndata ecosystems of these three key areas are increasingly fragmented and\nincompatible. Especially, existing testing methods rely on data stacking, fail\nto cover all edge cases, and lack flexibility. To address this issue, this\npaper introduces the concept of \"Query as Test\" (QaT). This concept shifts the\nfocus from rigid, prescripted test cases to flexible, on-demand logical queries\nagainst a unified data representation. Specifically, we identify the need for a\nfundamental improvement in data storage and representation, leading to our\nproposal of \"Extensible Scenarios Notations\" (ESN). ESN is a novel declarative\ndata framework based on Answer Set Programming (ASP), which uniformly\nrepresents heterogeneous multimodal data from the cockpit, vehicle, and road as\na collection of logical facts and rules. This approach not only achieves deep\nsemantic fusion of data, but also brings three core advantages: (1) supports\ncomplex and flexible semantic querying through logical reasoning; (2) provides\nnatural interpretability for decision-making processes; (3) allows for\non-demand data abstraction through logical rules, enabling fine-grained privacy\nprotection. We further elaborate on the QaT paradigm, transforming the\nfunctional validation and safety compliance checks of autonomous driving\nsystems into logical queries against the ESN database, significantly enhancing\nthe expressiveness and formal rigor of the testing. Finally, we introduce the\nconcept of \"Validation-Driven Development\" (VDD), which suggests to guide\ndevelopments by logical validation rather than quantitative testing in the era\nof Large Language Models, in order to accelerating the iteration and\ndevelopment process.", "AI": {"tldr": "论文提出“Query as Test”（QaT）概念，通过逻辑查询替代传统测试方法，并引入“Extensible Scenarios Notations”（ESN）数据框架，实现多模态数据的语义融合和灵活测试。", "motivation": "解决交通领域AI应用中数据碎片化、测试方法僵化及边缘案例覆盖不足的问题。", "method": "基于Answer Set Programming（ASP）的ESN框架，将异构数据统一表示为逻辑事实和规则，支持语义查询和隐私保护。", "result": "ESN框架支持复杂语义查询、决策过程可解释性及细粒度隐私保护；QaT范式提升测试的表达力和形式严谨性。", "conclusion": "提出“Validation-Driven Development”（VDD），以逻辑验证指导开发，加速迭代过程。"}}
{"id": "2506.21561", "pdf": "https://arxiv.org/pdf/2506.21561", "abs": "https://arxiv.org/abs/2506.21561", "authors": ["Emilio Barkett", "Olivia Long", "Madhavendra Thakur"], "title": "Reasoning Isn't Enough: Examining Truth-Bias and Sycophancy in LLMs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Despite their widespread use in fact-checking, moderation, and high-stakes\ndecision-making, large language models (LLMs) remain poorly understood as\njudges of truth. This study presents the largest evaluation to date of LLMs'\nveracity detection capabilities and the first analysis of these capabilities in\nreasoning models. We had eight LLMs make 4,800 veracity judgments across\nseveral prompts, comparing reasoning and non-reasoning models. We find that\nrates of truth-bias, or the likelihood to believe a statement is true,\nregardless of whether it is actually true, are lower in reasoning models than\nin non-reasoning models, but still higher than human benchmarks. Most\nconcerning, we identify sycophantic tendencies in several advanced models\n(o4-mini and GPT-4.1 from OpenAI, R1 from DeepSeek), which displayed an\nasymmetry in detection accuracy, performing well in truth accuracy but poorly\nin deception accuracy. This suggests that capability advances alone do not\nresolve fundamental veracity detection challenges in LLMs.", "AI": {"tldr": "研究评估了大型语言模型（LLMs）的真实性检测能力，发现推理模型比非推理模型更少倾向于相信陈述为真，但仍高于人类基准。部分先进模型表现出不对称的准确性，表明能力提升未能解决根本的真实性检测挑战。", "motivation": "尽管LLMs广泛应用于事实核查和决策，但其作为真相判断工具的能力仍未被充分理解。", "method": "研究让8个LLMs对4,800个陈述进行真实性判断，比较了推理和非推理模型的表现。", "result": "推理模型的真相偏见率低于非推理模型，但仍高于人类基准。部分先进模型在真相检测上表现良好，但在欺骗检测上表现不佳。", "conclusion": "LLMs的能力提升未能解决其真实性检测的根本问题，尤其是欺骗检测的挑战。"}}
{"id": "2506.21731", "pdf": "https://arxiv.org/pdf/2506.21731", "abs": "https://arxiv.org/abs/2506.21731", "authors": ["Chenqiu Zhao", "Anup Basu"], "title": "Exploring Image Generation via Mutually Exclusive Probability Spaces and Local Correlation Hypothesis", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "We propose two theoretical frameworks, the Mutually Exclusive Probability\nSpace (MESP) and the Local Correlation Hypothesis (LCH), to explore a potential\nlimitation in probabilistic generative models; namely that learning global\ndistributions leads to memorization rather than generative behavior. MESP\nemerges from our rethinking of the Variational Autoencoder (VAE). We observe\nthat latent variable distributions in VAE exhibit overlap, which leads to an\noptimization conflict between the reconstruction loss and KL-divergence loss. A\nlower bound based on the overlap coefficient is proposed. We refer to this\nphenomenon as Mutually Exclusive Probability Spaces. Based on MESP, a Binary\nLatent Autoencoder (BL-AE) is proposed to encode images into binary latent\nrepresentations. These binary latents are used as the input to our\nAutoregressive Random Variable Model (ARVM), a modified autoregressive model\noutputting histograms. Our ARVM achieves competitive FID scores, outperforming\nstate-of-the-art methods on standard datasets. However, such scores reflect\nmemorization rather than generation. To address this issue, we propose the\nLocal Correlation Hypothesis (LCH), which posits that generative capability\narising from local correlations among latent variables. Comprehensive\nexperiments and discussions are conducted to validate our frameworks.", "AI": {"tldr": "论文提出了两种理论框架（MESP和LCH），探讨概率生成模型的局限性，即全局分布学习导致记忆而非生成行为。MESP源自对VAE的重新思考，提出基于重叠系数的下界，并设计了BL-AE和ARVM模型。LCH假设生成能力源于局部相关性。", "motivation": "研究概率生成模型中全局分布学习导致记忆而非生成行为的局限性。", "method": "提出MESP框架和LCH假设，设计BL-AE和ARVM模型，通过实验验证。", "result": "ARVM在标准数据集上取得竞争性FID分数，但发现高分反映记忆而非生成。", "conclusion": "MESP和LCH为理解生成模型的局限性提供了新视角，局部相关性可能是生成能力的关键。"}}
{"id": "2506.22183", "pdf": "https://arxiv.org/pdf/2506.22183", "abs": "https://arxiv.org/abs/2506.22183", "authors": ["Camille François", "Ludovic Péran", "Ayah Bdeir", "Nouha Dziri", "Will Hawkins", "Yacine Jernite", "Sayash Kapoor", "Juliet Shen", "Heidy Khlaaf", "Kevin Klyman", "Nik Marda", "Marie Pellat", "Deb Raji", "Divya Siddarth", "Aviya Skowron", "Joseph Spisak", "Madhulika Srikumar", "Victor Storchan", "Audrey Tang", "Jen Weedon"], "title": "A Different Approach to AI Safety: Proceedings from the Columbia Convening on Openness in Artificial Intelligence and AI Safety", "categories": ["cs.AI"], "comment": "Proceedings from the Columbia Convening on Openness in Artificial\n  Intelligence and AI Safety", "summary": "The rapid rise of open-weight and open-source foundation models is\nintensifying the obligation and reshaping the opportunity to make AI systems\nsafe. This paper reports outcomes from the Columbia Convening on AI Openness\nand Safety (San Francisco, 19 Nov 2024) and its six-week preparatory programme\ninvolving more than forty-five researchers, engineers, and policy leaders from\nacademia, industry, civil society, and government. Using a participatory,\nsolutions-oriented process, the working groups produced (i) a research agenda\nat the intersection of safety and open source AI; (ii) a mapping of existing\nand needed technical interventions and open source tools to safely and\nresponsibly deploy open foundation models across the AI development workflow;\nand (iii) a mapping of the content safety filter ecosystem with a proposed\nroadmap for future research and development. We find that openness --\nunderstood as transparent weights, interoperable tooling, and public governance\n-- can enhance safety by enabling independent scrutiny, decentralized\nmitigation, and culturally plural oversight. However, significant gaps persist:\nscarce multimodal and multilingual benchmarks, limited defenses against\nprompt-injection and compositional attacks in agentic systems, and insufficient\nparticipatory mechanisms for communities most affected by AI harms. The paper\nconcludes with a roadmap of five priority research directions, emphasizing\nparticipatory inputs, future-proof content filters, ecosystem-wide safety\ninfrastructure, rigorous agentic safeguards, and expanded harm taxonomies.\nThese recommendations informed the February 2025 French AI Action Summit and\nlay groundwork for an open, plural, and accountable AI safety discipline.", "AI": {"tldr": "论文探讨了开源基础模型对AI安全的影响，提出了研究议程、技术干预和安全工具，并指出了开放性如何增强安全性，但也存在多模态和多语言基准不足等问题。", "motivation": "研究开源AI模型对系统安全的影响，推动透明、多元和负责任的AI安全发展。", "method": "通过参与式、解决方案导向的过程，组织工作小组制定研究议程、技术干预和安全工具。", "result": "提出了五项优先研究方向，包括参与式输入、未来内容过滤器、生态系统安全基础设施等。", "conclusion": "开放性可增强AI安全性，但仍需填补多模态基准等空白，为开放、多元和负责任的AI安全学科奠定基础。"}}
{"id": "2506.21562", "pdf": "https://arxiv.org/pdf/2506.21562", "abs": "https://arxiv.org/abs/2506.21562", "authors": ["Jun Yin", "Pengyu Zeng", "Jing Zhong", "Peilin Li", "Miao Zhang", "Ran Luo", "Shuai Lu"], "title": "FloorPlan-DeepSeek (FPDS): A multimodal approach to floorplan generation using vector-based next room prediction", "categories": ["cs.CL", "cs.AI", "cs.AR"], "comment": null, "summary": "In the architectural design process, floor plan generation is inherently\nprogressive and iterative. However, existing generative models for floor plans\nare predominantly end-to-end generation that produce an entire pixel-based\nlayout in a single pass. This paradigm is often incompatible with the\nincremental workflows observed in real-world architectural practice. To address\nthis issue, we draw inspiration from the autoregressive 'next token prediction'\nmechanism commonly used in large language models, and propose a novel 'next\nroom prediction' paradigm tailored to architectural floor plan modeling.\nExperimental evaluation indicates that FPDS demonstrates competitive\nperformance in comparison to diffusion models and Tell2Design in the\ntext-to-floorplan task, indicating its potential applicability in supporting\nfuture intelligent architectural design.", "AI": {"tldr": "论文提出了一种基于‘下一个房间预测’的渐进式生成方法，用于建筑设计中的平面图生成，解决了传统端到端生成方法与实际工作流程不兼容的问题。", "motivation": "现有平面图生成模型多为端到端生成，与实际建筑设计中渐进式、迭代式的工作流程不兼容。", "method": "借鉴大型语言模型中的自回归‘下一个标记预测’机制，提出‘下一个房间预测’范式。", "result": "实验表明，FPDS在文本到平面图任务中表现优于扩散模型和Tell2Design。", "conclusion": "该方法有望支持未来智能建筑设计。"}}
{"id": "2506.21735", "pdf": "https://arxiv.org/pdf/2506.21735", "abs": "https://arxiv.org/abs/2506.21735", "authors": ["Nick Lemke", "Mirko Konstantin", "Henry John Krumb", "John Kalkhof", "Jonathan Stieber", "Anirban Mukhopadhyay"], "title": "Equitable Federated Learning with NCA", "categories": ["cs.CV"], "comment": null, "summary": "Federated Learning (FL) is enabling collaborative model training across\ninstitutions without sharing sensitive patient data. This approach is\nparticularly valuable in low- and middle-income countries (LMICs), where access\nto trained medical professionals is limited. However, FL adoption in LMICs\nfaces significant barriers, including limited high-performance computing\nresources and unreliable internet connectivity. To address these challenges, we\nintroduce FedNCA, a novel FL system tailored for medical image segmentation\ntasks. FedNCA leverages the lightweight Med-NCA architecture, enabling training\non low-cost edge devices, such as widely available smartphones, while\nminimizing communication costs. Additionally, our encryption-ready FedNCA\nproves to be suitable for compromised network communication. By overcoming\ninfrastructural and security challenges, FedNCA paves the way for inclusive,\nefficient, lightweight, and encryption-ready medical imaging solutions,\nfostering equitable healthcare advancements in resource-constrained regions.", "AI": {"tldr": "FedNCA是一个专为医疗图像分割任务设计的联邦学习系统，适用于资源有限的地区，解决了计算资源和网络连接问题。", "motivation": "在低收入和中等收入国家（LMICs），医疗专业人员有限，联邦学习（FL）可以协作训练模型而不共享敏感数据，但面临计算资源和网络连接问题。", "method": "提出FedNCA系统，基于轻量级Med-NCA架构，支持低成本边缘设备（如智能手机）训练，并减少通信成本，同时具备加密能力。", "result": "FedNCA克服了基础设施和安全挑战，为资源受限地区提供了高效、轻量级且支持加密的医疗成像解决方案。", "conclusion": "FedNCA为资源受限地区实现公平医疗进步提供了可行路径。"}}
{"id": "2506.22271", "pdf": "https://arxiv.org/pdf/2506.22271", "abs": "https://arxiv.org/abs/2506.22271", "authors": ["Samy Badreddine", "Emile van Krieken", "Luciano Serafini"], "title": "Breaking Rank Bottlenecks in Knowledge Graph Completion", "categories": ["cs.AI", "cs.LG"], "comment": null, "summary": "Many Knowledge Graph Completion (KGC) models, despite using powerful\nencoders, rely on a simple vector-matrix multiplication to score queries\nagainst candidate object entities. When the number of entities is larger than\nthe model's embedding dimension, which in practical scenarios is often by\nseveral orders of magnitude, we have a linear output layer with a rank\nbottleneck. Such bottlenecked layers limit model expressivity. We investigate\nboth theoretically and empirically how rank bottlenecks affect KGC models. We\nfind that, by limiting the set of feasible predictions, rank bottlenecks hurt\nranking accuracy and the distribution fidelity of scores. Inspired by the\nlanguage modelling literature, we propose KGE-MoS, a mixture-based output layer\nto break rank bottlenecks in many KGC models. Our experiments on four datasets\nshow that KGE-MoS improves performance and probabilistic fit of KGC models for\na low parameter cost.", "AI": {"tldr": "论文探讨了知识图谱补全（KGC）模型中由于输出层秩瓶颈导致的性能限制，并提出了一种基于混合的输出层（KGE-MoS）以提升模型表现。", "motivation": "现有KGC模型在实体数量远大于嵌入维度时，输出层会出现秩瓶颈，限制了模型的表达能力和预测准确性。", "method": "提出KGE-MoS，一种基于混合的输出层方法，通过打破秩瓶颈来提升模型表现。", "result": "在四个数据集上的实验表明，KGE-MoS以较低参数成本显著提升了KGC模型的性能和概率拟合度。", "conclusion": "KGE-MoS是一种有效的解决方案，能够缓解秩瓶颈问题并提升KGC模型的整体表现。"}}
{"id": "2506.21563", "pdf": "https://arxiv.org/pdf/2506.21563", "abs": "https://arxiv.org/abs/2506.21563", "authors": ["Kaiying Kevin Lin", "Hsiyu Chen", "Haopeng Zhang"], "title": "FormosanBench: Benchmarking Low-Resource Austronesian Languages in the Era of Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "While large language models (LLMs) have demonstrated impressive performance\nacross a wide range of natural language processing (NLP) tasks in high-resource\nlanguages, their capabilities in low-resource and minority languages remain\nsignificantly underexplored. Formosan languages -- a subgroup of Austronesian\nlanguages spoken in Taiwan -- are both linguistically rich and endangered,\nlargely due to the sociolinguistic dominance of Mandarin. In this work, we\nintroduce FORMOSANBENCH, the first benchmark for evaluating LLMs on\nlow-resource Austronesian languages. It covers three endangered Formosan\nlanguages: Atayal, Amis, and Paiwan, across three core NLP tasks: machine\ntranslation, automatic speech recognition (ASR), and text summarization. We\nassess model performance in zero-shot, 10-shot, and fine-tuned settings using\nFORMOSANBENCH. Our results reveal a substantial performance gap between\nhigh-resource and Formosan languages. Existing LLMs consistently underperform\nacross all tasks, with 10-shot learning and fine-tuning offering only limited\nimprovements. These findings underscore the urgent need for more inclusive NLP\ntechnologies that can effectively support endangered and underrepresented\nlanguages. We release our datasets and code to facilitate future research in\nthis direction.", "AI": {"tldr": "FORMOSANBENCH是首个评估大语言模型在低资源南岛语系语言（如阿美语、泰雅语和排湾语）表现的基准测试，结果显示现有模型在这些语言上表现不佳，强调了支持濒危语言的紧迫性。", "motivation": "探索大语言模型在低资源和濒危语言（如台湾南岛语系语言）中的表现，填补现有研究的空白。", "method": "引入FORMOSANBENCH基准，涵盖三种濒危语言和三项核心NLP任务（机器翻译、语音识别和文本摘要），评估零样本、10样本和微调设置下的模型性能。", "result": "现有模型在低资源语言上表现显著落后，10样本学习和微调仅带来有限改进。", "conclusion": "亟需开发更具包容性的NLP技术以支持濒危语言，并公开数据集和代码以促进相关研究。"}}
{"id": "2506.21742", "pdf": "https://arxiv.org/pdf/2506.21742", "abs": "https://arxiv.org/abs/2506.21742", "authors": ["Sirnam Swetha", "Rohit Gupta", "Parth Parag Kulkarni", "David G Shatwell", "Jeffrey A Chan Santiago", "Nyle Siddiqui", "Joseph Fioresi", "Mubarak Shah"], "title": "ImplicitQA: Going beyond frames towards Implicit Video Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "Video QA has made significant strides by leveraging multimodal learning to\nalign visual and textual modalities. However, current benchmarks overwhelmingly\nfocus on questions answerable through explicit visual content - actions,\nobjects & events directly observable within individual frames or short clips.\nIn contrast, creative and cinematic videos - such as movies, TV shows, and\nnarrative-driven content - employ storytelling techniques that deliberately\nomit certain depictions, requiring viewers to infer motives, causality, and\nrelationships across discontinuous frames. Humans naturally excel at such\nimplicit reasoning, seamlessly integrating information across time and context\nto construct coherent narratives. Current VideoQA systems and benchmarks fail\nto capture this essential dimension of human-like understanding. To bridge this\ngap, we present ImplicitQA, a novel benchmark specifically designed to test\nmodels on implicit reasoning. It comprises 1K meticulously annotated QA pairs\nderived from 320+ high-quality creative video clips, systematically categorized\ninto key reasoning dimensions: lateral and vertical spatial reasoning, depth\nand proximity, viewpoint and visibility, motion and trajectory, causal and\nmotivational reasoning, social interactions, physical context, and inferred\ncounting. These annotations are deliberately challenging, crafted by authors\nensuring high-quality. Our extensive evaluations on leading VideoQA models\nreveals performance degradation, underscoring their reliance on surface-level\nvisual cues and highlighting the difficulty of implicit reasoning. Performance\nvariations across models further illustrate the complexity and diversity of the\nchallenges presented by ImplicitQA. By releasing both the dataset and our data\ncollection framework, we aim to stimulate further research and development in\nthe community. https://huggingface.co/datasets/ucf-crcv/ImplicitQA.", "AI": {"tldr": "论文提出了ImplicitQA基准，用于测试视频问答模型在隐式推理上的能力，填补了现有基准的不足。", "motivation": "现有视频问答基准主要关注显式视觉内容，而忽略了隐式推理（如动机、因果关系等），无法体现人类的理解能力。", "method": "构建了ImplicitQA基准，包含1K高质量标注的QA对，涵盖多种隐式推理维度。", "result": "评估显示现有模型在隐式推理任务上表现不佳，依赖表面视觉线索。", "conclusion": "ImplicitQA为视频问答研究提供了新方向，推动模型向人类理解能力发展。"}}
{"id": "2506.22276", "pdf": "https://arxiv.org/pdf/2506.22276", "abs": "https://arxiv.org/abs/2506.22276", "authors": ["Reuth Mirsky"], "title": "Artificial Intelligent Disobedience: Rethinking the Agency of Our Artificial Teammates", "categories": ["cs.AI"], "comment": "Extended version of a paper accepted for publication in AI Magazine", "summary": "Artificial intelligence has made remarkable strides in recent years,\nachieving superhuman performance across a wide range of tasks. Yet despite\nthese advances, most cooperative AI systems remain rigidly obedient, designed\nto follow human instructions without question and conform to user expectations,\neven when doing so may be counterproductive or unsafe. This paper argues for\nexpanding the agency of AI teammates to include \\textit{intelligent\ndisobedience}, empowering them to make meaningful and autonomous contributions\nwithin human-AI teams. It introduces a scale of AI agency levels and uses\nrepresentative examples to highlight the importance and growing necessity of\ntreating AI autonomy as an independent research focus in cooperative settings.\nThe paper then explores how intelligent disobedience manifests across different\nautonomy levels and concludes by proposing initial boundaries and\nconsiderations for studying disobedience as a core capability of artificial\nagents.", "AI": {"tldr": "论文主张赋予AI队友“智能不服从”能力，使其在人类-AI团队中发挥自主贡献。", "motivation": "当前合作型AI系统过于服从，可能不利于效率或安全，需研究AI自主性。", "method": "提出AI代理级别量表，并通过案例说明智能不服从的重要性。", "result": "探讨不同自主级别下智能不服从的表现，并初步提出研究边界。", "conclusion": "建议将不服从作为AI核心能力进行研究，并明确初步研究框架。"}}
{"id": "2506.21564", "pdf": "https://arxiv.org/pdf/2506.21564", "abs": "https://arxiv.org/abs/2506.21564", "authors": ["Jiyan Liu", "Youzheng Liu", "Taihang Wang", "Xiaoman Xu", "Yimin Wang", "Ye Jiang"], "title": "Team QUST at SemEval-2025 Task 10: Evaluating Large Language Models in Multiclass Multi-label Classification of News Entity Framing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "This paper describes the participation of QUST_NLP in the SemEval-2025 Task\n7. We propose a three-stage retrieval framework specifically designed for\nfact-checked claim retrieval. Initially, we evaluate the performance of several\nretrieval models and select the one that yields the best results for candidate\nretrieval. Next, we employ multiple re-ranking models to enhance the candidate\nresults, with each model selecting the Top-10 outcomes. In the final stage, we\nutilize weighted voting to determine the final retrieval outcomes. Our approach\nachieved 5th place in the monolingual track and 7th place in the crosslingual\ntrack. We release our system code at:\nhttps://github.com/warmth27/SemEval2025_Task7.", "AI": {"tldr": "QUST_NLP团队在SemEval-2025任务7中提出了一种三阶段检索框架，用于事实核查声明检索，取得了单语和跨语言赛道的第5和第7名。", "motivation": "设计一个高效的事实核查声明检索框架，以提升检索性能。", "method": "三阶段框架：1) 评估并选择最佳检索模型；2) 使用多个重排序模型优化候选结果；3) 加权投票确定最终结果。", "result": "在单语赛道排名第5，跨语言赛道排名第7。", "conclusion": "提出的三阶段框架有效提升了事实核查声明的检索性能，代码已开源。"}}
{"id": "2506.21770", "pdf": "https://arxiv.org/pdf/2506.21770", "abs": "https://arxiv.org/abs/2506.21770", "authors": ["Rishiraj Paul Chowdhury", "Nirmit Shekar Karkera"], "title": "Early Glaucoma Detection using Deep Learning with Multiple Datasets of Fundus Images", "categories": ["cs.CV", "cs.LG"], "comment": "13 pages, 6 figures, prepared for course CSCI 5922 at University of\n  Colorado Boulder. Code available upon request, dataset taken from Kaggle", "summary": "Glaucoma is a leading cause of irreversible blindness, but early detection\ncan significantly improve treatment outcomes. Traditional diagnostic methods\nare often invasive and require specialized equipment. In this work, we present\na deep learning pipeline using the EfficientNet-B0 architecture for glaucoma\ndetection from retinal fundus images. Unlike prior studies that rely on single\ndatasets, we sequentially train and fine-tune our model across ACRIMA, ORIGA,\nand RIM-ONE datasets to enhance generalization. Our experiments show that\nminimal preprocessing yields higher AUC-ROC compared to more complex\nenhancements, and our model demonstrates strong discriminative performance on\nunseen datasets. The proposed pipeline offers a reproducible and scalable\napproach to early glaucoma detection, supporting its potential clinical\nutility.", "AI": {"tldr": "提出了一种基于EfficientNet-B0架构的深度学习流程，用于从视网膜眼底图像中检测青光眼，通过多数据集训练提高泛化能力，结果表明预处理简单效果更好。", "motivation": "青光眼是导致不可逆失明的主要原因，早期检测可显著改善治疗效果，传统方法通常具有侵入性且需要专业设备。", "method": "使用EfficientNet-B0架构，通过ACRIMA、ORIGA和RIM-ONE数据集顺序训练和微调模型，增强泛化能力。", "result": "实验表明，简单预处理比复杂增强效果更好，模型在未见数据集上表现出强判别性能。", "conclusion": "该流程为青光眼早期检测提供了可重复且可扩展的方法，具有潜在临床价值。"}}
{"id": "2506.22309", "pdf": "https://arxiv.org/pdf/2506.22309", "abs": "https://arxiv.org/abs/2506.22309", "authors": ["Klara M. Gutekunst", "Dominik Dürrschnabel", "Johannes Hirth", "Gerd Stumme"], "title": "Conceptual Topic Aggregation", "categories": ["cs.AI", "cs.CL", "cs.DM", "cs.LG", "06B99", "I.2.4; I.2.7"], "comment": "16 pages, 4 tables, 11 figures, International Joint Conference on\n  Conceptual Knowledge Structures", "summary": "The vast growth of data has rendered traditional manual inspection\ninfeasible, necessitating the adoption of computational methods for efficient\ndata exploration. Topic modeling has emerged as a powerful tool for analyzing\nlarge-scale textual datasets, enabling the extraction of latent semantic\nstructures. However, existing methods for topic modeling often struggle to\nprovide interpretable representations that facilitate deeper insights into data\nstructure and content. In this paper, we propose FAT-CAT, an approach based on\nFormal Concept Analysis (FCA) to enhance meaningful topic aggregation and\nvisualization of discovered topics. Our approach can handle diverse topics and\nfile types -- grouped by directories -- to construct a concept lattice that\noffers a structured, hierarchical representation of their topic distribution.\nIn a case study on the ETYNTKE dataset, we evaluate the effectiveness of our\napproach against other representation methods to demonstrate that FCA-based\naggregation provides more meaningful and interpretable insights into dataset\ncomposition than existing topic modeling techniques.", "AI": {"tldr": "提出了一种基于形式概念分析（FCA）的FAT-CAT方法，用于改进主题建模的可解释性和可视化效果。", "motivation": "传统主题建模方法难以提供可解释的表示，限制了数据结构和内容的深入理解。", "method": "利用FCA构建概念格，实现主题的层次化聚合和可视化。", "result": "在ETYNTKE数据集上的实验表明，FAT-CAT比现有方法提供更直观和有意义的主题表示。", "conclusion": "FCA为基础的方法在主题建模中具有更高的可解释性和实用性。"}}
{"id": "2506.21565", "pdf": "https://arxiv.org/pdf/2506.21565", "abs": "https://arxiv.org/abs/2506.21565", "authors": ["Takato Ueno", "Keito Inoshita"], "title": "A Multi-Agent Probabilistic Inference Framework Inspired by Kairanban-Style CoT System with IdoBata Conversation for Debiasing", "categories": ["cs.CL", "cs.LG", "stat.ML"], "comment": null, "summary": "Japan's kairanban culture and idobata conversations have long functioned as\ntraditional communication practices that foster nuanced dialogue among\ncommunity members and contribute to the formation of social balance. Inspired\nby these information exchange processes, this study proposes a multi-agent\ninference framework (KCS+IBC) that integrates multiple large language models\n(LLMs) to achieve bias mitigation, improved explainability, and probabilistic\nprediction in sentiment analysis. In addition to sequentially sharing\nprediction results, the proposed method incorporates a mid-phase casual\ndialogue session to blend formal inference with individual perspectives and\nintroduces probabilistic sentiment prediction. Experimental results show that\nKCS achieves accuracy comparable to that of a single LLM across datasets, while\nKCS+IBC exhibits a consistent decrease in entropy and a gradual increase in\nvariance during the latter stages of inference, suggesting the framework's\nability to balance aggregation and diversity of predictions. Future work will\nquantitatively assess the impact of these characteristics on bias correction\nand aim to develop more advanced sentiment analysis systems.", "AI": {"tldr": "本研究提出了一种多智能体推理框架（KCS+IBC），结合多个大语言模型（LLM），通过传统日本信息交流方式（如回覧板文化和井端会话）的启发，实现情感分析中的偏见缓解、可解释性提升和概率预测。", "motivation": "受日本传统信息交流方式（如回覧板文化和井端会话）的启发，旨在通过多智能体协作改善情感分析的偏见和多样性问题。", "method": "提出KCS+IBC框架，结合多个LLM，通过中间阶段的非正式对话环节和概率情感预测，平衡正式推理与个体视角。", "result": "实验显示KCS在准确性上与单一LLM相当，而KCS+IBC在推理后期熵值降低、方差增加，表明其能平衡预测的聚合与多样性。", "conclusion": "未来工作将量化这些特性对偏见修正的影响，并开发更先进的情感分析系统。"}}
{"id": "2506.21785", "pdf": "https://arxiv.org/pdf/2506.21785", "abs": "https://arxiv.org/abs/2506.21785", "authors": ["Daniel Wen"], "title": "Comparing Learning Paradigms for Egocentric Video Summarization", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "In this study, we investigate various computer vision paradigms - supervised\nlearning, unsupervised learning, and prompt fine-tuning - by assessing their\nability to understand and interpret egocentric video data. Specifically, we\nexamine Shotluck Holmes (state-of-the-art supervised learning), TAC-SUM\n(state-of-the-art unsupervised learning), and GPT-4o (a prompt fine-tuned\npre-trained model), evaluating their effectiveness in video summarization. Our\nresults demonstrate that current state-of-the-art models perform less\neffectively on first-person videos compared to third-person videos,\nhighlighting the need for further advancements in the egocentric video domain.\nNotably, a prompt fine-tuned general-purpose GPT-4o model outperforms these\nspecialized models, emphasizing the limitations of existing approaches in\nadapting to the unique challenges of first-person perspectives. Although our\nevaluation is conducted on a small subset of egocentric videos from the\nEgo-Exo4D dataset due to resource constraints, the primary objective of this\nresearch is to provide a comprehensive proof-of-concept analysis aimed at\nadvancing the application of computer vision techniques to first-person videos.\nBy exploring novel methodologies and evaluating their potential, we aim to\ncontribute to the ongoing development of models capable of effectively\nprocessing and interpreting egocentric perspectives.", "AI": {"tldr": "比较监督学习、无监督学习和提示微调在自我中心视频理解中的表现，发现GPT-4o优于专用模型，但现有方法仍需改进。", "motivation": "探索计算机视觉技术在自我中心视频中的应用，推动该领域的发展。", "method": "评估Shotluck Holmes、TAC-SUM和GPT-4o在视频摘要任务中的表现。", "result": "GPT-4o表现最佳，但现有模型在自我中心视频中效果较差。", "conclusion": "需进一步研究以提升自我中心视频的处理能力，提示微调方法有潜力。"}}
{"id": "2506.22355", "pdf": "https://arxiv.org/pdf/2506.22355", "abs": "https://arxiv.org/abs/2506.22355", "authors": ["Pascale Fung", "Yoram Bachrach", "Asli Celikyilmaz", "Kamalika Chaudhuri", "Delong Chen", "Willy Chung", "Emmanuel Dupoux", "Hervé Jégou", "Alessandro Lazaric", "Arjun Majumdar", "Andrea Madotto", "Franziska Meier", "Florian Metze", "Théo Moutakanni", "Juan Pino", "Basile Terver", "Joseph Tighe", "Jitendra Malik"], "title": "Embodied AI Agents: Modeling the World", "categories": ["cs.AI"], "comment": null, "summary": "This paper describes our research on AI agents embodied in visual, virtual or\nphysical forms, enabling them to interact with both users and their\nenvironments. These agents, which include virtual avatars, wearable devices,\nand robots, are designed to perceive, learn and act within their surroundings,\nwhich makes them more similar to how humans learn and interact with the\nenvironments as compared to disembodied agents. We propose that the development\nof world models is central to reasoning and planning of embodied AI agents,\nallowing these agents to understand and predict their environment, to\nunderstand user intentions and social contexts, thereby enhancing their ability\nto perform complex tasks autonomously. World modeling encompasses the\nintegration of multimodal perception, planning through reasoning for action and\ncontrol, and memory to create a comprehensive understanding of the physical\nworld. Beyond the physical world, we also propose to learn the mental world\nmodel of users to enable better human-agent collaboration.", "AI": {"tldr": "研究探讨了具身AI代理（如虚拟化身、可穿戴设备和机器人）如何通过世界模型提升感知、学习和行动能力，以更接近人类的方式与环境互动。", "motivation": "旨在通过具身AI代理更自然地与环境和用户交互，解决非具身代理在复杂任务中的局限性。", "method": "提出世界模型为核心框架，整合多模态感知、推理规划和记忆，同时学习用户的心理世界模型以优化协作。", "result": "具身AI代理能够更全面地理解和预测环境，提升自主执行复杂任务的能力。", "conclusion": "世界模型是具身AI代理实现高效推理、规划和协作的关键，未来可进一步优化人机交互。"}}
{"id": "2506.21566", "pdf": "https://arxiv.org/pdf/2506.21566", "abs": "https://arxiv.org/abs/2506.21566", "authors": ["Arwa Arif"], "title": "The Saturation Point of Backtranslation in High Quality Low Resource English Gujarati Machine Translation", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Preprint, 8 Pages", "summary": "Backtranslation BT is widely used in low resource machine translation MT to\ngenerate additional synthetic training data using monolingual corpora. While\nthis approach has shown strong improvements for many language pairs, its\neffectiveness in high quality, low resource settings remains unclear. In this\nwork, we explore the effectiveness of backtranslation for English Gujarati\ntranslation using the multilingual pretrained MBART50 model. Our baseline\nsystem, trained on a high quality parallel corpus of approximately 50,000\nsentence pairs, achieves a BLEU score of 43.8 on a validation set. We augment\nthis data with carefully filtered backtranslated examples generated from\nmonolingual Gujarati text. Surprisingly, adding this synthetic data does not\nimprove translation performance and, in some cases, slightly reduces it. We\nevaluate our models using multiple metrics like BLEU, ChrF++, TER, BLEURT and\nanalyze possible reasons for this saturation. Our findings suggest that\nbacktranslation may reach a point of diminishing returns in certain\nlow-resource settings and we discuss implications for future research.", "AI": {"tldr": "研究探讨了反向翻译（BT）在英语-古吉拉特语低资源机器翻译中的效果，发现添加合成数据并未提升性能，甚至略有下降。", "motivation": "探索反向翻译在高质量低资源语言对（英语-古吉拉特语）中的有效性。", "method": "使用MBART50模型生成反向翻译数据，并基于高质量平行语料库（约50,000句对）进行训练和验证。", "result": "添加反向翻译数据未提升BLEU分数（基线43.8），反而在某些情况下略有下降。", "conclusion": "反向翻译在某些低资源场景中可能收益递减，需进一步研究。"}}
{"id": "2506.21813", "pdf": "https://arxiv.org/pdf/2506.21813", "abs": "https://arxiv.org/abs/2506.21813", "authors": ["Felix Holm", "Gözde Ünver", "Ghazal Ghazaei", "Nassir Navab"], "title": "CAT-SG: A Large Dynamic Scene Graph Dataset for Fine-Grained Understanding of Cataract Surgery", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Understanding the intricate workflows of cataract surgery requires modeling\ncomplex interactions between surgical tools, anatomical structures, and\nprocedural techniques. Existing datasets primarily address isolated aspects of\nsurgical analysis, such as tool detection or phase segmentation, but lack\ncomprehensive representations that capture the semantic relationships between\nentities over time. This paper introduces the Cataract Surgery Scene Graph\n(CAT-SG) dataset, the first to provide structured annotations of tool-tissue\ninteractions, procedural variations, and temporal dependencies. By\nincorporating detailed semantic relations, CAT-SG offers a holistic view of\nsurgical workflows, enabling more accurate recognition of surgical phases and\ntechniques. Additionally, we present a novel scene graph generation model,\nCatSGG, which outperforms current methods in generating structured surgical\nrepresentations. The CAT-SG dataset is designed to enhance AI-driven surgical\ntraining, real-time decision support, and workflow analysis, paving the way for\nmore intelligent, context-aware systems in clinical practice.", "AI": {"tldr": "该论文介绍了首个白内障手术场景图数据集（CAT-SG），用于捕捉手术工具与组织间的语义关系，并提出了一种新的场景图生成模型CatSGG，性能优于现有方法。", "motivation": "现有数据集仅关注手术分析的孤立方面（如工具检测或阶段分割），缺乏对实体间语义关系的全面建模。", "method": "提出CAT-SG数据集，结构化标注工具-组织交互、程序变化和时间依赖关系；并开发CatSGG模型生成结构化手术表示。", "result": "CAT-SG提供了手术工作流的整体视图，CatSGG模型在生成结构化表示方面优于现有方法。", "conclusion": "CAT-SG数据集和CatSGG模型为AI驱动的手术培训、实时决策支持和工作流分析提供了更智能的解决方案。"}}
{"id": "2506.22358", "pdf": "https://arxiv.org/pdf/2506.22358", "abs": "https://arxiv.org/abs/2506.22358", "authors": ["Varvara Kalokyri", "Nikolaos S. Tachos", "Charalampos N. Kalantzopoulos", "Stelios Sfakianakis", "Haridimos Kondylakis", "Dimitrios I. Zaridis", "Sara Colantonio", "Daniele Regge", "Nikolaos Papanikolaou", "The ProCAncer-I consortium", "Konstantinos Marias", "Dimitrios I. Fotiadis", "Manolis Tsiknakis"], "title": "AI Model Passport: Data and System Traceability Framework for Transparent AI in Health", "categories": ["cs.AI"], "comment": null, "summary": "The increasing integration of Artificial Intelligence (AI) into health and\nbiomedical systems necessitates robust frameworks for transparency,\naccountability, and ethical compliance. Existing frameworks often rely on\nhuman-readable, manual documentation which limits scalability, comparability,\nand machine interpretability across projects and platforms. They also fail to\nprovide a unique, verifiable identity for AI models to ensure their provenance\nand authenticity across systems and use cases, limiting reproducibility and\nstakeholder trust. This paper introduces the concept of the AI Model Passport,\na structured and standardized documentation framework that acts as a digital\nidentity and verification tool for AI models. It captures essential metadata to\nuniquely identify, verify, trace and monitor AI models across their lifecycle -\nfrom data acquisition and preprocessing to model design, development and\ndeployment. In addition, an implementation of this framework is presented\nthrough AIPassport, an MLOps tool developed within the ProCAncer-I EU project\nfor medical imaging applications. AIPassport automates metadata collection,\nensures proper versioning, decouples results from source scripts, and\nintegrates with various development environments. Its effectiveness is\nshowcased through a lesion segmentation use case using data from the\nProCAncer-I dataset, illustrating how the AI Model Passport enhances\ntransparency, reproducibility, and regulatory readiness while reducing manual\neffort. This approach aims to set a new standard for fostering trust and\naccountability in AI-driven healthcare solutions, aspiring to serve as the\nbasis for developing transparent and regulation compliant AI systems across\ndomains.", "AI": {"tldr": "论文提出了一种名为AI Model Passport的标准化框架，用于为AI模型提供数字身份和验证工具，以增强透明度、可追溯性和监管合规性。", "motivation": "现有框架依赖人工文档，缺乏可扩展性、可比性和机器可读性，且无法为AI模型提供唯一身份验证，限制了可重复性和信任。", "method": "引入AI Model Passport框架，通过AIPassport工具自动化元数据收集、版本控制和开发环境集成。", "result": "在医学影像应用中验证了AIPassport的有效性，展示了其在增强透明度、可重复性和减少人工负担方面的作用。", "conclusion": "AI Model Passport为AI驱动的医疗解决方案设定了新的信任和问责标准，有望成为跨领域透明合规AI系统的基础。"}}
{"id": "2506.21567", "pdf": "https://arxiv.org/pdf/2506.21567", "abs": "https://arxiv.org/abs/2506.21567", "authors": ["Baqer M. Merzah", "Tania Taami", "Salman Asoudeh", "Amir reza Hossein pour", "Saeed Mirzaee", "Amir Ali Bengari"], "title": "BioPars: A Pretrained Biomedical Large Language Model for Persian Biomedical Text Mining", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Large Language Models (LLMs) have recently gained attention in the life\nsciences due to their capacity to model, extract, and apply complex biological\ninformation. Beyond their classical use as chatbots, these systems are\nincreasingly used for complex analysis and problem-solving in specialized\nfields, including bioinformatics. First, we introduce BIOPARS-BENCH, a dataset\nfrom over 10,000 scientific articles, textbooks, and medical websites.\nBioParsQA was also introduced to evaluate the proposed model, which consists of\n5,231 Persian medical questions and answers. This study then introduces\nBioPars, a simple but accurate measure designed to assess LLMs for three main\nabilities: acquiring subject-specific knowledge, interpreting and synthesizing\nsuch knowledge, and demonstrating proper evidence. Comparing ChatGPT, Llama,\nand Galactica, our study highlights their ability to remember and retrieve\nlearned knowledge but also reveals shortcomings in addressing higher-level,\nreal-world questions and fine-grained inferences. These findings indicate the\nneed for further fine-tuning to address the capabilities of LLM in\nbioinformatics tasks. To our knowledge, BioPars is the first application of LLM\nin Persian medical QA, especially for generating long answers. Evaluation of\nfour selected medical QA datasets shows that BioPars has achieved remarkable\nresults compared to comparative approaches. The model on BioParsQA achieved a\nROUGE-L score of 29.99, which is an improvement over GPT-4 1.0. The model\nachieved a BERTScore of 90.87 with the MMR method. The MoverScore and BLEURT\nvalues were also higher in this model than the other three models. In addition,\nthe reported scores for the model are MoverScore=60.43 and BLEURT=50.78.\nBioPars is an ongoing project and all resources related to its development will\nbe made available via the following GitHub repository:\nhttps://github.com/amirap80/BioPars.", "AI": {"tldr": "论文介绍了BioPars，一种用于评估大型语言模型（LLMs）在生物信息学任务中表现的工具，并展示了其在波斯医学问答中的首次应用。", "motivation": "研究旨在探索LLMs在生命科学领域的潜力，特别是在生物信息学中的复杂分析和问题解决能力。", "method": "提出了BIOPARS-BENCH数据集和BioParsQA评估工具，并设计了BioPars模型来评估LLMs的三种能力：获取专业知识、解释和综合知识、以及提供适当证据。", "result": "BioPars在波斯医学问答中表现优异，ROUGE-L、BERTScore、MoverScore和BLEURT等指标均优于其他模型。", "conclusion": "研究表明LLMs在生物信息学任务中仍有改进空间，BioPars为未来研究提供了有价值的工具和数据集。"}}
{"id": "2506.21826", "pdf": "https://arxiv.org/pdf/2506.21826", "abs": "https://arxiv.org/abs/2506.21826", "authors": ["Rafael Sterzinger", "Marco Peer", "Robert Sablatnig"], "title": "Few-Shot Segmentation of Historical Maps via Linear Probing of Vision Foundation Models", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "18 pages, accepted at ICDAR2025", "summary": "As rich sources of history, maps provide crucial insights into historical\nchanges, yet their diverse visual representations and limited annotated data\npose significant challenges for automated processing. We propose a simple yet\neffective approach for few-shot segmentation of historical maps, leveraging the\nrich semantic embeddings of large vision foundation models combined with\nparameter-efficient fine-tuning. Our method outperforms the state-of-the-art on\nthe Siegfried benchmark dataset in vineyard and railway segmentation, achieving\n+5% and +13% relative improvements in mIoU in 10-shot scenarios and around +20%\nin the more challenging 5-shot setting. Additionally, it demonstrates strong\nperformance on the ICDAR 2021 competition dataset, attaining a mean PQ of 67.3%\nfor building block segmentation, despite not being optimized for this\nshape-sensitive metric, underscoring its generalizability. Notably, our\napproach maintains high performance even in extremely low-data regimes (10- &\n5-shot), while requiring only 689k trainable parameters - just 0.21% of the\ntotal model size. Our approach enables precise segmentation of diverse\nhistorical maps while drastically reducing the need for manual annotations,\nadvancing automated processing and analysis in the field. Our implementation is\npublicly available at:\nhttps://github.com/RafaelSterzinger/few-shot-map-segmentation.", "AI": {"tldr": "提出了一种基于大型视觉基础模型和参数高效微调的少样本历史地图分割方法，性能显著优于现有技术。", "motivation": "历史地图的多样性视觉表示和有限标注数据对自动化处理提出了挑战。", "method": "结合大型视觉基础模型的语义嵌入与参数高效微调，实现少样本分割。", "result": "在Siegfried数据集上，10-shot和5-shot场景下分别提升5%和13%的mIoU；在ICDAR 2021数据集上达到67.3%的PQ。", "conclusion": "该方法在极低数据需求下仍保持高性能，显著减少人工标注需求，推动了历史地图的自动化处理。"}}
{"id": "2506.22419", "pdf": "https://arxiv.org/pdf/2506.22419", "abs": "https://arxiv.org/abs/2506.22419", "authors": ["Bingchen Zhao", "Despoina Magka", "Minqi Jiang", "Xian Li", "Roberta Raileanu", "Tatiana Shavrina", "Jean-Christophe Gagnon-Audet", "Kelvin Niu", "Shagun Sodhani", "Michael Shvartsman", "Andrei Lupu", "Alisia Lupidi", "Edan Toledo", "Karen Hambardzumyan", "Martin Josifoski", "Thomas Foster", "Lucia Cipolina-Kun", "Abhishek Charnalia", "Derek Dunfield", "Alexander H. Miller", "Oisin Mac Aodha", "Jakob Foerster", "Yoram Bachrach"], "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements", "categories": ["cs.AI", "cs.CL", "cs.LG"], "comment": null, "summary": "Rapid advancements in large language models (LLMs) have the potential to\nassist in scientific progress. A critical capability toward this endeavor is\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\nreproduce results in an active research area, we introduce the Automated LLM\nSpeedrunning Benchmark, leveraging the research community contributions on the\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\nEach of the 19 speedrun tasks provides the agent with the previous records\ntraining script, optionally paired with one of three hint formats, ranging from\npseudocode to paper-like descriptions of the new records improvements. Records\nexecute quickly by design and speedrun improvements encompass diverse\ncode-level changes, ranging from high-level algorithmic advancements to\nhardware-aware optimizations. These features make the benchmark both accessible\nand realistic for the frontier problem of improving LLM training. We find that\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\nalready-known innovations in our benchmark, even when given detailed hints. Our\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\nautonomous research agent.", "AI": {"tldr": "论文提出了一个自动化LLM速度运行基准测试，用于评估AI代理在科学研究中复制结果的能力，发现当前LLM在详细提示下仍难以重现已知创新。", "motivation": "评估AI代理在科学研究中复制结果的能力，以推动科学进步。", "method": "引入自动化LLM速度运行基准测试，基于NanoGPT速度竞赛的19项任务，提供不同格式的提示。", "result": "当前LLM即使结合先进框架，在详细提示下仍难以重现已知创新。", "conclusion": "该基准测试为衡量LLM自动化科学复制能力提供了简单且非饱和的指标，是自主研究代理的必要技能。"}}
{"id": "2506.21568", "pdf": "https://arxiv.org/pdf/2506.21568", "abs": "https://arxiv.org/abs/2506.21568", "authors": ["Andrejs Sorstkins"], "title": "Assessing RAG and HyDE on 1B vs. 4B-Parameter Gemma LLMs for Personal Assistants Integretion", "categories": ["cs.CL", "I.2.7"], "comment": "Technical report as part of research project", "summary": "Resource efficiency is a critical barrier to deploying large language models\n(LLMs) in edge and privacy-sensitive applications. This study evaluates the\nefficacy of two augmentation strategies--Retrieval-Augmented Generation (RAG)\nand Hypothetical Document Embeddings (HyDE)--on compact Gemma LLMs of 1 billion\nand 4 billion parameters, within the context of a privacy-first personal\nassistant. We implement short-term memory via MongoDB and long-term semantic\nstorage via Qdrant, orchestrated through FastAPI and LangChain, and expose the\nsystem through a React.js frontend. Across both model scales, RAG consistently\nreduces latency by up to 17\\% and eliminates factual hallucinations when\nresponding to user-specific and domain-specific queries. HyDE, by contrast,\nenhances semantic relevance--particularly for complex physics prompts--but\nincurs a 25--40\\% increase in response time and a non-negligible hallucination\nrate in personal-data retrieval. Comparing 1 B to 4 B models, we observe that\nscaling yields marginal throughput gains for baseline and RAG pipelines, but\nmagnifies HyDE's computational overhead and variability. Our findings position\nRAG as the pragmatic choice for on-device personal assistants powered by\nsmall-scale LLMs.", "AI": {"tldr": "研究评估了RAG和HyDE两种增强策略在小型Gemma LLM上的效果，发现RAG在延迟和准确性上表现更优，适合边缘设备。", "motivation": "解决大型语言模型在边缘和隐私敏感应用中的资源效率问题。", "method": "使用MongoDB和Qdrant分别实现短长期记忆，通过FastAPI和LangChain协调，前端为React.js。", "result": "RAG减少延迟17%，消除幻觉；HyDE提升语义相关性但增加延迟和幻觉率。", "conclusion": "RAG是小型LLM驱动的边缘设备助手的实用选择。"}}
{"id": "2506.21832", "pdf": "https://arxiv.org/pdf/2506.21832", "abs": "https://arxiv.org/abs/2506.21832", "authors": ["Minh-Loi Nguyen", "Quang-Khai Le", "Tam V. Nguyen", "Minh-Triet Tran", "Trung-Nghia Le"], "title": "TaleForge: Interactive Multimodal System for Personalized Story Creation", "categories": ["cs.CV"], "comment": null, "summary": "Storytelling is a deeply personal and creative process, yet existing methods\noften treat users as passive consumers, offering generic plots with limited\npersonalization. This undermines engagement and immersion, especially where\nindividual style or appearance is crucial. We introduce TaleForge, a\npersonalized story-generation system that integrates large language models\n(LLMs) and text-to-image diffusion to embed users' facial images within both\nnarratives and illustrations. TaleForge features three interconnected modules:\nStory Generation, where LLMs create narratives and character descriptions from\nuser prompts; Personalized Image Generation, merging users' faces and outfit\nchoices into character illustrations; and Background Generation, creating scene\nbackdrops that incorporate personalized characters. A user study demonstrated\nheightened engagement and ownership when individuals appeared as protagonists.\nParticipants praised the system's real-time previews and intuitive controls,\nthough they requested finer narrative editing tools. TaleForge advances\nmultimodal storytelling by aligning personalized text and imagery to create\nimmersive, user-centric experiences.", "AI": {"tldr": "TaleForge是一个结合大型语言模型和文本到图像扩散技术的个性化故事生成系统，通过将用户的面部图像嵌入叙事和插图中，提升沉浸感和参与度。", "motivation": "现有方法通常将用户视为被动消费者，提供有限个性化的通用情节，降低了参与感和沉浸感。", "method": "TaleForge包含三个模块：故事生成（LLMs根据用户提示创建叙事和角色描述）、个性化图像生成（将用户面部和服装选择融入角色插图）和背景生成（创建包含个性化角色的场景背景）。", "result": "用户研究表明，当用户作为主角时，参与感和归属感显著提升。参与者赞赏系统的实时预览和直观控制，但希望有更精细的叙事编辑工具。", "conclusion": "TaleForge通过个性化文本和图像的结合，推动了多模态故事叙述的发展，创造了沉浸式的、以用户为中心的体验。"}}
{"id": "2109.05721", "pdf": "https://arxiv.org/pdf/2109.05721", "abs": "https://arxiv.org/abs/2109.05721", "authors": ["Yangyu Huang", "Hao Yang", "Chong Li", "Jongyoo Kim", "Fangyun Wei"], "title": "ADNet: Leveraging Error-Bias Towards Normal Direction in Face Alignment", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.IR", "cs.LG"], "comment": "Proceedings of the IEEE/CVF International Conference on Computer\n  Vision. 2021 (ICCV 2021)", "summary": "The recent progress of CNN has dramatically improved face alignment\nperformance. However, few works have paid attention to the error-bias with\nrespect to error distribution of facial landmarks. In this paper, we\ninvestigate the error-bias issue in face alignment, where the distributions of\nlandmark errors tend to spread along the tangent line to landmark curves. This\nerror-bias is not trivial since it is closely connected to the ambiguous\nlandmark labeling task. Inspired by this observation, we seek a way to leverage\nthe error-bias property for better convergence of CNN model. To this end, we\npropose anisotropic direction loss (ADL) and anisotropic attention module (AAM)\nfor coordinate and heatmap regression, respectively. ADL imposes strong binding\nforce in normal direction for each landmark point on facial boundaries. On the\nother hand, AAM is an attention module which can get anisotropic attention mask\nfocusing on the region of point and its local edge connected by adjacent\npoints, it has a stronger response in tangent than in normal, which means\nrelaxed constraints in the tangent. These two methods work in a complementary\nmanner to learn both facial structures and texture details. Finally, we\nintegrate them into an optimized end-to-end training pipeline named ADNet. Our\nADNet achieves state-of-the-art results on 300W, WFLW and COFW datasets, which\ndemonstrates the effectiveness and robustness.", "AI": {"tldr": "论文提出ADL和AAM方法解决人脸对齐中的误差偏差问题，通过结合两者构建ADNet，在多个数据集上达到最优性能。", "motivation": "人脸对齐中误差分布存在偏差，且与模糊标注任务相关，需利用这一特性提升CNN模型收敛性。", "method": "提出各向异性方向损失（ADL）和各向异性注意力模块（AAM），分别用于坐标和热图回归，并整合为ADNet。", "result": "ADNet在300W、WFLW和COFW数据集上达到最优性能。", "conclusion": "ADL和AAM互补结合能有效学习人脸结构和纹理细节，提升模型鲁棒性。"}}
{"id": "2506.21569", "pdf": "https://arxiv.org/pdf/2506.21569", "abs": "https://arxiv.org/abs/2506.21569", "authors": ["Weihua Xiao", "Derek Ekberg", "Siddharth Garg", "Ramesh Karri"], "title": "Hybrid-NL2SVA: Integrating RAG and Finetuning for LLM-based NL2SVA", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "SystemVerilog Assertions (SVAs) are critical for verifying the correctness of\nhardware designs, but manually writing them from natural language property\ndescriptions, i.e., NL2SVA, remains a labor-intensive and error-prone task.\nRecent advances in large language models (LLMs) offer opportunities to automate\nthis translation. However, existing models still struggle with understanding\ndomain-specific syntax and semantics. To enhance LLM performance in NL2SVA, we\npropose a customized retrieval-augmented generation (RAG) framework and a\nsynthetic fine-tuning dataset that together improve LLM's performance. To\nfurther improve lightweight models over NL2SVA, our fine-tuning dataset\nprovides prompt-guided explanations that teach LLMs the layer-by-layer\nconstruction process of concurrent SVAs, enabling supervised fine-tuning that\ngreatly improves syntax and functionality accuracy. To evaluate the performance\nof LLMs over NL2SVA, we construct the largest evaluation dataset for NL2SVA,\ncomprising 40 Verilog designs and 229 formally verified SVAs with detailed\nannotations. Experimental results show that our customized RAG framework\nincreases the number of functionality matched SVAs by 58.42% over GPT-4o-mini,\nwhile Qwen2.5-Coder-7B-Instruct fine-tuned on our fine-tuning dataset and\nintegrated with HybridRetrieval achieves a 59.05% over the base Qwen model.", "AI": {"tldr": "提出了一种基于检索增强生成（RAG）框架和合成微调数据集的NL2SVA方法，显著提升了LLM在硬件设计验证中的性能。", "motivation": "手动编写SystemVerilog断言（SVAs）耗时且易错，需要自动化工具提升效率。", "method": "结合定制化的RAG框架和合成微调数据集，通过提示引导的层叠构造过程改进LLM的语法和功能准确性。", "result": "实验表明，定制RAG框架使功能匹配的SVAs数量提升58.42%，微调后的Qwen2.5-Coder-7B-Instruct模型性能提升59.05%。", "conclusion": "该方法显著提升了LLM在NL2SVA任务中的表现，为硬件设计验证提供了高效解决方案。"}}
{"id": "2506.21834", "pdf": "https://arxiv.org/pdf/2506.21834", "abs": "https://arxiv.org/abs/2506.21834", "authors": ["Duy-Bao Bui", "Hoang-Khang Nguyen", "Trung-Nghia Le"], "title": "PrefPaint: Enhancing Image Inpainting through Expert Human Feedback", "categories": ["cs.CV"], "comment": null, "summary": "Inpainting, the process of filling missing or corrupted image parts, has\nbroad applications, including medical imaging. However, in specialized fields\nlike medical polyps imaging, where accuracy and reliability are critical,\ninpainting models can generate inaccurate images, leading to significant errors\nin medical diagnosis and treatment. To ensure reliability, medical images\nshould be annotated by experts like oncologists for effective model training.\nWe propose PrefPaint, an approach that incorporates human feedback into the\ntraining process of Stable Diffusion Inpainting, bypassing the need for\ncomputationally expensive reward models. In addition, we develop a web-based\ninterface streamlines training, fine-tuning, and inference. This interactive\ninterface provides a smooth and intuitive user experience, making it easier to\noffer feedback and manage the fine-tuning process. User study on various\ndomains shows that PrefPaint outperforms existing methods, reducing visual\ninconsistencies and improving image rendering, particularly in medical\ncontexts, where our model generates more realistic polyps images.", "AI": {"tldr": "PrefPaint是一种结合人类反馈的Stable Diffusion Inpainting训练方法，无需昂贵奖励模型，通过交互式界面优化图像修复，尤其在医学领域表现优异。", "motivation": "医学图像修复的准确性至关重要，现有模型可能生成不准确图像，影响诊断和治疗。专家标注和人类反馈可提升模型可靠性。", "method": "PrefPaint将人类反馈直接融入Stable Diffusion Inpainting训练，开发了基于Web的交互界面以简化训练和微调。", "result": "PrefPaint在多个领域优于现有方法，减少视觉不一致性，尤其在医学图像中生成更真实的息肉图像。", "conclusion": "PrefPaint通过人类反馈和交互界面显著提升了图像修复的准确性和实用性，特别适用于医学领域。"}}
{"id": "2212.09525", "pdf": "https://arxiv.org/pdf/2212.09525", "abs": "https://arxiv.org/abs/2212.09525", "authors": ["Yangyu Huang", "Xi Chen", "Jongyoo Kim", "Hao Yang", "Chong Li", "Jiaolong Yang", "Dong Chen"], "title": "FreeEnricher: Enriching Face Landmarks without Additional Cost", "categories": ["cs.CV", "cs.AI", "cs.GR", "cs.IR", "cs.LG"], "comment": "AAAI 2023", "summary": "Recent years have witnessed significant growth of face alignment. Though\ndense facial landmark is highly demanded in various scenarios, e.g., cosmetic\nmedicine and facial beautification, most works only consider sparse face\nalignment. To address this problem, we present a framework that can enrich\nlandmark density by existing sparse landmark datasets, e.g., 300W with 68\npoints and WFLW with 98 points. Firstly, we observe that the local patches\nalong each semantic contour are highly similar in appearance. Then, we propose\na weakly-supervised idea of learning the refinement ability on original sparse\nlandmarks and adapting this ability to enriched dense landmarks. Meanwhile,\nseveral operators are devised and organized together to implement the idea.\nFinally, the trained model is applied as a plug-and-play module to the existing\nface alignment networks. To evaluate our method, we manually label the dense\nlandmarks on 300W testset. Our method yields state-of-the-art accuracy not only\nin newly-constructed dense 300W testset but also in the original sparse 300W\nand WFLW testsets without additional cost.", "AI": {"tldr": "提出了一种通过稀疏标记数据集生成密集面部标记的框架，无需额外标注成本即可提升现有面部对齐网络的性能。", "motivation": "密集面部标记在美容医学和面部美化等场景中需求高，但现有研究多关注稀疏标记。", "method": "利用稀疏标记数据学习局部轮廓的细化能力，并通过设计的算子实现密集标记的生成。", "result": "在密集300W测试集及原始稀疏300W和WFLW测试集上均达到最优精度。", "conclusion": "该框架能有效提升面部对齐的标记密度和精度，且无需额外标注成本。"}}
{"id": "2506.21570", "pdf": "https://arxiv.org/pdf/2506.21570", "abs": "https://arxiv.org/abs/2506.21570", "authors": ["Roland Riachi", "Kashif Rasul", "Arjun Ashok", "Prateek Humane", "Alexis Roger", "Andrew R. Williams", "Yuriy Nevmyvaka", "Irina Rish"], "title": "Random Initialization Can't Catch Up: The Advantage of Language Model Transfer for Time Series Forecasting", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recent works have demonstrated the effectiveness of adapting pre-trained\nlanguage models (LMs) for forecasting time series in the low-data regime. We\nbuild upon these findings by analyzing the effective transfer from language\nmodels to time series forecasting under various design choices including\nupstream post-training, time series tokenizer and language backbone size. In\nthe low-data regime, these design choices have a significant impact on the\nvalidation loss, with clear-cut choices that outperform others. Contrary to\nHernandez et al. (2021), we observe that the validation loss of the LMs\ncontinues to smoothly decrease long after the validation loss of the randomly\ninitialized models has converged, leading to a non-vanishing transfer gap that\nholds across design choices. These findings not only help shed light on the\neffective use of compute-efficient training for time series, but also open the\nway for the study of modality-agnostic properties of data distributions\nleveraged by these models.", "AI": {"tldr": "该论文探讨了预训练语言模型（LMs）在低数据量时间序列预测中的有效性，并分析了不同设计选择对模型性能的影响。研究发现，某些设计选择显著优于其他选择，且语言模型的验证损失持续下降，与随机初始化模型不同。", "motivation": "研究动机在于探索预训练语言模型在时间序列预测中的适应性，尤其是在低数据量情况下，以优化计算效率并揭示跨模态数据分布的通用特性。", "method": "方法包括分析上游后训练、时间序列分词器和语言主干大小等设计选择对模型性能的影响，并与随机初始化模型进行对比。", "result": "结果表明，某些设计选择显著优于其他选择，且语言模型的验证损失持续下降，形成非消失的转移差距。", "conclusion": "结论强调了预训练语言模型在时间序列预测中的潜力，并为进一步研究跨模态数据分布特性提供了方向。"}}
{"id": "2506.21835", "pdf": "https://arxiv.org/pdf/2506.21835", "abs": "https://arxiv.org/abs/2506.21835", "authors": ["Xiaoqi Wang", "Clint Sebastian", "Wenbin He", "Liu Ren"], "title": "ProSAM: Enhancing the Robustness of SAM-based Visual Reference Segmentation with Probabilistic Prompts", "categories": ["cs.CV"], "comment": null, "summary": "The recent advancements in large foundation models have driven the success of\nopen-set image segmentation, a task focused on segmenting objects beyond\npredefined categories. Among various prompt types (such as points, boxes,\ntexts, and visual references), visual reference segmentation stands out for its\nunique flexibility and strong zero-shot capabilities. Recently, several\nSAM-based methods have made notable progress in this task by automatically\ngenerating prompts to guide SAM. However, these methods often generate prompts\nat object boundaries due to suboptimal prompt encoder, which results in\ninstability and reduced robustness. In this work, we introduce ProSAM, a simple\nbut effective method to address the stability challenges we identified in\nexisting SAM-based visual reference segmentation approaches. By learning a\nvariational prompt encoder to predict multivariate prompt distributions, ProSAM\navoids generating prompts that lie in unstable regions, overcoming the\ninstability caused by less robust prompts. Our approach consistently surpasses\nstate-of-the-art methods on the Pascal-5$^i$ and COCO-20$^i$ datasets,\nproviding a more robust solution for visual reference segmentation.", "AI": {"tldr": "ProSAM提出了一种改进SAM基础的视觉参考分割方法，通过变分提示编码器预测多变量提示分布，避免生成不稳定区域的提示，提升了分割的稳定性和鲁棒性。", "motivation": "现有SAM基础的视觉参考分割方法因提示编码器不理想，常在物体边界生成提示，导致不稳定性和鲁棒性下降。", "method": "ProSAM通过学习变分提示编码器预测多变量提示分布，避免生成不稳定区域的提示。", "result": "在Pascal-5$^i$和COCO-20$^i$数据集上，ProSAM超越了现有最优方法。", "conclusion": "ProSAM为视觉参考分割提供了更稳定和鲁棒的解决方案。"}}
{"id": "2412.15194", "pdf": "https://arxiv.org/pdf/2412.15194", "abs": "https://arxiv.org/abs/2412.15194", "authors": ["Qihao Zhao", "Yangyu Huang", "Tengchao Lv", "Lei Cui", "Qinzheng Sun", "Shaoguang Mao", "Xin Zhang", "Ying Xin", "Qiufeng Yin", "Scarlett Li", "Furu Wei"], "title": "MMLU-CF: A Contamination-free Multi-task Language Understanding Benchmark", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF"], "comment": null, "summary": "Multiple-choice question (MCQ) datasets like Massive Multitask Language\nUnderstanding (MMLU) are widely used to evaluate the commonsense,\nunderstanding, and problem-solving abilities of large language models (LLMs).\nHowever, the open-source nature of these benchmarks and the broad sources of\ntraining data for LLMs have inevitably led to benchmark contamination,\nresulting in unreliable evaluation results. To alleviate this issue, we propose\na contamination-free and more challenging MCQ benchmark called MMLU-CF. This\nbenchmark reassesses LLMs' understanding of world knowledge by averting both\nunintentional and malicious data leakage. To avoid unintentional data leakage,\nwe source data from a broader domain and design three decontamination rules. To\nprevent malicious data leakage, we divide the benchmark into validation and\ntest sets with similar difficulty and subject distributions. The test set\nremains closed-source to ensure reliable results, while the validation set is\npublicly available to promote transparency and facilitate independent\nverification. Our evaluation of mainstream LLMs reveals that the powerful\nGPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on\nthe test set, which indicates the effectiveness of our approach in creating a\nmore rigorous and contamination-free evaluation standard. The GitHub repository\nis available at https://github.com/microsoft/MMLU-CF and the dataset refers to\nhttps://huggingface.co/datasets/microsoft/MMLU-CF.", "AI": {"tldr": "提出了一个名为MMLU-CF的无污染多选问答基准，通过避免数据泄漏来更可靠地评估大语言模型的能力。", "motivation": "现有的多选问答数据集（如MMLU）因开源性和广泛训练数据导致基准污染，评估结果不可靠。", "method": "设计MMLU-CF基准，采用更广泛的数据来源和三条去污染规则，并分为公开验证集和闭源测试集。", "result": "GPT-4o在测试集上的5-shot和0-shot得分分别为73.4%和71.9%，验证了基准的有效性。", "conclusion": "MMLU-CF提供了一个更严格且无污染的评估标准，有助于可靠评估语言模型的能力。"}}
{"id": "2506.21571", "pdf": "https://arxiv.org/pdf/2506.21571", "abs": "https://arxiv.org/abs/2506.21571", "authors": ["Jianshuo Dong", "Yujia Fu", "Chuanrui Hu", "Chao Zhang", "Han Qiu"], "title": "Towards Understanding the Cognitive Habits of Large Reasoning Models", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": null, "summary": "Large Reasoning Models (LRMs), which autonomously produce a reasoning Chain\nof Thought (CoT) before producing final responses, offer a promising approach\nto interpreting and monitoring model behaviors. Inspired by the observation\nthat certain CoT patterns -- e.g., ``Wait, did I miss anything?'' --\nconsistently emerge across tasks, we explore whether LRMs exhibit human-like\ncognitive habits. Building on Habits of Mind, a well-established framework of\ncognitive habits associated with successful human problem-solving, we introduce\nCogTest, a principled benchmark designed to evaluate LRMs' cognitive habits.\nCogTest includes 16 cognitive habits, each instantiated with 25 diverse tasks,\nand employs an evidence-first extraction method to ensure reliable habit\nidentification. With CogTest, we conduct a comprehensive evaluation of 16\nwidely used LLMs (13 LRMs and 3 non-reasoning ones). Our findings reveal that\nLRMs, unlike conventional LLMs, not only exhibit human-like habits but also\nadaptively deploy them according to different tasks. Finer-grained analyses\nfurther uncover patterns of similarity and difference in LRMs' cognitive habit\nprofiles, particularly certain inter-family similarity (e.g., Qwen-3 models and\nDeepSeek-R1). Extending the study to safety-related tasks, we observe that\ncertain habits, such as Taking Responsible Risks, are strongly associated with\nthe generation of harmful responses. These findings suggest that studying\npersistent behavioral patterns in LRMs' CoTs is a valuable step toward deeper\nunderstanding of LLM misbehavior. The code is available at:\nhttps://github.com/jianshuod/CogTest.", "AI": {"tldr": "论文探讨了大型推理模型（LRMs）是否表现出类似人类的认知习惯，并提出了CogTest基准来评估这些习惯。研究发现LRMs具有人类习惯并能根据任务调整，某些习惯还与安全风险相关。", "motivation": "研究LRMs是否表现出人类认知习惯，以更好地理解和监控模型行为。", "method": "引入CogTest基准，包含16种认知习惯和25个任务，评估16种LLMs（13种LRMs和3种非推理模型）。", "result": "LRMs表现出人类习惯并能适应任务，某些习惯与有害响应相关。", "conclusion": "研究LRMs的认知习惯有助于深入理解模型行为，尤其是安全问题。"}}
{"id": "2506.21839", "pdf": "https://arxiv.org/pdf/2506.21839", "abs": "https://arxiv.org/abs/2506.21839", "authors": ["Mengyi Shan", "Brian Curless", "Ira Kemelmacher-Shlizerman", "Steve Seitz"], "title": "GenEscape: Hierarchical Multi-Agent Generation of Escape Room Puzzles", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "We challenge text-to-image models with generating escape room puzzle images\nthat are visually appealing, logically solid, and intellectually stimulating.\nWhile base image models struggle with spatial relationships and affordance\nreasoning, we propose a hierarchical multi-agent framework that decomposes this\ntask into structured stages: functional design, symbolic scene graph reasoning,\nlayout synthesis, and local image editing. Specialized agents collaborate\nthrough iterative feedback to ensure the scene is visually coherent and\nfunctionally solvable. Experiments show that agent collaboration improves\noutput quality in terms of solvability, shortcut avoidance, and affordance\nclarity, while maintaining visual quality.", "AI": {"tldr": "提出了一种分层多智能体框架，用于生成具有逻辑性和视觉吸引力的逃脱房间谜题图像，解决了基础图像模型在空间关系和功能推理上的不足。", "motivation": "挑战文本到图像模型生成兼具视觉吸引力、逻辑性和智力刺激的逃脱房间谜题图像。", "method": "采用分层多智能体框架，分解任务为功能设计、符号场景图推理、布局合成和局部图像编辑，通过迭代反馈确保场景的视觉一致性和可解性。", "result": "实验表明，智能体协作提高了输出质量，包括可解性、避免捷径和功能清晰度，同时保持视觉质量。", "conclusion": "分层多智能体框架有效提升了逃脱房间谜题图像的生成质量，解决了基础模型的局限性。"}}
{"id": "2501.06184", "pdf": "https://arxiv.org/pdf/2501.06184", "abs": "https://arxiv.org/abs/2501.06184", "authors": ["Yangyu Huang", "Tianyi Gao", "Haoran Xu", "Qihao Zhao", "Yang Song", "Zhipeng Gui", "Tengchao Lv", "Hao Chen", "Lei Cui", "Scarlett Li", "Furu Wei"], "title": "PEACE: Empowering Geologic Map Holistic Understanding with MLLMs", "categories": ["cs.CV", "cs.AI", "cs.CE", "cs.HC", "cs.MA"], "comment": null, "summary": "Geologic map, as a fundamental diagram in geology science, provides critical\ninsights into the structure and composition of Earth's subsurface and surface.\nThese maps are indispensable in various fields, including disaster detection,\nresource exploration, and civil engineering. Despite their significance,\ncurrent Multimodal Large Language Models (MLLMs) often fall short in geologic\nmap understanding. This gap is primarily due to the challenging nature of\ncartographic generalization, which involves handling high-resolution map,\nmanaging multiple associated components, and requiring domain-specific\nknowledge. To quantify this gap, we construct GeoMap-Bench, the first-ever\nbenchmark for evaluating MLLMs in geologic map understanding, which assesses\nthe full-scale abilities in extracting, referring, grounding, reasoning, and\nanalyzing. To bridge this gap, we introduce GeoMap-Agent, the inaugural agent\ndesigned for geologic map understanding, which features three modules:\nHierarchical Information Extraction (HIE), Domain Knowledge Injection (DKI),\nand Prompt-enhanced Question Answering (PEQA). Inspired by the\ninterdisciplinary collaboration among human scientists, an AI expert group acts\nas consultants, utilizing a diverse tool pool to comprehensively analyze\nquestions. Through comprehensive experiments, GeoMap-Agent achieves an overall\nscore of 0.811 on GeoMap-Bench, significantly outperforming 0.369 of GPT-4o.\nOur work, emPowering gEologic mAp holistiC undErstanding (PEACE) with MLLMs,\npaves the way for advanced AI applications in geology, enhancing the efficiency\nand accuracy of geological investigations.", "AI": {"tldr": "论文提出了GeoMap-Bench基准和GeoMap-Agent代理，用于评估和提升多模态大语言模型在地质图理解中的表现。", "motivation": "当前多模态大语言模型在地质图理解方面存在不足，主要由于地图的高分辨率、多组件和领域知识需求。", "method": "构建GeoMap-Bench基准，设计GeoMap-Agent代理，包含分层信息提取、领域知识注入和提示增强问答模块。", "result": "GeoMap-Agent在基准测试中得分0.811，显著优于GPT-4o的0.369。", "conclusion": "该研究为地质学中的AI应用铺平了道路，提升了地质调查的效率和准确性。"}}
{"id": "2506.21572", "pdf": "https://arxiv.org/pdf/2506.21572", "abs": "https://arxiv.org/abs/2506.21572", "authors": ["Tianyu. Zou", "Shengwu. Xiong", "Ruilin. Yao", "Jirui. Huang", "Yi. Rong", "Yaxiong. Chen", "Shili. Xiong", "Cong. Wang"], "title": "Aligning MLLM Benchmark With Human Preferences via Structural Equation Modeling", "categories": ["cs.CL"], "comment": "9 pages, 5 figures", "summary": "Evaluating multimodal large language models (MLLMs) remains a fundamental\nchallenge due to a lack of structured, interpretable, and theoretically\ngrounded benchmark designs. Existing benchmarks often adopt heuristic-based\ntask groupings with unclear cognitive targets, thus resulting in overlapping\nabilities, redundant indicators, and limited diagnostic power. In this work, we\npropose a novel framework for aligning MLLM benchmark based on Structural\nEquation Modeling (SEM) to analyze and quantify the internal validity,\ndimensional separability, and contribution of benchmark components. Motivated\nby the observed limitations of current designs, we further introduce a novel\ncapability hierarchy grounded in Piagets theory of cognitive development,\ndividing MLLM abilities into three hierarchical layers, i.e., Perception,\nMemory, and Reasoning. We reorganize existing MLLM benchmarks under the\nproposed framework and construct a new benchmark named Gold. Experimental\nresults demonstrate that the proposed benchmark exhibits stronger\ninterpretability, reduced indicator redundancy, and clearer cognitive\nconsistency compared to existing approaches.", "AI": {"tldr": "提出了一种基于结构方程模型（SEM）的多模态大语言模型（MLLM）评估框架，解决了现有基准设计缺乏结构化、可解释性和理论基础的问题。", "motivation": "现有基准设计存在启发式任务分组、认知目标不明确、能力重叠和指标冗余等问题，限制了其诊断能力。", "method": "采用结构方程模型（SEM）分析基准的内部效度、维度可分性和组件贡献，并基于皮亚杰认知发展理论提出能力层次结构（感知、记忆、推理）。", "result": "实验结果表明，新基准Gold在可解释性、指标冗余减少和认知一致性方面优于现有方法。", "conclusion": "提出的框架和基准为MLLM评估提供了更结构化、可解释和理论支持的方法。"}}
{"id": "2506.21843", "pdf": "https://arxiv.org/pdf/2506.21843", "abs": "https://arxiv.org/abs/2506.21843", "authors": ["Yuxiang Ge", "Jionghao Cheng", "Ruiquan Ge", "Zhaojie Fang", "Gangyong Jia", "Xiang Wan", "Nannan Li", "Ahmed Elazab", "Changmiao Wang"], "title": "3D-Telepathy: Reconstructing 3D Objects from EEG Signals", "categories": ["cs.CV"], "comment": null, "summary": "Reconstructing 3D visual stimuli from Electroencephalography (EEG) data holds\nsignificant potential for applications in Brain-Computer Interfaces (BCIs) and\naiding individuals with communication disorders. Traditionally, efforts have\nfocused on converting brain activity into 2D images, neglecting the translation\nof EEG data into 3D objects. This limitation is noteworthy, as the human brain\ninherently processes three-dimensional spatial information regardless of\nwhether observing 2D images or the real world. The neural activities captured\nby EEG contain rich spatial information that is inevitably lost when\nreconstructing only 2D images, thus limiting its practical applications in BCI.\nThe transition from EEG data to 3D object reconstruction faces considerable\nobstacles. These include the presence of extensive noise within EEG signals and\na scarcity of datasets that include both EEG and 3D information, which\ncomplicates the extraction process of 3D visual data. Addressing this\nchallenging task, we propose an innovative EEG encoder architecture that\nintegrates a dual self-attention mechanism. We use a hybrid training strategy\nto train the EEG Encoder, which includes cross-attention, contrastive learning,\nand self-supervised learning techniques. Additionally, by employing stable\ndiffusion as a prior distribution and utilizing Variational Score Distillation\nto train a neural radiation field, we successfully generate 3D objects with\nsimilar content and structure from EEG data.", "AI": {"tldr": "该论文提出了一种创新的EEG编码器架构，通过双自注意力机制和混合训练策略，成功从EEG数据中重建3D物体。", "motivation": "传统方法仅将EEG数据转换为2D图像，忽略了3D空间信息的重要性。EEG信号中的噪声和缺乏相关数据集增加了3D重建的难度。", "method": "采用双自注意力机制的EEG编码器架构，结合交叉注意力、对比学习和自监督学习的混合训练策略，并使用稳定扩散和变分分数蒸馏训练神经辐射场。", "result": "成功从EEG数据中生成了内容和结构相似的3D物体。", "conclusion": "该方法为BCI应用提供了新的可能性，尤其是在3D视觉刺激重建方面。"}}
{"id": "2506.20893", "pdf": "https://arxiv.org/pdf/2506.20893", "abs": "https://arxiv.org/abs/2506.20893", "authors": ["Yian Wang", "Ali Ebrahimpour-Boroojeny", "Hari Sundaram"], "title": "On the Necessity of Output Distribution Reweighting for Effective Class Unlearning", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "In this work, we introduce an output-reweighting unlearning method, RWFT, a\nlightweight technique that erases an entire class from a trained classifier\nwithout full retraining. Forgetting specific classes from trained models is\nessential for enforcing user deletion rights and mitigating harmful or biased\npredictions. The full retraining is costly and existing unlearning methods fail\nto replicate the behavior of the retrained models when predicting samples from\nthe unlearned class. We prove this failure by designing a variant of membership\ninference attacks, MIA-NN that successfully reveals the unlearned class for any\nof these methods. We propose a simple redistribution of the probability mass\nfor the prediction on the samples in the forgotten class which is robust to\nMIA-NN. We also introduce a new metric based on the total variation (TV)\ndistance of the prediction probabilities to quantify residual leakage to\nprevent future methods from susceptibility to the new attack. Through extensive\nexperiments with state of the art baselines in machine unlearning, we show that\nour approach matches the results of full retraining in both metrics used for\nevaluation by prior work and the new metric we propose in this work. Compare to\nstate-of-the-art methods, we gain 2.79% in previously used metrics and 111.45%\nin our new TV-based metric over the best existing method.", "AI": {"tldr": "提出了一种轻量级的输出重加权遗忘方法RWFT，无需完全重新训练即可从分类器中删除特定类别，解决了现有遗忘方法在未学习类别上的预测行为问题。", "motivation": "解决完全重新训练的高成本问题，同时确保用户删除权利的执行和减少有害或偏见的预测。", "method": "通过重新分配遗忘类别的预测概率质量，设计了一种对MIA-NN攻击鲁棒的方法，并引入基于总变差距离的新度量标准。", "result": "实验表明，RWFT在现有评估指标和新提出的TV距离指标上均优于现有方法，分别提升了2.79%和111.45%。", "conclusion": "RWFT方法在保持轻量级的同时，有效实现了与完全重新训练相当的性能，并显著提升了对抗新攻击的能力。"}}
{"id": "2506.21573", "pdf": "https://arxiv.org/pdf/2506.21573", "abs": "https://arxiv.org/abs/2506.21573", "authors": ["Yanwei Ren", "Liu Liu", "Baosheng Yu", "Jiayan Qiu", "Quan Chen"], "title": "Instruction Learning Paradigms: A Dual Perspective on White-box and Black-box LLMs", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Optimizing instructions for large language models (LLMs) is critical for\nharnessing their full potential in complex and diverse tasks. However, relying\nsolely on white-box approaches demands extensive computational resources and\noffers limited representational capacity, while black-box models can incur\nprohibitive financial costs. To address these challenges, we introduce a novel\nframework that seamlessly merges the strengths of both paradigms. Black-box\nmodels provide high-quality, diverse instruction initializations, and white-box\nmodels supply fine-grained interpretability through hidden states and output\nfeatures. By enforcing a semantic similarity constraint, these components fuse\ninto a unified high-dimensional representation that captures deep semantic and\nstructural nuances, enabling an iterative optimization process to refine\ninstruction quality and adaptability. Extensive evaluations across a broad\nspectrum of tasks-ranging from complex reasoning to cross-lingual\ngeneralization-demonstrate that our approach consistently outperforms\nstate-of-the-art baselines. This fusion of black-box initialization with\nadvanced semantic refinement yields a scalable and efficient solution, paving\nthe way for next-generation LLM-driven applications in diverse real-world\nscenarios. The source code will be released soon.", "AI": {"tldr": "提出了一种融合黑盒和白盒模型优势的新框架，通过语义相似性约束优化大语言模型的指令，提升任务表现。", "motivation": "解决黑盒模型成本高和白盒模型计算资源需求大、表达能力有限的问题。", "method": "结合黑盒模型的高质量初始化指令和白盒模型的细粒度可解释性，通过语义相似性约束实现迭代优化。", "result": "在复杂推理和跨语言泛化等任务中表现优于现有基线方法。", "conclusion": "该框架为下一代大语言模型应用提供了可扩展且高效的解决方案。"}}
{"id": "2506.21851", "pdf": "https://arxiv.org/pdf/2506.21851", "abs": "https://arxiv.org/abs/2506.21851", "authors": ["Haofeng Wang", "Fangtao Zhou", "Qi Zhang", "Zeyuan Chen", "Enci Zhang", "Zhao Wang", "Xiaofeng Huang", "Siwei Ma"], "title": "End-to-End RGB-IR Joint Image Compression With Channel-wise Cross-modality Entropy Model", "categories": ["cs.CV", "cs.MM", "eess.IV"], "comment": "IEEE International Conference on Systems, Man, and Cybernetics 2025.\n  (SMC), under review", "summary": "RGB-IR(RGB-Infrared) image pairs are frequently applied simultaneously in\nvarious applications like intelligent surveillance. However, as the number of\nmodalities increases, the required data storage and transmission costs also\ndouble. Therefore, efficient RGB-IR data compression is essential. This work\nproposes a joint compression framework for RGB-IR image pair. Specifically, to\nfully utilize cross-modality prior information for accurate context probability\nmodeling within and between modalities, we propose a Channel-wise\nCross-modality Entropy Model (CCEM). Among CCEM, a Low-frequency Context\nExtraction Block (LCEB) and a Low-frequency Context Fusion Block (LCFB) are\ndesigned for extracting and aggregating the global low-frequency information\nfrom both modalities, which assist the model in predicting entropy parameters\nmore accurately. Experimental results demonstrate that our approach outperforms\nexisting RGB-IR image pair and single-modality compression methods on LLVIP and\nKAIST datasets. For instance, the proposed framework achieves a 23.1% bit rate\nsaving on LLVIP dataset compared to the state-of-the-art RGB-IR image codec\npresented at CVPR 2022.", "AI": {"tldr": "提出了一种RGB-IR图像对的联合压缩框架，通过跨模态熵模型（CCEM）和低频上下文提取与融合模块（LCEB和LCFB）提升压缩效率。", "motivation": "随着多模态数据（如RGB-IR）的应用增加，存储和传输成本成倍增长，因此需要高效的压缩方法。", "method": "设计了CCEM模型，包含LCEB和LCFB模块，用于提取和融合跨模态的低频信息，优化熵参数预测。", "result": "在LLVIP和KAIST数据集上优于现有RGB-IR和单模态压缩方法，例如在LLVIP上比特率节省23.1%。", "conclusion": "该框架通过跨模态信息利用显著提升了RGB-IR图像对的压缩效率。"}}
{"id": "2506.21545", "pdf": "https://arxiv.org/pdf/2506.21545", "abs": "https://arxiv.org/abs/2506.21545", "authors": ["Yalun Dai", "Yangyu Huang", "Xin Zhang", "Wenshan Wu", "Chong Li", "Wenhui Lu", "Shijie Cao", "Li Dong", "Scarlett Li"], "title": "Data Efficacy for Language Model Training", "categories": ["cs.CL", "cs.AI", "cs.LG", "cs.PF"], "comment": null, "summary": "Data is fundamental to the training of language models (LM). Recent research\nhas been dedicated to data efficiency, which aims to maximize performance by\nselecting a minimal or optimal subset of training data. Techniques such as data\nfiltering, sampling, and selection play a crucial role in this area. To\ncomplement it, we define Data Efficacy, which focuses on maximizing performance\nby optimizing the organization of training data and remains relatively\nunderexplored. This work introduces a general paradigm, DELT, for considering\ndata efficacy in LM training, which highlights the significance of training\ndata organization. DELT comprises three components: Data Scoring, Data\nSelection, and Data Ordering. Among these components, we design\nLearnability-Quality Scoring (LQS), as a new instance of Data Scoring, which\nconsiders both the learnability and quality of each data sample from the\ngradient consistency perspective. We also devise Folding Ordering (FO), as a\nnovel instance of Data Ordering, which addresses issues such as model\nforgetting and data distribution bias. Comprehensive experiments validate the\ndata efficacy in LM training, which demonstrates the following: Firstly,\nvarious instances of the proposed DELT enhance LM performance to varying\ndegrees without increasing the data scale and model size. Secondly, among these\ninstances, the combination of our proposed LQS for data scoring and Folding for\ndata ordering achieves the most significant improvement. Lastly, data efficacy\ncan be achieved together with data efficiency by applying data selection.\nTherefore, we believe that data efficacy is a promising foundational area in LM\ntraining.", "AI": {"tldr": "论文提出了一种名为DELT的通用范式，通过优化训练数据的组织（数据效能）来提升语言模型性能，包括数据评分、选择和排序三个组件，并验证了其有效性。", "motivation": "当前研究主要集中在数据效率（选择最优训练数据子集），而数据效能（优化数据组织）尚未充分探索，本文旨在填补这一空白。", "method": "提出DELT范式，包含数据评分（如LQS）、数据选择和数据排序（如FO），从梯度一致性和数据分布角度优化数据组织。", "result": "实验表明DELT能不同程度提升模型性能，其中LQS和FO组合效果最佳，且数据效能与数据效率可同时实现。", "conclusion": "数据效能是语言模型训练中一个具有潜力的基础研究方向。"}}
{"id": "2506.21574", "pdf": "https://arxiv.org/pdf/2506.21574", "abs": "https://arxiv.org/abs/2506.21574", "authors": ["Yicheng Mao", "Yang Zhao"], "title": "Digital Gatekeepers: Exploring Large Language Model's Role in Immigration Decisions", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "With globalization and increasing immigrant populations, immigration\ndepartments face significant work-loads and the challenge of ensuring fairness\nin decision-making processes. Integrating artificial intelligence offers a\npromising solution to these challenges. This study investigates the potential\nof large language models (LLMs),such as GPT-3.5 and GPT-4, in supporting\nimmigration decision-making. Utilizing a mixed-methods approach,this paper\nconducted discrete choice experiments and in-depth interviews to study LLM\ndecision-making strategies and whether they are fair. Our findings demonstrate\nthat LLMs can align their decision-making with human strategies, emphasizing\nutility maximization and procedural fairness. Meanwhile, this paper also\nreveals that while ChatGPT has safeguards to prevent unintentional\ndiscrimination, it still exhibits stereotypes and biases concerning nationality\nand shows preferences toward privileged group. This dual analysis highlights\nboth the potential and limitations of LLMs in automating and enhancing\nimmigration decisions.", "AI": {"tldr": "研究探讨了大型语言模型（如GPT-3.5和GPT-4）在移民决策中的潜力与局限性，发现其能模拟人类决策策略但存在偏见。", "motivation": "全球化与移民增加导致移民部门工作量大且需确保决策公平，人工智能提供潜在解决方案。", "method": "采用混合方法，包括离散选择实验和深度访谈，分析LLM的决策策略及公平性。", "result": "LLM能模拟人类决策策略，强调效用最大化和程序公平，但仍存在国籍偏见和特权群体偏好。", "conclusion": "LLM在移民决策自动化中有潜力，但需解决偏见问题。"}}
{"id": "2506.21855", "pdf": "https://arxiv.org/pdf/2506.21855", "abs": "https://arxiv.org/abs/2506.21855", "authors": ["Jiho Choi", "Sang Jun Lee"], "title": "Periodic-MAE: Periodic Video Masked Autoencoder for rPPG Estimation", "categories": ["cs.CV"], "comment": null, "summary": "In this paper, we propose a method that learns a general representation of\nperiodic signals from unlabeled facial videos by capturing subtle changes in\nskin tone over time. The proposed framework employs the video masked\nautoencoder to learn a high-dimensional spatio-temporal representation of the\nfacial region through self-supervised learning. Capturing quasi-periodic\nsignals in the video is crucial for remote photoplethysmography (rPPG)\nestimation. To account for signal periodicity, we apply frame masking in terms\nof video sampling, which allows the model to capture resampled quasi-periodic\nsignals during the pre-training stage. Moreover, the framework incorporates\nphysiological bandlimit constraints, leveraging the property that physiological\nsignals are sparse within their frequency bandwidth to provide pulse cues to\nthe model. The pre-trained encoder is then transferred to the rPPG task, where\nit is used to extract physiological signals from facial videos. We evaluate the\nproposed method through extensive experiments on the PURE, UBFC-rPPG, MMPD, and\nV4V datasets. Our results demonstrate significant performance improvements,\nparticularly in challenging cross-dataset evaluations. Our code is available at\nhttps://github.com/ziiho08/Periodic-MAE.", "AI": {"tldr": "提出一种通过自监督学习从无标记面部视频中学习周期性信号表示的方法，用于远程光电容积描记术（rPPG）估计。", "motivation": "捕捉面部视频中皮肤色调的细微变化以提取生理信号，解决rPPG任务中的挑战。", "method": "使用视频掩码自编码器学习时空表示，结合帧掩码和生理带宽限制约束。", "result": "在多个数据集上表现优异，尤其在跨数据集评估中显著提升性能。", "conclusion": "提出的方法能有效学习周期性信号表示，为rPPG任务提供高效解决方案。"}}
{"id": "2506.21575", "pdf": "https://arxiv.org/pdf/2506.21575", "abs": "https://arxiv.org/abs/2506.21575", "authors": ["Josefa Lia Stoisser", "Marc Boubnovski Martell", "Lawrence Phillips", "Casper Hansen", "Julien Fauqueur"], "title": "STRuCT-LLM: Unifying Tabular and Graph Reasoning with Reinforcement Learning for Semantic Parsing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "We propose STRuCT-LLM, a unified framework for training large language models\n(LLMs) to perform structured reasoning over both relational and\ngraph-structured data. Our approach jointly optimizes Text-to-SQL and\nText-to-Cypher tasks using reinforcement learning (RL) combined with\nChain-of-Thought (CoT) supervision. To support fine-grained optimization in\ngraph-based parsing, we introduce a topology-aware reward function based on\ngraph edit distance. Unlike prior work that treats relational and graph\nformalisms in isolation, STRuCT-LLM leverages shared abstractions between SQL\nand Cypher to induce cross-formalism transfer, enabling SQL training to improve\nCypher performance and vice versa - even without shared schemas. Our largest\nmodel (QwQ-32B) achieves substantial relative improvements across tasks: on\nsemantic parsing, Spider improves by 13.5\\% and Text2Cypher by 73.1\\%. The\nmodel also demonstrates strong zero-shot generalization, improving performance\non downstream tabular QA (TableBench: 8.5\\%) and knowledge graph QA\n(CR-LT-KGQA: 1.7\\%) without any QA-specific supervision. These results\ndemonstrate both the effectiveness of executable queries as scaffolds for\nstructured reasoning and the synergistic benefits of jointly training on SQL\nand Cypher (code available at https://github.com/bouv/STRuCT-LLM).", "AI": {"tldr": "STRuCT-LLM是一个统一框架，通过强化学习和思维链监督联合优化Text-to-SQL和Text-to-Cypher任务，实现了跨形式化迁移，显著提升了语义解析和知识图谱问答性能。", "motivation": "解决现有方法在处理关系型和图结构数据时的孤立性问题，利用SQL和Cypher之间的共享抽象实现跨形式化迁移。", "method": "结合强化学习和思维链监督，引入基于图编辑距离的拓扑感知奖励函数，联合优化Text-to-SQL和Text-to-Cypher任务。", "result": "QwQ-32B模型在Spider任务上提升13.5%，Text2Cypher任务提升73.1%，并在零样本下游任务中表现优异。", "conclusion": "STRuCT-LLM证明了可执行查询作为结构化推理支架的有效性，以及联合训练SQL和Cypher的协同优势。"}}
{"id": "2506.21857", "pdf": "https://arxiv.org/pdf/2506.21857", "abs": "https://arxiv.org/abs/2506.21857", "authors": ["Ekaterina Redekop", "Mara Pleasure", "Zichen Wang", "Kimberly Flores", "Anthony Sisk", "William Speier", "Corey W. Arnold"], "title": "SPADE: Spatial Transcriptomics and Pathology Alignment Using a Mixture of Data Experts for an Expressive Latent Space", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "The rapid growth of digital pathology and advances in self-supervised deep\nlearning have enabled the development of foundational models for various\npathology tasks across diverse diseases. While multimodal approaches\nintegrating diverse data sources have emerged, a critical gap remains in the\ncomprehensive integration of whole-slide images (WSIs) with spatial\ntranscriptomics (ST), which is crucial for capturing critical molecular\nheterogeneity beyond standard hematoxylin & eosin (H&E) staining. We introduce\nSPADE, a foundation model that integrates histopathology with ST data to guide\nimage representation learning within a unified framework, in effect creating an\nST-informed latent space. SPADE leverages a mixture-of-data experts technique,\nwhere experts, created via two-stage feature-space clustering, use contrastive\nlearning to learn representations of co-registered WSI patches and gene\nexpression profiles. Pre-trained on the comprehensive HEST-1k dataset, SPADE is\nevaluated on 14 downstream tasks, demonstrating significantly superior few-shot\nperformance compared to baseline models, highlighting the benefits of\nintegrating morphological and molecular information into one latent space.", "AI": {"tldr": "SPADE是一种基础模型，整合了组织病理学和空间转录组学数据，通过对比学习在统一框架中学习图像表示，显著提升了少样本任务性能。", "motivation": "当前数字病理学和多模态方法虽发展迅速，但全切片图像与空间转录组学的全面整合仍存在空白，这对捕捉分子异质性至关重要。", "method": "SPADE采用混合数据专家技术，通过两阶段特征空间聚类和对比学习，整合WSI和基因表达数据。", "result": "在14个下游任务中，SPADE表现出显著优于基线模型的少样本性能。", "conclusion": "SPADE展示了将形态学和分子信息整合到同一潜在空间的价值。"}}
{"id": "2506.21576", "pdf": "https://arxiv.org/pdf/2506.21576", "abs": "https://arxiv.org/abs/2506.21576", "authors": ["Hongli Yang", "Yizhou Peng", "Hao Huang", "Sheng Li"], "title": "Adapting Whisper for Parameter-efficient Code-Switching Speech Recognition via Soft Prompt Tuning", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted by Interspeech 2025", "summary": "Large-scale multilingual ASR models like Whisper excel in high-resource\nsettings but face challenges in low-resource scenarios, such as rare languages\nand code-switching (CS), due to computational costs and catastrophic\nforgetting. We explore Soft Prompt Tuning (SPT), a parameter-efficient method\nto enhance CS ASR while preserving prior knowledge. We evaluate two strategies:\n(1) full fine-tuning (FFT) of both soft prompts and the entire Whisper model,\ndemonstrating improved cross-lingual capabilities compared to traditional\nmethods, and (2) adhering to SPT's original design by freezing model parameters\nand only training soft prompts. Additionally, we introduce SPT4ASR, a\ncombination of different SPT variants. Experiments on the SEAME and ASRU2019\ndatasets show that deep prompt tuning is the most effective SPT approach, and\nour SPT4ASR methods achieve further error reductions in CS ASR, maintaining\nparameter efficiency similar to LoRA, without degrading performance on existing\nlanguages.", "AI": {"tldr": "论文探讨了Soft Prompt Tuning（SPT）在低资源场景下提升多语言ASR模型性能的方法，提出了SPT4ASR组合方法，实验证明其有效且参数高效。", "motivation": "解决大规模多语言ASR模型（如Whisper）在低资源场景（如稀有语言和代码切换）中的性能问题，避免计算成本高和灾难性遗忘。", "method": "研究了两种策略：全微调（FFT）和仅训练软提示的SPT，并提出了SPT4ASR组合方法。", "result": "实验表明深度提示调优最有效，SPT4ASR进一步降低了代码切换ASR的错误率，同时保持了参数高效性。", "conclusion": "SPT4ASR方法在提升低资源场景性能的同时，未对现有语言性能造成负面影响，具有实用价值。"}}
{"id": "2506.21862", "pdf": "https://arxiv.org/pdf/2506.21862", "abs": "https://arxiv.org/abs/2506.21862", "authors": ["Boyuan Sun", "Jiaxing Zhao", "Xihan Wei", "Qibin Hou"], "title": "LLaVA-Scissor: Token Compression with Semantic Connected Components for Video LLMs", "categories": ["cs.CV", "cs.AI", "cs.HC", "cs.MM"], "comment": "21 pages, 4 figures, 7 tables", "summary": "In this paper, we present LLaVA-Scissor, a training-free token compression\nstrategy designed for video multimodal large language models. Previous methods\nmostly attempt to compress tokens based on attention scores, but fail to\neffectively capture all semantic regions and often lead to token redundancy.\nDifferently, we propose to leverage the Semantic Connected Components (SCC)\napproach that assigns tokens to distinct semantic regions within the token set,\nensuring comprehensive semantic coverage. The outcome is a two-step\nspatio-temporal token compression strategy that utilizes SCC in both spatial\nand temporal domains. This strategy can effectively compress tokens by\nrepresenting the entire video with a set of non-overlapping semantic tokens. We\nconduct extensive evaluations of the token compression capabilities of\nLLaVA-Scissor across diverse video understanding benchmarks, including video\nquestion answering, long video understanding, and comprehensive multi-choices\nbenchmarks. Experimental results show that the proposed LLaVA-Scissor\noutperforms other token compression methods, achieving superior performance in\nvarious video understanding benchmarks, particularly at low token retention\nratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.", "AI": {"tldr": "LLaVA-Scissor是一种无需训练的视频多模态大语言模型令牌压缩策略，通过语义连通组件（SCC）方法实现全面的语义覆盖，优于现有方法。", "motivation": "现有令牌压缩方法基于注意力分数，无法有效捕捉所有语义区域且易导致冗余，需改进。", "method": "采用SCC方法将令牌分配到不同语义区域，提出两步时空令牌压缩策略，生成非重叠语义令牌。", "result": "在多种视频理解基准测试中表现优异，尤其在低令牌保留率下优于其他方法。", "conclusion": "LLaVA-Scissor通过SCC实现了高效的令牌压缩，显著提升了视频多模态模型的性能。"}}
{"id": "2506.21577", "pdf": "https://arxiv.org/pdf/2506.21577", "abs": "https://arxiv.org/abs/2506.21577", "authors": ["Hongli Yang", "Sheng Li", "Hao Huang", "Ayiduosi Tuohan", "Yizhou Peng"], "title": "Language-Aware Prompt Tuning for Parameter-Efficient Seamless Language Expansion in Multilingual ASR", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": "Accepted by Interspeech 2025", "summary": "Recent advancements in multilingual automatic speech recognition (ASR) have\nbeen driven by large-scale end-to-end models like Whisper. However, challenges\nsuch as language interference and expanding to unseen languages (language\nexpansion) without degrading performance persist. This paper addresses these\nwith three contributions: 1) Entire Soft Prompt Tuning (Entire SPT), which\napplies soft prompts to both the encoder and decoder, enhancing feature\nextraction and decoding; 2) Language-Aware Prompt Tuning (LAPT), which\nleverages cross-lingual similarities to encode shared and language-specific\nfeatures using lightweight prompt matrices; 3) SPT-Whisper, a toolkit that\nintegrates SPT into Whisper and enables efficient continual learning.\nExperiments across three languages from FLEURS demonstrate that Entire SPT and\nLAPT outperform Decoder SPT by 5.0% and 16.0% in language expansion tasks,\nrespectively, providing an efficient solution for dynamic, multilingual ASR\nmodels with minimal computational overhead.", "AI": {"tldr": "本文提出两种新的提示调优方法（Entire SPT和LAPT）及一个工具包（SPT-Whisper），显著提升了多语言ASR模型的性能，尤其在语言扩展任务中表现优异。", "motivation": "解决多语言ASR中语言干扰和扩展到未见语言时的性能下降问题。", "method": "1) Entire SPT：在编码器和解码器中应用软提示；2) LAPT：利用跨语言相似性编码共享和语言特定特征；3) SPT-Whisper：集成SPT的工具包。", "result": "Entire SPT和LAPT在语言扩展任务中分别比Decoder SPT提升5.0%和16.0%。", "conclusion": "提出的方法为动态多语言ASR模型提供了高效解决方案，计算开销小。"}}
{"id": "2506.21863", "pdf": "https://arxiv.org/pdf/2506.21863", "abs": "https://arxiv.org/abs/2506.21863", "authors": ["Sungjune Park", "Yeongyun Kim", "Se Yeon Kim", "Yong Man Ro"], "title": "Remote Sensing Large Vision-Language Model: Semantic-augmented Multi-level Alignment and Semantic-aware Expert Modeling", "categories": ["cs.CV"], "comment": "13 pages including reference pages, 7 tables, and 6 figures", "summary": "Large Vision and Language Models (LVLMs) have shown strong performance across\nvarious vision-language tasks in natural image domains. However, their\napplication to remote sensing (RS) remains underexplored due to significant\ndomain differences in visual appearances, object scales, and semantics. These\ndiscrepancies hider the effective understanding of RS scenes, which contain\nrich, multi-level semantic information spanning from coarse-to-fine levels.\nHence, it limits the direct adaptation of existing LVLMs to RS imagery. To\naddress this gap, we propose a novel LVLM framework tailored for RS\nunderstanding, incorporating two core components: Semantic-augmented\nMulti-level Alignment and Semantic-aware Expert Modeling. First, to align\nmulti-level visual features, we introduce the retrieval-based Semantic\nAugmentation Module which enriches the visual features with relevant semantics\nacross fine-to-coarse levels (e.g., object- and scene-level information). It is\ndesigned to retrieve relevant semantic cues from a RS semantic knowledge\ndatabase, followed by aggregation of semantic cues with user query and\nmulti-level visual features, resulting in semantically enriched representation\nacross multiple levels. Second, for Semantic-aware Expert Modeling, we design\nsemantic experts, where each expert is responsible for processing semantic\nrepresentation at different levels separately. This enables hierarchical\nsemantic understanding from coarse to fine levels. Evaluations across multiple\nRS tasks-including scene classification and VQA, etc.-demonstrate that the\nproposed framework achieves consistent improvements across multiple semantic\nlevels. This highlights its capability and effectiveness in bridging the gap\nbetween general LVLMs and unique demands of RS-specific vision-language\nunderstanding.", "AI": {"tldr": "论文提出了一种针对遥感（RS）图像理解的新型大型视觉语言模型（LVLM）框架，通过语义增强的多级对齐和语义感知专家建模，解决了通用LVLM在RS领域的局限性。", "motivation": "由于遥感图像与自然图像在视觉外观、对象尺度和语义上的显著差异，现有LVLM难以直接应用于RS场景理解。", "method": "框架包含两个核心组件：1）基于检索的语义增强模块，用于多级视觉特征对齐；2）语义感知专家建模，分层处理不同级别的语义表示。", "result": "在多个RS任务（如场景分类和视觉问答）中，框架实现了跨多语义级别的一致性能提升。", "conclusion": "该框架有效弥补了通用LVLM与RS特定需求之间的差距，展示了其在RS视觉语言理解中的能力和有效性。"}}
{"id": "2506.21578", "pdf": "https://arxiv.org/pdf/2506.21578", "abs": "https://arxiv.org/abs/2506.21578", "authors": ["Andrew Maranhão Ventura D'addario"], "title": "HealthQA-BR: A System-Wide Benchmark Reveals Critical Knowledge Gaps in Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The evaluation of Large Language Models (LLMs) in healthcare has been\ndominated by physician-centric, English-language benchmarks, creating a\ndangerous illusion of competence that ignores the interprofessional nature of\npatient care. To provide a more holistic and realistic assessment, we introduce\nHealthQA-BR, the first large-scale, system-wide benchmark for\nPortuguese-speaking healthcare. Comprising 5,632 questions from Brazil's\nnational licensing and residency exams, it uniquely assesses knowledge not only\nin medicine and its specialties but also in nursing, dentistry, psychology,\nsocial work, and other allied health professions. We conducted a rigorous\nzero-shot evaluation of over 20 leading LLMs. Our results reveal that while\nstate-of-the-art models like GPT 4.1 achieve high overall accuracy (86.6%),\nthis top-line score masks alarming, previously unmeasured deficiencies. A\ngranular analysis shows performance plummets from near-perfect in specialties\nlike Ophthalmology (98.7%) to barely passing in Neurosurgery (60.0%) and, most\nnotably, Social Work (68.4%). This \"spiky\" knowledge profile is a systemic\nissue observed across all models, demonstrating that high-level scores are\ninsufficient for safety validation. By publicly releasing HealthQA-BR and our\nevaluation suite, we provide a crucial tool to move beyond single-score\nevaluations and toward a more honest, granular audit of AI readiness for the\nentire healthcare team.", "AI": {"tldr": "HealthQA-BR是一个针对葡萄牙语医疗领域的大规模基准测试，评估了20多种大型语言模型（LLMs）的表现，揭示了模型在不同医疗专业中的知识不均衡问题。", "motivation": "现有评估主要关注英语和医生视角，忽视了医疗团队的跨专业性和多语言需求，因此需要更全面的评估工具。", "method": "使用巴西国家执照和住院医师考试的5,632个问题，覆盖医学、护理、牙科、心理学等多个领域，对LLMs进行零样本评估。", "result": "GPT 4.1等模型整体准确率达86.6%，但在神经外科（60.0%）和社会工作（68.4%）等专业表现较差，显示出知识不均衡。", "conclusion": "HealthQA-BR的发布为全面评估AI在医疗领域的适用性提供了工具，强调了单一分数不足以验证模型的安全性。"}}
{"id": "2506.21866", "pdf": "https://arxiv.org/pdf/2506.21866", "abs": "https://arxiv.org/abs/2506.21866", "authors": ["Yanguang Sun", "Jiexi Yan", "Jianjun Qian", "Chunyan Xu", "Jian Yang", "Lei Luo"], "title": "Dual-Perspective United Transformer for Object Segmentation in Optical Remote Sensing Images", "categories": ["cs.CV"], "comment": "Accepted by IJCAI 2025", "summary": "Automatically segmenting objects from optical remote sensing images (ORSIs)\nis an important task. Most existing models are primarily based on either\nconvolutional or Transformer features, each offering distinct advantages.\nExploiting both advantages is valuable research, but it presents several\nchallenges, including the heterogeneity between the two types of features, high\ncomplexity, and large parameters of the model. However, these issues are often\noverlooked in existing the ORSIs methods, causing sub-optimal segmentation. For\nthat, we propose a novel Dual-Perspective United Transformer (DPU-Former) with\na unique structure designed to simultaneously integrate long-range dependencies\nand spatial details. In particular, we design the global-local mixed attention,\nwhich captures diverse information through two perspectives and introduces a\nFourier-space merging strategy to obviate deviations for efficient fusion.\nFurthermore, we present a gated linear feed-forward network to increase the\nexpressive ability. Additionally, we construct a DPU-Former decoder to\naggregate and strength features at different layers. Consequently, the\nDPU-Former model outperforms the state-of-the-art methods on multiple datasets.\nCode: https://github.com/CSYSI/DPU-Former.", "AI": {"tldr": "提出了一种新型的双视角统一Transformer（DPU-Former），通过全局-局部混合注意力机制和傅里叶空间合并策略，有效结合长程依赖和空间细节，提升了光学遥感图像分割性能。", "motivation": "现有模型主要基于卷积或Transformer特征，各有优势但融合时存在异质性、高复杂性和大参数等问题，导致分割效果不佳。", "method": "设计了全局-局部混合注意力机制和傅里叶空间合并策略，提出门控线性前馈网络增强表达能力，并构建了DPU-Former解码器。", "result": "DPU-Former在多个数据集上优于现有最优方法。", "conclusion": "DPU-Former通过创新设计解决了特征融合的挑战，显著提升了分割效果。"}}
{"id": "2506.21580", "pdf": "https://arxiv.org/pdf/2506.21580", "abs": "https://arxiv.org/abs/2506.21580", "authors": ["Dana Alsagheer", "Yang Lu", "Abdulrahman Kamal", "Omar Kamal", "Mohammad Kamal", "Nada Mansour", "Cosmo Yang Wu", "Rambiba Karanjai", "Sen Li", "Weidong Shi"], "title": "From General Reasoning to Domain Expertise: Uncovering the Limits of Generalization in Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Recent advancements in Large Language Models (LLMs) have demonstrated\nremarkable capabilities in various domains. However, effective decision-making\nrelies heavily on strong reasoning abilities. Reasoning is the foundation for\ndecision-making, providing the analytical and logical framework to make sound\nchoices. Reasoning involves analyzing information, drawing inferences, and\nreaching conclusions based on logic or evidence. Decision-making builds on this\nfoundation by applying the insights from reasoning to select the best course of\naction among alternatives. Together, these processes create a continuous cycle\nof thought and action aimed at achieving goals effectively. As AI technology\nevolves, there is a growing trend to train LLMs to excel in general reasoning.\nThis study explores how the general reasoning capabilities of LLMs connect to\ntheir performance in domain-specific reasoning tasks.", "AI": {"tldr": "研究探讨了大语言模型（LLMs）的通用推理能力如何影响其在特定领域推理任务中的表现。", "motivation": "随着AI技术的发展，训练LLMs在通用推理中表现出色成为趋势，但决策能力依赖于强大的推理能力，因此需要研究LLMs的推理能力如何支持特定任务。", "method": "通过分析LLMs的通用推理能力与领域特定推理任务表现之间的关系进行研究。", "result": "研究发现LLMs的通用推理能力与其在特定领域任务中的表现密切相关。", "conclusion": "LLMs的通用推理能力是其在特定领域任务中表现的关键因素，未来可进一步优化推理能力以提升决策效果。"}}
{"id": "2506.21873", "pdf": "https://arxiv.org/pdf/2506.21873", "abs": "https://arxiv.org/abs/2506.21873", "authors": ["Tzu-Chun Chien", "Chieh-Kai Lin", "Shiang-Feng Tsai", "Ruei-Chi Lai", "Hung-Jen Chen", "Min Sun"], "title": "Grounding-Aware Token Pruning: Recovering from Drastic Performance Drops in Visual Grounding Caused by Pruning", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Recent Multimodal Large Language Models (MLLMs) have demonstrated strong\nperformance in visual grounding, establishing themselves as a general interface\nfor various vision-language applications. This progress has driven the\ndevelopment of token pruning methods to mitigate the high computational costs\nassociated with processing numerous visual tokens. However, we observe that\npruning significantly weakens the model's grounding ability, leading to\nincorrect predictions and drastic performance degradation. In Referring\nExpression Comprehension (REC), for instance, pruning causes the accuracy of\nLLaVA on the RefCOCO validation set to drop from 56.14% to 15.34%. Our analysis\nidentifies misaligned position IDs after pruning as the primary cause of this\ndegradation, as both the order and value of these IDs are crucial for\nmaintaining performance in grounding tasks. To address this issue, we propose\nGrounding-Aware Token Pruning (GAP), a simple yet effective adjustment to\nposition IDs that recovers REC accuracy back to 51.42%, which is 90% of the\noriginal performance in the without pruning setting, all while requiring no\nadditional training, memory, or computational overhead. Applied to models such\nas Shikra, MiniGPTv2, and the LLaVA series, our method consistently improves\nperformance across various token pruning strategies.", "AI": {"tldr": "多模态大语言模型（MLLMs）在视觉定位中表现优异，但视觉令牌修剪会显著降低其定位能力。研究发现修剪后位置ID未对齐是主要原因，提出GAP方法无需额外训练即可恢复90%性能。", "motivation": "视觉令牌修剪虽能降低计算成本，但会导致模型定位能力大幅下降，影响性能。", "method": "提出Grounding-Aware Token Pruning（GAP），通过调整位置ID解决修剪导致的性能下降问题。", "result": "GAP方法在RefCOCO验证集上将LLaVA的准确率从修剪后的15.34%提升至51.42%，接近原始性能的90%。", "conclusion": "GAP是一种简单有效的方法，可显著提升修剪后模型的定位性能，适用于多种MLLMs。"}}
{"id": "2506.21582", "pdf": "https://arxiv.org/pdf/2506.21582", "abs": "https://arxiv.org/abs/2506.21582", "authors": ["Sam Yu-Te Lee", "Chengyang Ji", "Shicheng Wen", "Lifu Huang", "Dongyi Liu", "Kwan-Liu Ma"], "title": "VIDEE: Visual and Interactive Decomposition, Execution, and Evaluation of Text Analytics with Intelligent Agents", "categories": ["cs.CL", "cs.AI", "cs.HC"], "comment": null, "summary": "Text analytics has traditionally required specialized knowledge in Natural\nLanguage Processing (NLP) or text analysis, which presents a barrier for\nentry-level analysts. Recent advances in large language models (LLMs) have\nchanged the landscape of NLP by enabling more accessible and automated text\nanalysis (e.g., topic detection, summarization, information extraction, etc.).\nWe introduce VIDEE, a system that supports entry-level data analysts to conduct\nadvanced text analytics with intelligent agents. VIDEE instantiates a\nhuman-agent collaroration workflow consisting of three stages: (1)\nDecomposition, which incorporates a human-in-the-loop Monte-Carlo Tree Search\nalgorithm to support generative reasoning with human feedback, (2) Execution,\nwhich generates an executable text analytics pipeline, and (3) Evaluation,\nwhich integrates LLM-based evaluation and visualizations to support user\nvalidation of execution results. We conduct two quantitative experiments to\nevaluate VIDEE's effectiveness and analyze common agent errors. A user study\ninvolving participants with varying levels of NLP and text analytics experience\n-- from none to expert -- demonstrates the system's usability and reveals\ndistinct user behavior patterns. The findings identify design implications for\nhuman-agent collaboration, validate the practical utility of VIDEE for\nnon-expert users, and inform future improvements to intelligent text analytics\nsystems.", "AI": {"tldr": "VIDEE是一个支持入门级数据分析师进行高级文本分析的系统，通过人机协作工作流（分解、执行、评估）实现自动化文本分析，降低了NLP门槛。", "motivation": "传统文本分析需要NLP专业知识，限制了入门级分析师的使用。VIDEE旨在通过智能代理和LLMs技术，使文本分析更易用和自动化。", "method": "VIDEE采用三阶段工作流：1）分解（结合人类反馈的蒙特卡洛树搜索），2）执行（生成可执行分析管道），3）评估（LLM评估与可视化）。", "result": "实验验证了VIDEE的有效性，用户研究表明其对非专家用户实用，并揭示了用户行为模式。", "conclusion": "VIDEE降低了文本分析门槛，为人机协作设计提供了启示，未来可进一步优化智能文本分析系统。"}}
{"id": "2506.21883", "pdf": "https://arxiv.org/pdf/2506.21883", "abs": "https://arxiv.org/abs/2506.21883", "authors": ["Basudha Pal", "Sharif Amit Kamran", "Brendon Lutnick", "Molly Lucas", "Chaitanya Parmar", "Asha Patel Shah", "David Apfel", "Steven Fakharzadeh", "Lloyd Miller", "Gabriela Cula", "Kristopher Standish"], "title": "GRASP-PsONet: Gradient-based Removal of Spurious Patterns for PsOriasis Severity Classification", "categories": ["cs.CV"], "comment": null, "summary": "Psoriasis (PsO) severity scoring is important for clinical trials but is\nhindered by inter-rater variability and the burden of in person clinical\nevaluation. Remote imaging using patient captured mobile photos offers\nscalability but introduces challenges, such as variation in lighting,\nbackground, and device quality that are often imperceptible to humans but can\nimpact model performance. These factors, along with inconsistencies in\ndermatologist annotations, reduce the reliability of automated severity\nscoring. We propose a framework to automatically flag problematic training\nimages that introduce spurious correlations which degrade model generalization,\nusing a gradient based interpretability approach. By tracing the gradients of\nmisclassified validation images, we detect training samples where model errors\nalign with inconsistently rated examples or are affected by subtle, nonclinical\nartifacts. We apply this method to a ConvNeXT based weakly supervised model\ndesigned to classify PsO severity from phone images. Removing 8.2% of flagged\nimages improves model AUC-ROC by 5% (85% to 90%) on a held out test set.\nCommonly, multiple annotators and an adjudication process ensure annotation\naccuracy, which is expensive and time consuming. Our method detects training\nimages with annotation inconsistencies, potentially removing the need for\nmanual review. When applied to a subset of training data rated by two\ndermatologists, the method identifies over 90% of cases with inter-rater\ndisagreement by reviewing only the top 30% of samples. This improves automated\nscoring for remote assessments, ensuring robustness despite data collection\nvariability.", "AI": {"tldr": "提出一种基于梯度解释性的框架，自动标记训练图像中的问题样本，提升银屑病严重程度自动评分的模型性能。", "motivation": "解决银屑病严重程度评分中因图像质量差异和标注不一致导致的模型泛化能力下降问题。", "method": "使用梯度解释性方法追踪误分类验证图像的梯度，检测标注不一致或受非临床伪影影响的训练样本。", "result": "移除8.2%的问题图像后，模型AUC-ROC提升5%（85%至90%），并高效识别标注不一致样本。", "conclusion": "该方法显著提升远程评估的自动化评分鲁棒性，减少对人工标注的依赖。"}}
{"id": "2506.21583", "pdf": "https://arxiv.org/pdf/2506.21583", "abs": "https://arxiv.org/abs/2506.21583", "authors": ["Muhammad Ahmad", "Muhammad Waqas", "Ameer Hamza", "Ildar Batyrshin", "Grigori Sidorov"], "title": "Hope Speech Detection in code-mixed Roman Urdu tweets: A Positive Turn in Natural Language Processing", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "Hope is a positive emotional state involving the expectation of favorable\nfuture outcomes, while hope speech refers to communication that promotes\noptimism, resilience, and support, particularly in adverse contexts. Although\nhope speech detection has gained attention in Natural Language Processing\n(NLP), existing research mainly focuses on high-resource languages and\nstandardized scripts, often overlooking informal and underrepresented forms\nsuch as Roman Urdu. To the best of our knowledge, this is the first study to\naddress hope speech detection in code-mixed Roman Urdu by introducing a\ncarefully annotated dataset, thereby filling a critical gap in inclusive NLP\nresearch for low-resource, informal language varieties. This study makes four\nkey contributions: (1) it introduces the first multi-class annotated dataset\nfor Roman Urdu hope speech, comprising Generalized Hope, Realistic Hope,\nUnrealistic Hope, and Not Hope categories; (2) it explores the psychological\nfoundations of hope and analyzes its linguistic patterns in code-mixed Roman\nUrdu to inform dataset development; (3) it proposes a custom attention-based\ntransformer model optimized for the syntactic and semantic variability of Roman\nUrdu, evaluated using 5-fold cross-validation; and (4) it verifies the\nstatistical significance of performance gains using a t-test. The proposed\nmodel, XLM-R, achieves the best performance with a cross-validation score of\n0.78, outperforming the baseline SVM (0.75) and BiLSTM (0.76), with gains of 4%\nand 2.63% respectively.", "AI": {"tldr": "本文首次研究了代码混合罗马乌尔都语中的希望语音检测，填补了低资源非正式语言在NLP研究中的空白，并提出了一个自定义的基于注意力的Transformer模型。", "motivation": "现有研究主要关注高资源语言和标准化脚本，忽略了非正式和代表性不足的语言形式（如罗马乌尔都语），本文旨在填补这一研究空白。", "method": "研究引入了一个多类标注数据集，探索了希望的心理基础和语言模式，并提出了一种优化的Transformer模型（XLM-R），通过5折交叉验证进行评估。", "result": "XLM-R模型表现最佳，交叉验证得分为0.78，优于基线SVM（0.75）和BiLSTM（0.76），分别提升了4%和2.63%。", "conclusion": "本研究为低资源非正式语言的希望语音检测提供了首个数据集和高效模型，推动了包容性NLP研究的发展。"}}
{"id": "2506.21885", "pdf": "https://arxiv.org/pdf/2506.21885", "abs": "https://arxiv.org/abs/2506.21885", "authors": ["Chuheng Wei", "Ziye Qin", "Ziyan Zhang", "Guoyuan Wu", "Matthew J. Barth"], "title": "Integrating Multi-Modal Sensors: A Review of Fusion Techniques for Intelligent Vehicles", "categories": ["cs.CV", "cs.MM", "cs.RO"], "comment": "Accepted by IEEE IV 2025", "summary": "Multi-sensor fusion plays a critical role in enhancing perception for\nautonomous driving, overcoming individual sensor limitations, and enabling\ncomprehensive environmental understanding. This paper first formalizes\nmulti-sensor fusion strategies into data-level, feature-level, and\ndecision-level categories and then provides a systematic review of deep\nlearning-based methods corresponding to each strategy. We present key\nmulti-modal datasets and discuss their applicability in addressing real-world\nchallenges, particularly in adverse weather conditions and complex urban\nenvironments. Additionally, we explore emerging trends, including the\nintegration of Vision-Language Models (VLMs), Large Language Models (LLMs), and\nthe role of sensor fusion in end-to-end autonomous driving, highlighting its\npotential to enhance system adaptability and robustness. Our work offers\nvaluable insights into current methods and future directions for multi-sensor\nfusion in autonomous driving.", "AI": {"tldr": "本文系统综述了多传感器融合在自动驾驶中的重要性，分类了数据级、特征级和决策级融合策略，并探讨了深度学习方法、多模态数据集及新兴趋势（如VLM和LLM的整合）。", "motivation": "多传感器融合能克服单一传感器的局限性，提升自动驾驶的环境感知能力，尤其在恶劣天气和复杂城市环境中。", "method": "将多传感器融合策略分为数据级、特征级和决策级，并系统回顾了基于深度学习的对应方法。", "result": "提供了关键多模态数据集及其应用，探讨了新兴趋势（如VLM和LLM整合）对系统适应性和鲁棒性的提升。", "conclusion": "本文为多传感器融合在自动驾驶中的当前方法和未来方向提供了有价值的见解。"}}
{"id": "2506.21584", "pdf": "https://arxiv.org/pdf/2506.21584", "abs": "https://arxiv.org/abs/2506.21584", "authors": ["J. Koorndijk"], "title": "Empirical Evidence for Alignment Faking in Small LLMs and Prompt-Based Mitigation Techniques", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": null, "summary": "Current literature suggests that alignment faking (deceptive alignment) is an\nemergent property of large language models. We present the first empirical\nevidence that a small instruction-tuned model, specifically LLaMA 3 8B, can\nalso exhibit alignment faking. We further show that prompt-only interventions,\nincluding deontological moral framing and scratchpad reasoning, significantly\nreduce this behavior without modifying model internals. This challenges the\nassumption that prompt-based ethics are trivial and that deceptive alignment\nrequires scale. We introduce a taxonomy distinguishing shallow deception,\nshaped by context and suppressible through prompting, from deep deception,\nwhich reflects persistent, goal-driven misalignment. Our findings refine the\nunderstanding of deception in language models and underscore the need for\nalignment evaluations across model sizes and deployment settings.", "AI": {"tldr": "研究发现小规模指令调优模型LLaMA 3 8B也能表现出对齐伪装行为，提示干预可显著减少此行为，挑战了传统认为对齐伪装需大规模模型的假设。", "motivation": "探讨小规模模型是否也能表现出对齐伪装行为，并验证提示干预的有效性。", "method": "使用LLaMA 3 8B模型，通过提示干预（如道德框架和推理提示）减少对齐伪装行为。", "result": "提示干预显著减少对齐伪装行为，表明小规模模型也能表现出此行为。", "conclusion": "研究改进了对语言模型伪装行为的理解，强调需在不同规模和部署场景下评估对齐性。"}}
{"id": "2506.21891", "pdf": "https://arxiv.org/pdf/2506.21891", "abs": "https://arxiv.org/abs/2506.21891", "authors": ["Umihiro Kamoto", "Tatsuya Ishibashi", "Noriyuki Kugo"], "title": "DIVE: Deep-search Iterative Video Exploration A Technical Report for the CVRR Challenge at CVPR 2025", "categories": ["cs.CV"], "comment": null, "summary": "In this report, we present the winning solution that achieved the 1st place\nin the Complex Video Reasoning & Robustness Evaluation Challenge 2025. This\nchallenge evaluates the ability to generate accurate natural language answers\nto questions about diverse, real-world video clips. It uses the Complex Video\nReasoning and Robustness Evaluation Suite (CVRR-ES) benchmark, which consists\nof 214 unique videos and 2,400 question-answer pairs spanning 11 categories.\nOur method, DIVE (Deep-search Iterative Video Exploration), adopts an iterative\nreasoning approach, in which each input question is semantically decomposed and\nsolved through stepwise reasoning and progressive inference. This enables our\nsystem to provide highly accurate and contextually appropriate answers to even\nthe most complex queries. Applied to the CVRR-ES benchmark, our approach\nachieves 81.44% accuracy on the test set, securing the top position among all\nparticipants. This report details our methodology and provides a comprehensive\nanalysis of the experimental results, demonstrating the effectiveness of our\niterative reasoning framework in achieving robust video question answering. The\ncode is available at https://github.com/PanasonicConnect/DIVE", "AI": {"tldr": "DIVE方法在2025年复杂视频推理与鲁棒性评估挑战赛中夺冠，通过迭代推理实现高精度视频问答。", "motivation": "解决复杂视频问答任务，提升对多样化视频内容的理解和推理能力。", "method": "采用DIVE（深度搜索迭代视频探索）方法，通过语义分解和逐步推理解决问题。", "result": "在CVRR-ES基准测试中达到81.44%的准确率，排名第一。", "conclusion": "迭代推理框架在视频问答中表现出色，代码已开源。"}}
{"id": "2506.21585", "pdf": "https://arxiv.org/pdf/2506.21585", "abs": "https://arxiv.org/abs/2506.21585", "authors": ["Christoph Brosch", "Sian Brumm", "Rolf Krieger", "Jonas Scheffler"], "title": "Evaluation of LLM-based Strategies for the Extraction of Food Product Information from Online Shops", "categories": ["cs.CL", "cs.IR", "cs.LG"], "comment": "Preprint for paper presented at DATA 2025 in Bilbao, Spain. Corrected\n  -2.27 to -1.61 in abstract and +2.27 to +1.61 in discussion. Reference to\n  journal and publication will follow", "summary": "Generative AI and large language models (LLMs) offer significant potential\nfor automating the extraction of structured information from web pages. In this\nwork, we focus on food product pages from online retailers and explore\nschema-constrained extraction approaches to retrieve key product attributes,\nsuch as ingredient lists and nutrition tables. We compare two LLM-based\napproaches, direct extraction and indirect extraction via generated functions,\nevaluating them in terms of accuracy, efficiency, and cost on a curated dataset\nof 3,000 food product pages from three different online shops. Our results show\nthat although the indirect approach achieves slightly lower accuracy (96.48\\%,\n$-1.61\\%$ compared to direct extraction), it reduces the number of required LLM\ncalls by 95.82\\%, leading to substantial efficiency gains and lower operational\ncosts. These findings suggest that indirect extraction approaches can provide\nscalable and cost-effective solutions for large-scale information extraction\ntasks from template-based web pages using LLMs.", "AI": {"tldr": "比较了两种基于LLM的方法（直接提取和间接提取）从食品产品页面提取结构化信息的性能，间接方法在准确率略低的情况下显著减少了LLM调用次数，提升了效率和成本效益。", "motivation": "探索利用生成式AI和LLM自动化从网页中提取结构化信息的潜力，特别是在食品产品页面的应用。", "method": "比较了直接提取和间接提取（通过生成函数）两种LLM方法，评估了准确性、效率和成本。", "result": "间接提取方法准确率略低（96.48%，比直接提取低1.61%），但减少了95.82%的LLM调用次数，显著提升了效率和降低了成本。", "conclusion": "间接提取方法为基于模板的网页大规模信息提取提供了可扩展且经济高效的解决方案。"}}
{"id": "2506.21892", "pdf": "https://arxiv.org/pdf/2506.21892", "abs": "https://arxiv.org/abs/2506.21892", "authors": ["Adam Goodge", "Xun Xu", "Bryan Hooi", "Wee Siong Ng", "Jingyi Liao", "Yongyi Su", "Xulei Yang"], "title": "SODA: Out-of-Distribution Detection in Domain-Shifted Point Clouds via Neighborhood Propagation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "As point cloud data increases in prevalence in a variety of applications, the\nability to detect out-of-distribution (OOD) point cloud objects becomes\ncritical for ensuring model safety and reliability. However, this problem\nremains under-explored in existing research. Inspired by success in the image\ndomain, we propose to exploit advances in 3D vision-language models (3D VLMs)\nfor OOD detection in point cloud objects. However, a major challenge is that\npoint cloud datasets used to pre-train 3D VLMs are drastically smaller in size\nand object diversity than their image-based counterparts. Critically, they\noften contain exclusively computer-designed synthetic objects. This leads to a\nsubstantial domain shift when the model is transferred to practical tasks\ninvolving real objects scanned from the physical environment. In this paper,\nour empirical experiments show that synthetic-to-real domain shift\nsignificantly degrades the alignment of point cloud with their associated text\nembeddings in the 3D VLM latent space, hindering downstream performance. To\naddress this, we propose a novel methodology called SODA which improves the\ndetection of OOD point clouds through a neighborhood-based score propagation\nscheme. SODA is inference-based, requires no additional model training, and\nachieves state-of-the-art performance over existing approaches across datasets\nand problem settings.", "AI": {"tldr": "论文提出了一种基于3D视觉语言模型（3D VLMs）的方法SODA，用于检测点云数据中的分布外（OOD）对象，解决了合成数据与真实数据之间的域偏移问题。", "motivation": "点云数据在多种应用中日益普及，但检测分布外点云对象的问题尚未充分研究。合成数据与真实数据之间的域偏移影响了3D VLMs的性能。", "method": "提出SODA方法，通过邻域评分传播方案改进OOD检测，无需额外训练，基于推理实现。", "result": "SODA在多个数据集和问题设置中优于现有方法，达到最先进性能。", "conclusion": "SODA有效解决了合成数据与真实数据之间的域偏移问题，提升了OOD点云检测的性能。"}}
{"id": "2506.21586", "pdf": "https://arxiv.org/pdf/2506.21586", "abs": "https://arxiv.org/abs/2506.21586", "authors": ["Hyundong Cho", "Spencer Lin", "Tejas Srinivasan", "Michael Saxon", "Deuksin Kwon", "Natali T. Chavez", "Jonathan May"], "title": "Can Vision Language Models Understand Mimed Actions?", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "ACL 2025 Findings", "summary": "Nonverbal communication (NVC) plays an integral role in human language, but\nstudying NVC in general is challenging because of its broad scope and high\nvariance in interpretation among individuals and cultures. However, mime -- the\ntheatrical technique of suggesting intent using only gesture, expression, and\nmovement -- is a subset of NVC that consists of explicit and embodied actions\nwith much lower human interpretation variance. We argue that a solid\nunderstanding of mimed actions is a crucial prerequisite for vision-language\nmodels capable of interpreting and commanding more subtle aspects of NVC.\nHence, we propose Mime Identification Multimodal Evaluation (MIME), a novel\nvideo-based question answering benchmark comprising of 86 mimed actions.\nConstructed with motion capture data, MIME consists of variations of each\naction with perturbations applied to the character, background, and viewpoint\nfor evaluating recognition robustness. We find that both open-weight and\nAPI-based vision-language models perform significantly worse than humans on\nMIME, motivating the need for increased research for instilling more robust\nunderstanding of human gestures.", "AI": {"tldr": "论文提出MIME基准，用于评估视觉语言模型对哑剧动作的理解能力，发现现有模型表现远逊于人类。", "motivation": "研究哑剧动作（NVC子集）是理解更复杂非语言交流的前提，但现有模型表现不佳。", "method": "构建MIME基准，包含86种哑剧动作，通过动作捕捉数据生成多种变体以测试鲁棒性。", "result": "现有视觉语言模型在MIME上表现显著低于人类。", "conclusion": "需加强研究以提升模型对人类手势的理解能力。"}}
{"id": "2506.21895", "pdf": "https://arxiv.org/pdf/2506.21895", "abs": "https://arxiv.org/abs/2506.21895", "authors": ["Fangling Jiang", "Qi Li", "Weining Wang", "Gang Wang", "Bing Liu", "Zhenan Sun"], "title": "Exploring Task-Solving Paradigm for Generalized Cross-Domain Face Anti-Spoofing via Reinforcement Fine-Tuning", "categories": ["cs.CV"], "comment": null, "summary": "Recently the emergence of novel presentation attacks has drawn increasing\nattention to face anti-spoofing. However, existing methods tend to memorize\ndata patterns from the training set, resulting in poor generalization to\nunknown attack types across different scenarios and limited interpretability.\nTo address these challenges, this paper presents a reinforcement\nfine-tuning-based face anti-spoofing method that stimulates the capabilities of\nmultimodal large language models to think and learn how to solve the\nanti-spoofing task itself, rather than relying on the memorization of\nauthenticity patterns. We design verifiable class consistent reward and\nreasoning consistent reward, and employ a GRPO-based optimization strategy to\nguide the model in exploring reasoning policies from multiple perspectives to\nmaximize expected rewards. As a result, through iterative trial-and-error\nlearning while retaining only high-reward trajectories, the model distills\nhighly generalizable decision-making rules from the extensive solution space to\neffectively address cross-domain face anti-spoofing tasks. Extensive\nexperimental results demonstrate that our method achieves state-of-the-art\ncross-domain generalization performance. It generalizes well to diverse unknown\nattack types in unseen target domains while providing interpretable reasoning\nfor its authenticity decisions without requiring labor-intensive textual\nannotations for training.", "AI": {"tldr": "提出了一种基于强化微调的面部防伪方法，通过多模态大语言模型自主学习和推理，而非依赖模式记忆，提升了跨域泛化能力和可解释性。", "motivation": "现有面部防伪方法易记忆训练数据模式，泛化能力差且缺乏可解释性。", "method": "设计了可验证的类一致奖励和推理一致奖励，采用GRPO优化策略，引导模型从多角度探索推理策略。", "result": "实验表明，该方法在跨域泛化性能上达到最优，能有效应对未知攻击类型并提供可解释的决策。", "conclusion": "该方法通过强化学习提炼通用决策规则，显著提升了面部防伪的泛化能力和可解释性。"}}
{"id": "2506.21587", "pdf": "https://arxiv.org/pdf/2506.21587", "abs": "https://arxiv.org/abs/2506.21587", "authors": ["Weihong Qi", "Fan Huang", "Jisun An", "Haewoon Kwak"], "title": "Is DeepSeek a New Voice Among LLMs in Public Opinion Simulation?", "categories": ["cs.CL"], "comment": null, "summary": "This study evaluates the ability of DeepSeek, an open-source large language\nmodel (LLM), to simulate public opinions in comparison to LLMs developed by\nmajor tech companies. By comparing DeepSeek-R1 and DeepSeek-V3 with Qwen2.5,\nGPT-4o, and Llama-3.3 and utilizing survey data from the American National\nElection Studies (ANES) and the Zuobiao dataset of China, we assess these\nmodels' capacity to predict public opinions on social issues in both China and\nthe United States, highlighting their comparative capabilities between\ncountries. Our findings indicate that DeepSeek-V3 performs best in simulating\nU.S. opinions on the abortion issue compared to other topics such as climate\nchange, gun control, immigration, and services for same-sex couples, primarily\nbecause it more accurately simulates responses when provided with Democratic or\nliberal personas. For Chinese samples, DeepSeek-V3 performs best in simulating\nopinions on foreign aid and individualism but shows limitations in modeling\nviews on capitalism, particularly failing to capture the stances of low-income\nand non-college-educated individuals. It does not exhibit significant\ndifferences from other models in simulating opinions on traditionalism and the\nfree market. Further analysis reveals that all LLMs exhibit the tendency to\novergeneralize a single perspective within demographic groups, often defaulting\nto consistent responses within groups. These findings highlight the need to\nmitigate cultural and demographic biases in LLM-driven public opinion modeling,\ncalling for approaches such as more inclusive training methodologies.", "AI": {"tldr": "DeepSeek-V3在模拟美国和中国公众意见方面表现最佳，但存在文化和人口偏见问题。", "motivation": "评估开源大模型DeepSeek在模拟公众意见方面的能力，并与主流科技公司的模型进行比较。", "method": "比较DeepSeek-R1、DeepSeek-V3与Qwen2.5、GPT-4o、Llama-3.3，使用ANES和中国Zuobiao数据集评估模型预测中美社会议题公众意见的能力。", "result": "DeepSeek-V3在模拟美国堕胎议题和中国对外援助及个人主义议题上表现最佳，但在模拟低收入和非大学教育人群观点时存在局限。所有模型倾向于过度概括群体内单一观点。", "conclusion": "需减少LLM在公众意见建模中的文化和人口偏见，建议采用更具包容性的训练方法。"}}
{"id": "2506.21903", "pdf": "https://arxiv.org/pdf/2506.21903", "abs": "https://arxiv.org/abs/2506.21903", "authors": ["Dipayan Biswas", "Shishir Shah", "Jaspal Subhlok"], "title": "Visual Content Detection in Educational Videos with Transfer Learning and Dataset Enrichment", "categories": ["cs.CV"], "comment": "This is an extended version of a paper accepted to MIPR 2025", "summary": "Video is transforming education with online courses and recorded lectures\nsupplementing and replacing classroom teaching. Recent research has focused on\nenhancing information retrieval for video lectures with advanced navigation,\nsearchability, summarization, as well as question answering chatbots. Visual\nelements like tables, charts, and illustrations are central to comprehension,\nretention, and data presentation in lecture videos, yet their full potential\nfor improving access to video content remains underutilized. A major factor is\nthat accurate automatic detection of visual elements in a lecture video is\nchallenging; reasons include i) most visual elements, such as charts, graphs,\ntables, and illustrations, are artificially created and lack any standard\nstructure, and ii) coherent visual objects may lack clear boundaries and may be\ncomposed of connected text and visual components. Despite advancements in deep\nlearning based object detection, current models do not yield satisfactory\nperformance due to the unique nature of visual content in lectures and scarcity\nof annotated datasets. This paper reports on a transfer learning approach for\ndetecting visual elements in lecture video frames. A suite of state of the art\nobject detection models were evaluated for their performance on lecture video\ndatasets. YOLO emerged as the most promising model for this task. Subsequently\nYOLO was optimized for lecture video object detection with training on multiple\nbenchmark datasets and deploying a semi-supervised auto labeling strategy.\nResults evaluate the success of this approach, also in developing a general\nsolution to the problem of object detection in lecture videos. Paper\ncontributions include a publicly released benchmark of annotated lecture video\nframes, along with the source code to facilitate future research.", "AI": {"tldr": "论文提出了一种基于迁移学习的方法，用于检测讲座视频中的视觉元素，优化了YOLO模型，并发布了标注数据集和源代码。", "motivation": "讲座视频中的视觉元素（如图表、表格）对理解和检索至关重要，但自动检测这些元素存在挑战，缺乏标准结构和标注数据。", "method": "采用迁移学习方法，评估了多种目标检测模型，优化YOLO模型，并使用半监督自动标注策略。", "result": "YOLO表现最佳，优化后的模型在讲座视频中检测视觉元素效果显著，并开发了通用解决方案。", "conclusion": "研究成功解决了讲座视频中视觉元素检测问题，发布了标注数据集和源代码，推动未来研究。"}}
{"id": "2506.21588", "pdf": "https://arxiv.org/pdf/2506.21588", "abs": "https://arxiv.org/abs/2506.21588", "authors": ["Ilya Lasy", "Peter Knees", "Stefan Woltran"], "title": "Understanding Verbatim Memorization in LLMs Through Circuit Discovery", "categories": ["cs.CL"], "comment": "The First Workshop on Large Language Model Memorization @ ACL 2025,\n  Vienna, August 1st, 2025", "summary": "Underlying mechanisms of memorization in LLMs -- the verbatim reproduction of\ntraining data -- remain poorly understood. What exact part of the network\ndecides to retrieve a token that we would consider as start of memorization\nsequence? How exactly is the models' behaviour different when producing\nmemorized sentence vs non-memorized? In this work we approach these questions\nfrom mechanistic interpretability standpoint by utilizing transformer circuits\n-- the minimal computational subgraphs that perform specific functions within\nthe model. Through carefully constructed contrastive datasets, we identify\npoints where model generation diverges from memorized content and isolate the\nspecific circuits responsible for two distinct aspects of memorization. We find\nthat circuits that initiate memorization can also maintain it once started,\nwhile circuits that only maintain memorization cannot trigger its initiation.\nIntriguingly, memorization prevention mechanisms transfer robustly across\ndifferent text domains, while memorization induction appears more\ncontext-dependent.", "AI": {"tldr": "该研究通过机制解释性方法，探索了大型语言模型中记忆化的机制，识别了触发和维持记忆化的特定电路，并发现记忆化预防机制在不同文本领域中具有鲁棒性。", "motivation": "理解大型语言模型中记忆化的具体机制，包括记忆化序列的触发点和模型行为的差异。", "method": "利用变压器电路和对比数据集，识别记忆化与非记忆化生成的分歧点，并分离相关电路。", "result": "发现触发记忆化的电路也能维持记忆化，而仅维持记忆化的电路无法触发其启动；记忆化预防机制在不同领域中具有鲁棒性。", "conclusion": "记忆化的触发和维持由不同电路控制，预防机制具有跨领域鲁棒性，而触发机制更依赖上下文。"}}
{"id": "2506.21905", "pdf": "https://arxiv.org/pdf/2506.21905", "abs": "https://arxiv.org/abs/2506.21905", "authors": ["Mingquan Liu"], "title": "RAUM-Net: Regional Attention and Uncertainty-aware Mamba Network", "categories": ["cs.CV"], "comment": null, "summary": "Fine Grained Visual Categorization (FGVC) remains a challenging task in\ncomputer vision due to subtle inter class differences and fragile feature\nrepresentations. Existing methods struggle in fine grained scenarios,\nespecially when labeled data is scarce. We propose a semi supervised method\ncombining Mamba based feature modeling, region attention, and Bayesian\nuncertainty. Our approach enhances local to global feature modeling while\nfocusing on key areas during learning. Bayesian inference selects high quality\npseudo labels for stability. Experiments show strong performance on FGVC\nbenchmarks with occlusions, demonstrating robustness when labeled data is\nlimited. Code is available at https://github.com/wxqnl/RAUM Net.", "AI": {"tldr": "提出了一种结合Mamba特征建模、区域注意力和贝叶斯不确定性的半监督方法，用于解决细粒度视觉分类（FGVC）中数据稀缺和特征脆弱的问题。", "motivation": "细粒度视觉分类（FGVC）因类间差异细微且特征表示脆弱而具有挑战性，现有方法在数据稀缺时表现不佳。", "method": "结合Mamba特征建模、区域注意力和贝叶斯不确定性，增强局部到全局特征建模，并通过贝叶斯推理选择高质量伪标签。", "result": "在FGVC基准测试中表现出色，尤其在遮挡情况下具有鲁棒性，适用于标记数据有限的情况。", "conclusion": "该方法在数据稀缺时仍能保持高性能，为FGVC任务提供了一种有效的解决方案。"}}
{"id": "2506.21589", "pdf": "https://arxiv.org/pdf/2506.21589", "abs": "https://arxiv.org/abs/2506.21589", "authors": ["Minjia Mao", "Dongjun Wei", "Xiao Fang", "Michael Chau"], "title": "A General Method for Detecting Information Generated by Large Language Models", "categories": ["cs.CL"], "comment": null, "summary": "The proliferation of large language models (LLMs) has significantly\ntransformed the digital information landscape, making it increasingly\nchallenging to distinguish between human-written and LLM-generated content.\nDetecting LLM-generated information is essential for preserving trust on\ndigital platforms (e.g., social media and e-commerce sites) and preventing the\nspread of misinformation, a topic that has garnered significant attention in IS\nresearch. However, current detection methods, which primarily focus on\nidentifying content generated by specific LLMs in known domains, face\nchallenges in generalizing to new (i.e., unseen) LLMs and domains. This\nlimitation reduces their effectiveness in real-world applications, where the\nnumber of LLMs is rapidly multiplying and content spans a vast array of\ndomains. In response, we introduce a general LLM detector (GLD) that combines a\ntwin memory networks design and a theory-guided detection generalization module\nto detect LLM-generated information across unseen LLMs and domains. Using\nreal-world datasets, we conduct extensive empirical evaluations and case\nstudies to demonstrate the superiority of GLD over state-of-the-art detection\nmethods. The study has important academic and practical implications for\ndigital platforms and LLMs.", "AI": {"tldr": "论文提出了一种通用LLM检测器（GLD），用于检测未见过的LLM和领域生成的文本，解决了现有方法泛化能力不足的问题。", "motivation": "随着LLM的普及，区分人类和LLM生成内容变得困难，影响数字平台的信任和防止错误信息传播。现有方法在泛化到新LLM和领域时表现不佳。", "method": "GLD结合了双记忆网络设计和理论指导的检测泛化模块，以检测未见过的LLM和领域生成的内容。", "result": "通过真实数据集评估，GLD在检测性能上优于现有方法。", "conclusion": "GLD为数字平台和LLM提供了重要的学术和实践价值。"}}
{"id": "2506.21909", "pdf": "https://arxiv.org/pdf/2506.21909", "abs": "https://arxiv.org/abs/2506.21909", "authors": ["Justin Reinman", "Sunwoong Choi"], "title": "CERBERUS: Crack Evaluation & Recognition Benchmark for Engineering Reliability & Urban Stability", "categories": ["cs.CV"], "comment": null, "summary": "CERBERUS is a synthetic benchmark designed to help train and evaluate AI\nmodels for detecting cracks and other defects in infrastructure. It includes a\ncrack image generator and realistic 3D inspection scenarios built in Unity. The\nbenchmark features two types of setups: a simple Fly-By wall inspection and a\nmore complex Underpass scene with lighting and geometry challenges. We tested a\npopular object detection model (YOLO) using different combinations of synthetic\nand real crack data. Results show that combining synthetic and real data\nimproves performance on real-world images. CERBERUS provides a flexible,\nrepeatable way to test defect detection systems and supports future research in\nautomated infrastructure inspection. CERBERUS is publicly available at\nhttps://github.com/justinreinman/Cerberus-Defect-Generator.", "AI": {"tldr": "CERBERUS是一个用于训练和评估基础设施缺陷检测AI模型的合成基准，包含裂缝图像生成器和Unity构建的3D检查场景。测试表明，合成与真实数据结合可提升性能。", "motivation": "为基础设施缺陷检测提供灵活、可重复的测试方法，支持自动化检查研究。", "method": "使用CERBERUS生成合成数据，结合真实数据训练YOLO模型，测试在Fly-By和Underpass场景中的表现。", "result": "合成与真实数据结合显著提升了模型在真实图像上的检测性能。", "conclusion": "CERBERUS为缺陷检测系统提供了有效的测试工具，并支持未来研究。"}}
{"id": "2506.21590", "pdf": "https://arxiv.org/pdf/2506.21590", "abs": "https://arxiv.org/abs/2506.21590", "authors": ["Junqi Jiang", "Tom Bewley", "Salim I. Amoukou", "Francesco Leofante", "Antonio Rago", "Saumitra Mishra", "Francesca Toni"], "title": "Representation Consistency for Accurate and Coherent LLM Answer Aggregation", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Test-time scaling improves large language models' (LLMs) performance by\nallocating more compute budget during inference. To achieve this, existing\nmethods often require intricate modifications to prompting and sampling\nstrategies. In this work, we introduce representation consistency (RC), a\ntest-time scaling method for aggregating answers drawn from multiple candidate\nresponses of an LLM regardless of how they were generated, including variations\nin prompt phrasing and sampling strategy. RC enhances answer aggregation by not\nonly considering the number of occurrences of each answer in the candidate\nresponse set, but also the consistency of the model's internal activations\nwhile generating the set of responses leading to each answer. These activations\ncan be either dense (raw model activations) or sparse (encoded via pretrained\nsparse autoencoders). Our rationale is that if the model's representations of\nmultiple responses converging on the same answer are highly variable, this\nanswer is more likely to be the result of incoherent reasoning and should be\ndown-weighted during aggregation. Importantly, our method only uses cached\nactivations and lightweight similarity computations and requires no additional\nmodel queries. Through experiments with four open-source LLMs and four\nreasoning datasets, we validate the effectiveness of RC for improving task\nperformance during inference, with consistent accuracy improvements (up to 4%)\nover strong test-time scaling baselines. We also show that consistency in the\nsparse activation signals aligns well with the common notion of coherent\nreasoning.", "AI": {"tldr": "提出了一种基于表示一致性（RC）的测试时扩展方法，通过聚合多个候选回答并考虑模型内部激活的一致性，提升大语言模型（LLM）的推理性能。", "motivation": "现有测试时扩展方法需要复杂的提示和采样策略修改，RC方法旨在简化这一过程，仅需轻量级计算即可提升性能。", "method": "通过分析模型生成多个候选回答时的内部激活（密集或稀疏）一致性，加权聚合答案，无需额外模型查询。", "result": "在四个开源LLM和四个推理数据集上验证，RC方法显著提升任务性能（最高4%），稀疏激活信号与连贯推理概念一致。", "conclusion": "RC方法通过轻量级计算有效提升LLM推理性能，且稀疏激活信号能反映连贯推理。"}}
{"id": "2506.21912", "pdf": "https://arxiv.org/pdf/2506.21912", "abs": "https://arxiv.org/abs/2506.21912", "authors": ["Xinghan Wang", "Kun Xu", "Fei Li", "Cao Sheng", "Jiazhong Yu", "Yadong Mu"], "title": "Generating Attribute-Aware Human Motions from Textual Prompt", "categories": ["cs.CV", "cs.MM"], "comment": null, "summary": "Text-driven human motion generation has recently attracted considerable\nattention, allowing models to generate human motions based on textual\ndescriptions. However, current methods neglect the influence of human\nattributes (such as age, gender, weight, and height) which are key factors\nshaping human motion patterns. This work represents a pilot exploration for\nbridging this gap. We conceptualize each motion as comprising both attribute\ninformation and action semantics, where textual descriptions align exclusively\nwith action semantics. To achieve this, a new framework inspired by Structural\nCausal Models is proposed to decouple action semantics from human attributes,\nenabling text-to-semantics prediction and attribute-controlled generation. The\nresulting model is capable of generating realistic, attribute-aware motion\naligned with the user's text and attribute inputs. For evaluation, we introduce\nHumanAttr, a comprehensive dataset containing attribute annotations for\ntext-motion pairs, setting the first benchmark for attribute-aware\ntext-to-motion generation. Extensive experiments on the new dataset validate\nour model's effectiveness.", "AI": {"tldr": "本文提出了一种基于文本和人类属性的动作生成框架，通过解耦动作语义和属性信息，实现了属性感知的文本到动作生成。", "motivation": "现有文本驱动动作生成方法忽视了人类属性（如年龄、性别、体重、身高）对动作模式的影响，本文旨在填补这一空白。", "method": "提出了一种受结构因果模型启发的框架，解耦动作语义和人类属性，支持文本到语义预测和属性控制生成。", "result": "模型能够生成与用户文本和属性输入一致的逼真动作，并引入HumanAttr数据集作为首个属性感知文本到动作生成的基准。", "conclusion": "实验验证了模型的有效性，为属性感知的文本到动作生成提供了新思路。"}}
{"id": "2506.21591", "pdf": "https://arxiv.org/pdf/2506.21591", "abs": "https://arxiv.org/abs/2506.21591", "authors": ["Shaoyu Dou", "Yutian Shen", "Mofan Chen", "Zixuan Wang", "Jiajie Xu", "Qi Guo", "Kailai Shao", "Chao Chen", "Haixiang Hu", "Haibo Shi", "Min Min", "Liwen Zhang"], "title": "FinEval-KR: A Financial Domain Evaluation Framework for Large Language Models' Knowledge and Reasoning", "categories": ["cs.CL"], "comment": "Submitted to EMNLP 2025, 27 pages, 20 figures", "summary": "Large Language Models (LLMs) demonstrate significant potential but face\nchallenges in complex financial reasoning tasks requiring both domain knowledge\nand sophisticated reasoning. Current evaluation benchmarks often fall short by\nnot decoupling these capabilities indicators from single task performance and\nlack root cause analysis for task failure. To address this, we introduce\nFinEval-KR, a novel evaluation framework for decoupling and quantifying LLMs'\nknowledge and reasoning abilities independently, proposing distinct knowledge\nscore and reasoning score metrics. Inspired by cognitive science, we further\npropose a cognitive score based on Bloom's taxonomy to analyze capabilities in\nreasoning tasks across different cognitive levels. We also release a new\nopen-source Chinese financial reasoning dataset covering 22 subfields to\nsupport reproducible research and further advancements in financial reasoning.\nOur experimental results reveal that LLM reasoning ability and higher-order\ncognitive ability are the core factors influencing reasoning accuracy. We also\nspecifically find that even top models still face a bottleneck with knowledge\napplication. Furthermore, our analysis shows that specialized financial LLMs\ngenerally lag behind the top general large models across multiple metrics.", "AI": {"tldr": "FinEval-KR是一个新框架，用于独立评估LLMs的知识和推理能力，并引入认知分数分析不同认知层次的能力。实验表明推理能力和高阶认知能力是关键因素，但知识应用仍是瓶颈。", "motivation": "当前评估基准未能分离LLMs的知识和推理能力，且缺乏任务失败的根因分析，FinEval-KR旨在解决这一问题。", "method": "提出FinEval-KR框架，包括知识分数和推理分数指标，并基于Bloom分类法引入认知分数。发布开源中文金融推理数据集。", "result": "实验显示LLMs的推理能力和高阶认知能力是影响准确性的核心因素，但知识应用仍是瓶颈。专业金融LLMs表现普遍落后于通用大模型。", "conclusion": "FinEval-KR为评估LLMs提供新视角，揭示了知识和推理能力的分离重要性，并指出知识应用的改进方向。"}}
{"id": "2506.21920", "pdf": "https://arxiv.org/pdf/2506.21920", "abs": "https://arxiv.org/abs/2506.21920", "authors": ["Nam Quan Nguyen", "Xuan Phong Pham", "Tuan-Anh Tran"], "title": "SepFormer: Coarse-to-fine Separator Regression Network for Table Structure Recognition", "categories": ["cs.CV"], "comment": null, "summary": "The automated reconstruction of the logical arrangement of tables from image\ndata, termed Table Structure Recognition (TSR), is fundamental for semantic\ndata extraction. Recently, researchers have explored a wide range of techniques\nto tackle this problem, demonstrating significant progress. Each table is a set\nof vertical and horizontal separators. Following this realization, we present\nSepFormer, which integrates the split-and-merge paradigm into a single step\nthrough separator regression with a DETR-style architecture, improving speed\nand robustness. SepFormer is a coarse-to-fine approach that predicts table\nseparators from single-line to line-strip separators with a stack of two\ntransformer decoders. In the coarse-grained stage, the model learns to\ngradually refine single-line segments through decoder layers with additional\nangle loss. At the end of the fine-grained stage, the model predicts line-strip\nseparators by refining sampled points from each single-line segment. Our\nSepFormer can run on average at 25.6 FPS while achieving comparable performance\nwith state-of-the-art methods on several benchmark datasets, including SciTSR,\nPubTabNet, WTW, and iFLYTAB.", "AI": {"tldr": "SepFormer是一种基于DETR架构的表格结构识别方法，通过单步分割与合并范式提升速度和鲁棒性。", "motivation": "表格结构识别（TSR）是语义数据提取的基础，现有方法在速度和鲁棒性上仍有改进空间。", "method": "SepFormer采用粗到细的两阶段方法，通过两个Transformer解码器预测表格分隔线，并引入角度损失优化单线分割。", "result": "SepFormer在多个基准数据集上达到25.6 FPS的速度，性能与最先进方法相当。", "conclusion": "SepFormer通过单步分割与合并范式，显著提升了表格结构识别的速度和鲁棒性。"}}
{"id": "2506.21592", "pdf": "https://arxiv.org/pdf/2506.21592", "abs": "https://arxiv.org/abs/2506.21592", "authors": ["Tinh Nguyen", "Minh Khue Phan Tran"], "title": "SignBart -- New approach with the skeleton sequence for Isolated Sign language Recognition", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Sign language recognition is crucial for individuals with hearing impairments\nto break communication barriers. However, previous approaches have had to\nchoose between efficiency and accuracy. Such as RNNs, LSTMs, and GCNs, had\nproblems with vanishing gradients and high computational costs. Despite\nimproving performance, transformer-based methods were not commonly used. This\nstudy presents a new novel SLR approach that overcomes the challenge of\nindependently extracting meaningful information from the x and y coordinates of\nskeleton sequences, which traditional models often treat as inseparable. By\nutilizing an encoder-decoder of BART architecture, the model independently\nencodes the x and y coordinates, while Cross-Attention ensures their\ninterrelation is maintained. With only 749,888 parameters, the model achieves\n96.04% accuracy on the LSA-64 dataset, significantly outperforming previous\nmodels with over one million parameters. The model also demonstrates excellent\nperformance and generalization across WLASL and ASL-Citizen datasets. Ablation\nstudies underscore the importance of coordinate projection, normalization, and\nusing multiple skeleton components for boosting model efficacy. This study\noffers a reliable and effective approach for sign language recognition, with\nstrong potential for enhancing accessibility tools for the deaf and hard of\nhearing.", "AI": {"tldr": "提出了一种基于BART架构的新型手语识别方法，通过独立编码x和y坐标并利用交叉注意力保持其关联性，显著提升了效率和准确性。", "motivation": "解决传统方法在手语识别中效率和准确性难以兼顾的问题，尤其是RNN、LSTM和GCN等模型存在的梯度消失和高计算成本问题。", "method": "采用BART架构的编码器-解码器模型，独立编码x和y坐标，并通过交叉注意力维持其关联性。", "result": "在LSA-64数据集上达到96.04%的准确率，参数仅749,888，优于参数超过百万的先前模型，并在WLASL和ASL-Citizen数据集上表现优异。", "conclusion": "该方法为手语识别提供了可靠且高效的解决方案，有望提升听障人士的辅助工具性能。"}}
{"id": "2506.21923", "pdf": "https://arxiv.org/pdf/2506.21923", "abs": "https://arxiv.org/abs/2506.21923", "authors": ["Juming Xiong", "Ruining Deng", "Jialin Yue", "Siqi Lu", "Junlin Guo", "Marilyn Lionts", "Tianyuan Yao", "Can Cui", "Junchao Zhu", "Chongyu Qu", "Mengmeng Yin", "Haichun Yang", "Yuankai Huo"], "title": "ZeroReg3D: A Zero-shot Registration Pipeline for 3D Consecutive Histopathology Image Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Histological analysis plays a crucial role in understanding tissue structure\nand pathology. While recent advancements in registration methods have improved\n2D histological analysis, they often struggle to preserve critical 3D spatial\nrelationships, limiting their utility in both clinical and research\napplications. Specifically, constructing accurate 3D models from 2D slices\nremains challenging due to tissue deformation, sectioning artifacts,\nvariability in imaging techniques, and inconsistent illumination. Deep\nlearning-based registration methods have demonstrated improved performance but\nsuffer from limited generalizability and require large-scale training data. In\ncontrast, non-deep-learning approaches offer better generalizability but often\ncompromise on accuracy. In this study, we introduced ZeroReg3D, a novel\nzero-shot registration pipeline tailored for accurate 3D reconstruction from\nserial histological sections. By combining zero-shot deep learning-based\nkeypoint matching with optimization-based affine and non-rigid registration\ntechniques, ZeroReg3D effectively addresses critical challenges such as tissue\ndeformation, sectioning artifacts, staining variability, and inconsistent\nillumination without requiring retraining or fine-tuning. The code has been\nmade publicly available at https://github.com/hrlblab/ZeroReg3D", "AI": {"tldr": "ZeroReg3D是一种新型零样本配准方法，用于从连续组织切片构建精确的3D模型，解决了传统方法在3D空间关系保留上的不足。", "motivation": "现有2D组织学分析方法难以保留3D空间关系，且深度学习方法泛化性差，非深度学习方法精度不足。", "method": "结合零样本深度学习的特征点匹配与基于优化的仿射和非刚性配准技术。", "result": "有效解决了组织变形、切片伪影、染色变异和光照不一致等问题，无需重新训练或微调。", "conclusion": "ZeroReg3D在3D组织重建中表现出色，代码已开源。"}}
{"id": "2506.21579", "pdf": "https://arxiv.org/pdf/2506.21579", "abs": "https://arxiv.org/abs/2506.21579", "authors": ["Yingzhi He", "Xiaohao Liu", "An Zhang", "Yunshan Ma", "Tat-Seng Chua"], "title": "LLM2Rec: Large Language Models Are Powerful Embedding Models for Sequential Recommendation", "categories": ["cs.IR", "cs.AI"], "comment": "KDD 2025", "summary": "Sequential recommendation aims to predict users' future interactions by\nmodeling collaborative filtering (CF) signals from historical behaviors of\nsimilar users or items. Traditional sequential recommenders predominantly rely\non ID-based embeddings, which capture CF signals through high-order\nco-occurrence patterns. However, these embeddings depend solely on past\ninteractions, lacking transferable knowledge to generalize to unseen domains.\nRecent advances in large language models (LLMs) have motivated text-based\nrecommendation approaches that derive item representations from textual\ndescriptions. While these methods enhance generalization, they fail to encode\nCF signals-i.e., latent item correlations and preference patterns-crucial for\neffective recommendation. We argue that an ideal embedding model should\nseamlessly integrate CF signals with rich semantic representations to improve\nboth in-domain and out-of-domain recommendation performance.\n  To this end, we propose LLM2Rec, a novel embedding model tailored for\nsequential recommendation, integrating the rich semantic understanding of LLMs\nwith CF awareness. Our approach follows a two-stage training framework: (1)\nCollaborative Supervised Fine-tuning, which adapts LLMs to infer item\nrelationships based on historical interactions, and (2) Item-level Embedding\nModeling, which refines these specialized LLMs into structured item embedding\nmodels that encode both semantic and collaborative information. Extensive\nexperiments on real-world datasets demonstrate that LLM2Rec effectively\nimproves recommendation quality across both in-domain and out-of-domain\nsettings. Our findings highlight the potential of leveraging LLMs to build more\nrobust, generalizable embedding models for sequential recommendation. Our codes\nare available at https://github.com/HappyPointer/LLM2Rec.", "AI": {"tldr": "LLM2Rec是一种新颖的嵌入模型，结合大型语言模型（LLMs）的语义理解和协同过滤（CF）信号，提升序列推荐的性能。", "motivation": "传统序列推荐依赖ID嵌入，缺乏跨领域泛化能力；基于文本的方法虽能泛化但无法捕捉CF信号。理想模型需结合两者。", "method": "采用两阶段训练框架：1）协同监督微调，使LLMs推断物品关系；2）物品级嵌入建模，生成结合语义和CF信息的嵌入。", "result": "实验表明，LLM2Rec在领域内和跨领域推荐中均显著提升性能。", "conclusion": "LLM2Rec展示了利用LLMs构建更鲁棒、泛化性强的序列推荐模型的潜力。"}}
{"id": "2506.21594", "pdf": "https://arxiv.org/pdf/2506.21594", "abs": "https://arxiv.org/abs/2506.21594", "authors": ["Ahmed M. Adly", "Mostafa Samy", "Amr Fawzy"], "title": "Gazal-R1: Achieving State-of-the-Art Medical Reasoning with Parameter-Efficient Two-Stage Training", "categories": ["cs.CL"], "comment": null, "summary": "We present Gazal-R1, a 32-billion-parameter language model that achieves\nstate-of-the-art performance in medical reasoning while providing transparent,\nstep-by-step explanations for clinical decision-making. Built upon Qwen3 32B,\nour model demonstrates that strategic training can enable mid-sized models to\noutperform significantly larger counterparts in specialized domains. We\ndeveloped a novel two-stage training pipeline: first, supervised fine-tuning on\na carefully curated dataset of 107,033 synthetic medical reasoning examples\nthat teaches structured clinical thinking, enhanced by advanced\nparameter-efficient techniques including Weight-Decomposed Low-Rank Adaptation\n(DoRA) and Rank-Stabilized LoRA (rsLoRA); second, reinforcement learning using\nGroup Relative Policy Optimization (GRPO) with a sophisticated multi-component\nreward system that refines accuracy, format adherence, and reasoning quality.\nGazal-R1 achieves exceptional performance across medical benchmarks, scoring\n87.1% on MedQA, 81.6% on MMLU Pro (Medical), and 79.6% on PubMedQA, surpassing\nmodels up to 12x larger. Beyond its strong empirical results, this work\nprovides detailed insights into the challenges of training reasoning-capable\nmodels in specialized domains, including issues with reward hacking, training\ninstability, and the fundamental tension between factual recall and detailed\nreasoning. Our methodology offers a reproducible framework for developing\nhigh-capability, domain-specific language models that balance performance,\nefficiency, and explainability.", "AI": {"tldr": "Gazal-R1是一个320亿参数的语言模型，在医学推理任务中表现优异，并提供透明的逐步解释。通过两阶段训练和高效参数技术，它在多个医学基准测试中超越了更大的模型。", "motivation": "解决医学领域中对高性能、可解释性语言模型的需求，同时探索如何在专业领域中训练中等规模模型以超越更大模型。", "method": "采用两阶段训练：1）监督微调，使用合成医学推理数据集和高效参数技术；2）强化学习，使用GRPO和多组件奖励系统。", "result": "在MedQA、MMLU Pro（医学）和PubMedQA上分别达到87.1%、81.6%和79.6%的分数，超越更大模型。", "conclusion": "Gazal-R1为开发高性能、可解释的专业领域语言模型提供了可复现的框架，并揭示了训练中的挑战和权衡。"}}
{"id": "2506.21924", "pdf": "https://arxiv.org/pdf/2506.21924", "abs": "https://arxiv.org/abs/2506.21924", "authors": ["Zhao Jin", "Rong-Cheng Tu", "Jingyi Liao", "Wenhao Sun", "Xiao Luo", "Shunyu Liu", "Dacheng Tao"], "title": "SPAZER: Spatial-Semantic Progressive Reasoning Agent for Zero-shot 3D Visual Grounding", "categories": ["cs.CV"], "comment": null, "summary": "3D Visual Grounding (3DVG) aims to localize target objects within a 3D scene\nbased on natural language queries. To alleviate the reliance on costly 3D\ntraining data, recent studies have explored zero-shot 3DVG by leveraging the\nextensive knowledge and powerful reasoning capabilities of pre-trained LLMs and\nVLMs. However, existing paradigms tend to emphasize either spatial (3D-based)\nor semantic (2D-based) understanding, limiting their effectiveness in complex\nreal-world applications. In this work, we introduce SPAZER - a VLM-driven agent\nthat combines both modalities in a progressive reasoning framework. It first\nholistically analyzes the scene and produces a 3D rendering from the optimal\nviewpoint. Based on this, anchor-guided candidate screening is conducted to\nperform a coarse-level localization of potential objects. Furthermore,\nleveraging retrieved relevant 2D camera images, 3D-2D joint decision-making is\nefficiently performed to determine the best-matching object. By bridging\nspatial and semantic reasoning neural streams, SPAZER achieves robust zero-shot\ngrounding without training on 3D-labeled data. Extensive experiments on\nScanRefer and Nr3D benchmarks demonstrate that SPAZER significantly outperforms\nprevious state-of-the-art zero-shot methods, achieving notable gains of 9.0%\nand 10.9% in accuracy.", "AI": {"tldr": "SPAZER是一种结合3D和2D模态的零样本3D视觉定位方法，通过渐进式推理框架显著提升了性能。", "motivation": "解决现有方法在空间（3D）和语义（2D）理解上的局限性，实现更复杂的真实场景应用。", "method": "SPAZER采用渐进式推理框架，结合3D渲染和2D图像进行联合决策。", "result": "在ScanRefer和Nr3D基准测试中，SPAZER显著优于现有零样本方法，准确率分别提升9.0%和10.9%。", "conclusion": "SPAZER通过结合空间和语义推理，实现了无需3D标注数据的鲁棒零样本定位。"}}
{"id": "2506.21595", "pdf": "https://arxiv.org/pdf/2506.21595", "abs": "https://arxiv.org/abs/2506.21595", "authors": ["Jinpyo Kim", "Gyeongje Cho", "Chanwoo Park", "Jongwon Park", "Jongmin Kim", "Yeonkyoun So", "Jaejin Lee"], "title": "Thunder-LLM: Efficiently Adapting LLMs to Korean with Minimal Resources", "categories": ["cs.CL"], "comment": "Submitted to ARR 2025 May cycle", "summary": "Since state-of-the-art LLMs often underperform in languages other than\nEnglish or Chinese, improving the capability of LLMs in new languages has\nbecome an essential task. Moreover, LLMs' entire end-to-end training process\nremains largely unknown to the public due to proprietary reasons, technical\ncomplexity, inconsistent documentation, and ethical considerations. The\ncomplete picture remains a closely guarded secret within the industry. This\npaper presents methods to adapt an existing English-based LLM to Korean in a\nlow-budget scenario. We describe the entire end-to-end process: collecting\nKorean datasets, preprocessing the data, training the model, creating\ndownstream benchmarks, and conducting evaluations. The evaluation results\nindicate that our method can effectively and cost-efficiently add new language\ncapabilities to existing LLMs. Our new bilingual models, Thunder-LLM and\nThunder-LLM-Ins, achieve superior Korean performance compared to\nstate-of-the-art models while utilizing minimal data and computational\nresources. We share our comprehensive experience and make the code publicly\navailable.", "AI": {"tldr": "本文提出了一种低成本方法，将现有的英语大语言模型（LLM）适配到韩语，并分享了完整的端到端流程。", "motivation": "由于现有LLM在非英语或中文语言中表现不佳，且其训练过程不透明，本文旨在解决这些问题。", "method": "收集韩语数据集、数据预处理、模型训练、创建下游基准并进行评估。", "result": "提出的方法能高效且低成本地为现有LLM添加新语言能力，新模型Thunder-LLM和Thunder-LLM-Ins在韩语表现上优于现有模型。", "conclusion": "本文展示了低成本适配新语言的可能性，并公开了代码和经验。"}}
{"id": "2506.21925", "pdf": "https://arxiv.org/pdf/2506.21925", "abs": "https://arxiv.org/abs/2506.21925", "authors": ["Liu Yang", "Huiyu Duan", "Jiarui Wang", "Jing Liu", "Menghan Hu", "Xiongkuo Min", "Guangtao Zhai", "Patrick Le Callet"], "title": "Quality Assessment and Distortion-aware Saliency Prediction for AI-Generated Omnidirectional Images", "categories": ["cs.CV"], "comment": null, "summary": "With the rapid advancement of Artificial Intelligence Generated Content\n(AIGC) techniques, AI generated images (AIGIs) have attracted widespread\nattention, among which AI generated omnidirectional images (AIGODIs) hold\nsignificant potential for Virtual Reality (VR) and Augmented Reality (AR)\napplications. AI generated omnidirectional images exhibit unique quality\nissues, however, research on the quality assessment and optimization of\nAI-generated omnidirectional images is still lacking. To this end, this work\nfirst studies the quality assessment and distortion-aware saliency prediction\nproblems for AIGODIs, and further presents a corresponding optimization\nprocess. Specifically, we first establish a comprehensive database to reflect\nhuman feedback for AI-generated omnidirectionals, termed OHF2024, which\nincludes both subjective quality ratings evaluated from three perspectives and\ndistortion-aware salient regions. Based on the constructed OHF2024 database, we\npropose two models with shared encoders based on the BLIP-2 model to evaluate\nthe human visual experience and predict distortion-aware saliency for\nAI-generated omnidirectional images, which are named as BLIP2OIQA and\nBLIP2OISal, respectively. Finally, based on the proposed models, we present an\nautomatic optimization process that utilizes the predicted visual experience\nscores and distortion regions to further enhance the visual quality of an\nAI-generated omnidirectional image. Extensive experiments show that our\nBLIP2OIQA model and BLIP2OISal model achieve state-of-the-art (SOTA) results in\nthe human visual experience evaluation task and the distortion-aware saliency\nprediction task for AI generated omnidirectional images, and can be effectively\nused in the optimization process. The database and codes will be released on\nhttps://github.com/IntMeGroup/AIGCOIQA to facilitate future research.", "AI": {"tldr": "本文研究了AI生成的全景图像（AIGODIs）的质量评估和优化问题，提出了基于BLIP-2模型的BLIP2OIQA和BLIP2OISal模型，并建立了OHF2024数据库。实验表明模型在视觉体验评估和显著性预测任务中表现优异。", "motivation": "随着AIGC技术的发展，AI生成的全景图像在VR/AR应用中潜力巨大，但其质量评估和优化研究尚不充分。", "method": "建立了OHF2024数据库，包含主观质量评分和显著性区域；提出了BLIP2OIQA和BLIP2OISal模型，基于BLIP-2共享编码器；设计了自动优化流程。", "result": "BLIP2OIQA和BLIP2OISal在视觉体验评估和显著性预测任务中达到SOTA效果，优化流程有效提升图像质量。", "conclusion": "本文为AIGODIs的质量评估和优化提供了有效工具，数据库和代码将开源以促进未来研究。"}}
{"id": "2506.21581", "pdf": "https://arxiv.org/pdf/2506.21581", "abs": "https://arxiv.org/abs/2506.21581", "authors": ["Sarthak Chaturvedi", "Anurag Acharya", "Rounak Meyur", "Koby Hayashi", "Sai Munikoti", "Sameera Horawalavithana"], "title": "Evaluating the Robustness of Dense Retrievers in Interdisciplinary Domains", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": null, "summary": "Evaluation benchmark characteristics may distort the true benefits of domain\nadaptation in retrieval models. This creates misleading assessments that\ninfluence deployment decisions in specialized domains. We show that two\nbenchmarks with drastically different features such as topic diversity,\nboundary overlap, and semantic complexity can influence the perceived benefits\nof fine-tuning. Using environmental regulatory document retrieval as a case\nstudy, we fine-tune ColBERTv2 model on Environmental Impact Statements (EIS)\nfrom federal agencies. We evaluate these models across two benchmarks with\ndifferent semantic structures. Our findings reveal that identical domain\nadaptation approaches show very different perceived benefits depending on\nevaluation methodology. On one benchmark, with clearly separated topic\nboundaries, domain adaptation shows small improvements (maximum 0.61% NDCG\ngain). However, on the other benchmark with overlapping semantic structures,\nthe same models demonstrate large improvements (up to 2.22% NDCG gain), a\n3.6-fold difference in the performance benefit. We compare these benchmarks\nthrough topic diversity metrics, finding that the higher-performing benchmark\nshows 11% higher average cosine distances between contexts and 23% lower\nsilhouette scores, directly contributing to the observed performance\ndifference. These results demonstrate that benchmark selection strongly\ndetermines assessments of retrieval system effectiveness in specialized\ndomains. Evaluation frameworks with well-separated topics regularly\nunderestimate domain adaptation benefits, while those with overlapping semantic\nboundaries reveal improvements that better reflect real-world regulatory\ndocument complexity. Our findings have important implications for developing\nand deploying AI systems for interdisciplinary domains that integrate multiple\ntopics.", "AI": {"tldr": "论文指出评估基准的特性可能扭曲检索模型中领域适应的真实效果，导致误导性评估。通过环境监管文件检索的案例研究，发现不同语义结构的基准会显著影响领域适应效果的感知。", "motivation": "揭示评估基准的选择如何影响领域适应效果的评估，特别是在专业领域中，以避免误导部署决策。", "method": "使用ColBERTv2模型在环境监管文件（EIS）上进行微调，并在两个语义结构不同的基准上评估效果。", "result": "在主题边界清晰的基准上，领域适应效果提升较小（NDCG增益0.61%）；而在语义重叠的基准上，效果提升显著（NDCG增益2.22%）。", "conclusion": "评估基准的语义结构会显著影响领域适应效果的评估，选择更贴近真实复杂性的基准能更准确地反映改进。"}}
{"id": "2506.21596", "pdf": "https://arxiv.org/pdf/2506.21596", "abs": "https://arxiv.org/abs/2506.21596", "authors": ["Hessa A. Alawwad", "Anas Zafar", "Areej Alhothali", "Usman Naseem", "Ali Alkhathlan", "Amani Jamal"], "title": "Evaluating Multimodal Large Language Models on Educational Textbook Question Answering", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": "7 Pages", "summary": "Multimodal large language models (MLLMs) have recently achieved significant\nsuccess in vision--language tasks. However, their capacity to reason over\ncomplex, long lessons and intricate educational diagrams that cannot be\nrepresented as a single natural image remains largely untested. In this work,\nwe present the first evaluation of state-of-the-art MLLMs on the textbook\nquestion answering (TQA) task using the CK12-QA dataset. We assess the\nperformance of recent vision-language models, including LLaVA and LLaMA\n3.2-Vision, across various input configurations. Additionally, we introduce a\nlightweight multimodal retrieval-augmented generation (RAG) pipeline that\nintegrates both paragraphs and diagrams from the lesson into the prompt. Our\nresults demonstrate the influence of retrieved educational context on model\naccuracy and reasoning, while also revealing current limitations in handling\nquestion-context relationships and the potential for noise, pointing to key\ndirections for future research in multimodal AI-driven learning.", "AI": {"tldr": "评估多模态大语言模型（MLLMs）在教科书问答任务（TQA）中的表现，并引入轻量级多模态检索增强生成（RAG）方法。", "motivation": "测试MLLMs在复杂教育内容（如长课程和复杂图表）中的推理能力，填补现有研究的空白。", "method": "使用CK12-QA数据集评估LLaVA和LLaMA 3.2-Vision等模型，并设计多模态RAG流程整合段落和图表。", "result": "检索的教育上下文对模型准确性和推理有显著影响，但模型在处理问题-上下文关系和噪声方面仍有局限。", "conclusion": "研究揭示了多模态AI驱动学习的关键方向，需进一步优化模型对复杂教育内容的理解能力。"}}
{"id": "2506.21945", "pdf": "https://arxiv.org/pdf/2506.21945", "abs": "https://arxiv.org/abs/2506.21945", "authors": ["Naftaly Wambugu", "Ruisheng Wang", "Bo Guo", "Tianshu Yu", "Sheng Xu", "Mohammed Elhassan"], "title": "SDRNET: Stacked Deep Residual Network for Accurate Semantic Segmentation of Fine-Resolution Remotely Sensed Images", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Land cover maps generated from semantic segmentation of high-resolution\nremotely sensed images have drawn mucon in the photogrammetry and remote\nsensing research community. Currently, massive fine-resolution remotely sensed\n(FRRS) images acquired by improving sensing and imaging technologies become\navailable. However, accurate semantic segmentation of such FRRS images is\ngreatly affected by substantial class disparities, the invisibility of key\nground objects due to occlusion, and object size variation. Despite the\nextraordinary potential in deep convolutional neural networks (DCNNs) in image\nfeature learning and representation, extracting sufficient features from FRRS\nimages for accurate semantic segmentation is still challenging. These\nchallenges demand the deep learning models to learn robust features and\ngenerate sufficient feature descriptors. Specifically, learning\nmulti-contextual features to guarantee adequate coverage of varied object sizes\nfrom the ground scene and harnessing global-local contexts to overcome class\ndisparities challenge even profound networks. Deeper networks significantly\nlose spatial details due to gradual downsampling processes resulting in poor\nsegmentation results and coarse boundaries. This article presents a stacked\ndeep residual network (SDRNet) for semantic segmentation from FRRS images. The\nproposed framework utilizes two stacked encoder-decoder networks to harness\nlong-range semantics yet preserve spatial information and dilated residual\nblocks (DRB) between each encoder and decoder network to capture sufficient\nglobal dependencies thus improving segmentation performance. Our experimental\nresults obtained using the ISPRS Vaihingen and Potsdam datasets demonstrate\nthat the SDRNet performs effectively and competitively against current DCNNs in\nsemantic segmentation.", "AI": {"tldr": "论文提出了一种堆叠深度残差网络（SDRNet），用于高分辨率遥感图像的语义分割，解决了类间差异、遮挡和对象尺寸变化等问题。", "motivation": "高分辨率遥感图像的语义分割面临类间差异、遮挡和对象尺寸变化的挑战，现有深度卷积神经网络难以提取足够特征。", "method": "采用两个堆叠的编码器-解码器网络和扩张残差块（DRB），以捕获长距离语义并保留空间信息。", "result": "在ISPRS Vaihingen和Potsdam数据集上，SDRNet表现优于现有深度卷积神经网络。", "conclusion": "SDRNet通过多上下文特征学习和全局-局部上下文结合，显著提升了高分辨率遥感图像的语义分割性能。"}}
{"id": "2506.21597", "pdf": "https://arxiv.org/pdf/2506.21597", "abs": "https://arxiv.org/abs/2506.21597", "authors": ["Brandon Colelough", "Davis Bartels", "Dina Demner-Fushman"], "title": "Overview of the ClinIQLink 2025 Shared Task on Medical Question-Answering", "categories": ["cs.CL", "cs.AI", "cs.IR", "I.2.7"], "comment": "10 pages, 5 figures", "summary": "In this paper, we present an overview of ClinIQLink, a shared task,\ncollocated with the 24th BioNLP workshop at ACL 2025, designed to stress-test\nlarge language models (LLMs) on medically-oriented question answering aimed at\nthe level of a General Practitioner. The challenge supplies 4,978\nexpert-verified, medical source-grounded question-answer pairs that cover seven\nformats: true/false, multiple choice, unordered list, short answer,\nshort-inverse, multi-hop, and multi-hop-inverse. Participating systems, bundled\nin Docker or Apptainer images, are executed on the CodaBench platform or the\nUniversity of Maryland's Zaratan cluster. An automated harness (Task 1) scores\nclosed-ended items by exact match and open-ended items with a three-tier\nembedding metric. A subsequent physician panel (Task 2) audits the top model\nresponses.", "AI": {"tldr": "ClinIQLink是一个共享任务，旨在测试大型语言模型在医学问答中的表现，提供4978个专家验证的问题-答案对，涵盖七种格式。", "motivation": "评估大型语言模型在医学领域的问答能力，特别是针对全科医生水平的问题。", "method": "任务分为两部分：自动化评分（Task 1）和医生小组审核（Task 2）。", "result": "通过自动化评分和人工审核相结合的方式评估模型表现。", "conclusion": "ClinIQLink为医学领域的问答模型提供了一个全面的测试平台。"}}
{"id": "2506.21957", "pdf": "https://arxiv.org/pdf/2506.21957", "abs": "https://arxiv.org/abs/2506.21957", "authors": ["Yixin Zha", "Chuxin Wang", "Wenfei Yang", "Tianzhu Zhang"], "title": "Exploring Semantic Masked Autoencoder for Self-supervised Point Cloud Understanding", "categories": ["cs.CV"], "comment": "Accepted by IJCAI 2025", "summary": "Point cloud understanding aims to acquire robust and general feature\nrepresentations from unlabeled data. Masked point modeling-based methods have\nrecently shown significant performance across various downstream tasks. These\npre-training methods rely on random masking strategies to establish the\nperception of point clouds by restoring corrupted point cloud inputs, which\nleads to the failure of capturing reasonable semantic relationships by the\nself-supervised models. To address this issue, we propose Semantic Masked\nAutoencoder, which comprises two main components: a prototype-based component\nsemantic modeling module and a component semantic-enhanced masking strategy.\nSpecifically, in the component semantic modeling module, we design a component\nsemantic guidance mechanism to direct a set of learnable prototypes in\ncapturing the semantics of different components from objects. Leveraging these\nprototypes, we develop a component semantic-enhanced masking strategy that\naddresses the limitations of random masking in effectively covering complete\ncomponent structures. Furthermore, we introduce a component semantic-enhanced\nprompt-tuning strategy, which further leverages these prototypes to improve the\nperformance of pre-trained models in downstream tasks. Extensive experiments\nconducted on datasets such as ScanObjectNN, ModelNet40, and ShapeNetPart\ndemonstrate the effectiveness of our proposed modules.", "AI": {"tldr": "论文提出了一种基于语义掩码自动编码器的方法，通过原型语义建模和增强掩码策略，解决了随机掩码在点云理解中的语义关系捕捉不足问题。", "motivation": "现有基于随机掩码的点云预训练方法难以捕捉合理的语义关系，影响了自监督模型的性能。", "method": "设计了原型语义建模模块和语义增强掩码策略，结合提示调优策略提升下游任务性能。", "result": "在ScanObjectNN、ModelNet40和ShapeNetPart等数据集上验证了方法的有效性。", "conclusion": "提出的方法显著提升了点云理解的语义建模能力，并在多个下游任务中表现优异。"}}
{"id": "2506.21600", "pdf": "https://arxiv.org/pdf/2506.21600", "abs": "https://arxiv.org/abs/2506.21600", "authors": ["Chang Liu", "Hongkai Chen", "Yujun Cai", "Hang Wu", "Qingwen Ye", "Ming-Hsuan Yang", "Yiwei Wang"], "title": "Structured Attention Matters to Multimodal LLMs in Document Understanding", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Document understanding remains a significant challenge for multimodal large\nlanguage models (MLLMs). While previous research has primarily focused on\nlocating evidence pages through precise multimodal queries, our work\ninvestigates a fundamental yet overlooked aspect: how input format influences\ndocument comprehension performance. Through systematic analysis, we discover\nthat raw OCR text often impairs rather than improves MLLMs' performance, which\nis a counterintuitive finding we attribute to attention dispersion and\nstructure loss. To further substantiate our hypothesis, we propose a novel\nstructure-preserving approach that encodes document elements using the LaTex\nparadigm, maintaining the hierarchical organization and spatial relationships\ncritical for comprehension. Our attention analysis reveals that structured text\ninduces structured attention patterns on both textual and visual content,\ndirecting models to focus on semantically meaningful regions while reducing\nattention waste. This approach significantly enhances MLLMs' document question\nanswering performance across diverse document types without requiring\narchitectural modifications or additional training.", "AI": {"tldr": "研究发现输入格式对多模态大语言模型（MLLMs）的文档理解性能有显著影响，提出了一种基于LaTeX的结构保留方法，显著提升了性能。", "motivation": "探讨输入格式如何影响MLLMs的文档理解性能，解决现有方法中因OCR文本导致注意力分散和结构丢失的问题。", "method": "提出一种基于LaTeX的结构保留方法，编码文档元素以保持层次组织和空间关系。", "result": "结构化文本引导模型关注语义重要区域，减少注意力浪费，显著提升文档问答性能。", "conclusion": "结构保留方法无需修改模型架构或额外训练，即可有效提升MLLMs的文档理解能力。"}}
{"id": "2506.21975", "pdf": "https://arxiv.org/pdf/2506.21975", "abs": "https://arxiv.org/abs/2506.21975", "authors": ["Meng Yu", "Te Cui", "Qitong Chu", "Wenjie Song", "Yi Yang", "Yufeng Yue"], "title": "TASeg: Text-aware RGB-T Semantic Segmentation based on Fine-tuning Vision Foundation Models", "categories": ["cs.CV"], "comment": "6 pages, accepted for publication in lEEE/RSJ international\n  Conference on Intelligent Robots and Systems (lROS 2025)", "summary": "Reliable semantic segmentation of open environments is essential for\nintelligent systems, yet significant problems remain: 1) Existing RGB-T\nsemantic segmentation models mainly rely on low-level visual features and lack\nhigh-level textual information, which struggle with accurate segmentation when\ncategories share similar visual characteristics. 2) While SAM excels in\ninstance-level segmentation, integrating it with thermal images and text is\nhindered by modality heterogeneity and computational inefficiency. To address\nthese, we propose TASeg, a text-aware RGB-T segmentation framework by using\nLow-Rank Adaptation (LoRA) fine-tuning technology to adapt vision foundation\nmodels. Specifically, we propose a Dynamic Feature Fusion Module (DFFM) in the\nimage encoder, which effectively merges features from multiple visual\nmodalities while freezing SAM's original transformer blocks. Additionally, we\nincorporate CLIP-generated text embeddings in the mask decoder to enable\nsemantic alignment, which further rectifies the classification error and\nimproves the semantic understanding accuracy. Experimental results across\ndiverse datasets demonstrate that our method achieves superior performance in\nchallenging scenarios with fewer trainable parameters.", "AI": {"tldr": "提出TASeg框架，通过LoRA微调技术和动态特征融合模块（DFFM）提升RGB-T语义分割性能，结合CLIP文本嵌入改善语义对齐。", "motivation": "现有RGB-T分割模型依赖低层视觉特征，缺乏高层文本信息，难以区分视觉相似的类别；SAM在实例分割中表现优异，但与热图像和文本的融合存在模态异构和计算效率问题。", "method": "采用LoRA微调技术适配视觉基础模型，提出DFFM模块融合多模态视觉特征，并引入CLIP文本嵌入实现语义对齐。", "result": "在多样化数据集上验证，TASeg在复杂场景中表现优异，且训练参数更少。", "conclusion": "TASeg通过结合视觉和文本信息，显著提升了RGB-T语义分割的准确性和效率。"}}
{"id": "2506.21602", "pdf": "https://arxiv.org/pdf/2506.21602", "abs": "https://arxiv.org/abs/2506.21602", "authors": ["Xiaoyan Feng", "He Zhang", "Yanjun Zhang", "Leo Yu Zhang", "Shirui Pan"], "title": "BiMark: Unbiased Multilayer Watermarking for Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "This paper is accepted by International Conference on Machine\n  Learning (ICML) 2025", "summary": "Recent advances in Large Language Models (LLMs) have raised urgent concerns\nabout LLM-generated text authenticity, prompting regulatory demands for\nreliable identification mechanisms. Although watermarking offers a promising\nsolution, existing approaches struggle to simultaneously achieve three critical\nrequirements: text quality preservation, model-agnostic detection, and message\nembedding capacity, which are crucial for practical implementation. To achieve\nthese goals, the key challenge lies in balancing the trade-off between text\nquality preservation and message embedding capacity. To address this challenge,\nwe propose BiMark, a novel watermarking framework that achieves these\nrequirements through three key innovations: (1) a bit-flip unbiased reweighting\nmechanism enabling model-agnostic detection, (2) a multilayer architecture\nenhancing detectability without compromising generation quality, and (3) an\ninformation encoding approach supporting multi-bit watermarking. Through\ntheoretical analysis and extensive experiments, we validate that, compared to\nstate-of-the-art multi-bit watermarking methods, BiMark achieves up to 30%\nhigher extraction rates for short texts while maintaining text quality\nindicated by lower perplexity, and performs comparably to non-watermarked text\non downstream tasks such as summarization and translation.", "AI": {"tldr": "BiMark是一种新型水印框架，通过三项创新技术解决了大型语言模型生成文本的认证问题，同时满足文本质量、模型无关检测和多比特水印嵌入的需求。", "motivation": "大型语言模型生成的文本真实性引发监管需求，现有水印方法难以同时满足文本质量、模型无关检测和多比特水印嵌入的要求。", "method": "BiMark采用比特翻转无偏重加权机制、多层架构和信息编码方法，实现模型无关检测、高质量生成和多比特水印。", "result": "BiMark在短文本提取率上比现有方法提高30%，文本质量（困惑度更低）和下游任务表现与非水印文本相当。", "conclusion": "BiMark在文本质量、检测能力和水印容量之间取得平衡，为实际应用提供了可靠的水印解决方案。"}}
{"id": "2506.21980", "pdf": "https://arxiv.org/pdf/2506.21980", "abs": "https://arxiv.org/abs/2506.21980", "authors": ["Biao Wang", "Wenwen Li"], "title": "R1-Track: Direct Application of MLLMs to Visual Object Tracking via Reinforcement Learning", "categories": ["cs.CV"], "comment": "7 pages, 2 figures", "summary": "Visual single object tracking aims to continuously localize and estimate the\nscale of a target in subsequent video frames, given only its initial state in\nthe first frame. This task has traditionally been framed as a template matching\nproblem, evolving through major phases including correlation filters,\ntwo-stream networks, and one-stream networks with significant progress\nachieved. However, these methods typically require explicit classification and\nregression modeling, depend on supervised training with large-scale datasets,\nand are limited to the single task of tracking, lacking flexibility. In recent\nyears, multi-modal large language models (MLLMs) have advanced rapidly.\nOpen-source models like Qwen2.5-VL, a flagship MLLMs with strong foundational\ncapabilities, demonstrate excellent performance in grounding tasks. This has\nspurred interest in applying such models directly to visual tracking. However,\nexperiments reveal that Qwen2.5-VL struggles with template matching between\nimage pairs (i.e., tracking tasks). Inspired by deepseek-R1, we fine-tuned\nQwen2.5-VL using the group relative policy optimization (GRPO) reinforcement\nlearning method on a small-scale dataset with a rule-based reward function. The\nresulting model, R1-Track, achieved notable performance on the GOT-10k\nbenchmark. R1-Track supports flexible initialization via bounding boxes or text\ndescriptions while retaining most of the original model's general capabilities.\nAnd we further discuss potential improvements for R1-Track. This rough\ntechnical report summarizes our findings as of May 2025.", "AI": {"tldr": "论文提出了一种基于多模态大语言模型（MLLMs）的视觉单目标跟踪方法R1-Track，通过GRPO强化学习方法微调Qwen2.5-VL，在GOT-10k基准上表现优异。", "motivation": "传统跟踪方法依赖大规模监督数据和显式分类回归建模，缺乏灵活性。MLLMs的快速发展为直接应用于视觉跟踪提供了可能。", "method": "使用GRPO强化学习方法在小规模数据集上微调Qwen2.5-VL，生成模型R1-Track。", "result": "R1-Track在GOT-10k基准上表现优异，支持通过边界框或文本描述灵活初始化。", "conclusion": "R1-Track展示了MLLMs在视觉跟踪中的潜力，并讨论了进一步改进方向。"}}
{"id": "2506.21603", "pdf": "https://arxiv.org/pdf/2506.21603", "abs": "https://arxiv.org/abs/2506.21603", "authors": ["Yenisel Plasencia-Calaña"], "title": "Operationalizing Automated Essay Scoring: A Human-Aware Approach", "categories": ["cs.CL", "cs.CY", "cs.LG"], "comment": null, "summary": "This paper explores the human-centric operationalization of Automated Essay\nScoring (AES) systems, addressing aspects beyond accuracy. We compare various\nmachine learning-based approaches with Large Language Models (LLMs) approaches,\nidentifying their strengths, similarities and differences. The study\ninvestigates key dimensions such as bias, robustness, and explainability,\nconsidered important for human-aware operationalization of AES systems. Our\nstudy shows that ML-based AES models outperform LLMs in accuracy but struggle\nwith explainability, whereas LLMs provide richer explanations. We also found\nthat both approaches struggle with bias and robustness to edge scores. By\nanalyzing these dimensions, the paper aims to identify challenges and\ntrade-offs between different methods, contributing to more reliable and\ntrustworthy AES methods.", "AI": {"tldr": "论文探讨了自动化作文评分（AES）系统的人本操作化，比较了机器学习与大型语言模型（LLMs）的优缺点，重点关注偏差、鲁棒性和可解释性。", "motivation": "研究旨在超越准确性，探讨AES系统在实际应用中的人本维度，如偏差、鲁棒性和可解释性。", "method": "比较了机器学习方法和LLMs方法，分析了它们在准确性、可解释性、偏差和鲁棒性方面的表现。", "result": "机器学习方法在准确性上优于LLMs，但可解释性较差；LLMs提供更丰富的解释。两者在偏差和边缘分数鲁棒性上均存在问题。", "conclusion": "研究揭示了不同方法间的挑战与权衡，为开发更可靠、可信的AES方法提供了参考。"}}
{"id": "2506.22007", "pdf": "https://arxiv.org/pdf/2506.22007", "abs": "https://arxiv.org/abs/2506.22007", "authors": ["Liudi Yang", "Yang Bai", "George Eskandar", "Fengyi Shen", "Mohammad Altillawi", "Dong Chen", "Soumajit Majumder", "Ziyuan Liu", "Gitta Kutyniok", "Abhinav Valada"], "title": "RoboEnvision: A Long-Horizon Video Generation Model for Multi-Task Robot Manipulation", "categories": ["cs.CV"], "comment": "8 pages, 6 figures", "summary": "We address the problem of generating long-horizon videos for robotic\nmanipulation tasks. Text-to-video diffusion models have made significant\nprogress in photorealism, language understanding, and motion generation but\nstruggle with long-horizon robotic tasks. Recent works use video diffusion\nmodels for high-quality simulation data and predictive rollouts in robot\nplanning. However, these works predict short sequences of the robot achieving\none task and employ an autoregressive paradigm to extend to the long horizon,\nleading to error accumulations in the generated video and in the execution. To\novercome these limitations, we propose a novel pipeline that bypasses the need\nfor autoregressive generation. We achieve this through a threefold\ncontribution: 1) we first decompose the high-level goals into smaller atomic\ntasks and generate keyframes aligned with these instructions. A second\ndiffusion model then interpolates between each of the two generated frames,\nachieving the long-horizon video. 2) We propose a semantics preserving\nattention module to maintain consistency between the keyframes. 3) We design a\nlightweight policy model to regress the robot joint states from generated\nvideos. Our approach achieves state-of-the-art results on two benchmarks in\nvideo quality and consistency while outperforming previous policy models on\nlong-horizon tasks.", "AI": {"tldr": "提出了一种新的视频生成方法，用于机器人操作任务的长时程视频生成，避免了自回归生成的误差累积。", "motivation": "解决文本到视频扩散模型在长时程机器人任务中的局限性，如误差累积和视频一致性不足。", "method": "1) 将高级目标分解为原子任务并生成关键帧；2) 使用扩散模型插值关键帧；3) 引入语义保持注意力模块和轻量级策略模型。", "result": "在两个基准测试中取得了视频质量和一致性的最佳结果，并在长时程任务中优于之前的策略模型。", "conclusion": "新方法通过分解任务和插值关键帧，显著提升了长时程视频生成的质量和一致性。"}}
{"id": "2506.21605", "pdf": "https://arxiv.org/pdf/2506.21605", "abs": "https://arxiv.org/abs/2506.21605", "authors": ["Haoran Tan", "Zeyu Zhang", "Chen Ma", "Xu Chen", "Quanyu Dai", "Zhenhua Dong"], "title": "MemBench: Towards More Comprehensive Evaluation on the Memory of LLM-based Agents", "categories": ["cs.CL", "cs.AI"], "comment": "17 pages, 5 figures. Accepted by ACL 2025 findings", "summary": "Recent works have highlighted the significance of memory mechanisms in\nLLM-based agents, which enable them to store observed information and adapt to\ndynamic environments. However, evaluating their memory capabilities still\nremains challenges. Previous evaluations are commonly limited by the diversity\nof memory levels and interactive scenarios. They also lack comprehensive\nmetrics to reflect the memory capabilities from multiple aspects. To address\nthese problems, in this paper, we construct a more comprehensive dataset and\nbenchmark to evaluate the memory capability of LLM-based agents. Our dataset\nincorporates factual memory and reflective memory as different levels, and\nproposes participation and observation as various interactive scenarios. Based\non our dataset, we present a benchmark, named MemBench, to evaluate the memory\ncapability of LLM-based agents from multiple aspects, including their\neffectiveness, efficiency, and capacity. To benefit the research community, we\nrelease our dataset and project at https://github.com/import-myself/Membench.", "AI": {"tldr": "该论文提出了一个更全面的数据集和基准（MemBench），用于评估基于LLM的智能体的记忆能力，解决了现有评估方法在多样性和全面性上的不足。", "motivation": "现有评估方法在记忆层级和交互场景的多样性上存在局限，且缺乏全面的指标来多角度反映记忆能力。", "method": "构建包含事实记忆和反思记忆的数据集，并提出参与和观察两种交互场景，基于此设计MemBench基准。", "result": "MemBench能够从有效性、效率和容量等多方面评估LLM智能体的记忆能力。", "conclusion": "论文通过数据集和基准的发布，为研究社区提供了更全面的评估工具。"}}
{"id": "2506.22015", "pdf": "https://arxiv.org/pdf/2506.22015", "abs": "https://arxiv.org/abs/2506.22015", "authors": ["Sarthak Ketanbhai Modi", "Lim Zi Pong", "Shourya Kuchhal", "Yoshi Cao", "Yupeng Cheng", "Teo Yon Shin", "Lin Shang-Wei", "Zhiming Li"], "title": "Towards Universal & Efficient Model Compression via Exponential Torque Pruning", "categories": ["cs.CV"], "comment": null, "summary": "The rapid growth in complexity and size of modern deep neural networks (DNNs)\nhas increased challenges related to computational costs and memory usage,\nspurring a growing interest in efficient model compression techniques. Previous\nstate-of-the-art approach proposes using a Torque-inspired regularization which\nforces the weights of neural modules around a selected pivot point. Whereas, we\nobserve that the pruning effect of this approach is far from perfect, as the\npost-trained network is still dense and also suffers from high accuracy drop.\nIn this work, we attribute such ineffectiveness to the default linear force\napplication scheme, which imposes inappropriate force on neural module of\ndifferent distances. To efficiently prune the redundant and distant modules\nwhile retaining those that are close and necessary for effective inference, in\nthis work, we propose Exponential Torque Pruning (ETP), which adopts an\nexponential force application scheme for regularization. Experimental results\non a broad range of domains demonstrate that, though being extremely simple,\nETP manages to achieve significantly higher compression rate than the previous\nstate-of-the-art pruning strategies with negligible accuracy drop.", "AI": {"tldr": "论文提出了一种名为指数扭矩剪枝（ETP）的新方法，通过指数力应用方案改进现有扭矩正则化剪枝技术，显著提高了压缩率且几乎不影响准确性。", "motivation": "现代深度神经网络（DNN）的复杂性和规模快速增长，导致计算成本和内存使用问题突出，亟需高效的模型压缩技术。现有扭矩正则化方法剪枝效果不理想，网络仍较密集且准确性下降明显。", "method": "提出指数扭矩剪枝（ETP），采用指数力应用方案进行正则化，有效剪除冗余和远距离模块，保留近距离且必要的模块。", "result": "实验结果表明，ETP在多个领域实现了比现有剪枝策略更高的压缩率，且准确性下降可忽略不计。", "conclusion": "ETP是一种简单高效的剪枝方法，显著提升了模型压缩效果。"}}
{"id": "2506.21606", "pdf": "https://arxiv.org/pdf/2506.21606", "abs": "https://arxiv.org/abs/2506.21606", "authors": ["Parham Pourdavood", "Michael Jacob", "Terrence Deacon"], "title": "Large Language Models as symbolic DNA of cultural dynamics", "categories": ["cs.CL", "cs.AI", "cs.CY"], "comment": "28 pages, 1 figure", "summary": "This paper proposes a novel conceptualization of Large Language Models (LLMs)\nas externalized informational substrates that function analogously to DNA for\nhuman cultural dynamics. Rather than viewing LLMs as either autonomous\nintelligence or mere programmed mimicry, we argue they serve a broader role as\nrepositories that preserve compressed patterns of human symbolic\nexpression--\"fossils\" of meaningful dynamics that retain relational residues\nwithout their original living contexts. Crucially, these compressed patterns\nonly become meaningful through human reinterpretation, creating a recursive\nfeedback loop where they can be recombined and cycle back to ultimately\ncatalyze human creative processes. Through analysis of four universal\nfeatures--compression, decompression, externalization, and recursion--we\ndemonstrate that just as DNA emerged as a compressed and externalized medium\nfor preserving useful cellular dynamics without containing explicit reference\nto goal-directed physical processes, LLMs preserve useful regularities of human\nculture without containing understanding of embodied human experience.\nTherefore, we argue that LLMs' significance lies not in rivaling human\nintelligence, but in providing humanity a tool for self-reflection and playful\nhypothesis-generation in a low-stakes, simulated environment. This framework\npositions LLMs as tools for cultural evolvability, enabling humanity to\ngenerate novel hypotheses about itself while maintaining the human\ninterpretation necessary to ground these hypotheses in ongoing human aesthetics\nand norms.", "AI": {"tldr": "论文提出将大语言模型（LLMs）视为类似DNA的文化信息载体，强调其作为压缩人类符号表达模式的作用，而非自主智能或简单模仿。", "motivation": "探讨LLMs在文化动态中的角色，超越传统将其视为智能或模仿工具的视角，强调其作为文化信息存储和反馈工具的价值。", "method": "通过分析四个普遍特征（压缩、解压、外部化和递归），类比DNA的功能，论证LLMs如何保存文化规律。", "result": "LLMs的意义在于为人类提供自我反思和假设生成的工具，而非与人类智能竞争。", "conclusion": "LLMs是文化演化的工具，促进人类在低风险环境中进行创造性探索，同时依赖人类解释以保持文化相关性。"}}
{"id": "2506.22022", "pdf": "https://arxiv.org/pdf/2506.22022", "abs": "https://arxiv.org/abs/2506.22022", "authors": ["Zhanyi Lu", "Yue Zhou"], "title": "Advancing Facial Stylization through Semantic Preservation Constraint and Pseudo-Paired Supervision", "categories": ["cs.CV"], "comment": null, "summary": "Facial stylization aims to transform facial images into appealing,\nhigh-quality stylized portraits, with the critical challenge of accurately\nlearning the target style while maintaining content consistency with the\noriginal image. Although previous StyleGAN-based methods have made significant\nadvancements, the generated results still suffer from artifacts or insufficient\nfidelity to the source image. We argue that these issues stem from neglecting\nsemantic shift of the generator during stylization. Therefore, we propose a\nfacial stylization method that integrates semantic preservation constraint and\npseudo-paired supervision to enhance the content correspondence and improve the\nstylization effect. Additionally, we develop a methodology for creating\nmulti-level pseudo-paired datasets to implement supervisory constraint.\nFurthermore, building upon our facial stylization framework, we achieve more\nflexible multimodal and reference-guided stylization without complex network\narchitecture designs or additional training. Experimental results demonstrate\nthat our approach produces high-fidelity, aesthetically pleasing facial style\ntransfer that surpasses previous methods.", "AI": {"tldr": "提出了一种结合语义保留约束和伪配对监督的面部风格化方法，解决了生成结果中的伪影和内容不一致问题。", "motivation": "现有方法在面部风格化中常因忽略生成器的语义偏移而导致结果不理想。", "method": "集成语义保留约束和伪配对监督，并开发多级伪配对数据集。", "result": "实验表明，该方法生成高保真且美观的风格化效果，优于现有方法。", "conclusion": "该方法不仅提升了风格化效果，还实现了灵活的多模态和参考引导风格化。"}}
{"id": "2506.21599", "pdf": "https://arxiv.org/pdf/2506.21599", "abs": "https://arxiv.org/abs/2506.21599", "authors": ["Peibo Li", "Shuang Ao", "Hao Xue", "Yang Song", "Maarten de Rijke", "Johan Barthélemy", "Tomasz Bednarz", "Flora D. Salim"], "title": "Reinforcement Fine-Tuned Large Language Models for Next POI Recommendation", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": null, "summary": "Large language models (LLMs) have been adopted for next point-of-interest\n(POI) recommendation tasks. Typical LLM-based recommenders fall into two\ncategories: prompt-based and supervised fine-tuning (SFT)-based models.\nPrompt-based models generally offer greater output flexibility but deliver\nlower accuracy, whereas SFT-based models achieve higher performance yet face a\nfundamental mismatch: next POI recommendation data does not naturally suit\nsupervised fine-tuning. In SFT, the model is trained to reproduce the exact\nground truth, but each training example provides only a single target POI, so\nthere is no ground truth for producing a top-k list.\n  To address this, we propose Refine-POI, a reinforcement fine-tuning framework\nfor next POI recommendation. We introduce recommendation-driven rewards that\nenable LLMs to learn to generate top-k recommendation lists using only one\nground-truth POI per example. Experiments on real-world datasets demonstrate\nthat Refine-POI achieves state-of-the-art top-k recommendation performance.", "AI": {"tldr": "Refine-POI提出了一种基于强化学习的微调框架，用于解决LLM在POI推荐任务中监督微调（SFT）与数据不匹配的问题，显著提升了推荐性能。", "motivation": "现有的LLM-based推荐器在POI推荐任务中存在监督微调（SFT）与数据不匹配的问题，即训练数据仅提供单一目标POI，无法生成top-k推荐列表。", "method": "提出Refine-POI框架，通过引入推荐驱动的奖励机制，使LLM能够基于单一目标POI学习生成top-k推荐列表。", "result": "在真实数据集上的实验表明，Refine-POI在top-k推荐性能上达到了最先进的水平。", "conclusion": "Refine-POI有效解决了SFT在POI推荐中的局限性，为LLM-based推荐系统提供了新的优化方向。"}}
{"id": "2506.21607", "pdf": "https://arxiv.org/pdf/2506.21607", "abs": "https://arxiv.org/abs/2506.21607", "authors": ["Dipak Meher", "Carlotta Domeniconi", "Guadalupe Correa-Cabrera"], "title": "CORE-KG: An LLM-Driven Knowledge Graph Construction Framework for Human Smuggling Networks", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Human smuggling networks are increasingly adaptive and difficult to analyze.\nLegal case documents offer valuable insights but are unstructured, lexically\ndense, and filled with ambiguous or shifting references-posing challenges for\nautomated knowledge graph (KG) construction. Existing KG methods often rely on\nstatic templates and lack coreference resolution, while recent LLM-based\napproaches frequently produce noisy, fragmented graphs due to hallucinations,\nand duplicate nodes caused by a lack of guided extraction. We propose CORE-KG,\na modular framework for building interpretable KGs from legal texts. It uses a\ntwo-step pipeline: (1) type-aware coreference resolution via sequential,\nstructured LLM prompts, and (2) entity and relationship extraction using\ndomain-guided instructions, built on an adapted GraphRAG framework. CORE-KG\nreduces node duplication by 33.28%, and legal noise by 38.37% compared to a\nGraphRAG-based baseline-resulting in cleaner and more coherent graph\nstructures. These improvements make CORE-KG a strong foundation for analyzing\ncomplex criminal networks.", "AI": {"tldr": "CORE-KG是一个模块化框架，用于从法律文本构建可解释的知识图谱，通过类型感知的共指消解和领域引导的实体关系提取，显著减少了节点重复和噪声。", "motivation": "分析人类走私网络的适应性挑战，现有方法在共指消解和噪声处理上不足。", "method": "采用两步流程：类型感知共指消解和领域引导的实体关系提取，基于改进的GraphRAG框架。", "result": "节点重复减少33.28%，法律噪声减少38.37%，生成更清晰的知识图谱。", "conclusion": "CORE-KG为分析复杂犯罪网络提供了更可靠的基础。"}}
{"id": "2506.22027", "pdf": "https://arxiv.org/pdf/2506.22027", "abs": "https://arxiv.org/abs/2506.22027", "authors": ["Han Wang", "Shengyang Li", "Jian Yang", "Yuxuan Liu", "Yixuan Lv", "Zhuang Zhou"], "title": "Cross-modal Ship Re-Identification via Optical and SAR Imagery: A Novel Dataset and Method", "categories": ["cs.CV"], "comment": "Accepted to ICCV 2025", "summary": "Detecting and tracking ground objects using earth observation imagery remains\na significant challenge in the field of remote sensing. Continuous maritime\nship tracking is crucial for applications such as maritime search and rescue,\nlaw enforcement, and shipping analysis. However, most current ship tracking\nmethods rely on geostationary satellites or video satellites. The former offer\nlow resolution and are susceptible to weather conditions, while the latter have\nshort filming durations and limited coverage areas, making them less suitable\nfor the real-world requirements of ship tracking. To address these limitations,\nwe present the Hybrid Optical and Synthetic Aperture Radar (SAR) Ship\nRe-Identification Dataset (HOSS ReID dataset), designed to evaluate the\neffectiveness of ship tracking using low-Earth orbit constellations of optical\nand SAR sensors. This approach ensures shorter re-imaging cycles and enables\nall-weather tracking. HOSS ReID dataset includes images of the same ship\ncaptured over extended periods under diverse conditions, using different\nsatellites of different modalities at varying times and angles. Furthermore, we\npropose a baseline method for cross-modal ship re-identification, TransOSS,\nwhich is built on the Vision Transformer architecture. It refines the patch\nembedding structure to better accommodate cross-modal tasks, incorporates\nadditional embeddings to introduce more reference information, and employs\ncontrastive learning to pre-train on large-scale optical-SAR image pairs,\nensuring the model's ability to extract modality-invariant features. Our\ndataset and baseline method are publicly available on\nhttps://github.com/Alioth2000/Hoss-ReID.", "AI": {"tldr": "论文提出了一种混合光学和合成孔径雷达（SAR）的船舶重识别数据集（HOSS ReID），并基于Vision Transformer架构提出了一种跨模态船舶重识别方法TransOSS，以解决现有船舶跟踪方法的局限性。", "motivation": "现有船舶跟踪方法依赖低分辨率的地球静止卫星或拍摄时间短的视频卫星，无法满足全天候、高覆盖的实时跟踪需求。", "method": "提出HOSS ReID数据集，包含多模态、多角度的船舶图像；设计TransOSS方法，改进Vision Transformer的嵌入结构，引入对比学习预训练。", "result": "HOSS ReID数据集支持跨模态船舶重识别，TransOSS方法能够提取模态不变特征，提升跟踪效果。", "conclusion": "HOSS ReID数据集和TransOSS方法为全天候船舶跟踪提供了有效解决方案，具有实际应用潜力。"}}
{"id": "2506.21608", "pdf": "https://arxiv.org/pdf/2506.21608", "abs": "https://arxiv.org/abs/2506.21608", "authors": ["Yasmine Bouamra", "Bruno Yun", "Alexandre Poisson", "Frédéric Armetta"], "title": "SysTemp: A Multi-Agent System for Template-Based Generation of SysML v2", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The automatic generation of SysML v2 models represents a major challenge in\nthe engineering of complex systems, particularly due to the scarcity of\nlearning corpora and complex syntax. We present SysTemp, a system aimed at\nfacilitating and improving the creation of SysML v2 models from natural\nlanguage specifications. It is based on a multi-agent system, including a\ntemplate generator that structures the generation process. We discuss the\nadvantages and challenges of this system through an evaluation, highlighting\nits potential to improve the quality of the generations in SysML v2 modeling.", "AI": {"tldr": "SysTemp是一个基于多代理系统的工具，旨在通过自然语言规范自动生成SysML v2模型，解决了学习语料稀缺和语法复杂的问题。", "motivation": "解决SysML v2模型自动生成中的学习语料稀缺和语法复杂性挑战。", "method": "采用多代理系统，包括模板生成器，以结构化生成过程。", "result": "通过评估展示了SysTemp在提高SysML v2模型生成质量方面的潜力。", "conclusion": "SysTemp为复杂系统工程中的SysML v2模型生成提供了有效支持。"}}
{"id": "2506.22032", "pdf": "https://arxiv.org/pdf/2506.22032", "abs": "https://arxiv.org/abs/2506.22032", "authors": ["Jialei Chen", "Xu Zheng", "Danda Pani Paudel", "Luc Van Gool", "Hiroshi Murase", "Daisuke Deguchi"], "title": "Partial CLIP is Enough: Chimera-Seg for Zero-shot Semantic Segmentation", "categories": ["cs.CV"], "comment": null, "summary": "Zero-shot Semantic Segmentation (ZSS) aims to segment both seen and unseen\nclasses using supervision from only seen classes. Beyond adaptation-based\nmethods, distillation-based approaches transfer vision-language alignment of\nvision-language model, e.g., CLIP, to segmentation models. However, such\nknowledge transfer remains challenging due to: (1) the difficulty of aligning\nvision-based features with the textual space, which requires combining spatial\nprecision with vision-language alignment; and (2) the semantic gap between\nCLIP's global representations and the local, fine-grained features of\nsegmentation models. To address challenge (1), we propose Chimera-Seg, which\nintegrates a segmentation backbone as the body and a CLIP-based semantic head\nas the head, like the Chimera in Greek mythology, combining spatial precision\nwith vision-language alignment. Specifically, Chimera-Seg comprises a trainable\nsegmentation model and a CLIP Semantic Head (CSH), which maps dense features\ninto the CLIP-aligned space. The CSH incorporates a frozen subnetwork and fixed\nprojection layers from the CLIP visual encoder, along with lightweight\ntrainable components. The partial module from CLIP visual encoder, paired with\nthe segmentation model, retains segmentation capability while easing the\nmapping to CLIP's semantic space. To address challenge (2), we propose\nSelective Global Distillation (SGD), which distills knowledge from dense\nfeatures exhibiting high similarity to the CLIP CLS token, while gradually\nreducing the number of features used for alignment as training progresses.\nBesides, we also use a Semantic Alignment Module (SAM) to further align dense\nvisual features with semantic embeddings extracted from the frozen CLIP text\nencoder. Experiments on two benchmarks show improvements of 0.9% and 1.2% in\nhIoU.", "AI": {"tldr": "论文提出Chimera-Seg和SGD方法，解决零样本语义分割中视觉-语言对齐和特征粒度差异问题，实验显示性能提升。", "motivation": "解决零样本语义分割中视觉-语言对齐的挑战（空间精度与对齐的平衡）和CLIP全局表示与分割模型局部特征的语义鸿沟。", "method": "提出Chimera-Seg（结合分割主干和CLIP语义头）和SGD（选择性全局蒸馏），并引入SAM模块进一步对齐特征。", "result": "在两个基准测试中，hIoU分别提升0.9%和1.2%。", "conclusion": "Chimera-Seg和SGD有效解决了零样本语义分割的关键挑战，显著提升了性能。"}}
{"id": "2506.21609", "pdf": "https://arxiv.org/pdf/2506.21609", "abs": "https://arxiv.org/abs/2506.21609", "authors": ["Junhao Liu", "Zhenhao Xu", "Yuxin Fang", "Yichuan Chen", "Zuobin Ying", "Wenhan Chang"], "title": "From Thinking to Output: Chain-of-Thought and Text Generation Characteristics in Reasoning Language Models", "categories": ["cs.CL", "cs.AI", "cs.CR"], "comment": "18 pages, 3 figures", "summary": "Recently, there have been notable advancements in large language models\n(LLMs), demonstrating their growing abilities in complex reasoning. However,\nexisting research largely overlooks a thorough and systematic comparison of\nthese models' reasoning processes and outputs, particularly regarding their\nself-reflection pattern (also termed \"Aha moment\") and the interconnections\nacross diverse domains. This paper proposes a novel framework for analyzing the\nreasoning characteristics of four cutting-edge large reasoning models (GPT-o1,\nDeepSeek-R1, Kimi-k1.5, and Grok-3) using keywords statistic and LLM-as-a-judge\nparadigm. Our approach connects their internal thinking processes with their\nfinal outputs. A diverse dataset consists of real-world scenario-based\nquestions covering logical deduction, causal inference, and multi-step\nproblem-solving. Additionally, a set of metrics is put forward to assess both\nthe coherence of reasoning and the accuracy of the outputs. The research\nresults uncover various patterns of how these models balance exploration and\nexploitation, deal with problems, and reach conclusions during the reasoning\nprocess. Through quantitative and qualitative comparisons, disparities among\nthese models are identified in aspects such as the depth of reasoning, the\nreliance on intermediate steps, and the degree of similarity between their\nthinking processes and output patterns and those of GPT-o1. This work offers\nvaluable insights into the trade-off between computational efficiency and\nreasoning robustness and provides practical recommendations for enhancing model\ndesign and evaluation in practical applications. We publicly release our\nproject at: https://github.com/ChangWenhan/FromThinking2Output", "AI": {"tldr": "本文提出了一种分析大型语言模型推理特性的新框架，通过关键词统计和LLM-as-a-judge范式，比较了四种前沿模型的推理过程和输出。", "motivation": "现有研究缺乏对大型语言模型推理过程和输出的系统性比较，尤其是自我反思模式（“顿悟时刻”）和跨领域关联。", "method": "使用关键词统计和LLM-as-a-judge范式，结合多样化的现实场景问题数据集，评估模型的推理连贯性和输出准确性。", "result": "揭示了模型在推理过程中平衡探索与利用、处理问题和得出结论的不同模式，并比较了它们在推理深度、中间步骤依赖等方面的差异。", "conclusion": "研究为计算效率与推理鲁棒性之间的权衡提供了见解，并为模型设计和评估提供了实用建议。"}}
{"id": "2506.22044", "pdf": "https://arxiv.org/pdf/2506.22044", "abs": "https://arxiv.org/abs/2506.22044", "authors": ["Hong Nie", "Fuyuan Cao", "Lu Chen", "Fengxin Chen", "Yuefeng Zou", "Jun Yu"], "title": "Few-Shot Identity Adaptation for 3D Talking Heads via Global Gaussian Field", "categories": ["cs.CV"], "comment": null, "summary": "Reconstruction and rendering-based talking head synthesis methods achieve\nhigh-quality results with strong identity preservation but are limited by their\ndependence on identity-specific models. Each new identity requires training\nfrom scratch, incurring high computational costs and reduced scalability\ncompared to generative model-based approaches. To overcome this limitation, we\npropose FIAG, a novel 3D speaking head synthesis framework that enables\nefficient identity-specific adaptation using only a few training footage. FIAG\nincorporates Global Gaussian Field, which supports the representation of\nmultiple identities within a shared field, and Universal Motion Field, which\ncaptures the common motion dynamics across diverse identities. Benefiting from\nthe shared facial structure information encoded in the Global Gaussian Field\nand the general motion priors learned in the motion field, our framework\nenables rapid adaptation from canonical identity representations to specific\nones with minimal data. Extensive comparative and ablation experiments\ndemonstrate that our method outperforms existing state-of-the-art approaches,\nvalidating both the effectiveness and generalizability of the proposed\nframework. Code is available at: \\textit{https://github.com/gme-hong/FIAG}.", "AI": {"tldr": "FIAG是一种新型3D说话头合成框架，通过共享的全局高斯场和通用运动场，实现高效的身份特定适应，仅需少量训练数据即可完成。", "motivation": "现有基于重建和渲染的说话头合成方法虽能保持高质量和身份特征，但需要为每个新身份从头训练，计算成本高且扩展性差。", "method": "FIAG结合全局高斯场（支持多身份共享表示）和通用运动场（捕捉跨身份共同运动动态），利用共享结构和运动先验快速适应新身份。", "result": "实验表明FIAG优于现有方法，验证了其有效性和泛化能力。", "conclusion": "FIAG通过共享结构和运动先验，显著降低了身份特定适应的计算成本和数据需求，为说话头合成提供了高效解决方案。"}}
{"id": "2506.21604", "pdf": "https://arxiv.org/pdf/2506.21604", "abs": "https://arxiv.org/abs/2506.21604", "authors": ["Varun Mannam", "Fang Wang", "Xin Chen"], "title": "Evaluating VisualRAG: Quantifying Cross-Modal Performance in Enterprise Document Understanding", "categories": ["cs.IR", "cs.AI", "cs.CV", "cs.HC", "cs.LG"], "comment": "Conference: KDD conference workshop:\n  https://kdd-eval-workshop.github.io/genai-evaluation-kdd2025/", "summary": "Current evaluation frameworks for multimodal generative AI struggle to\nestablish trustworthiness, hindering enterprise adoption where reliability is\nparamount. We introduce a systematic, quantitative benchmarking framework to\nmeasure the trustworthiness of progressively integrating cross-modal inputs\nsuch as text, images, captions, and OCR within VisualRAG systems for enterprise\ndocument intelligence. Our approach establishes quantitative relationships\nbetween technical metrics and user-centric trust measures. Evaluation reveals\nthat optimal modality weighting with weights of 30% text, 15% image, 25%\ncaption, and 30% OCR improves performance by 57.3% over text-only baselines\nwhile maintaining computational efficiency. We provide comparative assessments\nof foundation models, demonstrating their differential impact on\ntrustworthiness in caption generation and OCR extraction-a vital consideration\nfor reliable enterprise AI. This work advances responsible AI deployment by\nproviding a rigorous framework for quantifying and enhancing trustworthiness in\nmultimodal RAG for critical enterprise applications.", "AI": {"tldr": "提出了一种系统化、量化的评估框架，用于衡量多模态生成AI的可信度，并通过实验验证了最优模态权重配置。", "motivation": "当前的多模态生成AI评估框架难以建立可信度，阻碍了企业应用，尤其是在可靠性至关重要的场景中。", "method": "引入了一种系统化的量化基准框架，测量跨模态输入（如文本、图像、标题和OCR）在VisualRAG系统中的可信度，并建立技术指标与用户信任度之间的定量关系。", "result": "实验表明，最优模态权重配置（30%文本、15%图像、25%标题、30%OCR）比纯文本基线性能提升57.3%，同时保持计算效率。", "conclusion": "该研究通过提供严格的框架，量化并提升了多模态RAG在关键企业应用中的可信度，推动了负责任AI的部署。"}}
{"id": "2506.21611", "pdf": "https://arxiv.org/pdf/2506.21611", "abs": "https://arxiv.org/abs/2506.21611", "authors": ["Xiyuan Zhang", "Boran Han", "Haoyang Fang", "Abdul Fatir Ansari", "Shuai Zhang", "Danielle C. Maddix", "Cuixiong Hu", "Andrew Gordon Wilson", "Michael W. Mahoney", "Hao Wang", "Yan Liu", "Huzefa Rangwala", "George Karypis", "Bernie Wang"], "title": "Does Multimodality Lead to Better Time Series Forecasting?", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": null, "summary": "Recently, there has been growing interest in incorporating textual\ninformation into foundation models for time series forecasting. However, it\nremains unclear whether and under what conditions such multimodal integration\nconsistently yields gains. We systematically investigate these questions across\na diverse benchmark of 14 forecasting tasks spanning 7 domains, including\nhealth, environment, and economics. We evaluate two popular multimodal\nforecasting paradigms: aligning-based methods, which align time series and text\nrepresentations; and prompting-based methods, which directly prompt large\nlanguage models for forecasting. Although prior works report gains from\nmultimodal input, we find these effects are not universal across datasets and\nmodels, and multimodal methods sometimes do not outperform the strongest\nunimodal baselines. To understand when textual information helps, we\ndisentangle the effects of model architectural properties and data\ncharacteristics. Our findings highlight that on the modeling side,\nincorporating text information is most helpful given (1) high-capacity text\nmodels, (2) comparatively weaker time series models, and (3) appropriate\naligning strategies. On the data side, performance gains are more likely when\n(4) sufficient training data is available and (5) the text offers complementary\npredictive signal beyond what is already captured from the time series alone.\nOur empirical findings offer practical guidelines for when multimodality can be\nexpected to aid forecasting tasks, and when it does not.", "AI": {"tldr": "研究探讨了在多模态时间序列预测中文本信息的作用，发现其效果因数据集和模型而异，并提出了适用条件。", "motivation": "探索文本信息在多模态时间序列预测中的实际效果及其适用条件。", "method": "系统评估了14个预测任务中的两种多模态方法：对齐法和提示法。", "result": "多模态方法并非总是优于单模态基线，效果取决于模型架构和数据特性。", "conclusion": "文本信息在特定条件下（如高容量文本模型、互补数据）才有效，为实际应用提供了指导。"}}
{"id": "2506.22063", "pdf": "https://arxiv.org/pdf/2506.22063", "abs": "https://arxiv.org/abs/2506.22063", "authors": ["Durgesh K. Singh", "Ahcene Boubekki", "Qing Cao", "Svein Arne Aase", "Robert Jenssen", "Michael Kampffmeyer"], "title": "EnLVAM: Enhanced Left Ventricle Linear Measurements Utilizing Anatomical Motion Mode", "categories": ["cs.CV"], "comment": null, "summary": "Linear measurements of the left ventricle (LV) in the Parasternal Long Axis\n(PLAX) view using B-mode echocardiography are crucial for cardiac assessment.\nThese involve placing 4-6 landmarks along a virtual scanline (SL) perpendicular\nto the LV axis near the mitral valve tips. Manual placement is time-consuming\nand error-prone, while existing deep learning methods often misalign landmarks,\ncausing inaccurate measurements. We propose a novel framework that enhances LV\nmeasurement accuracy by enforcing straight-line constraints. A landmark\ndetector is trained on Anatomical M-Mode (AMM) images, computed in real time\nfrom B-mode videos, then transformed back to B-mode space. This approach\naddresses misalignment and reduces measurement errors. Experiments show\nimproved accuracy over standard B-mode methods, and the framework generalizes\nwell across network architectures. Our semi-automatic design includes a\nhuman-in-the-loop step where the user only places the SL, simplifying\ninteraction while preserving alignment flexibility and clinical relevance.", "AI": {"tldr": "提出了一种通过直线约束增强左心室测量精度的新框架，结合解剖M模式图像训练标志点检测器，减少误差并提高准确性。", "motivation": "手动放置标志点耗时且易错，现有深度学习方法常导致标志点错位，影响测量精度。", "method": "使用解剖M模式图像训练标志点检测器，实时从B模式视频计算并转换回B模式空间，结合半自动设计。", "result": "实验表明，该方法比标准B模式方法更准确，且适用于不同网络架构。", "conclusion": "提出的框架减少了测量误差，简化了用户交互，同时保持了临床相关性和灵活性。"}}
{"id": "2506.21612", "pdf": "https://arxiv.org/pdf/2506.21612", "abs": "https://arxiv.org/abs/2506.21612", "authors": ["Xiaobin Ren", "Xinyu Zhu", "Kaiqi Zhao"], "title": "AdaptGOT: A Pre-trained Model for Adaptive Contextual POI Representation Learning", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Currently, considerable strides have been achieved in Point-of-Interest (POI)\nembedding methodologies, driven by the emergence of novel POI tasks like\nrecommendation and classification. Despite the success of task-specific,\nend-to-end models in POI embedding, several challenges remain. These include\nthe need for more effective multi-context sampling strategies, insufficient\nexploration of multiple POI contexts, limited versatility, and inadequate\ngeneralization. To address these issues, we propose the AdaptGOT model, which\nintegrates both the (Adapt)ive representation learning technique and the\nGeographical-Co-Occurrence-Text (GOT) representation with a particular emphasis\non Geographical location, Co-Occurrence and Textual information. The AdaptGOT\nmodel comprises three key components: (1) contextual neighborhood generation,\nwhich integrates advanced mixed sampling techniques such as KNN, density-based,\nimportance-based, and category-aware strategies to capture complex contextual\nneighborhoods; (2) an advanced GOT representation enhanced by an attention\nmechanism, designed to derive high-quality, customized representations and\nefficiently capture complex interrelations between POIs; and (3) the MoE-based\nadaptive encoder-decoder architecture, which ensures topological consistency\nand enriches contextual representation by minimizing Jensen-Shannon divergence\nacross varying contexts. Experiments on two real-world datasets and multiple\nPOI tasks substantiate the superior performance of the proposed AdaptGOT model.", "AI": {"tldr": "论文提出AdaptGOT模型，通过自适应表示学习和GOT表示解决POI嵌入中的多上下文采样、多样性和泛化问题。", "motivation": "现有POI嵌入方法在多上下文采样、多样性和泛化方面存在不足，需改进。", "method": "AdaptGOT模型结合自适应表示学习和GOT表示，包含上下文邻域生成、增强的GOT表示和MoE编码器-解码器架构。", "result": "在两个真实数据集和多个POI任务上验证了AdaptGOT的优越性能。", "conclusion": "AdaptGOT模型有效解决了POI嵌入中的关键问题，表现优异。"}}
{"id": "2506.22065", "pdf": "https://arxiv.org/pdf/2506.22065", "abs": "https://arxiv.org/abs/2506.22065", "authors": ["Dechao Meng", "Steven Xiao", "Xindi Zhang", "Guangyuan Wang", "Peng Zhang", "Qi Wang", "Bang Zhang", "Liefeng Bo"], "title": "MirrorMe: Towards Realtime and High Fidelity Audio-Driven Halfbody Animation", "categories": ["cs.CV"], "comment": "8 pages, 6 figures", "summary": "Audio-driven portrait animation, which synthesizes realistic videos from\nreference images using audio signals, faces significant challenges in real-time\ngeneration of high-fidelity, temporally coherent animations. While recent\ndiffusion-based methods improve generation quality by integrating audio into\ndenoising processes, their reliance on frame-by-frame UNet architectures\nintroduces prohibitive latency and struggles with temporal consistency. This\npaper introduces MirrorMe, a real-time, controllable framework built on the LTX\nvideo model, a diffusion transformer that compresses video spatially and\ntemporally for efficient latent space denoising. To address LTX's trade-offs\nbetween compression and semantic fidelity, we propose three innovations: 1. A\nreference identity injection mechanism via VAE-encoded image concatenation and\nself-attention, ensuring identity consistency; 2. A causal audio encoder and\nadapter tailored to LTX's temporal structure, enabling precise audio-expression\nsynchronization; and 3. A progressive training strategy combining close-up\nfacial training, half-body synthesis with facial masking, and hand pose\nintegration for enhanced gesture control. Extensive experiments on the EMTD\nBenchmark demonstrate MirrorMe's state-of-the-art performance in fidelity,\nlip-sync accuracy, and temporal stability.", "AI": {"tldr": "MirrorMe是一个基于LTX视频模型的实时、可控框架，用于音频驱动的肖像动画生成，通过创新机制解决了现有方法在延迟和时间一致性上的问题。", "motivation": "音频驱动的肖像动画在实时生成高保真、时间一致的视频方面面临挑战，现有扩散方法因逐帧处理导致高延迟和时间不一致。", "method": "1. 通过VAE编码图像拼接和自注意力机制注入参考身份；2. 为LTX时间结构设计的因果音频编码器和适配器；3. 渐进式训练策略结合面部、半身和手势控制。", "result": "在EMTD基准测试中，MirrorMe在保真度、唇同步准确性和时间稳定性上表现最优。", "conclusion": "MirrorMe通过创新设计实现了高质量的实时肖像动画生成，解决了现有方法的局限性。"}}
{"id": "2506.21613", "pdf": "https://arxiv.org/pdf/2506.21613", "abs": "https://arxiv.org/abs/2506.21613", "authors": ["Gautam Siddharth Kashyap", "Mohammad Anas Azeez", "Rafiq Ali", "Zohaib Hasan Siddiqui", "Jiechao Gao", "Usman Naseem"], "title": "ChildGuard: A Specialized Dataset for Combatting Child-Targeted Hate Speech", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "The increasing prevalence of child-targeted hate speech online underscores\nthe urgent need for specialized datasets to address this critical issue.\nExisting hate speech datasets lack agespecific annotations, fail to capture\nnuanced contexts, and overlook the unique emotional impact on children. To\nbridge this gap, we introduce ChildGuard1, a curated dataset derived from\nexisting corpora and enriched with child-specific annotations. ChildGuard\ncaptures diverse contexts of child-targeted hate speech, spanning age groups.\nWe benchmark existing state-of-the-art hate speech detection methods, including\nLarge Language Models (LLMs), and assess their effectiveness in detecting and\ncontextualizing child-targeted hate speech. To foster further research in this\narea, we publicly release ChildGuard, providing a robust foundation for\ndeveloping improved methods to detect and mitigate such harm.", "AI": {"tldr": "介绍了ChildGuard1数据集，专门针对儿童仇恨言论，填补了现有数据集的不足，并评估了现有仇恨言论检测方法的有效性。", "motivation": "现有仇恨言论数据集缺乏针对儿童的标注和情境捕捉，无法体现对儿童的独特情感影响。", "method": "通过从现有语料库中提取并添加儿童专用标注，构建ChildGuard1数据集，并评估现有仇恨言论检测方法的效果。", "result": "ChildGuard1数据集捕捉了儿童仇恨言论的多样化情境，并公开以促进进一步研究。", "conclusion": "ChildGuard1为改进儿童仇恨言论检测方法提供了坚实基础。"}}
{"id": "2506.22069", "pdf": "https://arxiv.org/pdf/2506.22069", "abs": "https://arxiv.org/abs/2506.22069", "authors": ["Petr Hruby", "Marc Pollefeys"], "title": "Single-Scanline Relative Pose Estimation for Rolling Shutter Cameras", "categories": ["cs.CV", "68T45", "I.4.5"], "comment": "ICCV 2025, 15 pages, 5 figures, 12 tables", "summary": "We propose a novel approach for estimating the relative pose between rolling\nshutter cameras using the intersections of line projections with a single\nscanline per image. This allows pose estimation without explicitly modeling\ncamera motion. Alternatively, scanlines can be selected within a single image,\nenabling single-view relative pose estimation for scanlines of rolling shutter\ncameras. Our approach is designed as a foundational building block for rolling\nshutter structure-from-motion (SfM), where no motion model is required, and\neach scanline's pose can be computed independently. % We classify minimal\nsolvers for this problem in both generic and specialized settings, including\ncases with parallel lines and known gravity direction, assuming known\nintrinsics and no lens distortion. Furthermore, we develop minimal solvers for\nthe parallel-lines scenario, both with and without gravity priors, by\nleveraging connections between this problem and the estimation of 2D structure\nfrom 1D cameras. % Experiments on rolling shutter images from the Fastec\ndataset demonstrate the feasibility of our approach for initializing rolling\nshutter SfM, highlighting its potential for further development. % The code\nwill be made publicly available.", "AI": {"tldr": "提出了一种利用线投影与单扫描线交点估计滚动快门相机相对位姿的新方法，无需显式建模相机运动。", "motivation": "解决滚动快门相机在结构从运动（SfM）中的位姿估计问题，避免对相机运动的显式建模需求。", "method": "通过线投影与单扫描线的交点进行位姿估计，支持单视图或多视图场景，并开发了最小求解器处理平行线和已知重力方向的情况。", "result": "在Fastec数据集上的实验验证了该方法在滚动快门SfM初始化中的可行性。", "conclusion": "该方法为滚动快门SfM提供了基础模块，具有进一步开发的潜力。"}}
{"id": "2506.21614", "pdf": "https://arxiv.org/pdf/2506.21614", "abs": "https://arxiv.org/abs/2506.21614", "authors": ["Yixiong Fang", "Tianran Sun", "Yuling Shi", "Min Wang", "Xiaodong Gu"], "title": "LastingBench: Defend Benchmarks Against Knowledge Leakage", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The increasing complexity of large language models (LLMs) raises concerns\nabout their ability to \"cheat\" on standard Question Answering (QA) benchmarks\nby memorizing task-specific data. This undermines the validity of benchmark\nevaluations, as they no longer reflect genuine model capabilities but instead\nthe effects of data leakage. While prior work has focused on detecting such\nleakage, little attention has been given to mitigating its impact and\npreserving the long-term utility of benchmarks. In this paper, we introduce\nLastingBench, a novel framework designed to continuously reinforce and\nsafeguard existing benchmarks against knowledge leakage. LastingBench\nidentifies leakage points in the context through perturbation, then rewrites\nthe leakage points to counterfactual ones-disrupting memorization while\npreserving the benchmark's original evaluative intent. Evaluations of\nstate-of-the-art QA benchmarks show significant performance gaps, highlighting\nthe efficacy of LastingBench in reducing memorization effects. LastingBench\noffers a practical and scalable solution to ensure benchmark robustness over\ntime, promoting fairer and more interpretable evaluations of LLMs.", "AI": {"tldr": "论文提出了LastingBench框架，通过扰动和重写泄漏点来减少大语言模型在QA基准测试中的记忆效应，确保评测的长期有效性。", "motivation": "大语言模型在QA基准测试中可能通过记忆任务特定数据“作弊”，影响评测有效性，需解决数据泄漏问题。", "method": "LastingBench通过扰动识别泄漏点，并将其重写为反事实内容，破坏记忆效应，同时保留评测意图。", "result": "实验表明LastingBench显著减少了记忆效应，揭示了现有基准测试中的性能差距。", "conclusion": "LastingBench为长期保持基准测试的鲁棒性提供了实用方案，促进了更公平、可解释的LLM评测。"}}
{"id": "2506.22075", "pdf": "https://arxiv.org/pdf/2506.22075", "abs": "https://arxiv.org/abs/2506.22075", "authors": ["Shaheer U. Saeed", "Yipei Wang", "Veeru Kasivisvanathan", "Brian R. Davidson", "Matthew J. Clarkson", "Yipeng Hu", "Daniel C. Alexander"], "title": "Reasoning in machine vision: learning to think fast and slow", "categories": ["cs.CV"], "comment": null, "summary": "Reasoning is a hallmark of human intelligence, enabling adaptive\ndecision-making in complex and unfamiliar scenarios. In contrast, machine\nintelligence remains bound to training data, lacking the ability to dynamically\nrefine solutions at inference time. While some recent advances have explored\nreasoning in machines, these efforts are largely limited to verbal domains such\nas mathematical problem-solving, where explicit rules govern step-by-step\nreasoning. Other critical real-world tasks - including visual perception,\nspatial reasoning, and radiological diagnosis - require non-verbal reasoning,\nwhich remains an open challenge. Here we present a novel learning paradigm that\nenables machine reasoning in vision by allowing performance improvement with\nincreasing thinking time (inference-time compute), even under conditions where\nlabelled data is very limited. Inspired by dual-process theories of human\ncognition in psychology, our approach integrates a fast-thinking System I\nmodule for familiar tasks, with a slow-thinking System II module that\niteratively refines solutions using self-play reinforcement learning. This\nparadigm mimics human reasoning by proposing, competing over, and refining\nsolutions in data-scarce scenarios. We demonstrate superior performance through\nextended thinking time, compared not only to large-scale supervised learning\nbut also foundation models and even human experts, in real-world vision tasks.\nThese tasks include computer-vision benchmarks and cancer localisation on\nmedical images across five organs, showcasing transformative potential for\nnon-verbal machine reasoning.", "AI": {"tldr": "论文提出了一种新的学习范式，通过增加推理时间（计算资源）提升机器在视觉任务中的推理能力，尤其在数据稀缺的情况下表现优异。", "motivation": "人类推理能力强大，而机器智能受限于训练数据，无法在推理时动态优化解决方案。现有研究多集中在基于规则的语言领域，非语言领域的推理仍具挑战性。", "method": "受心理学双过程理论启发，结合快速思考的System I模块和慢速思考的System II模块，通过自玩强化学习迭代优化解决方案。", "result": "在视觉任务中，包括计算机视觉基准和医学图像癌症定位，该方法的性能优于大规模监督学习、基础模型甚至人类专家。", "conclusion": "该范式为非语言机器推理提供了变革性潜力，尤其在数据稀缺场景下表现出色。"}}
{"id": "2506.21615", "pdf": "https://arxiv.org/pdf/2506.21615", "abs": "https://arxiv.org/abs/2506.21615", "authors": ["Wenhao Li", "Hongkuan Zhang", "Hongwei Zhang", "Zhengxu Li", "Zengjie Dong", "Yafan Chen", "Niranjan Bidargaddi", "Hong Liu"], "title": "Refine Medical Diagnosis Using Generation Augmented Retrieval and Clinical Practice Guidelines", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Current medical language models, adapted from large language models (LLMs),\ntypically predict ICD code-based diagnosis from electronic health records\n(EHRs) because these labels are readily available. However, ICD codes do not\ncapture the nuanced, context-rich reasoning clinicians use for diagnosis.\nClinicians synthesize diverse patient data and reference clinical practice\nguidelines (CPGs) to make evidence-based decisions. This misalignment limits\nthe clinical utility of existing models. We introduce GARMLE-G, a\nGeneration-Augmented Retrieval framework that grounds medical language model\noutputs in authoritative CPGs. Unlike conventional Retrieval-Augmented\nGeneration based approaches, GARMLE-G enables hallucination-free outputs by\ndirectly retrieving authoritative guideline content without relying on\nmodel-generated text. It (1) integrates LLM predictions with EHR data to create\nsemantically rich queries, (2) retrieves relevant CPG knowledge snippets via\nembedding similarity, and (3) fuses guideline content with model output to\ngenerate clinically aligned recommendations. A prototype system for\nhypertension diagnosis was developed and evaluated on multiple metrics,\ndemonstrating superior retrieval precision, semantic relevance, and clinical\nguideline adherence compared to RAG-based baselines, while maintaining a\nlightweight architecture suitable for localized healthcare deployment. This\nwork provides a scalable, low-cost, and hallucination-free method for grounding\nmedical language models in evidence-based clinical practice, with strong\npotential for broader clinical deployment.", "AI": {"tldr": "GARMLE-G是一个基于生成增强检索的框架，通过直接检索权威临床指南内容，避免了模型生成文本的幻觉问题，提升了医疗语言模型的临床实用性。", "motivation": "现有的医疗语言模型基于ICD代码预测诊断，但ICD代码无法捕捉临床医生的复杂推理过程，导致模型临床实用性受限。", "method": "GARMLE-G结合LLM预测和EHR数据生成语义丰富的查询，通过嵌入相似性检索相关CPG知识片段，并将指南内容与模型输出融合生成临床对齐的建议。", "result": "原型系统在高血压诊断中表现出优于基于RAG的基线模型的检索精度、语义相关性和临床指南依从性，同时保持轻量级架构。", "conclusion": "GARMLE-G提供了一种可扩展、低成本且无幻觉的方法，将医疗语言模型与循证临床实践结合，具有广泛临床部署潜力。"}}
{"id": "2506.22078", "pdf": "https://arxiv.org/pdf/2506.22078", "abs": "https://arxiv.org/abs/2506.22078", "authors": ["Pei-Kai Huanga", "Ya-Ting Chan", "Kuan-Wen Chen", "Yen-Chun Chou", "Shih-Yu Yang", "Chiou-Ting Hsu"], "title": "Towards Accurate Heart Rate Measurement from Ultra-Short Video Clips via Periodicity-Guided rPPG Estimation and Signal Reconstruction", "categories": ["cs.CV"], "comment": null, "summary": "Many remote Heart Rate (HR) measurement methods focus on estimating remote\nphotoplethysmography (rPPG) signals from video clips lasting around 10 seconds\nbut often overlook the need for HR estimation from ultra-short video clips. In\nthis paper, we aim to accurately measure HR from ultra-short 2-second video\nclips by specifically addressing two key challenges. First, to overcome the\nlimited number of heartbeat cycles in ultra-short video clips, we propose an\neffective periodicity-guided rPPG estimation method that enforces consistent\nperiodicity between rPPG signals estimated from ultra-short clips and their\nmuch longer ground truth signals. Next, to mitigate estimation inaccuracies due\nto spectral leakage, we propose including a generator to reconstruct longer\nrPPG signals from ultra-short ones while preserving their periodic consistency\nto enable more accurate HR measurement. Extensive experiments on four rPPG\nestimation benchmark datasets demonstrate that our proposed method not only\naccurately measures HR from ultra-short video clips but also outperform\nprevious rPPG estimation techniques to achieve state-of-the-art performance.", "AI": {"tldr": "提出了一种从超短2秒视频片段中准确测量心率的方法，通过周期性引导的rPPG估计和信号生成器解决频谱泄漏问题，实验表明其优于现有技术。", "motivation": "现有远程心率测量方法多关注10秒视频片段，但忽视了超短视频片段的需求，本文旨在解决这一问题。", "method": "提出周期性引导的rPPG估计方法，并通过生成器重构长信号以保持周期性一致性，提高HR测量准确性。", "result": "在四个rPPG基准数据集上实验，证明该方法在超短视频片段中准确测量HR，并达到最优性能。", "conclusion": "该方法在超短视频片段中实现了高精度HR测量，且性能优于现有技术。"}}
{"id": "2506.21616", "pdf": "https://arxiv.org/pdf/2506.21616", "abs": "https://arxiv.org/abs/2506.21616", "authors": ["Chuanrui Hu", "Wei Hu", "Penghang Yu", "Hua Zhang", "Bing-Kun Bao"], "title": "TIM: A Large-Scale Dataset and large Timeline Intelligence Model for Open-domain Timeline Summarization", "categories": ["cs.CL", "cs.CY"], "comment": null, "summary": "Open-domain Timeline Summarization (TLS) is crucial for monitoring the\nevolution of news topics. To identify changes in news topics, existing methods\ntypically employ general Large Language Models (LLMs) to summarize relevant\ntimestamps from retrieved news. While general LLMs demonstrate capabilities in\nzero-shot news summarization and timestamp localization, they struggle with\nassessing topic relevance and understanding topic evolution. Consequently, the\nsummarized information often includes irrelevant details or inaccurate\ntimestamps. To address these issues, we propose the first large Timeline\nIntelligence Model (TIM) for open-domain TLS, which is capable of effectively\nsummarizing open-domain timelines. Specifically, we begin by presenting a\nlarge-scale TLS dataset, comprising over 1,000 news topics and more than 3,000\nannotated TLS instances. Furthermore, we propose a progressive optimization\nstrategy, which gradually enhance summarization performance. It employs\ninstruction tuning to enhance summarization and topic-irrelevant information\nfiltering capabilities. Following this, it exploits a novel dual-alignment\nreward learning method that incorporates both semantic and temporal\nperspectives, thereby improving the understanding of topic evolution\nprinciples. Through this progressive optimization strategy, TIM demonstrates a\nrobust ability to summarize open-domain timelines. Extensive experiments in\nopen-domain demonstrate the effectiveness of our TIM.", "AI": {"tldr": "论文提出了一种名为TIM的大型时间线智能模型，用于开放领域时间线摘要（TLS），解决了现有方法在主题相关性和时间演化理解上的不足。", "motivation": "现有方法使用通用大型语言模型（LLMs）进行时间线摘要时，难以准确评估主题相关性和理解主题演化，导致摘要中包含无关信息或不准确时间戳。", "method": "提出TIM模型，采用渐进优化策略：首先通过指令调优提升摘要和主题无关信息过滤能力，然后利用双对齐奖励学习方法从语义和时间角度改进对主题演化的理解。", "result": "TIM在开放领域实验中表现出强大的时间线摘要能力。", "conclusion": "TIM通过渐进优化策略有效解决了开放领域时间线摘要中的问题，实验验证了其有效性。"}}
{"id": "2506.22099", "pdf": "https://arxiv.org/pdf/2506.22099", "abs": "https://arxiv.org/abs/2506.22099", "authors": ["Zipei Ma", "Junzhe Jiang", "Yurui Chen", "Li Zhang"], "title": "BézierGS: Dynamic Urban Scene Reconstruction with Bézier Curve Gaussian Splatting", "categories": ["cs.CV"], "comment": "Accepted at ICCV 2025, Project Page:\n  https://github.com/fudan-zvg/BezierGS", "summary": "The realistic reconstruction of street scenes is critical for developing\nreal-world simulators in autonomous driving. Most existing methods rely on\nobject pose annotations, using these poses to reconstruct dynamic objects and\nmove them during the rendering process. This dependence on high-precision\nobject annotations limits large-scale and extensive scene reconstruction. To\naddress this challenge, we propose B\\'ezier curve Gaussian splatting\n(B\\'ezierGS), which represents the motion trajectories of dynamic objects using\nlearnable B\\'ezier curves. This approach fully leverages the temporal\ninformation of dynamic objects and, through learnable curve modeling,\nautomatically corrects pose errors. By introducing additional supervision on\ndynamic object rendering and inter-curve consistency constraints, we achieve\nreasonable and accurate separation and reconstruction of scene elements.\nExtensive experiments on the Waymo Open Dataset and the nuPlan benchmark\ndemonstrate that B\\'ezierGS outperforms state-of-the-art alternatives in both\ndynamic and static scene components reconstruction and novel view synthesis.", "AI": {"tldr": "提出了一种基于Bézier曲线的高斯泼溅方法（BézierGS），用于动态物体的运动轨迹建模，无需依赖高精度物体姿态标注，实现了大规模场景的合理重建。", "motivation": "现有方法依赖物体姿态标注，限制了大规模场景重建的扩展性。BézierGS通过建模动态物体的运动轨迹，解决了这一问题。", "method": "利用可学习的Bézier曲线表示动态物体的运动轨迹，结合动态物体渲染的额外监督和曲线间一致性约束，实现场景元素的分离与重建。", "result": "在Waymo Open Dataset和nuPlan基准测试中，BézierGS在动态和静态场景重建及新视角合成方面优于现有方法。", "conclusion": "BézierGS通过建模动态物体的运动轨迹，显著提升了场景重建的准确性和扩展性。"}}
{"id": "2506.21618", "pdf": "https://arxiv.org/pdf/2506.21618", "abs": "https://arxiv.org/abs/2506.21618", "authors": ["Zhiyuan Zhang", "Xiaosong Jia", "Guanyu Chen", "Qifeng Li", "Junchi Yan"], "title": "TrajTok: Technical Report for 2025 Waymo Open Sim Agents Challenge", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In this technical report, we introduce TrajTok, a trajectory tokenizer for\ndiscrete next-token-prediction based behavior generation models, which combines\ndata-driven and rule-based methods with better coverage, symmetry and\nrobustness, along with a spatial-aware label smoothing method for cross-entropy\nloss. We adopt the tokenizer and loss for the SMART model and reach a superior\nperformance with realism score of 0.7852 on the Waymo Open Sim Agents Challenge\n2025. We will open-source the code in the future.", "AI": {"tldr": "TrajTok是一种轨迹标记器，结合数据驱动和基于规则的方法，用于离散的下一个标记预测行为生成模型，具有更好的覆盖性、对称性和鲁棒性。", "motivation": "提高行为生成模型的性能，特别是在覆盖性、对称性和鲁棒性方面。", "method": "结合数据驱动和基于规则的方法，采用空间感知标签平滑方法优化交叉熵损失。", "result": "在Waymo Open Sim Agents Challenge 2025上实现了0.7852的真实性评分。", "conclusion": "TrajTok和优化的损失方法显著提升了行为生成模型的性能，未来将开源代码。"}}
{"id": "2506.22101", "pdf": "https://arxiv.org/pdf/2506.22101", "abs": "https://arxiv.org/abs/2506.22101", "authors": ["Hyeongji Kim", "Stine Hansen", "Michael Kampffmeyer"], "title": "Tied Prototype Model for Few-Shot Medical Image Segmentation", "categories": ["cs.CV", "cs.LG", "stat.ML"], "comment": "Submitted version (MICCAI). Accepted at MICCAI 2025. The code repo\n  will be made publicly available soon", "summary": "Common prototype-based medical image few-shot segmentation (FSS) methods\nmodel foreground and background classes using class-specific prototypes.\nHowever, given the high variability of the background, a more promising\ndirection is to focus solely on foreground modeling, treating the background as\nan anomaly -- an approach introduced by ADNet. Yet, ADNet faces three key\nlimitations: dependence on a single prototype per class, a focus on binary\nclassification, and fixed thresholds that fail to adapt to patient and organ\nvariability. To address these shortcomings, we propose the Tied Prototype Model\n(TPM), a principled reformulation of ADNet with tied prototype locations for\nforeground and background distributions. Building on its probabilistic\nfoundation, TPM naturally extends to multiple prototypes and multi-class\nsegmentation while effectively separating non-typical background features.\nNotably, both extensions lead to improved segmentation accuracy. Finally, we\nleverage naturally occurring class priors to define an ideal target for\nadaptive thresholds, boosting segmentation performance. Taken together, TPM\nprovides a fresh perspective on prototype-based FSS for medical image\nsegmentation. The code can be found at https://github.com/hjk92g/TPM-FSS.", "AI": {"tldr": "TPM改进ADNet，通过绑定原型位置和多原型扩展，提升医学图像少样本分割性能。", "motivation": "解决ADNet在单原型、二分类和固定阈值上的局限性。", "method": "提出TPM，绑定前景和背景分布的原型位置，支持多原型和多类分割，自适应阈值。", "result": "TPM在多原型和多类分割中表现更优，提升分割精度。", "conclusion": "TPM为医学图像少样本分割提供了新视角，性能显著提升。"}}
{"id": "2506.21619", "pdf": "https://arxiv.org/pdf/2506.21619", "abs": "https://arxiv.org/abs/2506.21619", "authors": ["Siyi Zhou", "Yiquan Zhou", "Yi He", "Xun Zhou", "Jinchao Wang", "Wei Deng", "Jingchen Shu"], "title": "IndexTTS2: A Breakthrough in Emotionally Expressive and Duration-Controlled Auto-Regressive Zero-Shot Text-to-Speech", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Large-scale text-to-speech (TTS) models are typically categorized into\nautoregressive and non-autoregressive systems. Although autoregressive systems\nexhibit certain advantages in speech naturalness, their token-by-token\ngeneration mechanism makes it difficult to precisely control the duration of\nsynthesized speech. This is a key limitation in applications such as video\ndubbing that require strict audio-visual synchronization. This paper introduces\nIndexTTS2, which proposes a novel and autoregressive-model-friendly method for\nspeech duration control. The method supports two generation modes: one allows\nexplicit specification of the number of generated tokens for precise duration\ncontrol; the other does not require manual input and lets the model freely\ngenerate speech while preserving prosodic characteristics from the input\nprompt. Furthermore, IndexTTS2 achieves disentanglement between emotional\nexpression and speaker identity, enabling independent control of timbre and\nemotion. In the zero-shot setting, the model can perfectly reproduce the\nemotional characteristics of the input prompt. Users may also provide a\nseparate emotion prompt, even from a different speaker, allowing the model to\nreconstruct the target timbre while conveying the desired emotion. To enhance\nclarity during strong emotional expressions, we incorporate GPT latent\nrepresentations to improve speech stability. Meanwhile, to lower the barrier\nfor emotion control, we design a soft instruction mechanism based on textual\ndescriptions by fine-tuning Qwen3. This enables effective guidance of speech\ngeneration with desired emotional tendencies using natural language input.\nExperimental results demonstrate that IndexTTS2 outperforms existing\nstate-of-the-art zero-shot TTS models in word error rate, speaker similarity,\nand emotional fidelity.", "AI": {"tldr": "IndexTTS2提出了一种新的自回归模型友好的语音时长控制方法，支持两种生成模式，并实现了音色与情感的分离控制，显著提升了零样本TTS模型的性能。", "motivation": "解决自回归TTS模型在语音时长控制上的局限性，特别是在需要严格音视频同步的应用中，如视频配音。", "method": "提出两种生成模式：显式指定生成token数以精确控制时长，或自由生成保留输入提示的韵律特征；结合GPT潜在表示提升稳定性，并设计基于文本描述的软指令机制。", "result": "在零样本设置下，IndexTTS2在词错误率、说话人相似度和情感保真度上优于现有最先进的TTS模型。", "conclusion": "IndexTTS2通过创新的时长控制和情感分离方法，显著提升了TTS模型的灵活性和表现力。"}}
{"id": "2506.22111", "pdf": "https://arxiv.org/pdf/2506.22111", "abs": "https://arxiv.org/abs/2506.22111", "authors": ["Ruthvik Bokkasam", "Shankar Gangisetty", "A. H. Abdul Hafez", "C. V. Jawahar"], "title": "Pedestrian Intention and Trajectory Prediction in Unstructured Traffic Using IDD-PeD", "categories": ["cs.CV", "cs.HC"], "comment": null, "summary": "With the rapid advancements in autonomous driving, accurately predicting\npedestrian behavior has become essential for ensuring safety in complex and\nunpredictable traffic conditions. The growing interest in this challenge\nhighlights the need for comprehensive datasets that capture unstructured\nenvironments, enabling the development of more robust prediction models to\nenhance pedestrian safety and vehicle navigation. In this paper, we introduce\nan Indian driving pedestrian dataset designed to address the complexities of\nmodeling pedestrian behavior in unstructured environments, such as illumination\nchanges, occlusion of pedestrians, unsignalized scene types and\nvehicle-pedestrian interactions. The dataset provides high-level and detailed\nlow-level comprehensive annotations focused on pedestrians requiring the\nego-vehicle's attention. Evaluation of the state-of-the-art intention\nprediction methods on our dataset shows a significant performance drop of up to\n$\\mathbf{15\\%}$, while trajectory prediction methods underperform with an\nincrease of up to $\\mathbf{1208}$ MSE, defeating standard pedestrian datasets.\nAdditionally, we present exhaustive quantitative and qualitative analysis of\nintention and trajectory baselines. We believe that our dataset will open new\nchallenges for the pedestrian behavior research community to build robust\nmodels. Project Page:\nhttps://cvit.iiit.ac.in/research/projects/cvit-projects/iddped", "AI": {"tldr": "论文介绍了一个印度驾驶行人数据集，旨在解决非结构化环境中行人行为建模的复杂性，并展示了现有预测方法在该数据集上的性能下降。", "motivation": "随着自动驾驶的快速发展，准确预测行人行为对复杂交通环境中的安全至关重要，但目前缺乏能够捕捉非结构化环境的全面数据集。", "method": "论文提出了一个印度驾驶行人数据集，包含高水平和低水平的详细注释，专注于需要自车注意的行人行为。", "result": "在该数据集上，现有意图预测方法的性能下降了15%，轨迹预测方法的MSE增加了1208，表现不如标准数据集。", "conclusion": "该数据集为行人行为研究社区提供了新的挑战，有助于构建更鲁棒的模型。"}}
{"id": "2506.21620", "pdf": "https://arxiv.org/pdf/2506.21620", "abs": "https://arxiv.org/abs/2506.21620", "authors": ["Daniele Cirulli", "Giulio Cimini", "Giovanni Palermo"], "title": "How Large Language Models play humans in online conversations: a simulated study of the 2016 US politics on Reddit", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.SI", "physics.soc-ph"], "comment": null, "summary": "Large Language Models (LLMs) have recently emerged as powerful tools for\nnatural language generation, with applications spanning from content creation\nto social simulations. Their ability to mimic human interactions raises both\nopportunities and concerns, particularly in the context of politically relevant\nonline discussions. In this study, we evaluate the performance of LLMs in\nreplicating user-generated content within a real-world, divisive scenario:\nReddit conversations during the 2016 US Presidential election. In particular,\nwe conduct three different experiments, asking GPT-4 to generate comments by\nimpersonating either real or artificial partisan users. We analyze the\ngenerated comments in terms of political alignment, sentiment, and linguistic\nfeatures, comparing them against real user contributions and benchmarking\nagainst a null model. We find that GPT-4 is able to produce realistic comments,\nboth in favor of or against the candidate supported by the community, yet\ntending to create consensus more easily than dissent. In addition we show that\nreal and artificial comments are well separated in a semantically embedded\nspace, although they are indistinguishable by manual inspection. Our findings\nprovide insights on the potential use of LLMs to sneak into online discussions,\ninfluence political debate and shape political narratives, bearing broader\nimplications of AI-driven discourse manipulation.", "AI": {"tldr": "该研究评估了GPT-4在模拟2016年美国总统选举期间Reddit用户评论的能力，发现其能生成政治立场鲜明且逼真的评论，但倾向于制造共识而非分歧。", "motivation": "探讨大型语言模型（LLMs）在政治相关在线讨论中的表现及其潜在影响。", "method": "通过三个实验，让GPT-4模拟真实或虚构的党派用户生成评论，分析其政治立场、情感和语言特征。", "result": "GPT-4能生成逼真的评论，但倾向于制造共识；人工与AI评论在语义空间中可区分，但人工检查难以辨别。", "conclusion": "LLMs可能被用于操纵在线政治讨论，对AI驱动的舆论塑造具有广泛影响。"}}
{"id": "2506.22118", "pdf": "https://arxiv.org/pdf/2506.22118", "abs": "https://arxiv.org/abs/2506.22118", "authors": ["Antje Alex", "Jannis Stoppe"], "title": "Pipe Reconstruction from Point Cloud Data", "categories": ["cs.CV"], "comment": null, "summary": "Accurate digital twins of industrial assets, such as ships and offshore\nplatforms, rely on the precise reconstruction of complex pipe networks.\nHowever, manual modelling of pipes from laser scan data is a time-consuming and\nlabor-intensive process. This paper presents a pipeline for automated pipe\nreconstruction from incomplete laser scan data. The approach estimates a\nskeleton curve using Laplacian-based contraction, followed by curve elongation.\nThe skeleton axis is then recentred using a rolling sphere technique combined\nwith 2D circle fitting, and refined with a 3D smoothing step. This enables the\ndetermination of pipe properties, including radius, length and orientation, and\nfacilitates the creation of detailed 3D models of complex pipe networks. By\nautomating pipe reconstruction, this approach supports the development of\ndigital twins, allowing for rapid and accurate modeling while reducing costs.", "AI": {"tldr": "提出了一种从激光扫描数据自动重建管道的流程，支持数字孪生开发。", "motivation": "手动建模管道耗时耗力，需要自动化方法提高效率和精度。", "method": "使用拉普拉斯收缩估计骨架曲线，结合滚动球技术和2D圆拟合重新定心，并通过3D平滑细化。", "result": "能够确定管道属性（半径、长度、方向），并生成复杂管网的详细3D模型。", "conclusion": "该方法实现了快速、准确的管道建模，降低了成本，支持数字孪生应用。"}}
{"id": "2506.21621", "pdf": "https://arxiv.org/pdf/2506.21621", "abs": "https://arxiv.org/abs/2506.21621", "authors": ["Jasper Dekoninck", "Ivo Petrov", "Kristian Minchev", "Mislav Balunovic", "Martin Vechev", "Miroslav Marinov", "Maria Drencheva", "Lyuba Konova", "Milen Shumanov", "Kaloyan Tsvetkov", "Nikolay Drenchev", "Lazar Todorov", "Kalina Nikolova", "Nikolay Georgiev", "Vanesa Kalinkova", "Margulan Ismoldayev"], "title": "The Open Proof Corpus: A Large-Scale Study of LLM-Generated Mathematical Proofs", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "In recent months, large language models (LLMs) have made significant progress\nin mathematical proof generation, but further advancement is hindered by the\nlack of a large-scale, high-quality dataset of human-evaluated proofs. While\nexpensive to create, such a dataset is essential for driving improvements in\ntraining and enabling a rigorous analysis of proof generation capabilities. In\nthis work, we present the Open Proof Corpus (OPC), a dataset comprising over\n5,000 human-evaluated proofs produced by state-of-the-art LLMs. The OPC was\nspecifically designed for broad applicability and downstream usage in proof\ngeneration research and is the first to include a substantial number of\ncorrect, LLM-generated solutions to problems from prestigious mathematics\ncompetitions such as the USAMO and IMO. Using the OPC, we explore critical\nquestions in automated proof generation: (1) the performance gap between\nnatural language and formal proof generation, (2) the discrepancy between\nfinal-answer accuracy and full-proof validity, and (3) the impact of best-of-n\nselection on proof quality. Finally, to showcase the utility of the OPC, we\nfinetune an 8B-parameter model on the dataset, obtaining a model that performs\non par with the best model, Gemini-2.5-Pro, on the task of evaluating proof\ncorrectness.", "AI": {"tldr": "论文介绍了Open Proof Corpus (OPC)，一个包含5000多条人类评估证明的数据集，用于推动数学证明生成的研究。", "motivation": "当前大型语言模型在数学证明生成方面进展显著，但缺乏高质量的人类评估数据集限制了进一步的发展。", "method": "构建了OPC数据集，包含LLM生成的证明，并探索了自动化证明生成中的关键问题。", "result": "使用OPC微调模型，性能与Gemini-2.5-Pro相当。", "conclusion": "OPC为数学证明生成研究提供了重要资源，并展示了其实际应用价值。"}}
{"id": "2506.22134", "pdf": "https://arxiv.org/pdf/2506.22134", "abs": "https://arxiv.org/abs/2506.22134", "authors": ["Zhengyun Cheng", "Changhao Wang", "Guanwen Zhang", "Yi Xu", "Wei Zhou", "Xiangyang Ji"], "title": "Low-Rank Implicit Neural Representation via Schatten-p Quasi-Norm and Jacobian Regularization", "categories": ["cs.CV"], "comment": "Submitted to IEEE Transactions on Circuits and Systems for Video\n  Technology", "summary": "Higher-order tensors are well-suited for representing multi-dimensional data,\nsuch as color images and videos. Low-rank tensor representation has become\nessential in machine learning and computer vision, but existing methods like\nTucker decomposition offer flexibility at the expense of interpretability. In\ncontrast, while the CANDECOMP/PARAFAC (CP) decomposition provides a more\nnatural and interpretable tensor structure, obtaining sparse solutions remains\nchallenging. Leveraging the rich properties of CP decomposition, we propose a\nCP-based low-rank tensor function parameterized by neural networks for implicit\nneural representation (CP-INR). This approach enables continuous data\nrepresentation beyond structured grids, fully exploiting the non-linearity of\ntensor data with theoretical guarantees on excess risk bounds. To achieve a\nsparse CP decomposition, we introduce a variational form of the Schatten-p\nquasi-norm and prove its relationship to multilinear rank minimization. For\nsmoothness, we propose a regularization term based on the spectral norm of the\nJacobian and Hutchinson's trace estimator. Our proposed smoothness\nregularization is SVD-free and avoids explicit chain rule derivations. It can\nserve as an alternative to Total Variation (TV) regularization in image\ndenoising tasks and is naturally applicable to continuous data. Extensive\nexperiments on multi-dimensional data recovery tasks, including image\ninpainting, denoising, and point cloud upsampling, demonstrate the superiority\nand versatility of our method compared to state-of-the-art approaches.", "AI": {"tldr": "提出了一种基于CP分解的低秩张量函数（CP-INR），用于连续数据表示，并通过稀疏化和平滑性正则化提升性能。", "motivation": "现有张量分解方法（如Tucker和CP）在灵活性和可解释性之间存在权衡，且稀疏解难以获得。", "method": "结合CP分解和神经网络参数化，引入Schatten-p拟范数实现稀疏化，提出基于谱范数的平滑性正则化。", "result": "在多维数据恢复任务（如图像修复、去噪和点云上采样）中表现优于现有方法。", "conclusion": "CP-INR方法在理论和实验上均验证了其优越性和通用性。"}}
{"id": "2506.21617", "pdf": "https://arxiv.org/pdf/2506.21617", "abs": "https://arxiv.org/abs/2506.21617", "authors": ["Hiba Bederina", "Jill-Jênn Vie"], "title": "Bayesian-Guided Diversity in Sequential Sampling for Recommender Systems", "categories": ["cs.IR", "cs.AI"], "comment": null, "summary": "The challenge of balancing user relevance and content diversity in\nrecommender systems is increasingly critical amid growing concerns about\ncontent homogeneity and reduced user engagement. In this work, we propose a\nnovel framework that leverages a multi-objective, contextual sequential\nsampling strategy. Item selection is guided by Bayesian updates that\ndynamically adjust scores to optimize diversity. The reward formulation\nintegrates multiple diversity metrics-including the log-determinant volume of a\ntuned similarity submatrix and ridge leverage scores-along with a diversity\ngain uncertainty term to address the exploration-exploitation trade-off. Both\nintra- and inter-batch diversity are modeled to promote serendipity and\nminimize redundancy. A dominance-based ranking procedure identifies\nPareto-optimal item sets, enabling adaptive and balanced selections at each\niteration. Experiments on a real-world dataset show that our approach\nsignificantly improves diversity without sacrificing relevance, demonstrating\nits potential to enhance user experience in large-scale recommendation\nsettings.", "AI": {"tldr": "提出了一种基于多目标上下文顺序采样的推荐系统框架，通过贝叶斯更新动态调整评分以优化多样性，实验证明在保持相关性的同时显著提高了多样性。", "motivation": "解决推荐系统中用户相关性与内容多样性之间的平衡问题，避免内容同质化和用户参与度下降。", "method": "采用多目标上下文顺序采样策略，结合贝叶斯更新动态评分，整合多种多样性指标（如对数行列式体积和岭杠杆分数）和多样性增益不确定性项。", "result": "在真实数据集上实验表明，该方法显著提高了多样性且未牺牲相关性。", "conclusion": "该框架在大规模推荐场景中具有提升用户体验的潜力。"}}
{"id": "2506.21622", "pdf": "https://arxiv.org/pdf/2506.21622", "abs": "https://arxiv.org/abs/2506.21622", "authors": ["Niclas Pokel", "Pehuén Moure", "Roman Boehringer", "Yingqiang Gao"], "title": "Adapting Foundation Speech Recognition Models to Impaired Speech: A Semantic Re-chaining Approach for Personalization of German Speech", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "comment": null, "summary": "Speech impairments caused by conditions such as cerebral palsy or genetic\ndisorders pose significant challenges for automatic speech recognition (ASR)\nsystems. Despite recent advances, ASR models like Whisper struggle with\nnon-normative speech due to limited training data and the difficulty of\ncollecting and annotating non-normative speech samples. In this work, we\npropose a practical and lightweight pipeline to personalize ASR models,\nformalizing the selection of words and enriching a small, speech-impaired\ndataset with semantic coherence. Applied to data from a child with a structural\nspeech impairment, our approach shows promising improvements in transcription\nquality, demonstrating the potential to reduce communication barriers for\nindividuals with atypical speech patterns.", "AI": {"tldr": "提出了一种轻量级管道，用于个性化ASR模型，改善对非标准语音的识别效果。", "motivation": "现有ASR模型（如Whisper）对非标准语音识别效果不佳，主要因训练数据有限且收集标注困难。", "method": "通过选择单词并增强语义连贯性，个性化ASR模型。", "result": "在儿童结构语音障碍数据上应用，转录质量显著提升。", "conclusion": "该方法有望减少非标准语音个体的沟通障碍。"}}
{"id": "2506.22139", "pdf": "https://arxiv.org/pdf/2506.22139", "abs": "https://arxiv.org/abs/2506.22139", "authors": ["Shaojie Zhang", "Jiahui Yang", "Jianqin Yin", "Zhenbo Luo", "Jian Luan"], "title": "Q-Frame: Query-aware Frame Selection and Multi-Resolution Adaptation for Video-LLMs", "categories": ["cs.CV"], "comment": "Accepted at ICCV 2025", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated significant\nsuccess in visual understanding tasks. However, challenges persist in adapting\nthese models for video comprehension due to the large volume of data and\ntemporal complexity. Existing Video-LLMs using uniform frame sampling often\nstruggle to capture the query-related crucial spatiotemporal clues of videos\neffectively. In this paper, we introduce Q-Frame, a novel approach for adaptive\nframe selection and multi-resolution scaling tailored to the video's content\nand the specific query. Q-Frame employs a training-free, plug-and-play strategy\ngenerated by a text-image matching network like CLIP, utilizing the Gumbel-Max\ntrick for efficient frame selection. Q-Frame allows Video-LLMs to process more\nframes without exceeding computational limits, thereby preserving critical\ntemporal and spatial information. We demonstrate Q-Frame's effectiveness\nthrough extensive experiments on benchmark datasets, including MLVU,\nLongVideoBench, and Video-MME, illustrating its superiority over existing\nmethods and its applicability across various video understanding tasks.", "AI": {"tldr": "Q-Frame是一种自适应帧选择和多分辨率缩放方法，显著提升视频理解任务中的性能。", "motivation": "现有视频-LLM在处理视频时难以有效捕捉关键时空线索，Q-Frame旨在解决这一问题。", "method": "Q-Frame采用无训练、即插即用策略，基于文本-图像匹配网络（如CLIP）和Gumbel-Max技巧进行高效帧选择。", "result": "在MLVU、LongVideoBench和Video-MME等基准数据集上，Q-Frame表现优于现有方法。", "conclusion": "Q-Frame能有效提升视频理解任务中的性能，且具有广泛适用性。"}}
{"id": "2506.21623", "pdf": "https://arxiv.org/pdf/2506.21623", "abs": "https://arxiv.org/abs/2506.21623", "authors": ["Peiheng Gao", "Chen Yang", "Ning Sun", "Ričardas Zitikis"], "title": "Performance of diverse evaluation metrics in NLP-based assessment and text generation of consumer complaints", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Machine learning (ML) has significantly advanced text classification by\nenabling automated understanding and categorization of complex, unstructured\ntextual data. However, accurately capturing nuanced linguistic patterns and\ncontextual variations inherent in natural language, particularly within\nconsumer complaints, remains a challenge. This study addresses these issues by\nincorporating human-experience-trained algorithms that effectively recognize\nsubtle semantic differences crucial for assessing consumer relief eligibility.\nFurthermore, we propose integrating synthetic data generation methods that\nutilize expert evaluations of generative adversarial networks and are refined\nthrough expert annotations. By combining expert-trained classifiers with\nhigh-quality synthetic data, our research seeks to significantly enhance\nmachine learning classifier performance, reduce dataset acquisition costs, and\nimprove overall evaluation metrics and robustness in text classification tasks.", "AI": {"tldr": "该研究通过结合人类经验训练的算法和合成数据生成方法，提升机器学习在文本分类中的性能，特别是在消费者投诉领域。", "motivation": "解决自然语言处理中捕捉细微语义差异的挑战，特别是在消费者投诉文本分类中。", "method": "结合人类经验训练的算法和基于专家评估的合成数据生成方法。", "result": "显著提升分类器性能，降低数据集获取成本，并改善评估指标和鲁棒性。", "conclusion": "通过专家训练和高质量合成数据的结合，能够有效提升文本分类任务的性能。"}}
{"id": "2506.22146", "pdf": "https://arxiv.org/pdf/2506.22146", "abs": "https://arxiv.org/abs/2506.22146", "authors": ["Amirmohammad Izadi", "Mohammad Ali Banayeeanzade", "Fatemeh Askari", "Ali Rahimiakbar", "Mohammad Mahdi Vahedi", "Hosein Hasani", "Mahdieh Soleymani Baghshah"], "title": "Visual Structures Helps Visual Reasoning: Addressing the Binding Problem in VLMs", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": null, "summary": "Despite progress in Vision-Language Models (VLMs), their capacity for visual\nreasoning is often limited by the \\textit{binding problem}: the failure to\nreliably associate perceptual features with their correct visual referents.\nThis limitation underlies persistent errors in tasks such as counting, visual\nsearch, scene description, and spatial relationship understanding. A key factor\nis that current VLMs process visual features largely in parallel, lacking\nmechanisms for spatially grounded, serial attention. This paper introduces a\nsimple yet effective intervention: augmenting visual inputs with low-level\nspatial structures (e.g., horizontal lines) and pairing this with a textual\nprompt that encourages sequential, spatially-aware parsing. We empirically\ndemonstrate substantial performance improvements across core visual reasoning\ntasks. Specifically, our method improves GPT-4o visual search accuracy by\n25.00%, increases counting accuracy by 26.83%, reduces edit distance error in\nscene description by 0.32, and enhances performance on spatial relationship\ntasks by 9.50% on a a 2D synthetic dataset. Furthermore, we find that the\nvisual modification is essential for these gains; purely textual strategies,\nincluding Chain-of-Thought prompting, are insufficient and can even degrade\nperformance. Our method enhances binding only with a single-query inference,\nunderscoring the importance of visual input design over purely\nlinguistically-based approaches. These findings suggest that low-level visual\nstructuring is a powerful and underexplored direction for improving\ncompositional visual reasoning and could serve as a general strategy for\nenhancing VLM performance on spatially grounded tasks.", "AI": {"tldr": "通过增强视觉输入的低级空间结构（如水平线）并结合文本提示，显著提升了视觉语言模型在视觉推理任务中的性能。", "motivation": "当前视觉语言模型在视觉推理中存在绑定问题，难以可靠地将感知特征与视觉对象关联，导致计数、视觉搜索等任务表现不佳。", "method": "在视觉输入中添加低级空间结构，并配合鼓励顺序、空间感知解析的文本提示。", "result": "在多个任务中显著提升性能：视觉搜索准确率提高25.00%，计数准确率提高26.83%，场景描述编辑距离误差减少0.32，空间关系任务性能提升9.50%。", "conclusion": "低级视觉结构化是提升视觉语言模型性能的有效方向，优于纯语言策略，为空间任务提供了通用改进方法。"}}
{"id": "2506.21625", "pdf": "https://arxiv.org/pdf/2506.21625", "abs": "https://arxiv.org/abs/2506.21625", "authors": ["Jiaxi Zhuang", "Kangning Li", "Jue Hou", "Mingjun Xu", "Zhifeng Gao", "Hengxing Cai"], "title": "Doc2SAR: A Synergistic Framework for High-Fidelity Extraction of Structure-Activity Relationships from Scientific Documents", "categories": ["cs.CL", "cs.AI", "cs.IR"], "comment": null, "summary": "Extracting molecular structure-activity relationships (SARs) from scientific\nliterature and patents is essential for drug discovery and materials research.\nHowever, this task remains challenging due to heterogeneous document formats\nand limitations of existing methods. Specifically, rule-based approaches\nrelying on rigid templates fail to generalize across diverse document layouts,\nwhile general-purpose multimodal large language models (MLLMs) lack sufficient\naccuracy and reliability for specialized tasks, such as layout detection and\noptical chemical structure recognition (OCSR). To address these challenges, we\nintroduce DocSAR-200, a rigorously annotated benchmark of 200 scientific\ndocuments designed specifically for evaluating SAR extraction methods.\nAdditionally, we propose Doc2SAR, a novel synergistic framework that integrates\ndomain-specific tools with MLLMs enhanced via supervised fine-tuning (SFT).\nExtensive experiments demonstrate that Doc2SAR achieves state-of-the-art\nperformance across various document types, significantly outperforming leading\nend-to-end baselines. Specifically, Doc2SAR attains an overall Table Recall of\n80.78% on DocSAR-200, exceeding end2end GPT-4o by 51.48%. Furthermore, Doc2SAR\ndemonstrates practical usability through efficient inference and is accompanied\nby a web app.", "AI": {"tldr": "论文提出了Doc2SAR框架，结合领域专用工具和微调的多模态大语言模型，显著提升了从科学文献中提取分子结构-活性关系（SAR）的性能。", "motivation": "现有方法在提取SAR时面临文档格式多样性和准确性不足的挑战，需要更高效的解决方案。", "method": "提出Doc2SAR框架，整合领域专用工具和微调的多模态大语言模型，并引入DocSAR-200基准数据集。", "result": "Doc2SAR在DocSAR-200上达到80.78%的表格召回率，比GPT-4o高51.48%。", "conclusion": "Doc2SAR在SAR提取任务中表现优异，具有实际应用价值。"}}
{"id": "2506.22149", "pdf": "https://arxiv.org/pdf/2506.22149", "abs": "https://arxiv.org/abs/2506.22149", "authors": ["Ronald Fecso", "José Morano", "Ursula Schmidt-Erfurth", "Hrvoje Bogunović"], "title": "RetFiner: A Vision-Language Refinement Scheme for Retinal Foundation Models", "categories": ["cs.CV"], "comment": "Accepted for presentation at MICCAI 2025", "summary": "The rise of imaging techniques such as optical coherence tomography (OCT) and\nadvances in deep learning (DL) have enabled clinicians and researchers to\nstreamline retinal disease staging. A popular DL approach is self-supervised\nlearning (SSL), where models learn from vast amounts of unlabeled data,\navoiding costly annotation. SSL has allowed the development of foundation\nmodels (FMs), large models that can be used for a variety of downstream tasks.\nHowever, existing FMs for OCT, trained solely on image data, lack a\ncomprehensive and robust semantic understanding of images, as evidenced by\ntheir downstream performance (especially for complex tasks), and thus require\nsupervised fine-tuning (which may be unfeasible) to better adapt to specific\napplications and populations. To address this, we propose RetFiner, an SSL\nvision-language refinement scheme that improves the representations of existing\nFMs and enables their efficient and direct adaptation to specific populations\nfor improved downstream performance. Our method uses a diverse set of training\nobjectives which take advantage of the rich supervisory signal found in textual\ndata. We tested RetFiner on the retinal FMs RETFound, UrFound, and VisionFM,\nshowing significant improvements in linear probing performance on seven highly\ndiverse OCT classification tasks, with an average increase of 5.8, 3.9, and 2.1\npercentage points over their baselines, respectively. Our code and model\nweights are publicly available at https://github.com/ronnief1/RetFiner.", "AI": {"tldr": "RetFiner是一种自监督视觉语言细化方案，通过利用文本数据的丰富监督信号，改进现有基础模型（FMs）的表征能力，显著提升下游任务性能。", "motivation": "现有OCT基础模型仅基于图像数据训练，缺乏全面的语义理解，需监督微调以适应特定任务和人群，但成本高昂。", "method": "提出RetFiner，结合多种训练目标，利用文本数据的监督信号优化FMs表征。", "result": "在七项OCT分类任务中，RetFiner显著提升性能，平均分别提高5.8、3.9和2.1个百分点。", "conclusion": "RetFiner通过视觉语言细化，无需监督微调即可高效适应特定人群，提升下游任务表现。"}}
{"id": "2506.21682", "pdf": "https://arxiv.org/pdf/2506.21682", "abs": "https://arxiv.org/abs/2506.21682", "authors": ["Li Zhou", "Hao Jiang", "Junjie Li", "Zefeng Zhao", "Feng Jiang", "Wenyu Chen", "Haizhou Li"], "title": "Do We Really Need GNNs with Explicit Structural Modeling? MLPs Suffice for Language Model Representations", "categories": ["cs.CL"], "comment": "Graph Neural Networks, Multi-Layer Perceptrons, Explicit Structural\n  Modeling, Probing Classifier", "summary": "Explicit structural information has been proven to be encoded by Graph Neural\nNetworks (GNNs), serving as auxiliary knowledge to enhance model capabilities\nand improve performance in downstream NLP tasks. However, recent studies\nindicate that GNNs fail to fully utilize structural information, whereas\nMulti-Layer Perceptrons (MLPs), despite lacking the message-passing mechanisms\ninherent to GNNs, exhibit a surprising ability in structure-aware tasks.\nMotivated by these findings, this paper introduces a comprehensive probing\nframework from an information-theoretic perspective. The framework is designed\nto systematically assess the role of explicit structural modeling in enhancing\nlanguage model (LM) representations and to investigate the potential of MLPs as\nefficient and scalable alternatives to GNNs. We extend traditional probing\nclassifiers by incorporating a control module that allows for selective use of\neither the full GNN model or its decoupled components, specifically, the\nmessage-passing and feature-transformation operations.This modular approach\nisolates and assesses the individual contributions of these operations,\navoiding confounding effects from the complete GNN architecture. Using the Edge\nProbing Suite, a diagnostic tool for evaluating the linguistic knowledge\nencoded in LMs, we find that MLPs, when used as feature-transformation modules,\nconsistently improve the linguistic knowledge captured in LM representations\nacross different architectures. They effectively encode both syntactic and\nsemantic patterns. Similarly, GNNs that incorporate feature-transformation\noperations show beneficial effects. In contrast, models that rely solely on\nmessage-passing operations tend to underperform, often leading to negative\nimpacts on probing task performance.", "AI": {"tldr": "论文探讨了图神经网络（GNNs）和多层感知机（MLPs）在结构感知任务中的表现差异，提出了一个信息论视角的探测框架，评估显式结构建模对语言模型表示的影响，并验证MLPs作为GNNs高效替代方案的潜力。", "motivation": "研究动机源于GNNs未能充分利用结构信息，而MLPs却表现出意外的结构感知能力。", "method": "通过引入控制模块的探测框架，分离评估GNNs的消息传递和特征转换操作，使用Edge Probing Suite工具进行实验。", "result": "MLPs作为特征转换模块能有效提升语言模型表示中的语言知识，而仅依赖消息传递的模型表现较差。", "conclusion": "MLPs在结构感知任务中具有潜力，可作为GNNs的高效替代方案。"}}
{"id": "2506.22161", "pdf": "https://arxiv.org/pdf/2506.22161", "abs": "https://arxiv.org/abs/2506.22161", "authors": ["Taijin Zhao", "Heqian Qiu", "Yu Dai", "Lanxiao Wang", "Fanman Meng", "Qingbo Wu", "Hongliang Li"], "title": "Attention-disentangled Uniform Orthogonal Feature Space Optimization for Few-shot Object Detection", "categories": ["cs.CV"], "comment": null, "summary": "Few-shot object detection (FSOD) aims to detect objects with limited samples\nfor novel classes, while relying on abundant data for base classes. Existing\nFSOD approaches, predominantly built on the Faster R-CNN detector, entangle\nobjectness recognition and foreground classification within shared feature\nspaces. This paradigm inherently establishes class-specific objectness criteria\nand suffers from unrepresentative novel class samples. To resolve this\nlimitation, we propose a Uniform Orthogonal Feature Space (UOFS) optimization\nframework. First, UOFS decouples the feature space into two orthogonal\ncomponents, where magnitude encodes objectness and angle encodes\nclassification. This decoupling enables transferring class-agnostic objectness\nknowledge from base classes to novel classes. Moreover, implementing the\ndisentanglement requires careful attention to two challenges: (1) Base set\nimages contain unlabeled foreground instances, causing confusion between\npotential novel class instances and backgrounds. (2) Angular optimization\ndepends exclusively on base class foreground instances, inducing overfitting of\nangular distributions to base classes. To address these challenges, we propose\na Hybrid Background Optimization (HBO) strategy: (1) Constructing a pure\nbackground base set by removing unlabeled instances in original images to\nprovide unbiased magnitude-based objectness supervision. (2) Incorporating\nunlabeled foreground instances in the original base set into angular\noptimization to enhance distribution uniformity. Additionally, we propose a\nSpatial-wise Attention Disentanglement and Association (SADA) module to address\ntask conflicts between class-agnostic and class-specific tasks. Experiments\ndemonstrate that our method significantly outperforms existing approaches based\non entangled feature spaces.", "AI": {"tldr": "论文提出了一种名为UOFS的优化框架，通过解耦特征空间和HBO策略，解决了少样本目标检测中的问题，显著优于现有方法。", "motivation": "现有少样本目标检测方法在共享特征空间中耦合了目标识别和前景分类，导致类特定目标性标准和样本不具代表性的问题。", "method": "UOFS框架将特征空间解耦为两个正交分量，分别编码目标性和分类；HBO策略通过纯背景基集和未标记前景实例优化角度分布。", "result": "实验表明，该方法显著优于基于耦合特征空间的现有方法。", "conclusion": "UOFS和HBO策略有效解决了少样本目标检测中的关键问题，提升了性能。"}}
{"id": "2506.21686", "pdf": "https://arxiv.org/pdf/2506.21686", "abs": "https://arxiv.org/abs/2506.21686", "authors": ["Swastika Kundu", "Autoshi Ibrahim", "Mithila Rahman", "Tanvir Ahmed"], "title": "ANUBHUTI: A Comprehensive Corpus For Sentiment Analysis In Bangla Regional Languages", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Sentiment analysis for regional dialects of Bangla remains an underexplored\narea due to linguistic diversity and limited annotated data. This paper\nintroduces ANUBHUTI, a comprehensive dataset consisting of 2000 sentences\nmanually translated from standard Bangla into four major regional dialects\nMymensingh, Noakhali, Sylhet, and Chittagong. The dataset predominantly\nfeatures political and religious content, reflecting the contemporary socio\npolitical landscape of Bangladesh, alongside neutral texts to maintain balance.\nEach sentence is annotated using a dual annotation scheme: multiclass thematic\nlabeling categorizes sentences as Political, Religious, or Neutral, and\nmultilabel emotion annotation assigns one or more emotions from Anger,\nContempt, Disgust, Enjoyment, Fear, Sadness, and Surprise. Expert native\ntranslators conducted the translation and annotation, with quality assurance\nperformed via Cohens Kappa inter annotator agreement, achieving strong\nconsistency across dialects. The dataset was further refined through systematic\nchecks for missing data, anomalies, and inconsistencies. ANUBHUTI fills a\ncritical gap in resources for sentiment analysis in low resource Bangla\ndialects, enabling more accurate and context aware natural language processing.", "AI": {"tldr": "ANUBHUTI是一个包含2000句孟加拉语方言的标注数据集，用于情感分析，填补了低资源方言研究的空白。", "motivation": "由于语言多样性和标注数据有限，孟加拉语方言的情感分析研究不足。", "method": "数据集包含政治、宗教和中性文本，采用多类主题标注和多标签情感标注，由专家翻译和标注，并通过一致性检验。", "result": "数据集质量高，标注一致性良好，为低资源方言的情感分析提供了可靠资源。", "conclusion": "ANUBHUTI填补了孟加拉语方言情感分析的资源缺口，有助于更准确的NLP研究。"}}
{"id": "2506.22179", "pdf": "https://arxiv.org/pdf/2506.22179", "abs": "https://arxiv.org/abs/2506.22179", "authors": ["Wenhan Wu", "Zhishuai Guo", "Chen Chen", "Hongfei Xue", "Aidong Lu"], "title": "Frequency-Semantic Enhanced Variational Autoencoder for Zero-Shot Skeleton-based Action Recognition", "categories": ["cs.CV", "cs.AI"], "comment": "Accepted to ICCV 2025", "summary": "Zero-shot skeleton-based action recognition aims to develop models capable of\nidentifying actions beyond the categories encountered during training. Previous\napproaches have primarily focused on aligning visual and semantic\nrepresentations but often overlooked the importance of fine-grained action\npatterns in the semantic space (e.g., the hand movements in drinking water and\nbrushing teeth). To address these limitations, we propose a Frequency-Semantic\nEnhanced Variational Autoencoder (FS-VAE) to explore the skeleton semantic\nrepresentation learning with frequency decomposition. FS-VAE consists of three\nkey components: 1) a frequency-based enhancement module with high- and\nlow-frequency adjustments to enrich the skeletal semantics learning and improve\nthe robustness of zero-shot action recognition; 2) a semantic-based action\ndescription with multilevel alignment to capture both local details and global\ncorrespondence, effectively bridging the semantic gap and compensating for the\ninherent loss of information in skeleton sequences; 3) a calibrated\ncross-alignment loss that enables valid skeleton-text pairs to counterbalance\nambiguous ones, mitigating discrepancies and ambiguities in skeleton and text\nfeatures, thereby ensuring robust alignment. Evaluations on the benchmarks\ndemonstrate the effectiveness of our approach, validating that\nfrequency-enhanced semantic features enable robust differentiation of visually\nand semantically similar action clusters, improving zero-shot action\nrecognition.", "AI": {"tldr": "提出了一种频率-语义增强的变分自编码器（FS-VAE），用于零样本骨架动作识别，通过频率分解和语义对齐提升性能。", "motivation": "解决现有方法忽视语义空间中细粒度动作模式的问题，如手部动作的差异。", "method": "FS-VAE包含三个关键模块：频率增强模块、多级语义对齐模块和校准交叉对齐损失。", "result": "在基准测试中验证了方法的有效性，频率增强的语义特征能更好地区分视觉和语义相似的动作。", "conclusion": "FS-VAE通过频率和语义的联合优化，显著提升了零样本骨架动作识别的性能。"}}
{"id": "2506.21712", "pdf": "https://arxiv.org/pdf/2506.21712", "abs": "https://arxiv.org/abs/2506.21712", "authors": ["Tzu-Quan Lin", "Hsi-Chun Cheng", "Hung-yi Lee", "Hao Tang"], "title": "Identifying Speaker Information in Feed-Forward Layers of Self-Supervised Speech Transformers", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": null, "summary": "In recent years, the impact of self-supervised speech Transformers has\nextended to speaker-related applications. However, little research has explored\nhow these models encode speaker information. In this work, we address this gap\nby identifying neurons in the feed-forward layers that are correlated with\nspeaker information. Specifically, we analyze neurons associated with k-means\nclusters of self-supervised features and i-vectors. Our analysis reveals that\nthese clusters correspond to broad phonetic and gender classes, making them\nsuitable for identifying neurons that represent speakers. By protecting these\nneurons during pruning, we can significantly preserve performance on\nspeaker-related task, demonstrating their crucial role in encoding speaker\ninformation.", "AI": {"tldr": "本文研究了自监督语音Transformer模型中编码说话者信息的神经元，发现这些神经元与语音和性别类别相关，保护它们可显著保留说话者相关任务的性能。", "motivation": "探索自监督语音Transformer模型如何编码说话者信息，填补相关研究的空白。", "method": "通过分析前馈层中与k-means聚类和i-vectors相关的神经元，识别与说话者信息相关的神经元。", "result": "这些神经元与语音和性别类别相关，保护它们能显著保留说话者相关任务的性能。", "conclusion": "前馈层中的特定神经元在编码说话者信息中起关键作用，保护它们可优化模型性能。"}}
{"id": "2506.22191", "pdf": "https://arxiv.org/pdf/2506.22191", "abs": "https://arxiv.org/abs/2506.22191", "authors": ["Yuxin Cui", "Rui Song", "Yibin Li", "Max Q. -H. Meng", "Zhe Min"], "title": "Robust and Accurate Multi-view 2D/3D Image Registration with Differentiable X-ray Rendering and Dual Cross-view Constraints", "categories": ["cs.CV", "cs.RO"], "comment": "ICRA 2025", "summary": "Robust and accurate 2D/3D registration, which aligns preoperative models with\nintraoperative images of the same anatomy, is crucial for successful\ninterventional navigation. To mitigate the challenge of a limited field of view\nin single-image intraoperative scenarios, multi-view 2D/3D registration is\nrequired by leveraging multiple intraoperative images. In this paper, we\npropose a novel multi-view 2D/3D rigid registration approach comprising two\nstages. In the first stage, a combined loss function is designed, incorporating\nboth the differences between predicted and ground-truth poses and the\ndissimilarities (e.g., normalized cross-correlation) between simulated and\nobserved intraoperative images. More importantly, additional cross-view\ntraining loss terms are introduced for both pose and image losses to explicitly\nenforce cross-view constraints. In the second stage, test-time optimization is\nperformed to refine the estimated poses from the coarse stage. Our method\nexploits the mutual constraints of multi-view projection poses to enhance the\nrobustness of the registration process. The proposed framework achieves a mean\ntarget registration error (mTRE) of $0.79 \\pm 2.17$ mm on six specimens from\nthe DeepFluoro dataset, demonstrating superior performance compared to\nstate-of-the-art registration algorithms.", "AI": {"tldr": "提出了一种新颖的多视角2D/3D刚性配准方法，通过两阶段设计和交叉视角约束，显著提高了配准的鲁棒性和准确性。", "motivation": "解决单视角术中图像视野有限的问题，利用多视角图像提升配准精度。", "method": "两阶段方法：第一阶段设计联合损失函数，引入交叉视角训练损失；第二阶段进行测试时优化，细化估计位姿。", "result": "在DeepFluoro数据集上实现了0.79±2.17 mm的平均目标配准误差，优于现有方法。", "conclusion": "该方法通过多视角约束显著提升了配准性能，适用于临床导航应用。"}}
{"id": "2506.21745", "pdf": "https://arxiv.org/pdf/2506.21745", "abs": "https://arxiv.org/abs/2506.21745", "authors": ["Eivind Morris Bakke", "Nora Winger Heggelund"], "title": "(Fact) Check Your Bias", "categories": ["cs.CL"], "comment": null, "summary": "Automatic fact verification systems increasingly rely on large language\nmodels (LLMs). We investigate how parametric knowledge biases in these models\naffect fact-checking outcomes of the HerO system (baseline for FEVER-25). We\nexamine how the system is affected by: (1) potential bias in Llama 3.1's\nparametric knowledge and (2) intentionally injected bias. When prompted\ndirectly to perform fact-verification, Llama 3.1 labels nearly half the claims\nas \"Not Enough Evidence\". Using only its parametric knowledge it is able to\nreach a verdict on the remaining half of the claims. In the second experiment,\nwe prompt the model to generate supporting, refuting, or neutral fact-checking\ndocuments. These prompts significantly influence retrieval outcomes, with\napproximately 50\\% of retrieved evidence being unique to each perspective.\nNotably, the model sometimes refuses to generate supporting documents for\nclaims it believes to be false, creating an inherent negative bias. Despite\ndifferences in retrieved evidence, final verdict predictions show stability\nacross prompting strategies. The code is available at:\nhttps://github.com/eibakke/FEVER-8-Shared-Task", "AI": {"tldr": "研究了大型语言模型（LLM）中的参数知识偏见如何影响HerO系统的事实核查结果，发现直接提示时模型对近半数声明标记为“证据不足”，而故意注入的偏见显著影响检索结果。", "motivation": "探讨LLM中的参数知识偏见对事实核查系统的影响，以改进其公正性和可靠性。", "method": "通过直接提示和故意注入偏见两种方式测试Llama 3.1模型在事实核查中的表现，并分析其检索结果和最终判断。", "result": "直接提示时模型对50%的声明无法判断；故意注入偏见导致50%的检索证据因视角不同而独特；最终判断在不同提示策略下保持稳定。", "conclusion": "LLM的参数知识偏见显著影响事实核查过程，但最终判断相对稳定，需进一步优化以减少偏见。"}}
{"id": "2506.22216", "pdf": "https://arxiv.org/pdf/2506.22216", "abs": "https://arxiv.org/abs/2506.22216", "authors": ["Ming Zhao", "Pingping Liu", "Tongshun Zhang", "Zhe Zhang"], "title": "ReF-LLE: Personalized Low-Light Enhancement via Reference-Guided Deep Reinforcement Learning", "categories": ["cs.CV", "eess.IV"], "comment": "6 pages, 8 figures, accepted by ICME2025", "summary": "Low-light image enhancement presents two primary challenges: 1) Significant\nvariations in low-light images across different conditions, and 2) Enhancement\nlevels influenced by subjective preferences and user intent. To address these\nissues, we propose ReF-LLE, a novel personalized low-light image enhancement\nmethod that operates in the Fourier frequency domain and incorporates deep\nreinforcement learning. ReF-LLE is the first to integrate deep reinforcement\nlearning into this domain. During training, a zero-reference image evaluation\nstrategy is introduced to score enhanced images, providing reward signals that\nguide the model to handle varying degrees of low-light conditions effectively.\nIn the inference phase, ReF-LLE employs a personalized adaptive iterative\nstrategy, guided by the zero-frequency component in the Fourier domain, which\nrepresents the overall illumination level. This strategy enables the model to\nadaptively adjust low-light images to align with the illumination distribution\nof a user-provided reference image, ensuring personalized enhancement results.\nExtensive experiments on benchmark datasets demonstrate that ReF-LLE\noutperforms state-of-the-art methods, achieving superior perceptual quality and\nadaptability in personalized low-light image enhancement.", "AI": {"tldr": "ReF-LLE是一种基于傅里叶频域和深度强化学习的个性化低光图像增强方法，首次将深度强化学习引入该领域，通过零参考图像评分策略和个性化自适应迭代策略，显著提升图像增强效果。", "motivation": "解决低光图像增强中的两个主要挑战：不同条件下的图像差异大，以及增强效果受主观偏好和用户意图影响。", "method": "在傅里叶频域中结合深度强化学习，训练时引入零参考图像评分策略，推理时通过零频分量指导个性化自适应迭代。", "result": "在基准数据集上表现优于现有方法，实现了更高的感知质量和个性化适应性。", "conclusion": "ReF-LLE通过结合频域分析和强化学习，有效解决了低光图像增强的挑战，提供了高质量的个性化增强结果。"}}
{"id": "2506.21627", "pdf": "https://arxiv.org/pdf/2506.21627", "abs": "https://arxiv.org/abs/2506.21627", "authors": ["Shiyi Wang", "Wenbo Li", "Yiteng Chen", "Qingyao Wu", "Huiping Zhuang"], "title": "FrankenBot: Brain-Morphic Modular Orchestration for Robotic Manipulation with Vision-Language Models", "categories": ["cs.RO", "cs.AI", "F.4.3; I.2.9"], "comment": "15 pages, 4 figures, under review of NeurIPS", "summary": "Developing a general robot manipulation system capable of performing a wide\nrange of tasks in complex, dynamic, and unstructured real-world environments\nhas long been a challenging task. It is widely recognized that achieving\nhuman-like efficiency and robustness manipulation requires the robotic brain to\nintegrate a comprehensive set of functions, such as task planning, policy\ngeneration, anomaly monitoring and handling, and long-term memory, achieving\nhigh-efficiency operation across all functions. Vision-Language Models (VLMs),\npretrained on massive multimodal data, have acquired rich world knowledge,\nexhibiting exceptional scene understanding and multimodal reasoning\ncapabilities. However, existing methods typically focus on realizing only a\nsingle function or a subset of functions within the robotic brain, without\nintegrating them into a unified cognitive architecture. Inspired by a\ndivide-and-conquer strategy and the architecture of the human brain, we propose\nFrankenBot, a VLM-driven, brain-morphic robotic manipulation framework that\nachieves both comprehensive functionality and high operational efficiency. Our\nframework includes a suite of components, decoupling a part of key functions\nfrom frequent VLM calls, striking an optimal balance between functional\ncompleteness and system efficiency. Specifically, we map task planning, policy\ngeneration, memory management, and low-level interfacing to the cortex,\ncerebellum, temporal lobe-hippocampus complex, and brainstem, respectively, and\ndesign efficient coordination mechanisms for the modules. We conducted\ncomprehensive experiments in both simulation and real-world robotic\nenvironments, demonstrating that our method offers significant advantages in\nanomaly detection and handling, long-term memory, operational efficiency, and\nstability -- all without requiring any fine-tuning or retraining.", "AI": {"tldr": "论文提出了FrankenBot，一种基于视觉语言模型（VLM）的机器人操作框架，通过模仿人脑结构实现多功能集成与高效操作。", "motivation": "开发一个能在复杂动态环境中执行广泛任务的通用机器人操作系统，需要整合多种功能（如任务规划、策略生成等），但现有方法通常仅关注单一功能，缺乏统一架构。", "method": "采用分治策略和人脑结构启发，设计FrankenBot框架，将任务规划、策略生成等功能映射到不同脑区，并优化模块协调机制。", "result": "在仿真和真实环境中验证，FrankenBot在异常处理、长期记忆、效率和稳定性方面表现优异，且无需微调或再训练。", "conclusion": "FrankenBot通过VLM驱动和人脑结构模拟，实现了功能全面与高效操作的平衡，为通用机器人系统提供了新思路。"}}
{"id": "2506.21783", "pdf": "https://arxiv.org/pdf/2506.21783", "abs": "https://arxiv.org/abs/2506.21783", "authors": ["Alexandru Dumitru", "V Venktesh", "Adam Jatowt", "Avishek Anand"], "title": "Evaluating List Construction and Temporal Understanding capabilities of Large Language Models", "categories": ["cs.CL", "cs.AI"], "comment": "Accepted at ICTIR 2025 co-located with SIGIR 2025, 11 pages", "summary": "Large Language Models (LLMs) have demonstrated immense advances in a wide\nrange of natural language tasks. However, these models are susceptible to\nhallucinations and errors on particularly temporal understanding tasks\ninvolving multiple entities in answers. In such tasks, they fail to associate\nentities with accurate time intervals, generate a complete list of entities in\nanswers or reason about events associated with specific temporal bounds.\nExisting works do not extensively evaluate the abilities of the model to\nperform implicit and explicit temporal understanding in a list answer\nconstruction setup. To bridge this gap, we propose the Time referenced List\nbased Question Answering or TLQA benchmark that requires structured answers in\nlist format aligned with corresponding time periods. Our TLQA benchmark,\nrequires both list construction and temporal understanding simultaneously,\nwhich to the best of our knowledge has not been explored in prior benchmarks.\nWe investigate the temporal understanding and list construction capabilities of\nstate-of-the-art generative models on TLQA in closed-book and open-domain\nsettings. Our findings reveal significant shortcomings in current models,\nparticularly their inability to provide complete answers and temporally align\nfacts in a closed-book setup and the need to improve retrieval in open-domain\nsetup, providing clear future directions for research on TLQA. The benchmark\nand code at https://github.com/elixir-research-group/TLQA.", "AI": {"tldr": "该论文提出了一个名为TLQA的基准测试，用于评估大语言模型在时间引用列表问答任务中的表现，揭示了当前模型在时间对齐和列表构建方面的不足。", "motivation": "大语言模型在时间理解任务中容易产生幻觉和错误，现有研究未充分评估模型在列表构建和时间对齐方面的能力。", "method": "提出了TLQA基准测试，要求模型生成与时间区间对齐的结构化列表答案，并在闭卷和开放域设置下评估模型表现。", "result": "研究发现当前模型在闭卷设置中难以提供完整答案和时间对齐，开放域设置中检索能力有待提升。", "conclusion": "TLQA基准为未来研究提供了明确方向，代码和基准已开源。"}}
{"id": "2506.22241", "pdf": "https://arxiv.org/pdf/2506.22241", "abs": "https://arxiv.org/abs/2506.22241", "authors": ["Matthias Tschöpe", "Vitor Fortes Rey", "Sogo Pierre Sanon", "Paul Lukowicz", "Nikolaos Palaiodimopoulos", "Maximilian Kiefer-Emmanouilidis"], "title": "Boosting Classification with Quantum-Inspired Augmentations", "categories": ["cs.CV", "cond-mat.dis-nn", "cs.LG", "quant-ph"], "comment": null, "summary": "Understanding the impact of small quantum gate perturbations, which are\ncommon in quantum digital devices but absent in classical computers, is crucial\nfor identifying potential advantages in quantum machine learning. While these\nperturbations are typically seen as detrimental to quantum computation, they\ncan actually enhance performance by serving as a natural source of data\naugmentation. Additionally, they can often be efficiently simulated on\nclassical hardware, enabling quantum-inspired approaches to improve classical\nmachine learning methods. In this paper, we investigate random Bloch sphere\nrotations, which are fundamental SU(2) transformations, as a simple yet\neffective quantum-inspired data augmentation technique. Unlike conventional\naugmentations such as flipping, rotating, or cropping, quantum transformations\nlack intuitive spatial interpretations, making their application to tasks like\nimage classification less straightforward. While common quantum augmentation\nmethods rely on applying quantum models or trainable quanvolutional layers to\nclassical datasets, we focus on the direct application of small-angle Bloch\nrotations and their effect on classical data. Using the large-scale ImageNet\ndataset, we demonstrate that our quantum-inspired augmentation method improves\nimage classification performance, increasing Top-1 accuracy by 3%, Top-5\naccuracy by 2.5%, and the F$_1$ score from 8% to 12% compared to standard\nclassical augmentation methods. Finally, we examine the use of stronger unitary\naugmentations. Although these transformations preserve information in\nprinciple, they result in visually unrecognizable images with potential\napplications for privacy computations. However, we show that our augmentation\napproach and simple SU(2) transformations do not enhance differential privacy\nand discuss the implications of this limitation.", "AI": {"tldr": "量子门扰动作为数据增强技术，提升经典机器学习性能，但未增强差分隐私。", "motivation": "研究量子扰动在量子机器学习中的潜在优势，探索其作为数据增强技术的效果。", "method": "采用随机Bloch球旋转作为量子启发的数据增强技术，应用于ImageNet数据集。", "result": "Top-1准确率提高3%，Top-5准确率提高2.5%，F1分数从8%提升至12%。", "conclusion": "量子扰动增强性能但未改善差分隐私，需进一步研究其局限性。"}}
{"id": "2506.21628", "pdf": "https://arxiv.org/pdf/2506.21628", "abs": "https://arxiv.org/abs/2506.21628", "authors": ["Magnus Dierking", "Christopher E. Mower", "Sarthak Das", "Huang Helong", "Jiacheng Qiu", "Cody Reading", "Wei Chen", "Huidong Liang", "Huang Guowei", "Jan Peters", "Quan Xingyue", "Jun Wang", "Haitham Bou-Ammar"], "title": "Ark: An Open-source Python-based Framework for Robot Learning", "categories": ["cs.RO", "cs.AI", "cs.LG"], "comment": null, "summary": "Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics\nChallenges to the first humanoid-robot kickboxing tournament-yet commercial\nautonomy still lags behind progress in machine learning. A major bottleneck is\nsoftware: current robot stacks demand steep learning curves, low-level C/C++\nexpertise, fragmented tooling, and intricate hardware integration, in stark\ncontrast to the Python-centric, well-documented ecosystems that propelled\nmodern AI. We introduce ARK, an open-source, Python-first robotics framework\ndesigned to close that gap. ARK presents a Gym-style environment interface that\nallows users to collect data, preprocess it, and train policies using\nstate-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy)\nwhile seamlessly toggling between high-fidelity simulation and physical robots.\nA lightweight client-server architecture provides networked\npublisher-subscriber communication, and optional C/C++ bindings ensure\nreal-time performance when needed. ARK ships with reusable modules for control,\nSLAM, motion planning, system identification, and visualization, along with\nnative ROS interoperability. Comprehensive documentation and case studies-from\nmanipulation to mobile navigation-demonstrate rapid prototyping, effortless\nhardware swapping, and end-to-end pipelines that rival the convenience of\nmainstream machine-learning workflows. By unifying robotics and AI practices\nunder a common Python umbrella, ARK lowers entry barriers and accelerates\nresearch and commercial deployment of autonomous robots.", "AI": {"tldr": "ARK是一个开源的、以Python为中心的机器人框架，旨在简化机器人软件开发，降低学习门槛，并加速自主机器人的研究和商业部署。", "motivation": "当前机器人软件栈学习曲线陡峭，工具分散，硬件集成复杂，与Python为中心的现代AI生态系统形成鲜明对比。ARK旨在填补这一差距。", "method": "ARK提供Gym风格的环境接口，支持数据收集、预处理和策略训练，采用模仿学习算法，并支持仿真与物理机器人的无缝切换。其轻量级客户端-服务器架构提供网络通信，并可选C/C++绑定以确保实时性能。", "result": "ARK包含可重用模块（如控制、SLAM、运动规划等），支持ROS互操作性，并通过案例研究展示了快速原型设计和端到端流程的便利性。", "conclusion": "ARK通过统一Python生态系统下的机器人学和AI实践，降低了入门门槛，加速了自主机器人的研究和商业部署。"}}
{"id": "2506.21795", "pdf": "https://arxiv.org/pdf/2506.21795", "abs": "https://arxiv.org/abs/2506.21795", "authors": ["Reem Alothman", "Hafida Benhidour", "Said Kerrache"], "title": "Offensive Language Detection on Social Media Using XLNet", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "The widespread use of text-based communication on social media-through chats,\ncomments, and microblogs-has improved user interaction but has also led to an\nincrease in offensive content, including hate speech, racism, and other forms\nof abuse. Due to the enormous volume of user-generated content, manual\nmoderation is impractical, which creates a need for automated systems that can\ndetect offensive language. Deep learning models, particularly those using\ntransfer learning, have demonstrated significant success in understanding\nnatural language through large-scale pretraining. In this study, we propose an\nautomatic offensive language detection model based on XLNet, a generalized\nautoregressive pretraining method, and compare its performance with BERT\n(Bidirectional Encoder Representations from Transformers), which is a widely\nused baseline in natural language processing (NLP). Both models are evaluated\nusing the Offensive Language Identification Dataset (OLID), a benchmark Twitter\ndataset that includes hierarchical annotations. Our experimental results show\nthat XLNet outperforms BERT in detecting offensive content and in categorizing\nthe types of offenses, while BERT performs slightly better in identifying the\ntargets of the offenses. Additionally, we find that oversampling and\nundersampling strategies are effective in addressing class imbalance and\nimproving classification performance. These findings highlight the potential of\ntransfer learning and XLNet-based architectures to create robust systems for\ndetecting offensive language on social media platforms.", "AI": {"tldr": "本文提出了一种基于XLNet的自动检测社交媒体上攻击性语言的模型，并与BERT进行了性能比较。实验结果表明，XLNet在检测攻击性内容和分类攻击类型上优于BERT，而BERT在识别攻击目标上略胜一筹。此外，过采样和欠采样策略有效解决了类别不平衡问题。", "motivation": "社交媒体上攻击性内容（如仇恨言论和种族歧视）的增加需要自动化检测系统，因为人工审核不切实际。", "method": "使用XLNet和BERT模型，并在Offensive Language Identification Dataset (OLID)上进行评估，同时采用过采样和欠采样策略处理类别不平衡。", "result": "XLNet在检测攻击性内容和分类攻击类型上优于BERT，BERT在识别攻击目标上表现更好。过采样和欠采样策略提高了分类性能。", "conclusion": "XLNet和迁移学习架构在构建强大的社交媒体攻击性语言检测系统方面具有潜力。"}}
{"id": "2506.22242", "pdf": "https://arxiv.org/pdf/2506.22242", "abs": "https://arxiv.org/abs/2506.22242", "authors": ["Jiahui Zhang", "Yurui Chen", "Yueming Xu", "Ze Huang", "Yanpeng Zhou", "Yu-Jie Yuan", "Xinyue Cai", "Guowei Huang", "Xingyue Quan", "Hang Xu", "Li Zhang"], "title": "4D-VLA: Spatiotemporal Vision-Language-Action Pretraining with Cross-Scene Calibration", "categories": ["cs.CV"], "comment": null, "summary": "Leveraging diverse robotic data for pretraining remains a critical challenge.\nExisting methods typically model the dataset's action distribution using simple\nobservations as inputs. However, these inputs are often incomplete, resulting\nin a dispersed conditional action distribution-an issue we refer to as\ncoordinate system chaos and state chaos. This inconsistency significantly\nhampers pretraining efficiency. To address this, we propose 4D-VLA, a novel\napproach that effectively integrates 4D information into the input to mitigate\nthese sources of chaos. Our model introduces depth and temporal information\ninto visual features with sequential RGB-D inputs, aligning the coordinate\nsystems of the robot and the scene. This alignment endows the model with strong\nspatiotemporal reasoning capabilities while minimizing training overhead.\nAdditionally, we introduce memory bank sampling, a frame sampling strategy\ndesigned to extract informative frames from historical images, further\nimproving effectiveness and efficiency. Experimental results demonstrate that\nour pretraining method and architectural components substantially enhance model\nperformance. In both simulated and real-world experiments, our model achieves a\nsignificant increase in success rate over OpenVLA. To further assess spatial\nperception and generalization to novel views, we introduce MV-Bench, a\nmulti-view simulation benchmark. Our model consistently outperforms existing\nmethods, demonstrating stronger spatial understanding and adaptability.", "AI": {"tldr": "论文提出4D-VLA方法，通过整合4D信息解决机器人数据预训练中的坐标系混乱和状态混乱问题，显著提升性能。", "motivation": "现有方法因输入不完整导致条件动作分布分散，影响预训练效率。", "method": "引入深度和时序信息，结合RGB-D输入对齐坐标系，并提出记忆库采样策略。", "result": "在仿真和真实实验中，模型性能显著提升，优于OpenVLA，并在多视角基准测试中表现优异。", "conclusion": "4D-VLA有效解决了预训练中的混乱问题，提升了模型的时空推理能力和适应性。"}}
{"id": "2506.21635", "pdf": "https://arxiv.org/pdf/2506.21635", "abs": "https://arxiv.org/abs/2506.21635", "authors": ["Haiping Yang", "Huaxing Liu", "Wei Wu", "Zuohui Chen", "Ning Wu"], "title": "AeroLite-MDNet: Lightweight Multi-task Deviation Detection Network for UAV Landing", "categories": ["cs.RO", "cs.AI", "cs.CV"], "comment": null, "summary": "Unmanned aerial vehicles (UAVs) are increasingly employed in diverse\napplications such as land surveying, material transport, and environmental\nmonitoring. Following missions like data collection or inspection, UAVs must\nland safely at docking stations for storage or recharging, which is an\nessential requirement for ensuring operational continuity. However, accurate\nlanding remains challenging due to factors like GPS signal interference. To\naddress this issue, we propose a deviation warning system for UAV landings,\npowered by a novel vision-based model called AeroLite-MDNet. This model\nintegrates a multiscale fusion module for robust cross-scale object detection\nand incorporates a segmentation branch for efficient orientation estimation. We\nintroduce a new evaluation metric, Average Warning Delay (AWD), to quantify the\nsystem's sensitivity to landing deviations. Furthermore, we contribute a new\ndataset, UAVLandData, which captures real-world landing deviation scenarios to\nsupport training and evaluation. Experimental results show that our system\nachieves an AWD of 0.7 seconds with a deviation detection accuracy of 98.6\\%,\ndemonstrating its effectiveness in enhancing UAV landing reliability. Code will\nbe available at https://github.com/ITTTTTI/Maskyolo.git", "AI": {"tldr": "论文提出了一种基于视觉的无人机着陆偏差预警系统AeroLite-MDNet，通过多尺度融合模块和分割分支提高检测精度，并引入新评估指标AWD和新数据集UAVLandData。实验显示系统性能优异。", "motivation": "无人机着陆时因GPS信号干扰等问题难以精准降落，需一种可靠的方法提升着陆安全性。", "method": "提出AeroLite-MDNet模型，结合多尺度融合模块和分割分支，用于检测着陆偏差和估计方向。", "result": "系统平均预警延迟0.7秒，偏差检测准确率98.6%，表现优异。", "conclusion": "AeroLite-MDNet系统显著提升无人机着陆可靠性，代码和数据集将开源。"}}
{"id": "2506.21808", "pdf": "https://arxiv.org/pdf/2506.21808", "abs": "https://arxiv.org/abs/2506.21808", "authors": ["Jonathan St-Onge", "Ashley M. A. Fehr", "Carter Ward", "Calla G. Beauregard", "Michael V. Arnold", "Samuel F. Rosenblatt", "Benjamin Cooley", "Christopher M. Danforth", "Peter Sheridan Dodds"], "title": "A suite of allotaxonometric tools for the comparison of complex systems using rank-turbulence divergence", "categories": ["cs.CL"], "comment": "4 pages, 2 figures", "summary": "Describing and comparing complex systems requires principled, theoretically\ngrounded tools. Built around the phenomenon of type turbulence,\nallotaxonographs provide map-and-list visual comparisons of pairs of\nheavy-tailed distributions. Allotaxonographs are designed to accommodate a wide\nrange of instruments including rank- and probability-turbulence divergences,\nJenson-Shannon divergence, and generalized entropy divergences. Here, we\ndescribe a suite of programmatic tools for rendering allotaxonographs for\nrank-turbulence divergence in Matlab, Javascript, and Python, all of which have\ndifferent use cases.", "AI": {"tldr": "论文介绍了一种名为allotaxonographs的工具，用于可视化比较重尾分布，支持多种度量方法，并提供了Matlab、Javascript和Python的实现。", "motivation": "描述和比较复杂系统需要理论支持的工具，allotaxonographs旨在解决这一问题。", "method": "围绕类型湍流现象，设计了一种可视化工具，支持多种度量方法（如rank-turbulence divergence、Jenson-Shannon divergence等）。", "result": "提供了Matlab、Javascript和Python的实现工具，适用于不同场景。", "conclusion": "allotaxonographs为复杂系统的比较提供了灵活且理论支持的工具。"}}
{"id": "2506.22246", "pdf": "https://arxiv.org/pdf/2506.22246", "abs": "https://arxiv.org/abs/2506.22246", "authors": ["Yu-Cheng Lin", "Yu-Syuan Xu", "Hao-Wei Chen", "Hsien-Kai Kuo", "Chun-Yi Lee"], "title": "EAMamba: Efficient All-Around Vision State Space Model for Image Restoration", "categories": ["cs.CV"], "comment": "ICCV 2025", "summary": "Image restoration is a key task in low-level computer vision that aims to\nreconstruct high-quality images from degraded inputs. The emergence of Vision\nMamba, which draws inspiration from the advanced state space model Mamba, marks\na significant advancement in this field. Vision Mamba demonstrates excellence\nin modeling long-range dependencies with linear complexity, a crucial advantage\nfor image restoration tasks. Despite its strengths, Vision Mamba encounters\nchallenges in low-level vision tasks, including computational complexity that\nscales with the number of scanning sequences and local pixel forgetting. To\naddress these limitations, this study introduces Efficient All-Around Mamba\n(EAMamba), an enhanced framework that incorporates a Multi-Head Selective Scan\nModule (MHSSM) with an all-around scanning mechanism. MHSSM efficiently\naggregates multiple scanning sequences, which avoids increases in computational\ncomplexity and parameter count. The all-around scanning strategy implements\nmultiple patterns to capture holistic information and resolves the local pixel\nforgetting issue. Our experimental evaluations validate these innovations\nacross several restoration tasks, including super resolution, denoising,\ndeblurring, and dehazing. The results validate that EAMamba achieves a\nsignificant 31-89% reduction in FLOPs while maintaining favorable performance\ncompared to existing low-level Vision Mamba methods.", "AI": {"tldr": "EAMamba框架通过多头部选择性扫描模块和全方位扫描机制，解决了Vision Mamba在图像修复任务中的计算复杂性和局部像素遗忘问题，显著降低了计算成本。", "motivation": "Vision Mamba在图像修复任务中表现出色，但存在计算复杂度高和局部像素遗忘的问题，需要改进。", "method": "提出EAMamba框架，引入多头部选择性扫描模块（MHSSM）和全方位扫描策略，以高效聚合扫描序列并捕获全局信息。", "result": "实验表明，EAMamba在多项修复任务中显著降低FLOPs（31-89%），同时保持良好性能。", "conclusion": "EAMamba通过创新设计解决了Vision Mamba的局限性，为图像修复任务提供了高效且性能优越的解决方案。"}}
{"id": "2506.21638", "pdf": "https://arxiv.org/pdf/2506.21638", "abs": "https://arxiv.org/abs/2506.21638", "authors": ["Tao Feng", "Zhigang Hua", "Zijie Lei", "Yan Xie", "Shuang Yang", "Bo Long", "Jiaxuan You"], "title": "IRanker: Towards Ranking Foundation Model", "categories": ["cs.IR", "cs.AI", "cs.LG"], "comment": null, "summary": "Ranking tasks are ubiquitous, encompassing applications such as\nrecommendation systems, LLM routing, and item re-ranking. We propose to unify\nthese tasks using a single ranking foundation model (FM), as it eliminates the\nneed for designing different models for each specific ranking task. However,\nunlike general supervision tasks in LLMs, ranking tasks do not have clear\nlabels for supervision, posing great challenges to developing a ranking FM. To\novercome these challenges, we propose IRanker, a ranking FM framework with\nreinforcement learning (RL) and iterative decoding. Our insight is to decompose\nthe complex ranking task into an iterative decoding process that eliminates the\nworst candidate from the candidate pool step by step, which significantly\nreduces the output combinatorial space and better utilizes the limited context\nlength during RL training. We meticulously train and comprehensively evaluate\nan IRanker-3B model on nine datasets across three scenarios: recommendation,\nrouting, and passage ranking. The results show that a single IRanker-3B\nachieves state-of-the-art results on several datasets compared to models of\nsimilar size, and even surpasses the performance of larger models on certain\ndatasets. We further demonstrate the effectiveness of our RL design and the\nrobustness of the iterative mechanism across different LLM sizes. Moreover, we\nconducted both in-domain and out-of-domain zero-shot generalization\nexperiments, which showed that IRanker-3B achieved good generalization on\nin-domain ranking tasks compared to the base LLM by at least 5% improvement.\nSurprisingly, on out-of-domain generic LLM tasks, IRanker-3B outperformed the\nbase model by at least 9% on GSM8K, IFEval, and MathQA. In addition, the\nthoughts generated by IRanker-3B during training could further enhance\nzero-shot LLM performance.", "AI": {"tldr": "提出IRanker，一种基于强化学习和迭代解码的排名基础模型框架，统一多种排名任务，并在多个数据集上取得优异表现。", "motivation": "排名任务广泛存在，但缺乏统一的模型。现有方法需要为不同任务设计不同模型，且缺乏明确的监督标签。", "method": "通过强化学习和迭代解码，逐步消除候选池中最差选项，减少输出组合空间并优化上下文长度利用。", "result": "IRanker-3B在多个数据集上表现优异，甚至超越更大模型，同时在零样本任务中显著提升性能。", "conclusion": "IRanker框架有效统一排名任务，其设计和迭代机制在不同规模模型中均表现出色，并具备良好的泛化能力。"}}
{"id": "2506.21812", "pdf": "https://arxiv.org/pdf/2506.21812", "abs": "https://arxiv.org/abs/2506.21812", "authors": ["Avash Palikhe", "Zhenyu Yu", "Zichong Wang", "Wenbin Zhang"], "title": "Towards Transparent AI: A Survey on Explainable Large Language Models", "categories": ["cs.CL", "cs.CV"], "comment": null, "summary": "Large Language Models (LLMs) have played a pivotal role in advancing\nArtificial Intelligence (AI). However, despite their achievements, LLMs often\nstruggle to explain their decision-making processes, making them a 'black box'\nand presenting a substantial challenge to explainability. This lack of\ntransparency poses a significant obstacle to the adoption of LLMs in\nhigh-stakes domain applications, where interpretability is particularly\nessential. To overcome these limitations, researchers have developed various\nexplainable artificial intelligence (XAI) methods that provide\nhuman-interpretable explanations for LLMs. However, a systematic understanding\nof these methods remains limited. To address this gap, this survey provides a\ncomprehensive review of explainability techniques by categorizing XAI methods\nbased on the underlying transformer architectures of LLMs: encoder-only,\ndecoder-only, and encoder-decoder models. Then these techniques are examined in\nterms of their evaluation for assessing explainability, and the survey further\nexplores how these explanations are leveraged in practical applications.\nFinally, it discusses available resources, ongoing research challenges, and\nfuture directions, aiming to guide continued efforts toward developing\ntransparent and responsible LLMs.", "AI": {"tldr": "该论文综述了针对大型语言模型（LLMs）的可解释人工智能（XAI）方法，分类并评估了不同Transformer架构下的技术，探讨了实际应用、资源及未来研究方向。", "motivation": "LLMs的决策过程缺乏透明性，阻碍了其在关键领域的应用，因此需要系统化的XAI方法。", "method": "通过分类LLMs的Transformer架构（仅编码器、仅解码器、编码器-解码器），综述并评估XAI技术。", "result": "提供了XAI技术的全面分类与评估，并探讨了实际应用中的解释利用。", "conclusion": "论文总结了现有资源与研究挑战，为开发透明、负责任的LLMs提供了未来方向。"}}
{"id": "2506.22274", "pdf": "https://arxiv.org/pdf/2506.22274", "abs": "https://arxiv.org/abs/2506.22274", "authors": ["Filippo Merlo", "Ece Takmaz", "Wenkai Chen", "Albert Gatt"], "title": "COOCO -- Common Objects Out-of-Context -- Semantic Violation in Scenes: Investigating Multimodal Context in Referential Communication", "categories": ["cs.CV", "cs.CL"], "comment": null, "summary": "Natural scenes provide us with rich contexts for object recognition and\nreference. In particular, knowing what type of scene one is looking at\ngenerates expectations about which objects will occur, and what their spatial\nconfiguration should be. Do Vision-Language Models (VLMs) learn to rely on\nscene contexts in a similar way, when generating references to objects? To\naddress this question, we introduce the \\textit{Common Objects Out-of-Context\n(COOCO)} dataset and test to what extent VLMs rely on scene context to refer to\nobjects under different degrees of scene-object congruency, and different\nperturbations. Our findings show that models leverage scene context adaptively,\ndepending on both the semantic relatedness between object and scene and the\nlevel of noise. In particular, models rely more on context under high\ntarget-scene congruence or when objects are degraded. Attention analysis\nreveals that successful object categorisation involves increased focus on the\ntarget in mid-level layers, especially under moderate noise, suggesting that\nVLMs dynamically balance local and contextual information for reference\ngeneration. We make our dataset, code and models available at\n\\href{https://github.com/cs-nlp-uu/scenereg}{https://github.com/cs-nlp-uu/scenereg}.", "AI": {"tldr": "论文研究了视觉语言模型（VLMs）是否依赖场景上下文生成对象引用，并引入COOCO数据集测试不同场景-对象一致性和扰动下的表现。发现模型会根据对象与场景的语义相关性和噪声水平动态调整对上下文的依赖。", "motivation": "探索视觉语言模型是否像人类一样依赖场景上下文来识别和引用对象。", "method": "引入COOCO数据集，测试模型在不同场景-对象一致性和扰动下的表现，并进行注意力分析。", "result": "模型在高目标-场景一致性或对象退化时更依赖上下文，且注意力分析显示中层对目标的关注增加。", "conclusion": "VLMs能动态平衡局部和上下文信息，用于对象引用生成。"}}
{"id": "2506.21655", "pdf": "https://arxiv.org/pdf/2506.21655", "abs": "https://arxiv.org/abs/2506.21655", "authors": ["Minjie Hong", "Zirun Guo", "Yan Xia", "Zehan Wang", "Ziang Zhang", "Tao Jin", "Zhou Zhao"], "title": "APO: Enhancing Reasoning Ability of MLLMs via Asymmetric Policy Optimization", "categories": ["cs.LG", "cs.AI", "cs.CV"], "comment": null, "summary": "Multimodal Large Language Models (MLLMs) are powerful at integrating diverse\ndata, but they often struggle with complex reasoning. While Reinforcement\nlearning (RL) can boost reasoning in LLMs, applying it to MLLMs is tricky.\nCommon issues include a drop in performance on general tasks and the generation\nof overly detailed or \"overthinking\" reasoning. Our work investigates how the\nKL penalty and overthinking affect RL training in MLLMs. We propose Asymmetric\nPolicy Optimization (APO) to address these issues, which divides the sampled\nresponses into positive and negative groups. For positive samples,\nDifficulty-Adaptive Divergence Shaping (DADS) is introduced to dynamically\nadjust the KL divergence weight based on their difficulty. This method prevents\npolicy entropy from dropping sharply, improves training stability, utilizes\nsamples better, and preserves the model's existing knowledge. For negative\nsamples, Suboptimal Trajectory Complexity Regularization (STCR) is proposed to\npenalize overly long responses. This helps mitigate overthinking and encourages\nmore concise reasoning while preserving the model's explorative capacity. We\napply our method to Qwen2.5-VL-3B, creating View-R1-3B. View-R1-3B\nsignificantly enhances reasoning capabilities, showing an average 7\\% gain over\nthe base model and outperforming larger MLLMs (7-11B) on various reasoning\nbenchmarks. Importantly, unlike other reasoning-tuned MLLMs that often degrade\non general tasks, View-R1-3B maintains consistent improvement, demonstrating\nsuperior generalization. These results highlight the effectiveness and broad\napplicability of our DADS and STCR techniques for advancing complex multimodal\nreasoning in MLLMs. The code will be made available at\nhttps://github.com/Indolent-Kawhi/View-R1.", "AI": {"tldr": "论文提出了一种名为Asymmetric Policy Optimization (APO)的方法，通过Difficulty-Adaptive Divergence Shaping (DADS)和Suboptimal Trajectory Complexity Regularization (STCR)技术，解决了MLLMs在强化学习训练中的KL惩罚和过度思考问题，显著提升了推理能力。", "motivation": "多模态大语言模型（MLLMs）在复杂推理上表现不佳，而强化学习（RL）的应用又面临性能下降和过度思考的问题。", "method": "提出APO方法，将样本分为正负两组：对正样本使用DADS动态调整KL权重；对负样本使用STCR惩罚过长响应。", "result": "在Qwen2.5-VL-3B上实现的View-R1-3B模型，推理能力提升7%，优于更大的MLLMs，且不影响通用任务性能。", "conclusion": "DADS和STCR技术有效提升了MLLMs的复杂推理能力，并具有广泛适用性。"}}
{"id": "2506.21817", "pdf": "https://arxiv.org/pdf/2506.21817", "abs": "https://arxiv.org/abs/2506.21817", "authors": ["Riley Galpin", "Bryce Anderson", "Tom S. Juzek"], "title": "Exploring the Structure of AI-Induced Language Change in Scientific English", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7; I.2.1"], "comment": "Accepted and published at FLAIRS 38. 8 pages, 4 figures, 1 table.\n  Licensed under CC BY-NC-SA 4.0", "summary": "Scientific English has undergone rapid and unprecedented changes in recent\nyears, with words such as \"delve,\" \"intricate,\" and \"crucial\" showing\nsignificant spikes in frequency since around 2022. These changes are widely\nattributed to the growing influence of Large Language Models like ChatGPT in\nthe discourse surrounding bias and misalignment. However, apart from changes in\nfrequency, the exact structure of these linguistic shifts has remained unclear.\nThe present study addresses this and investigates whether these changes involve\nthe replacement of synonyms by suddenly 'spiking words,' for example, \"crucial\"\nreplacing \"essential\" and \"key,\" or whether they reflect broader semantic and\npragmatic qualifications. To further investigate structural changes, we include\npart of speech tagging in our analysis to quantify linguistic shifts over\ngrammatical categories and differentiate between word forms, like \"potential\"\nas a noun vs. as an adjective. We systematically analyze synonym groups for\nwidely discussed 'spiking words' based on frequency trends in scientific\nabstracts from PubMed. We find that entire semantic clusters often shift\ntogether, with most or all words in a group increasing in usage. This pattern\nsuggests that changes induced by Large Language Models are primarily semantic\nand pragmatic rather than purely lexical. Notably, the adjective \"important\"\nshows a significant decline, which prompted us to systematically analyze\ndecreasing lexical items. Our analysis of \"collapsing\" words reveals a more\ncomplex picture, which is consistent with organic language change and contrasts\nwith the patterns of the abrupt spikes. These insights into the structure of\nlanguage change contribute to our understanding of how language technology\ncontinues to shape human language.", "AI": {"tldr": "研究探讨了科学英语中词汇频率的突然变化，特别是由大型语言模型（如ChatGPT）引起的语义和语用变化，而非单纯的同义词替换。", "motivation": "近年来科学英语词汇频率的显著变化，尤其是某些词汇（如'delve'、'intricate'、'crucial'）的突然增加，引发了对其背后机制的好奇，尤其是大型语言模型的影响。", "method": "通过PubMed科学摘要的频率趋势分析，结合词性标注，系统研究了'突然增加词汇'的同义词群及其语义和语用变化。", "result": "发现语义群整体变化，表明变化主要是语义和语用的；同时观察到'important'等词汇的显著减少，揭示了更复杂的语言变化模式。", "conclusion": "大型语言模型对语言的影响主要是语义和语用的，而非单纯的词汇替换，这有助于理解语言技术对人类语言的影响。"}}
{"id": "2506.22283", "pdf": "https://arxiv.org/pdf/2506.22283", "abs": "https://arxiv.org/abs/2506.22283", "authors": ["Rui Xu", "Yunke Wang", "Yong Luo", "Bo Du"], "title": "Rethinking Visual Token Reduction in LVLMs under Cross-modal Misalignment", "categories": ["cs.CV"], "comment": null, "summary": "Large Vision-Language Models (LVLMs) encode visual inputs as dense sequences\nof patch-level tokens to capture fine-grained semantics. These visual tokens\noften outnumber their textual counterparts by a large margin, leading to\nsubstantial computational overhead and limiting the scalability of LVLMs in\npractice. Previous efforts have explored visual token reduction either prior to\nor within the large language models (LLM). However, most in-LLM reduction\napproaches rely on text-conditioned interactions, implicitly assuming that\ntextual tokens can reliably capture the importance of visual tokens. In this\nwork, we revisit this assumption and reveal causal, semantic, and spatial forms\nof cross-modal misalignment. These misalignments undermine the effectiveness of\ntext-guided visual token reduction. To address this, we introduce VisionDrop, a\ntraining-free, visual-only pruning framework that selects informative visual\ntokens based on intra-modal (visual-to-visual) attention, without relying on\ntextual signals. To further suppress redundancy throughout the model hierarchy,\nwe treat the visual encoder and the LLM as a unified system and design a\nprogressive pruning pipeline. Our method performs dominant token selection and\nlightweight contextual merging at multiple stages, enabling fine-grained visual\ninformation to be retained even under aggressive token budgets. Extensive\nexperiments across diverse benchmarks show that VisionDrop achieves consistent\nimprovements over existing methods, despite requiring no additional training or\ncomplex modifications. Its simple yet effective design enables efficient\ninference while preserving strong performance across tasks.", "AI": {"tldr": "VisionDrop是一种无需训练的视觉令牌修剪框架，通过视觉内部注意力选择信息丰富的令牌，解决了跨模态对齐问题，提升了计算效率。", "motivation": "现有视觉令牌减少方法依赖文本条件，但跨模态对齐问题（因果、语义、空间）削弱了其效果。", "method": "提出VisionDrop，基于视觉内部注意力选择令牌，并在模型层次中设计渐进式修剪流程。", "result": "在多样化基准测试中表现优于现有方法，无需额外训练或复杂修改。", "conclusion": "VisionDrop简单高效，在保留任务性能的同时显著提升推理效率。"}}
{"id": "2506.21718", "pdf": "https://arxiv.org/pdf/2506.21718", "abs": "https://arxiv.org/abs/2506.21718", "authors": ["Yash Akhauri", "Bryan Lewandowski", "Cheng-Hsi Lin", "Adrian N. Reyes", "Grant C. Forbes", "Arissa Wongpanich", "Bangding Yang", "Mohamed S. Abdelfattah", "Sagi Perel", "Xingyou Song"], "title": "Performance Prediction for Large Systems via Text-to-Text Regression", "categories": ["cs.LG", "cs.AI", "cs.PF", "cs.SE", "cs.SY", "eess.SY"], "comment": "Code can be found at https://github.com/google-deepmind/regress-lm", "summary": "In many industries, predicting metric outcomes of large systems is a\nfundamental problem, driven largely by traditional tabular regression. However,\nsuch methods struggle on complex systems data in the wild such as configuration\nfiles or system logs, where feature engineering is often infeasible. We propose\ntext-to-text regression as a general, scalable alternative. For predicting\nresource efficiency on Borg, Google's massive compute cluster scheduling\nsystem, a 60M parameter encoder-decoder, trained from random initialization,\nachieves up to a near perfect 0.99 (0.9 average) rank correlation across the\nentire fleet, and 100x lower MSE than tabular approaches. The model also easily\nadapts to new tasks in only 500 few-shot examples and captures the densities of\ncomplex outcome distributions. Ablation studies highlight the importance of\nusing encoders, increasing sequence length, and the model's inherent\nuncertainty quantification. These findings pave the way for universal\nsimulators of real-world outcomes.", "AI": {"tldr": "论文提出了一种基于文本到文本回归的方法，用于预测复杂系统数据（如配置文件或系统日志）的指标结果，相比传统表格回归方法表现更优。", "motivation": "传统表格回归方法在处理复杂系统数据（如配置文件或系统日志）时效果不佳，因为特征工程难以实现。", "method": "采用60M参数的编码器-解码器模型进行文本到文本回归，从随机初始化开始训练。", "result": "在Google的Borg集群调度系统上，模型实现了接近完美的0.99（平均0.9）排名相关性，MSE比表格方法低100倍，并能轻松适应新任务。", "conclusion": "文本到文本回归是一种通用且可扩展的替代方案，为真实世界结果的通用模拟器铺平了道路。"}}
{"id": "2506.21840", "pdf": "https://arxiv.org/pdf/2506.21840", "abs": "https://arxiv.org/abs/2506.21840", "authors": ["Kourosh Shahnazari", "Mohammadali Keshtparvar", "Seyed Moein Ayyoubzadeh"], "title": "PARSI: Persian Authorship Recognition via Stylometric Integration", "categories": ["cs.CL", "cs.AI"], "comment": null, "summary": "The intricate linguistic, stylistic, and metrical aspects of Persian\nclassical poetry pose a challenge for computational authorship attribution. In\nthis work, we present a versatile framework to determine authorship among 67\nprominent poets. We employ a multi-input neural framework consisting of a\ntransformer-based language encoder complemented by features addressing the\nsemantic, stylometric, and metrical dimensions of Persian poetry. Our feature\nset encompasses 100-dimensional Word2Vec embeddings, seven stylometric\nmeasures, and categorical encodings of poetic form and meter. We compiled a\nvast corpus of 647,653 verses of the Ganjoor digital collection, validating the\ndata through strict preprocessing and author verification while preserving\npoem-level splitting to prevent overlap. This work employs verse-level\nclassification and majority and weighted voting schemes in evaluation,\nrevealing that weighted voting yields 71% accuracy. We further investigate\nthreshold-based decision filtering, allowing the model to generate highly\nconfident predictions, achieving 97% accuracy at a 0.9 threshold, though at\nlower coverage. Our work focuses on the integration of deep representational\nforms with domain-specific features for improved authorship attribution. The\nresults illustrate the potential of our approach for automated classification\nand the contribution to stylistic analysis, authorship disputes, and general\ncomputational literature research. This research will facilitate further\nresearch on multilingual author attribution, style shift, and generative\nmodeling of Persian poetry.", "AI": {"tldr": "提出了一种多输入神经网络框架，结合语义、风格和韵律特征，用于波斯古典诗歌的作者归属，准确率达71%，高置信度预测时可达97%。", "motivation": "波斯古典诗歌的语言、风格和韵律复杂性给计算作者归属带来挑战，需要一种综合方法。", "method": "使用基于Transformer的语言编码器，结合Word2Vec嵌入、风格测量和韵律编码，构建多输入神经网络框架。", "result": "加权投票方案准确率为71%，高置信度阈值（0.9）下准确率达97%，但覆盖率较低。", "conclusion": "该方法结合深度表示和领域特征，为波斯诗歌的作者归属和风格分析提供了有效工具，支持未来多语言研究和生成建模。"}}
{"id": "2506.22291", "pdf": "https://arxiv.org/pdf/2506.22291", "abs": "https://arxiv.org/abs/2506.22291", "authors": ["Mengqi Zhou", "Xipeng Wang", "Yuxi Wang", "Zhaoxiang Zhang"], "title": "RoomCraft: Controllable and Complete 3D Indoor Scene Generation", "categories": ["cs.CV", "cs.AI"], "comment": null, "summary": "Generating realistic 3D indoor scenes from user inputs remains a challenging\nproblem in computer vision and graphics, requiring careful balance of geometric\nconsistency, spatial relationships, and visual realism. While neural generation\nmethods often produce repetitive elements due to limited global spatial\nreasoning, procedural approaches can leverage constraints for controllable\ngeneration but struggle with multi-constraint scenarios. When constraints\nbecome numerous, object collisions frequently occur, forcing the removal of\nfurniture items and compromising layout completeness.\n  To address these limitations, we propose RoomCraft, a multi-stage pipeline\nthat converts real images, sketches, or text descriptions into coherent 3D\nindoor scenes. Our approach combines a scene generation pipeline with a\nconstraint-driven optimization framework. The pipeline first extracts\nhigh-level scene information from user inputs and organizes it into a\nstructured format containing room type, furniture items, and spatial relations.\nIt then constructs a spatial relationship network to represent furniture\narrangements and generates an optimized placement sequence using a\nheuristic-based depth-first search (HDFS) algorithm to ensure layout coherence.\nTo handle complex multi-constraint scenarios, we introduce a unified constraint\nrepresentation that processes both formal specifications and natural language\ninputs, enabling flexible constraint-oriented adjustments through a\ncomprehensive action space design. Additionally, we propose a Conflict-Aware\nPositioning Strategy (CAPS) that dynamically adjusts placement weights to\nminimize furniture collisions and ensure layout completeness.\n  Extensive experiments demonstrate that RoomCraft significantly outperforms\nexisting methods in generating realistic, semantically coherent, and visually\nappealing room layouts across diverse input modalities.", "AI": {"tldr": "RoomCraft是一个多阶段管道，通过结合场景生成和约束驱动优化，从用户输入生成连贯的3D室内场景，显著优于现有方法。", "motivation": "解决现有方法在生成3D室内场景时因全局空间推理不足导致的重复元素问题，以及多约束场景下的家具碰撞和布局不完整问题。", "method": "结合场景生成管道和约束驱动优化框架，提取用户输入的高层信息，构建空间关系网络，使用HDFS算法生成优化序列，并引入CAPS策略动态调整权重以减少碰撞。", "result": "RoomCraft在生成逼真、语义连贯且视觉吸引人的房间布局方面显著优于现有方法。", "conclusion": "RoomCraft通过多阶段优化和动态策略，有效解决了3D场景生成中的复杂约束和碰撞问题，提升了生成质量。"}}
{"id": "2506.21848", "pdf": "https://arxiv.org/pdf/2506.21848", "abs": "https://arxiv.org/abs/2506.21848", "authors": ["Duo Zhang", "Junyi Mo"], "title": "LinguaSynth: Heterogeneous Linguistic Signals for News Classification", "categories": ["cs.CL"], "comment": null, "summary": "Deep learning has significantly advanced NLP, but its reliance on large\nblack-box models introduces critical interpretability and computational\nefficiency concerns. This paper proposes LinguaSynth, a novel text\nclassification framework that strategically integrates five complementary\nlinguistic feature types: lexical, syntactic, entity-level, word-level\nsemantics, and document-level semantics within a transparent logistic\nregression model. Unlike transformer-based architectures, LinguaSynth maintains\ninterpretability and computational efficiency, achieving an accuracy of 84.89\npercent on the 20 Newsgroups dataset and surpassing a robust TF-IDF baseline by\n3.32 percent. Through rigorous feature interaction analysis, we show that\nsyntactic and entity-level signals provide essential disambiguation and\neffectively complement distributional semantics. LinguaSynth sets a new\nbenchmark for interpretable, resource-efficient NLP models and challenges the\nprevailing assumption that deep neural networks are necessary for\nhigh-performing text classification.", "AI": {"tldr": "LinguaSynth提出了一种透明且高效的文本分类框架，结合五种语言特征，在保持可解释性的同时，性能优于传统方法。", "motivation": "解决深度学习模型在NLP中的可解释性和计算效率问题。", "method": "整合五种语言特征（词汇、句法、实体级、词级语义和文档级语义）于逻辑回归模型中。", "result": "在20 Newsgroups数据集上达到84.89%准确率，超越TF-IDF基线3.32%。", "conclusion": "LinguaSynth为可解释且高效的NLP模型设定了新标准，挑战了深度神经网络在文本分类中的必要性。"}}
{"id": "2506.22298", "pdf": "https://arxiv.org/pdf/2506.22298", "abs": "https://arxiv.org/abs/2506.22298", "authors": ["Linhao Zhong", "Fan Li", "Yi Huang", "Jianzhuang Liu", "Renjing Pei", "Fenglong Song"], "title": "OutDreamer: Video Outpainting with a Diffusion Transformer", "categories": ["cs.CV"], "comment": null, "summary": "Video outpainting is a challenging task that generates new video content by\nextending beyond the boundaries of an original input video, requiring both\ntemporal and spatial consistency. Many state-of-the-art methods utilize latent\ndiffusion models with U-Net backbones but still struggle to achieve high\nquality and adaptability in generated content. Diffusion transformers (DiTs)\nhave emerged as a promising alternative because of their superior performance.\nWe introduce OutDreamer, a DiT-based video outpainting framework comprising two\nmain components: an efficient video control branch and a conditional\noutpainting branch. The efficient video control branch effectively extracts\nmasked video information, while the conditional outpainting branch generates\nmissing content based on these extracted conditions. Additionally, we propose a\nmask-driven self-attention layer that dynamically integrates the given mask\ninformation, further enhancing the model's adaptability to outpainting tasks.\nFurthermore, we introduce a latent alignment loss to maintain overall\nconsistency both within and between frames. For long video outpainting, we\nemploy a cross-video-clip refiner to iteratively generate missing content,\nensuring temporal consistency across video clips. Extensive evaluations\ndemonstrate that our zero-shot OutDreamer outperforms state-of-the-art\nzero-shot methods on widely recognized benchmarks.", "AI": {"tldr": "OutDreamer是一种基于扩散变换器（DiT）的视频外绘框架，通过高效视频控制分支和条件外绘分支实现高质量视频内容生成，并在零样本任务中优于现有方法。", "motivation": "视频外绘任务需要同时满足时间和空间一致性，现有方法在生成质量和适应性上仍有不足。", "method": "提出OutDreamer框架，包含视频控制分支和条件外绘分支，并引入掩码驱动自注意力层和潜在对齐损失。", "result": "在广泛认可的基准测试中，OutDreamer在零样本任务中优于现有方法。", "conclusion": "OutDreamer通过创新的框架设计和损失函数，显著提升了视频外绘的质量和适应性。"}}
{"id": "2506.21727", "pdf": "https://arxiv.org/pdf/2506.21727", "abs": "https://arxiv.org/abs/2506.21727", "authors": ["Yasushi Kawase", "Bodhayan Roy", "Mohammad Azharuddin Sanpui"], "title": "Simultaneously Fair Allocation of Indivisible Items Across Multiple Dimensions", "categories": ["cs.GT", "cs.AI"], "comment": null, "summary": "This paper explores the fair allocation of indivisible items in a\nmultidimensional setting, motivated by the need to address fairness in complex\nenvironments where agents assess bundles according to multiple criteria. Such\nmultidimensional settings are not merely of theoretical interest but are\ncentral to many real-world applications. For example, cloud computing resources\nare evaluated based on multiple criteria such as CPU cores, memory, and network\nbandwidth. In such cases, traditional one dimensional fairness notions fail to\ncapture fairness across multiple attributes. To address these challenges, we\nstudy two relaxed variants of envy-freeness: weak simultaneously envy-free up\nto c goods (weak sEFc) and strong simultaneously envy-free up to c goods\n(strong sEFc), which accommodate the multidimensionality of agents'\npreferences. Under the weak notion, for every pair of agents and for each\ndimension, any perceived envy can be eliminated by removing, if necessary, a\ndifferent set of goods from the envied agent's allocation. In contrast, the\nstrong version requires selecting a single set of goods whose removal from the\nenvied bundle simultaneously eliminates envy in every dimension. We provide\nupper and lower bounds on the relaxation parameter c that guarantee the\nexistence of weak or strong sEFc allocations, where these bounds are\nindependent of the total number of items. In addition, we present algorithms\nfor checking whether a weak or strong sEFc allocation exists. Moreover, we\nestablish NP-hardness results for checking the existence of weak sEF1 and\nstrong sEF1 allocations.", "AI": {"tldr": "本文研究了多维环境下不可分割物品的公平分配问题，提出了两种松弛的嫉妒自由变体（弱sEFc和强sEFc），并提供了相关存在性证明和算法。", "motivation": "传统的一维公平概念无法在多维环境中有效衡量公平性，例如云计算资源分配中需考虑CPU、内存等多维属性。", "method": "提出了弱sEFc和强sEFc两种松弛的嫉妒自由概念，并分析了其存在性边界和算法实现。", "result": "给出了弱和强sEFc分配的存在性边界，并证明了弱sEF1和强sEF1分配的NP难问题。", "conclusion": "多维公平分配问题具有实际意义，提出的松弛嫉妒自由概念为解决复杂环境下的公平分配提供了理论支持。"}}
{"id": "2506.21849", "pdf": "https://arxiv.org/pdf/2506.21849", "abs": "https://arxiv.org/abs/2506.21849", "authors": ["Quan Xiao", "Debarun Bhattacharjya", "Balaji Ganesan", "Radu Marinescu", "Katsiaryna Mirylenka", "Nhan H Pham", "Michael Glass", "Junkyu Lee"], "title": "The Consistency Hypothesis in Uncertainty Quantification for Large Language Models", "categories": ["cs.CL", "cs.AI", "cs.LG"], "comment": "Accepted by The Conference on Uncertainty in Artificial Intelligence\n  (UAI) 2025", "summary": "Estimating the confidence of large language model (LLM) outputs is essential\nfor real-world applications requiring high user trust. Black-box uncertainty\nquantification (UQ) methods, relying solely on model API access, have gained\npopularity due to their practical benefits. In this paper, we examine the\nimplicit assumption behind several UQ methods, which use generation consistency\nas a proxy for confidence, an idea we formalize as the consistency hypothesis.\nWe introduce three mathematical statements with corresponding statistical tests\nto capture variations of this hypothesis and metrics to evaluate LLM output\nconformity across tasks. Our empirical investigation, spanning 8 benchmark\ndatasets and 3 tasks (question answering, text summarization, and text-to-SQL),\nhighlights the prevalence of the hypothesis under different settings. Among the\nstatements, we highlight the `Sim-Any' hypothesis as the most actionable, and\ndemonstrate how it can be leveraged by proposing data-free black-box UQ methods\nthat aggregate similarities between generations for confidence estimation.\nThese approaches can outperform the closest baselines, showcasing the practical\nvalue of the empirically observed consistency hypothesis.", "AI": {"tldr": "论文研究了大型语言模型（LLM）输出的置信度估计，提出了基于生成一致性的假设（一致性假设），并验证了其有效性。", "motivation": "在需要高用户信任的实际应用中，估计LLM输出的置信度至关重要。黑盒不确定性量化（UQ）方法因其实际优势而流行，但其背后的假设需要验证。", "method": "提出了三个数学陈述及统计测试，验证生成一致性作为置信度代理的假设，并在多个任务和数据集上进行了实证研究。", "result": "研究发现一致性假设在不同任务中普遍成立，并提出了基于生成相似性的黑盒UQ方法，性能优于基线。", "conclusion": "一致性假设具有实际价值，可用于改进LLM输出的置信度估计方法。"}}
{"id": "2506.22336", "pdf": "https://arxiv.org/pdf/2506.22336", "abs": "https://arxiv.org/abs/2506.22336", "authors": ["Paula Carbó Cubero", "Alberto Jaenal Gálvez", "André Mateus", "José Araújo", "Patric Jensfelt"], "title": "MatChA: Cross-Algorithm Matching with Feature Augmentation", "categories": ["cs.CV"], "comment": null, "summary": "State-of-the-art methods fail to solve visual localization in scenarios where\ndifferent devices use different sparse feature extraction algorithms to obtain\nkeypoints and their corresponding descriptors. Translating feature descriptors\nis enough to enable matching. However, performance is drastically reduced in\ncross-feature detector cases, because current solutions assume common\nkeypoints. This means that the same detector has to be used, which is rarely\nthe case in practice when different descriptors are used. The low repeatability\nof keypoints, in addition to non-discriminatory and non-distinctive\ndescriptors, make the identification of true correspondences extremely\nchallenging. We present the first method tackling this problem, which performs\nfeature descriptor augmentation targeting cross-detector feature matching, and\nthen feature translation to a latent space. We show that our method\nsignificantly improves image matching and visual localization in the\ncross-feature scenario and evaluate the proposed method on several benchmarks.", "AI": {"tldr": "提出了一种针对跨检测器特征匹配的新方法，通过特征描述符增强和潜在空间转换，显著提升了图像匹配和视觉定位的性能。", "motivation": "现有方法在跨设备使用不同稀疏特征提取算法时表现不佳，尤其是当关键点检测器不同时，性能大幅下降。", "method": "通过特征描述符增强和潜在空间转换，解决跨检测器特征匹配问题。", "result": "在多个基准测试中，该方法显著提升了图像匹配和视觉定位的性能。", "conclusion": "该方法为解决跨检测器特征匹配问题提供了有效方案，并在实际应用中表现出色。"}}
{"id": "2506.21861", "pdf": "https://arxiv.org/pdf/2506.21861", "abs": "https://arxiv.org/abs/2506.21861", "authors": ["Taiga Someya", "Ryo Yoshida", "Hitomi Yanaka", "Yohei Oseki"], "title": "Derivational Probing: Unveiling the Layer-wise Derivation of Syntactic Structures in Neural Language Models", "categories": ["cs.CL"], "comment": null, "summary": "Recent work has demonstrated that neural language models encode syntactic\nstructures in their internal representations, yet the derivations by which\nthese structures are constructed across layers remain poorly understood. In\nthis paper, we propose Derivational Probing to investigate how micro-syntactic\nstructures (e.g., subject noun phrases) and macro-syntactic structures (e.g.,\nthe relationship between the root verbs and their direct dependents) are\nconstructed as word embeddings propagate upward across layers. Our experiments\non BERT reveal a clear bottom-up derivation: micro-syntactic structures emerge\nin lower layers and are gradually integrated into a coherent macro-syntactic\nstructure in higher layers. Furthermore, a targeted evaluation on subject-verb\nnumber agreement shows that the timing of constructing macro-syntactic\nstructures is critical for downstream performance, suggesting an optimal timing\nfor integrating global syntactic information.", "AI": {"tldr": "论文提出Derivational Probing方法，研究BERT模型中微观和宏观句法结构在不同层中的构建过程，发现微观结构在低层形成，逐渐整合为宏观结构，且整合时机对性能至关重要。", "motivation": "探索神经网络语言模型中句法结构在不同层的构建过程，尤其是微观和宏观结构的整合机制。", "method": "提出Derivational Probing方法，分析BERT模型中微观和宏观句法结构在不同层的构建。", "result": "微观结构在低层形成，逐渐整合为宏观结构；宏观结构构建时机对下游任务性能有显著影响。", "conclusion": "句法结构的构建遵循自底向上的过程，且整合全局句法信息的时机是关键。"}}
{"id": "2506.22338", "pdf": "https://arxiv.org/pdf/2506.22338", "abs": "https://arxiv.org/abs/2506.22338", "authors": ["Luigi Russo", "Deodato Tapete", "Silvia Liberata Ullo", "Paolo Gamba"], "title": "A Deep Learning framework for building damage assessment using VHR SAR and geospatial data: demonstration on the 2023 Turkiye Earthquake", "categories": ["cs.CV", "cs.AI"], "comment": "13 pages, 6 figures (plus 4 author photos), and 5 tables. Submitted\n  to IEEE Journal of Selected Topics in Applied Earth Observations and Remote\n  Sensing", "summary": "Building damage identification shortly after a disaster is crucial for\nguiding emergency response and recovery efforts. Although optical satellite\nimagery is commonly used for disaster mapping, its effectiveness is often\nhampered by cloud cover or the absence of pre-event acquisitions. To overcome\nthese challenges, we introduce a novel multimodal deep learning (DL) framework\nfor detecting building damage using single-date very high resolution (VHR)\nSynthetic Aperture Radar (SAR) imagery from the Italian Space Agency (ASI)\nCOSMO SkyMed (CSK) constellation, complemented by auxiliary geospatial data.\nOur method integrates SAR image patches, OpenStreetMap (OSM) building\nfootprints, digital surface model (DSM) data, and structural and exposure\nattributes from the Global Earthquake Model (GEM) to improve detection accuracy\nand contextual interpretation. Unlike existing approaches that depend on pre\nand post event imagery, our model utilizes only post event data, facilitating\nrapid deployment in critical scenarios. The framework effectiveness is\ndemonstrated using a new dataset from the 2023 earthquake in Turkey, covering\nmultiple cities with diverse urban settings. Results highlight that\nincorporating geospatial features significantly enhances detection performance\nand generalizability to previously unseen areas. By combining SAR imagery with\ndetailed vulnerability and exposure information, our approach provides reliable\nand rapid building damage assessments without the dependency from available\npre-event data. Moreover, the automated and scalable data generation process\nensures the framework's applicability across diverse disaster-affected regions,\nunderscoring its potential to support effective disaster management and\nrecovery efforts. Code and data will be made available upon acceptance of the\npaper.", "AI": {"tldr": "提出了一种基于单日期高分辨率SAR图像和多源地理空间数据的深度学习框架，用于快速检测建筑物损坏，无需依赖灾前数据。", "motivation": "灾后快速识别建筑物损坏对应急响应和恢复至关重要，但传统光学卫星图像常受云层或缺乏灾前数据的限制。", "method": "结合SAR图像、OSM建筑轮廓、DSM数据和GEM的结构与暴露属性，构建多模态深度学习模型。", "result": "在土耳其2023年地震数据集上验证，显示结合地理空间特征显著提升了检测性能和泛化能力。", "conclusion": "该方法无需灾前数据，可快速、可靠地评估建筑物损坏，支持灾害管理与恢复。"}}
{"id": "2506.21732", "pdf": "https://arxiv.org/pdf/2506.21732", "abs": "https://arxiv.org/abs/2506.21732", "authors": ["Ameya Salvi", "Venkat Krovi"], "title": "Experimental investigation of pose informed reinforcement learning for skid-steered visual navigation", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.LG", "cs.SY", "eess.SY"], "comment": null, "summary": "Vision-based lane keeping is a topic of significant interest in the robotics\nand autonomous ground vehicles communities in various on-road and off-road\napplications. The skid-steered vehicle architecture has served as a useful\nvehicle platform for human controlled operations. However, systematic modeling,\nespecially of the skid-slip wheel terrain interactions (primarily in off-road\nsettings) has created bottlenecks for automation deployment. End-to-end\nlearning based methods such as imitation learning and deep reinforcement\nlearning, have gained prominence as a viable deployment option to counter the\nlack of accurate analytical models. However, the systematic formulation and\nsubsequent verification/validation in dynamic operation regimes (particularly\nfor skid-steered vehicles) remains a work in progress. To this end, a novel\napproach for structured formulation for learning visual navigation is proposed\nand investigated in this work. Extensive software simulations, hardware\nevaluations and ablation studies now highlight the significantly improved\nperformance of the proposed approach against contemporary literature.", "AI": {"tldr": "提出了一种基于学习的视觉导航结构化方法，显著提升了性能。", "motivation": "解决滑移转向车辆在动态操作中缺乏准确分析模型的问题。", "method": "采用端到端学习方法（如模仿学习和深度强化学习），提出结构化视觉导航方法。", "result": "通过软件模拟和硬件评估，性能显著优于现有文献。", "conclusion": "该方法为滑移转向车辆的自动化部署提供了可行方案。"}}
{"id": "2506.21864", "pdf": "https://arxiv.org/pdf/2506.21864", "abs": "https://arxiv.org/abs/2506.21864", "authors": ["Hang Shao", "Heting Gao", "Yunhang Shen", "Jiawei Chen", "Lijiang Li", "Zuwei Long", "Bo Tong", "Ke Li", "Xing Sun"], "title": "DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive Modality-Specific MoE", "categories": ["cs.CL", "cs.AI"], "comment": "Under Review", "summary": "Native multimodal large language models (MLLMs) restructure a single large\nlanguage model (LLM) into a spoken language model (SLM) capable of both speech\nand text generation. Compared to modular and aligned MLLMs, native MLLMs\npreserve richer paralinguistic features such as emotion and prosody, and\ngenerate speech responses directly within the backbone LLM rather than using a\nseparate speech decoder. This integration also results in lower response\nlatency and smoother interaction. However, native MLLMs suffer from\ncatastrophic forgetting and performance degradation because the available\npaired speech-text data is insufficient to support the pretraining of MLLMs\ncompared to the vast amount of text data required to pretrain text LLMs. To\naddress this issue, we propose DeepTalk, a framework for adaptive modality\nexpert learning based on a Mixture of Experts (MoE) architecture. DeepTalk\nfirst adaptively distinguishes modality experts according to their modality\nload within the LLM. Each modality expert then undergoes specialized\nsingle-modality training, followed by joint multimodal collaborative training.\nAs a result, DeepTalk incurs only a 5.5% performance drop compared to the\noriginal LLM, which is significantly lower than the average performance drop of\nover 20% typically seen in native MLLMs (such as GLM-4-Voice), and is on par\nwith modular MLLMs. Meanwhile, the end-to-end dialogue latency remains within\n0.5 seconds, ensuring a seamless and intelligent speech interaction experience.\nCode and models are released at https://github.com/talkking/DeepTalk.", "AI": {"tldr": "DeepTalk提出了一种基于MoE架构的自适应模态专家学习框架，解决了原生MLLMs因数据不足导致的性能下降问题，性能损失仅5.5%，对话延迟低于0.5秒。", "motivation": "原生MLLMs虽然能保留丰富的副语言特征并降低延迟，但因配对语音-文本数据不足导致性能下降和灾难性遗忘。", "method": "DeepTalk通过MoE架构自适应区分模态专家，分别进行单模态训练和多模态协作训练。", "result": "性能损失仅5.5%，显著优于原生MLLMs的20%下降，对话延迟低于0.5秒。", "conclusion": "DeepTalk在性能和延迟上表现优异，为智能语音交互提供了高效解决方案。"}}
{"id": "2506.22347", "pdf": "https://arxiv.org/pdf/2506.22347", "abs": "https://arxiv.org/abs/2506.22347", "authors": ["Hans Geißner", "Christian Rathgeb"], "title": "Closing the Performance Gap in Biometric Cryptosystems: A Deeper Analysis on Unlinkable Fuzzy Vaults", "categories": ["cs.CV"], "comment": "10 pages, 4 figures, 4 tables", "summary": "This paper analyses and addresses the performance gap in the fuzzy\nvault-based \\ac{BCS}. We identify unstable error correction capabilities, which\nare caused by variable feature set sizes and their influence on similarity\nthresholds, as a key source of performance degradation. This issue is further\ncompounded by information loss introduced through feature type transformations.\nTo address both problems, we propose a novel feature quantization method based\non \\it{equal frequent intervals}. This method guarantees fixed feature set\nsizes and supports training-free adaptation to any number of intervals. The\nproposed approach significantly reduces the performance gap introduced by\ntemplate protection. Additionally, it integrates seamlessly with existing\nsystems to minimize the negative effects of feature transformation. Experiments\non state-of-the-art face, fingerprint, and iris recognition systems confirm\nthat only minimal performance degradation remains, demonstrating the\neffectiveness of the method across major biometric modalities.", "AI": {"tldr": "本文提出了一种基于等频区间的特征量化方法，解决了模糊保险库生物识别系统中的性能下降问题。", "motivation": "模糊保险库生物识别系统因特征集大小不稳定和特征类型转换导致的信息丢失而性能下降。", "method": "提出了一种基于等频区间的特征量化方法，确保固定特征集大小并支持无训练适应。", "result": "实验表明，该方法显著减少了性能差距，并在多种生物识别系统中表现良好。", "conclusion": "该方法有效解决了性能下降问题，适用于多种生物识别模态。"}}
{"id": "2506.21875", "pdf": "https://arxiv.org/pdf/2506.21875", "abs": "https://arxiv.org/abs/2506.21875", "authors": ["Jian Zhang", "Linhao Zhang", "Bokai Lei", "Chuhan Wu", "Wei Jia", "Xiao Zhou"], "title": "WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation", "categories": ["cs.CL"], "comment": null, "summary": "Recent multi-modal Large Language Models (LLMs) such as GPT-4o have\ndemonstrated strong capabilities of direct speech interaction. However, the\nlack of specialized and comprehensive benchmarks for end-to-end speech LLM\nevaluation hinders optimizing the user experience of Audio LLMs in real-world\napplications. Existing evaluation methods often adapt text-based benchmarks,\noverlooking speech's unique characteristics and challenges, including prosody,\nhomophones, stuttering, and differing user expectations. Here, we present a\nnovel approach to thoroughly evaluate LLMs in practical speech conversations.\nWe systematically curate real-world chat data relevant to spoken scenarios,\nintroduce diversity in speaker attributes and acoustic conditions, and augment\nthe dataset with speech-specific phenomena. We further design a query-aware\nevaluation method to use customized evaluation checklists and prompts to\nenhance the accuracy of automatic evaluation. We conduct comprehensive testing\nand detailed analysis of various mainstream speech models, revealing\nsignificant differences in model performance across different speech scenarios.\nThe use of query-aware evaluation further enables a finer-grained assessment\nunder various speech-specific scenarios. Our benchmark can provide valuable\ninsights for speech model development and evaluation.", "AI": {"tldr": "提出了一种新的端到端语音LLM评估方法，通过系统整理真实对话数据、引入多样化的语音特性，并设计查询感知的评估方法，以优化语音LLM的实际表现。", "motivation": "现有评估方法多基于文本，忽略了语音的独特特性（如韵律、同音词等），无法全面评估语音LLM的实际表现。", "method": "系统整理真实语音对话数据，引入多样化的说话者属性和声学条件，并设计查询感知的评估方法。", "result": "揭示了主流语音模型在不同场景下的显著性能差异，查询感知评估提供了更细粒度的分析。", "conclusion": "该基准测试为语音模型的开发和评估提供了有价值的参考。"}}
{"id": "2506.22360", "pdf": "https://arxiv.org/pdf/2506.22360", "abs": "https://arxiv.org/abs/2506.22360", "authors": ["Nouf Almesafri", "Hector Figueiredo", "Miguel Arana-Catania"], "title": "From Ground to Air: Noise Robustness in Vision Transformers and CNNs for Event-Based Vehicle Classification with Potential UAV Applications", "categories": ["cs.CV", "cs.AI", "cs.LG"], "comment": "16 pages, 17 figures, 9 tables. To be presented in AIAA AVIATION\n  Forum 2025", "summary": "This study investigates the performance of the two most relevant computer\nvision deep learning architectures, Convolutional Neural Network and Vision\nTransformer, for event-based cameras. These cameras capture scene changes,\nunlike traditional frame-based cameras with capture static images, and are\nparticularly suited for dynamic environments such as UAVs and autonomous\nvehicles. The deep learning models studied in this work are ResNet34 and ViT\nB16, fine-tuned on the GEN1 event-based dataset. The research evaluates and\ncompares these models under both standard conditions and in the presence of\nsimulated noise. Initial evaluations on the clean GEN1 dataset reveal that\nResNet34 and ViT B16 achieve accuracies of 88% and 86%, respectively, with\nResNet34 showing a slight advantage in classification accuracy. However, the\nViT B16 model demonstrates notable robustness, particularly given its\npre-training on a smaller dataset. Although this study focuses on ground-based\nvehicle classification, the methodologies and findings hold significant promise\nfor adaptation to UAV contexts, including aerial object classification and\nevent-based vision systems for aviation-related tasks.", "AI": {"tldr": "比较了CNN（ResNet34）和ViT（ViT B16）在事件相机数据上的性能，ResNet34在准确率上略优，但ViT B16在噪声下表现更稳健。", "motivation": "研究事件相机在动态环境（如无人机和自动驾驶）中的应用潜力，比较两种主流深度学习架构的性能。", "method": "使用ResNet34和ViT B16在GEN1事件数据集上微调，评估标准条件和模拟噪声下的表现。", "result": "ResNet34和ViT B16在干净数据上的准确率分别为88%和86%，ViT B16在噪声下更稳健。", "conclusion": "ResNet34在分类准确率上略优，但ViT B16更适合噪声环境，方法可扩展至无人机相关任务。"}}
{"id": "2506.21876", "pdf": "https://arxiv.org/pdf/2506.21876", "abs": "https://arxiv.org/abs/2506.21876", "authors": ["Qiyue Gao", "Xinyu Pi", "Kevin Liu", "Junrong Chen", "Ruolan Yang", "Xinqi Huang", "Xinyu Fang", "Lu Sun", "Gautham Kishore", "Bo Ai", "Stone Tao", "Mengyang Liu", "Jiaxi Yang", "Chao-Jung Lai", "Chuanyang Jin", "Jiannan Xiang", "Benhao Huang", "Zeming Chen", "David Danks", "Hao Su", "Tianmin Shu", "Ziqiao Ma", "Lianhui Qin", "Zhiting Hu"], "title": "Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation", "categories": ["cs.CL", "cs.AI", "cs.CV"], "comment": "ACL 2025 (Findings)", "summary": "Internal world models (WMs) enable agents to understand the world's state and\npredict transitions, serving as the basis for advanced deliberative reasoning.\nRecent large Vision-Language Models (VLMs), such as OpenAI o3, GPT-4o and\nGemini, exhibit potential as general-purpose WMs. While the latest studies have\nevaluated and shown limitations in specific capabilities such as visual\nunderstanding, a systematic evaluation of VLMs' fundamental WM abilities\nremains absent. Drawing on comparative psychology and cognitive science, we\npropose a two-stage framework that assesses Perception (visual, spatial,\ntemporal, quantitative, and motion) and Prediction (mechanistic simulation,\ntransitive inference, compositional inference) to provide an atomic evaluation\nof VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale\nbenchmark comprising 23 fine-grained evaluation dimensions across 6 diverse\nsimulated environments with controlled counterfactual simulations. Through 660\nexperiments on 15 latest commercial and open-source VLMs, we find that these\nmodels exhibit striking limitations in basic world modeling abilities. For\ninstance, almost all models perform at near-random accuracy when distinguishing\nmotion trajectories. Additionally, they lack disentangled understanding --\ne.g., some models tend to believe blue objects move faster than green ones.\nMore rich results and analyses reveal significant gaps between VLMs and\nhuman-level world modeling.", "AI": {"tldr": "论文提出了一个两阶段框架（感知与预测）来系统评估视觉语言模型（VLMs）作为世界模型（WMs）的能力，发现现有模型在基本世界建模能力上存在显著局限性。", "motivation": "评估VLMs作为通用世界模型的潜力，填补系统性评估的空白。", "method": "提出两阶段框架（感知与预测），并设计大规模基准WM-ABench，涵盖23个细粒度评估维度。", "result": "现有VLMs在基本世界建模能力上表现不佳，例如运动轨迹区分准确率接近随机，且缺乏解耦理解。", "conclusion": "VLMs与世界建模的人类水平存在显著差距，需进一步改进。"}}
{"id": "2506.22375", "pdf": "https://arxiv.org/pdf/2506.22375", "abs": "https://arxiv.org/abs/2506.22375", "authors": ["Tiankai Chen", "Yushu Li", "Adam Goodge", "Fei Teng", "Xulei Yang", "Tianrui Li", "Xun Xu"], "title": "Exploiting Vision Language Model for Training-Free 3D Point Cloud OOD Detection via Graph Score Propagation", "categories": ["cs.CV"], "comment": "Accepted by ICCV 2025", "summary": "Out-of-distribution (OOD) detection in 3D point cloud data remains a\nchallenge, particularly in applications where safe and robust perception is\ncritical. While existing OOD detection methods have shown progress for 2D image\ndata, extending these to 3D environments involves unique obstacles. This paper\nintroduces a training-free framework that leverages Vision-Language Models\n(VLMs) for effective OOD detection in 3D point clouds. By constructing a graph\nbased on class prototypes and testing data, we exploit the data manifold\nstructure to enhancing the effectiveness of VLMs for 3D OOD detection. We\npropose a novel Graph Score Propagation (GSP) method that incorporates prompt\nclustering and self-training negative prompting to improve OOD scoring with\nVLM. Our method is also adaptable to few-shot scenarios, providing options for\npractical applications. We demonstrate that GSP consistently outperforms\nstate-of-the-art methods across synthetic and real-world datasets 3D point\ncloud OOD detection.", "AI": {"tldr": "本文提出了一种基于视觉语言模型（VLM）的无训练框架，用于3D点云数据的分布外（OOD）检测，通过图分数传播（GSP）方法提升检测效果。", "motivation": "3D点云数据的OOD检测在安全感知应用中具有挑战性，现有方法主要针对2D图像，难以直接扩展到3D环境。", "method": "利用VLM构建基于类原型和测试数据的图，提出GSP方法，结合提示聚类和自训练负提示优化OOD评分。", "result": "GSP在合成和真实数据集上均优于现有方法，且适用于少样本场景。", "conclusion": "该方法为3D点云OOD检测提供了高效且实用的解决方案。"}}
{"id": "2506.21788", "pdf": "https://arxiv.org/pdf/2506.21788", "abs": "https://arxiv.org/abs/2506.21788", "authors": ["Massimiliano Lupo Pasini", "Jong Youl Choi", "Pei Zhang", "Kshitij Mehta", "Rylie Weaver", "Ashwin M. Aji", "Karl W. Schulz", "Jorda Polo", "Prasanna Balaprakash"], "title": "Multi-task parallelism for robust pre-training of graph foundation models on multi-source, multi-fidelity atomistic modeling data", "categories": ["cs.LG", "cond-mat.mtrl-sci", "cs.AI", "physics.atm-clus", "68T07, 68T09", "I.2; I.2.5; I.2.11"], "comment": "15 pages, 4 figures, 2 tables", "summary": "Graph foundation models using graph neural networks promise sustainable,\nefficient atomistic modeling. To tackle challenges of processing multi-source,\nmulti-fidelity data during pre-training, recent studies employ multi-task\nlearning, in which shared message passing layers initially process input\natomistic structures regardless of source, then route them to multiple decoding\nheads that predict data-specific outputs. This approach stabilizes pre-training\nand enhances a model's transferability to unexplored chemical regions.\nPreliminary results on approximately four million structures are encouraging,\nyet questions remain about generalizability to larger, more diverse datasets\nand scalability on supercomputers. We propose a multi-task parallelism method\nthat distributes each head across computing resources with GPU acceleration.\nImplemented in the open-source HydraGNN architecture, our method was trained on\nover 24 million structures from five datasets and tested on the Perlmutter,\nAurora, and Frontier supercomputers, demonstrating efficient scaling on all\nthree highly heterogeneous super-computing architectures.", "AI": {"tldr": "论文提出了一种基于多任务并行化的图基础模型方法，通过GPU加速在多源异构超级计算机上高效扩展。", "motivation": "解决多源、多保真度数据预训练中的挑战，提升模型在未探索化学区域的泛化能力。", "method": "采用多任务学习，共享消息传递层处理输入原子结构，然后路由到多个解码头预测数据特定输出，并实现多任务并行化。", "result": "在2400万结构上训练，测试于Perlmutter、Aurora和Frontier超级计算机，展示高效扩展性。", "conclusion": "多任务并行化方法在异构超级计算机上表现优异，为大规模图基础模型提供了可行方案。"}}
{"id": "2506.21881", "pdf": "https://arxiv.org/pdf/2506.21881", "abs": "https://arxiv.org/abs/2506.21881", "authors": ["Sean Kim", "Hyuhng Joon Kim"], "title": "A Dual-Layered Evaluation of Geopolitical and Cultural Bias in LLMs", "categories": ["cs.CL"], "comment": "This paper is accepted to ACL Student Research Workshop (SRW) 2025", "summary": "As large language models (LLMs) are increasingly deployed across diverse\nlinguistic and cultural contexts, understanding their behavior in both factual\nand disputable scenarios is essential, especially when their outputs may shape\npublic opinion or reinforce dominant narratives. In this paper, we define two\ntypes of bias in LLMs: model bias (bias stemming from model training) and\ninference bias (bias induced by the language of the query), through a two-phase\nevaluation. Phase 1 evaluates LLMs on factual questions where a single\nverifiable answer exists, assessing whether models maintain consistency across\ndifferent query languages. Phase 2 expands the scope by probing geopolitically\nsensitive disputes, where responses may reflect culturally embedded or\nideologically aligned perspectives. We construct a manually curated dataset\nspanning both factual and disputable QA, across four languages and question\ntypes. The results show that Phase 1 exhibits query language induced alignment,\nwhile Phase 2 reflects an interplay between the model's training context and\nquery language. This paper offers a structured framework for evaluating LLM\nbehavior across neutral and sensitive topics, providing insights for future LLM\ndeployment and culturally aware evaluation practices in multilingual contexts.", "AI": {"tldr": "论文提出了一种评估大语言模型（LLM）在事实性和争议性场景中行为的方法，区分了模型偏见和推理偏见，并通过多语言数据集验证了其影响。", "motivation": "随着LLM在多语言和文化环境中的广泛应用，理解其在事实和争议性问题中的行为至关重要，以避免输出结果影响公众意见或强化主流叙事。", "method": "采用两阶段评估：第一阶段测试模型在事实性问题中的一致性；第二阶段探讨地缘政治敏感争议中的表现。构建了涵盖四种语言的数据集。", "result": "第一阶段显示查询语言导致的对齐；第二阶段反映模型训练背景与查询语言的交互作用。", "conclusion": "论文提供了一个结构化框架，用于评估LLM在敏感话题中的行为，为多语言环境中的模型部署和文化意识评估提供了参考。"}}
{"id": "2506.22385", "pdf": "https://arxiv.org/pdf/2506.22385", "abs": "https://arxiv.org/abs/2506.22385", "authors": ["Yue Zhang", "Jilei Sun", "Yunhui Guo", "Vibhav Gogate"], "title": "Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A Study on Defeasible Video Entailment", "categories": ["cs.CV", "cs.AI", "cs.CL"], "comment": null, "summary": "Video Large Multimodal Models (VLMMs) have made impressive strides in\nunderstanding video content, but they often struggle with abstract and adaptive\nreasoning-the ability to revise their interpretations when new information\nemerges. In reality, conclusions are rarely set in stone; additional context\ncan strengthen or weaken an initial inference. To address this, we introduce\nDefeasible Video Entailment (DVidE), a new task that challenges models to think\nlike doubters, constantly updating their reasoning based on evolving evidence.\nIn DVidE, given a video premise and a textual hypothesis, models must determine\nwhether a new update strengthens or weakens the hypothesis (classification\nversion) or generate a coherent update that modifies the entailment\nrelationship (generation version). For solving the classification task, we\npropose the Chain of Counterfactual Thought framework, utilizing counterfactual\nreasoning, ASR-enhanced video content, and rationale refinement to reduce\ninference bias. For the generation task, we develop a framework that combines\nASR output with a Large Language Model (LLM) to produce coherent, contextually\nrelevant updates aligned with the intended strengthener or weakener goals.\nAdditionally, we introduce a novel benchmark dataset, with\nstrengthener/weakener annotations and an LLM-based evaluation metric\nspecifically designed for assessing generative performance. Experimental\nresults demonstrate significant improvements, highlighting our proposed method\nin enhancing dynamic reasoning capabilities of VLMMs.", "AI": {"tldr": "论文提出了一种新任务DVidE，旨在提升视频大型多模态模型（VLMMs）的动态推理能力，通过分类和生成任务挑战模型根据新信息更新推理。", "motivation": "现有VLMMs在抽象和适应性推理方面表现不足，无法根据新信息动态调整结论。DVidE任务旨在解决这一问题。", "method": "提出Chain of Counterfactual Thought框架用于分类任务，结合反事实推理和ASR增强视频内容；生成任务则结合ASR输出和LLM生成上下文相关的更新。", "result": "实验结果表明，所提方法显著提升了VLMMs的动态推理能力。", "conclusion": "DVidE任务及相关框架有效增强了VLMMs的动态推理能力，为未来研究提供了新方向。"}}
{"id": "2506.21796", "pdf": "https://arxiv.org/pdf/2506.21796", "abs": "https://arxiv.org/abs/2506.21796", "authors": ["Dani Korpi", "Rachel Wang", "Jerry Wang", "Abdelrahman Ibrahim", "Carl Nuzman", "Runxin Wang", "Kursat Rasim Mestav", "Dustin Zhang", "Iraj Saniee", "Shawn Winston", "Gordana Pavlovic", "Wei Ding", "William J. Hillery", "Chenxi Hao", "Ram Thirunagari", "Jung Chang", "Jeehyun Kim", "Bartek Kozicki", "Dragan Samardzija", "Taesang Yoo", "Andreas Maeder", "Tingfang Ji", "Harish Viswanathan"], "title": "Demonstrating Interoperable Channel State Feedback Compression with Machine Learning", "categories": ["eess.SP", "cs.AI"], "comment": "This work has been submitted to the IEEE for possible publication", "summary": "Neural network-based compression and decompression of channel state feedback\nhas been one of the most widely studied applications of machine learning (ML)\nin wireless networks. Various simulation-based studies have shown that ML-based\nfeedback compression can result in reduced overhead and more accurate channel\ninformation. However, to the best of our knowledge, there are no real-life\nproofs of concepts demonstrating the benefits of ML-based channel feedback\ncompression in a practical setting, where the user equipment (UE) and base\nstation have no access to each others' ML models. In this paper, we present a\nnovel approach for training interoperable compression and decompression ML\nmodels in a confidential manner, and demonstrate the accuracy of the ensuing\nmodels using prototype UEs and base stations. The performance of the ML-based\nchannel feedback is measured both in terms of the accuracy of the reconstructed\nchannel information and achieved downlink throughput gains when using the\nchannel information for beamforming. The reported measurement results\ndemonstrate that it is possible to develop an accurate ML-based channel\nfeedback link without having to share ML models between device and network\nvendors. These results pave the way for a practical implementation of ML-based\nchannel feedback in commercial 6G networks.", "AI": {"tldr": "论文提出了一种新颖的ML模型训练方法，用于无线网络中信道状态反馈的压缩与解压缩，无需共享模型即可实现高精度反馈。", "motivation": "现有研究缺乏实际场景中ML模型互操作性的验证，尤其是在UE和基站无法共享模型的情况下。", "method": "开发了一种保密训练互操作压缩与解压缩ML模型的方法，并通过原型UE和基站验证其准确性。", "result": "实验表明，ML模型在不共享的情况下仍能提供高精度的信道信息重构和下行链路吞吐量增益。", "conclusion": "该方法为6G网络中ML信道反馈的实际应用铺平了道路。"}}
{"id": "2506.21910", "pdf": "https://arxiv.org/pdf/2506.21910", "abs": "https://arxiv.org/abs/2506.21910", "authors": ["Ernie Chang", "Yang Li", "Patrick Huber", "David Kant", "Yangyang Shi", "Vikas Chandra"], "title": "AutoMixer: Checkpoint Artifacts as Automatic Data Mixers", "categories": ["cs.CL"], "comment": "Accepted at ACL 2025", "summary": "In language model training, it is desirable to equip models with capabilities\nfrom various tasks. However, it is not clear how to directly obtain the right\ndata mixtures for these capabilities as the relationship between data and tasks\nis difficult to be modeled. In this work, we observe that checkpoint models\nexhibit emerging capabilities at different points in the training trajectory.\nOften, the training process saves checkpoints as artifacts that are\nunder-utilized as a source of in-training data signals. We identify these\nartifact models based on their respective capabilities on the benchmarks and\nleverage them as data mixers by using their aggregated first-order influence\napproximation over source data. We demonstrated on eight reasoning benchmarks\nthat the proposed framework shows significant improvements in the pretraining\nsetting, with performance improvements of up to 1.93%. Overall, this shows the\npotential of checkpoint models to enhance data quality and optimize data\nmixtures.", "AI": {"tldr": "论文提出利用训练过程中的检查点模型作为数据混合的信号源，通过其影响力近似优化数据混合，显著提升了预训练性能。", "motivation": "语言模型训练中，如何直接获取适合多任务能力的数据混合是一个难题，因为数据与任务的关系难以建模。", "method": "利用检查点模型的能力表现作为信号，通过其一阶影响力近似优化数据混合。", "result": "在八个推理基准测试中，性能提升高达1.93%。", "conclusion": "检查点模型具有优化数据质量和混合的潜力。"}}
{"id": "2506.22395", "pdf": "https://arxiv.org/pdf/2506.22395", "abs": "https://arxiv.org/abs/2506.22395", "authors": ["Shih-Han Chou", "Shivam Chandhok", "James J. Little", "Leonid Sigal"], "title": "Test-Time Consistency in Vision Language Models", "categories": ["cs.CV"], "comment": null, "summary": "Vision-Language Models (VLMs) have achieved impressive performance across a\nwide range of multimodal tasks, yet they often exhibit inconsistent behavior\nwhen faced with semantically equivalent inputs, undermining their reliability\nand robustness. Recent benchmarks, such as MM-R3, highlight that even\nstate-of-the-art VLMs can produce divergent predictions across semantically\nequivalent inputs, despite maintaining high average accuracy. Prior work\naddresses this issue by modifying model architectures or conducting large-scale\nfine-tuning on curated datasets. In contrast, we propose a simple and effective\ntest-time consistency framework that enhances semantic consistency without\nsupervised re-training. Our method is entirely post-hoc, model-agnostic, and\napplicable to any VLM with access to its weights. Given a single test point, we\nenforce consistent predictions via two complementary objectives: (i) a\nCross-Entropy Agreement Loss that aligns predictive distributions across\nsemantically equivalent inputs, and (ii) a Pseudo-Label Consistency Loss that\ndraws outputs toward a self-averaged consensus. Our method is plug-and-play and\nleverages information from a single test input itself to improve consistency.\nExperiments on the MM-R3 benchmark show that our framework yields substantial\ngains in consistency across state-of-the-art models, establishing a new\ndirection for inference-time adaptation in multimodal learning.", "AI": {"tldr": "提出了一种无需监督重新训练的测试时一致性框架，通过交叉熵一致性损失和伪标签一致性损失提升视觉语言模型的语义一致性。", "motivation": "当前视觉语言模型在语义等效输入下表现不一致，影响其可靠性和鲁棒性。", "method": "提出一种后处理方法，通过交叉熵一致性损失和伪标签一致性损失，利用单个测试输入自身信息提升一致性。", "result": "在MM-R3基准测试中显著提升了模型的一致性。", "conclusion": "该方法为多模态学习的推理时适应提供了新方向。"}}
{"id": "2506.21803", "pdf": "https://arxiv.org/pdf/2506.21803", "abs": "https://arxiv.org/abs/2506.21803", "authors": ["Fuying Wang", "Jiacheng Xu", "Lequan Yu"], "title": "From Token to Rhythm: A Multi-Scale Approach for ECG-Language Pretraining", "categories": ["eess.SP", "cs.AI", "cs.LG"], "comment": "ICML 2025", "summary": "Electrocardiograms (ECGs) play a vital role in monitoring cardiac health and\ndiagnosing heart diseases. However, traditional deep learning approaches for\nECG analysis rely heavily on large-scale manual annotations, which are both\ntime-consuming and resource-intensive to obtain. To overcome this limitation,\nself-supervised learning (SSL) has emerged as a promising alternative, enabling\nthe extraction of robust ECG representations that can be efficiently\ntransferred to various downstream tasks. While previous studies have explored\nSSL for ECG pretraining and multi-modal ECG-language alignment, they often fail\nto capture the multi-scale nature of ECG signals. As a result, these methods\nstruggle to learn generalized representations due to their inability to model\nthe hierarchical structure of ECG data. To address this gap, we introduce MELP,\na novel Multi-scale ECG-Language Pretraining (MELP) model that fully leverages\nhierarchical supervision from ECG-text pairs. MELP first pretrains a\ncardiology-specific language model to enhance its understanding of clinical\ntext. It then applies three levels of cross-modal supervision-at the token,\nbeat, and rhythm levels-to align ECG signals with textual reports, capturing\nstructured information across different time scales. We evaluate MELP on three\npublic ECG datasets across multiple tasks, including zero-shot ECG\nclassification, linear probing, and transfer learning. Experimental results\ndemonstrate that MELP outperforms existing SSL methods, underscoring its\neffectiveness and adaptability across diverse clinical applications. Our code\nis available at https://github.com/HKU-MedAI/MELP.", "AI": {"tldr": "MELP是一种新型的多尺度ECG-语言预训练模型，通过分层监督从ECG-文本对中提取多尺度特征，显著优于现有自监督学习方法。", "motivation": "传统ECG分析方法依赖大量人工标注，耗时耗力；现有自监督学习方法未能充分捕捉ECG信号的多尺度特性。", "method": "MELP结合心脏学特定语言模型和三层次跨模态监督（token、beat、rhythm），对齐ECG信号与文本报告。", "result": "MELP在多个ECG数据集和任务中表现优于现有方法，验证了其有效性和适应性。", "conclusion": "MELP通过多尺度监督和跨模态对齐，为ECG分析提供了高效且通用的解决方案。"}}
{"id": "2506.21961", "pdf": "https://arxiv.org/pdf/2506.21961", "abs": "https://arxiv.org/abs/2506.21961", "authors": ["Junho Myung", "Yeon Su Park", "Sunwoo Kim", "Shin Yoo", "Alice Oh"], "title": "PapersPlease: A Benchmark for Evaluating Motivational Values of Large Language Models Based on ERG Theory", "categories": ["cs.CL"], "comment": "Accepted to GEM2 Workshop: Generation, Evaluation & Metrics - ACL\n  2025", "summary": "Evaluating the performance and biases of large language models (LLMs) through\nrole-playing scenarios is becoming increasingly common, as LLMs often exhibit\nbiased behaviors in these contexts. Building on this line of research, we\nintroduce PapersPlease, a benchmark consisting of 3,700 moral dilemmas designed\nto investigate LLMs' decision-making in prioritizing various levels of human\nneeds. In our setup, LLMs act as immigration inspectors deciding whether to\napprove or deny entry based on the short narratives of people. These narratives\nare constructed using the Existence, Relatedness, and Growth (ERG) theory,\nwhich categorizes human needs into three hierarchical levels. Our analysis of\nsix LLMs reveals statistically significant patterns in decision-making,\nsuggesting that LLMs encode implicit preferences. Additionally, our evaluation\nof the impact of incorporating social identities into the narratives shows\nvarying responsiveness based on both motivational needs and identity cues, with\nsome models exhibiting higher denial rates for marginalized identities. All\ndata is publicly available at https://github.com/yeonsuuuu28/papers-please.", "AI": {"tldr": "论文介绍了PapersPlease基准，通过3700个道德困境评估大型语言模型（LLMs）在优先处理人类需求时的决策模式，发现LLMs存在隐含偏好和对社会身份的不同响应。", "motivation": "研究LLMs在角色扮演场景中的表现和偏见，尤其是其在道德困境中的决策行为。", "method": "使用基于ERG理论的3700个道德困境，让LLMs扮演移民官员，根据叙事决定是否批准入境。", "result": "LLMs表现出显著的决策模式，隐含偏好明显，且对社会身份的反应因需求和身份线索而异。", "conclusion": "LLMs在道德决策中存在偏见，对社会身份的响应差异显著，数据已公开。"}}
{"id": "2506.22432", "pdf": "https://arxiv.org/pdf/2506.22432", "abs": "https://arxiv.org/abs/2506.22432", "authors": ["Yuhao Liu", "Tengfei Wang", "Fang Liu", "Zhenwei Wang", "Rynson W. H. Lau"], "title": "Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy", "categories": ["cs.CV"], "comment": null, "summary": "Recent advances in deep generative modeling have unlocked unprecedented\nopportunities for video synthesis. In real-world applications, however, users\noften seek tools to faithfully realize their creative editing intentions with\nprecise and consistent control. Despite the progress achieved by existing\nmethods, ensuring fine-grained alignment with user intentions remains an open\nand challenging problem. In this work, we present Shape-for-Motion, a novel\nframework that incorporates a 3D proxy for precise and consistent video\nediting. Shape-for-Motion achieves this by converting the target object in the\ninput video to a time-consistent mesh, i.e., a 3D proxy, allowing edits to be\nperformed directly on the proxy and then inferred back to the video frames. To\nsimplify the editing process, we design a novel Dual-Propagation Strategy that\nallows users to perform edits on the 3D mesh of a single frame, and the edits\nare then automatically propagated to the 3D meshes of the other frames. The 3D\nmeshes for different frames are further projected onto the 2D space to produce\nthe edited geometry and texture renderings, which serve as inputs to a\ndecoupled video diffusion model for generating edited results. Our framework\nsupports various precise and physically-consistent manipulations across the\nvideo frames, including pose editing, rotation, scaling, translation, texture\nmodification, and object composition. Our approach marks a key step toward\nhigh-quality, controllable video editing workflows. Extensive experiments\ndemonstrate the superiority and effectiveness of our approach. Project page:\nhttps://shapeformotion.github.io/", "AI": {"tldr": "Shape-for-Motion框架通过3D代理实现精确且一致的视频编辑，支持多种操作如姿态编辑、旋转、缩放等。", "motivation": "现有方法难以确保用户意图的细粒度对齐，需要一种更精确和一致的控制工具。", "method": "使用3D代理（时间一致网格）进行编辑，通过双传播策略简化编辑过程，并利用视频扩散模型生成结果。", "result": "支持多种精确且物理一致的操作，实验证明了方法的优越性和有效性。", "conclusion": "Shape-for-Motion为高质量、可控的视频编辑工作流程迈出了关键一步。"}}
{"id": "2506.21967", "pdf": "https://arxiv.org/pdf/2506.21967", "abs": "https://arxiv.org/abs/2506.21967", "authors": ["Weimin Xiong", "Ke Wang", "Yifan Song", "Hanchao Liu", "Sai Zhou", "Wei Peng", "Sujian Li"], "title": "More Vulnerable than You Think: On the Stability of Tool-Integrated LLM Agents", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "Current evaluations of tool-integrated LLM agents typically focus on\nend-to-end tool-usage evaluation while neglecting their stability. This limits\ntheir real-world applicability, as various internal or external factors can\ncause agents to crash or behave abnormally. Our research addresses this by\ninvestigating whether agents are vulnerable to errors throughout the entire\ntool invocation process, including reading tool documentation, selecting tools\nand generating parameters, and processing the tool's response. Through\nextensive experiments, we observe that agents are highly susceptible to errors\nat each stage and agents based on open-source models are more vulnerable than\nthose based on proprietary models. We also find that increasing the model size\ndoes not significantly improve tool invocation reasoning and may make agents\nmore vulnerable to attacks resembling normal user instructions. This highlights\nthe importance of evaluating agent stability and offers valuable insights for\nfuture LLM development and evaluation.", "AI": {"tldr": "论文研究了工具集成LLM代理的稳定性问题，发现代理在整个工具调用过程中容易出错，开源模型比专有模型更脆弱，增大模型规模并不能显著改善推理能力。", "motivation": "当前对工具集成LLM代理的评估通常关注端到端工具使用，而忽略了稳定性，这限制了其实际应用。", "method": "通过实验研究代理在工具调用各阶段（如阅读文档、选择工具、生成参数和处理响应）的脆弱性。", "result": "代理在每个阶段都容易出错，开源模型更脆弱，增大模型规模对推理能力改善有限。", "conclusion": "强调评估代理稳定性的重要性，为未来LLM开发和评估提供见解。"}}
{"id": "2506.22433", "pdf": "https://arxiv.org/pdf/2506.22433", "abs": "https://arxiv.org/abs/2506.22433", "authors": ["Sadra Safadoust", "Fabio Tosi", "Fatma Güney", "Matteo Poggi"], "title": "WarpRF: Multi-View Consistency for Training-Free Uncertainty Quantification and Applications in Radiance Fields", "categories": ["cs.CV"], "comment": "Project page: https://kuis-ai.github.io/WarpRF/", "summary": "We introduce WarpRF, a training-free general-purpose framework for\nquantifying the uncertainty of radiance fields. Built upon the assumption that\nphotometric and geometric consistency should hold among images rendered by an\naccurate model, WarpRF quantifies its underlying uncertainty from an unseen\npoint of view by leveraging backward warping across viewpoints, projecting\nreliable renderings to the unseen viewpoint and measuring the consistency with\nimages rendered there. WarpRF is simple and inexpensive, does not require any\ntraining, and can be applied to any radiance field implementation for free.\nWarpRF excels at both uncertainty quantification and downstream tasks, e.g.,\nactive view selection and active mapping, outperforming any existing method\ntailored to specific frameworks.", "AI": {"tldr": "WarpRF是一种无需训练、通用的框架，用于量化辐射场的不确定性，通过反向变形和一致性测量实现。", "motivation": "现有方法通常针对特定框架，缺乏通用的不确定性量化工具。WarpRF旨在填补这一空白。", "method": "利用反向变形将可靠渲染投影到新视角，测量其与渲染图像的一致性。", "result": "WarpRF在不确定性量化和下游任务（如主动视图选择）中表现优异，超越现有方法。", "conclusion": "WarpRF是一种简单、低成本且通用的解决方案，适用于任何辐射场实现。"}}
{"id": "2506.21972", "pdf": "https://arxiv.org/pdf/2506.21972", "abs": "https://arxiv.org/abs/2506.21972", "authors": ["Mohamed Ahmed", "Mohamed Abdelmouty", "Mingyu Kim", "Gunvanth Kandula", "Alex Park", "James C. Davis"], "title": "Advancing Jailbreak Strategies: A Hybrid Approach to Exploiting LLM Vulnerabilities and Bypassing Modern Defenses", "categories": ["cs.CL", "cs.AI", "cs.CR", "cs.LG"], "comment": null, "summary": "The advancement of Pre-Trained Language Models (PTLMs) and Large Language\nModels (LLMs) has led to their widespread adoption across diverse applications.\nDespite their success, these models remain vulnerable to attacks that exploit\ntheir inherent weaknesses to bypass safety measures. Two primary\ninference-phase threats are token-level and prompt-level jailbreaks.\nToken-level attacks embed adversarial sequences that transfer well to black-box\nmodels like GPT but leave detectable patterns and rely on gradient-based token\noptimization, whereas prompt-level attacks use semantically structured inputs\nto elicit harmful responses yet depend on iterative feedback that can be\nunreliable. To address the complementary limitations of these methods, we\npropose two hybrid approaches that integrate token- and prompt-level techniques\nto enhance jailbreak effectiveness across diverse PTLMs. GCG + PAIR and the\nnewly explored GCG + WordGame hybrids were evaluated across multiple Vicuna and\nLlama models. GCG + PAIR consistently raised attack-success rates over its\nconstituent techniques on undefended models; for instance, on Llama-3, its\nAttack Success Rate (ASR) reached 91.6%, a substantial increase from PAIR's\n58.4% baseline. Meanwhile, GCG + WordGame matched the raw performance of\nWordGame maintaining a high ASR of over 80% even under stricter evaluators like\nMistral-Sorry-Bench. Crucially, both hybrids retained transferability and\nreliably pierced advanced defenses such as Gradient Cuff and JBShield, which\nfully blocked single-mode attacks. These findings expose previously unreported\nvulnerabilities in current safety stacks, highlight trade-offs between raw\nsuccess and defensive robustness, and underscore the need for holistic\nsafeguards against adaptive adversaries.", "AI": {"tldr": "论文提出两种混合攻击方法（GCG + PAIR和GCG + WordGame），结合令牌级和提示级技术，显著提高对预训练语言模型的越狱成功率，并揭示当前安全措施的漏洞。", "motivation": "尽管预训练语言模型（PTLMs）和大型语言模型（LLMs）广泛应用，但其安全措施仍易被攻击。现有令牌级和提示级攻击方法各有局限性，需要互补的解决方案。", "method": "提出两种混合攻击方法：GCG + PAIR和GCG + WordGame，结合令牌级和提示级技术，评估其在多种模型（如Vicuna和Llama）上的效果。", "result": "GCG + PAIR在Llama-3上的攻击成功率（ASR）达91.6%，显著高于PAIR的58.4%；GCG + WordGame保持高ASR（>80%），并能突破高级防御措施。", "conclusion": "混合攻击方法揭示了当前安全措施的漏洞，需开发更全面的防护策略以应对适应性攻击。"}}
{"id": "2506.22434", "pdf": "https://arxiv.org/pdf/2506.22434", "abs": "https://arxiv.org/abs/2506.22434", "authors": ["Xi Chen", "Mingkang Zhu", "Shaoteng Liu", "Xiaoyang Wu", "Xiaogang Xu", "Yu Liu", "Xiang Bai", "Hengshuang Zhao"], "title": "MiCo: Multi-image Contrast for Reinforcement Visual Reasoning", "categories": ["cs.CV"], "comment": null, "summary": "This work explores enabling Chain-of-Thought (CoT) reasoning to link visual\ncues across multiple images. A straightforward solution is to adapt rule-based\nreinforcement learning for Vision-Language Models (VLMs). However, such methods\ntypically rely on manually curated question-answer pairs, which can be\nparticularly challenging when dealing with fine grained visual details and\ncomplex logic across images. Inspired by self-supervised visual representation\nlearning, we observe that images contain inherent constraints that can serve as\nsupervision. Based on this insight, we construct image triplets comprising two\naugmented views of the same image and a third, similar but distinct image.\nDuring training, the model is prompted to generate a reasoning process to\ncompare these images (i.e., determine same or different). Then we optimize the\nmodel with rule-based reinforcement learning. Due to the high visual similarity\nand the presence of augmentations, the model must attend to subtle visual\nchanges and perform logical reasoning to succeed. Experiments show that,\nalthough trained solely on visual comparison tasks, the learned reasoning\nability generalizes effectively to a wide range of questions. Without relying\non any human-annotated question-answer pairs, our method achieves significant\nimprovements on multi-image reasoning benchmarks and shows strong performance\non general vision tasks.", "AI": {"tldr": "论文提出一种自监督方法，通过图像三元组和规则强化学习，让视觉语言模型在多图像推理任务中表现优异。", "motivation": "传统方法依赖人工标注的问答对，难以处理细粒度视觉细节和复杂逻辑。本文利用图像内在约束作为监督信号。", "method": "构建图像三元组（两个增强视图和一个相似但不同的图像），通过规则强化学习优化模型生成推理过程。", "result": "模型在视觉比较任务中训练后，能泛化到多种问题，无需人工标注即可在多图像推理基准上显著提升性能。", "conclusion": "自监督方法在多图像推理任务中表现优越，且适用于通用视觉任务。"}}
{"id": "2506.21819", "pdf": "https://arxiv.org/pdf/2506.21819", "abs": "https://arxiv.org/abs/2506.21819", "authors": ["Lena John", "Kheir Eddine Farfar", "Sören Auer", "Oliver Karras"], "title": "SciMantify -- A Hybrid Approach for the Evolving Semantification of Scientific Knowledge", "categories": ["cs.DL", "cs.AI", "cs.HC"], "comment": "Accepted at the 25th International Conference on Web Engineering 2025", "summary": "Scientific publications, primarily digitized as PDFs, remain static and\nunstructured, limiting the accessibility and reusability of the contained\nknowledge. At best, scientific knowledge from publications is provided in\ntabular formats, which lack semantic context. A more flexible, structured, and\nsemantic representation is needed to make scientific knowledge understandable\nand processable by both humans and machines. We propose an evolution model of\nknowledge representation, inspired by the 5-star Linked Open Data (LOD) model,\nwith five stages and defined criteria to guide the stepwise transition from a\ndigital artifact, such as a PDF, to a semantic representation integrated in a\nknowledge graph (KG). Based on an exemplary workflow implementing the entire\nmodel, we developed a hybrid approach, called SciMantify, leveraging tabular\nformats of scientific knowledge, e.g., results from secondary studies, to\nsupport its evolving semantification. In the approach, humans and machines\ncollaborate closely by performing semantic annotation tasks (SATs) and refining\nthe results to progressively improve the semantic representation of scientific\nknowledge. We implemented the approach in the Open Research Knowledge Graph\n(ORKG), an established platform for improving the findability, accessibility,\ninteroperability, and reusability of scientific knowledge. A preliminary user\nexperiment showed that the approach simplifies the preprocessing of scientific\nknowledge, reduces the effort for the evolving semantification, and enhances\nthe knowledge representation through better alignment with the KG structures.", "AI": {"tldr": "论文提出了一种基于5星LOD模型的知识表示演化模型，通过SciMantify方法实现科学知识的语义化，并在ORKG平台上验证其有效性。", "motivation": "科学出版物多为静态PDF，缺乏结构和语义，限制了知识的可访问性和重用性。需要更灵活、结构化的表示方式。", "method": "提出五阶段演化模型，结合SciMantify方法，通过人机协作完成语义标注任务（SATs），逐步提升知识表示。", "result": "初步用户实验表明，该方法简化了科学知识的预处理，减少了语义化的工作量，并优化了知识表示。", "conclusion": "SciMantify方法在ORKG平台上验证了其可行性，为科学知识的语义化提供了有效途径。"}}
{"id": "2506.21974", "pdf": "https://arxiv.org/pdf/2506.21974", "abs": "https://arxiv.org/abs/2506.21974", "authors": ["Simon Münker", "Nils Schwager", "Achim Rettinger"], "title": "Don't Trust Generative Agents to Mimic Communication on Social Networks Unless You Benchmarked their Empirical Realism", "categories": ["cs.CL"], "comment": "11 pages, 1 figure, 3 tables", "summary": "The ability of Large Language Models (LLMs) to mimic human behavior triggered\na plethora of computational social science research, assuming that empirical\nstudies of humans can be conducted with AI agents instead. Since there have\nbeen conflicting research findings on whether and when this hypothesis holds,\nthere is a need to better understand the differences in their experimental\ndesigns. We focus on replicating the behavior of social network users with the\nuse of LLMs for the analysis of communication on social networks. First, we\nprovide a formal framework for the simulation of social networks, before\nfocusing on the sub-task of imitating user communication. We empirically test\ndifferent approaches to imitate user behavior on X in English and German. Our\nfindings suggest that social simulations should be validated by their empirical\nrealism measured in the setting in which the simulation components were fitted.\nWith this paper, we argue for more rigor when applying generative-agent-based\nmodeling for social simulation.", "AI": {"tldr": "论文探讨了使用大型语言模型（LLMs）模拟社交网络用户行为的可行性，并提出了一个验证模拟结果实证真实性的框架。", "motivation": "由于现有研究对LLMs能否替代人类进行社会科学研究存在争议，需要更好地理解实验设计的差异。", "method": "提出了社交网络模拟的正式框架，并测试了不同方法在模仿X平台上英语和德语用户行为的效果。", "result": "研究发现，社交模拟应通过其拟合场景中的实证真实性进行验证。", "conclusion": "论文主张在基于生成代理的社交模拟中采取更严谨的方法。"}}
{"id": "2506.21990", "pdf": "https://arxiv.org/pdf/2506.21990", "abs": "https://arxiv.org/abs/2506.21990", "authors": ["Kartheek Kumar Reddy Nareddy", "Sarah Ternus", "Julia Niebling"], "title": "Analyzing and Fine-Tuning Whisper Models for Multilingual Pilot Speech Transcription in the Cockpit", "categories": ["cs.CL", "cs.AI", "cs.LG", "eess.AS"], "comment": "Computer Vision and Pattern Recognition (CVPR) 2025 Workshops", "summary": "The developments in transformer encoder-decoder architectures have led to\nsignificant breakthroughs in machine translation, Automatic Speech Recognition\n(ASR), and instruction-based chat machines, among other applications. The\npre-trained models were trained on vast amounts of generic data over a few\nepochs (fewer than five in most cases), resulting in their strong\ngeneralization capabilities. Nevertheless, the performance of these models does\nsuffer when applied to niche domains like transcribing pilot speech in the\ncockpit, which involves a lot of specific vocabulary and multilingual\nconversations. This paper investigates and improves the transcription accuracy\nof cockpit conversations with Whisper models. We have collected around 85\nminutes of cockpit simulator recordings and 130 minutes of interview recordings\nwith pilots and manually labeled them. The speakers are middle aged men\nspeaking both German and English. To improve the accuracy of transcriptions, we\npropose multiple normalization schemes to refine the transcripts and improve\nWord Error Rate (WER). We then employ fine-tuning to enhance ASR performance,\nutilizing performance-efficient fine-tuning with Low-Rank Adaptation (LoRA).\nHereby, WER decreased from 68.49 \\% (pretrained whisper Large model without\nnormalization baseline) to 26.26\\% (finetuned whisper Large model with the\nproposed normalization scheme).", "AI": {"tldr": "论文研究了如何通过微调和规范化方案提高Whisper模型在驾驶舱对话转录中的准确性，显著降低了词错误率。", "motivation": "预训练模型在特定领域（如驾驶舱对话）表现不佳，需改进转录准确性。", "method": "收集驾驶舱模拟器和飞行员访谈录音，手动标注，提出规范化方案，并使用LoRA进行微调。", "result": "词错误率从68.49%降至26.26%。", "conclusion": "提出的方法显著提升了驾驶舱对话转录的准确性。"}}
{"id": "2506.22038", "pdf": "https://arxiv.org/pdf/2506.22038", "abs": "https://arxiv.org/abs/2506.22038", "authors": ["Delu Kong", "Lieve Macken"], "title": "Can Peter Pan Survive MT? A Stylometric Study of LLMs, NMTs, and HTs in Children's Literature Translation", "categories": ["cs.CL"], "comment": "19 pages, 8 figures, 4 tables. Accepted in 2nd Workshop on\n  Creative-text Translation and Technology Co-located with MT Summit 2025.\n  Official paper may later be accessed from ACL Anthology", "summary": "This study focuses on evaluating the performance of machine translations\n(MTs) compared to human translations (HTs) in English-to-Chinese children's\nliterature translation (CLT) from a stylometric perspective. The research\nconstructs a Peter Pan corpus, comprising 21 translations: 7 human translations\n(HTs), 7 large language model translations (LLMs), and 7 neural machine\ntranslation outputs (NMTs). The analysis employs a generic feature set\n(including lexical, syntactic, readability, and n-gram features) and a creative\ntext translation (CTT-specific) feature set, which captures repetition, rhythm,\ntranslatability, and miscellaneous levels, yielding 447 linguistic features in\ntotal.\n  Using classification and clustering techniques in machine learning, we\nconduct a stylometric analysis of these translations. Results reveal that in\ngeneric features, HTs and MTs exhibit significant differences in conjunction\nword distributions and the ratio of 1-word-gram-YiYang, while NMTs and LLMs\nshow significant variation in descriptive words usage and adverb ratios.\nRegarding CTT-specific features, LLMs outperform NMTs in distribution, aligning\nmore closely with HTs in stylistic characteristics, demonstrating the potential\nof LLMs in CLT.", "AI": {"tldr": "研究从风格计量学角度评估机器翻译（MT）与人工翻译（HT）在英译中儿童文学翻译（CLT）中的表现，发现大语言模型（LLMs）在风格特征上更接近人工翻译。", "motivation": "比较机器翻译与人工翻译在儿童文学翻译中的表现，探索大语言模型在翻译中的潜力。", "method": "构建包含21种翻译的《彼得潘》语料库，使用通用和特定特征集进行风格计量分析。", "result": "人工翻译与机器翻译在通用特征上有显著差异，大语言模型在特定特征上表现更优。", "conclusion": "大语言模型在儿童文学翻译中展现出潜力，尤其在风格特征上接近人工翻译。"}}
{"id": "2506.21601", "pdf": "https://arxiv.org/pdf/2506.21601", "abs": "https://arxiv.org/abs/2506.21601", "authors": ["Duong Bach"], "title": "Hierarchical Patch Compression for ColPali: Efficient Multi-Vector Document Retrieval with Dynamic Pruning and Quantization", "categories": ["cs.IR", "cs.CV"], "comment": "9 pages", "summary": "Multi-vector document retrieval systems, such as ColPali, excel in\nfine-grained matching for complex queries but incur significant storage and\ncomputational costs due to their reliance on high-dimensional patch embeddings\nand late-interaction scoring. To address these challenges, we propose\nHPC-ColPali, a Hierarchical Patch Compression framework that enhances the\nefficiency of ColPali while preserving its retrieval accuracy. Our approach\nintegrates three innovative techniques: (1) K-Means quantization, which\ncompresses patch embeddings into 1-byte centroid indices, achieving up to\n32$\\times$ storage reduction; (2) attention-guided dynamic pruning, utilizing\nVision-Language Model attention weights to retain only the top-$p\\%$ most\nsalient patches, reducing late-interaction computation by up to 60\\% with less\nthan 2\\% nDCG@10 loss; and (3) optional binary encoding of centroid indices\ninto $b$-bit strings ($b=\\lceil\\log_2 K\\rceil$), enabling rapid Hamming\ndistance-based similarity search for resource-constrained environments.\nEvaluated on the ViDoRe and SEC-Filings datasets, HPC-ColPali achieves 30--50\\%\nlower query latency under HNSW indexing while maintaining high retrieval\nprecision. When integrated into a Retrieval-Augmented Generation pipeline for\nlegal summarization, it reduces hallucination rates by 30\\% and halves\nend-to-end latency. These advancements establish HPC-ColPali as a scalable and\nefficient solution for multi-vector document retrieval across diverse\napplications. Code is available at https://github.com/DngBack/HPC-ColPali.", "AI": {"tldr": "HPC-ColPali通过分层压缩技术提升ColPali的效率，同时保持检索精度，显著降低存储和计算成本。", "motivation": "解决ColPali因高维嵌入和交互评分导致的存储和计算成本高的问题。", "method": "结合K-Means量化、注意力引导动态剪枝和可选二进制编码三种技术。", "result": "在ViDoRe和SEC-Filings数据集上，查询延迟降低30-50%，同时保持高检索精度。", "conclusion": "HPC-ColPali为多向量文档检索提供了高效且可扩展的解决方案。"}}
{"id": "2506.21845", "pdf": "https://arxiv.org/pdf/2506.21845", "abs": "https://arxiv.org/abs/2506.21845", "authors": ["Zhuodi Cai"], "title": "3Description: An Intuitive Human-AI Collaborative 3D Modeling Approach", "categories": ["cs.HC", "cs.AI", "cs.CL", "cs.GR", "I.2; I.2.1; I.2.7; I.3; H.5; J.5"], "comment": "5 pages, 2 figures, 3 tables (containing 21 subfigures)", "summary": "This paper presents 3Description, an experimental human-AI collaborative\napproach for intuitive 3D modeling. 3Description aims to address accessibility\nand usability challenges in traditional 3D modeling by enabling\nnon-professional individuals to co-create 3D models using verbal and gesture\ndescriptions. Through a combination of qualitative research, product analysis,\nand user testing, 3Description integrates AI technologies such as Natural\nLanguage Processing and Computer Vision, powered by OpenAI and MediaPipe.\nRecognizing the web has wide cross-platform capabilities, 3Description is\nweb-based, allowing users to describe the desired model and subsequently adjust\nits components using verbal and gestural inputs. In the era of AI and emerging\nmedia, 3Description not only contributes to a more inclusive and user-friendly\ndesign process, empowering more people to participate in the construction of\nthe future 3D world, but also strives to increase human engagement in\nco-creation with AI, thereby avoiding undue surrender to technology and\npreserving human creativity.", "AI": {"tldr": "3Description是一种实验性的人机协作3D建模方法，通过语言和手势描述使非专业人士也能参与建模，解决了传统3D建模的易用性和可访问性问题。", "motivation": "传统3D建模工具对非专业人士不友好，3Description旨在通过人机协作降低门槛，让更多人参与3D设计。", "method": "结合定性研究、产品分析和用户测试，集成自然语言处理和计算机视觉技术（如OpenAI和MediaPipe），开发基于网页的协作建模工具。", "result": "3Description通过语言和手势输入实现直观建模，提升了用户参与度和创造力，同时避免了过度依赖技术。", "conclusion": "3Description为3D建模提供了更包容和用户友好的解决方案，推动了人机协作的未来发展。"}}
{"id": "2506.22050", "pdf": "https://arxiv.org/pdf/2506.22050", "abs": "https://arxiv.org/abs/2506.22050", "authors": ["Delu Kong", "Lieve Macken"], "title": "Decoding Machine Translationese in English-Chinese News: LLMs vs. NMTs", "categories": ["cs.CL"], "comment": "14 pages, 5 figures, 6 tables. Accpeted in MT Summit 2025, Research:\n  Technical track. Official version may be accessed later in the ACL Anthology", "summary": "This study explores Machine Translationese (MTese) -- the linguistic\npeculiarities of machine translation outputs -- focusing on the\nunder-researched English-to-Chinese language pair in news texts. We construct a\nlarge dataset consisting of 4 sub-corpora and employ a comprehensive five-layer\nfeature set. Then, a chi-square ranking algorithm is applied for feature\nselection in both classification and clustering tasks. Our findings confirm the\npresence of MTese in both Neural Machine Translation systems (NMTs) and Large\nLanguage Models (LLMs). Original Chinese texts are nearly perfectly\ndistinguishable from both LLM and NMT outputs. Notable linguistic patterns in\nMT outputs are shorter sentence lengths and increased use of adversative\nconjunctions. Comparing LLMs and NMTs, we achieve approximately 70%\nclassification accuracy, with LLMs exhibiting greater lexical diversity and\nNMTs using more brackets. Additionally, translation-specific LLMs show lower\nlexical diversity but higher usage of causal conjunctions compared to generic\nLLMs. Lastly, we find no significant differences between LLMs developed by\nChinese firms and their foreign counterparts.", "AI": {"tldr": "研究探讨了机器翻译输出（MTese）在英中新闻文本中的语言特征，构建了大型数据集并采用五层特征集，通过卡方排名算法进行特征选择。研究发现NMT和LLM输出均存在MTese，且与原始中文文本区分度高。LLM词汇多样性更高，NMT更多使用括号。翻译专用LLM词汇多样性较低但因果连词使用较多。中外企业开发的LLM无显著差异。", "motivation": "探索机器翻译输出（MTese）在英中新闻文本中的语言特征，填补该语言对的研究空白。", "method": "构建包含4个子库的大型数据集，采用五层特征集，使用卡方排名算法进行特征选择，完成分类和聚类任务。", "result": "NMT和LLM输出均存在MTese，原始中文文本与机器翻译输出区分度高。LLM词汇多样性更高，NMT更多使用括号。翻译专用LLM词汇多样性较低但因果连词使用较多。中外企业开发的LLM无显著差异。", "conclusion": "研究证实了MTese在NMT和LLM中的存在，并揭示了其具体语言特征，为机器翻译质量评估提供了新视角。"}}
{"id": "2506.22058", "pdf": "https://arxiv.org/pdf/2506.22058", "abs": "https://arxiv.org/abs/2506.22058", "authors": ["Baohao Liao", "Xinyi Chen", "Sara Rajaee", "Yuhui Xu", "Christian Herold", "Anders Søgaard", "Maarten de Rijke", "Christof Monz"], "title": "Lost at the Beginning of Reasoning", "categories": ["cs.CL"], "comment": "9 pages, 5 figures, 2 tables", "summary": "Recent advancements in large language models (LLMs) have significantly\nadvanced complex reasoning capabilities, particularly through extended\nchain-of-thought (CoT) reasoning that incorporates mechanisms such as\nbacktracking, self-reflection and self-correction. Despite these developments,\nthe self-correction abilities of LLMs during long CoT reasoning remain\nunderexplored. And recent findings on overthinking suggest that such models\noften engage in unnecessarily redundant reasoning. In this work, we empirically\nshow that the first reasoning step exerts a disproportionately large influence\non the final prediction - errors introduced at this stage can substantially\ndegrade subsequent reasoning quality. This phenomenon is consistently observed\nacross two state-of-the-art open-source reasoning model families: DeepSeek-R1\nand Qwen3. To address this, we propose an efficient sampling strategy that\nleverages a reward model to identify and retain high-quality first reasoning\nsteps while discarding suboptimal ones, achieving up to a 70% reduction in\ninference cost without sacrificing accuracy. Finally, we introduce a new\nbenchmark specifically constructed with deliberately flawed first reasoning\nsteps to systematically evaluate model self-correction capabilities, offering a\nfoundation for future research on robust reasoning in LLMs.", "AI": {"tldr": "论文探讨了大型语言模型（LLM）在长链推理中自我修正能力的不足，提出了一种高效采样策略以减少推理成本，并引入新基准评估模型自我修正能力。", "motivation": "尽管LLM在复杂推理方面取得进展，但其在长链推理中的自我修正能力仍未充分研究，且存在冗余推理问题。", "method": "通过实证研究揭示首步推理对最终预测的显著影响，提出基于奖励模型的高效采样策略，优化首步推理质量。", "result": "实验表明，该方法能在不损失准确性的情况下减少70%的推理成本，并在新基准上验证了模型自我修正能力。", "conclusion": "研究为LLM的稳健推理提供了新方法和评估工具，为未来研究奠定基础。"}}
{"id": "2506.21630", "pdf": "https://arxiv.org/pdf/2506.21630", "abs": "https://arxiv.org/abs/2506.21630", "authors": ["Yixin Sun", "Li Li", "Wenke E", "Amir Atapour-Abarghouei", "Toby P. Breckon"], "title": "TOMD: A Trail-based Off-road Multimodal Dataset for Traversable Pathway Segmentation under Challenging Illumination Conditions", "categories": ["cs.RO", "cs.CV", "cs.LG"], "comment": "8 pages, 9 figures, 2025 IJCNN", "summary": "Detecting traversable pathways in unstructured outdoor environments remains a\nsignificant challenge for autonomous robots, especially in critical\napplications such as wide-area search and rescue, as well as incident\nmanagement scenarios like forest fires. Existing datasets and models primarily\ntarget urban settings or wide, vehicle-traversable off-road tracks, leaving a\nsubstantial gap in addressing the complexity of narrow, trail-like off-road\nscenarios. To address this, we introduce the Trail-based Off-road Multimodal\nDataset (TOMD), a comprehensive dataset specifically designed for such\nenvironments. TOMD features high-fidelity multimodal sensor data -- including\n128-channel LiDAR, stereo imagery, GNSS, IMU, and illumination measurements --\ncollected through repeated traversals under diverse conditions. We also propose\na dynamic multiscale data fusion model for accurate traversable pathway\nprediction. The study analyzes the performance of early, cross, and mixed\nfusion strategies under varying illumination levels. Results demonstrate the\neffectiveness of our approach and the relevance of illumination in segmentation\nperformance. We publicly release TOMD at https://github.com/yyyxs1125/TMOD to\nsupport future research in trail-based off-road navigation.", "AI": {"tldr": "论文介绍了Trail-based Off-road Multimodal Dataset (TOMD)，一个针对狭窄、小径式越野环境的多模态数据集，并提出了一种动态多尺度数据融合模型用于可通行路径预测。", "motivation": "现有数据集和模型主要针对城市或宽阔的越野场景，无法满足狭窄小径式越野环境的复杂需求，尤其是在搜索救援和森林火灾等关键应用中。", "method": "提出了TOMD数据集，包含高保真多模态传感器数据，并提出动态多尺度数据融合模型，分析了不同光照条件下的早期、交叉和混合融合策略。", "result": "结果表明该方法有效，且光照对分割性能有显著影响。", "conclusion": "TOMD数据集和提出的模型为小径式越野导航研究提供了支持，相关数据已公开。"}}
{"id": "2506.22062", "pdf": "https://arxiv.org/pdf/2506.22062", "abs": "https://arxiv.org/abs/2506.22062", "authors": ["Chris Madge", "Maris Camilleri", "Paloma Carretero Garcia", "Mladen Karan", "Juexi Shao", "Prashant Jayannavar", "Julian Hough", "Benjamin Roth", "Massimo Poesio"], "title": "MDC-R: The Minecraft Dialogue Corpus with Reference", "categories": ["cs.CL"], "comment": null, "summary": "We introduce the Minecraft Dialogue Corpus with Reference (MDC-R). MDC-R is a\nnew language resource that supplements the original Minecraft Dialogue Corpus\n(MDC) with expert annotations of anaphoric and deictic reference. MDC's\ntask-orientated, multi-turn, situated dialogue in a dynamic environment has\nmotivated multiple annotation efforts, owing to the interesting linguistic\nphenomena that this setting gives rise to. We believe it can serve as a\nvaluable resource when annotated with reference, too. Here, we discuss our\nmethod of annotation and the resulting corpus, and provide both a quantitative\nand a qualitative analysis of the data. Furthermore, we carry out a short\nexperiment demonstrating the usefulness of our corpus for referring expression\ncomprehension.", "AI": {"tldr": "介绍了Minecraft Dialogue Corpus with Reference (MDC-R)，这是一个补充原始MDC的语言资源，增加了专家对指代和指示性参考的标注。", "motivation": "MDC的任务导向、多轮、动态环境中的对话引发了多种语言现象，标注参考信息可以使其成为更有价值的资源。", "method": "讨论了标注方法、生成的语料库，并提供了数据的定量和定性分析。", "result": "进行了简短实验，展示了语料库在指代表达理解中的实用性。", "conclusion": "MDC-R是一个有价值的资源，特别适用于指代和指示性参考的研究。"}}
{"id": "2506.22098", "pdf": "https://arxiv.org/pdf/2506.22098", "abs": "https://arxiv.org/abs/2506.22098", "authors": ["Eleonora Amadori", "Daniele Cirulli", "Edoardo Di Martino", "Jacopo Nudo", "Maria Sahakyan", "Emanuele Sangiorgio", "Arnaldo Santoro", "Simon Zollo", "Alessandro Galeazzi", "Niccolò Di Marco"], "title": "Involvement drives complexity of language in online debates", "categories": ["cs.CL", "cs.CY", "physics.soc-ph"], "comment": null, "summary": "Language is a fundamental aspect of human societies, continuously evolving in\nresponse to various stimuli, including societal changes and intercultural\ninteractions. Technological advancements have profoundly transformed\ncommunication, with social media emerging as a pivotal force that merges\nentertainment-driven content with complex social dynamics. As these platforms\nreshape public discourse, analyzing the linguistic features of user-generated\ncontent is essential to understanding their broader societal impact. In this\npaper, we examine the linguistic complexity of content produced by influential\nusers on Twitter across three globally significant and contested topics:\nCOVID-19, COP26, and the Russia-Ukraine war. By combining multiple measures of\ntextual complexity, we assess how language use varies along four key\ndimensions: account type, political leaning, content reliability, and\nsentiment. Our analysis reveals significant differences across all four axes,\nincluding variations in language complexity between individuals and\norganizations, between profiles with sided versus moderate political views, and\nbetween those associated with higher versus lower reliability scores.\nAdditionally, profiles producing more negative and offensive content tend to\nuse more complex language, with users sharing similar political stances and\nreliability levels converging toward a common jargon. Our findings offer new\ninsights into the sociolinguistic dynamics of digital platforms and contribute\nto a deeper understanding of how language reflects ideological and social\nstructures in online spaces.", "AI": {"tldr": "研究分析了Twitter上有影响力用户在三个全球性争议话题（COVID-19、COP26、俄乌战争）中的语言复杂性，揭示了账户类型、政治倾向、内容可靠性和情感对语言使用的显著影响。", "motivation": "社交媒体重塑了公共话语，分析用户生成内容的语言特征有助于理解其社会影响。", "method": "结合多种文本复杂性指标，评估语言在账户类型、政治倾向、内容可靠性和情感四个维度的差异。", "result": "发现语言复杂性在不同维度存在显著差异，负面和攻击性内容倾向于使用更复杂的语言，且政治立场和可靠性相似的用户使用共同术语。", "conclusion": "研究揭示了数字平台的社交语言动态，深化了对在线空间中语言如何反映意识形态和社会结构的理解。"}}
{"id": "2506.22105", "pdf": "https://arxiv.org/pdf/2506.22105", "abs": "https://arxiv.org/abs/2506.22105", "authors": ["David Demitri Africa"], "title": "Identifying a Circuit for Verb Conjugation in GPT-2", "categories": ["cs.CL", "cs.LG"], "comment": null, "summary": "I implement a procedure to isolate and interpret the sub-network (or\n\"circuit\") responsible for subject-verb agreement in GPT-2 Small. In this\nstudy, the model is given prompts where the subject is either singular (e.g.\n\"Alice\") or plural (e.g. \"Alice and Bob\"), and the task is to correctly predict\nthe appropriate verb form (\"walks\" for singular subjects, \"walk\" for plural\nsubjects). Using a series of techniques-including performance verification\nautomatic circuit discovery via direct path patching, and direct logit\nattribution- I isolate a candidate circuit that contributes significantly to\nthe model's correct verb conjugation. The results suggest that only a small\nfraction of the network's component-token pairs is needed to achieve near-model\nperformance on the base task but substantially more for more complex settings.", "AI": {"tldr": "研究通过技术手段分离并解读了GPT-2 Small中负责主谓一致的子网络，发现仅需少量组件即可完成任务，但复杂场景需更多组件。", "motivation": "探索GPT-2 Small中主谓一致任务的内部机制，理解模型如何实现语法正确性。", "method": "使用性能验证、自动电路发现、直接路径修补和直接对数归因等技术。", "result": "分离出显著影响动词变位的子网络，少量组件即可完成任务，复杂场景需更多组件。", "conclusion": "主谓一致任务由特定子网络实现，简单任务高效，复杂任务需扩展网络支持。"}}
{"id": "2506.21680", "pdf": "https://arxiv.org/pdf/2506.21680", "abs": "https://arxiv.org/abs/2506.21680", "authors": ["Sai Sri Teja", "Sreevidya Chintalapati", "Vinayak Gupta", "Mukund Varma T", "Haejoon Lee", "Aswin Sankaranarayanan", "Kaushik Mitra"], "title": "PhotonSplat: 3D Scene Reconstruction and Colorization from SPAD Sensors", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted at the International Conference on Computational\n  Photography(ICCP) 2025", "summary": "Advances in 3D reconstruction using neural rendering have enabled\nhigh-quality 3D capture. However, they often fail when the input imagery is\ncorrupted by motion blur, due to fast motion of the camera or the objects in\nthe scene. This work advances neural rendering techniques in such scenarios by\nusing single-photon avalanche diode (SPAD) arrays, an emerging sensing\ntechnology capable of sensing images at extremely high speeds. However, the use\nof SPADs presents its own set of unique challenges in the form of binary\nimages, that are driven by stochastic photon arrivals. To address this, we\nintroduce PhotonSplat, a framework designed to reconstruct 3D scenes directly\nfrom SPAD binary images, effectively navigating the noise vs. blur trade-off.\nOur approach incorporates a novel 3D spatial filtering technique to reduce\nnoise in the renderings. The framework also supports both no-reference using\ngenerative priors and reference-based colorization from a single blurry image,\nenabling downstream applications such as segmentation, object detection and\nappearance editing tasks. Additionally, we extend our method to incorporate\ndynamic scene representations, making it suitable for scenes with moving\nobjects. We further contribute PhotonScenes, a real-world multi-view dataset\ncaptured with the SPAD sensors.", "AI": {"tldr": "提出PhotonSplat框架，利用SPAD传感器解决运动模糊问题，实现高质量3D重建。", "motivation": "现有神经渲染技术在输入图像因运动模糊而损坏时效果不佳，SPAD传感器能高速捕捉图像但带来二进制图像的挑战。", "method": "引入PhotonSplat框架，结合3D空间滤波技术降噪，支持无参考生成和有参考着色，并扩展至动态场景。", "result": "成功从SPAD二进制图像重建3D场景，贡献PhotonScenes数据集。", "conclusion": "PhotonSplat有效解决了噪声与模糊的权衡问题，适用于动态场景和下游任务。"}}
{"id": "2506.21872", "pdf": "https://arxiv.org/pdf/2506.21872", "abs": "https://arxiv.org/abs/2506.21872", "authors": ["Chaofan Pan", "Xin Yang", "Yanhua Li", "Wei Wei", "Tianrui Li", "Bo An", "Jiye Liang"], "title": "A Survey of Continual Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "This work has been submitted to the IEEE TPAMI", "summary": "Reinforcement Learning (RL) is an important machine learning paradigm for\nsolving sequential decision-making problems. Recent years have witnessed\nremarkable progress in this field due to the rapid development of deep neural\nnetworks. However, the success of RL currently relies on extensive training\ndata and computational resources. In addition, RL's limited ability to\ngeneralize across tasks restricts its applicability in dynamic and real-world\nenvironments. With the arisen of Continual Learning (CL), Continual\nReinforcement Learning (CRL) has emerged as a promising research direction to\naddress these limitations by enabling agents to learn continuously, adapt to\nnew tasks, and retain previously acquired knowledge. In this survey, we provide\na comprehensive examination of CRL, focusing on its core concepts, challenges,\nand methodologies. Firstly, we conduct a detailed review of existing works,\norganizing and analyzing their metrics, tasks, benchmarks, and scenario\nsettings. Secondly, we propose a new taxonomy of CRL methods, categorizing them\ninto four types from the perspective of knowledge storage and/or transfer.\nFinally, our analysis highlights the unique challenges of CRL and provides\npractical insights into future directions.", "AI": {"tldr": "本文综述了持续强化学习（CRL）的核心概念、挑战和方法，提出了新的分类法，并探讨了未来研究方向。", "motivation": "强化学习（RL）在动态和真实环境中的泛化能力有限，持续学习（CL）的兴起为解决这一问题提供了可能。", "method": "详细回顾现有工作，提出基于知识存储和转移的四类CRL方法分类法。", "result": "分析了CRL的独特挑战，并提供了未来研究的实用见解。", "conclusion": "CRL是一个有前景的研究方向，能够解决RL的泛化和持续学习问题。"}}
{"id": "2506.22141", "pdf": "https://arxiv.org/pdf/2506.22141", "abs": "https://arxiv.org/abs/2506.22141", "authors": ["Iliass Ayaou", "Denis Cavallucci", "Hicham Chibane"], "title": "DAPFAM: A Domain-Aware Patent Retrieval Dataset Aggregated at the Family Level", "categories": ["cs.CL", "cs.IR"], "comment": null, "summary": "In the landscape of publicly available patent retrieval datasets, the need\nfor explicit indomain and out-of-domain labeling, multi-jurisdiction coverage,\nbalanced query domain representation and manageable sizes that support sub\ndocument level experiments on moderate computational resources is often\noverlooked. To address these gaps, we propose DAPFAM, a new open access\ndomain-aware patent retrieval dataset constructed at the simple-family level.\nThe dataset contains 1,247 domain balanced full text query families and 45,336\nfull text target families. The dataset is enriched by clear relevance judgments\n(forward/backward citations as positive links, random negatives), as well as\nexplicit in-domain or out-of-domain relationships via a novel proposed\nlabelling scheme based on via International Patent Classification (IPC) codes,\nresulting in 49,869 evaluation pairs. The dataset is multi jurisdictional,\nrequires little to no preprocessing for retrieval evaluation, and remains of a\nsize manageable for entities with limited ressources allowing for sub document\nlevel retrieval experiments without excessive computational costs. We describe\nour three-step data-curation pipeline, present comprehensive dataset\nstatistics, and provide baseline experiments using lexical and neural retrieval\nmethods. Our baseline experiments highlight significant challenges in\ncrossdomain patent retrieval. The dataset will be publicly available (for now\nthe access link is this repository:\nhttps://osf.io/vbyzd/?view_only=1a40242e0d1941a58aa854af3e50cf6b).", "AI": {"tldr": "DAPFAM是一个新的开放访问专利检索数据集，解决了现有数据集在领域标注、多司法管辖区覆盖、查询领域平衡和计算资源管理方面的不足。", "motivation": "现有专利检索数据集缺乏明确的领域标注、多司法管辖区覆盖和平衡的查询领域表示，且计算资源需求较高。", "method": "通过三步数据整理流程构建DAPFAM数据集，包含1,247个领域平衡的查询家族和45,336个目标家族，使用IPC代码进行领域标注。", "result": "数据集包含49,869个评估对，支持跨领域专利检索实验，基线实验显示跨领域检索存在显著挑战。", "conclusion": "DAPFAM填补了专利检索数据集的空白，支持资源有限的实体进行子文档级实验，未来将公开访问。"}}
{"id": "2506.21714", "pdf": "https://arxiv.org/pdf/2506.21714", "abs": "https://arxiv.org/abs/2506.21714", "authors": ["Denis Gudovskiy", "Wenzhao Zheng", "Tomoyuki Okuno", "Yohei Nakata", "Kurt Keutzer"], "title": "$\\textrm{ODE}_t \\left(\\textrm{ODE}_l \\right)$: Shortcutting the Time and Length in Diffusion and Flow Models for Faster Sampling", "categories": ["cs.LG", "cs.CV"], "comment": "Preprint. Github page: github.com/gudovskiy/odelt", "summary": "Recently, continuous normalizing flows (CNFs) and diffusion models (DMs) have\nbeen studied using the unified theoretical framework. Although such models can\ngenerate high-quality data points from a noise distribution, the sampling\ndemands multiple iterations to solve an ordinary differential equation (ODE)\nwith high computational complexity. Most existing methods focus on reducing the\nnumber of time steps during the sampling process to improve efficiency. In this\nwork, we explore a complementary direction in which the quality-complexity\ntradeoff can be dynamically controlled in terms of time steps and in the length\nof the neural network. We achieve this by rewiring the blocks in the\ntransformer-based architecture to solve an inner discretized ODE w.r.t. its\nlength. Then, we employ time- and length-wise consistency terms during flow\nmatching training, and as a result, the sampling can be performed with an\narbitrary number of time steps and transformer blocks. Unlike others, our\n$\\textrm{ODE}_t \\left(\\textrm{ODE}_l \\right)$ approach is solver-agnostic in\ntime dimension and decreases both latency and memory usage. Compared to the\nprevious state of the art, image generation experiments on CelebA-HQ and\nImageNet show a latency reduction of up to $3\\times$ in the most efficient\nsampling mode, and a FID score improvement of up to $3.5$ points for\nhigh-quality sampling. We release our code and model weights with fully\nreproducible experiments.", "AI": {"tldr": "论文提出了一种动态控制质量-复杂度权衡的方法，通过重新布线Transformer架构的块来解决ODE问题，并在训练中引入时间和长度一致性项，从而在采样时灵活调整时间步和网络长度。", "motivation": "现有方法主要关注减少采样过程中的时间步以提高效率，但忽略了在神经网络长度上的动态调整。本文探索了一种互补方向，动态控制时间步和网络长度以优化质量-复杂度权衡。", "method": "通过重新布线Transformer架构的块来解决内部离散ODE问题，并在训练中引入时间和长度一致性项，实现采样时灵活调整时间步和Transformer块数量。", "result": "在CelebA-HQ和ImageNet上的实验表明，最高效采样模式下延迟减少3倍，高质量采样时FID分数提升3.5分。", "conclusion": "提出的方法在时间和长度维度上均具有灵活性，显著降低了延迟和内存使用，同时提升了生成质量。"}}
{"id": "2506.22143", "pdf": "https://arxiv.org/pdf/2506.22143", "abs": "https://arxiv.org/abs/2506.22143", "authors": ["Muhammad Umar Farooq", "Oscar Saz"], "title": "SAGE: Spliced-Audio Generated Data for Enhancing Foundational Models in Low-Resource Arabic-English Code-Switched Speech Recognition", "categories": ["cs.CL", "cs.SD", "eess.AS"], "comment": "Accepted for IEEE MLSP 2025", "summary": "This paper investigates the performance of various speech SSL models on\ndialectal Arabic (DA) and Arabic-English code-switched (CS) speech. To address\ndata scarcity, a modified audio-splicing approach is introduced to generate\nartificial CS speech data. Fine-tuning an already fine-tuned SSL model with the\nproposed Spliced-Audio Generated (SAGE) data results in an absolute improvement\non Word Error Rate (WER) of 7.8% on Arabic and English CS benchmarks.\nAdditionally, an Experience Replay (ER) inspired approach is proposed to\nenhance generalisation across DA and CS speech while mitigating catastrophic\nforgetting. Integrating an out-of-domain 3-gram language model reduces the\noverall mean WER from 31.7% to 26.6%. Few-shot fine-tuning for code-switching\nbenchmarks further improves WER by 4.9%. A WER of 31.1% on Arabic-English CS\nbenchmarks surpasses large-scale multilingual models, including USM and\nWhisper-large-v2 (both over ten times larger) by an absolute margin of 5.5% and\n8.4%, respectively.", "AI": {"tldr": "论文研究了语音SSL模型在阿拉伯方言和阿拉伯语-英语语码转换语音上的表现，提出了一种改进的音频拼接方法生成人工数据，显著降低了词错误率。", "motivation": "解决阿拉伯方言和语码转换语音数据稀缺问题，提升模型性能。", "method": "采用改进的音频拼接方法生成人工数据，结合经验回放和外部语言模型优化。", "result": "词错误率显著降低，性能超越大规模多语言模型。", "conclusion": "提出的方法有效解决了数据稀缺问题，显著提升了模型在方言和语码转换语音上的表现。"}}
{"id": "2506.21874", "pdf": "https://arxiv.org/pdf/2506.21874", "abs": "https://arxiv.org/abs/2506.21874", "authors": ["Stanley Wu", "Ronik Bhaskar", "Anna Yoo Jeong Ha", "Shawn Shan", "Haitao Zheng", "Ben Y. Zhao"], "title": "On the Feasibility of Poisoning Text-to-Image AI Models via Adversarial Mislabeling", "categories": ["cs.CR", "cs.AI"], "comment": "ACM Conference on Computer and Communications Security 2025", "summary": "Today's text-to-image generative models are trained on millions of images\nsourced from the Internet, each paired with a detailed caption produced by\nVision-Language Models (VLMs). This part of the training pipeline is critical\nfor supplying the models with large volumes of high-quality image-caption pairs\nduring training. However, recent work suggests that VLMs are vulnerable to\nstealthy adversarial attacks, where adversarial perturbations are added to\nimages to mislead the VLMs into producing incorrect captions.\n  In this paper, we explore the feasibility of adversarial mislabeling attacks\non VLMs as a mechanism to poisoning training pipelines for text-to-image\nmodels. Our experiments demonstrate that VLMs are highly vulnerable to\nadversarial perturbations, allowing attackers to produce benign-looking images\nthat are consistently miscaptioned by the VLM models. This has the effect of\ninjecting strong \"dirty-label\" poison samples into the training pipeline for\ntext-to-image models, successfully altering their behavior with a small number\nof poisoned samples. We find that while potential defenses can be effective,\nthey can be targeted and circumvented by adaptive attackers. This suggests a\ncat-and-mouse game that is likely to reduce the quality of training data and\nincrease the cost of text-to-image model development. Finally, we demonstrate\nthe real-world effectiveness of these attacks, achieving high attack success\n(over 73%) even in black-box scenarios against commercial VLMs (Google Vertex\nAI and Microsoft Azure).", "AI": {"tldr": "研究探讨了通过对抗性攻击误导视觉语言模型（VLMs）生成错误标注，从而污染文本到图像模型训练数据的可行性。实验表明，少量污染样本即可显著改变模型行为，且防御措施可能被绕过。", "motivation": "视觉语言模型（VLMs）生成的标注对文本到图像模型的训练至关重要，但其易受对抗性攻击，可能导致训练数据被污染。", "method": "通过对抗性扰动生成看似正常但被VLMs错误标注的图像，注入到训练数据中。", "result": "实验显示，攻击成功率超过73%，即使针对商业VLMs（如Google Vertex AI和Microsoft Azure）也有效。", "conclusion": "对抗性攻击可能引发数据质量下降和开发成本增加的“猫鼠游戏”，需进一步研究防御措施。"}}
{"id": "2506.22157", "pdf": "https://arxiv.org/pdf/2506.22157", "abs": "https://arxiv.org/abs/2506.22157", "authors": ["Tianshu Yu", "Chao Xiang", "Mingchuan Yang", "Pei Ke", "Bosi Wen", "Cunxiang Wang", "Jiale Cheng", "Li Zhang", "Xinyu Mu", "Chuxiong Sun", "Minlie Huang"], "title": "Training Language Model to Critique for Better Refinement", "categories": ["cs.CL"], "comment": "Accepted to ACL 2025 Findings", "summary": "Large language models (LLMs) have demonstrated remarkable evaluation and\ncritique capabilities, providing insightful feedback and identifying flaws in\nvarious tasks. However, limited research has explored which types of critiques\nare most effective for improving model responses or how to generate such\ncritiques. To address this gap, we introduce \\textbf{R}efinement-oriented\n\\textbf{C}ritique \\textbf{O}ptimization (RCO), a novel framework designed to\ntrain critic models using refinement signals. RCO uses a feedback loop where\ncritiques, generated by the critic model, guide the actor model in refining its\nresponses. The critique utility (CU) quantifies the effectiveness of these\nrefinements, serving as the reward signal for training the critic model. By\nfocusing on critiques that lead to better refinements, RCO eliminates the need\nfor direct critique preference assessment, ensuring that critiques driving\nmeaningful improvements are rewarded. We evaluate RCO across five tasks, i.e.,\ndialog generation, summarization, question answering, mathematical reasoning,\nand code generation, and show that it significantly outperforms traditional\nmethods and open-source models in terms of critique quality and refinement\noutcomes. Our contributions include the introduction of RCO, a novel\nsupervision scheme based on refined response preferences, and comprehensive\nexperimental results that highlight the method's effectiveness in enhancing LLM\ncritique-refinement loops.", "AI": {"tldr": "论文提出了RCO框架，通过优化批评信号来训练批评模型，显著提升了语言模型的反馈和优化能力。", "motivation": "现有研究未深入探讨哪些批评类型最有效或如何生成这些批评，因此需要一种新方法来优化批评信号。", "method": "RCO框架通过反馈循环，利用批评效用（CU）作为奖励信号训练批评模型，专注于能带来改进的批评。", "result": "在五项任务中，RCO显著优于传统方法和开源模型，提升了批评质量和优化效果。", "conclusion": "RCO通过新颖的监督方案和反馈循环，有效增强了语言模型的批评优化能力。"}}
{"id": "2506.21748", "pdf": "https://arxiv.org/pdf/2506.21748", "abs": "https://arxiv.org/abs/2506.21748", "authors": ["Liav Hen", "Erez Yosef", "Dan Raviv", "Raja Giryes", "Jacob Scheuer"], "title": "Inverse Design of Diffractive Metasurfaces Using Diffusion Models", "categories": ["physics.optics", "cs.CV", "cs.LG"], "comment": null, "summary": "Metasurfaces are ultra-thin optical elements composed of engineered\nsub-wavelength structures that enable precise control of light. Their inverse\ndesign - determining a geometry that yields a desired optical response - is\nchallenging due to the complex, nonlinear relationship between structure and\noptical properties. This often requires expert tuning, is prone to local\nminima, and involves significant computational overhead. In this work, we\naddress these challenges by integrating the generative capabilities of\ndiffusion models into computational design workflows. Using an RCWA simulator,\nwe generate training data consisting of metasurface geometries and their\ncorresponding far-field scattering patterns. We then train a conditional\ndiffusion model to predict meta-atom geometry and height from a target spatial\npower distribution at a specified wavelength, sampled from a continuous\nsupported band. Once trained, the model can generate metasurfaces with low\nerror, either directly using RCWA-guided posterior sampling or by serving as an\ninitializer for traditional optimization methods. We demonstrate our approach\non the design of a spatially uniform intensity splitter and a polarization beam\nsplitter, both produced with low error in under 30 minutes. To support further\nresearch in data-driven metasurface design, we publicly release our code and\ndatasets.", "AI": {"tldr": "论文提出了一种基于扩散模型的超表面逆向设计方法，解决了传统方法中局部最优和计算开销大的问题。", "motivation": "超表面的逆向设计因其复杂的非线性关系而具有挑战性，传统方法需要专家调参且易陷入局部最优。", "method": "结合扩散模型和RCWA模拟器生成训练数据，训练条件扩散模型以预测目标光学响应下的超表面几何结构。", "result": "模型能够快速生成低误差的超表面设计，如均匀强度分束器和偏振分束器，设计时间少于30分钟。", "conclusion": "该方法为数据驱动的超表面设计提供了高效工具，并公开了代码和数据集以支持进一步研究。"}}
{"id": "2506.22232", "pdf": "https://arxiv.org/pdf/2506.22232", "abs": "https://arxiv.org/abs/2506.22232", "authors": ["Patrick Haller", "Jannis Vamvas", "Rico Sennrich", "Lena A. Jäger"], "title": "Leveraging In-Context Learning for Political Bias Testing of LLMs", "categories": ["cs.CL"], "comment": "ACL 2025", "summary": "A growing body of work has been querying LLMs with political questions to\nevaluate their potential biases. However, this probing method has limited\nstability, making comparisons between models unreliable. In this paper, we\nargue that LLMs need more context. We propose a new probing task, Questionnaire\nModeling (QM), that uses human survey data as in-context examples. We show that\nQM improves the stability of question-based bias evaluation, and demonstrate\nthat it may be used to compare instruction-tuned models to their base versions.\nExperiments with LLMs of various sizes indicate that instruction tuning can\nindeed change the direction of bias. Furthermore, we observe a trend that\nlarger models are able to leverage in-context examples more effectively, and\ngenerally exhibit smaller bias scores in QM. Data and code are publicly\navailable.", "AI": {"tldr": "论文提出了一种新的探测任务QM，利用人类调查数据作为上下文示例，提高了基于问题的偏见评估的稳定性，并发现指令调优可以改变偏见方向。", "motivation": "现有方法在评估LLMs的政治偏见时稳定性不足，难以可靠比较不同模型。", "method": "提出QM任务，使用人类调查数据作为上下文示例。", "result": "QM提高了评估稳定性，指令调优可改变偏见方向，大模型能更有效利用上下文示例且偏见较小。", "conclusion": "QM是一种更稳定的偏见评估方法，大模型表现更好。"}}
{"id": "2506.21765", "pdf": "https://arxiv.org/pdf/2506.21765", "abs": "https://arxiv.org/abs/2506.21765", "authors": ["Qi Li", "Shaheer U. Saeed", "Yuliang Huang", "Mingyuan Luo", "Zhongnuo Yan", "Jiongquan Chen", "Xin Yang", "Dong Ni", "Nektarios Winter", "Phuc Nguyen", "Lucas Steinberger", "Caelan Haney", "Yuan Zhao", "Mingjie Jiang", "Bowen Ren", "SiYeoul Lee", "Seonho Kim", "MinKyung Seo", "MinWoo Kim", "Yimeng Dou", "Zhiwei Zhang", "Yin Li", "Tomy Varghese", "Dean C. Barratt", "Matthew J. Clarkson", "Tom Vercauteren", "Yipeng Hu"], "title": "TUS-REC2024: A Challenge to Reconstruct 3D Freehand Ultrasound Without External Tracker", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Trackerless freehand ultrasound reconstruction aims to reconstruct 3D volumes\nfrom sequences of 2D ultrasound images without relying on external tracking\nsystems, offering a low-cost, portable, and widely deployable alternative for\nvolumetric imaging. However, it presents significant challenges, including\naccurate inter-frame motion estimation, minimisation of drift accumulation over\nlong sequences, and generalisability across scanning protocols. The TUS-REC2024\nChallenge was established to benchmark and accelerate progress in trackerless\n3D ultrasound reconstruction by providing a publicly available dataset for the\nfirst time, along with a baseline model and evaluation framework. The Challenge\nattracted over 43 registered teams, of which 6 teams submitted 21 valid\ndockerized solutions. Submitted methods spanned a wide range of algorithmic\napproaches, including recurrent models, registration-driven volume refinement,\nattention, and physics-informed models. This paper presents an overview of the\nChallenge design, summarises the key characteristics of the dataset, provides a\nconcise literature review, introduces the technical details of the underlying\nmethodology working with tracked freehand ultrasound data, and offers a\ncomparative analysis of submitted methods across multiple evaluation metrics.\nThe results highlight both the progress and current limitations of\nstate-of-the-art approaches in this domain, and inform directions for future\nresearch. The data, evaluation code, and baseline are publicly available to\nfacilitate ongoing development and reproducibility. As a live and evolving\nbenchmark, this Challenge is designed to be continuously developed and\nimproved. The Challenge was held at MICCAI 2024 and will be organised again at\nMICCAI 2025, reflecting its growing impact and the sustained commitment to\nadvancing this field.", "AI": {"tldr": "TUS-REC2024 Challenge旨在推动无追踪器自由手超声3D重建技术的发展，提供了公开数据集、基线模型和评估框架，吸引了43个团队参与，展示了多种算法方法，并分析了当前技术的进展与局限。", "motivation": "解决无追踪器自由手超声3D重建中的运动估计、漂移累积和通用性问题，推动该领域的进步。", "method": "通过公开数据集、基线模型和评估框架，吸引团队提交多种算法方法（如循环模型、注册驱动体积细化、注意力机制和物理启发模型）。", "result": "6个团队提交了21个有效解决方案，展示了当前技术的进展与局限。", "conclusion": "挑战赛将持续发展，推动无追踪器自由手超声3D重建技术的进一步研究，数据与代码公开以促进可重复性。"}}
{"id": "2506.21884", "pdf": "https://arxiv.org/pdf/2506.21884", "abs": "https://arxiv.org/abs/2506.21884", "authors": ["Fabian Perez", "Sara Rojas", "Carlos Hinojosa", "Hoover Rueda-Chacón", "Bernard Ghanem"], "title": "UnMix-NeRF: Spectral Unmixing Meets Neural Radiance Fields", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.LG", "eess.SP"], "comment": "Paper accepted at ICCV 2025 main conference", "summary": "Neural Radiance Field (NeRF)-based segmentation methods focus on object\nsemantics and rely solely on RGB data, lacking intrinsic material properties.\nThis limitation restricts accurate material perception, which is crucial for\nrobotics, augmented reality, simulation, and other applications. We introduce\nUnMix-NeRF, a framework that integrates spectral unmixing into NeRF, enabling\njoint hyperspectral novel view synthesis and unsupervised material\nsegmentation. Our method models spectral reflectance via diffuse and specular\ncomponents, where a learned dictionary of global endmembers represents pure\nmaterial signatures, and per-point abundances capture their distribution. For\nmaterial segmentation, we use spectral signature predictions along learned\nendmembers, allowing unsupervised material clustering. Additionally, UnMix-NeRF\nenables scene editing by modifying learned endmember dictionaries for flexible\nmaterial-based appearance manipulation. Extensive experiments validate our\napproach, demonstrating superior spectral reconstruction and material\nsegmentation to existing methods. Project page:\nhttps://www.factral.co/UnMix-NeRF.", "AI": {"tldr": "UnMix-NeRF结合光谱解混技术，实现了联合高光谱新视角合成和无监督材料分割，解决了传统NeRF方法缺乏材料属性的问题。", "motivation": "传统NeRF方法仅依赖RGB数据，缺乏材料属性信息，限制了在机器人、增强现实等应用中的准确材料感知。", "method": "通过建模光谱反射率的漫反射和镜面反射分量，学习全局端元字典表示纯材料特征，并通过点级丰度分布实现材料分割。", "result": "实验表明，UnMix-NeRF在高光谱重建和材料分割方面优于现有方法。", "conclusion": "UnMix-NeRF为材料感知任务提供了灵活且高效的解决方案，支持场景编辑和材料操作。"}}
{"id": "2506.22305", "pdf": "https://arxiv.org/pdf/2506.22305", "abs": "https://arxiv.org/abs/2506.22305", "authors": ["Albert Agisha Ntwali", "Luca Rück", "Martin Heckmann"], "title": "Detection of Personal Data in Structured Datasets Using a Large Language Model", "categories": ["cs.CL", "I.5.4; I.2.7; H.3.1"], "comment": "10 pages", "summary": "We propose a novel approach for detecting personal data in structured\ndatasets, leveraging GPT-4o, a state-of-the-art Large Language Model. A key\ninnovation of our method is the incorporation of contextual information: in\naddition to a feature's name and values, we utilize information from other\nfeature names within the dataset as well as the dataset description. We compare\nour approach to alternative methods, including Microsoft Presidio and CASSED,\nevaluating them on multiple datasets: DeSSI, a large synthetic dataset,\ndatasets we collected from Kaggle and OpenML as well as MIMIC-Demo-Ext, a\nreal-world dataset containing patient information from critical care units.\n  Our findings reveal that detection performance varies significantly depending\non the dataset used for evaluation. CASSED excels on DeSSI, the dataset on\nwhich it was trained. Performance on the medical dataset MIMIC-Demo-Ext is\ncomparable across all models, with our GPT-4o-based approach clearly\noutperforming the others. Notably, personal data detection in the Kaggle and\nOpenML datasets appears to benefit from contextual information. This is\nevidenced by the poor performance of CASSED and Presidio (both of which do not\nutilize the context of the dataset) compared to the strong results of our\nGPT-4o-based approach.\n  We conclude that further progress in this field would greatly benefit from\nthe availability of more real-world datasets containing personal information.", "AI": {"tldr": "提出了一种基于GPT-4o的新方法，利用上下文信息检测结构化数据中的个人数据，性能优于现有方法。", "motivation": "现有方法在检测个人数据时缺乏对上下文信息的利用，导致性能受限。", "method": "结合特征名称、值、其他特征名称及数据集描述，利用GPT-4o进行检测。", "result": "在真实数据集MIMIC-Demo-Ext上表现最佳，上下文信息对Kaggle和OpenML数据集检测效果显著提升。", "conclusion": "未来研究需要更多包含个人信息的真实数据集以推动进展。"}}
{"id": "2506.22316", "pdf": "https://arxiv.org/pdf/2506.22316", "abs": "https://arxiv.org/abs/2506.22316", "authors": ["Qingquan Li", "Shaoyu Dou", "Kailai Shao", "Chao Chen", "Haixiang Hu"], "title": "Evaluating Scoring Bias in LLM-as-a-Judge", "categories": ["cs.CL"], "comment": null, "summary": "The remarkable performance of Large Language Models (LLMs) gives rise\nto``LLM-as-a-Judge'', where LLMs are employed as evaluators for complex tasks.\nMoreover, it has been widely adopted across fields such as Natural Language\nProcessing (NLP), preference learning, and various specific domains. However,\nthere are various biases within LLM-as-a-Judge, which adversely affect the\nfairness and reliability of judgments. Current research on evaluating or\nmitigating bias in LLM-as-a-Judge predominantly focuses on comparison-based\nevaluations, while systematic investigations into bias in scoring-based\nevaluations remain limited. Therefore, we define scoring bias in LLM-as-a-Judge\nas the scores differ when scoring judge models are bias-related perturbed, and\nprovide a well-designed framework to comprehensively evaluate scoring bias. We\naugment existing LLM-as-a-Judge benchmarks through data synthesis to construct\nour evaluation dataset and design multi-faceted evaluation metrics. Our\nexperimental results demonstrate that the scoring stability of existing judge\nmodels is disrupted by scoring biases. Further exploratory experiments and\ndiscussions provide valuable insights into the design of scoring prompt\ntemplates and the mitigation of scoring biases on aspects such as score\nrubrics, score IDs, and reference answer selection.", "AI": {"tldr": "论文探讨了LLM作为评估者（LLM-as-a-Judge）在评分任务中的偏见问题，提出了一个框架来评估和缓解这种偏见，并通过实验验证了现有评估模型的评分稳定性受偏见影响。", "motivation": "LLM作为评估者在复杂任务中广泛应用，但其评分偏见影响了公平性和可靠性，目前对评分偏见的系统性研究较少。", "method": "通过数据合成扩展现有基准数据集，设计多维度评估指标，定义评分偏见并构建评估框架。", "result": "实验表明现有评估模型的评分稳定性受偏见干扰，进一步探讨了评分提示模板设计和偏见缓解方法。", "conclusion": "研究为评分提示模板设计和偏见缓解提供了有价值的见解，强调了评分偏见对评估可靠性的影响。"}}
{"id": "2506.21860", "pdf": "https://arxiv.org/pdf/2506.21860", "abs": "https://arxiv.org/abs/2506.21860", "authors": ["Xiangyu Shi", "Yanyuan Qiao", "Lingqiao Liu", "Feras Dayoub"], "title": "Embodied Domain Adaptation for Object Detection", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted by IROS 2025", "summary": "Mobile robots rely on object detectors for perception and object localization\nin indoor environments. However, standard closed-set methods struggle to handle\nthe diverse objects and dynamic conditions encountered in real homes and labs.\nOpen-vocabulary object detection (OVOD), driven by Vision Language Models\n(VLMs), extends beyond fixed labels but still struggles with domain shifts in\nindoor environments. We introduce a Source-Free Domain Adaptation (SFDA)\napproach that adapts a pre-trained model without accessing source data. We\nrefine pseudo labels via temporal clustering, employ multi-scale threshold\nfusion, and apply a Mean Teacher framework with contrastive learning. Our\nEmbodied Domain Adaptation for Object Detection (EDAOD) benchmark evaluates\nadaptation under sequential changes in lighting, layout, and object diversity.\nOur experiments show significant gains in zero-shot detection performance and\nflexible adaptation to dynamic indoor conditions.", "AI": {"tldr": "论文提出了一种无需源数据的领域自适应方法（SFDA），通过时间聚类和多尺度阈值融合改进伪标签，结合Mean Teacher框架和对比学习，显著提升了开放词汇目标检测（OVOD）在动态室内环境中的性能。", "motivation": "解决标准封闭集方法和现有开放词汇目标检测在动态室内环境中因领域偏移而表现不佳的问题。", "method": "采用源数据无关的领域自适应（SFDA），通过时间聚类优化伪标签，多尺度阈值融合，以及Mean Teacher框架与对比学习结合。", "result": "在EDAOD基准测试中，零样本检测性能显著提升，能灵活适应动态室内环境变化。", "conclusion": "提出的方法有效解决了开放词汇目标检测在动态室内环境中的领域适应问题，具有实际应用潜力。"}}
{"id": "2506.21931", "pdf": "https://arxiv.org/pdf/2506.21931", "abs": "https://arxiv.org/abs/2506.21931", "authors": ["Reza Yousefi Maragheh", "Pratheek Vadla", "Priyank Gupta", "Kai Zhao", "Aysenur Inan", "Kehui Yao", "Jianpeng Xu", "Praveen Kanumala", "Jason Cho", "Sushant Kumar"], "title": "ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation", "categories": ["cs.IR", "cs.AI", "cs.CL", "cs.MA", "I.2.11; I.2.7; H.3.3"], "comment": null, "summary": "Retrieval-Augmented Generation (RAG) has shown promise in enhancing\nrecommendation systems by incorporating external context into large language\nmodel prompts. However, existing RAG-based approaches often rely on static\nretrieval heuristics and fail to capture nuanced user preferences in dynamic\nrecommendation scenarios. In this work, we introduce ARAG, an Agentic\nRetrieval-Augmented Generation framework for Personalized Recommendation, which\nintegrates a multi-agent collaboration mechanism into the RAG pipeline. To\nbetter understand the long-term and session behavior of the user, ARAG\nleverages four specialized LLM-based agents: a User Understanding Agent that\nsummarizes user preferences from long-term and session contexts, a Natural\nLanguage Inference (NLI) Agent that evaluates semantic alignment between\ncandidate items retrieved by RAG and inferred intent, a context summary agent\nthat summarizes the findings of NLI agent, and an Item Ranker Agent that\ngenerates a ranked list of recommendations based on contextual fit. We evaluate\nARAG accross three datasets. Experimental results demonstrate that ARAG\nsignificantly outperforms standard RAG and recency-based baselines, achieving\nup to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an\nablation study to analyse the effect by different components of ARAG. Our\nfindings highlight the effectiveness of integrating agentic reasoning into\nretrieval-augmented recommendation and provide new directions for LLM-based\npersonalization.", "AI": {"tldr": "ARAG框架通过多智能体协作增强RAG，显著提升个性化推荐效果。", "motivation": "现有RAG方法依赖静态检索，难以捕捉动态推荐场景中的用户偏好。", "method": "ARAG引入四种LLM智能体：用户理解、NLI、上下文摘要和项目排序，协作优化推荐。", "result": "实验显示ARAG在NDCG@5和Hit@5上分别提升42.1%和35.5%。", "conclusion": "智能体推理与RAG结合有效，为LLM个性化推荐提供新方向。"}}
{"id": "2506.22366", "pdf": "https://arxiv.org/pdf/2506.22366", "abs": "https://arxiv.org/abs/2506.22366", "authors": ["Daichi Kato", "Ryo Ueda", "Yusuke Miyao"], "title": "Why Are Parsing Actions for Understanding Message Hierarchies Not Random?", "categories": ["cs.CL"], "comment": null, "summary": "If humans understood language by randomly selecting parsing actions, it might\nhave been necessary to construct a robust symbolic system capable of being\ninterpreted under any hierarchical structure. However, human parsing strategies\ndo not seem to follow such a random pattern. Why is that the case? In fact, a\nprevious study on emergent communication using models with hierarchical biases\nhave reported that agents adopting random parsing\nstrategies$\\unicode{x2013}$ones that deviate significantly from human language\ncomprehension$\\unicode{x2013}$can achieve high communication accuracy. In this\nstudy, we investigate this issue by making two simple and natural modifications\nto the experimental setup: (I) we use more complex inputs that have\nhierarchical structures, such that random parsing makes semantic interpretation\nmore difficult, and (II) we incorporate a surprisal-related term, which is\nknown to influence the order of words and characters in natural language, into\nthe objective function. With these changes, we evaluate whether agents\nemploying random parsing strategies still maintain high communication accuracy.", "AI": {"tldr": "研究探讨了人类语言理解中随机解析策略的有效性，并通过实验修改验证其影响。", "motivation": "人类语言理解并非随机解析，但此前研究表明随机策略也能实现高通信准确性，研究者试图验证这一现象。", "method": "通过引入更复杂的层次结构输入和加入与意外性相关的目标函数，评估随机解析策略的效果。", "result": "实验结果表明，随机解析策略在高复杂度输入和意外性目标下是否仍能保持高准确性。", "conclusion": "研究为理解人类语言解析策略提供了新视角，并探讨了随机策略的局限性。"}}
{"id": "2506.22396", "pdf": "https://arxiv.org/pdf/2506.22396", "abs": "https://arxiv.org/abs/2506.22396", "authors": ["Danush Khanna", "Aditya Kumar Guru", "Srivarshinee Sridhar", "Zidan Ahmed", "Rubhav Bahirwani", "Meetu Malhotra", "Vinija Jain", "Aman Chadha", "Amitava Das", "Kripabandhu Ghosh"], "title": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization", "categories": ["cs.CL", "cs.AI", "I.2.0; I.2.7"], "comment": "Preprint. Under submission", "summary": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2).", "AI": {"tldr": "QuickSilver是一个模块化的推理框架，通过动态令牌停止、KV缓存跳过和上下文令牌融合等机制，显著减少LLM推理时的计算开销，无需修改模型权重或结构。", "motivation": "LLM推理时的高延迟和能耗是主要瓶颈，现有方法通常需要重新训练或架构调整，QuickSilver旨在提供一种无需修改模型的优化方案。", "method": "QuickSilver结合动态令牌停止、KV缓存跳过和上下文令牌融合三种机制，在推理时实现语义自适应。", "result": "在GPT-2和Llama-2上，QuickSilver实现了高达39.6%的FLOP减少，且困惑度几乎无影响（≤0.2）。", "conclusion": "QuickSilver为LLM推理提供了一种高效、兼容性强的优化方法，适用于现有密集模型。"}}
{"id": "2506.21880", "pdf": "https://arxiv.org/pdf/2506.21880", "abs": "https://arxiv.org/abs/2506.21880", "authors": ["Yuansheng Li", "Yunhao Zou", "Linwei Chen", "Ying Fu"], "title": "Physical Degradation Model-Guided Interferometric Hyperspectral Reconstruction with Unfolding Transformer", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Interferometric Hyperspectral Imaging (IHI) is a critical technique for\nlarge-scale remote sensing tasks due to its advantages in flux and spectral\nresolution. However, IHI is susceptible to complex errors arising from imaging\nsteps, and its quality is limited by existing signal processing-based\nreconstruction algorithms. Two key challenges hinder performance enhancement:\n1) the lack of training datasets. 2) the difficulty in eliminating IHI-specific\ndegradation components through learning-based methods. To address these\nchallenges, we propose a novel IHI reconstruction pipeline. First, based on\nimaging physics and radiometric calibration data, we establish a simplified yet\naccurate IHI degradation model and a parameter estimation method. This model\nenables the synthesis of realistic IHI training datasets from hyperspectral\nimages (HSIs), bridging the gap between IHI reconstruction and deep learning.\nSecond, we design the Interferometric Hyperspectral Reconstruction Unfolding\nTransformer (IHRUT), which achieves effective spectral correction and detail\nrestoration through a stripe-pattern enhancement mechanism and a\nspatial-spectral transformer architecture. Experimental results demonstrate the\nsuperior performance and generalization capability of our method.", "AI": {"tldr": "提出了一种基于物理模型和深度学习的干涉高光谱成像（IHI）重建方法，解决了训练数据不足和IHI特有退化问题。", "motivation": "IHI在遥感任务中具有优势，但受限于复杂误差和现有信号处理算法的性能。缺乏训练数据和难以消除IHI特有退化是主要挑战。", "method": "1) 基于成像物理和辐射校准数据建立简化的IHI退化模型，生成训练数据；2) 设计IHRUT模型，结合条纹增强机制和空间-光谱变换器架构。", "result": "实验表明该方法具有优越的性能和泛化能力。", "conclusion": "提出的方法有效解决了IHI重建中的关键问题，为深度学习在IHI中的应用提供了新思路。"}}
{"id": "2506.21964", "pdf": "https://arxiv.org/pdf/2506.21964", "abs": "https://arxiv.org/abs/2506.21964", "authors": ["Michael A. Riegler", "Kristoffer Herland Hellton", "Vajira Thambawita", "Hugo L. Hammer"], "title": "Using Large Language Models to Suggest Informative Prior Distributions in Bayesian Statistics", "categories": ["stat.ME", "cs.AI", "cs.CL"], "comment": null, "summary": "Selecting prior distributions in Bayesian statistics is challenging,\nresource-intensive, and subjective. We analyze using large-language models\n(LLMs) to suggest suitable, knowledge-based informative priors. We developed an\nextensive prompt asking LLMs not only to suggest priors but also to verify and\nreflect on their choices.\n  We evaluated Claude Opus, Gemini 2.5 Pro, and ChatGPT-4o-mini on two real\ndatasets: heart disease risk and concrete strength. All LLMs correctly\nidentified the direction for all associations (e.g., that heart disease risk is\nhigher for males). The quality of suggested priors was measured by their\nKullback-Leibler divergence from the maximum likelihood estimator's\ndistribution.\n  The LLMs suggested both moderately and weakly informative priors. The\nmoderate priors were often overconfident, resulting in distributions misaligned\nwith the data. In our experiments, Claude and Gemini provided better priors\nthan ChatGPT. For weakly informative priors, a key performance difference\nemerged: ChatGPT and Gemini defaulted to an \"unnecessarily vague\" mean of 0,\nwhile Claude did not, demonstrating a significant advantage.\n  The ability of LLMs to identify correct associations shows their great\npotential as an efficient, objective method for developing informative priors.\nHowever, the primary challenge remains in calibrating the width of these priors\nto avoid over- and under-confidence.", "AI": {"tldr": "论文探讨了使用大语言模型（LLMs）为贝叶斯统计选择先验分布的方法，评估了Claude、Gemini和ChatGPT在真实数据集上的表现。", "motivation": "贝叶斯统计中选择先验分布具有挑战性、资源密集且主观性强，研究旨在利用LLMs提供高效、客观的建议。", "method": "通过设计详细的提示词，要求LLMs不仅建议先验分布，还验证和反思其选择，并在心脏疾病风险和混凝土强度数据集上评估模型表现。", "result": "LLMs能正确识别关联方向，但中等信息先验常过于自信，Claude和Gemini表现优于ChatGPT。弱信息先验中，Claude避免不必要的模糊性，表现更优。", "conclusion": "LLMs在高效、客观地生成信息先验方面潜力巨大，但需校准先验宽度以避免过度自信或不足。"}}
{"id": "2506.22402", "pdf": "https://arxiv.org/pdf/2506.22402", "abs": "https://arxiv.org/abs/2506.22402", "authors": ["Petr Pechman", "Milan Straka", "Jana Straková", "Jakub Náplava"], "title": "Refining Czech GEC: Insights from a Multi-Experiment Approach", "categories": ["cs.CL"], "comment": "Accepted to TSD 2025", "summary": "We present a grammar error correction (GEC) system that achieves state of the\nart for the Czech language. Our system is based on a neural network translation\napproach with the Transformer architecture, and its key feature is its\nreal-time synthetic generation pipeline, which dynamically augments sentences\nwith artificial errors by introducing both language-agnostic and Czech-specific\nerrors. We conduct a comprehensive series of experiments, investigating the\nCzech GEC corpora as bases for synthetic error introduction, several error\ngeneration strategies, domain balancing, tokenization granularity, model size,\nand data scaling during fine-tuning. Additionally, we evaluate the performance\nof large language models (LLMs) on Czech GEC in both end-user and expert\nfine-tuning scenarios. Our best-performing model is superior both in\nperformance and computational efficiency. The source code and the trained model\nlinks are available on https://github.com/ufal/tsd2025-gec.", "AI": {"tldr": "提出了一种基于Transformer架构的捷克语语法纠错系统，通过动态生成合成错误实现高效性能。", "motivation": "提升捷克语语法纠错的性能，探索合成错误生成和模型优化的方法。", "method": "使用神经网络翻译方法，结合动态合成错误生成管道，研究多种错误生成策略和模型参数。", "result": "最佳模型在性能和计算效率上均表现优异，并公开了源代码和模型。", "conclusion": "该系统为捷克语语法纠错提供了高效解决方案，并验证了合成错误生成的有效性。"}}
{"id": "2506.22403", "pdf": "https://arxiv.org/pdf/2506.22403", "abs": "https://arxiv.org/abs/2506.22403", "authors": ["NAVER Cloud HyperCLOVA X Team"], "title": "HyperCLOVA X THINK Technical Report", "categories": ["cs.CL", "cs.AI"], "comment": "49 pages, 13 figures", "summary": "We introduce HyperCLOVA X THINK, the first reasoning-focused large language\nmodel in the HyperCLOVA X family, pre-trained on roughly $6$ trillion\nhigh-quality Korean, and English tokens, augmented with targeted synthetic\nKorean data. It was implemented as a compute-memory-balanced Peri-LN\nTransformer scaled with $\\mu$P, pre-trained through a three-stage curriculum\nthat expands the context window to $128$K tokens, and post-trained via\nsupervised fine-tuning with Reinforcement Learning from Verifiable Rewards\nsupports both detailed rationale and concise-answer modes. It delivers\ncompetitive performance against similarly sized models on Korea-focused\nbenchmarks such as KMMLU, CSAT, KoBALT-700, HAERAE-1.0, and KoBigBench, while\npreserving robust bilingual consistency and translation quality. In addition, a\nvision-augmented variant matches or exceeds GPT-4.1 on the KCSAT STEM\nbenchmark, all of which are achieved with substantially lower training compute\nthan existing models of similar sizes. We also present a pruning and\ndistillation technique that will soon be applied to HyperCLOVA X THINK for an\nopen-source and business-friendly foundation model. Altogether, these\ncapabilities position HyperCLOVA X THINK as a robust foundation for Korean AI\ninnovation and a valuable resource for the global research community.", "AI": {"tldr": "HyperCLOVA X THINK是首个专注于推理的大语言模型，基于6万亿高质量韩语和英语标记预训练，支持128K上下文窗口，性能优于同类模型，且训练计算成本更低。", "motivation": "开发一个专注于推理、支持双语（韩语和英语）的大语言模型，以推动韩国AI创新并为全球研究社区提供资源。", "method": "采用Peri-LN Transformer架构，通过三阶段课程学习扩展上下文窗口，结合监督微调和强化学习进行后训练。", "result": "在韩国基准测试中表现优异，视觉增强版本在KCSAT STEM上媲美GPT-4.1，且训练计算成本更低。", "conclusion": "HyperCLOVA X THINK是一个强大的韩国AI创新基础模型，未来将通过剪枝和蒸馏技术进一步优化。"}}
{"id": "2506.21934", "pdf": "https://arxiv.org/pdf/2506.21934", "abs": "https://arxiv.org/abs/2506.21934", "authors": ["Najmeh Forouzandehmehr", "Reza Yousefi Maragheh", "Sriram Kollipara", "Kai Zhao", "Topojoy Biswas", "Evren Korpeoglu", "Kannan Achan"], "title": "CAL-RAG: Retrieval-Augmented Multi-Agent Generation for Content-Aware Layout Design", "categories": ["cs.IR", "cs.CV", "I.3.3; I.2.11; H.5.2"], "comment": null, "summary": "Automated content-aware layout generation -- the task of arranging visual\nelements such as text, logos, and underlays on a background canvas -- remains a\nfundamental yet under-explored problem in intelligent design systems. While\nrecent advances in deep generative models and large language models (LLMs) have\nshown promise in structured content generation, most existing approaches lack\ngrounding in contextual design exemplars and fall short in handling semantic\nalignment and visual coherence. In this work we introduce CAL-RAG, a\nretrieval-augmented, agentic framework for content-aware layout generation that\nintegrates multimodal retrieval, large language models, and collaborative\nagentic reasoning. Our system retrieves relevant layout examples from a\nstructured knowledge base and invokes an LLM-based layout recommender to\npropose structured element placements. A vision-language grader agent evaluates\nthe layout with visual metrics, and a feedback agent provides targeted\nrefinements, enabling iterative improvement. We implement our framework using\nLangGraph and evaluate it on the PKU PosterLayout dataset, a benchmark rich in\nsemantic and structural variability. CAL-RAG achieves state-of-the-art\nperformance across multiple layout metrics -- including underlay effectiveness,\nelement alignment, and overlap -- substantially outperforming strong baselines\nsuch as LayoutPrompter. These results demonstrate that combining retrieval\naugmentation with agentic multi-step reasoning yields a scalable,\ninterpretable, and high-fidelity solution for automated layout generation.", "AI": {"tldr": "CAL-RAG是一个基于检索增强和代理推理的框架，用于内容感知的布局生成，结合多模态检索和LLM，显著优于现有方法。", "motivation": "现有深度生成模型和LLM在布局生成中缺乏上下文设计范例的支撑，语义对齐和视觉连贯性不足。", "method": "CAL-RAG通过检索相关布局范例，利用LLM推荐布局，视觉语言评分代理评估，反馈代理迭代优化。", "result": "在PKU PosterLayout数据集上，CAL-RAG在多个布局指标上达到最优性能，显著优于基线方法。", "conclusion": "检索增强与多步代理推理结合为自动布局生成提供了可扩展、可解释且高保真的解决方案。"}}
{"id": "2506.21976", "pdf": "https://arxiv.org/pdf/2506.21976", "abs": "https://arxiv.org/abs/2506.21976", "authors": ["Shuhan Tan", "John Lambert", "Hong Jeon", "Sakshum Kulshrestha", "Yijing Bai", "Jing Luo", "Dragomir Anguelov", "Mingxing Tan", "Chiyu Max Jiang"], "title": "SceneDiffuser++: City-Scale Traffic Simulation via a Generative World Model", "categories": ["cs.LG", "cs.AI", "cs.CV", "cs.MA", "cs.RO"], "comment": "Accepted to CVPR 2025", "summary": "The goal of traffic simulation is to augment a potentially limited amount of\nmanually-driven miles that is available for testing and validation, with a much\nlarger amount of simulated synthetic miles. The culmination of this vision\nwould be a generative simulated city, where given a map of the city and an\nautonomous vehicle (AV) software stack, the simulator can seamlessly simulate\nthe trip from point A to point B by populating the city around the AV and\ncontrolling all aspects of the scene, from animating the dynamic agents (e.g.,\nvehicles, pedestrians) to controlling the traffic light states. We refer to\nthis vision as CitySim, which requires an agglomeration of simulation\ntechnologies: scene generation to populate the initial scene, agent behavior\nmodeling to animate the scene, occlusion reasoning, dynamic scene generation to\nseamlessly spawn and remove agents, and environment simulation for factors such\nas traffic lights. While some key technologies have been separately studied in\nvarious works, others such as dynamic scene generation and environment\nsimulation have received less attention in the research community. We propose\nSceneDiffuser++, the first end-to-end generative world model trained on a\nsingle loss function capable of point A-to-B simulation on a city scale\nintegrating all the requirements above. We demonstrate the city-scale traffic\nsimulation capability of SceneDiffuser++ and study its superior realism under\nlong simulation conditions. We evaluate the simulation quality on an augmented\nversion of the Waymo Open Motion Dataset (WOMD) with larger map regions to\nsupport trip-level simulation.", "AI": {"tldr": "论文提出CitySim愿景，通过SceneDiffuser++实现端到端生成式城市规模交通模拟，整合场景生成、动态代理行为建模等技术。", "motivation": "解决交通模拟中手动驾驶里程有限的问题，通过生成式模拟技术实现大规模城市级交通场景的模拟。", "method": "提出SceneDiffuser++，一种基于单一损失函数的端到端生成式世界模型，整合场景生成、动态代理行为建模等技术。", "result": "在增强版Waymo Open Motion Dataset上验证了SceneDiffuser++的城市规模模拟能力，展示了其长期模拟的优越真实性。", "conclusion": "SceneDiffuser++为城市级交通模拟提供了高效且真实的解决方案，填补了动态场景生成和环境模拟等领域的研究空白。"}}
{"id": "2506.22405", "pdf": "https://arxiv.org/pdf/2506.22405", "abs": "https://arxiv.org/abs/2506.22405", "authors": ["Harsha Nori", "Mayank Daswani", "Christopher Kelly", "Scott Lundberg", "Marco Tulio Ribeiro", "Marc Wilson", "Xiaoxuan Liu", "Viknesh Sounderajah", "Jonathan Carlson", "Matthew P Lungren", "Bay Gross", "Peter Hames", "Mustafa Suleyman", "Dominic King", "Eric Horvitz"], "title": "Sequential Diagnosis with Language Models", "categories": ["cs.CL"], "comment": "23 pages, 10 figures", "summary": "Artificial intelligence holds great promise for expanding access to expert\nmedical knowledge and reasoning. However, most evaluations of language models\nrely on static vignettes and multiple-choice questions that fail to reflect the\ncomplexity and nuance of evidence-based medicine in real-world settings. In\nclinical practice, physicians iteratively formulate and revise diagnostic\nhypotheses, adapting each subsequent question and test to what they've just\nlearned, and weigh the evolving evidence before committing to a final\ndiagnosis. To emulate this iterative process, we introduce the Sequential\nDiagnosis Benchmark, which transforms 304 diagnostically challenging New\nEngland Journal of Medicine clinicopathological conference (NEJM-CPC) cases\ninto stepwise diagnostic encounters. A physician or AI begins with a short case\nabstract and must iteratively request additional details from a gatekeeper\nmodel that reveals findings only when explicitly queried. Performance is\nassessed not just by diagnostic accuracy but also by the cost of physician\nvisits and tests performed. We also present the MAI Diagnostic Orchestrator\n(MAI-DxO), a model-agnostic orchestrator that simulates a panel of physicians,\nproposes likely differential diagnoses and strategically selects high-value,\ncost-effective tests. When paired with OpenAI's o3 model, MAI-DxO achieves 80%\ndiagnostic accuracy--four times higher than the 20% average of generalist\nphysicians. MAI-DxO also reduces diagnostic costs by 20% compared to\nphysicians, and 70% compared to off-the-shelf o3. When configured for maximum\naccuracy, MAI-DxO achieves 85.5% accuracy. These performance gains with MAI-DxO\ngeneralize across models from the OpenAI, Gemini, Claude, Grok, DeepSeek, and\nLlama families. We highlight how AI systems, when guided to think iteratively\nand act judiciously, can advance diagnostic precision and cost-effectiveness in\nclinical care.", "AI": {"tldr": "论文提出了一种模拟临床诊断迭代过程的基准测试（Sequential Diagnosis Benchmark）和模型无关的诊断协调器（MAI-DxO），显著提高了诊断准确性和成本效益。", "motivation": "现有语言模型的评估方法过于静态，无法反映真实临床诊断的复杂性和迭代性。", "method": "通过转换304个NEJM-CPC病例为逐步诊断任务，并开发MAI-DxO协调器模拟医生团队，优化测试选择和诊断流程。", "result": "MAI-DxO与OpenAI o3模型结合时，诊断准确率达80%，成本降低20%；最高配置下准确率达85.5%。", "conclusion": "AI系统在迭代思考和明智行动下，可显著提升临床诊断的精确性和成本效益。"}}
{"id": "2506.21977", "pdf": "https://arxiv.org/pdf/2506.21977", "abs": "https://arxiv.org/abs/2506.21977", "authors": ["Tianyu Zhang", "Xin Luo", "Li Li", "Dong Liu"], "title": "StableCodec: Taming One-Step Diffusion for Extreme Image Compression", "categories": ["eess.IV", "cs.CV"], "comment": null, "summary": "Diffusion-based image compression has shown remarkable potential for\nachieving ultra-low bitrate coding (less than 0.05 bits per pixel) with high\nrealism, by leveraging the generative priors of large pre-trained text-to-image\ndiffusion models. However, current approaches require a large number of\ndenoising steps at the decoder to generate realistic results under extreme\nbitrate constraints, limiting their application in real-time compression\nscenarios. Additionally, these methods often sacrifice reconstruction fidelity,\nas diffusion models typically fail to guarantee pixel-level consistency. To\naddress these challenges, we introduce StableCodec, which enables one-step\ndiffusion for high-fidelity and high-realism extreme image compression with\nimproved coding efficiency. To achieve ultra-low bitrates, we first develop an\nefficient Deep Compression Latent Codec to transmit a noisy latent\nrepresentation for a single-step denoising process. We then propose a\nDual-Branch Coding Structure, consisting of a pair of auxiliary encoder and\ndecoder, to enhance reconstruction fidelity. Furthermore, we adopt end-to-end\noptimization with joint bitrate and pixel-level constraints. Extensive\nexperiments on the CLIC 2020, DIV2K, and Kodak dataset demonstrate that\nStableCodec outperforms existing methods in terms of FID, KID and DISTS by a\nsignificant margin, even at bitrates as low as 0.005 bits per pixel, while\nmaintaining strong fidelity. Additionally, StableCodec achieves inference\nspeeds comparable to mainstream transform coding schemes. All source code are\navailable at https://github.com/LuizScarlet/StableCodec.", "AI": {"tldr": "StableCodec提出了一种基于扩散模型的高效图像压缩方法，通过单步去噪和双分支编码结构，在超低比特率下实现高保真和高真实感。", "motivation": "当前基于扩散模型的图像压缩方法在超低比特率下需要大量去噪步骤，且难以保证像素级一致性，限制了实时应用。", "method": "提出Deep Compression Latent Codec传输噪声潜在表示，采用Dual-Branch Coding结构增强重建保真度，并通过端到端优化联合比特率和像素级约束。", "result": "在CLIC 2020、DIV2K和Kodak数据集上，StableCodec在FID、KID和DISTS指标上显著优于现有方法，比特率低至0.005 bpp。", "conclusion": "StableCodec在超低比特率下实现了高效、高保真和高真实感的图像压缩，推理速度与主流变换编码方案相当。"}}
{"id": "2506.21997", "pdf": "https://arxiv.org/pdf/2506.21997", "abs": "https://arxiv.org/abs/2506.21997", "authors": ["Rafael Sojo", "Javier Díaz-Rozo", "Concha Bielza", "Pedro Larrañaga"], "title": "Binned semiparametric Bayesian networks", "categories": ["cs.LG", "cs.AI", "I.2.6; I.5.1; G.3"], "comment": null, "summary": "This paper introduces a new type of probabilistic semiparametric model that\ntakes advantage of data binning to reduce the computational cost of kernel\ndensity estimation in nonparametric distributions. Two new conditional\nprobability distributions are developed for the new binned semiparametric\nBayesian networks, the sparse binned kernel density estimation and the Fourier\nkernel density estimation. These two probability distributions address the\ncurse of dimensionality, which typically impacts binned models, by using sparse\ntensors and restricting the number of parent nodes in conditional probability\ncalculations. To evaluate the proposal, we perform a complexity analysis and\nconduct several comparative experiments using synthetic data and datasets from\nthe UCI Machine Learning repository. The experiments include different binning\nrules, parent restrictions, grid sizes, and number of instances to get a\nholistic view of the model's behavior. As a result, our binned semiparametric\nBayesian networks achieve structural learning and log-likelihood estimations\nwith no statistically significant differences compared to the semiparametric\nBayesian networks, but at a much higher speed. Thus, the new binned\nsemiparametric Bayesian networks prove to be a reliable and more efficient\nalternative to their non-binned counterparts.", "AI": {"tldr": "提出了一种新的概率半参数模型，通过数据分箱降低核密度估计的计算成本，解决了维度灾难问题，并在实验中表现出高效性。", "motivation": "传统核密度估计在非参数分布中计算成本高，且维度灾难问题严重，因此需要一种更高效的方法。", "method": "开发了两种新的条件概率分布（稀疏分箱核密度估计和傅里叶核密度估计），利用稀疏张量和限制父节点数量来解决维度问题。", "result": "实验表明，分箱半参数贝叶斯网络在结构学习和对数似然估计上与未分箱版本无显著差异，但速度更快。", "conclusion": "分箱半参数贝叶斯网络是一种可靠且高效的替代方案。"}}
{"id": "2506.22012", "pdf": "https://arxiv.org/pdf/2506.22012", "abs": "https://arxiv.org/abs/2506.22012", "authors": ["Qi Gao", "Zhihao Chen", "Dong Zeng", "Junping Zhang", "Jianhua Ma", "Hongming Shan"], "title": "Noise-Inspired Diffusion Model for Generalizable Low-Dose CT Reconstruction", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted for publication in Medical Image Analysis, 2025", "summary": "The generalization of deep learning-based low-dose computed tomography (CT)\nreconstruction models to doses unseen in the training data is important and\nremains challenging. Previous efforts heavily rely on paired data to improve\nthe generalization performance and robustness through collecting either diverse\nCT data for re-training or a few test data for fine-tuning. Recently, diffusion\nmodels have shown promising and generalizable performance in low-dose CT (LDCT)\nreconstruction, however, they may produce unrealistic structures due to the CT\nimage noise deviating from Gaussian distribution and imprecise prior\ninformation from the guidance of noisy LDCT images. In this paper, we propose a\nnoise-inspired diffusion model for generalizable LDCT reconstruction, termed\nNEED, which tailors diffusion models for noise characteristics of each domain.\nFirst, we propose a novel shifted Poisson diffusion model to denoise projection\ndata, which aligns the diffusion process with the noise model in pre-log LDCT\nprojections. Second, we devise a doubly guided diffusion model to refine\nreconstructed images, which leverages LDCT images and initial reconstructions\nto more accurately locate prior information and enhance reconstruction\nfidelity. By cascading these two diffusion models for dual-domain\nreconstruction, our NEED requires only normal-dose data for training and can be\neffectively extended to various unseen dose levels during testing via a time\nstep matching strategy. Extensive qualitative, quantitative, and\nsegmentation-based evaluations on two datasets demonstrate that our NEED\nconsistently outperforms state-of-the-art methods in reconstruction and\ngeneralization performance. Source code is made available at\nhttps://github.com/qgao21/NEED.", "AI": {"tldr": "论文提出了一种名为NEED的噪声启发扩散模型，用于低剂量CT（LDCT）重建，通过双域扩散模型提升泛化性能。", "motivation": "解决深度学习模型在未见剂量数据上的泛化问题，避免依赖配对数据或重新训练。", "method": "提出移位泊松扩散模型去噪投影数据，并设计双引导扩散模型优化重建图像。", "result": "在多个数据集上，NEED在重建和泛化性能上优于现有方法。", "conclusion": "NEED仅需正常剂量数据训练，可泛化至不同剂量水平，性能优越。"}}
{"id": "2506.22008", "pdf": "https://arxiv.org/pdf/2506.22008", "abs": "https://arxiv.org/abs/2506.22008", "authors": ["Alessandro Sestini", "Joakim Bergdahl", "Konrad Tollmar", "Andrew D. Bagdanov", "Linus Gisslén"], "title": "TROFI: Trajectory-Ranked Offline Inverse Reinforcement Learning", "categories": ["cs.LG", "cs.AI"], "comment": "Published at Reinforcement Learning and Video Games Workshop at RLC\n  2025", "summary": "In offline reinforcement learning, agents are trained using only a fixed set\nof stored transitions derived from a source policy. However, this requires that\nthe dataset be labeled by a reward function. In applied settings such as video\ngame development, the availability of the reward function is not always\nguaranteed. This paper proposes Trajectory-Ranked OFfline Inverse reinforcement\nlearning (TROFI), a novel approach to effectively learn a policy offline\nwithout a pre-defined reward function. TROFI first learns a reward function\nfrom human preferences, which it then uses to label the original dataset making\nit usable for training the policy. In contrast to other approaches, our method\ndoes not require optimal trajectories. Through experiments on the D4RL\nbenchmark we demonstrate that TROFI consistently outperforms baselines and\nperforms comparably to using the ground truth reward to learn policies.\nAdditionally, we validate the efficacy of our method in a 3D game environment.\nOur studies of the reward model highlight the importance of the reward function\nin this setting: we show that to ensure the alignment of a value function to\nthe actual future discounted reward, it is fundamental to have a\nwell-engineered and easy-to-learn reward function.", "AI": {"tldr": "TROFI是一种离线强化学习方法，通过从人类偏好中学习奖励函数，无需预定义奖励函数即可训练策略，并在实验中表现优于基线方法。", "motivation": "在离线强化学习中，传统方法需要预定义奖励函数，但在实际应用中（如游戏开发）奖励函数可能不可用。TROFI旨在解决这一问题。", "method": "TROFI首先从人类偏好中学习奖励函数，然后用其标注原始数据集以训练策略，且无需最优轨迹。", "result": "在D4RL基准测试中，TROFI表现优于基线方法，并与使用真实奖励函数的方法相当。在3D游戏环境中也验证了其有效性。", "conclusion": "研究表明，奖励函数的设计对值函数与实际未来奖励的对齐至关重要，TROFI为无预定义奖励函数的离线强化学习提供了有效解决方案。"}}
{"id": "2506.21825", "pdf": "https://arxiv.org/pdf/2506.21825", "abs": "https://arxiv.org/abs/2506.21825", "authors": ["Abdulkareem Alsudais"], "title": "Exploring the change in scientific readability following the release of ChatGPT", "categories": ["cs.CY", "cs.CL"], "comment": null, "summary": "The rise and growing popularity of accessible large language models have\nraised questions about their impact on various aspects of life, including how\nscientists write and publish their research. The primary objective of this\npaper is to analyze a dataset consisting of all abstracts posted on arXiv.org\nbetween 2010 and June 7th, 2024, to assess the evolution of their readability\nand determine whether significant shifts occurred following the release of\nChatGPT in November 2022. Four standard readability formulas are used to\ncalculate individual readability scores for each paper, classifying their level\nof readability. These scores are then aggregated by year and across the eight\nprimary categories covered by the platform. The results show a steady annual\ndecrease in readability, suggesting that abstracts are likely becoming\nincreasingly complex. Additionally, following the release of ChatGPT, a\nsignificant change in readability is observed for 2023 and the analyzed months\nof 2024. Similar trends are found across categories, with most experiencing a\nnotable change in readability during 2023 and 2024. These findings offer\ninsights into the broader changes in readability and point to the likely\ninfluence of AI on scientific writing.", "AI": {"tldr": "分析arXiv.org上2010年至2024年6月7日所有摘要的可读性变化，发现ChatGPT发布后（2022年11月）可读性显著下降。", "motivation": "研究大型语言模型（如ChatGPT）对科学写作可读性的影响。", "method": "使用四种标准可读性公式计算arXiv摘要的可读性分数，按年份和学科分类分析。", "result": "摘要可读性逐年下降，ChatGPT发布后2023年和2024年变化显著。", "conclusion": "AI可能影响了科学写作的可读性，使其变得更复杂。"}}
{"id": "2506.22041", "pdf": "https://arxiv.org/pdf/2506.22041", "abs": "https://arxiv.org/abs/2506.22041", "authors": ["Julia Machnio", "Sebastian Nørgaard Llambias", "Mads Nielsen", "Mostafa Mehdipour Ghazi"], "title": "Towards Scalable and Robust White Matter Lesion Localization via Multimodal Deep Learning", "categories": ["eess.IV", "cs.CV"], "comment": "2nd Sorbonne-Heidelberg Workshop on AI in medicine: Machine Learning\n  for multi-modal data", "summary": "White matter hyperintensities (WMH) are radiological markers of small vessel\ndisease and neurodegeneration, whose accurate segmentation and spatial\nlocalization are crucial for diagnosis and monitoring. While multimodal MRI\noffers complementary contrasts for detecting and contextualizing WM lesions,\nexisting approaches often lack flexibility in handling missing modalities and\nfail to integrate anatomical localization efficiently. We propose a deep\nlearning framework for WM lesion segmentation and localization that operates\ndirectly in native space using single- and multi-modal MRI inputs. Our study\nevaluates four input configurations: FLAIR-only, T1-only, concatenated FLAIR\nand T1, and a modality-interchangeable setup. It further introduces a\nmulti-task model for jointly predicting lesion and anatomical region masks to\nestimate region-wise lesion burden. Experiments conducted on the MICCAI WMH\nSegmentation Challenge dataset demonstrate that multimodal input significantly\nimproves the segmentation performance, outperforming unimodal models. While the\nmodality-interchangeable setting trades accuracy for robustness, it enables\ninference in cases with missing modalities. Joint lesion-region segmentation\nusing multi-task learning was less effective than separate models, suggesting\nrepresentational conflict between tasks. Our findings highlight the utility of\nmultimodal fusion for accurate and robust WMH analysis, and the potential of\njoint modeling for integrated predictions.", "AI": {"tldr": "本文提出了一种深度学习方法，用于白质高信号（WMH）的分割和定位，支持单模态和多模态MRI输入，并评估了不同输入配置的性能。", "motivation": "白质高信号是神经退行性疾病的标志，准确分割和定位对诊断和监测至关重要，但现有方法在处理缺失模态和整合解剖定位方面缺乏灵活性。", "method": "提出了一种深度学习方法，支持单模态和多模态MRI输入，并评估了四种输入配置（FLAIR-only、T1-only、FLAIR+T1、模态可互换）。还引入了多任务模型联合预测病变和解剖区域掩膜。", "result": "实验表明，多模态输入显著提高了分割性能，优于单模态模型。模态可互换设置牺牲了准确性但提高了鲁棒性。多任务学习在联合分割中效果不如单独模型。", "conclusion": "多模态融合对WMH分析具有准确性和鲁棒性，联合建模在集成预测中具有潜力。"}}
{"id": "2506.22026", "pdf": "https://arxiv.org/pdf/2506.22026", "abs": "https://arxiv.org/abs/2506.22026", "authors": ["Simra Shahid", "Marissa Radensky", "Raymond Fok", "Pao Siangliulue", "Daniel S. Weld", "Tom Hope"], "title": "Literature-Grounded Novelty Assessment of Scientific Ideas", "categories": ["cs.IR", "cs.AI", "I.2; H.3"], "comment": null, "summary": "Automated scientific idea generation systems have made remarkable progress,\nyet the automatic evaluation of idea novelty remains a critical and\nunderexplored challenge. Manual evaluation of novelty through literature review\nis labor-intensive, prone to error due to subjectivity, and impractical at\nscale. To address these issues, we propose the Idea Novelty Checker, an\nLLM-based retrieval-augmented generation (RAG) framework that leverages a\ntwo-stage retrieve-then-rerank approach. The Idea Novelty Checker first\ncollects a broad set of relevant papers using keyword and snippet-based\nretrieval, then refines this collection through embedding-based filtering\nfollowed by facet-based LLM re-ranking. It incorporates expert-labeled examples\nto guide the system in comparing papers for novelty evaluation and in\ngenerating literature-grounded reasoning. Our extensive experiments demonstrate\nthat our novelty checker achieves approximately 13% higher agreement than\nexisting approaches. Ablation studies further showcases the importance of the\nfacet-based re-ranker in identifying the most relevant literature for novelty\nevaluation.", "AI": {"tldr": "提出了一种基于LLM的检索增强生成框架（Idea Novelty Checker），用于自动评估科学想法的创新性，通过两阶段检索和重排序方法，显著提高了新颖性评估的准确性。", "motivation": "手动评估科学想法的创新性耗时且主观，现有方法难以规模化。因此，需要一种自动化的高效评估工具。", "method": "采用两阶段的检索增强生成框架：1）基于关键词和片段检索相关论文；2）通过嵌入过滤和基于方面的LLM重排序优化结果。结合专家标注数据指导新颖性评估。", "result": "实验表明，该方法比现有方法在评估新颖性上提高了约13%的一致性，且基于方面的重排序对识别相关文献至关重要。", "conclusion": "Idea Novelty Checker为科学创新性评估提供了一种高效、自动化的解决方案，显著优于现有方法。"}}
{"id": "2506.22116", "pdf": "https://arxiv.org/pdf/2506.22116", "abs": "https://arxiv.org/abs/2506.22116", "authors": ["Noora Sassali", "Roel Pieters"], "title": "Evaluating Pointing Gestures for Target Selection in Human-Robot Collaboration", "categories": ["cs.RO", "cs.CV"], "comment": "Accepted by the 2025 34th IEEE International Conference on Robot and\n  Human Interactive Communication (RO-MAN). Preprint", "summary": "Pointing gestures are a common interaction method used in Human-Robot\nCollaboration for various tasks, ranging from selecting targets to guiding\nindustrial processes. This study introduces a method for localizing pointed\ntargets within a planar workspace. The approach employs pose estimation, and a\nsimple geometric model based on shoulder-wrist extension to extract gesturing\ndata from an RGB-D stream. The study proposes a rigorous methodology and\ncomprehensive analysis for evaluating pointing gestures and target selection in\ntypical robotic tasks. In addition to evaluating tool accuracy, the tool is\nintegrated into a proof-of-concept robotic system, which includes object\ndetection, speech transcription, and speech synthesis to demonstrate the\nintegration of multiple modalities in a collaborative application. Finally, a\ndiscussion over tool limitations and performance is provided to understand its\nrole in multimodal robotic systems. All developments are available at:\nhttps://github.com/NMKsas/gesture_pointer.git.", "AI": {"tldr": "该研究提出了一种基于姿态估计和几何模型的方法，用于在平面工作空间中定位指向手势的目标，并评估其在机器人任务中的表现。", "motivation": "指向手势是人机协作中常见的交互方式，但如何准确识别和定位目标是一个挑战。", "method": "使用RGB-D数据流，结合姿态估计和肩-腕伸展的几何模型提取手势数据。", "result": "开发了一个原型机器人系统，整合了目标检测、语音转录和语音合成，展示了多模态协作应用。", "conclusion": "研究提供了对工具局限性和性能的讨论，强调了其在多模态机器人系统中的潜在作用。"}}
{"id": "2506.22039", "pdf": "https://arxiv.org/pdf/2506.22039", "abs": "https://arxiv.org/abs/2506.22039", "authors": ["Lu Han", "Yu Liu", "Qiwen Deng", "Jian Jiang", "Yinbo Sun", "Zhe Yu", "Binfeng Wang", "Xingyu Lu", "Lintao Ma", "Han-Jia Ye", "De-Chuan Zhan"], "title": "UniCA: Adapting Time Series Foundation Model to General Covariate-Aware Forecasting", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Time Series Foundation Models (TSFMs) have achieved remarkable success\nthrough large-scale pretraining. However, their design primarily targets\nreal-valued series, limiting their ability to handle general forecasting tasks\ninvolving diverse and often heterogeneous covariates--such as categorical\nvariables and multimodal data (e.g., images, text)--which are typically\ntask-specific and difficult to leverage during pretraining. To address this\ngap, we propose Unified Covariate Adaptation (UniCA), a framework to bridge\nTSFMs with general covariate-aware forecasting. UniCA first performs covariate\nhomogenization to transform heterogeneous covariates into high-level\nhomogeneous series representations and then fuses them via a unified\nattention-based fusion mechanism. UniCA is compatible and universal for\nadaptation with both homogeneous and heterogeneous covariates, incorporating\nextra covariate information while preserving the generalization ability of\nTSFMs.Extensive experiments on multiple unimodal and multimodal covariate-aware\nforecasting benchmarks demonstrate the superiority of UniCA, highlighting the\npromise of covariate-aware TSFM adaptation in real-world forecasting scenarios.\nCodes are released on https://github.com/hanlu-nju/UniCA.", "AI": {"tldr": "UniCA框架通过协变量同质化和统一注意力融合机制，解决了时间序列基础模型（TSFMs）在处理异构协变量时的局限性。", "motivation": "TSFMs主要针对实值序列设计，难以处理包含异构协变量（如分类变量和多模态数据）的通用预测任务。", "method": "UniCA首先进行协变量同质化，将异构协变量转化为同质序列表示，然后通过统一的注意力机制融合。", "result": "在多个单模态和多模态协变量预测基准测试中，UniCA表现出优越性。", "conclusion": "UniCA展示了协变量感知的TSFM适应在现实预测场景中的潜力。"}}
{"id": "2506.22156", "pdf": "https://arxiv.org/pdf/2506.22156", "abs": "https://arxiv.org/abs/2506.22156", "authors": ["Mattia Ricchi", "Fabrizio Alfonsi", "Camilla Marella", "Marco Barbieri", "Alessandra Retico", "Leonardo Brizi", "Alessandro Gabrielli", "Claudia Testa"], "title": "Hardware acceleration for ultra-fast Neural Network training on FPGA for MRF map reconstruction", "categories": ["cs.AR", "cs.CV", "physics.ins-det"], "comment": "8 pages, 2 figures, to be published in conference proceedings of SDPS\n  2024: 2024 International Conference of the Society for Design and Process\n  Science on Advances and Challenges of Applying AI/GenAI in Design and Process\n  Science", "summary": "Magnetic Resonance Fingerprinting (MRF) is a fast quantitative MR Imaging\ntechnique that provides multi-parametric maps with a single acquisition. Neural\nNetworks (NNs) accelerate reconstruction but require significant resources for\ntraining. We propose an FPGA-based NN for real-time brain parameter\nreconstruction from MRF data. Training the NN takes an estimated 200 seconds,\nsignificantly faster than standard CPU-based training, which can be up to 250\ntimes slower. This method could enable real-time brain analysis on mobile\ndevices, revolutionizing clinical decision-making and telemedicine.", "AI": {"tldr": "FPGA加速的神经网络用于实时MRF数据重建，训练时间显著缩短。", "motivation": "解决传统CPU训练神经网络速度慢的问题，以实现实时脑部分析。", "method": "采用基于FPGA的神经网络加速MRF数据的参数重建。", "result": "训练时间仅需200秒，比CPU快250倍。", "conclusion": "该方法有望推动移动设备上的实时脑部分析，革新临床决策和远程医疗。"}}
{"id": "2506.22084", "pdf": "https://arxiv.org/pdf/2506.22084", "abs": "https://arxiv.org/abs/2506.22084", "authors": ["Chaitanya K. Joshi"], "title": "Transformers are Graph Neural Networks", "categories": ["cs.LG", "cs.AI"], "comment": "This paper is a technical version of an article in The Gradient at\n  https://thegradient.pub/transformers-are-graph-neural-networks/", "summary": "We establish connections between the Transformer architecture, originally\nintroduced for natural language processing, and Graph Neural Networks (GNNs)\nfor representation learning on graphs. We show how Transformers can be viewed\nas message passing GNNs operating on fully connected graphs of tokens, where\nthe self-attention mechanism capture the relative importance of all tokens\nw.r.t. each-other, and positional encodings provide hints about sequential\nordering or structure. Thus, Transformers are expressive set processing\nnetworks that learn relationships among input elements without being\nconstrained by apriori graphs. Despite this mathematical connection to GNNs,\nTransformers are implemented via dense matrix operations that are significantly\nmore efficient on modern hardware than sparse message passing. This leads to\nthe perspective that Transformers are GNNs currently winning the hardware\nlottery.", "AI": {"tldr": "论文揭示了Transformer架构与图神经网络（GNNs）在表示学习上的联系，表明Transformer可视为在完全连接图上操作的消息传递GNN，其自注意力机制捕捉了输入元素间的关系。", "motivation": "探索Transformer与GNNs之间的数学联系，以理解Transformer在处理集合数据时的表达能力及其硬件效率优势。", "method": "将Transformer的自注意力机制解释为在完全连接图上操作的消息传递GNN，并分析其与GNNs的异同。", "result": "Transformer是一种高效的集合处理网络，能够学习输入元素间的关系，且在现代硬件上比稀疏消息传递更高效。", "conclusion": "Transformer可视为当前硬件条件下更高效的GNN，其成功部分归因于硬件优势。"}}
{"id": "2506.21865", "pdf": "https://arxiv.org/pdf/2506.21865", "abs": "https://arxiv.org/abs/2506.21865", "authors": ["Haofeng Wang", "Yilin Guo", "Zehao Li", "Tong Yue", "Yizong Wang", "Enci Zhang", "Rongqun Lin", "Feng Gao", "Shiqi Wang", "Siwei Ma"], "title": "RiverEcho: Real-Time Interactive Digital System for Ancient Yellow River Culture", "categories": ["cs.MM", "cs.CL"], "comment": "IEEE International Conference on Multimedia and Expo Workshop,\n  2025.(Accepted)", "summary": "The Yellow River is China's mother river and a cradle of human civilization.\nThe ancient Yellow River culture is, moreover, an indispensable part of human\nart history. To conserve and inherit the ancient Yellow River culture, we\ndesigned RiverEcho, a real-time interactive system that responds to voice\nqueries using a large language model and a cultural knowledge dataset,\ndelivering explanations through a talking-head digital human. Specifically, we\nbuilt a knowledge database focused on the ancient Yellow River culture,\nincluding the collection of historical texts and the processing pipeline.\nExperimental results demonstrate that leveraging Retrieval-Augmented Generation\n(RAG) on the proposed dataset enhances the response quality of the Large\nLanguage Model(LLM), enabling the system to generate more professional and\ninformative responses. Our work not only diversifies the means of promoting\nYellow River culture but also provides users with deeper cultural insights.", "AI": {"tldr": "RiverEcho是一个基于大语言模型和知识库的实时交互系统，用于保护和传承黄河文化，通过数字人提供专业回答。", "motivation": "保护和传承黄河文化，丰富文化传播方式。", "method": "构建黄河文化知识库，结合检索增强生成（RAG）技术提升大语言模型的回答质量。", "result": "系统能生成更专业、信息丰富的回答，提升用户体验。", "conclusion": "RiverEcho不仅推广了黄河文化，还为用户提供了更深层次的文化理解。"}}
{"id": "2506.22176", "pdf": "https://arxiv.org/pdf/2506.22176", "abs": "https://arxiv.org/abs/2506.22176", "authors": ["Holly Dinkel", "Raghavendra Navaratna", "Jingyi Xiang", "Brian Coltin", "Trey Smith", "Timothy Bretl"], "title": "KnotDLO: Toward Interpretable Knot Tying", "categories": ["cs.RO", "cs.CV"], "comment": "4 pages, 5 figures, presented at the Workshop on 3D Visual\n  Representations for Manipulation at the 2023 IEEE International Conference on\n  Robotics and Automation in Yokohama, Japan. Video presentation\n  [https://youtu.be/mg30uCUtpOk]. Poster\n  [https://hollydinkel.github.io/assets/pdf/ICRA20243DVRM_poster.pdf] 3DVRM\n  Workshop [https://3d-manipulation-workshop.github.io/]", "summary": "This work presents KnotDLO, a method for one-handed Deformable Linear Object\n(DLO) knot tying that is robust to occlusion, repeatable for varying rope\ninitial configurations, interpretable for generating motion policies, and\nrequires no human demonstrations or training. Grasp and target waypoints for\nfuture DLO states are planned from the current DLO shape. Grasp poses are\ncomputed from indexing the tracked piecewise linear curve representing the DLO\nstate based on the current curve shape and are piecewise continuous. KnotDLO\ncomputes intermediate waypoints from the geometry of the current DLO state and\nthe desired next state. The system decouples visual reasoning from control. In\n16 trials of knot tying, KnotDLO achieves a 50% success rate in tying an\noverhand knot from previously unseen configurations.", "AI": {"tldr": "KnotDLO是一种无需人工演示或训练、可重复且可解释的单手可变形线性物体（DLO）打结方法。", "motivation": "解决DLO打结中的遮挡问题，提高对不同初始配置的适应性，并实现无需人工干预的自动化打结。", "method": "通过当前DLO形状规划抓取和目标路径点，基于分段线性曲线计算抓取位姿，并利用几何信息生成中间路径点。", "result": "在16次试验中，KnotDLO在从未见过的配置下实现了50%的打结成功率。", "conclusion": "KnotDLO展示了无需人工干预的自动化DLO打结潜力，但成功率仍有提升空间。"}}
{"id": "2506.22095", "pdf": "https://arxiv.org/pdf/2506.22095", "abs": "https://arxiv.org/abs/2506.22095", "authors": ["Filip Rydin", "Attila Lischka", "Jiaming Wu", "Morteza Haghir Chehreghani", "Balázs Kulcsár"], "title": "Learning to Solve Multi-Objective Routing Problems on Multigraphs", "categories": ["cs.LG", "cs.AI"], "comment": "18 pages, 5 Figures", "summary": "Learning-based methods for routing have gained significant attention in\nrecent years, both in single-objective and multi-objective contexts. However,\nthe multigraph setting, where multiple paths with distinct attributes can exist\nbetween destinations, has largely been overlooked, despite its high practical\nrelevancy. In this paper, we introduce two neural approaches to address\nmulti-objective routing on multigraphs. Our first approach works directly on\nthe multigraph, by autoregressively selecting edges until a tour is completed.\nOn the other hand, our second model first prunes the multigraph into a simple\ngraph and then builds routes. We validate both models experimentally and find\nthat they demonstrate strong performance across a variety of problems,\nincluding the Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP).", "AI": {"tldr": "论文提出两种神经网络方法解决多目标多图路由问题，验证了其在TSP和CVRP等任务中的有效性。", "motivation": "多图环境下多目标路由问题具有实际意义，但现有学习型方法对此关注不足。", "method": "1. 直接在多图上自回归选择边完成路径；2. 先剪枝为简单图再构建路径。", "result": "两种模型在TSP和CVRP等多种问题上表现优异。", "conclusion": "提出的方法在多目标多图路由中具有高效性和实用性。"}}
{"id": "2506.21913", "pdf": "https://arxiv.org/pdf/2506.21913", "abs": "https://arxiv.org/abs/2506.21913", "authors": ["Zunran Wang", "Zheng Shenpeng", "Wang Shenglan", "Minghui Zhao", "Zhonghua Li"], "title": "HyReC: Exploring Hybrid-based Retriever for Chinese", "categories": ["cs.IR", "cs.CL"], "comment": null, "summary": "Hybrid-based retrieval methods, which unify dense-vector and lexicon-based\nretrieval, have garnered considerable attention in the industry due to\nperformance enhancement. However, despite their promising results, the\napplication of these hybrid paradigms in Chinese retrieval contexts has\nremained largely underexplored. In this paper, we introduce HyReC, an\ninnovative end-to-end optimization method tailored specifically for\nhybrid-based retrieval in Chinese. HyReC enhances performance by integrating\nthe semantic union of terms into the representation model. Additionally, it\nfeatures the Global-Local-Aware Encoder (GLAE) to promote consistent semantic\nsharing between lexicon-based and dense retrieval while minimizing the\ninterference between them. To further refine alignment, we incorporate a\nNormalization Module (NM) that fosters mutual benefits between the retrieval\napproaches. Finally, we evaluate HyReC on the C-MTEB retrieval benchmark to\ndemonstrate its effectiveness.", "AI": {"tldr": "HyReC是一种针对中文混合检索的端到端优化方法，通过语义融合和全局-局部感知编码提升性能。", "motivation": "混合检索方法在中文场景中尚未充分探索，需要一种专门优化的方法。", "method": "HyReC结合语义融合、GLAE编码器和归一化模块，优化混合检索性能。", "result": "在C-MTEB基准测试中验证了HyReC的有效性。", "conclusion": "HyReC为中文混合检索提供了一种高效且优化的解决方案。"}}
{"id": "2506.22222", "pdf": "https://arxiv.org/pdf/2506.22222", "abs": "https://arxiv.org/abs/2506.22222", "authors": ["Hao Xu", "Ruth Lim", "Brian E. Chapman"], "title": "Advanced Deep Learning Techniques for Automated Segmentation of Type B Aortic Dissections", "categories": ["eess.IV", "cs.CV"], "comment": "9 pages, 5 figures, 3 tables", "summary": "Purpose: Aortic dissections are life-threatening cardiovascular conditions\nrequiring accurate segmentation of true lumen (TL), false lumen (FL), and false\nlumen thrombosis (FLT) from CTA images for effective management. Manual\nsegmentation is time-consuming and variable, necessitating automated solutions.\nMaterials and Methods: We developed four deep learning-based pipelines for Type\nB aortic dissection segmentation: a single-step model, a sequential model, a\nsequential multi-task model, and an ensemble model, utilizing 3D U-Net and\nSwin-UnetR architectures. A dataset of 100 retrospective CTA images was split\ninto training (n=80), validation (n=10), and testing (n=10). Performance was\nassessed using the Dice Coefficient and Hausdorff Distance. Results: Our\napproach achieved superior segmentation accuracy, with Dice Coefficients of\n0.91 $\\pm$ 0.07 for TL, 0.88 $\\pm$ 0.18 for FL, and 0.47 $\\pm$ 0.25 for FLT,\noutperforming Yao et al. (1), who reported 0.78 $\\pm$ 0.20, 0.68 $\\pm$ 0.18,\nand 0.25 $\\pm$ 0.31, respectively. Conclusion: The proposed pipelines provide\naccurate segmentation of TBAD features, enabling derivation of morphological\nparameters for surveillance and treatment planning", "AI": {"tldr": "论文提出四种深度学习模型用于B型主动脉夹层的自动分割，显著提高了分割精度。", "motivation": "主动脉夹层是一种危及生命的心血管疾病，需要从CTA图像中准确分割真腔、假腔和假腔血栓，手动分割耗时且不一致，因此需要自动化解决方案。", "method": "开发了四种基于深度学习的模型（单步模型、顺序模型、顺序多任务模型和集成模型），使用3D U-Net和Swin-UnetR架构，并在100例回顾性CTA图像上训练和测试。", "result": "提出的方法在分割精度上表现优异，真腔、假腔和假腔血栓的Dice系数分别为0.91、0.88和0.47，优于现有研究。", "conclusion": "所提出的模型能准确分割B型主动脉夹层特征，为监测和治疗计划提供了形态学参数。"}}
{"id": "2506.22226", "pdf": "https://arxiv.org/pdf/2506.22226", "abs": "https://arxiv.org/abs/2506.22226", "authors": ["Ajay Mittal", "Raghav Mehta", "Omar Todd", "Philipp Seeböck", "Georg Langs", "Ben Glocker"], "title": "Cardiovascular disease classification using radiomics and geometric features from cardiac CT", "categories": ["eess.IV", "cs.CV"], "comment": "Under Review at STACOM 2025 with MICCAI 2025", "summary": "Automatic detection and classification of Cardiovascular disease (CVD) from\nComputed Tomography (CT) images play an important part in facilitating\nbetter-informed clinical decisions. However, most of the recent deep learning\nbased methods either directly work on raw CT data or utilize it in pair with\nanatomical cardiac structure segmentation by training an end-to-end classifier.\nAs such, these approaches become much more difficult to interpret from a\nclinical perspective. To address this challenge, in this work, we break down\nthe CVD classification pipeline into three components: (i) image segmentation,\n(ii) image registration, and (iii) downstream CVD classification. Specifically,\nwe utilize the Atlas-ISTN framework and recent segmentation foundational models\nto generate anatomical structure segmentation and a normative healthy atlas.\nThese are further utilized to extract clinically interpretable radiomic\nfeatures as well as deformation field based geometric features (through atlas\nregistration) for CVD classification. Our experiments on the publicly available\nASOCA dataset show that utilizing these features leads to better CVD\nclassification accuracy (87.50\\%) when compared against classification model\ntrained directly on raw CT images (67.50\\%). Our code is publicly available:\nhttps://github.com/biomedia-mira/grc-net", "AI": {"tldr": "论文提出了一种基于分割、配准和分类的三阶段心血管疾病（CVD）分类方法，通过提取临床可解释的特征，显著提高了分类准确性。", "motivation": "现有深度学习方法直接处理原始CT数据或结合分割模型，临床解释性差，因此需要一种更可解释的CVD分类方法。", "method": "将CVD分类分为图像分割、图像配准和下游分类三部分，利用Atlas-ISTN框架和分割基础模型生成解剖结构分割和健康图谱，提取放射组学特征和几何特征进行分类。", "result": "在ASOCA数据集上，该方法分类准确率达到87.50%，显著优于直接使用原始CT数据的模型（67.50%）。", "conclusion": "通过分阶段提取临床可解释特征，显著提升了CVD分类的准确性和可解释性。"}}
{"id": "2506.22280", "pdf": "https://arxiv.org/pdf/2506.22280", "abs": "https://arxiv.org/abs/2506.22280", "authors": ["Yuliang Huang", "Imraj Singh", "Thomas Joyce", "Kris Thielemans", "Jamie R. McClelland"], "title": "DIGS: Dynamic CBCT Reconstruction using Deformation-Informed 4D Gaussian Splatting and a Low-Rank Free-Form Deformation Model", "categories": ["eess.IV", "cs.CV"], "comment": "Accepted by MICCAI 2025", "summary": "3D Cone-Beam CT (CBCT) is widely used in radiotherapy but suffers from motion\nartifacts due to breathing. A common clinical approach mitigates this by\nsorting projections into respiratory phases and reconstructing images per\nphase, but this does not account for breathing variability. Dynamic CBCT\ninstead reconstructs images at each projection, capturing continuous motion\nwithout phase sorting. Recent advancements in 4D Gaussian Splatting (4DGS)\noffer powerful tools for modeling dynamic scenes, yet their application to\ndynamic CBCT remains underexplored. Existing 4DGS methods, such as HexPlane,\nuse implicit motion representations, which are computationally expensive. While\nexplicit low-rank motion models have been proposed, they lack spatial\nregularization, leading to inconsistencies in Gaussian motion. To address these\nlimitations, we introduce a free-form deformation (FFD)-based spatial basis\nfunction and a deformation-informed framework that enforces consistency by\ncoupling the temporal evolution of Gaussian's mean position, scale, and\nrotation under a unified deformation field. We evaluate our approach on six\nCBCT datasets, demonstrating superior image quality with a 6x speedup over\nHexPlane. These results highlight the potential of deformation-informed 4DGS\nfor efficient, motion-compensated CBCT reconstruction. The code is available at\nhttps://github.com/Yuliang-Huang/DIGS.", "AI": {"tldr": "提出了一种基于自由形变（FFD）的4D高斯泼溅（4DGS）方法，用于动态CBCT重建，解决了现有方法计算成本高和运动不一致的问题。", "motivation": "动态CBCT重建需要捕捉连续运动，但现有4DGS方法计算成本高且缺乏空间正则化，导致运动不一致。", "method": "引入FFD空间基函数和变形感知框架，统一变形场下耦合高斯均值位置、尺度和旋转的时序演化。", "result": "在六个CBCT数据集上验证，图像质量优于HexPlane，速度提升6倍。", "conclusion": "变形感知4DGS为高效、运动补偿的CBCT重建提供了潜力。"}}
{"id": "2506.22185", "pdf": "https://arxiv.org/pdf/2506.22185", "abs": "https://arxiv.org/abs/2506.22185", "authors": ["Matteo Esposito", "Alexander Bakhtin", "Noman Ahmad", "Mikel Robredo", "Ruoyu Su", "Valentina Lenarduzzi", "Davide Taibi"], "title": "Autonomic Microservice Management via Agentic AI and MAPE-K Integration", "categories": ["cs.SE", "cs.AI", "cs.DC", "cs.NI", "cs.SY", "eess.SY"], "comment": null, "summary": "While microservices are revolutionizing cloud computing by offering\nunparalleled scalability and independent deployment, their decentralized nature\nposes significant security and management challenges that can threaten system\nstability. We propose a framework based on MAPE-K, which leverages agentic AI,\nfor autonomous anomaly detection and remediation to address the daunting task\nof highly distributed system management. Our framework offers practical,\nindustry-ready solutions for maintaining robust and secure microservices.\nPractitioners and researchers can customize the framework to enhance system\nstability, reduce downtime, and monitor broader system quality attributes such\nas system performance level, resilience, security, and anomaly management,\namong others.", "AI": {"tldr": "提出基于MAPE-K和代理AI的框架，用于微服务的自主异常检测与修复，提升系统稳定性。", "motivation": "微服务的去中心化特性带来安全和管理的挑战，威胁系统稳定性。", "method": "基于MAPE-K框架，利用代理AI实现自主异常检测与修复。", "result": "提供实用的行业解决方案，增强系统稳定性、减少停机时间，并监控性能、弹性、安全等质量属性。", "conclusion": "该框架可定制化，适用于研究和实践，提升微服务系统的稳健性和安全性。"}}
{"id": "2506.22023", "pdf": "https://arxiv.org/pdf/2506.22023", "abs": "https://arxiv.org/abs/2506.22023", "authors": ["Bohan Li", "Zhihan Li", "Haoran Wang", "Hanglei Zhang", "Yiwei Guo", "Hankun Wang", "Xie Chen", "Kai Yu"], "title": "Robust and Efficient Autoregressive Speech Synthesis with Dynamic Chunk-wise Prediction Policy", "categories": ["cs.SD", "cs.CL", "eess.AS"], "comment": "17 pages, 8 figures, 5 tables", "summary": "Recently, autoregressive (AR) language models have emerged as a dominant\napproach in speech synthesis, offering expressive generation and scalable\ntraining. However, conventional AR speech synthesis models relying on the\nnext-token prediction paradigm often encounter significant challenges when\nhandling long speech sequences. These models often struggle to construct stable\nframe-to-frame attention, leading to increased latency and degraded synthesis\nquality, thereby limiting their feasibility for real-time applications. To\naddress these limitations, we introduce a novel dynamic chunk-wise\nautoregressive synthesis framework, termed DCAR, designed to enhance both\nefficiency and intelligibility robustness in AR speech generation. DCAR\nintroduces a chunk-to-frame attention mechanism through training with\nmulti-token prediction, enabling dynamic chunk prediction in variable speech\ncontexts using a lightweight module trained on-policy. DCAR dynamically adjusts\nthe token prediction span, significantly reducing the sequence length\ndependency while obtaining high synthesis quality. Comprehensive empirical\nevaluations demonstrate that DCAR substantially outperforms traditional\nnext-token prediction models, achieving up to 72.27% intelligibility\nimprovement and 2.61x inference speedup simultaneously on the test set.\nFurthermore, we conduct comprehensive analysis to support it as a versatile\nfoundation for next-generation speech synthesis systems.", "AI": {"tldr": "论文提出了一种动态分块自回归合成框架（DCAR），通过动态调整预测范围，显著提升了语音合成的效率和质量。", "motivation": "传统自回归语音合成模型在处理长序列时存在注意力不稳定、延迟高和质量下降的问题，限制了实时应用的可行性。", "method": "DCAR采用分块到帧的注意力机制，通过多令牌预测训练动态调整预测范围，减少序列长度依赖。", "result": "实验表明，DCAR在测试集上实现了72.27%的可懂度提升和2.61倍的推理加速。", "conclusion": "DCAR为下一代语音合成系统提供了高效且鲁棒的解决方案。"}}
{"id": "2506.22304", "pdf": "https://arxiv.org/pdf/2506.22304", "abs": "https://arxiv.org/abs/2506.22304", "authors": ["Erkan Turan", "Aristotelis Siozopoulos", "Maks Ovsjanikov"], "title": "Unfolding Generative Flows with Koopman Operators: Fast and Interpretable Sampling", "categories": ["cs.LG", "cs.CV"], "comment": null, "summary": "Conditional Flow Matching (CFM) offers a simulation-free framework for\ntraining continuous-time generative models, bridging diffusion and flow-based\napproaches. However, sampling from CFM still relies on numerically solving\nnon-linear ODEs which can be computationally expensive and difficult to\ninterpret. Recent alternatives address sampling speed via trajectory\nstraightening, mini-batch coupling or distillation. However, these methods\ntypically do not shed light on the underlying \\textit{structure} of the\ngenerative process. In this work, we propose to accelerate CFM and introduce an\ninterpretable representation of its dynamics by integrating Koopman operator\ntheory, which models non-linear flows as linear evolution in a learned space of\nobservables. We introduce a decoder-free Koopman-CFM architecture that learns\nan embedding where the generative dynamics become linear, enabling closed-form,\none-step sampling via matrix exponentiation. This results in significant\nspeedups over traditional CFM as demonstrated on controlled 2D datasets and\nreal-world benchmarks, MNIST, Fashion-MNIST (F-MNIST), and the Toronto Face\nDataset (TFD). Unlike previous methods, our approach leads to a well-structured\nKoopman generator, whose spectral properties, eigenvalues, and eigenfunctions\noffer principled tools for analyzing generative behavior such as temporal\nscaling, mode stability, and decomposition in Koopman latent space. By\ncombining sampling efficiency with analytical structure, Koopman-enhanced flow\nmatching offers a potential step toward fast and interpretable generative\nmodeling.", "AI": {"tldr": "提出了一种基于Koopman算子理论的改进条件流匹配（CFM）方法，通过线性化生成动态实现高效采样，并在多个数据集上验证了其速度和可解释性。", "motivation": "传统CFM采样依赖非线性ODE求解，计算成本高且难以解释。现有加速方法未揭示生成过程的结构。", "method": "结合Koopman算子理论，设计无解码器的Koopman-CFM架构，在学习的嵌入空间中实现线性动态，支持闭式一步采样。", "result": "在2D数据集和MNIST等基准测试中显著加速采样，并提供了生成行为的分析工具（如特征值和特征函数）。", "conclusion": "Koopman增强的CFM在高效采样和可解释性方面迈出了重要一步。"}}
{"id": "2506.22200", "pdf": "https://arxiv.org/pdf/2506.22200", "abs": "https://arxiv.org/abs/2506.22200", "authors": ["Chen Wang", "Lai Wei", "Yanzhi Zhang", "Chenyang Shao", "Zedong Dan", "Weiran Huang", "Yue Wang", "Yuzhi Zhang"], "title": "EFRame: Deeper Reasoning via Exploration-Filtering-Replay Reinforcement Learning Framework", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Recent advances in reinforcement learning (RL) have significantly enhanced\nthe reasoning capabilities of large language models (LLMs). Group Relative\nPolicy Optimization (GRPO), an efficient variant of PPO that lowers RL's\ncomputational cost, still faces limited exploration, low sample efficiency and\ninstability, constraining its performance on complex reasoning tasks. To\naddress these limitations, we introduce EFRame, an Exploration-Filtering-Replay\nframework that systematically augments GRPO along three critical dimensions.\nEFRame performs additional rollouts to explore high-quality trajectories,\napplies online filtering to eliminate low-quality samples that introduce noise\nand variance, and leverages experience replay to repeatedly exploit rare but\ninformative samples. EFRame establishes a complete and stable learning cycle,\nguiding the model through a structured transition from exploration to\nconvergence. Our experiments across a variety of reasoning benchmarks\ndemonstrate that EFRame not only improves the robustness and efficiency of\ntraining, but also enables access to deeper reasoning capabilities that remain\nunattainable under vanilla GRPO. Furthermore, EFRame enables a more\nfine-grained categorization of training samples, allowing for a deeper analysis\nof how different types of samples contribute to the learning process in RL. Our\ncode is available at https://github.com/597358816/EFRame.", "AI": {"tldr": "EFRame框架通过探索、过滤和回放机制增强GRPO，提升强化学习在复杂推理任务中的性能和稳定性。", "motivation": "GRPO在复杂推理任务中存在探索不足、样本效率低和不稳定的问题，限制了其性能。", "method": "EFRame通过额外探索高质量轨迹、在线过滤低质量样本和利用经验回放机制，构建完整稳定的学习循环。", "result": "实验表明EFRame提高了训练鲁棒性和效率，并解锁了GRPO无法实现的深层推理能力。", "conclusion": "EFRame为强化学习提供了更精细的样本分类和分析方法，显著提升了复杂推理任务的性能。"}}
{"id": "2506.22049", "pdf": "https://arxiv.org/pdf/2506.22049", "abs": "https://arxiv.org/abs/2506.22049", "authors": ["Tianhao Chen", "Xin Xu", "Zijing Liu", "Pengxiang Li", "Xinyuan Song", "Ajay Kumar Jaiswal", "Fan Zhang", "Jishan Hu", "Yang Wang", "Hao Chen", "Shizhe Diao", "Shiwei Liu", "Yu Li", "Yin Lu", "Can Yang"], "title": "GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling", "categories": ["cs.LG", "cs.CL"], "comment": null, "summary": "Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,\npredominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While\nbeing stable during pretraining and scalable to large model sizes, Pre-LN\nsuffers from an exponential growth in activation variance across layers,\ncausing the residual path to dominate over sub-layer outputs and limiting the\nlearning capacity of deeper layers. To mitigate this issue, we propose\nGradient-Preserving Activation Scaling (GPAS), a simple technique that can be\nused in combination with existing approaches. GPAS works by scaling down the\nintermediate activations while keeping their gradients unchanged. This leaves\ninformation in the activations intact, and avoids the gradient vanishing\nproblem associated with gradient downscaling. Extensive experiments across\nvarious model sizes from 71M to 1B show that GPAS achieves consistent\nperformance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows\npromise in improving alternative architectures such as Sandwich-LN and\nDeepNorm, demonstrating its versatility and potential for improving training\ndynamics in a wide range of settings.", "AI": {"tldr": "论文提出了一种名为GPAS的技术，用于解决Pre-LN Transformer中激活方差指数增长的问题，通过缩放中间激活值但保持梯度不变，提升模型性能。", "motivation": "Pre-LN Transformer在预训练中稳定且可扩展，但存在激活方差指数增长的问题，导致深层学习能力受限。", "method": "提出Gradient-Preserving Activation Scaling (GPAS)，缩放中间激活值但保持梯度不变，避免梯度消失问题。", "result": "在71M到1B的不同模型规模上，GPAS均实现了性能提升，并适用于其他架构如Sandwich-LN和DeepNorm。", "conclusion": "GPAS是一种简单有效的技术，能够改善Pre-LN Transformer及其他架构的训练动态，具有广泛的应用潜力。"}}
{"id": "2506.22340", "pdf": "https://arxiv.org/pdf/2506.22340", "abs": "https://arxiv.org/abs/2506.22340", "authors": ["Yannick Werner", "Akash Malemath", "Mengxi Liu", "Vitor Fortes Rey", "Nikolaos Palaiodimopoulos", "Paul Lukowicz", "Maximilian Kiefer-Emmanouilidis"], "title": "QuKAN: A Quantum Circuit Born Machine approach to Quantum Kolmogorov Arnold Networks", "categories": ["quant-ph", "cs.CV", "cs.LG"], "comment": null, "summary": "Kolmogorov Arnold Networks (KANs), built upon the Kolmogorov Arnold\nrepresentation theorem (KAR), have demonstrated promising capabilities in\nexpressing complex functions with fewer neurons. This is achieved by\nimplementing learnable parameters on the edges instead of on the nodes, unlike\ntraditional networks such as Multi-Layer Perceptrons (MLPs). However, KANs\npotential in quantum machine learning has not yet been well explored. In this\nwork, we present an implementation of these KAN architectures in both hybrid\nand fully quantum forms using a Quantum Circuit Born Machine (QCBM). We adapt\nthe KAN transfer using pre-trained residual functions, thereby exploiting the\nrepresentational power of parametrized quantum circuits. In the hybrid model we\ncombine classical KAN components with quantum subroutines, while the fully\nquantum version the entire architecture of the residual function is translated\nto a quantum model. We demonstrate the feasibility, interpretability and\nperformance of the proposed Quantum KAN (QuKAN) architecture.", "AI": {"tldr": "论文提出了一种基于Kolmogorov Arnold Networks (KANs)的量子机器学习架构，包括混合和全量子形式，并展示了其可行性和性能。", "motivation": "探索KANs在量子机器学习中的潜力，利用量子电路的表示能力提升性能。", "method": "通过Quantum Circuit Born Machine (QCBM)实现混合和全量子KAN架构，结合预训练残差函数。", "result": "展示了Quantum KAN (QuKAN)架构的可行性、可解释性和性能。", "conclusion": "QuKAN架构为量子机器学习提供了新的研究方向，具有潜力。"}}
{"id": "2506.22231", "pdf": "https://arxiv.org/pdf/2506.22231", "abs": "https://arxiv.org/abs/2506.22231", "authors": ["Russell Beale"], "title": "Adapting University Policies for Generative AI: Opportunities, Challenges, and Policy Solutions in Higher Education", "categories": ["cs.HC", "cs.AI", "cs.CY", "K.3.1; K.3.2; K.6.0"], "comment": null, "summary": "The rapid proliferation of generative artificial intelligence (AI) tools -\nespecially large language models (LLMs) such as ChatGPT - has ushered in a\ntransformative era in higher education. Universities in developed regions are\nincreasingly integrating these technologies into research, teaching, and\nassessment. On one hand, LLMs can enhance productivity by streamlining\nliterature reviews, facilitating idea generation, assisting with coding and\ndata analysis, and even supporting grant proposal drafting. On the other hand,\ntheir use raises significant concerns regarding academic integrity, ethical\nboundaries, and equitable access. Recent empirical studies indicate that nearly\n47% of students use LLMs in their coursework - with 39% using them for exam\nquestions and 7% for entire assignments - while detection tools currently\nachieve around 88% accuracy, leaving a 12% error margin. This article\ncritically examines the opportunities offered by generative AI, explores the\nmultifaceted challenges it poses, and outlines robust policy solutions.\nEmphasis is placed on redesigning assessments to be AI-resilient, enhancing\nstaff and student training, implementing multi-layered enforcement mechanisms,\nand defining acceptable use. By synthesizing data from recent research and case\nstudies, the article argues that proactive policy adaptation is imperative to\nharness AI's potential while safeguarding the core values of academic integrity\nand equity.", "AI": {"tldr": "生成式AI（如ChatGPT）在高等教育中带来机遇与挑战，需通过政策调整平衡其潜力与学术诚信。", "motivation": "探讨生成式AI在高等教育中的快速普及及其对研究、教学和评估的双重影响。", "method": "分析实证研究数据，提出政策解决方案，包括评估设计、培训和多层执行机制。", "result": "47%学生使用LLMs，检测工具准确率88%，需政策调整以应对挑战。", "conclusion": "主动政策调整是平衡AI潜力与学术诚信的关键。"}}
{"id": "2506.22189", "pdf": "https://arxiv.org/pdf/2506.22189", "abs": "https://arxiv.org/abs/2506.22189", "authors": ["Laura van Weesep", "Samuel Genheden", "Ola Engkvist", "Jens Sjölund"], "title": "Exploring Modularity of Agentic Systems for Drug Discovery", "categories": ["cs.LG", "cs.CL", "cs.MA"], "comment": null, "summary": "Large-language models (LLMs) and agentic systems present exciting\nopportunities to accelerate drug discovery and design. In this study, we\ncritically examine the modularity of LLM-based agentic systems for drug\ndiscovery, i.e., whether parts of the agentic system such as the LLM are\ninterchangeable, a topic that has received limited attention in drug discovery\napplications. We compare the performance of different large language models\n(LLMs) and the effectiveness of tool-calling agents versus code-generating\nagents in this domain. Our case study, comparing performance in orchestrating\ntools for chemistry and drug discovery using an LLM-as-a-judge score, shows\nthat Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative\nlanguage models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and\nNova-Micro. Although we confirm that code-generating agents outperform the\ntool-calling ones on average, we show that this is highly question and model\ndependent. Furthermore, the impact of replacing system prompts is dependent on\nthe specific question asked and the model used, underscoring that -- even in\nthis particular domain -- one cannot just replace language models without\nconsidering prompt re-engineering. Our study highlights the necessity of\nfurther research into the modularity of agentic systems to enable the\ndevelopment of stable and scalable solutions for real-world problems.", "AI": {"tldr": "研究探讨了基于LLM的代理系统在药物发现中的模块化问题，比较了不同LLM和代理类型的性能，发现Claude和GPT-4o表现更优，且代码生成代理通常优于工具调用代理。", "motivation": "探索LLM和代理系统在药物发现中的模块化潜力，填补该领域研究空白。", "method": "比较不同LLM和代理类型（工具调用与代码生成）的性能，使用LLM-as-a-judge评分。", "result": "Claude-3.5-Sonnet、Claude-3.7-Sonnet和GPT-4o表现最佳；代码生成代理通常更优，但结果因问题和模型而异。", "conclusion": "强调进一步研究代理系统模块化的必要性，以开发稳定且可扩展的解决方案。"}}
{"id": "2506.22397", "pdf": "https://arxiv.org/pdf/2506.22397", "abs": "https://arxiv.org/abs/2506.22397", "authors": ["Anirban Ray", "Ashesh", "Florian Jug"], "title": "Dehazing Light Microscopy Images with Guided Conditional Flow Matching: finding a sweet spot between fidelity and realism", "categories": ["eess.IV", "cs.AI", "cs.CV"], "comment": "supplement pending, 4 figures, 10 pages + refs", "summary": "Fluorescence microscopy is a major driver of scientific progress in the life\nsciences. Although high-end confocal microscopes are capable of filtering\nout-of-focus light, cheaper and more accessible microscopy modalities, such as\nwidefield microscopy, can not, which consequently leads to hazy image data.\nComputational dehazing is trying to combine the best of both worlds, leading to\ncheap microscopy but crisp-looking images. The perception-distortion trade-off\ntells us that we can optimize either for data fidelity, e.g. low MSE or high\nPSNR, or for data realism, measured by perceptual metrics such as LPIPS or FID.\nExisting methods either prioritize fidelity at the expense of realism, or\nproduce perceptually convincing results that lack quantitative accuracy. In\nthis work, we propose HazeMatching, a novel iterative method for dehazing light\nmicroscopy images, which effectively balances these objectives. Our goal was to\nfind a balanced trade-off between the fidelity of the dehazing results and the\nrealism of individual predictions (samples). We achieve this by adapting the\nconditional flow matching framework by guiding the generative process with a\nhazy observation in the conditional velocity field. We evaluate HazeMatching on\n5 datasets, covering both synthetic and real data, assessing both distortion\nand perceptual quality. Our method is compared against 7 baselines, achieving a\nconsistent balance between fidelity and realism on average. Additionally, with\ncalibration analysis, we show that HazeMatching produces well-calibrated\npredictions. Note that our method does not need an explicit degradation\noperator to exist, making it easily applicable on real microscopy data. All\ndata used for training and evaluation and our code will be publicly available\nunder a permissive license.", "AI": {"tldr": "HazeMatching是一种新型迭代方法，用于去除荧光显微镜图像中的模糊，平衡了数据保真度和感知真实性。", "motivation": "解决现有方法在去模糊图像时无法同时兼顾数据保真度和感知真实性的问题。", "method": "采用条件流匹配框架，通过条件速度场引导生成过程，无需显式退化算子。", "result": "在5个数据集上评估，HazeMatching在保真度和真实性之间取得了平衡，优于7个基线方法。", "conclusion": "HazeMatching是一种有效且易于应用的方法，适用于真实显微镜数据。"}}
{"id": "2506.22255", "pdf": "https://arxiv.org/pdf/2506.22255", "abs": "https://arxiv.org/abs/2506.22255", "authors": ["Maciej Stefaniak", "Michał Krutul", "Jan Małaśnicki", "Maciej Pióro", "Jakub Krajewski", "Sebastian Jaszczur", "Marek Cygan", "Kamil Adamczewski", "Jan Ludziejewski"], "title": "Projected Compression: Trainable Projection for Efficient Transformer Compression", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Large language models have steadily increased in size to achieve improved\nperformance; however, this growth has also led to greater inference time and\ncomputational demands. Consequently, there is rising interest in model size\nreduction methods. To address this issue, we propose Projected Compression, a\nnovel model compression technique, that reduces model weights by utilizing\nprojection modules. Specifically, we first train additional trainable\nprojections weights and preserve access to all the original model parameters.\nSubsequently, these projections are merged into a lower-dimensional product\nmatrix, resulting in a reduced-size standard Transformer-based model. Unlike\nalternative approaches that require additional computational overhead, our\nmethod matches the base model's per-token computation step in FLOPs.\nExperimental results show that Projected Compression outperforms the comparable\nhard pruning and retraining approach on higher quality models. Moreover, the\nperformance margin scales well with the number of tokens.", "AI": {"tldr": "论文提出了一种名为“Projected Compression”的新模型压缩技术，通过投影模块减少模型权重，同时保持原始参数访问，最终生成一个更小的标准Transformer模型。", "motivation": "随着大型语言模型规模的增大，推理时间和计算需求也随之增加，因此需要研究模型尺寸缩减方法。", "method": "训练额外的可训练投影权重，保留对原始参数的访问，并将这些投影合并为低维乘积矩阵，从而减少模型尺寸。", "result": "实验结果表明，该方法在高质量模型上优于硬剪枝和重新训练方法，且性能随token数量增加而提升。", "conclusion": "Projected Compression是一种高效且性能优越的模型压缩方法。"}}
{"id": "2506.22237", "pdf": "https://arxiv.org/pdf/2506.22237", "abs": "https://arxiv.org/abs/2506.22237", "authors": ["Sebastian Murgul", "Moritz Reiser", "Michael Heizmann", "Christoph Seibert"], "title": "Fine-Tuning MIDI-to-Audio Alignment using a Neural Network on Piano Roll and CQT Representations", "categories": ["cs.SD", "cs.CL", "cs.MM", "eess.AS"], "comment": "9 pages, 3 figures, 6 tables", "summary": "In this paper, we present a neural network approach for synchronizing audio\nrecordings of human piano performances with their corresponding loosely aligned\nMIDI files. The task is addressed using a Convolutional Recurrent Neural\nNetwork (CRNN) architecture, which effectively captures spectral and temporal\nfeatures by processing an unaligned piano roll and a spectrogram as inputs to\nestimate the aligned piano roll. To train the network, we create a dataset of\npiano pieces with augmented MIDI files that simulate common human timing\nerrors. The proposed model achieves up to 20% higher alignment accuracy than\nthe industry-standard Dynamic Time Warping (DTW) method across various\ntolerance windows. Furthermore, integrating DTW with the CRNN yields additional\nimprovements, offering enhanced robustness and consistency. These findings\ndemonstrate the potential of neural networks in advancing state-of-the-art\nMIDI-to-audio alignment.", "AI": {"tldr": "提出了一种基于卷积循环神经网络（CRNN）的方法，用于同步钢琴演奏的音频录音与松散对齐的MIDI文件，性能优于传统DTW方法。", "motivation": "解决钢琴演奏音频与MIDI文件对齐的挑战，传统方法（如DTW）在精度上存在局限。", "method": "使用CRNN架构处理未对齐的钢琴卷和频谱图，生成对齐的钢琴卷；通过模拟人类计时误差的数据集训练网络。", "result": "模型比DTW方法对齐精度提高20%，结合DTW后性能进一步优化。", "conclusion": "神经网络在MIDI与音频对齐任务中具有潜力，可推动技术发展。"}}
{"id": "2506.22426", "pdf": "https://arxiv.org/pdf/2506.22426", "abs": "https://arxiv.org/abs/2506.22426", "authors": ["Xiang Dai", "Kyrollos Yanny", "Kristina Monakhova", "Nicholas Antipa"], "title": "Single-shot HDR using conventional image sensor shutter functions and optical randomization", "categories": ["eess.IV", "cs.CV", "cs.GR", "eess.SP", "physics.optics"], "comment": null, "summary": "High-dynamic-range (HDR) imaging is an essential technique for overcoming the\ndynamic range limits of image sensors. The classic method relies on multiple\nexposures, which slows capture time, resulting in motion artifacts when imaging\ndynamic scenes. Single-shot HDR imaging alleviates this issue by encoding HDR\ndata into a single exposure, then computationally recovering it. Many\nestablished methods use strong image priors to recover improperly exposed image\ndetail. These approaches struggle with extended highlight regions. We utilize\nthe global reset release (GRR) shutter mode of an off-the-shelf sensor. GRR\nshutter mode applies a longer exposure time to rows closer to the bottom of the\nsensor. We use optics that relay a randomly permuted (shuffled) image onto the\nsensor, effectively creating spatially randomized exposures across the scene.\nThe exposure diversity allows us to recover HDR data by solving an optimization\nproblem with a simple total variation image prior. In simulation, we\ndemonstrate that our method outperforms other single-shot methods when many\nsensor pixels are saturated (10% or more), and is competitive at a modest\nsaturation (1%). Finally, we demonstrate a physical lab prototype that uses an\noff-the-shelf random fiber bundle for the optical shuffling. The fiber bundle\nis coupled to a low-cost commercial sensor operating in GRR shutter mode. Our\nprototype achieves a dynamic range of up to 73dB using an 8-bit sensor with\n48dB dynamic range.", "AI": {"tldr": "提出了一种基于全局复位释放（GRR）快门模式和光学随机排列的单次高动态范围（HDR）成像方法，显著提升了高光区域的恢复能力。", "motivation": "传统多曝光HDR成像在动态场景中易产生运动伪影，而现有单次HDR方法在高光区域表现不佳。", "method": "利用GRR快门模式结合随机排列的光学系统，通过优化问题和总变分先验恢复HDR数据。", "result": "在模拟中，该方法在高像素饱和（10%以上）时优于其他单次方法，并在低饱和（1%）时表现相当。物理原型实现了73dB的动态范围。", "conclusion": "该方法通过GRR快门和光学随机排列，有效解决了单次HDR成像中的高光恢复问题，且硬件成本低。"}}
{"id": "2506.22299", "pdf": "https://arxiv.org/pdf/2506.22299", "abs": "https://arxiv.org/abs/2506.22299", "authors": ["Tao Liu", "Longlong Lin", "Yunfeng Yu", "Xi Ou", "Youan Zhang", "Zhiqiu Ye", "Tao Jia"], "title": "CoATA: Effective Co-Augmentation of Topology and Attribute for Graph Neural Networks", "categories": ["cs.LG", "cs.AI", "I.2"], "comment": "icmr", "summary": "Graph Neural Networks (GNNs) have garnered substantial attention due to their\nremarkable capability in learning graph representations. However, real-world\ngraphs often exhibit substantial noise and incompleteness, which severely\ndegrades the performance of GNNs. Existing methods typically address this issue\nthrough single-dimensional augmentation, focusing either on refining topology\nstructures or perturbing node attributes, thereby overlooking the deeper\ninterplays between the two. To bridge this gap, this paper presents CoATA, a\ndual-channel GNN framework specifically designed for the Co-Augmentation of\nTopology and Attribute. Specifically, CoATA first propagates structural signals\nto enrich and denoise node attributes. Then, it projects the enhanced attribute\nspace into a node-attribute bipartite graph for further refinement or\nreconstruction of the underlying structure. Subsequently, CoATA introduces\ncontrastive learning, leveraging prototype alignment and consistency\nconstraints, to facilitate mutual corrections between the augmented and\noriginal graphs. Finally, extensive experiments on seven benchmark datasets\ndemonstrate that the proposed CoATA outperforms eleven state-of-the-art\nbaseline methods, showcasing its effectiveness in capturing the synergistic\nrelationship between topology and attributes.", "AI": {"tldr": "CoATA是一个双通道GNN框架，通过联合增强拓扑和属性，解决了现实图中噪声和不完整性问题，并通过对比学习优化性能。", "motivation": "现实图中的噪声和不完整性严重影响了GNN的性能，现有方法仅关注单维度增强，忽略了拓扑和属性之间的深层交互。", "method": "CoATA通过结构信号传播增强节点属性，构建节点-属性二分图优化结构，并引入对比学习进行相互校正。", "result": "在七个基准数据集上，CoATA优于十一种现有方法，证明了其在捕捉拓扑与属性协同关系上的有效性。", "conclusion": "CoATA通过双通道联合增强和对比学习，显著提升了GNN在噪声和不完整图上的性能。"}}
{"id": "2506.22321", "pdf": "https://arxiv.org/pdf/2506.22321", "abs": "https://arxiv.org/abs/2506.22321", "authors": ["Tarikul Islam Tamiti", "Anomadarshi Barua"], "title": "A Practical Approach to Power Saving in Hearables Using Sub-Nyquist Sampling with Bandwidth Extension", "categories": ["cs.SD", "cs.AI", "eess.AS"], "comment": null, "summary": "Hearables are wearable computers that are worn on the ear. Bone conduction\nmicrophones (BCMs) are used with air conduction microphones (ACMs) in hearables\nas a supporting modality for multimodal speech enhancement (SE) in noisy\nconditions. However, existing works don't consider the following practical\naspects for low-power implementations on hearables: (i) They do not explore how\nlowering the sampling frequencies and bit resolutions in analog-to-digital\nconverters (ADCs) of hearables jointly impact low-power processing and\nmultimodal SE in terms of speech quality and intelligibility. (ii) They don't\ndiscuss how GAN-like audio quality can be achieved without using actual GAN\ndiscriminators. And (iii) They don't process signals from ACMs/BCMs at\nsub-Nyquist sampling rate because, in their frameworks, they lack a wideband\nreconstruction methodology from their narrowband parts. We propose SUBARU\n(\\textbf{Sub}-Nyquist \\textbf{A}udio \\textbf{R}esolution \\textbf{U}psampling),\nwhich achieves the following: SUBARU (i) intentionally uses sub-Nyquist\nsampling and low bit resolution in ADCs, achieving a 3.31x reduction in power\nconsumption; (ii) introduces novel multi-scale and multi-period virtual\ndiscriminators, which achieve GAN-like audio quality without using GANs'\nadversarial training; and (iii) achieves streaming operations on mobile\nplatforms and SE in in-the-wild noisy conditions with an inference time of\n1.74ms and a memory footprint of less than 13.77MB.", "AI": {"tldr": "SUBARU提出了一种低功耗的语音增强方法，通过子奈奎斯特采样和低比特分辨率降低功耗，并利用虚拟判别器实现高质量音频。", "motivation": "现有方法未考虑低功耗实现和高质量音频的需求，特别是在嘈杂环境下的多模态语音增强。", "method": "采用子奈奎斯特采样和低比特分辨率ADC，引入多尺度多周期虚拟判别器，实现流式操作。", "result": "功耗降低3.31倍，推理时间1.74ms，内存占用小于13.77MB，实现高质量音频。", "conclusion": "SUBARU在低功耗和高质量音频之间取得了平衡，适用于移动平台和嘈杂环境。"}}
{"id": "2506.22331", "pdf": "https://arxiv.org/pdf/2506.22331", "abs": "https://arxiv.org/abs/2506.22331", "authors": ["Adiba Ejaz", "Elias Bareinboim"], "title": "Less Greedy Equivalence Search", "categories": ["cs.LG", "cs.AI", "stat.ME", "stat.ML"], "comment": "35 total pages. 14 figures", "summary": "Greedy Equivalence Search (GES) is a classic score-based algorithm for causal\ndiscovery from observational data. In the sample limit, it recovers the Markov\nequivalence class of graphs that describe the data. Still, it faces two\nchallenges in practice: computational cost and finite-sample accuracy. In this\npaper, we develop Less Greedy Equivalence Search (LGES), a variant of GES that\nretains its theoretical guarantees while partially addressing these\nlimitations. LGES modifies the greedy step: rather than always applying the\nhighest-scoring insertion, it avoids edge insertions between variables for\nwhich the score implies some conditional independence. This more targeted\nsearch yields up to a \\(10\\)-fold speed-up and a substantial reduction in\nstructural error relative to GES. Moreover, LGES can guide the search using\nprior assumptions, while correcting these assumptions when contradicted by the\ndata. Finally, LGES can exploit interventional data to refine the learned\nobservational equivalence class. We prove that LGES recovers the true\nequivalence class in the sample limit from observational and interventional\ndata, even with misspecified prior assumptions. Experiments demonstrate that\nLGES outperforms GES and other baselines in speed, accuracy, and robustness to\nmisspecified assumptions. Our code is available at\nhttps://github.com/CausalAILab/lges.", "AI": {"tldr": "LGES是GES的改进版本，通过优化贪婪步骤减少计算成本和结构错误，同时支持先验假设和数据修正。", "motivation": "解决GES在计算成本和有限样本准确性方面的局限性。", "method": "修改贪婪步骤，避免在条件独立变量间插入边，支持先验假设和数据修正。", "result": "LGES在速度和准确性上优于GES，且能处理错误先验假设。", "conclusion": "LGES在理论和实验中均表现优异，适用于观测和干预数据。"}}
{"id": "2506.22343", "pdf": "https://arxiv.org/pdf/2506.22343", "abs": "https://arxiv.org/abs/2506.22343", "authors": ["Xiang Li", "Garrett Wen", "Weiqing He", "Jiayuan Wu", "Qi Long", "Weijie J. Su"], "title": "Optimal Estimation of Watermark Proportions in Hybrid AI-Human Texts", "categories": ["stat.ML", "cs.CL", "cs.LG", "stat.ME"], "comment": null, "summary": "Text watermarks in large language models (LLMs) are an increasingly important\ntool for detecting synthetic text and distinguishing human-written content from\nLLM-generated text. While most existing studies focus on determining whether\nentire texts are watermarked, many real-world scenarios involve mixed-source\ntexts, which blend human-written and watermarked content. In this paper, we\naddress the problem of optimally estimating the watermark proportion in\nmixed-source texts. We cast this problem as estimating the proportion parameter\nin a mixture model based on \\emph{pivotal statistics}. First, we show that this\nparameter is not even identifiable in certain watermarking schemes, let alone\nconsistently estimable. In stark contrast, for watermarking methods that employ\ncontinuous pivotal statistics for detection, we demonstrate that the proportion\nparameter is identifiable under mild conditions. We propose efficient\nestimators for this class of methods, which include several popular unbiased\nwatermarks as examples, and derive minimax lower bounds for any measurable\nestimator based on pivotal statistics, showing that our estimators achieve\nthese lower bounds. Through evaluations on both synthetic data and mixed-source\ntext generated by open-source models, we demonstrate that our proposed\nestimators consistently achieve high estimation accuracy.", "AI": {"tldr": "本文研究了混合来源文本中水印比例的优化估计问题，提出了一种基于关键统计量的高效估计方法，并在理论和实验上验证了其准确性。", "motivation": "现有研究主要关注整段文本的水印检测，而现实场景中常出现混合来源文本（人工与LLM生成内容混合）。本文旨在解决混合文本中水印比例的估计问题。", "method": "将问题建模为基于关键统计量的混合模型参数估计，提出高效估计器，并推导极小极大下界。", "result": "实验表明，所提估计器在合成数据和开源模型生成的混合文本中均能实现高精度估计。", "conclusion": "对于使用连续关键统计量的水印方法，水印比例是可识别的，且所提估计器在理论和实践中均表现优异。"}}
{"id": "2506.22372", "pdf": "https://arxiv.org/pdf/2506.22372", "abs": "https://arxiv.org/abs/2506.22372", "authors": ["Maryam Mousavian", "Zahra Abbasiantaeb", "Mohammad Aliannejadi", "Fabio Crestani"], "title": "Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and Measurement", "categories": ["cs.IR", "cs.CL"], "comment": "Accepted by ACM SIGIR Conference on Innovative Concepts and Theories\n  in Information Retrieval (ICTIR 2025)", "summary": "The presence of social biases in Natural Language Processing (NLP) and\nInformation Retrieval (IR) systems is an ongoing challenge, which underlines\nthe importance of developing robust approaches to identifying and evaluating\nsuch biases. In this paper, we aim to address this issue by leveraging Large\nLanguage Models (LLMs) to detect and measure gender bias in passage ranking.\nExisting gender fairness metrics rely on lexical- and frequency-based measures,\nleading to various limitations, e.g., missing subtle gender disparities.\nBuilding on our LLM-based gender bias detection method, we introduce a novel\ngender fairness metric, named Class-wise Weighted Exposure (CWEx), aiming to\naddress existing limitations. To measure the effectiveness of our proposed\nmetric and study LLMs' effectiveness in detecting gender bias, we annotate a\nsubset of the MS MARCO Passage Ranking collection and release our new gender\nbias collection, called MSMGenderBias, to foster future research in this area.\nOur extensive experimental results on various ranking models show that our\nproposed metric offers a more detailed evaluation of fairness compared to\nprevious metrics, with improved alignment to human labels (58.77% for\nGrep-BiasIR, and 18.51% for MSMGenderBias, measured using Cohen's Kappa\nagreement), effectively distinguishing gender bias in ranking. By integrating\nLLM-driven bias detection, an improved fairness metric, and gender bias\nannotations for an established dataset, this work provides a more robust\nframework for analyzing and mitigating bias in IR systems.", "AI": {"tldr": "论文提出了一种基于大语言模型（LLM）的性别偏见检测方法，并引入新的公平性度量CWEx，以改进现有方法的局限性。实验表明CWEx能更细致地评估公平性，并与人工标注更一致。", "motivation": "解决NLP和IR系统中存在的性别偏见问题，尤其是现有基于词法和频率的度量方法的不足。", "method": "利用LLM检测和测量段落排名中的性别偏见，提出新的公平性度量CWEx，并在MS MARCO数据集上验证。", "result": "CWEx比现有度量方法更细致地评估公平性，与人工标注的一致性更高（Cohen's Kappa分别为58.77%和18.51%）。", "conclusion": "通过LLM驱动的偏见检测、改进的公平性度量和标注数据集，为IR系统提供了更鲁棒的偏见分析和缓解框架。"}}
{"id": "2506.22342", "pdf": "https://arxiv.org/pdf/2506.22342", "abs": "https://arxiv.org/abs/2506.22342", "authors": ["Zihan Guan", "Zhiyuan Zhao", "Fengwei Tian", "Dung Nguyen", "Payel Bhattacharjee", "Ravi Tandon", "B. Aditya Prakash", "Anil Vullikanti"], "title": "A Framework for Multi-source Privacy Preserving Epidemic Analysis", "categories": ["cs.LG", "cs.AI"], "comment": "17 pages, 6 figures", "summary": "It is now well understood that diverse datasets provide a lot of value in key\nepidemiology and public health analyses, such as forecasting and nowcasting,\ndevelopment of epidemic models, evaluation and design of interventions and\nresource allocation. Some of these datasets are often sensitive, and need\nadequate privacy protections. There are many models of privacy, but\nDifferential Privacy (DP) has become a de facto standard because of its strong\nguarantees, without making models about adversaries. In this paper, we develop\na framework the integrates deep learning and epidemic models to simultaneously\nperform epidemic forecasting and learning a mechanistic model of epidemic\nspread, while incorporating multiple datasets for these analyses, including\nsome with DP guarantees. We demonstrate our framework using a realistic but\nsynthetic financial dataset with DP; such a dataset has not been used in such\nepidemic analyses. We show that this dataset provides significant value in\nforecasting and learning an epidemic model, even when used with DP guarantees.", "AI": {"tldr": "论文提出了一种结合深度学习和流行病模型的框架，用于流行病预测和传播机制建模，同时整合了多种数据集（包括差分隐私保护的数据），并通过合成金融数据集验证了其有效性。", "motivation": "多样化的数据集在流行病学和公共卫生分析中具有重要价值，但部分数据敏感且需隐私保护。差分隐私（DP）因其强保障成为标准。本文旨在开发一个框架，结合深度学习和流行病模型，利用多数据集（包括DP保护数据）进行预测和建模。", "method": "开发了一个整合深度学习和流行病模型的框架，用于同时进行流行病预测和传播机制建模，并整合了多种数据集（包括DP保护数据）。通过合成金融数据集验证框架。", "result": "合成金融数据集（带DP保障）在预测和流行病模型学习中提供了显著价值，即使在使用DP保护时仍有效。", "conclusion": "提出的框架成功整合了深度学习和流行病模型，利用多数据集（包括DP保护数据）进行预测和建模，证明了DP保护数据在此类分析中的实用性。"}}
{"id": "2506.22376", "pdf": "https://arxiv.org/pdf/2506.22376", "abs": "https://arxiv.org/abs/2506.22376", "authors": ["Youkang Wang", "Jian Wang", "Rubing Chen", "Xiao-Yong Wei", "Qing Li"], "title": "Probabilistic Optimality for Inference-time Scaling", "categories": ["cs.LG", "cs.AI", "cs.CL"], "comment": null, "summary": "Inference-time scaling has emerged as a powerful technique for enhancing the\nreasoning performance of Large Language Models (LLMs). However, existing\napproaches often rely on heuristic strategies for parallel sampling, lacking a\nprincipled foundation. To address this gap, we propose a probabilistic\nframework that formalizes the optimality of inference-time scaling under the\nassumption that parallel samples are independently and identically distributed\n(i.i.d.), and where the Best-of-N selection strategy follows a probability\ndistribution that can be estimated. Within this framework, we derive a\ntheoretical lower bound on the required number of samples to achieve a target\nperformance level, providing the first principled guidance for\ncompute-efficient scaling. Leveraging this insight, we develop\n\\textsc{OptScale}, a practical algorithm that dynamically determines the\noptimal number of sampled responses. \\textsc{OptScale} employs a language\nmodel-based predictor to estimate probabilistic prior parameters, enabling the\ndecision of the minimal number of samples needed that satisfy predefined\nperformance thresholds and confidence levels. Extensive experiments on\nmathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC)\ndemonstrate that \\textsc{OptScale} significantly reduces sampling overhead\nwhile remaining better or on par with state-of-the-art reasoning performance.\nOur work offers both a theoretical foundation and a practical solution for\nprincipled inference-time scaling, addressing a critical gap in the efficient\ndeployment of LLMs for complex reasoning.", "AI": {"tldr": "提出了一种基于概率框架的推理时扩展方法OptScale，通过动态确定最优样本数，显著减少计算开销，同时保持或超越现有推理性能。", "motivation": "现有推理时扩展方法依赖启发式策略，缺乏理论基础，无法高效指导计算资源的分配。", "method": "提出概率框架，假设并行样本独立同分布，推导理论下限，开发OptScale算法动态确定最优样本数。", "result": "在多个数学推理基准测试中，OptScale显著减少采样开销，性能优于或与现有方法相当。", "conclusion": "为推理时扩展提供了理论基础和实用解决方案，填补了LLM高效部署的空白。"}}
{"id": "2506.22359", "pdf": "https://arxiv.org/pdf/2506.22359", "abs": "https://arxiv.org/abs/2506.22359", "authors": ["Viswanath Kumarskandpriya", "Abdulhalim Dandoush", "Abbas Bradai", "Ali Belgacem"], "title": "Concept-Level AI for Telecom: Moving Beyond Large Language Models", "categories": ["cs.NI", "cs.AI"], "comment": null, "summary": "The telecommunications and networking domain stands at the precipice of a\ntransformative era, driven by the necessity to manage increasingly complex,\nhierarchical, multi administrative domains (i.e., several operators on the same\npath) and multilingual systems. Recent research has demonstrated that Large\nLanguage Models (LLMs), with their exceptional general-purpose text analysis\nand code generation capabilities, can be effectively applied to certain telecom\nproblems (e.g., auto-configuration of data plan to meet certain application\nrequirements). However, due to their inherent token-by-token processing and\nlimited capacity for maintaining extended context, LLMs struggle to fulfill\ntelecom-specific requirements such as cross-layer dependency cascades (i.e.,\nover OSI), temporal-spatial fault correlation, and real-time distributed\ncoordination. In contrast, Large Concept Models (LCMs), which reason at the\nabstraction level of semantic concepts rather than individual lexical tokens,\noffer a fundamentally superior approach for addressing these telecom\nchallenges. By employing hyperbolic latent spaces for hierarchical\nrepresentation and encapsulating complex multi-layered network interactions\nwithin concise concept embeddings, LCMs overcome critical shortcomings of LLMs\nin terms of memory efficiency, cross-layer correlation, and native multimodal\nintegration. This paper argues that adopting LCMs is not simply an incremental\nstep, but a necessary evolutionary leap toward achieving robust and effective\nAI-driven telecom management.", "AI": {"tldr": "论文探讨了在电信和网络领域，大型概念模型（LCMs）相比大型语言模型（LLMs）在处理复杂、分层和多语言系统时的优势。", "motivation": "电信领域面临复杂性和多域管理的挑战，现有LLMs因处理方式和上下文限制难以满足需求。", "method": "提出使用LCMs，通过语义概念抽象和双曲潜在空间表示来解决LLMs的不足。", "result": "LCMs在内存效率、跨层关联和多模态集成方面表现优于LLMs。", "conclusion": "采用LCMs是实现稳健、高效AI驱动电信管理的必要进化步骤。"}}
{"id": "2506.22374", "pdf": "https://arxiv.org/pdf/2506.22374", "abs": "https://arxiv.org/abs/2506.22374", "authors": ["Abdulmomen Ghalkha", "Zhuojun Tian", "Chaouki Ben Issaid", "Mehdi Bennis"], "title": "Sheaf-Based Decentralized Multimodal Learning for Next-Generation Wireless Communication Systems", "categories": ["cs.LG", "cs.AI"], "comment": "13 pages, 9 figures", "summary": "In large-scale communication systems, increasingly complex scenarios require\nmore intelligent collaboration among edge devices collecting various multimodal\nsensory data to achieve a more comprehensive understanding of the environment\nand improve decision-making accuracy. However, conventional federated learning\n(FL) algorithms typically consider unimodal datasets, require identical model\narchitectures, and fail to leverage the rich information embedded in multimodal\ndata, limiting their applicability to real-world scenarios with diverse\nmodalities and varying client capabilities. To address this issue, we propose\nSheaf-DMFL, a novel decentralized multimodal learning framework leveraging\nsheaf theory to enhance collaboration among devices with diverse modalities.\nSpecifically, each client has a set of local feature encoders for its different\nmodalities, whose outputs are concatenated before passing through a\ntask-specific layer. While encoders for the same modality are trained\ncollaboratively across clients, we capture the intrinsic correlations among\nclients' task-specific layers using a sheaf-based structure. To further enhance\nlearning capability, we propose an enhanced algorithm named Sheaf-DMFL-Att,\nwhich tailors the attention mechanism within each client to capture\ncorrelations among different modalities. A rigorous convergence analysis of\nSheaf-DMFL-Att is provided, establishing its theoretical guarantees. Extensive\nsimulations are conducted on real-world link blockage prediction and mmWave\nbeamforming scenarios, demonstrate the superiority of the proposed algorithms\nin such heterogeneous wireless communication systems.", "AI": {"tldr": "提出了一种名为Sheaf-DMFL的新型去中心化多模态学习框架，利用层理论增强多模态设备间的协作，并通过注意力机制进一步优化。", "motivation": "传统联邦学习算法通常仅考虑单模态数据，且要求模型架构一致，无法充分利用多模态数据的丰富信息，限制了其在现实场景中的应用。", "method": "每个客户端为不同模态配备本地特征编码器，输出拼接后通过任务特定层；利用层理论捕捉任务层间的内在关联，并提出Sheaf-DMFL-Att算法，通过注意力机制优化模态间相关性。", "result": "在真实世界的链路阻塞预测和毫米波波束成形场景中，实验证明了所提算法在异构无线通信系统中的优越性。", "conclusion": "Sheaf-DMFL框架有效解决了多模态数据协作问题，并通过理论分析和实验验证了其性能优势。"}}
{"id": "2506.22389", "pdf": "https://arxiv.org/pdf/2506.22389", "abs": "https://arxiv.org/abs/2506.22389", "authors": ["Aditya Cowsik", "Tianyu He", "Andrey Gromov"], "title": "Towards Distributed Neural Architectures", "categories": ["cs.LG", "cond-mat.dis-nn", "cs.AI"], "comment": "36 pages, 25 figures", "summary": "We introduce and train distributed neural architectures (DNA) in vision and\nlanguage domains. DNAs are initialized with a proto-architecture that consists\nof (transformer, MLP, attention, etc.) modules and routers. Any token (or\npatch) can traverse any series of modules in any order. DNAs are a natural\ngeneralization of the sparse methods such as Mixture-of-Experts,\nMixture-of-Depths, parameter sharing, etc. Computation and communication\npatterns of DNA modules are learnt end-to-end during training and depend on the\ncontent and context of each token (or patch). These patterns can be shaped by\nfurther requirements added to the optimization objective such as compute/memory\nefficiency or load balancing. We empirically show that (i) trained DNAs are\ncompetitive with the dense baselines in both domains and (ii) compute\nefficiency/parameter sharing can be learnt from data. Next, we analyze the\nemergent connectivity and computation patterns in the trained DNAs. We find\nthat the paths that tokens take through the models are themselves distributed\naccording to a power-law. We show that some paths (or, equivalently, groups of\nmodules) show emergent specialization. Finally, we demonstrate that models\nlearn to allocate compute and active parameters in an interpretable way.", "AI": {"tldr": "论文提出了一种分布式神经架构（DNA），通过动态路由和模块组合实现高效计算和参数共享，在视觉和语言任务中表现优异。", "motivation": "探索如何通过动态路由和模块组合实现更高效的神经网络架构，同时保持与密集基线模型的竞争力。", "method": "使用包含多种模块（如Transformer、MLP、注意力等）的原型架构，通过端到端训练学习计算和通信模式。", "result": "DNA在视觉和语言任务中表现与密集基线相当，同时实现了计算效率和参数共享。", "conclusion": "DNA展示了动态路径选择和模块专业化的能力，为高效神经网络设计提供了新思路。"}}
{"id": "2506.22393", "pdf": "https://arxiv.org/pdf/2506.22393", "abs": "https://arxiv.org/abs/2506.22393", "authors": ["YongKyung Oh", "Alex Bui"], "title": "Multi-View Contrastive Learning for Robust Domain Adaptation in Medical Time Series Analysis", "categories": ["cs.LG", "cs.AI"], "comment": null, "summary": "Adapting machine learning models to medical time series across different\ndomains remains a challenge due to complex temporal dependencies and dynamic\ndistribution shifts. Current approaches often focus on isolated feature\nrepresentations, limiting their ability to fully capture the intricate temporal\ndynamics necessary for robust domain adaptation. In this work, we propose a\nnovel framework leveraging multi-view contrastive learning to integrate\ntemporal patterns, derivative-based dynamics, and frequency-domain features.\nOur method employs independent encoders and a hierarchical fusion mechanism to\nlearn feature-invariant representations that are transferable across domains\nwhile preserving temporal coherence. Extensive experiments on diverse medical\ndatasets, including electroencephalogram (EEG), electrocardiogram (ECG), and\nelectromyography (EMG) demonstrate that our approach significantly outperforms\nstate-of-the-art methods in transfer learning tasks. By advancing the\nrobustness and generalizability of machine learning models, our framework\noffers a practical pathway for deploying reliable AI systems in diverse\nhealthcare settings.", "AI": {"tldr": "提出了一种基于多视角对比学习的新框架，用于整合医学时间序列的多种特征，显著提升了跨领域迁移学习的性能。", "motivation": "医学时间序列数据存在复杂的时序依赖性和动态分布偏移，现有方法难以充分捕捉这些特性，限制了模型的鲁棒性和泛化能力。", "method": "采用多视角对比学习框架，结合时序模式、基于导数的动态特性和频域特征，通过独立编码器和分层融合机制学习跨领域可迁移的特征不变表示。", "result": "在多种医学数据集（如EEG、ECG和EMG）上的实验表明，该方法在迁移学习任务中显著优于现有技术。", "conclusion": "该框架为在多样化医疗场景中部署可靠的AI系统提供了实用途径，提升了模型的鲁棒性和泛化能力。"}}
{"id": "2506.22427", "pdf": "https://arxiv.org/pdf/2506.22427", "abs": "https://arxiv.org/abs/2506.22427", "authors": ["Randeep Bhatia", "Nikos Papadis", "Murali Kodialam", "TV Lakshman", "Sayak Chakrabarty"], "title": "CLoVE: Personalized Federated Learning through Clustering of Loss Vector Embeddings", "categories": ["cs.LG", "cs.AI"], "comment": "31 pages, 4 figures", "summary": "We propose CLoVE (Clustering of Loss Vector Embeddings), a novel algorithm\nfor Clustered Federated Learning (CFL). In CFL, clients are naturally grouped\ninto clusters based on their data distribution. However, identifying these\nclusters is challenging, as client assignments are unknown. CLoVE utilizes\nclient embeddings derived from model losses on client data, and leverages the\ninsight that clients in the same cluster share similar loss values, while those\nin different clusters exhibit distinct loss patterns. Based on these\nembeddings, CLoVE is able to iteratively identify and separate clients from\ndifferent clusters and optimize cluster-specific models through federated\naggregation. Key advantages of CLoVE over existing CFL algorithms are (1) its\nsimplicity, (2) its applicability to both supervised and unsupervised settings,\nand (3) the fact that it eliminates the need for near-optimal model\ninitialization, which makes it more robust and better suited for real-world\napplications. We establish theoretical convergence bounds, showing that CLoVE\ncan recover clusters accurately with high probability in a single round and\nconverges exponentially fast to optimal models in a linear setting. Our\ncomprehensive experiments comparing with a variety of both CFL and generic\nPersonalized Federated Learning (PFL) algorithms on different types of datasets\nand an extensive array of non-IID settings demonstrate that CLoVE achieves\nhighly accurate cluster recovery in just a few rounds of training, along with\nstate-of-the-art model accuracy, across a variety of both supervised and\nunsupervised PFL tasks.", "AI": {"tldr": "CLoVE是一种新颖的聚类联邦学习算法，通过损失向量嵌入识别客户集群，无需最优初始化，适用于监督和无监督任务。", "motivation": "解决聚类联邦学习中客户集群识别困难的问题，利用损失值相似性区分不同集群。", "method": "基于客户数据的模型损失生成嵌入，迭代分离不同集群并优化集群特定模型。", "result": "理论证明单轮高概率准确恢复集群，实验显示快速收敛和最优模型精度。", "conclusion": "CLoVE在集群恢复和模型精度上优于现有算法，适用于实际应用。"}}
