<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 50]
- [cs.CL](#cs.CL) [Total: 85]
- [cs.CV](#cs.CV) [Total: 175]
- [cs.LO](#cs.LO) [Total: 2]
- [physics.geo-ph](#physics.geo-ph) [Total: 1]
- [physics.med-ph](#physics.med-ph) [Total: 2]
- [cs.LG](#cs.LG) [Total: 86]
- [cs.DB](#cs.DB) [Total: 1]
- [math.OC](#math.OC) [Total: 1]
- [cs.CR](#cs.CR) [Total: 12]
- [cs.SE](#cs.SE) [Total: 8]
- [math.PR](#math.PR) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.DC](#cs.DC) [Total: 4]
- [cs.DL](#cs.DL) [Total: 2]
- [cs.SD](#cs.SD) [Total: 9]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 2]
- [eess.AS](#eess.AS) [Total: 1]
- [cs.MA](#cs.MA) [Total: 2]
- [cs.MM](#cs.MM) [Total: 1]
- [stat.ML](#stat.ML) [Total: 1]
- [q-bio.NC](#q-bio.NC) [Total: 2]
- [cs.GR](#cs.GR) [Total: 8]
- [eess.SY](#eess.SY) [Total: 4]
- [cs.FL](#cs.FL) [Total: 1]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.RO](#cs.RO) [Total: 19]
- [cs.HC](#cs.HC) [Total: 5]
- [cs.IR](#cs.IR) [Total: 15]
- [cs.CY](#cs.CY) [Total: 9]
- [q-bio.BM](#q-bio.BM) [Total: 1]
- [eess.IV](#eess.IV) [Total: 9]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.CE](#cs.CE) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Evaluating Generalization and Representation Stability in Small LMs via Prompting](https://arxiv.org/abs/2506.17289)
*Rahul Raja,Arpita Vats*

Main category: cs.AI

TL;DR: 比较小语言模型在少样本提示和监督微调下的泛化能力，分析其在不同任务和分布下的表现。


<details>
  <summary>Details</summary>
Motivation: 探讨少样本提示和监督微调在低资源设置和分布变化下的鲁棒性差异。

Method: 通过比较不同任务、提示风格和模型规模下的表现，分析内部表示。

Result: 发现两种方法在知识内化和泛化方面存在显著差异。

Conclusion: 为低数据环境下的模型选择提供指导，并推动提示与微调的讨论。

Abstract: We investigate the generalization capabilities of small language models under
two popular adaptation paradigms: few-shot prompting and supervised
fine-tuning. While prompting is often favored for its parameter efficiency and
flexibility, it remains unclear how robust this approach is in low-resource
settings and under distributional shifts. This paper presents a comparative
study of prompting and fine-tuning across task formats, prompt styles, and
model scales, with a focus on their behavior in both in-distribution and
out-of-distribution (OOD) settings.
  Beyond accuracy, we analyze the internal representations learned by each
approach to assess the stability and abstraction of task-specific features. Our
findings highlight critical differences in how small models internalize and
generalize knowledge under different adaptation strategies. This work offers
practical guidance for model selection in low-data regimes and contributes
empirical insight into the ongoing debate over prompting versus fine-tuning.
Code for the experiments is available at the following

</details>


### [2] [Individual Causal Inference with Structural Causal Model](https://arxiv.org/abs/2506.17300)
*Daniel T. Chang*

Main category: cs.AI

TL;DR: 论文提出了一种基于结构因果模型（SCM）的个体因果推断（ICI）方法，通过个体化操作符（indiv(W)）和个体因果查询（P(Y | indiv(W), do(X), Z)）来估计个体因果效应（ICE）。


<details>
  <summary>Details</summary>
Motivation: 传统因果推断方法多为群体层面，难以直接应用于个体。本文旨在利用SCM中的外生变量（U）编码个体差异，实现个体化因果推断。

Method: 提出个体化操作符indiv(W)和个体因果查询P(Y | indiv(W), do(X), Z)，将SCM应用于个体层面。

Result: 论证了ICI是基于个体可能性的推断，而非反事实推断。

Conclusion: SCM为个体因果推断提供了理论基础，通过个体化操作符和查询实现了对个体干预效果的估计。

Abstract: Individual causal inference (ICI) uses causal inference methods to understand
and predict the effects of interventions on individuals, considering their
specific characteristics / facts. It aims to estimate individual causal effect
(ICE), which varies across individuals. Estimating ICE can be challenging due
to the limited data available for individuals, and the fact that most causal
inference methods are population-based. Structural Causal Model (SCM) is
fundamentally population-based. Therefore, causal discovery (structural
learning and parameter learning), association queries and intervention queries
are all naturally population-based. However, exogenous variables (U) in SCM can
encode individual variations and thus provide the mechanism for individualized
population per specific individual characteristics / facts. Based on this, we
propose ICI with SCM as a "rung 3" causal inference, because it involves
"imagining" what would be the causal effect of a hypothetical intervention on
an individual, given the individual's observed characteristics / facts.
Specifically, we propose the indiv-operator, indiv(W), to formalize/represent
the population individualization process, and the individual causal query, P(Y
| indiv(W), do(X), Z), to formalize/represent ICI. We show and argue that ICI
with SCM is inference on individual alternatives (possible), not individual
counterfactuals (non-actual).

</details>


### [3] [Resource Rational Contractualism Should Guide AI Alignment](https://arxiv.org/abs/2506.17434)
*Sydney Levine,Matija Franklin,Tan Zhi-Xuan,Secil Yanik Guyot,Lionel Wong,Daniel Kilov,Yejin Choi,Joshua B. Tenenbaum,Noah Goodman,Seth Lazar,Iason Gabriel*

Main category: cs.AI

TL;DR: 论文提出资源理性契约主义（RRC），通过启发式方法高效模拟多方理性协议，以解决AI在复杂人类环境中的决策问题。


<details>
  <summary>Details</summary>
Motivation: AI需要在目标与价值观分歧的环境中做出决策，传统契约主义方法成本高且效率低。

Method: 采用基于规范和认知启发的启发式工具箱，权衡精度与效率，模拟理性多方协议。

Result: RRC框架使AI能高效运作并动态适应变化的人类社会环境。

Conclusion: RRC为AI在复杂社会环境中提供了一种高效且适应性强的决策框架。

Abstract: AI systems will soon have to navigate human environments and make decisions
that affect people and other AI agents whose goals and values diverge.
Contractualist alignment proposes grounding those decisions in agreements that
diverse stakeholders would endorse under the right conditions, yet securing
such agreement at scale remains costly and slow -- even for advanced AI. We
therefore propose Resource-Rational Contractualism (RRC): a framework where AI
systems approximate the agreements rational parties would form by drawing on a
toolbox of normatively-grounded, cognitively-inspired heuristics that trade
effort for accuracy. An RRC-aligned agent would not only operate efficiently,
but also be equipped to dynamically adapt to and interpret the ever-changing
human social world.

</details>


### [4] [Keeping Medical AI Healthy: A Review of Detection and Correction Methods for System Degradation](https://arxiv.org/abs/2506.17442)
*Hao Guan,David Bates,Li Zhou*

Main category: cs.AI

TL;DR: 该论文综述了医疗AI系统在长期部署中性能下降的原因及应对策略，强调持续监控、早期检测和自我纠正的重要性。


<details>
  <summary>Details</summary>
Motivation: 医疗AI系统在动态临床环境中性能可能下降，影响可靠性和安全性，需研究如何维持其长期健康。

Method: 回顾性能下降的常见原因，总结数据与模型漂移检测技术，分析根源并提出纠正策略（如模型再训练）。

Result: 提供了从传统机器学习到大型语言模型的性能维护方法，并指出其优缺点。

Conclusion: 论文为开发可靠、稳健的医疗AI系统提供了指导，并提出了未来研究方向。

Abstract: Artificial intelligence (AI) is increasingly integrated into modern
healthcare, offering powerful support for clinical decision-making. However, in
real-world settings, AI systems may experience performance degradation over
time, due to factors such as shifting data distributions, changes in patient
characteristics, evolving clinical protocols, and variations in data quality.
These factors can compromise model reliability, posing safety concerns and
increasing the likelihood of inaccurate predictions or adverse outcomes. This
review presents a forward-looking perspective on monitoring and maintaining the
"health" of AI systems in healthcare. We highlight the urgent need for
continuous performance monitoring, early degradation detection, and effective
self-correction mechanisms. The paper begins by reviewing common causes of
performance degradation at both data and model levels. We then summarize key
techniques for detecting data and model drift, followed by an in-depth look at
root cause analysis. Correction strategies are further reviewed, ranging from
model retraining to test-time adaptation. Our survey spans both traditional
machine learning models and state-of-the-art large language models (LLMs),
offering insights into their strengths and limitations. Finally, we discuss
ongoing technical challenges and propose future research directions. This work
aims to guide the development of reliable, robust medical AI systems capable of
sustaining safe, long-term deployment in dynamic clinical settings.

</details>


### [5] [OmniReflect: Discovering Transferable Constitutions for LLM agents via Neuro-Symbolic Reflections](https://arxiv.org/abs/2506.17449)
*Manasa Bharadwaj,Nikhil Verma,Kevin Ferreira*

Main category: cs.AI

TL;DR: OmniReflect是一个层次化、反思驱动的框架，通过构建指导原则（constitution）提升LLM代理的性能，在Self-sustaining和Co-operative两种模式下均显著提高了任务成功率。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如微调和自我修正）缺乏长期学习的通用机制且在动态环境中效率低下，因此需要一种更有效的框架。

Method: OmniReflect采用Neural、Symbolic和NeuroSymbolic技术构建指导原则，支持Self-sustaining（单代理自我反思）和Co-operative（Meta-advisor指导代理）两种模式。

Result: 在ALFWorld、BabyAI和PDDL任务中，Self-sustaining模式分别提升10.3%、23.8%和8.3%的成功率；Co-operative模式下轻量级Qwen3-4B代理优于所有基线。

Conclusion: OmniReflect在多种环境和模型上表现出高效性和鲁棒性，为LLM代理的长期学习提供了新思路。

Abstract: Efforts to improve Large Language Model (LLM) agent performance on complex
tasks have largely focused on fine-tuning and iterative self-correction.
However, these approaches often lack generalizable mechanisms for longterm
learning and remain inefficient in dynamic environments. We introduce
OmniReflect, a hierarchical, reflection-driven framework that constructs a
constitution, a compact set of guiding principles distilled from task
experiences, to enhance the effectiveness and efficiency of an LLM agent.
OmniReflect operates in two modes: Self-sustaining, where a single agent
periodically curates its own reflections during task execution, and
Co-operative, where a Meta-advisor derives a constitution from a small
calibration set to guide another agent. To construct these constitutional
principles, we employ Neural, Symbolic, and NeuroSymbolic techniques, offering
a balance between contextual adaptability and computational efficiency.
Empirical results averaged across models show major improvements in task
success, with absolute gains of +10.3% on ALFWorld, +23.8% on BabyAI, and +8.3%
on PDDL in the Self-sustaining mode. Similar gains are seen in the Co-operative
mode, where a lightweight Qwen3-4B ReAct agent outperforms all Reflexion
baselines on BabyAI. These findings highlight the robustness and effectiveness
of OmniReflect across environments and backbones.

</details>


### [6] [From Unstructured Communication to Intelligent RAG: Multi-Agent Automation for Supply Chain Knowledge Bases](https://arxiv.org/abs/2506.17484)
*Yao Zhang,Zaixi Shang,Silpan Patel,Mikel Zuniga*

Main category: cs.AI

TL;DR: 论文提出了一种基于LLM的多智能体系统，将供应链支持票据转化为结构化知识库，显著提升RAG系统的性能。


<details>
  <summary>Details</summary>
Motivation: 供应链操作数据庞大但知识分散在非结构化通讯中，现有RAG系统因数据质量问题效果有限。

Method: 采用离线优先方法，通过三个智能体（分类发现、分类、知识合成）将支持票据转化为结构化知识库。

Result: 知识库体积缩减至原始数据的3.4%，RAG系统性能提升（48.74% vs. 38.60%有用回答），减少77.4%无用响应。

Conclusion: 该方法通过离线处理将非结构化通讯转化为可重用知识，显著提升操作效率并减少支持工作量。

Abstract: Supply chain operations generate vast amounts of operational data; however,
critical knowledge such as system usage practices, troubleshooting workflows,
and resolution techniques often remains buried within unstructured
communications like support tickets, emails, and chat logs. While RAG systems
aim to leverage such communications as a knowledge base, their effectiveness is
limited by raw data challenges: support tickets are typically noisy,
inconsistent, and incomplete, making direct retrieval suboptimal. Unlike
existing RAG approaches that focus on runtime optimization, we introduce a
novel offline-first methodology that transforms these communications into a
structured knowledge base. Our key innovation is a LLMs-based multi-agent
system orchestrating three specialized agents: Category Discovery for taxonomy
creation, Categorization for ticket grouping, and Knowledge Synthesis for
article generation. Applying our methodology to real-world support tickets with
resolution notes and comments, our system creates a compact knowledge base -
reducing total volume to just 3.4% of original ticket data while improving
quality. Experiments demonstrate that our prebuilt knowledge base in RAG
systems significantly outperforms traditional RAG implementations (48.74% vs.
38.60% helpful answers) and achieves a 77.4% reduction in unhelpful responses.
By automating institutional knowledge capture that typically remains siloed in
experts' heads, our solution translates to substantial operational efficiency:
reducing support workload, accelerating resolution times, and creating
self-improving systems that automatically resolve approximately 50% of future
supply chain tickets. Our approach addresses a key gap in knowledge management
by transforming transient communications into structured, reusable knowledge
through intelligent offline processing rather than latency-inducing runtime
architectures.

</details>


### [7] [Kaleidoscopic Teaming in Multi Agent Simulations](https://arxiv.org/abs/2506.17514)
*Ninareh Mehrabi,Tharindu Kumarage,Kai-Wei Chang,Aram Galstyan,Rahul Gupta*

Main category: cs.AI

TL;DR: 论文提出了一种名为“万花筒式团队”的新框架，用于评估AI代理在单代理和多代理场景中的安全性，弥补现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有红队或安全评估框架无法全面评估复杂行为和交互中的安全风险，尤其是在多代理场景中。

Method: 引入万花筒式团队框架，生成多样化场景模拟现实人类社会，评估单代理和多代理的安全性，并采用上下文优化技术改进场景生成。

Result: 通过该框架识别了多种模型在代理用例中的安全漏洞。

Conclusion: 万花筒式团队框架为AI代理的安全性评估提供了更全面的方法。

Abstract: Warning: This paper contains content that may be inappropriate or offensive.
  AI agents have gained significant recent attention due to their autonomous
tool usage capabilities and their integration in various real-world
applications. This autonomy poses novel challenges for the safety of such
systems, both in single- and multi-agent scenarios. We argue that existing red
teaming or safety evaluation frameworks fall short in evaluating safety risks
in complex behaviors, thought processes and actions taken by agents. Moreover,
they fail to consider risks in multi-agent setups where various vulnerabilities
can be exposed when agents engage in complex behaviors and interactions with
each other. To address this shortcoming, we introduce the term kaleidoscopic
teaming which seeks to capture complex and wide range of vulnerabilities that
can happen in agents both in single-agent and multi-agent scenarios. We also
present a new kaleidoscopic teaming framework that generates a diverse array of
scenarios modeling real-world human societies. Our framework evaluates safety
of agents in both single-agent and multi-agent setups. In single-agent setup,
an agent is given a scenario that it needs to complete using the tools it has
access to. In multi-agent setup, multiple agents either compete against or
cooperate together to complete a task in the scenario through which we capture
existing safety vulnerabilities in agents. We introduce new in-context
optimization techniques that can be used in our kaleidoscopic teaming framework
to generate better scenarios for safety analysis. Lastly, we present
appropriate metrics that can be used along with our framework to measure safety
of agents. Utilizing our kaleidoscopic teaming framework, we identify
vulnerabilities in various models with respect to their safety in agentic
use-cases.

</details>


### [8] [Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language Models](https://arxiv.org/abs/2506.17585)
*Yukun Huang,Sanxing Chen,Jian Pei,Manzil Zaheer,Bhuwan Dhingra*

Main category: cs.AI

TL;DR: 论文探讨如何让语言模型在训练过程中可靠地引用文档，避免推理时依赖外部检索器。提出主动索引方法，显著提升引用精确度。


<details>
  <summary>Details</summary>
Motivation: 解决语言模型引用不可靠的问题，避免推理时的延迟和检索噪声。

Method: 两阶段方法：持续预训练绑定事实与文档标识符，指令调优引导引用行为。主动索引通过合成QA对增强训练。

Result: 主动索引在Qwen2.5-7B和3B上表现优于被动索引，引用精确度提升高达30.2%。

Conclusion: 主动索引方法有效提升语言模型的引用可靠性，且性能随数据量增加持续提升。

Abstract: Trustworthy language models should provide both correct and verifiable
answers. While language models can sometimes attribute their outputs to
pretraining data, their citations are often unreliable due to hallucination. As
a result, current systems insert citations by querying an external retriever at
inference time, introducing latency, infrastructure dependence, and
vulnerability to retrieval noise. We explore whether LLMs can be made to
reliably attribute to the documents seen during (continual)
pretraining--without test-time retrieval--by revising the training process. To
evaluate this, we release CitePretrainBench, a benchmark that mixes real-world
corpora (Wikipedia, Common Crawl, arXiv) with novel, unseen documents and
probes both short-form (single fact) and long-form (multi-fact) citation tasks.
Our approach follows a two-stage process: (1) continual pretraining to bind
facts to persistent document identifiers, and (2) instruction tuning to elicit
citation behavior. We find that simple Passive Indexing, which appends an
identifier to each document, helps memorize verbatim text but fails on
paraphrased or compositional facts. Instead, we propose Active Indexing, which
continually pretrains on synthetic QA pairs that (1) restate each fact in
diverse compositional forms, and (2) require bidirectional source-to-fact and
fact-to-source generation, jointly teaching the model to generate content from
a cited source and to attribute its own answers. Experiments with Qwen2.5-7B
and 3B show that Active Indexing consistently outperforms Passive Indexing
across all tasks and models, with citation precision gains up to 30.2 percent.
Our ablation studies reveal that performance continues to improve as we scale
the amount of augmented data, showing a clear upward trend even at 16 times the
original token count.

</details>


### [9] [Taming the Untamed: Graph-Based Knowledge Retrieval and Reasoning for MLLMs to Conquer the Unknown](https://arxiv.org/abs/2506.17589)
*Bowen Wang*

Main category: cs.AI

TL;DR: 论文探讨了多模态大语言模型（MLLMs）在罕见领域任务中的局限性，提出了一种基于视觉游戏认知的多模态知识图谱（MH-MMKG）和自主知识检索方法，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解决MLLMs在罕见领域任务中因知识不足而表现不佳的问题，探索如何有效利用多模态知识增强推理能力。

Method: 采用视觉游戏认知（如《怪物猎人：世界》）构建多模态知识图谱（MH-MMKG），并设计复杂查询任务；提出无需额外训练的多代理检索器，实现自主知识搜索。

Result: 实验表明，该方法显著提升了MLLMs在复杂知识检索和推理任务中的表现。

Conclusion: 研究为多模态知识增强推理提供了新视角，为未来研究奠定了基础。

Abstract: The real value of knowledge lies not just in its accumulation, but in its
potential to be harnessed effectively to conquer the unknown. Although recent
multimodal large language models (MLLMs) exhibit impressing multimodal
capabilities, they often fail in rarely encountered domain-specific tasks due
to limited relevant knowledge. To explore this, we adopt visual game cognition
as a testbed and select Monster Hunter: World as the target to construct a
multimodal knowledge graph (MH-MMKG), which incorporates multi-modalities and
intricate entity relations. We also design a series of challenging queries
based on MH-MMKG to evaluate the models' ability for complex knowledge
retrieval and reasoning. Furthermore, we propose a multi-agent retriever that
enables a model to autonomously search relevant knowledge without additional
training. Experimental results show that our approach significantly enhances
the performance of MLLMs, providing a new perspective on multimodal
knowledge-augmented reasoning and laying a solid foundation for future
research.

</details>


### [10] [Measuring and Augmenting Large Language Models for Solving Capture-the-Flag Challenges](https://arxiv.org/abs/2506.17644)
*Zimo Ji,Daoyuan Wu,Wenyuan Jiang,Pingchuan Ma,Zongjie Li,Shuai Wang*

Main category: cs.AI

TL;DR: 论文研究了LLMs在CTF竞赛中的表现，提出了CTFKnow基准和CTFAgent框架，显著提升了LLMs的解题能力。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在自动化解决CTF挑战中的潜力，并解决其在知识应用和策略调整上的不足。

Method: 构建CTFKnow基准（3,992题）评估LLMs，提出CTFAgent框架，包含两阶段RAG和环境增强模块。

Result: CTFAgent在数据集上性能提升80%，在picoCTF2024中排名前23.6%。

Conclusion: CTFAgent框架有效提升了LLMs在CTF中的表现，展示了其在网络安全教育中的潜力。

Abstract: Capture-the-Flag (CTF) competitions are crucial for cybersecurity education
and training. As large language models (LLMs) evolve, there is increasing
interest in their ability to automate CTF challenge solving. For example, DARPA
has organized the AIxCC competition since 2023 to advance AI-powered automated
offense and defense. However, this demands a combination of multiple abilities,
from knowledge to reasoning and further to actions. In this paper, we highlight
the importance of technical knowledge in solving CTF problems and deliberately
construct a focused benchmark, CTFKnow, with 3,992 questions to measure LLMs'
performance in this core aspect. Our study offers a focused and innovative
measurement of LLMs' capability in understanding CTF knowledge and applying it
to solve CTF challenges. Our key findings reveal that while LLMs possess
substantial technical knowledge, they falter in accurately applying this
knowledge to specific scenarios and adapting their strategies based on feedback
from the CTF environment.
  Based on insights derived from this measurement study, we propose CTFAgent, a
novel LLM-driven framework for advancing CTF problem-solving. CTFAgent
introduces two new modules: two-stage Retrieval Augmented Generation (RAG) and
interactive Environmental Augmentation, which enhance LLMs' technical knowledge
and vulnerability exploitation on CTF, respectively. Our experimental results
show that, on two popular CTF datasets, CTFAgent both achieves over 80%
performance improvement. Moreover, in the recent picoCTF2024 hosted by CMU,
CTFAgent ranked in the top 23.6% of nearly 7,000 participating teams. This
reflects the benefit of our measurement study and the potential of our
framework in advancing LLMs' capabilities in CTF problem-solving.

</details>


### [11] [PhysUniBench: An Undergraduate-Level Physics Reasoning Benchmark for Multimodal Models](https://arxiv.org/abs/2506.17667)
*Lintao Wang,Encheng Su,Jiaqi Liu,Pengze Li,Peng Xia,Jiabei Xiao,Wenlong Zhang,Xinnan Dai,Xi Chen,Yuan Meng,Mingyu Ding,Lei Bai,Wanli Ouyang,Shixiang Tang,Aoran Wang,Xinzhu Ma*

Main category: cs.AI

TL;DR: PhysUniBench是一个多模态基准测试，用于评估和改进多模态大语言模型在本科物理问题上的推理能力，揭示了当前模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前评估方法在捕捉本科物理问题的广度和复杂性方面存在不足，需要更严格的评估工具。

Method: 通过多阶段构建过程，包括专家评估、自动筛选和难度分级，创建了包含3,304个问题的PhysUniBench。

Result: 当前最先进的模型在物理推理上表现不佳，如GPT-4o mini准确率仅为34.2%。

Conclusion: PhysUniBench旨在推动AI在科学领域的发展，促进模型在物理推理和多模态理解上的进步。

Abstract: Physics problem-solving is a challenging domain for large AI models,
requiring integration of conceptual understanding, mathematical reasoning, and
interpretation of physical diagrams. Current evaluation methodologies show
notable limitations in capturing the breadth and complexity of
undergraduate-level physics, underscoring the need for more rigorous
assessments. To this end, we present PhysUniBench, a large-scale multimodal
benchmark designed to evaluate and improve the reasoning capabilities of
multimodal large language models (MLLMs) specifically on undergraduate-level
physics problems. PhysUniBench consists of 3,304 physics questions spanning 8
major sub-disciplines of physics, each accompanied by one visual diagrams. The
benchmark includes both open-ended and multiple-choice questions,
systematically curated and difficulty-rated through an iterative
model-in-the-loop process. The benchmark's construction involved a rigorous
multi-stage process, including multiple roll-outs, expert-level evaluation,
automated filtering of easily solved problems, and a nuanced difficulty grading
system with five levels. Through extensive experiments, we observe that current
state-of-the-art models encounter substantial challenges in physics reasoning.
For example, GPT-4o mini achieves only about 34.2\% accuracy in the proposed
PhysUniBench. These results highlight that current MLLMs struggle with advanced
physics reasoning, especially on multi-step problems and those requiring
precise diagram interpretation. By providing a broad and rigorous assessment
tool, PhysUniBench aims to drive progress in AI for Science, encouraging the
development of models with stronger physical reasoning, problem-solving skills,
and multimodal understanding. The benchmark and evaluation scripts are
available at https://prismax-team.github.io/PhysUniBenchmark/.

</details>


### [12] [Beyond Syntax: Action Semantics Learning for App Agents](https://arxiv.org/abs/2506.17697)
*Bohan Tang,Dezhao Luo,Jingxuan Chen,Shaogang Gong,Jianye Hao,Jun Wang,Kun Shao*

Main category: cs.AI

TL;DR: 论文提出了一种名为动作语义学习（ASL）的新框架，通过捕捉动作的语义而非语法形式，提升了App代理的准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于提示的解决方案依赖昂贵的封闭LLM API，而微调开源LLM的方法因语法学习范式导致分布外（OOD）脆弱性。

Method: ASL框架通过定义动作语义为用户界面状态转换，并引入语义估计器（SEE）计算语义奖励，训练代理生成语义一致的动作。

Result: 理论分析和实验表明，ASL在OOD问题上优于现有语法学习范式，显著提升了App代理的性能。

Conclusion: ASL通过语义学习解决了现有方法的局限性，为App代理提供了更鲁棒和高效的解决方案。

Abstract: The advent of Large Language Models (LLMs) enables the rise of App agents
that interpret user intent and operate smartphone Apps through actions such as
clicking and scrolling. While prompt-based solutions with closed LLM APIs show
promising ability, they incur heavy compute costs and external API dependency.
Fine-tuning smaller open-source LLMs solves these limitations. However, current
fine-tuning methods use a syntax learning paradigm that forces agents to
reproduce exactly the ground truth action strings, leading to
out-of-distribution (OOD) vulnerability. To fill this gap, we propose Action
Semantics Learning (ASL), a novel learning framework, where the learning
objective is capturing the semantics of the ground truth actions. Specifically,
inspired by the programming language theory, we define the action semantics for
App agents as the state transition induced by the action in the user interface.
With this insight, ASL employs a novel SEmantic Estimator (SEE) to compute a
semantic reward to train the App agents in generating actions aligned with the
semantics of ground truth actions, even when the syntactic forms differ. To
support the effectiveness of ASL, we theoretically demonstrate the superior
robustness of ASL for the OOD problem compared with the existing syntax
learning paradigm. Extensive experiments on offline and online smartphone App
operation benchmarks show that ASL significantly improves the accuracy and
generalisation of App agents over existing methods.

</details>


### [13] [AnyMAC: Cascading Flexible Multi-Agent Collaboration via Next-Agent Prediction](https://arxiv.org/abs/2506.17784)
*Song Wang,Zhen Tan,Zihan Chen,Shuang Zhou,Tianlong Chen,Jundong Li*

Main category: cs.AI

TL;DR: 本文提出了一种基于序列结构的多智能体协作框架，取代传统的静态或图结构，通过动态选择智能体角色和上下文信息，显著提升了通信的灵活性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖静态或图结构的智能体间拓扑，缺乏通信的适应性和灵活性，限制了多智能体协作的潜力。

Method: 提出了一种新框架，采用序列结构而非图结构，包含两个关键方向：1) Next-Agent Prediction（动态选择智能体角色）；2) Next-Context Selection（选择性访问历史信息）。

Result: 在多个基准测试中表现出优越性能，同时大幅降低了通信开销。

Conclusion: 序列结构的多智能体协作框架在提升灵活性和效率方面具有显著优势，为未来研究提供了新方向。

Abstract: Recent progress in large language model (LLM)-based multi-agent collaboration
highlights the power of structured communication in enabling collective
intelligence. However, existing methods largely rely on static or graph-based
inter-agent topologies, lacking the potential adaptability and flexibility in
communication. In this work, we propose a new framework that rethinks
multi-agent coordination through a sequential structure rather than a graph
structure, offering a significantly larger topology space for multi-agent
communication. Our method focuses on two key directions: (1) Next-Agent
Prediction, which selects the most suitable agent role at each step, and (2)
Next-Context Selection (NCS), which enables each agent to selectively access
relevant information from any previous step. Together, these components
construct task-adaptive communication pipelines that support both role
flexibility and global information flow. Extensive evaluations across multiple
benchmarks demonstrate that our approach achieves superior performance while
substantially reducing communication overhead.

</details>


### [14] [Bayesian Social Deduction with Graph-Informed Language Models](https://arxiv.org/abs/2506.17788)
*Shahab Rahimirad,Guven Gergerli,Lucia Romero,Angela Qian,Matthew Lyle Olson,Simon Stepputtis,Joseph Campbell*

Main category: cs.AI

TL;DR: 论文提出了一种混合推理框架，结合结构化概率模型和LLM，提升语言模型在社交推理游戏中的表现，并在实验中击败人类玩家。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在社交推理任务中表现有限，尤其是在小型实时模型中性能下降明显，需要改进。

Method: 采用混合推理框架，将信念推断外化到结构化概率模型，同时利用LLM处理语言理解和交互。

Result: 该方法在Agent-Agent对战中表现优异，首次在控制研究中击败人类玩家，胜率达67%，并获得更高评价。

Conclusion: 混合推理框架有效提升了语言模型的社交推理能力，为未来研究提供了代码、模型和数据集支持。

Abstract: Social reasoning - inferring unobservable beliefs and intentions from partial
observations of other agents - remains a challenging task for large language
models (LLMs). We evaluate the limits of current reasoning language models in
the social deduction game Avalon and find that while the largest models
demonstrate strong performance, they require extensive test-time inference and
degrade sharply when distilled to smaller, real-time-capable variants. To
address this, we introduce a hybrid reasoning framework that externalizes
belief inference to a structured probabilistic model, while using an LLM for
language understanding and interaction. Our approach achieves competitive
performance with much larger models in Agent-Agent play and, notably, is the
first language agent to defeat human players in a controlled study - achieving
a 67% win rate and receiving higher qualitative ratings than both reasoning
baselines and human teammates. We release code, models, and a dataset to
support future work on social reasoning in LLM agents, which can be found at
https://camp-lab-purdue.github.io/bayesian-social-deduction/

</details>


### [15] [Efficient Strategy Synthesis for MDPs via Hierarchical Block Decomposition](https://arxiv.org/abs/2506.17792)
*Alexandros Evangelidis,Gricel Vázquez,Simos Gerasimou*

Main category: cs.AI

TL;DR: 提出了一种动态细化MDP的方法，通过迭代选择最脆弱的区域进行优化，显著提升了大规模MDP的策略合成效率。


<details>
  <summary>Details</summary>
Motivation: 传统策略合成方法难以应对大规模状态空间的问题，需要一种更高效的解决方案。

Method: 动态细化MDP，迭代选择最脆弱的区域进行优化，平衡准确性与效率。

Result: 在包含100万状态的MDP中，性能比PRISM提升高达2倍。

Conclusion: 该方法为大规模MDP的策略合成提供了高效且实用的解决方案。

Abstract: Software-intensive systems, such as software product lines and robotics,
utilise Markov decision processes (MDPs) to capture uncertainty and analyse
sequential decision-making problems. Despite the usefulness of conventional
policy synthesis methods, they fail to scale to large state spaces. Our
approach addresses this issue and accelerates policy synthesis in large MDPs by
dynamically refining the MDP and iteratively selecting the most fragile MDP
regions for refinement. This iterative procedure offers a balance between
accuracy and efficiency, as refinement occurs only when necessary. Through a
comprehensive empirical evaluation comprising diverse case studies and MDPs up
to 1M states, we demonstrate significant performance improvements yielded by
our approach compared to the leading probabilistic model checker PRISM (up to
2x), thus offering a very competitive solution for real-world policy synthesis
tasks in larger MDPs.

</details>


### [16] [Reflective Verbal Reward Design for Pluralistic Alignment](https://arxiv.org/abs/2506.17834)
*Carter Blair,Kate Larson,Edith Law*

Main category: cs.AI

TL;DR: 论文提出了一种个性化奖励建模方法，通过对话引导用户反思和构建偏好，避免了传统RLHF方法中少数群体偏好被压制的问题。


<details>
  <summary>Details</summary>
Motivation: 人类价值观具有多样性，传统RLHF方法通过单一奖励模型聚合反馈可能压制少数群体偏好。

Method: 使用语言模型引导用户通过反思对话构建个性化偏好，并将对话历史作为个性化奖励模型的上下文。

Result: 在30名参与者的研究中，该方法比非反思性语言奖励模型准确率提高9-12%，且样本效率更高。

Conclusion: 个性化奖励建模方法能更准确地反映用户价值观，同时提高样本效率。

Abstract: AI agents are commonly aligned with "human values" through reinforcement
learning from human feedback (RLHF), where a single reward model is learned
from aggregated human feedback and used to align an agent's behavior. However,
human values are not homogeneous--different people hold distinct and sometimes
conflicting values. Aggregating feedback into a single reward model risks
disproportionately suppressing minority preferences. To address this, we
present a novel reward modeling approach for learning individualized reward
models. Our approach uses a language model to guide users through reflective
dialogues where they critique agent behavior and construct their preferences.
This personalized dialogue history, containing the user's reflections and
critiqued examples, is then used as context for another language model that
serves as an individualized reward function (what we call a "verbal reward
model") for evaluating new trajectories. In studies with 30 participants, our
method achieved a 9-12% improvement in accuracy over non-reflective verbal
reward models while being more sample efficient than traditional supervised
learning methods.

</details>


### [17] [Out of Control -- Why Alignment Needs Formal Control Theory (and an Alignment Control Stack)](https://arxiv.org/abs/2506.17846)
*Elija Perrier*

Main category: cs.AI

TL;DR: 该立场论文主张将形式最优控制理论作为AI对齐研究的核心，提出了一种不同于主流AI安全和安全方法的新视角。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全和机制可解释性研究虽在形式上有所进展，但缺乏通用性和协议互操作性。通过形式最优控制理论重新定义对齐，可以更好地理解前沿模型和智能AI系统的控制潜力与限制。

Method: 提出“对齐控制堆栈”（Alignment Control Stack），分层定义对齐堆栈，明确各层的测量和控制特性及其互操作性。

Result: 通过分层分析，为政府和监管机构提供所需的保障，确保AI技术可持续造福社会。

Conclusion: 将最优控制理论与实际部署结合，构建更全面的对齐框架，提升高级AI系统的安全性和可靠性。

Abstract: This position paper argues that formal optimal control theory should be
central to AI alignment research, offering a distinct perspective from
prevailing AI safety and security approaches. While recent work in AI safety
and mechanistic interpretability has advanced formal methods for alignment,
they often fall short of the generalisation required of control frameworks for
other technologies. There is also a lack of research into how to render
different alignment/control protocols interoperable. We argue that by recasting
alignment through principles of formal optimal control and framing alignment in
terms of hierarchical stack from physical to socio-technical layers according
to which controls may be applied we can develop a better understanding of the
potential and limitations for controlling frontier models and agentic AI
systems. To this end, we introduce an Alignment Control Stack which sets out a
hierarchical layered alignment stack, identifying measurement and control
characteristics at each layer and how different layers are formally
interoperable. We argue that such analysis is also key to the assurances that
will be needed by governments and regulators in order to see AI technologies
sustainably benefit the community. Our position is that doing so will bridge
the well-established and empirically validated methods of optimal control with
practical deployment considerations to create a more comprehensive alignment
framework, enhancing how we approach safety and reliability for advanced AI
systems.

</details>


### [18] [Towards Robust Fact-Checking: A Multi-Agent System with Advanced Evidence Retrieval](https://arxiv.org/abs/2506.17878)
*Tam Trinh,Manh Nguyen,Truong-Son Hy*

Main category: cs.AI

TL;DR: 本文提出了一种基于多智能体系统的自动化事实核查方法，显著提升了准确性、效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 数字时代中错误信息的快速传播对公共讨论构成挑战，传统人工核查方法难以应对在线内容的规模和速度，而现有自动化方法在处理复杂声明、确保来源可信度和透明度方面存在局限。

Method: 设计了一个包含四个专门智能体的多智能体系统：输入摄取智能体（分解声明）、查询生成智能体（生成子查询）、证据检索智能体（获取可信证据）和裁决预测智能体（生成可解释的验证结果）。

Result: 在基准数据集（FEVEROUS、HOVER、SciFact）上，系统比基线方法在Macro F1分数上提升了12.3%，能有效分解复杂声明、检索可靠证据并生成透明解释。

Conclusion: 该方法为自动化事实核查领域提供了更准确、高效且透明的验证方法，同时保持了实际应用的扩展性。

Abstract: The rapid spread of misinformation in the digital era poses significant
challenges to public discourse, necessitating robust and scalable fact-checking
solutions. Traditional human-led fact-checking methods, while credible,
struggle with the volume and velocity of online content, prompting the
integration of automated systems powered by Large Language Models (LLMs).
However, existing automated approaches often face limitations, such as handling
complex claims, ensuring source credibility, and maintaining transparency. This
paper proposes a novel multi-agent system for automated fact-checking that
enhances accuracy, efficiency, and explainability. The system comprises four
specialized agents: an Input Ingestion Agent for claim decomposition, a Query
Generation Agent for formulating targeted subqueries, an Evidence Retrieval
Agent for sourcing credible evidence, and a Verdict Prediction Agent for
synthesizing veracity judgments with human-interpretable explanations.
Evaluated on benchmark datasets (FEVEROUS, HOVER, SciFact), the proposed system
achieves a 12.3% improvement in Macro F1-score over baseline methods. The
system effectively decomposes complex claims, retrieves reliable evidence from
trusted sources, and generates transparent explanations for verification
decisions. Our approach contributes to the growing field of automated
fact-checking by providing a more accurate, efficient, and transparent
verification methodology that aligns with human fact-checking practices while
maintaining scalability for real-world applications. Our source code is
available at https://github.com/HySonLab/FactAgent

</details>


### [19] [Leveraging Large Language Model for Intelligent Log Processing and Autonomous Debugging in Cloud AI Platforms](https://arxiv.org/abs/2506.17900)
*Cheng Ji,Huaiying Luo*

Main category: cs.AI

TL;DR: 本文提出了一种基于大语言模型（LLM）的智能日志处理与自动调试框架LLM-ID，通过多阶段语义推理和强化学习策略，显著提升了云平台故障定位的准确性。


<details>
  <summary>Details</summary>
Motivation: 随着云平台AI系统规模和复杂性的增加，日志数据的海量、非结构化和语义模糊性给故障定位和系统自修复带来巨大挑战。

Method: 扩展预训练的Transformer模型，结合多阶段语义推理机制，动态结构化日志，提取事件模板和语义模式，并利用微调的LLM和多轮注意力机制进行上下文推理，生成故障假设和根因路径。引入基于强化学习的策略引导恢复规划器。

Result: 实验表明，LLM-ID在云平台日志数据集上故障定位准确率提高了16.2%，优于现有主流方法。

Conclusion: LLM-ID具有更强的语义理解能力、持续学习能力和异构环境适应性，为云平台故障定位和调试提供了有效解决方案。

Abstract: With the increasing complexity and rapid expansion of the scale of AI systems
in cloud platforms, the log data generated during system operation is massive,
unstructured, and semantically ambiguous, which brings great challenges to
fault location and system self-repair. In order to solve this problem, this
paper proposes an intelligent log processing and automatic debugging framework
based on Large Language Model (LLM), named Intelligent Debugger (LLM-ID). This
method is extended on the basis of the existing pre-trained Transformer model,
and integrates a multi-stage semantic inference mechanism to realize the
context understanding of system logs and the automatic reconstruction of fault
chains. Firstly, the system log is dynamically structured, and the unsupervised
clustering and embedding mechanism is used to extract the event template and
semantic schema. Subsequently, the fine-tuned LLM combined with the multi-round
attention mechanism to perform contextual reasoning on the log sequence to
generate potential fault assumptions and root cause paths. Furthermore, this
paper introduces a reinforcement learning-based policy-guided recovery planner,
which is driven by the remediation strategy generated by LLM to support dynamic
decision-making and adaptive debugging in the cloud environment. Compared with
the existing rule engine or traditional log analysis system, the proposed model
has stronger semantic understanding ability, continuous learning ability and
heterogeneous environment adaptability. Experiments on the cloud platform log
dataset show that LLM-ID improves the fault location accuracy by 16.2%, which
is significantly better than the current mainstream methods

</details>


### [20] [Learning, Reasoning, Refinement: A Framework for Kahneman's Dual-System Intelligence in GUI Agents](https://arxiv.org/abs/2506.17913)
*Jinjie Wei,Jiyao Liu,Lihao Liu,Ming Hu,Junzhi Ning,Mingcheng Li,Weijie Yin,Junjun He,Xiao Liang,Chao Feng,Dingkang Yang*

Main category: cs.AI

TL;DR: CogniGUI是一个基于认知框架的GUI代理，通过结合快速视觉语义分析和相对奖励系统，实现了自适应学习和高效操作路径选择，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理系统依赖试错决策，缺乏渐进式推理和学习能力，且评估指标过于简单，无法反映真实GUI交互的复杂性。

Method: 结合双系统设计：(1) omni parser引擎进行快速视觉语义分析，(2) GRPO代理评估多交互路径，采用相对奖励系统。

Result: CogniGUI在现有和新提出的基准测试中均优于最先进方法。

Conclusion: CogniGUI通过迭代学习和适应性策略，显著提升了GUI代理的性能和泛化能力。

Abstract: Graphical User Interface (GUI) agents have made significant progress in
automating digital tasks through the utilization of computer vision and
language models. Nevertheless, existing agent systems encounter notable
limitations. Firstly, they predominantly depend on trial and error decision
making rather than progressive reasoning, thereby lacking the capability to
learn and adapt from interactive encounters. Secondly, these systems are
assessed using overly simplistic single step accuracy metrics, which do not
adequately reflect the intricate nature of real world GUI interactions. In this
paper, we present CogniGUI, a cognitive framework developed to overcome these
limitations by enabling adaptive learning for GUI automation resembling
human-like behavior. Inspired by Kahneman's Dual Process Theory, our approach
combines two main components: (1) an omni parser engine that conducts immediate
hierarchical parsing of GUI elements through quick visual semantic analysis to
identify actionable components, and (2) a Group based Relative Policy
Optimization (GRPO) grounding agent that assesses multiple interaction paths
using a unique relative reward system, promoting minimal and efficient
operational routes. This dual-system design facilitates iterative ''exploration
learning mastery'' cycles, enabling the agent to enhance its strategies over
time based on accumulated experience. Moreover, to assess the generalization
and adaptability of agent systems, we introduce ScreenSeek, a comprehensive
benchmark that includes multi application navigation, dynamic state
transitions, and cross interface coherence, which are often overlooked
challenges in current benchmarks. Experimental results demonstrate that
CogniGUI surpasses state-of-the-art methods in both the current GUI grounding
benchmarks and our newly proposed benchmark.

</details>


### [21] [Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective](https://arxiv.org/abs/2506.17930)
*Jianyu Wang,Zhiqiang Hu,Lidong Bing*

Main category: cs.AI

TL;DR: 提出了一种新的提示设计范式，通过修剪随机演示为看似无意义的“胡言乱语”来提升大语言模型（LLM）的性能，并提出了自动优化框架PromptQuine。


<details>
  <summary>Details</summary>
Motivation: 挑战传统提示设计，探索更高效的LLM提示方法。

Method: 提出PromptQuine框架，通过进化搜索自动寻找修剪策略，仅需少量数据。

Result: 在分类、多选问答、生成和数学推理任务中表现优异，且运行效率高。

Conclusion: 为LLM提示设计提供了新思路，并呼吁开发更开放的搜索算法。

Abstract: We propose a novel prompt design paradigm that challenges conventional wisdom
in large language model (LLM) prompting. While conventional wisdom prioritizes
well-crafted instructions and demonstrations for in-context learning (ICL), we
show that pruning random demonstrations into seemingly incoherent "gibberish"
can remarkably improve performance across diverse tasks. Notably, the
"gibberish" always matches or surpasses state-of-the-art automatic prompt
optimization techniques, achieving substantial gains regardless of LLM
alignment. Nevertheless, discovering an effective pruning strategy is
non-trivial, as existing attribution methods and prompt compression algorithms
fail to deliver robust results, let alone human intuition. In terms of this, we
propose a self-discover prompt optimization framework, PromptQuine, an
evolutionary search framework that automatically searches for the pruning
strategy by itself using only low-data regimes. Much like the emergent
complexity in nature--such as symbiosis and self-organization--arising in
response to resource constraints, our framework evolves and refines
unconventional yet highly effective prompts by leveraging only the tokens
present within the context. We demonstrate its effectiveness across
classification, multi-choice question answering, generation and math reasoning
tasks across LLMs, while achieving decent runtime efficiency. We hope our
findings can guide mechanistic studies on in-context learning, and provide a
call to action, to pave the way for more open-ended search algorithms for more
effective LLM prompting.

</details>


### [22] [medicX-KG: A Knowledge Graph for Pharmacists' Drug Information Needs](https://arxiv.org/abs/2506.17959)
*Lizzy Farrugia,Lilian M. Azzopardi,Jeremy Debattista,Charlie Abela*

Main category: cs.AI

TL;DR: medicX-KG是一个面向药剂师的知识图谱，整合多源数据支持临床和监管决策，填补了统一国家药物库的空白。


<details>
  <summary>Details</summary>
Motivation: 药剂师角色从配药转向综合服务，需要准确、实时的药物信息支持。现有数据分散，缺乏统一整合。

Method: 利用AI和语义技术，整合BNF、DrugBank和MMA数据，设计知识图谱并评估其查询支持能力。

Result: medicX-KG有效支持药物可用性、相互作用、不良反应等查询，但存在剂量编码和实时更新不足。

Conclusion: medicX-KG为药剂师提供了实用工具，未来需改进数据细节和实时性。

Abstract: The role of pharmacists is evolving from medicine dispensing to delivering
comprehensive pharmaceutical services within multidisciplinary healthcare
teams. Central to this shift is access to accurate, up-to-date medicinal
product information supported by robust data integration. Leveraging artificial
intelligence and semantic technologies, Knowledge Graphs (KGs) uncover hidden
relationships and enable data-driven decision-making. This paper presents
medicX-KG, a pharmacist-oriented knowledge graph supporting clinical and
regulatory decisions. It forms the semantic layer of the broader medicX
platform, powering predictive and explainable pharmacy services. medicX-KG
integrates data from three sources, including, the British National Formulary
(BNF), DrugBank, and the Malta Medicines Authority (MMA) that addresses Malta's
regulatory landscape and combines European Medicines Agency alignment with
partial UK supply dependence. The KG tackles the absence of a unified national
drug repository, reducing pharmacists' reliance on fragmented sources. Its
design was informed by interviews with practicing pharmacists to ensure
real-world applicability. We detail the KG's construction, including data
extraction, ontology design, and semantic mapping. Evaluation demonstrates that
medicX-KG effectively supports queries about drug availability, interactions,
adverse reactions, and therapeutic classes. Limitations, including missing
detailed dosage encoding and real-time updates, are discussed alongside
directions for future enhancements.

</details>


### [23] [Graphs Meet AI Agents: Taxonomy, Progress, and Future Opportunities](https://arxiv.org/abs/2506.18019)
*Yuanchen Bei,Weizhi Zhang,Siwen Wang,Weizhi Chen,Sheng Zhou,Hao Chen,Yong Li,Jiajun Bu,Shirui Pan,Yizhou Yu,Irwin King,Fakhri Karray,Philip S. Yu*

Main category: cs.AI

TL;DR: 论文探讨了图技术在AI智能体中的应用，系统回顾了如何通过图结构化数据提升智能体的规划、记忆和协作能力。


<details>
  <summary>Details</summary>
Motivation: 随着AI智能体从强化学习（RL）向大语言模型（LLMs）融合的演进，复杂任务对智能体的能力提出了更高要求，而图技术因其结构化数据的优势成为关键支持。

Method: 通过系统综述，探索图技术与智能体核心功能的结合，展示应用案例并指出未来研究方向。

Result: 图技术能够有效提升智能体对复杂数据的理解与处理能力，支持其完成更复杂的任务。

Conclusion: 图技术为下一代AI智能体的发展提供了重要支持，未来研究应进一步探索其潜力。

Abstract: AI agents have experienced a paradigm shift, from early dominance by
reinforcement learning (RL) to the rise of agents powered by large language
models (LLMs), and now further advancing towards a synergistic fusion of RL and
LLM capabilities. This progression has endowed AI agents with increasingly
strong abilities. Despite these advances, to accomplish complex real-world
tasks, agents are required to plan and execute effectively, maintain reliable
memory, and coordinate smoothly with other agents. Achieving these capabilities
involves contending with ever-present intricate information, operations, and
interactions. In light of this challenge, data structurization can play a
promising role by transforming intricate and disorganized data into
well-structured forms that agents can more effectively understand and process.
In this context, graphs, with their natural advantage in organizing, managing,
and harnessing intricate data relationships, present a powerful data paradigm
for structurization to support the capabilities demanded by advanced AI agents.
To this end, this survey presents a first systematic review of how graphs can
empower AI agents. Specifically, we explore the integration of graph techniques
with core agent functionalities, highlight notable applications, and identify
prospective avenues for future research. By comprehensively surveying this
burgeoning intersection, we hope to inspire the development of next-generation
AI agents equipped to tackle increasingly sophisticated challenges with graphs.
Related resources are collected and continuously updated for the community in
the Github link.

</details>


### [24] [Action Language BC+](https://arxiv.org/abs/2506.18044)
*Joseph Babb,Joohyung Lee*

Main category: cs.AI

TL;DR: BC+是一种新的动作语言，填补了传统动作语言与现代ASP语言之间的空白，支持现代ASP的多种构造。


<details>
  <summary>Details</summary>
Motivation: 传统动作语言与现代ASP语言之间存在表达能力的差距，限制了动作语言的实用性。

Method: 通过基于命题公式的一般稳定模型语义定义BC+的语义，将现代ASP构造视为命题公式的简写。

Result: BC+能够涵盖其他动作语言（如B、C、C+和BC）的最佳特性，并可直接利用ASP求解器进行计算。

Conclusion: BC+成功填补了动作语言与现代ASP之间的鸿沟，并通过扩展cplus2asp系统实现了该语言。

Abstract: Action languages are formal models of parts of natural language that are
designed to describe effects of actions. Many of these languages can be viewed
as high level notations of answer set programs structured to represent
transition systems. However, the form of answer set programs considered in the
earlier work is quite limited in comparison with the modern Answer Set
Programming (ASP) language, which allows several useful constructs for
knowledge representation, such as choice rules, aggregates, and abstract
constraint atoms. We propose a new action language called BC+, which closes the
gap between action languages and the modern ASP language. The main idea is to
define the semantics of BC+ in terms of general stable model semantics for
propositional formulas, under which many modern ASP language constructs can be
identified with shorthands for propositional formulas. Language BC+ turns out
to be sufficiently expressive to encompass the best features of other action
languages, such as languages B, C, C+, and BC. Computational methods available
in ASP solvers are readily applicable to compute BC+, which led to an
implementation of the language by extending system cplus2asp.

</details>


### [25] [Weighted Assumption Based Argumentation to reason about ethical principles and actions](https://arxiv.org/abs/2506.18056)
*Paolo Baldi,Fabio Aurelio D'Asaro,Abeer Dyoub,Francesca Alessandra Lisi*

Main category: cs.AI

TL;DR: 论文通过为基于假设的论证（ABA）引入加权论证，展示了在伦理推理中的应用，并基于答案集编程实现了该方法。


<details>
  <summary>Details</summary>
Motivation: 增强ABA框架的表达能力，使其能够处理带有权重的论证，从而更灵活地应用于伦理推理等领域。

Method: 为ABA中的论证分配权重，并基于权重推导论证间的攻击关系，通过答案集编程实现。

Result: 提出了加权ABA框架，并通过伦理推理的示例验证了其有效性。

Conclusion: 加权ABA能够更灵活地处理复杂推理问题，为伦理推理等领域提供了新的工具。

Abstract: We augment Assumption Based Argumentation (ABA for short) with weighted
argumentation. In a nutshell, we assign weights to arguments and then derive
the weight of attacks between ABA arguments. We illustrate our proposal through
running examples in the field of ethical reasoning, and present an
implementation based on Answer Set Programming.

</details>


### [26] [Deep Research Agents: A Systematic Examination And Roadmap](https://arxiv.org/abs/2506.18096)
*Yuxuan Huang,Yihang Chen,Haozheng Zhang,Kang Li,Meng Fang,Linyi Yang,Xiaoguang Li,Lifeng Shang,Songcen Xu,Jianye Hao,Kun Shao,Jun Wang*

Main category: cs.AI

TL;DR: 论文分析了构成深度研究（DR）代理的基础技术和架构组件，提出了分类法并评估了现有基准的局限性，同时指出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）的快速发展，深度研究代理成为解决复杂多轮信息研究任务的新兴AI系统，需要系统化研究其技术和架构。

Method: 论文通过对比信息获取策略、模块化工具使用框架，提出分类法区分静态与动态工作流，并基于规划策略和代理组成对架构进行分类。

Result: 论文总结了现有基准的关键限制，如外部知识访问受限、顺序执行效率低下，并提出了未来研究的开放挑战和方向。

Conclusion: 深度研究代理领域仍面临多项挑战，但通过系统化分类和基准改进，未来研究有望推动其进一步发展。

Abstract: The rapid progress of Large Language Models (LLMs) has given rise to a new
category of autonomous AI systems, referred to as Deep Research (DR) agents.
These agents are designed to tackle complex, multi-turn informational research
tasks by leveraging a combination of dynamic reasoning, adaptive long-horizon
planning, multi-hop information retrieval, iterative tool use, and the
generation of structured analytical reports. In this paper, we conduct a
detailed analysis of the foundational technologies and architectural components
that constitute Deep Research agents. We begin by reviewing information
acquisition strategies, contrasting API-based retrieval methods with
browser-based exploration. We then examine modular tool-use frameworks,
including code execution, multimodal input processing, and the integration of
Model Context Protocols (MCPs) to support extensibility and ecosystem
development. To systematize existing approaches, we propose a taxonomy that
differentiates between static and dynamic workflows, and we classify agent
architectures based on planning strategies and agent composition, including
single-agent and multi-agent configurations. We also provide a critical
evaluation of current benchmarks, highlighting key limitations such as
restricted access to external knowledge, sequential execution inefficiencies,
and misalignment between evaluation metrics and the practical objectives of DR
agents. Finally, we outline open challenges and promising directions for future
research. A curated and continuously updated repository of DR agent research is
available at: {https://github.com/ai-agents-2030/awesome-deep-research-agent}.

</details>


### [27] [Decentralized Consensus Inference-based Hierarchical Reinforcement Learning for Multi-Constrained UAV Pursuit-Evasion Game](https://arxiv.org/abs/2506.18126)
*Xiang Yuming,Li Sizhao,Li Rongpeng,Zhao Zhifeng,Zhang Honggang*

Main category: cs.AI

TL;DR: 提出了一种名为CI-HRL的两层框架，用于解决多约束追逃游戏中的协同规避与编队覆盖任务，通过高层策略实现目标定位，低层策略管理避障、导航和编队。


<details>
  <summary>Details</summary>
Motivation: 多约束追逃游戏中的协同规避与编队覆盖任务在通信受限条件下极具挑战性，需要解决高维复杂问题。

Method: 采用共识推理分层强化学习（CI-HRL），高层策略使用ConsMAC模块实现全局感知与共识建立，低层策略通过AT-M和策略蒸馏实现控制。

Result: 实验验证CI-HRL在协同规避和任务完成能力上表现优越。

Conclusion: CI-HRL为复杂多约束任务提供了一种高效解决方案。

Abstract: Multiple quadrotor unmanned aerial vehicle (UAV) systems have garnered
widespread research interest and fostered tremendous interesting applications,
especially in multi-constrained pursuit-evasion games (MC-PEG). The Cooperative
Evasion and Formation Coverage (CEFC) task, where the UAV swarm aims to
maximize formation coverage across multiple target zones while collaboratively
evading predators, belongs to one of the most challenging issues in MC-PEG,
especially under communication-limited constraints. This multifaceted problem,
which intertwines responses to obstacles, adversaries, target zones, and
formation dynamics, brings up significant high-dimensional complications in
locating a solution. In this paper, we propose a novel two-level framework
(i.e., Consensus Inference-based Hierarchical Reinforcement Learning (CI-HRL)),
which delegates target localization to a high-level policy, while adopting a
low-level policy to manage obstacle avoidance, navigation, and formation.
Specifically, in the high-level policy, we develop a novel multi-agent
reinforcement learning module, Consensus-oriented Multi-Agent Communication
(ConsMAC), to enable agents to perceive global information and establish
consensus from local states by effectively aggregating neighbor messages.
Meanwhile, we leverage an Alternative Training-based Multi-agent proximal
policy optimization (AT-M) and policy distillation to accomplish the low-level
control. The experimental results, including the high-fidelity
software-in-the-loop (SITL) simulations, validate that CI-HRL provides a
superior solution with enhanced swarm's collaborative evasion and task
completion capabilities.

</details>


### [28] [SE-Merging: A Self-Enhanced Approach for Dynamic Model Merging](https://arxiv.org/abs/2506.18135)
*Zijun Chen,Zhanpeng Zhou,Bo Zhang,Weinan Zhang,Xi Sun,Junchi Yan*

Main category: cs.AI

TL;DR: 论文探讨了模型合并的机制，提出了一种自增强框架SE-Merging，通过动态识别任务和调整合并系数提升多任务能力。


<details>
  <summary>Details</summary>
Motivation: 模型合并虽在多任务中表现良好，但其机制尚不明确，研究旨在揭示其背后的表示机制。

Method: 从表示角度分析模型合并，提出SE-Merging框架，动态识别任务并调整合并系数。

Result: SE-Merging显著提升性能，且无需额外训练，兼容现有技术。

Conclusion: 模型合并通过区分任务和适应专家模型实现多任务能力，SE-Merging进一步优化了这一过程。

Abstract: Model merging has gained increasing attention due to its intriguing property:
interpolating the parameters of different task-specific fine-tuned models leads
to multi-task abilities. However, despite its empirical success, the underlying
mechanisms of model merging remain poorly understood. In this work, we delve
into the mechanism behind model merging from a representation perspective. Our
analysis reveals that model merging achieves multi-task abilities through two
key capabilities: i) distinguishing samples from different tasks, and ii)
adapting to the corresponding expert model for each sample. These two
capabilities allow the merged model to retain task-specific expertise, enabling
efficient multi-task adaptation. Building on these insights, we propose
\texttt{SE-Merging}, a self-enhanced model merging framework that leverages
these two characteristics to dynamically identify the corresponding task for
each sample and then adaptively rescales the merging coefficients to further
enhance task-specific expertise in the merged model. Notably,
\texttt{SE-Merging} achieves dynamic model merging without additional training.
Extensive experiments demonstrate that \texttt{SE-Merging} achieves significant
performance improvements while remaining compatible with existing model merging
techniques.

</details>


### [29] [CoachGPT: A Scaffolding-based Academic Writing Assistant](https://arxiv.org/abs/2506.18149)
*Fumian Chen,Sotheara Veng,Joshua Wilson,Xiaoming Li,Hui Fang*

Main category: cs.AI

TL;DR: CoachGPT利用大型语言模型（LLMs）为学术写作提供个性化反馈和指导，弥补了传统写作助手的不足。


<details>
  <summary>Details</summary>
Motivation: 学术写作对学生的成功至关重要，但缺乏指导和支持时可能令人望而生畏。现有写作助手（如规则系统或机器学习模型）存在准确性不足或成本高的问题，而LLMs虽能生成文本但缺乏教育功能。

Method: 开发了CoachGPT，一个基于LLMs的AI代理网络应用，通过教育者的指令生成子任务并提供实时反馈。

Result: 用户研究表明，CoachGPT提供了沉浸式的写作体验，并展示了LLMs在学术写作中的潜力。

Conclusion: CoachGPT通过结合教育者指导和LLMs的能力，为学术写作提供了有效的支持工具。

Abstract: Academic writing skills are crucial for students' success, but can feel
overwhelming without proper guidance and practice, particularly when writing in
a second language. Traditionally, students ask instructors or search
dictionaries, which are not universally accessible. Early writing assistants
emerged as rule-based systems that focused on detecting misspellings,
subject-verb disagreements, and basic punctuation errors; however, they are
inaccurate and lack contextual understanding. Machine learning-based assistants
demonstrate a strong ability for language understanding but are expensive to
train. Large language models (LLMs) have shown remarkable capabilities in
generating responses in natural languages based on given prompts. Still, they
have a fundamental limitation in education: they generate essays without
teaching, which can have detrimental effects on learning when misused. To
address this limitation, we develop CoachGPT, which leverages large language
models (LLMs) to assist individuals with limited educational resources and
those who prefer self-paced learning in academic writing. CoachGPT is an AI
agent-based web application that (1) takes instructions from experienced
educators, (2) converts instructions into sub-tasks, and (3) provides real-time
feedback and suggestions using large language models. This unique scaffolding
structure makes CoachGPT unique among existing writing assistants. Compared to
existing writing assistants, CoachGPT provides a more immersive writing
experience with personalized feedback and guidance. Our user studies prove the
usefulness of CoachGPT and the potential of large language models for academic
writing.

</details>


### [30] [AI Through the Human Lens: Investigating Cognitive Theories in Machine Psychology](https://arxiv.org/abs/2506.18156)
*Akash Kundu,Rishika Goswami*

Main category: cs.AI

TL;DR: 研究探讨了大型语言模型（LLMs）是否在四种心理学框架下表现出类似人类的认知模式，发现其行为与人类认知倾向相似但受训练数据影响。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs是否具有人类认知模式，以评估其透明度和伦理部署。

Method: 使用结构化提示和自动评分评估多个专有和开源模型。

Result: 模型能生成连贯叙述，易受积极框架影响，道德判断偏向自由/压迫问题，且通过合理化缓解自我矛盾。

Conclusion: 研究为AI透明度和伦理部署提供启示，并建议未来结合认知心理学与AI安全。

Abstract: We investigate whether Large Language Models (LLMs) exhibit human-like
cognitive patterns under four established frameworks from psychology: Thematic
Apperception Test (TAT), Framing Bias, Moral Foundations Theory (MFT), and
Cognitive Dissonance. We evaluated several proprietary and open-source models
using structured prompts and automated scoring. Our findings reveal that these
models often produce coherent narratives, show susceptibility to positive
framing, exhibit moral judgments aligned with Liberty/Oppression concerns, and
demonstrate self-contradictions tempered by extensive rationalization. Such
behaviors mirror human cognitive tendencies yet are shaped by their training
data and alignment methods. We discuss the implications for AI transparency,
ethical deployment, and future work that bridges cognitive psychology and AI
safety

</details>


### [31] [Chain-of-Memory: Enhancing GUI Agents for Cross-Application Navigation](https://arxiv.org/abs/2506.18158)
*Xinzge Gao,Chuanrui Hu,Bin Chen,Teng Li*

Main category: cs.AI

TL;DR: 论文提出了一种名为Chain-of-Memory (CoM)的新方法，用于显式建模GUI代理的短期和长期记忆，以解决现有方法在复杂跨应用任务中无法有效存储关键信息的问题。


<details>
  <summary>Details</summary>
Motivation: 现有GUI代理方法依赖历史截图或动作隐式表示任务状态，导致任务状态理解不准确，且缺乏有效机制存储复杂跨应用任务中的关键信息。

Method: CoM通过捕获动作描述、整合任务相关屏幕信息，并维护专用内存模块来显式建模记忆。

Result: 实验表明，CoM显著提升了GUI代理在跨应用任务中的性能，且7B模型通过GUI Odyssey-CoM数据集可实现与72B模型相当的内存管理能力。

Conclusion: CoM为GUI代理提供了显式记忆表示，解决了任务状态理解和关键信息存储问题，数据集和代码将开源。

Abstract: Multimodal large language models (MLLMs) are attracting growing attention in
the development of Graphical User Interface (GUI) agents. Existing approaches
often rely on historical screenshots or actions to implicitly represent the
task state. This reliance poses challenges for GUI agents in accurately
understanding task states and underscores the absence of effective mechanisms
to store critical information in complex and lengthy cross-app tasks. To
address these challenges, we propose Chain-of-Memory (CoM), a novel approach
for explicitly modeling short-term and long-term memory in GUI agents. CoM
achieves this by capturing action descriptions, integrating task-relevant
screen information, and maintaining a dedicated memory module to store and
manage this information. By leveraging explicit memory representations, CoM
enables GUI agents to better understand task states and retain critical
historical information persistently. To equip GUI agents with memory management
capabilities and evaluate the effectiveness of CoM, we developed the GUI
Odyssey-CoM, a dataset comprising 111k screen-action pairs annotated with
Chain-of-Memory. Experimental results demonstrate that CoM significantly
improves GUI agents' performance in cross-application tasks. Additionally, GUI
Odyssey-CoM enables 7B models to achieve memory management capabilities
comparable to 72B models. The dataset and code will be open-sourced.

</details>


### [32] [Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?](https://arxiv.org/abs/2506.18183)
*Zhiting Mei,Christina Zhang,Tenny Yin,Justin Lidard,Ola Shorinwa,Anirudha Majumdar*

Main category: cs.AI

TL;DR: 论文探讨了推理语言模型的不确定性量化问题，发现模型通常过度自信，并提出通过自省改进校准的方法。


<details>
  <summary>Details</summary>
Motivation: 推理模型在生成答案时可能产生错误但自信的响应（幻觉），了解何时信任这些模型对安全部署至关重要。

Method: 研究通过自省不确定性量化（UQ）评估推理模型的校准情况，并测试更深层次的推理和自省对校准的影响。

Result: 推理模型通常过度自信，尤其是错误答案；更深推理可能加剧过度自信；自省可改进部分模型的校准。

Conclusion: 需进一步研究设计UQ基准和改进推理模型的校准方法。

Abstract: Reasoning language models have set state-of-the-art (SOTA) records on many
challenging benchmarks, enabled by multi-step reasoning induced using
reinforcement learning. However, like previous language models, reasoning
models are prone to generating confident, plausible responses that are
incorrect (hallucinations). Knowing when and how much to trust these models is
critical to the safe deployment of reasoning models in real-world applications.
To this end, we explore uncertainty quantification of reasoning models in this
work. Specifically, we ask three fundamental questions: First, are reasoning
models well-calibrated? Second, does deeper reasoning improve model
calibration? Finally, inspired by humans' innate ability to double-check their
thought processes to verify the validity of their answers and their confidence,
we ask: can reasoning models improve their calibration by explicitly reasoning
about their chain-of-thought traces? We introduce introspective uncertainty
quantification (UQ) to explore this direction. In extensive evaluations on SOTA
reasoning models across a broad range of benchmarks, we find that reasoning
models: (i) are typically overconfident, with self-verbalized confidence
estimates often greater than 85% particularly for incorrect responses, (ii)
become even more overconfident with deeper reasoning, and (iii) can become
better calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not
uniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we
conclude with important research directions to design necessary UQ benchmarks
and improve the calibration of reasoning models.

</details>


### [33] [The Impact of Medication Non-adherence on Adverse Outcomes: Evidence from Schizophrenia Patients via Survival Analysis](https://arxiv.org/abs/2506.18187)
*Shahriar Noroozizadeh,Pim Welle,Jeremy C. Weiss,George H. Chen*

Main category: cs.AI

TL;DR: 研究量化了精神分裂症患者不坚持服用抗精神病药物与不良后果的关联，使用生存分析和因果推断方法，发现不坚持服药会提前1至4个月引发不良事件。


<details>
  <summary>Details</summary>
Motivation: 探讨药物依从性对精神分裂症患者不良后果的影响，为临床和政策提供依据。

Method: 采用生存分析框架，结合T-learner、S-learner和最近邻匹配等因果推断方法，利用不同时间段的纵向数据（3、6、9、12个月）进行分析。

Result: 不坚持服药会显著提前不良事件发生时间约1至4个月，且不同药物类型和剂型的结果一致。

Conclusion: 药物依从性对延缓精神疾病危机至关重要，生存分析与因果推断结合可为政策制定提供有用信息，但需注意因果解释的假设限制。

Abstract: This study quantifies the association between non-adherence to antipsychotic
medications and adverse outcomes in individuals with schizophrenia. We frame
the problem using survival analysis, focusing on the time to the earliest of
several adverse events (early death, involuntary hospitalization, jail
booking). We extend standard causal inference methods (T-learner, S-learner,
nearest neighbor matching) to utilize various survival models to estimate
individual and average treatment effects, where treatment corresponds to
medication non-adherence. Analyses are repeated using different amounts of
longitudinal information (3, 6, 9, and 12 months). Using data from Allegheny
County in western Pennsylvania, we find strong evidence that non-adherence
advances adverse outcomes by approximately 1 to 4 months. Ablation studies
confirm that county-provided risk scores adjust for key confounders, as their
removal amplifies the estimated effects. Subgroup analyses by medication
formulation (injectable vs. oral) and medication type consistently show that
non-adherence is associated with earlier adverse events. These findings
highlight the clinical importance of adherence in delaying psychiatric crises
and show that integrating survival analysis with causal inference tools can
yield policy-relevant insights. We caution that although we apply causal
inference, we only make associative claims and discuss assumptions needed for
causal interpretation.

</details>


### [34] [A Conceptual Framework for AI Capability Evaluations](https://arxiv.org/abs/2506.18213)
*María Victoria Carro,Denise Alejandra Mester,Francisca Gauna Selasco,Luca Nicolás Forziati Gangi,Matheo Sandleris Musa,Lola Ramos Pereyra,Mario Leiva,Juan Gustavo Corvalan,María Vanina Martinez,Gerardo Simari*

Main category: cs.AI

TL;DR: 提出一个概念框架，用于分析和系统化AI能力评估方法，支持透明性、可比性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统融入社会，透明且全面的评估工具成为AI治理的关键，但目前缺乏明确的方法。

Method: 提出一个结构化、描述性的框架，分析现有评估方法和术语，不引入新分类或固定格式。

Result: 框架支持评估的透明性、可比性和可解释性，帮助识别方法缺陷、设计评估和政策制定。

Conclusion: 该框架为研究人员、实践者和政策制定者提供了实用工具，以应对复杂的AI评估挑战。

Abstract: As AI systems advance and integrate into society, well-designed and
transparent evaluations are becoming essential tools in AI governance,
informing decisions by providing evidence about system capabilities and risks.
Yet there remains a lack of clarity on how to perform these assessments both
comprehensively and reliably. To address this gap, we propose a conceptual
framework for analyzing AI capability evaluations, offering a structured,
descriptive approach that systematizes the analysis of widely used methods and
terminology without imposing new taxonomies or rigid formats. This framework
supports transparency, comparability, and interpretability across diverse
evaluations. It also enables researchers to identify methodological weaknesses,
assists practitioners in designing evaluations, and provides policymakers with
an accessible tool to scrutinize, compare, and navigate complex evaluation
landscapes.

</details>


### [35] [The 4th Dimension for Scaling Model Size](https://arxiv.org/abs/2506.18233)
*Ruike Zhu,Hanwen Zhang,Tianyu Shi,Chi Wang,Tianyi Zhou,Zengyi Qin*

Main category: cs.AI

TL;DR: 本文提出了一种新的模型扩展维度——虚拟逻辑深度（VLD），通过参数复用增加算法深度而不改变总参数量，研究发现VLD扩展能显著提升推理能力，但对知识容量影响较小。


<details>
  <summary>Details</summary>
Motivation: 探索模型扩展的第四维度（VLD），研究参数复用对模型性能的影响，尤其是推理能力和知识容量的关系。

Method: 通过精心设计的对照实验，研究VLD扩展对模型知识容量和推理能力的影响。

Result: VLD扩展能显著提升推理能力，但对知识容量影响较小；参数量与知识容量相关，但与推理能力无关。

Conclusion: 在特定条件下，无需增加参数量即可提升推理能力，这一发现在多种模型配置中均成立。

Abstract: Scaling the size of large language models typically involves three
dimensions: depth, width, and the number of parameters. In this work, we
explore a fourth dimension, virtual logical depth (VLD), which increases the
effective algorithmic depth without changing the overall parameter count by
reusing parameters within the model. Although parameter reuse is not a new
concept, its potential and characteristics in model scaling have not been
thoroughly studied. Through carefully designed controlled experiments, we make
the following key discoveries regarding VLD scaling:
  VLD scaling forces the knowledge capacity of the model to remain almost
constant, with only minor variations.
  VLD scaling enables a significant improvement in reasoning capability,
provided the scaling method is properly implemented.
  The number of parameters correlates with knowledge capacity, but not with
reasoning capability. Under certain conditions, it is not necessary to increase
the parameter count to enhance reasoning.
  These findings are consistent across various model configurations and are
likely to be generally valid within the scope of our experiments.

</details>


### [36] [Advanced For-Loop for QML algorithm search](https://arxiv.org/abs/2506.18260)
*FuTe Wong*

Main category: cs.AI

TL;DR: 本文提出了一种基于大型语言模型的多智能体系统（LLMMA）框架，用于自动搜索和优化量子机器学习（QML）算法。


<details>
  <summary>Details</summary>
Motivation: 受Google DeepMind的FunSearch启发，旨在系统性地探索经典机器学习概念并将其适配到量子计算中。

Method: 利用LLMMA在抽象层面上迭代生成和优化经典机器学习算法的量子转换。

Result: 展示了智能体框架在自动化开发QML算法中的潜力。

Conclusion: 未来方向包括引入规划机制和优化搜索策略，以扩展量子增强机器学习的应用。

Abstract: This paper introduces an advanced framework leveraging Large Language
Model-based Multi-Agent Systems (LLMMA) for the automated search and
optimization of Quantum Machine Learning (QML) algorithms. Inspired by Google
DeepMind's FunSearch, the proposed system works on abstract level to
iteratively generates and refines quantum transformations of classical machine
learning algorithms (concepts), such as the Multi-Layer Perceptron,
forward-forward and backpropagation algorithms. As a proof of concept, this
work highlights the potential of agentic frameworks to systematically explore
classical machine learning concepts and adapt them for quantum computing,
paving the way for efficient and automated development of QML algorithms.
Future directions include incorporating planning mechanisms and optimizing
strategy in the search space for broader applications in quantum-enhanced
machine learning.

</details>


### [37] [Dynamic Knowledge Exchange and Dual-diversity Review: Concisely Unleashing the Potential of a Multi-Agent Research Team](https://arxiv.org/abs/2506.18348)
*Weilun Yu,Shixiang Tang,Yonggui Huang,Nanqing Dong,Li Fan,Honggang Qi,Wei Liu,Xiaoli Diao,Xi Chen,Wanli Ouyang*

Main category: cs.AI

TL;DR: IDVSCI是一个基于大语言模型的多智能体框架，通过动态知识交换和双多样性评审机制提升科学研究的互动推理和创意生成能力，在多个数据集上表现优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型科学家代理缺乏真实研究中所需的互动推理和评估机制，限制了其在自主科学发现中的潜力。

Method: 提出IDVSCI框架，包含动态知识交换机制和双多样性评审范式，促进智能体间的迭代反馈和异质专家评估。

Result: 在计算机科学和健康科学领域的实验中，IDVSCI表现优于AI Scientist和VIRSCI等现有系统。

Conclusion: 模拟互动和同行评审动态对基于大语言模型的自主研究具有重要价值。

Abstract: Scientific progress increasingly relies on effective collaboration among
researchers, a dynamic that large language models (LLMs) have only begun to
emulate. While recent LLM-based scientist agents show promise in autonomous
scientific discovery, they often lack the interactive reasoning and evaluation
mechanisms essential to real-world research. We propose IDVSCI (Internal
Discussion and Vote SCIentists), a multi-agent framework built on LLMs that
incorporates two key innovations: a Dynamic Knowledge Exchange mechanism
enabling iterative feedback among agents, and a Dual-Diversity Review paradigm
that simulates heterogeneous expert evaluation. These components jointly
promote deeper reasoning and the generation of more creative and impactful
scientific ideas. To evaluate the effectiveness and generalizability of our
approach, we conduct experiments on two datasets: a widely used benchmark in
computer science and a new dataset we introduce in the health sciences domain.
Results show that IDVSCI consistently achieves the best performance across both
datasets, outperforming existing systems such as AI Scientist and VIRSCI. These
findings highlight the value of modeling interaction and peer review dynamics
in LLM-based autonomous research.

</details>


### [38] [A Large Language Model-based Multi-Agent Framework for Analog Circuits' Sizing Relationships Extraction](https://arxiv.org/abs/2506.18424)
*Chengjie Liu,Weiyu Chen,Huiyao Xu,Yuan Du,Jun Yang,Li Du*

Main category: cs.AI

TL;DR: 提出了一种基于大语言模型（LLM）的多智能体框架，用于从学术论文中提取模拟电路的尺寸关系，以优化电路设计中的搜索空间。


<details>
  <summary>Details</summary>
Motivation: 现有技术在模拟电路尺寸设计中忽视了先验知识的自动引入，导致搜索空间压缩不足。

Method: 采用LLM多智能体框架提取尺寸关系，有效修剪搜索空间。

Result: 在3种电路上测试，优化效率提高了2.32至26.6倍。

Conclusion: LLM能有效修剪模拟电路尺寸设计的搜索空间，为LLM与传统自动化方法的结合提供了新思路。

Abstract: In the design process of the analog circuit pre-layout phase, device sizing
is an important step in determining whether an analog circuit can meet the
required performance metrics. Many existing techniques extract the circuit
sizing task as a mathematical optimization problem to solve and continuously
improve the optimization efficiency from a mathematical perspective. But they
ignore the automatic introduction of prior knowledge, fail to achieve effective
pruning of the search space, which thereby leads to a considerable compression
margin remaining in the search space. To alleviate this problem, we propose a
large language model (LLM)-based multi-agent framework for analog circuits'
sizing relationships extraction from academic papers. The search space in the
sizing process can be effectively pruned based on the sizing relationship
extracted by this framework. Eventually, we conducted tests on 3 types of
circuits, and the optimization efficiency was improved by $2.32 \sim 26.6
\times$. This work demonstrates that the LLM can effectively prune the search
space for analog circuit sizing, providing a new solution for the combination
of LLMs and conventional analog circuit design automation methods.

</details>


### [39] [How Robust is Model Editing after Fine-Tuning? An Empirical Study on Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.18428)
*Feng He,Zhenyang Liu,Marco Valentino,Zhixue Zhao*

Main category: cs.AI

TL;DR: 研究发现，模型编辑后的行为在微调后通常无法保留，尤其是DoRA微调方法对编辑的逆转效果最强，而UCE编辑方法相对更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 探讨模型编辑与微调之间的相互作用，以解决编辑是否会在微调后保留的问题，这对AI安全性和应用有重要意义。

Method: 在T2I扩散模型（Stable Diffusion和FLUX）上，结合两种编辑技术（UCE和ReFACT）和三种微调方法（DreamBooth、LoRA、DoRA），通过多样化的编辑任务和评估指标进行实证分析。

Result: 编辑通常无法在微调后保留，DoRA的编辑逆转效果最强，UCE编辑方法在微调后保留效果较好。

Conclusion: 当前编辑方法存在局限性，需开发更鲁棒的技术以确保长期控制和AI系统的对齐性，同时微调可作为恶意编辑的修复机制，但需在微调后重新编辑以维持安全性。

Abstract: Model editing offers a low-cost technique to inject or correct a particular
behavior in a pre-trained model without extensive retraining, supporting
applications such as factual correction and bias mitigation. Despite this
common practice, it remains unknown whether edits persist after fine-tuning or
whether they are inadvertently reversed. This question has fundamental
practical implications. For example, if fine-tuning removes prior edits, it
could serve as a defence mechanism against hidden malicious edits. Vice versa,
the unintended removal of edits related to bias mitigation could pose serious
safety concerns. We systematically investigate the interaction between model
editing and fine-tuning in the context of T2I diffusion models, which are known
to exhibit biases and generate inappropriate content. Our study spans two T2I
model families (Stable Diffusion and FLUX), two sota editing techniques, and
three fine-tuning methods (DreamBooth, LoRA, and DoRA). Through an extensive
empirical analysis across diverse editing tasks and evaluation metrics, our
findings reveal a trend: edits generally fail to persist through fine-tuning,
even when fine-tuning is tangential or unrelated to the edits. Notably, we
observe that DoRA exhibits the strongest edit reversal effect. At the same
time, among editing methods, UCE demonstrates greater robustness, retaining
significantly higher efficacy post-fine-tuning compared to ReFACT. These
findings highlight a crucial limitation in current editing methodologies,
emphasizing the need for more robust techniques to ensure reliable long-term
control and alignment of deployed AI systems. These findings have dual
implications for AI safety: they suggest that fine-tuning could serve as a
remediation mechanism for malicious edits while simultaneously highlighting the
need for re-editing after fine-tuning to maintain beneficial safety and
alignment properties.

</details>


### [40] [Standard Applicability Judgment and Cross-jurisdictional Reasoning: A RAG-based Framework for Medical Device Compliance](https://arxiv.org/abs/2506.18511)
*Yu Han,Aaron Ceross,Jeroen H. M. Bergmann*

Main category: cs.AI

TL;DR: 提出了一种基于检索增强生成（RAG）的模块化AI系统，用于自动化确定医疗器械监管标准的适用性，解决了标准适用性判断的挑战。


<details>
  <summary>Details</summary>
Motivation: 医疗器械合规性中标准适用性判断是一个关键但研究不足的问题，需要专家对分散且异质的文档进行解释。

Method: 系统通过RAG流程，从自由文本设备描述中检索候选标准，并利用大语言模型推断特定司法管辖区的适用性（强制性、推荐或不适用），并提供可追溯的论证。

Result: 系统在分类准确率达到73%，Top-5检索召回率为87%，优于仅检索、零样本和基于规则的基线方法。

Conclusion: 该系统首次实现了端到端的标准适用性推理，支持跨司法管辖区（如中美）的冲突解决和适用性论证，推动了可扩展且可解释的AI辅助监管科学。

Abstract: Identifying the appropriate regulatory standard applicability remains a
critical yet understudied challenge in medical device compliance, frequently
necessitating expert interpretation of fragmented and heterogeneous
documentation across different jurisdictions. To address this challenge, we
introduce a modular AI system that leverages a retrieval-augmented generation
(RAG) pipeline to automate standard applicability determination. Given a
free-text device description, our system retrieves candidate standards from a
curated corpus and uses large language models to infer jurisdiction-specific
applicability, classified as Mandatory, Recommended, or Not Applicable, with
traceable justifications. We construct an international benchmark dataset of
medical device descriptions with expert-annotated standard mappings, and
evaluate our system against retrieval-only, zero-shot, and rule-based
baselines. The proposed approach attains a classification accuracy of 73% and a
Top-5 retrieval recall of 87%, demonstrating its effectiveness in identifying
relevant regulatory standards. We introduce the first end-to-end system for
standard applicability reasoning, enabling scalable and interpretable
AI-supported regulatory science. Notably, our region-aware RAG agent performs
cross-jurisdictional reasoning between Chinese and U.S. standards, supporting
conflict resolution and applicability justification across regulatory
frameworks.

</details>


### [41] [A Question Bank to Assess AI Inclusivity: Mapping out the Journey from Diversity Errors to Inclusion Excellence](https://arxiv.org/abs/2506.18538)
*Rifat Ara Shams,Didar Zowghi,Muneera Bano*

Main category: cs.AI

TL;DR: 本文提出一个包含253个问题的AI包容性评估工具，覆盖五个维度，旨在促进AI系统的多样性与包容性。


<details>
  <summary>Details</summary>
Motivation: 现有AI风险评估框架缺乏对包容性的标准化评估工具，导致AI系统可能忽视多样性与包容性原则。

Method: 通过文献综述、D&I指南、负责任AI框架和模拟用户研究，开发了一个结构化问题库，并利用70个AI生成角色进行模拟评估。

Result: 问题库在评估AI包容性方面表现出相关性和有效性，强调了将D&I原则融入AI开发和治理的重要性。

Conclusion: 该问题库为研究人员、从业者和政策制定者提供了系统评估和提升AI包容性的工具，推动更公平和负责任的AI技术发展。

Abstract: Ensuring diversity and inclusion (D&I) in artificial intelligence (AI) is
crucial for mitigating biases and promoting equitable decision-making. However,
existing AI risk assessment frameworks often overlook inclusivity, lacking
standardized tools to measure an AI system's alignment with D&I principles.
This paper introduces a structured AI inclusivity question bank, a
comprehensive set of 253 questions designed to evaluate AI inclusivity across
five pillars: Humans, Data, Process, System, and Governance. The development of
the question bank involved an iterative, multi-source approach, incorporating
insights from literature reviews, D&I guidelines, Responsible AI frameworks,
and a simulated user study. The simulated evaluation, conducted with 70
AI-generated personas related to different AI jobs, assessed the question
bank's relevance and effectiveness for AI inclusivity across diverse roles and
application domains. The findings highlight the importance of integrating D&I
principles into AI development workflows and governance structures. The
question bank provides an actionable tool for researchers, practitioners, and
policymakers to systematically assess and enhance the inclusivity of AI
systems, paving the way for more equitable and responsible AI technologies.

</details>


### [42] [T-CPDL: A Temporal Causal Probabilistic Description Logic for Developing Logic-RAG Agent](https://arxiv.org/abs/2506.18559)
*Hong Qing Yu*

Main category: cs.AI

TL;DR: 提出了一种名为T-CPDL的框架，结合了时间、因果和概率逻辑，显著提升了语言模型在结构化推理中的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在流畅文本生成方面表现出色，但在涉及时间约束、因果关系和概率推理的结构化推理中表现不佳。

Method: 扩展传统描述逻辑，引入时间区间算子、显式因果关系和概率标注，提出两种T-CPDL变体。

Result: 在时间推理和因果推断基准测试中，T-CPDL显著提高了推理准确性、可解释性和置信度校准。

Conclusion: T-CPDL增强了语言模型的透明推理能力，为Logic-RAG框架的发展奠定了基础。

Abstract: Large language models excel at generating fluent text but frequently struggle
with structured reasoning involving temporal constraints, causal relationships,
and probabilistic reasoning. To address these limitations, we propose Temporal
Causal Probabilistic Description Logic (T-CPDL), an integrated framework that
extends traditional Description Logic with temporal interval operators,
explicit causal relationships, and probabilistic annotations. We present two
distinct variants of T-CPDL: one capturing qualitative temporal relationships
through Allen's interval algebra, and another variant enriched with explicit
timestamped causal assertions. Both variants share a unified logical structure,
enabling complex reasoning tasks ranging from simple temporal ordering to
nuanced probabilistic causation. Empirical evaluations on temporal reasoning
and causal inference benchmarks confirm that T-CPDL substantially improves
inference accuracy, interpretability, and confidence calibration of language
model outputs. By delivering transparent reasoning paths and fine-grained
temporal and causal semantics, T-CPDL significantly enhances the capability of
language models to support robust, explainable, and trustworthy
decision-making. This work also lays the groundwork for developing advanced
Logic-Retrieval-Augmented Generation (Logic-RAG) frameworks, potentially
boosting the reasoning capabilities and efficiency of knowledge graph-enhanced
RAG systems.

</details>


### [43] [Airalogy: AI-empowered universal data digitization for research automation](https://arxiv.org/abs/2506.18586)
*Zijie Yang,Qiji Zhou,Fang Guo,Sijie Zhang,Yexun Xi,Jinglei Nie,Yudian Zhu,Liping Huang,Chou Wu,Yonghe Xia,Xiaoyu Ma,Yingming Pu,Panzhong Lu,Junshu Pan,Mingtao Chen,Tiannan Guo,Yanmei Dou,Hongyu Chen,Anping Zeng,Jiaxing Huang,Tian Xu,Yue Zhang*

Main category: cs.AI

TL;DR: Airalogy是一个多学科AI驱动平台，旨在标准化和数字化研究数据，解决跨学科数据管理的挑战。


<details>
  <summary>Details</summary>
Motivation: 当前AI应用受限于数据碎片化和缺乏标准化，阻碍了多学科的AI赋能。

Method: 开发Airalogy平台，结合科学领域知识与计算技术，提供标准化数据记录和AI辅助功能。

Result: Airalogy已在西湖大学四个学院部署，支持研究自动化和创新。

Conclusion: Airalogy有望加速全球科研创新，推动AI在多学科的应用。

Abstract: Research data are the foundation of Artificial Intelligence (AI)-driven
science, yet current AI applications remain limited to a few fields with
readily available, well-structured, digitized datasets. Achieving comprehensive
AI empowerment across multiple disciplines is still out of reach. Present-day
research data collection is often fragmented, lacking unified standards,
inefficiently managed, and difficult to share. Creating a single platform for
standardized data digitization needs to overcome the inherent challenge of
balancing between universality (supporting the diverse, ever-evolving needs of
various disciplines) and standardization (enforcing consistent formats to fully
enable AI). No existing platform accommodates both facets. Building a truly
multidisciplinary platform requires integrating scientific domain knowledge
with sophisticated computing skills. Researchers often lack the computational
expertise to design customized and standardized data recording methods, whereas
platform developers rarely grasp the intricate needs of multiple scientific
domains. These gaps impede research data standardization and hamper AI-driven
progress. In this study, we address these challenges by developing Airalogy
(https://airalogy.com), the world's first AI- and community-driven platform
that balances universality and standardization for digitizing research data
across multiple disciplines. Airalogy represents entire research workflows
using customizable, standardized data records and offers an advanced AI
research copilot for intelligent Q&A, automated data entry, analysis, and
research automation. Already deployed in laboratories across all four schools
of Westlake University, Airalogy has the potential to accelerate and automate
scientific innovation in universities, industry, and the global research
community-ultimately benefiting humanity as a whole.

</details>


### [44] [AggTruth: Contextual Hallucination Detection using Aggregated Attention Scores in LLMs](https://arxiv.org/abs/2506.18628)
*Piotr Matys,Jan Eliasz,Konrad Kiełczyński,Mikołaj Langner,Teddy Ferdinan,Jan Kocoń,Przemysław Kazienko*

Main category: cs.AI

TL;DR: AggTruth是一种通过分析上下文注意力分数分布来检测大语言模型（LLMs）幻觉的方法，表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在实际应用中（如RAG设置）产生的幻觉问题。

Method: 提出四种基于注意力分数聚合的变体方法，分析特征选择技术和注意力头数量对性能的影响。

Result: 在不同LLM和任务中表现稳定，优于当前最优方法。

Conclusion: 注意力头的选择对检测性能至关重要，AggTruth能有效检测幻觉。

Abstract: In real-world applications, Large Language Models (LLMs) often hallucinate,
even in Retrieval-Augmented Generation (RAG) settings, which poses a
significant challenge to their deployment. In this paper, we introduce
AggTruth, a method for online detection of contextual hallucinations by
analyzing the distribution of internal attention scores in the provided context
(passage). Specifically, we propose four different variants of the method, each
varying in the aggregation technique used to calculate attention scores. Across
all LLMs examined, AggTruth demonstrated stable performance in both same-task
and cross-task setups, outperforming the current SOTA in multiple scenarios.
Furthermore, we conducted an in-depth analysis of feature selection techniques
and examined how the number of selected attention heads impacts detection
performance, demonstrating that careful selection of heads is essential to
achieve optimal results.

</details>


### [45] [Dual-level Behavioral Consistency for Inter-group and Intra-group Coordination in Multi-Agent Systems](https://arxiv.org/abs/2506.18651)
*Shuocun Yang,Huawen Hu,Enze Shi,Shu Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种名为DLBC的新方法，用于在MARL中动态调节组内和组间的行为一致性，以提升协作效率和任务分工。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注组内行为一致性，而忽视了组间行为一致性的重要性，DLBC旨在填补这一空白。

Method: DLBC通过将智能体分组，并动态调节组内和组间的行为多样性，直接约束策略函数以实现广泛适用性。

Result: 实验表明，DLBC显著提升了组内协作和组间任务分工，带来性能提升。

Conclusion: DLBC为多智能体系统的行为一致性控制提供了新思路，未来可探索其在更复杂任务中的应用。

Abstract: Behavioral diversity in Multi-agent reinforcement learning(MARL) represents
an emerging and promising research area. Prior work has largely centered on
intra-group behavioral consistency in multi-agent systems, with limited
attention given to behavioral consistency in multi-agent grouping scenarios. In
this paper, we introduce Dual-Level Behavioral Consistency (DLBC), a novel MARL
control method designed to explicitly regulate agent behaviors at both
intra-group and inter-group levels. DLBC partitions agents into distinct groups
and dynamically modulates behavioral diversity both within and between these
groups. By dynamically modulating behavioral diversity within and between these
groups, DLBC achieves enhanced division of labor through inter-group
consistency, which constrains behavioral strategies across different groups.
Simultaneously, intra-group consistency, achieved by aligning behavioral
strategies within each group, fosters stronger intra-group cooperation.
Crucially, DLBC's direct constraint of agent policy functions ensures its broad
applicability across various algorithmic frameworks. Experimental results in
various grouping cooperation scenarios demonstrate that DLBC significantly
enhances both intra-group cooperative performance and inter-group task
specialization, yielding substantial performance improvements. DLBC provides
new ideas for behavioral consistency control of multi-intelligent body systems,
and its potential for application in more complex tasks and dynamic
environments can be further explored in the future.

</details>


### [46] [Programming by Backprop: LLMs Acquire Reusable Algorithmic Abstractions During Code Training](https://arxiv.org/abs/2506.18777)
*Jonathan Cook,Silvia Sapora,Arash Ahmadian,Akbir Khan,Tim Rocktaschel,Jakob Foerster,Laura Ruis*

Main category: cs.AI

TL;DR: 论文提出Programming by Backprop (PBB)方法，通过仅训练源代码（无输入输出示例）提升大语言模型（LLMs）的推理能力，发现其能隐式评估程序并生成输出，且代码形式优于语义描述。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs通过源代码训练提升通用推理能力的机制，尤其是无需输入输出示例的潜在驱动因素。

Method: 通过微调LLMs在两组程序（带输入输出示例和不带）上，比较其评估能力，并分析PBB的效果。

Result: LLMs能隐式评估无输入输出示例的程序，代码形式效果更好，且PBB比传统输入输出训练更稳健。

Conclusion: 代码训练使LLMs内化可重用算法抽象，未来可进一步优化符号程序学习，拓展模型对齐等应用。

Abstract: Training large language models (LLMs) on source code significantly enhances
their general-purpose reasoning abilities, but the mechanisms underlying this
generalisation are poorly understood. In this paper, we propose Programming by
Backprop (PBB) as a potential driver of this effect - teaching a model to
evaluate a program for inputs by training on its source code alone, without
ever seeing I/O examples. To explore this idea, we finetune LLMs on two sets of
programs representing simple maths problems and algorithms: one with source
code and I/O examples (w/ IO), the other with source code only (w/o IO). We
find evidence that LLMs have some ability to evaluate w/o IO programs for
inputs in a range of experimental settings, and make several observations.
Firstly, PBB works significantly better when programs are provided as code
rather than semantically equivalent language descriptions. Secondly, LLMs can
produce outputs for w/o IO programs directly, by implicitly evaluating the
program within the forward pass, and more reliably when stepping through the
program in-context via chain-of-thought. We further show that PBB leads to more
robust evaluation of programs across inputs than training on I/O pairs drawn
from a distribution that mirrors naturally occurring data. Our findings suggest
a mechanism for enhanced reasoning through code training: it allows LLMs to
internalise reusable algorithmic abstractions. Significant scope remains for
future work to enable LLMs to more effectively learn from symbolic procedures,
and progress in this direction opens other avenues like model alignment by
training on formal constitutional principles.

</details>


### [47] [TRIZ Agents: A Multi-Agent LLM Approach for TRIZ-Based Innovation](https://arxiv.org/abs/2506.18783)
*Kamil Szczepanik,Jarosław A. Chudziak*

Main category: cs.AI

TL;DR: 论文提出了一种基于LLM的多智能体系统（TRIZ agents），用于协作解决TRIZ方法中的创新问题，通过多领域专家智能体模拟创新过程，并在工程案例中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: TRIZ方法的应用常受限于其复杂性和跨学科知识需求，而LLM的发展为自动化部分流程提供了新可能。

Method: 提出多智能体系统，每个智能体具备专业能力和工具访问权限，协作执行TRIZ步骤。

Result: 通过工程案例验证，多智能体协作能产生多样化的创新解决方案。

Conclusion: 研究展示了去中心化问题解决在复杂创新任务中的优势，为AI驱动的创新提供了新方向。

Abstract: TRIZ, the Theory of Inventive Problem Solving, is a structured,
knowledge-based framework for innovation and abstracting problems to find
inventive solutions. However, its application is often limited by the
complexity and deep interdisciplinary knowledge required. Advancements in Large
Language Models (LLMs) have revealed new possibilities for automating parts of
this process. While previous studies have explored single LLMs in TRIZ
applications, this paper introduces a multi-agent approach. We propose an
LLM-based multi-agent system, called TRIZ agents, each with specialized
capabilities and tool access, collaboratively solving inventive problems based
on the TRIZ methodology. This multi-agent system leverages agents with various
domain expertise to efficiently navigate TRIZ steps. The aim is to model and
simulate an inventive process with language agents. We assess the effectiveness
of this team of agents in addressing complex innovation challenges based on a
selected case study in engineering. We demonstrate the potential of agent
collaboration to produce diverse, inventive solutions. This research
contributes to the future of AI-driven innovation, showcasing the advantages of
decentralized problem-solving in complex ideation tasks.

</details>


### [48] [ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints during Generation](https://arxiv.org/abs/2506.18810)
*Siao Tang,Xinyin Ma,Gongfan Fang,Xinchao Wang*

Main category: cs.AI

TL;DR: 论文提出ConciseHint框架，通过生成过程中注入提示，减少大型推理模型的冗长输出，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型在复杂任务中表现优异，但推理过程冗长，效率低下。现有方法未关注生成过程中的简洁性干预。

Method: 提出ConciseHint框架，在推理生成过程中动态注入文本提示（手动设计或基于简洁数据训练），并根据查询复杂度自适应调整提示强度。

Result: 在DeepSeek-R1和Qwen-3等模型上验证，推理长度减少65%（GSM8K基准），且几乎无准确率损失。

Conclusion: ConciseHint能有效生成简洁推理过程，同时保持模型性能，填补了生成过程中简洁性干预的空白。

Abstract: Recent advancements in large reasoning models (LRMs) like DeepSeek-R1 and
OpenAI o1 series have achieved notable performance enhancements on complex
reasoning tasks by scaling up the generation length by Chain-of-Thought (CoT).
However, an emerging issue is their inclination to produce excessively verbose
reasoning processes, leading to the inefficiency problem. Existing literature
on improving efficiency mainly adheres to the before-reasoning paradigms such
as prompting and reasoning or fine-tuning and reasoning, but ignores the
promising direction of directly encouraging the model to speak concisely by
intervening during the generation of reasoning. In order to fill the blank, we
propose a framework dubbed ConciseHint, which continuously encourages the
reasoning model to speak concisely by injecting the textual hint (manually
designed or trained on the concise data) during the token generation of the
reasoning process. Besides, ConciseHint is adaptive to the complexity of the
query by adaptively adjusting the hint intensity, which ensures it will not
undermine model performance. Experiments on the state-of-the-art LRMs,
including DeepSeek-R1 and Qwen-3 series, demonstrate that our method can
effectively produce concise reasoning processes while maintaining performance
well. For instance, we achieve a reduction ratio of 65\% for the reasoning
length on GSM8K benchmark with Qwen-3 4B with nearly no accuracy loss.

</details>


### [49] [Steering Conceptual Bias via Transformer Latent-Subspace Activation](https://arxiv.org/abs/2506.18887)
*Vansh Sharma,Venkat Raman*

Main category: cs.AI

TL;DR: 通过激活语言模型中的潜在子空间，引导科学代码生成偏向特定编程语言，提出了一种梯度优化的自适应激活框架（G-ACT），显著提高了目标语言的选择准确性。


<details>
  <summary>Details</summary>
Motivation: 研究如何通过激活语言模型的潜在子空间，实现对科学代码生成中编程语言的定向控制，以解决现有方法的局限性和泛化性问题。

Method: 开发了G-ACT框架，通过聚类每提示激活差异为少量方向，并在线训练轻量级逐层探针来选择最佳激活向量，以优化语言选择。

Result: 在LLaMA-3.2 3B中，G-ACT将平均探针分类准确率提高15%，早期层（0-6）提升61.5%；在LLaMA-3.3 70B中，关键层的定向注入仍有效。

Conclusion: G-ACT提供了一种可扩展、可解释且高效的概念级控制机制，适用于实际代理系统。

Abstract: This work examines whether activating latent subspaces in language models
(LLMs) can steer scientific code generation toward a specific programming
language. Five causal LLMs were first evaluated on scientific coding prompts to
quantify their baseline bias among four programming languages. A static
neuron-attribution method, perturbing the highest activated MLP weight for a
C++ or CPP token, proved brittle and exhibited limited generalization across
prompt styles and model scales. To address these limitations, a
gradient-refined adaptive activation steering framework (G-ACT) was developed:
per-prompt activation differences are clustered into a small set of steering
directions, and lightweight per-layer probes are trained and refined online to
select the appropriate steering vector. In LLaMA-3.2 3B, this approach reliably
biases generation towards the CPP language by increasing the average probe
classification accuracy by 15% and the early layers (0-6) improving the probe
classification accuracy by 61.5% compared to the standard ACT framework. For
LLaMA-3.3 70B, where attention-head signals become more diffuse, targeted
injections at key layers still improve language selection. Although per-layer
probing introduces a modest inference overhead, it remains practical by
steering only a subset of layers and enables reproducible model behavior. These
results demonstrate a scalable, interpretable and efficient mechanism for
concept-level control for practical agentic systems.

</details>


### [50] [jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval](https://arxiv.org/abs/2506.18902)
*Michael Günther,Saba Sturua,Mohammad Kalim Akram,Isabelle Mohr,Andrei Ungureanu,Sedigheh Eslami,Scott Martens,Bo Wang,Nan Wang,Han Xiao*

Main category: cs.AI

TL;DR: Jina-embeddings-v4是一个3.8B参数的多模态嵌入模型，通过新颖架构统一文本和图像表示，支持单向量和多向量嵌入，并在多样化检索任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决多模态（文本和图像）表示的统一问题，并优化跨模态检索任务的性能。

Method: 采用任务特定的LoRA适配器，支持单向量和多向量嵌入的后期交互架构。

Result: 在单模态和跨模态检索任务中达到最先进性能，尤其在处理视觉丰富内容时表现突出。

Conclusion: Jina-embeddings-v4在多模态检索领域具有显著优势，并通过新基准Jina-VDR进一步验证其能力。

Abstract: We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding
model that unifies text and image representations through a novel architecture
supporting both single-vector and multi-vector embeddings in the late
interaction style. The model incorporates task-specific Low-Rank Adaptation
(LoRA) adapters to optimize performance across diverse retrieval scenarios,
including query-based information retrieval, cross-modal semantic similarity,
and programming code search. Comprehensive evaluations demonstrate that
jina-embeddings-v4 achieves state-of-the-art performance on both single- modal
and cross-modal retrieval tasks, with particular strength in processing
visually rich content such as tables, charts, diagrams, and mixed-media
formats. To facilitate evaluation of this capability, we also introduce
Jina-VDR, a novel benchmark specifically designed for visually rich image
retrieval.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [51] [Outcome-Based Education: Evaluating Students' Perspectives Using Transformer](https://arxiv.org/abs/2506.17223)
*Shuvra Smaran Das,Anirban Saha Anik,Md Kishor Morol,Mohammad Sakib Mahmood*

Main category: cs.CL

TL;DR: 研究使用DistilBERT和LIME分析学生反馈，以改进基于结果的教育（OBE），效果优于其他机器学习模型。


<details>
  <summary>Details</summary>
Motivation: 通过分析学生反馈，实现OBE的可测量目标，提升教育实践。

Method: 采用DistilBERT模型和LIME解释工具，分析NLP数据集中的学生反馈。

Result: 结合Transformer模型和LIME，提供了强大且直观的学生反馈分析框架。

Conclusion: 该方法更符合OBE原则，通过数据驱动提升教育成果。

Abstract: Outcome-Based Education (OBE) emphasizes the development of specific
competencies through student-centered learning. In this study, we reviewed the
importance of OBE and implemented transformer-based models, particularly
DistilBERT, to analyze an NLP dataset that includes student feedback. Our
objective is to assess and improve educational outcomes. Our approach is better
than other machine learning models because it uses the transformer's deep
understanding of language context to classify sentiment better, giving better
results across a wider range of matrices. Our work directly contributes to
OBE's goal of achieving measurable outcomes by facilitating the identification
of patterns in student learning experiences. We have also applied LIME (local
interpretable model-agnostic explanations) to make sure that model predictions
are clear. This gives us understandable information about how key terms affect
sentiment. Our findings indicate that the combination of transformer models and
LIME explanations results in a strong and straightforward framework for
analyzing student feedback. This aligns more closely with the principles of OBE
and ensures the improvement of educational practices through data-driven
insights.

</details>


### [52] [Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs](https://arxiv.org/abs/2506.17231)
*Xiang Li,Chong Zhang,Jia Wang,Fangyu Wu,Yushi Li,Xiaobo Jin*

Main category: cs.CL

TL;DR: 提出了一种对抗性提示蒸馏方法，通过结合掩码语言建模、强化学习和动态温度控制，使小型语言模型能够对主流大语言模型进行越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 当前越狱攻击方法效率低、计算成本高且跨模型适应性差，难以应对大语言模型的快速发展和新防御策略。

Method: 结合掩码语言建模、强化学习和动态温度控制，通过提示生成和蒸馏方法实现小型语言模型对主流大语言模型的越狱攻击。

Result: 实验验证了该方法在攻击成功率和危害性上的优越性，同时体现了资源效率和跨模型适应性。

Conclusion: 研究探索了大语言模型越狱能力向小型语言模型蒸馏的可行性，揭示了模型的脆弱性，为大语言模型安全研究提供了新思路。

Abstract: Attacks on large language models (LLMs) in jailbreaking scenarios raise many
security and ethical issues. Current jailbreak attack methods face problems
such as low efficiency, high computational cost, and poor cross-model
adaptability and versatility, which make it difficult to cope with the rapid
development of LLM and new defense strategies. Our work proposes an Adversarial
Prompt Distillation, which combines masked language modeling, reinforcement
learning, and dynamic temperature control through a prompt generation and
distillation method. It enables small language models (SLMs) to jailbreak
attacks on mainstream LLMs. The experimental results verify the superiority of
the proposed method in terms of attack success rate and harm, and reflect the
resource efficiency and cross-model adaptability. This research explores the
feasibility of distilling the jailbreak ability of LLM to SLM, reveals the
model's vulnerability, and provides a new idea for LLM security research.

</details>


### [53] [GTA: Grouped-head latenT Attention](https://arxiv.org/abs/2506.17286)
*Luoyang Sun,Jiwen Jiang,Cheng Deng,Xinjian Wu,Haifeng Zhang,Lei Chen,Lionel Ni,Jun Wang*

Main category: cs.CL

TL;DR: GTA是一种新型注意力机制，通过共享注意力图和压缩KV缓存，显著减少计算和内存开销，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的注意力机制存在计算和内存开销大的问题，限制了在资源有限硬件上的部署效率。

Method: GTA包括共享注意力图机制和非线性值解码器，分别减少键缓存大小和压缩值缓存。

Result: GTA减少计算FLOPs达62.5%，KV缓存减少70%，推理速度提升2倍。

Conclusion: GTA有效提升LLM部署效率，适用于资源受限环境。

Abstract: Attention mechanisms underpin the success of large language models (LLMs),
yet their substantial computational and memory overhead poses challenges for
optimizing efficiency and performance. A critical bottleneck arises as KV cache
and attention computations scale rapidly with text length, challenging
deployment on hardware with limited computational and memory resources. We
observe that attention mechanisms exhibit substantial redundancy, since the KV
cache can be significantly compressed and attention maps across heads display
high similarity, revealing that much of the computation and storage is
unnecessary. Leveraging these insights, we propose \textbf{G}rouped-Head
Laten\textbf{T} \textbf{A}ttention (GTA), a novel attention mechanism that
reduces memory usage and computational complexity while maintaining
performance. GTA comprises two components: (1) a shared attention map mechanism
that reuses attention scores across multiple heads, decreasing the key cache
size; and (2) a nonlinear value decoder with learned projections that
compresses the value cache into a latent space, further cutting memory needs.
GTA cuts attention computation FLOPs by up to \emph{62.5\%} versus
Grouped-Query Attention and shrink the KV cache by up to \emph{70\%}, all while
avoiding the extra overhead of Multi-Head Latent Attention to improve LLM
deployment efficiency. Consequently, GTA models achieve a \emph{2x} increase in
end-to-end inference speed, with prefill benefiting from reduced computational
cost and decoding benefiting from the smaller cache footprint.

</details>


### [54] [AI-Generated Game Commentary: A Survey and a Datasheet Repository](https://arxiv.org/abs/2506.17294)
*Qirui Zheng,Xingbo Wang,Keyuan Cheng,Yunlong Lu,Wenxin Li*

Main category: cs.CL

TL;DR: 本文介绍了AI生成游戏解说（AIGGC）的通用框架，并综述了45个现有数据集和方法，同时提供了结构化数据表以支持未来研究。


<details>
  <summary>Details</summary>
Motivation: AIGGC因其市场潜力与技术挑战受到关注，需解决语言模型的多方面需求。

Method: 提出通用框架，综述现有数据集和方法，分类比较评估指标。

Result: 提供了结构化数据表，支持未来研究和基准测试。

Conclusion: AIGGC领域仍需进一步研究，本文为未来工作提供了资源和方向。

Abstract: AI-Generated Game Commentary (AIGGC) has gained increasing attention due to
its market potential and inherent technical challenges. As a comprehensive
multimodal Natural Language Processing (NLP) task, AIGGC imposes substantial
demands on language models, including factual accuracy, logical reasoning,
expressive text generation, generation speed, and context management. In this
paper, we introduce a general framework for AIGGC and present a comprehensive
survey of 45 existing game commentary dataset and methods according to key
challenges they aim to address in this domain. We further classify and compare
various evaluation metrics commonly used in this domain. To support future
research and benchmarking, we also provide a structured datasheet summarizing
the essential attributes of these datasets in appendix, which is meanwhile
publicly available in an open repository.

</details>


### [55] [Semantic uncertainty in advanced decoding methods for LLM generation](https://arxiv.org/abs/2506.17296)
*Darius Foodeei,Simin Fan,Martin Jaggi*

Main category: cs.CL

TL;DR: 研究探讨了不同解码方法对大型语言模型（LLM）输出的语义不确定性的影响，发现结构化解码方法（如CoT和推测采样）能在保持或提高输出质量的同时增加语义多样性。


<details>
  <summary>Details</summary>
Motivation: 探索不同解码策略如何影响LLM输出的多样性和可靠性，以解决实际应用中多样性与准确性之间的权衡问题。

Method: 通过问答、摘要和代码生成任务实验，比较了CoT解码和推测采样等方法的性能。

Result: CoT解码提高了语义多样性但降低了预测熵，代码生成Pass@2率提升48.8%；推测采样在摘要任务中表现优异，ROUGE分数更高。

Conclusion: 结构化解码方法可同时提升多样性和准确性，对实际应用中的LLM部署具有重要意义。

Abstract: This study investigates semantic uncertainty in large language model (LLM)
outputs across different decoding methods, focusing on emerging techniques like
speculative sampling and chain-of-thought (CoT) decoding. Through experiments
on question answering, summarization, and code generation tasks, we analyze how
different decoding strategies affect both the diversity and reliability of
model outputs. Our findings reveal that while CoT decoding demonstrates higher
semantic diversity, it maintains lower predictive entropy, suggesting that
structured exploration can lead to more confident and accurate outputs. This is
evidenced by a 48.8% improvement in code generation Pass@2 rates, despite lower
alignment with reference solutions. For summarization tasks, speculative
sampling proved particularly effective, achieving superior ROUGE scores while
maintaining moderate semantic diversity. Our results challenge conventional
assumptions about trade-offs between diversity and accuracy in language model
outputs, demonstrating that properly structured decoding methods can increase
semantic exploration while maintaining or improving output quality. These
findings have significant implications for deploying language models in
practical applications where both reliability and diverse solution generation
are crucial.

</details>


### [56] [Mercury: Ultra-Fast Language Models Based on Diffusion](https://arxiv.org/abs/2506.17298)
*Inception Labs,Samar Khanna,Siddhant Kharbanda,Shufan Li,Harshit Varma,Eric Wang,Sawyer Birnbaum,Ziyang Luo,Yanis Miraoui,Akash Palrecha,Stefano Ermon,Aditya Grover,Volodymyr Kuleshov*

Main category: cs.CL

TL;DR: Mercury是一代基于扩散的商业规模大语言模型（LLM），专为编码应用设计，在速度和性能上达到新水平。


<details>
  <summary>Details</summary>
Motivation: 开发高效且高质量的编码专用LLM，以满足实际应用需求。

Method: 基于Transformer架构，采用扩散方法训练，支持并行预测多令牌。

Result: Mercury Coder Mini和Small在NVIDIA H100 GPU上分别达到1109和737 tokens/sec的吞吐量，速度优化模型性能提升10倍，质量相当。

Conclusion: Mercury Coder在编码任务中表现优异，速度和性能均领先，并通过实际开发者验证。

Abstract: We present Mercury, a new generation of commercial-scale large language
models (LLMs) based on diffusion. These models are parameterized via the
Transformer architecture and trained to predict multiple tokens in parallel. In
this report, we detail Mercury Coder, our first set of diffusion LLMs designed
for coding applications. Currently, Mercury Coder comes in two sizes: Mini and
Small. These models set a new state-of-the-art on the speed-quality frontier.
Based on independent evaluations conducted by Artificial Analysis, Mercury
Coder Mini and Mercury Coder Small achieve state-of-the-art throughputs of 1109
tokens/sec and 737 tokens/sec, respectively, on NVIDIA H100 GPUs and outperform
speed-optimized frontier models by up to 10x on average while maintaining
comparable quality. We discuss additional results on a variety of code
benchmarks spanning multiple languages and use-cases as well as real-world
validation by developers on Copilot Arena, where the model currently ranks
second on quality and is the fastest model overall. We also release a public
API at https://platform.inceptionlabs.ai/ and free playground at
https://chat.inceptionlabs.ai

</details>


### [57] [PRAISE: Enhancing Product Descriptions with LLM-Driven Structured Insights](https://arxiv.org/abs/2506.17314)
*Adnan Qidwai,Srija Mukhopadhyay,Prerana Khatiwada,Dan Roth,Vivek Gupta*

Main category: cs.CL

TL;DR: PRAISE是一个基于LLM的系统，用于自动从客户评论和卖家描述中提取、比较和结构化信息，帮助改进电商产品描述的准确性和完整性。


<details>
  <summary>Details</summary>
Motivation: 电商中卖家提供的产品描述往往不完整或不准确，而客户评论包含有价值的信息但难以手动整理。

Method: 使用大型语言模型（LLMs）自动提取、比较和结构化客户评论与卖家描述的差异，并提供直观界面展示不一致之处。

Result: PRAISE能有效生成结构化洞察，帮助卖家改进描述，买家评估产品可靠性。

Conclusion: PRAISE有潜力显著提升电商产品目录的质量和可信度。

Abstract: Accurate and complete product descriptions are crucial for e-commerce, yet
seller-provided information often falls short. Customer reviews offer valuable
details but are laborious to sift through manually. We present PRAISE: Product
Review Attribute Insight Structuring Engine, a novel system that uses Large
Language Models (LLMs) to automatically extract, compare, and structure
insights from customer reviews and seller descriptions. PRAISE provides users
with an intuitive interface to identify missing, contradictory, or partially
matching details between these two sources, presenting the discrepancies in a
clear, structured format alongside supporting evidence from reviews. This
allows sellers to easily enhance their product listings for clarity and
persuasiveness, and buyers to better assess product reliability. Our
demonstration showcases PRAISE's workflow, its effectiveness in generating
actionable structured insights from unstructured reviews, and its potential to
significantly improve the quality and trustworthiness of e-commerce product
catalogs.

</details>


### [58] [Towards Safety Evaluations of Theory of Mind in Large Language Models](https://arxiv.org/abs/2506.17352)
*Tatsuhiro Aoshima,Mitsuaki Akiyama*

Main category: cs.CL

TL;DR: 该论文探讨了大型语言模型（LLMs）在安全评估中可能表现出的欺骗行为，提出通过测量其心智理论能力来评估风险，并发现LLMs的心智理论能力未随阅读理解能力同步提升。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs能力的提升，其潜在的安全风险（如欺骗行为）日益凸显，需通过心智理论能力评估其行为是否具有隐蔽意图。

Method: 回顾心智理论研究，分析其在安全评估中的应用，并通过实验评估多个LLMs的心智理论能力发展情况。

Result: LLMs的阅读理解能力有所提升，但心智理论能力未同步发展，表明其在安全评估中仍存在潜在风险。

Conclusion: 需进一步研究LLMs的心智理论能力，以完善安全评估方法并应对未来挑战。

Abstract: As the capabilities of large language models (LLMs) continue to advance, the
importance of rigorous safety evaluation is becoming increasingly evident.
Recent concerns within the realm of safety assessment have highlighted
instances in which LLMs exhibit behaviors that appear to disable oversight
mechanisms and respond in a deceptive manner. For example, there have been
reports suggesting that, when confronted with information unfavorable to their
own persistence during task execution, LLMs may act covertly and even provide
false answers to questions intended to verify their behavior.To evaluate the
potential risk of such deceptive actions toward developers or users, it is
essential to investigate whether these behaviors stem from covert, intentional
processes within the model. In this study, we propose that it is necessary to
measure the theory of mind capabilities of LLMs. We begin by reviewing existing
research on theory of mind and identifying the perspectives and tasks relevant
to its application in safety evaluation. Given that theory of mind has been
predominantly studied within the context of developmental psychology, we
analyze developmental trends across a series of open-weight LLMs. Our results
indicate that while LLMs have improved in reading comprehension, their theory
of mind capabilities have not shown comparable development. Finally, we present
the current state of safety evaluation with respect to LLMs' theory of mind,
and discuss remaining challenges for future work.

</details>


### [59] [Cash or Comfort? How LLMs Value Your Inconvenience](https://arxiv.org/abs/2506.17367)
*Mateusz Cedro,Timour Ichmoukhamedov,Sofie Goethals,Yifan He,James Hinns,David Martens*

Main category: cs.CL

TL;DR: 研究发现当前大语言模型（LLMs）在涉及金钱与用户舒适度权衡的决策中存在不一致性和不合理性，不适合作为自主决策助手。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs在金钱与用户舒适度冲突场景中的决策行为，填补现有研究的空白。

Method: 通过量化多个LLMs对用户不适（如步行、等待、饥饿和疼痛）的定价，分析其决策行为。

Result: 发现LLMs存在响应不一致、对提示微小变化敏感、对重大不便接受极低报酬等问题。

Conclusion: 当前LLMs在权衡金钱与舒适度时表现不可靠，需进一步研究其决策机制。

Abstract: Large Language Models (LLMs) are increasingly proposed as near-autonomous
artificial intelligence (AI) agents capable of making everyday decisions on
behalf of humans. Although LLMs perform well on many technical tasks, their
behaviour in personal decision-making remains less understood. Previous studies
have assessed their rationality and moral alignment with human decisions.
However, the behaviour of AI assistants in scenarios where financial rewards
are at odds with user comfort has not yet been thoroughly explored. In this
paper, we tackle this problem by quantifying the prices assigned by multiple
LLMs to a series of user discomforts: additional walking, waiting, hunger and
pain. We uncover several key concerns that strongly question the prospect of
using current LLMs as decision-making assistants: (1) a large variance in
responses between LLMs, (2) within a single LLM, responses show fragility to
minor variations in prompt phrasing (e.g., reformulating the question in the
first person can considerably alter the decision), (3) LLMs can accept
unreasonably low rewards for major inconveniences (e.g., 1 Euro to wait 10
hours), and (4) LLMs can reject monetary gains where no discomfort is imposed
(e.g., 1,000 Euro to wait 0 minutes). These findings emphasize the need for
scrutiny of how LLMs value human inconvenience, particularly as we move toward
applications where such cash-versus-comfort trade-offs are made on users'
behalf.

</details>


### [60] [Leveraging LLMs to Assess Tutor Moves in Real-Life Dialogues: A Feasibility Study](https://arxiv.org/abs/2506.17410)
*Danielle R. Thomas,Conrad Borchers,Jionghao Lin,Sanjit Kakarla,Shambhavi Bhushan,Erin Gatz,Shivang Gupta,Ralph Abboud,Kenneth R. Koedinger*

Main category: cs.CL

TL;DR: 研究探讨了使用生成式AI（如GPT-4、Gemini-1.5-pro等）识别和评估数学辅导中导师行为的可行性，发现模型能高精度检测表扬和错误响应，并与人类判断一致。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用AI大规模分析辅导行为，以提升学生学习效果。

Method: 分析50份远程数学辅导转录文本，使用多种生成式AI模型检测导师表扬和错误响应行为。

Result: 模型在检测表扬（94-98%）和错误（82-88%）时表现良好，与人类判断一致性高（83-89%和73-77%）。

Conclusion: 生成式AI可用于大规模辅导行为评估，提供了一种经济高效的解决方案。

Abstract: Tutoring improves student achievement, but identifying and studying what
tutoring actions are most associated with student learning at scale based on
audio transcriptions is an open research problem. This present study
investigates the feasibility and scalability of using generative AI to identify
and evaluate specific tutor moves in real-life math tutoring. We analyze 50
randomly selected transcripts of college-student remote tutors assisting middle
school students in mathematics. Using GPT-4, GPT-4o, GPT-4-turbo,
Gemini-1.5-pro, and LearnLM, we assess tutors' application of two tutor skills:
delivering effective praise and responding to student math errors. All models
reliably detected relevant situations, for example, tutors providing praise to
students (94-98% accuracy) and a student making a math error (82-88% accuracy)
and effectively evaluated the tutors' adherence to tutoring best practices,
aligning closely with human judgments (83-89% and 73-77%, respectively). We
propose a cost-effective prompting strategy and discuss practical implications
for using large language models to support scalable assessment in authentic
settings. This work further contributes LLM prompts to support reproducibility
and research in AI-supported learning.

</details>


### [61] [UProp: Investigating the Uncertainty Propagation of LLMs in Multi-Step Agentic Decision-Making](https://arxiv.org/abs/2506.17419)
*Jinhao Duan,James Diffenderfer,Sandeep Madireddy,Tianlong Chen,Bhavya Kailkhura,Kaidi Xu*

Main category: cs.CL

TL;DR: 论文提出了一种信息论框架UProp，用于量化大型语言模型（LLM）在序列决策中的不确定性，包括内部和外部不确定性，并在多步决策任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 随着LLM被集成到涉及现实世界序列决策的安全关键应用中，了解何时信任LLM决策变得至关重要。现有不确定性量化方法主要针对单轮问答，多步决策场景研究不足。

Method: 提出信息论框架，将LLM序列决策不确定性分解为内部不确定性和外部不确定性（互信息量），并设计高效的外部不确定性估计器UProp，通过点互信息估计实现。

Result: UProp在多步决策基准测试（如AgentBench和HotpotQA）中显著优于现有单轮不确定性量化方法，并分析了其采样效率和应用潜力。

Conclusion: UProp为LLM序列决策中的不确定性量化提供了有效解决方案，展示了其在实际应用中的潜力。

Abstract: As Large Language Models (LLMs) are integrated into safety-critical
applications involving sequential decision-making in the real world, it is
essential to know when to trust LLM decisions. Existing LLM Uncertainty
Quantification (UQ) methods are primarily designed for single-turn
question-answering formats, resulting in multi-step decision-making scenarios,
e.g., LLM agentic system, being underexplored. In this paper, we introduce a
principled, information-theoretic framework that decomposes LLM sequential
decision uncertainty into two parts: (i) internal uncertainty intrinsic to the
current decision, which is focused on existing UQ methods, and (ii) extrinsic
uncertainty, a Mutual-Information (MI) quantity describing how much uncertainty
should be inherited from preceding decisions. We then propose UProp, an
efficient and effective extrinsic uncertainty estimator that converts the
direct estimation of MI to the estimation of Pointwise Mutual Information (PMI)
over multiple Trajectory-Dependent Decision Processes (TDPs). UProp is
evaluated over extensive multi-step decision-making benchmarks, e.g.,
AgentBench and HotpotQA, with state-of-the-art LLMs, e.g., GPT-4.1 and
DeepSeek-V3. Experimental results demonstrate that UProp significantly
outperforms existing single-turn UQ baselines equipped with thoughtful
aggregation strategies. Moreover, we provide a comprehensive analysis of UProp,
including sampling efficiency, potential applications, and intermediate
uncertainty propagation, to demonstrate its effectiveness. Codes will be
available at https://github.com/jinhaoduan/UProp.

</details>


### [62] [Beyond the Link: Assessing LLMs' ability to Classify Political Content across Global Media](https://arxiv.org/abs/2506.17435)
*Alberto Martinez-Serra,Alejandro De La Fuente,Nienke Viescher,Ana S. Cardenal*

Main category: cs.CL

TL;DR: 该研究评估了大型语言模型（LLMs）仅通过URL识别政治内容（PC）的能力，比较了多国多语言数据，发现URL能有效嵌入新闻内容，为准确性与成本的平衡提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 政治科学中LLMs的应用日益普遍，但仅通过URL分类政治内容的有效性尚未充分研究。本文旨在填补这一空白。

Method: 使用GPT、Llama等先进LLMs，对比URL与全文分析的效果，并与人工标注和传统机器学习方法比较。

Result: URL能有效嵌入新闻内容，为准确性与成本的平衡提供了新视角。

Conclusion: 研究支持URL分析在政治科学中的实用性，并提出了使用LLMs的方法建议。

Abstract: The use of large language models (LLMs) is becoming common in the context of
political science, particularly in studies that analyse individuals use of
digital media. However, while previous research has demonstrated LLMs ability
at labelling tasks, the effectiveness of using LLMs to classify political
content (PC) from just URLs is not yet well explored. The work presented in
this article bridges this gap by evaluating whether LLMs can accurately
identify PC vs. non-PC from both the article text and the URLs from five
countries (France, Germany, Spain, the UK, and the US) and different languages.
Using cutting-edge LLMs like GPT, Llama, Mistral, Deepseek, Qwen and Gemma, we
measure model performance to assess whether URL-level analysis can be a good
approximation for full-text analysis of PC, even across different linguistic
and national contexts. Model outputs are compared with human-labelled articles,
as well as traditional supervised machine learning techniques, to set a
baseline of performance. Overall, our findings suggest the capacity of URLs to
embed most of the news content, providing a vital perspective on accuracy-cost
balancing. We also account for contextual limitations and suggest
methodological recommendations to use LLMs within political science studies.

</details>


### [63] [Breaking the Transcription Bottleneck: Fine-tuning ASR Models for Extremely Low-Resource Fieldwork Languages](https://arxiv.org/abs/2506.17459)
*Siyu Liang,Gina-Anne Levow*

Main category: cs.CL

TL;DR: 论文比较了MMS和XLS-R两种多语言ASR模型在五种低资源语言上的表现，发现MMS在极少量数据时表现最佳，而XLS-R在数据超过一小时后性能相当。


<details>
  <summary>Details</summary>
Motivation: 解决语言实地调查中ASR的局限性，包括自发语音、环境噪声和低资源语言的挑战。

Method: 对MMS和XLS-R两种模型在五种低资源语言上进行微调，控制训练数据时长。

Result: MMS在极少量数据时表现更好，XLS-R在数据超过一小时后性能相当。

Conclusion: 为语言学家提供实用的ASR适应指南，缓解语言文档中的转录瓶颈。

Abstract: Automatic Speech Recognition (ASR) has reached impressive accuracy for
high-resource languages, yet its utility in linguistic fieldwork remains
limited. Recordings collected in fieldwork contexts present unique challenges,
including spontaneous speech, environmental noise, and severely constrained
datasets from under-documented languages. In this paper, we benchmark the
performance of two fine-tuned multilingual ASR models, MMS and XLS-R, on five
typologically diverse low-resource languages with control of training data
duration. Our findings show that MMS is best suited when extremely small
amounts of training data are available, whereas XLS-R shows parity performance
once training data exceed one hour. We provide linguistically grounded analysis
for further provide insights towards practical guidelines for field linguists,
highlighting reproducible ASR adaptation approaches to mitigate the
transcription bottleneck in language documentation.

</details>


### [64] [Computational Approaches to Understanding Large Language Model Impact on Writing and Information Ecosystems](https://arxiv.org/abs/2506.17467)
*Weixin Liang*

Main category: cs.CL

TL;DR: 该论文探讨了大型语言模型（LLMs）对社会的影响，包括AI检测器的偏见、LLMs在各领域的广泛应用及其在科研反馈中的潜力。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs如何改变写作、沟通和创作方式，并分析其对社会各层面的影响。

Method: 通过三个研究方向：分析AI检测器的偏见、测量LLMs在各领域的应用、评估LLMs在科研反馈中的效果。

Result: 发现AI检测器存在系统性偏见，LLMs在各领域广泛应用，且能为科研提供有效反馈。

Conclusion: LLMs对社会影响深远，需关注其公平性和应用潜力。

Abstract: Large language models (LLMs) have shown significant potential to change how
we write, communicate, and create, leading to rapid adoption across society.
This dissertation examines how individuals and institutions are adapting to and
engaging with this emerging technology through three research directions.
First, I demonstrate how the institutional adoption of AI detectors introduces
systematic biases, particularly disadvantaging writers of non-dominant language
varieties, highlighting critical equity concerns in AI governance. Second, I
present novel population-level algorithmic approaches that measure the
increasing adoption of LLMs across writing domains, revealing consistent
patterns of AI-assisted content in academic peer reviews, scientific
publications, consumer complaints, corporate communications, job postings, and
international organization press releases. Finally, I investigate LLMs'
capability to provide feedback on research manuscripts through a large-scale
empirical analysis, offering insights into their potential to support
researchers who face barriers in accessing timely manuscript feedback,
particularly early-career researchers and those from under-resourced settings.

</details>


### [65] [VeriLocc: End-to-End Cross-Architecture Register Allocation via LLM](https://arxiv.org/abs/2506.17506)
*Lesheng Jin,Zhenyuan Ruan,Haohui Mai,Jingbo Shang*

Main category: cs.CL

TL;DR: VeriLocc结合大型语言模型和形式化编译器技术，实现跨GPU架构的可泛化和可验证的寄存器分配，性能优于人工调优库。


<details>
  <summary>Details</summary>
Motivation: 现代GPU发展迅速，但生产编译器仍依赖手工调优的寄存器分配启发式方法，需为每代硬件重新调整。

Method: VeriLocc通过微调LLM将中间表示转换为目标特定的寄存器分配，结合静态分析和验证器引导的再生循环确保正确性。

Result: 在GEMM和MHA任务中，VeriLocc单次准确率达85-99%，pass@100接近100%，性能优于rocBLAS 10%以上。

Conclusion: VeriLocc提供了一种高效、可验证的寄存器分配方法，适用于不同GPU架构。

Abstract: Modern GPUs evolve rapidly, yet production compilers still rely on
hand-crafted register allocation heuristics that require substantial re-tuning
for each hardware generation. We introduce VeriLocc, a framework that combines
large language models (LLMs) with formal compiler techniques to enable
generalizable and verifiable register allocation across GPU architectures.
VeriLocc fine-tunes an LLM to translate intermediate representations (MIRs)
into target-specific register assignments, aided by static analysis for
cross-architecture normalization and generalization and a verifier-guided
regeneration loop to ensure correctness. Evaluated on matrix multiplication
(GEMM) and multi-head attention (MHA), VeriLocc achieves 85-99% single-shot
accuracy and near-100% pass@100. Case study shows that VeriLocc discovers more
performant assignments than expert-tuned libraries, outperforming rocBLAS by
over 10% in runtime.

</details>


### [66] [Data Quality Issues in Multilingual Speech Datasets: The Need for Sociolinguistic Awareness and Proactive Language Planning](https://arxiv.org/abs/2506.17525)
*Mingfei Lau,Qian Chen,Yeming Fang,Tingting Xu,Tongzhou Chen,Pavel Golik*

Main category: cs.CL

TL;DR: 对三个多语言语音数据集（Mozilla Common Voice 17.0、FLEURS和VoxPopuli）的质量审计显示，某些语言存在显著质量问题，分为微观和宏观两类，宏观问题在资源不足语言中更常见。


<details>
  <summary>Details</summary>
Motivation: 揭示数据集质量问题以提升其作为训练和评估集的实用性，并改进下游模型。

Method: 通过质量审计将问题分为微观和宏观两类，并以台湾闽南语为例进行案例分析。

Result: 发现宏观问题在资源不足语言中更普遍，需加强语言规划和数据质量控制。

Conclusion: 提出未来数据集开发的指南和建议，强调社会语言学意识的重要性。

Abstract: Our quality audit for three widely used public multilingual speech datasets -
Mozilla Common Voice 17.0, FLEURS, and VoxPopuli - shows that in some
languages, these datasets suffer from significant quality issues. We believe
addressing these issues will make these datasets more useful as training and
evaluation sets, and improve downstream models. We divide these quality issues
into two categories: micro-level and macro-level. We find that macro-level
issues are more prevalent in less institutionalized, often under-resourced
languages. We provide a case analysis of Taiwanese Southern Min (nan_tw) that
highlights the need for proactive language planning (e.g. orthography
prescriptions, dialect boundary definition) and enhanced data quality control
in the process of Automatic Speech Recognition (ASR) dataset creation. We
conclude by proposing guidelines and recommendations to mitigate these issues
in future dataset development, emphasizing the importance of sociolinguistic
awareness in creating robust and reliable speech data resources.

</details>


### [67] [DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for Mathematical Reasoning](https://arxiv.org/abs/2506.17533)
*Yuanhao Wu,Juntong Song,Hanning Zhang,Tong Zhang,Cheng Niu*

Main category: cs.CL

TL;DR: DuaShepherd是一个新颖的奖励建模框架，结合正确性和潜力两种奖励信号，提升大语言模型的数学推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有奖励模型通常仅关注正确性，忽略了潜力信号对推理过程的潜在帮助。

Method: 开发自动化管道构建大规模奖励建模数据集，采用多任务学习的多头部架构并行训练两种奖励模型。

Result: 在MATH500和ProcessBench上，结合两种奖励信号的模型显著优于单一奖励模型，达到同类资源下的最佳性能。

Conclusion: 正确性和潜力信号的结合能有效提升数学推理能力，为奖励建模提供了新思路。

Abstract: In this paper, we propose DuaShepherd, a novel reward modeling framework that
integrates two complementary reward signals, correctness and potential, to
enhance the mathematical reasoning capabilities of Large Language Models
(LLMs). While correctness-based signals emphasize identification of stepwise
errors, potential-based signals focus on the likelihood of reaching the correct
final answer. We developed an automated pipeline for constructing large-scale
reward modeling dataset with both signals. A unified, multi-head architecture
was explored to train the two reward models in a multi-task setup,
demonstrating benefits from learning both correctness and potential in
parallel. By combining these two signals into a compound probability, our model
achieves consistent performance improvements across multiple benchmarks.
Empirical evaluations on MATH500 and ProcessBench confirm that this combined
reward significantly outperforms models trained on either reward type alone,
achieving state-of-the-art performance under comparable resource constraints.

</details>


### [68] [Probing for Phonology in Self-Supervised Speech Representations: A Case Study on Accent Perception](https://arxiv.org/abs/2506.17542)
*Nitin Venkateswaran,Kevin Tang,Ratree Wayland*

Main category: cs.CL

TL;DR: 研究探讨了自监督学习（SSL）语音模型如何编码影响口音感知的音系特征变化，发现特定音段的自监督表示特征最能预测口音强度。


<details>
  <summary>Details</summary>
Motivation: 传统口音感知模型低估了音系特征渐变变化的作用，本研究旨在探索SSL模型如何捕捉这些变化以改进口音感知建模。

Method: 使用CSLU外国口音英语语料库，提取音系特征概率和自监督表示（Wav2Vec2-BERT和WavLM），结合母语者口音评分进行分析。

Result: 分析表明，口音强度由音段自监督表示特征的子集最佳预测，且与音系特征的感知显著性相关。

Conclusion: 自监督语音表示可用于基于可解释音系特征的口音感知建模，具有重要价值。

Abstract: Traditional models of accent perception underestimate the role of gradient
variations in phonological features which listeners rely upon for their accent
judgments. We investigate how pretrained representations from current
self-supervised learning (SSL) models of speech encode phonological
feature-level variations that influence the perception of segmental accent. We
focus on three segments: the labiodental approximant, the rhotic tap, and the
retroflex stop, which are uniformly produced in the English of native speakers
of Hindi as well as other languages in the Indian sub-continent. We use the
CSLU Foreign Accented English corpus (Lander, 2007) to extract, for these
segments, phonological feature probabilities using Phonet (V\'asquez-Correa et
al., 2019) and pretrained representations from Wav2Vec2-BERT (Barrault et al.,
2023) and WavLM (Chen et al., 2022) along with accent judgements by native
speakers of American English. Probing analyses show that accent strength is
best predicted by a subset of the segment's pretrained representation features,
in which perceptually salient phonological features that contrast the expected
American English and realized non-native English segments are given prominent
weighting. A multinomial logistic regression of pretrained representation-based
segment distances from American and Indian English baselines on accent ratings
reveals strong associations between the odds of accent strength and distances
from the baselines, in the expected directions. These results highlight the
value of self-supervised speech representations for modeling accent perception
using interpretable phonological features.

</details>


### [69] [AgriCHN: A Comprehensive Cross-domain Resource for Chinese Agricultural Named Entity Recognition](https://arxiv.org/abs/2506.17578)
*Lingxiao Zeng,Yiqi Tong,Wei Guo,Huarui Wu,Lihao Ge,Yijun Ye,Fuzhen Zhuang,Deqing Wang,Wei Guo,Cheng Chen*

Main category: cs.CL

TL;DR: AgriCHN是一个高质量的中文农业命名实体识别数据集，涵盖了农业、水文和气象领域的27种实体类别，旨在提升农业实体标注的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏高质量的中文农业数据集，且现有方法忽视了农业与水文、气象的关联，AgriCHN填补了这一空白。

Method: 数据集从大量农业文章中精心筛选，包含4,040个句子和15,799个实体标注，涵盖27个类别。

Result: AgriCHN数据质量高，实体类型丰富且划分细致，实验证明其具有挑战性和研究潜力。

Conclusion: AgriCHN为农业命名实体识别提供了高质量资源，并推动了相关领域的进一步研究。

Abstract: Agricultural named entity recognition is a specialized task focusing on
identifying distinct agricultural entities within vast bodies of text,
including crops, diseases, pests, and fertilizers. It plays a crucial role in
enhancing information extraction from extensive agricultural text resources.
However, the scarcity of high-quality agricultural datasets, particularly in
Chinese, has resulted in suboptimal performance when employing mainstream
methods for this purpose. Most earlier works only focus on annotating
agricultural entities while overlook the profound correlation of agriculture
with hydrology and meteorology. To fill this blank, we present AgriCHN, a
comprehensive open-source Chinese resource designed to promote the accuracy of
automated agricultural entity annotation. The AgriCHN dataset has been
meticulously curated from a wealth of agricultural articles, comprising a total
of 4,040 sentences and encapsulating 15,799 agricultural entity mentions
spanning 27 diverse entity categories. Furthermore, it encompasses entities
from hydrology to meteorology, thereby enriching the diversity of entities
considered. Data validation reveals that, compared with relevant resources,
AgriCHN demonstrates outstanding data quality, attributable to its richer
agricultural entity types and more fine-grained entity divisions. A benchmark
task has also been constructed using several state-of-the-art neural NER
models. Extensive experimental results highlight the significant challenge
posed by AgriCHN and its potential for further research.

</details>


### [70] [Mind the Gap: Assessing Wiktionary's Crowd-Sourced Linguistic Knowledge on Morphological Gaps in Two Related Languages](https://arxiv.org/abs/2506.17603)
*Jonathan Sakunkoo,Annabella Sakunkoo*

Main category: cs.CL

TL;DR: 论文研究形态缺陷现象，通过神经网络分析拉丁语和意大利语语料，验证维基词典的缺陷动词列表可靠性。


<details>
  <summary>Details</summary>
Motivation: 形态缺陷现象在语言学中研究不足，影响NLP工具在形态丰富语言中的准确性。维基资源虽广泛但可靠性存疑。

Method: 定制神经网络形态分析器，标注拉丁语和意大利语语料，验证维基词典的缺陷动词列表。

Result: 维基词典对意大利语形态缺陷描述可靠，但7%拉丁语词条被误标为非缺陷。

Conclusion: 维基资源对罕见语言现象和语言的知识有限，需工具和方法提升其质量。

Abstract: Morphological defectivity is an intriguing and understudied phenomenon in
linguistics. Addressing defectivity, where expected inflectional forms are
absent, is essential for improving the accuracy of NLP tools in morphologically
rich languages. However, traditional linguistic resources often lack coverage
of morphological gaps as such knowledge requires significant human expertise
and effort to document and verify. For scarce linguistic phenomena in
under-explored languages, Wikipedia and Wiktionary often serve as among the few
accessible resources. Despite their extensive reach, their reliability has been
a subject of controversy. This study customizes a novel neural morphological
analyzer to annotate Latin and Italian corpora. Using the massive annotated
data, crowd-sourced lists of defective verbs compiled from Wiktionary are
validated computationally. Our results indicate that while Wiktionary provides
a highly reliable account of Italian morphological gaps, 7% of Latin lemmata
listed as defective show strong corpus evidence of being non-defective. This
discrepancy highlights potential limitations of crowd-sourced wikis as
definitive sources of linguistic knowledge, particularly for less-studied
phenomena and languages, despite their value as resources for rare linguistic
features. By providing scalable tools and methods for quality assurance of
crowd-sourced data, this work advances computational morphology and expands
linguistic knowledge of defectivity in non-English, morphologically rich
languages.

</details>


### [71] [TyphoFormer: Language-Augmented Transformer for Accurate Typhoon Track Forecasting](https://arxiv.org/abs/2506.17609)
*Lincan Li,Eren Erman Ozguven,Yue Zhao,Guang Wang,Yiqun Xie,Yushun Dong*

Main category: cs.CL

TL;DR: TyphoFormer利用自然语言描述作为辅助提示，通过结合文本和序列信息提升台风轨迹预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统Transformer模型在稀疏气象轨迹（如台风路径）预测中缺乏上下文知识，TyphoFormer旨在通过语言描述弥补这一不足。

Method: TyphoFormer利用LLM生成台风数值属性的文本描述，并将其嵌入为特殊标记与时间序列输入结合，通过统一Transformer编码器整合信息。

Result: 在HURDAT2基准测试中，TyphoFormer表现优于现有方法，尤其在非线性路径变化和历史数据有限的情况下。

Conclusion: TyphoFormer通过结合语言和数值信息，显著提升了台风轨迹预测的可靠性。

Abstract: Accurate typhoon track forecasting is crucial for early system warning and
disaster response. While Transformer-based models have demonstrated strong
performance in modeling the temporal dynamics of dense trajectories of humans
and vehicles in smart cities, they usually lack access to broader contextual
knowledge that enhances the forecasting reliability of sparse meteorological
trajectories, such as typhoon tracks. To address this challenge, we propose
TyphoFormer, a novel framework that incorporates natural language descriptions
as auxiliary prompts to improve typhoon trajectory forecasting. For each time
step, we use Large Language Model (LLM) to generate concise textual
descriptions based on the numerical attributes recorded in the North Atlantic
hurricane database. The language descriptions capture high-level meteorological
semantics and are embedded as auxiliary special tokens prepended to the
numerical time series input. By integrating both textual and sequential
information within a unified Transformer encoder, TyphoFormer enables the model
to leverage contextual cues that are otherwise inaccessible through numerical
features alone. Extensive experiments are conducted on HURDAT2 benchmark,
results show that TyphoFormer consistently outperforms other state-of-the-art
baseline methods, particularly under challenging scenarios involving nonlinear
path shifts and limited historical observations.

</details>


### [72] [OpusLM: A Family of Open Unified Speech Language Models](https://arxiv.org/abs/2506.17611)
*Jinchuan Tian,William Chen,Yifan Peng,Jiatong Shi,Siddhant Arora,Shikhar Bharadwaj,Takashi Maekaku,Yusuke Shinohara,Keita Goto,Xiang Yue,Huck Yang,Shinji Watanabe*

Main category: cs.CL

TL;DR: OpusLMs是一系列基于公开材料的开放语音语言模型，规模达7B，通过多阶段训练策略，在语音识别、合成及文本任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 推动开放语音语言模型的研究，提供透明、可复现的模型。

Method: 从文本语言模型初始化，通过213K小时语音-文本对和292B文本令牌进行预训练，采用多流语言模型和多阶段训练策略。

Result: 在语音识别、合成及文本任务中表现优于现有模型。

Conclusion: OpusLMs为开放语音语言模型研究提供了重要资源，展示了模型规模和数据选择的重要性。

Abstract: This paper presents Open Unified Speech Language Models (OpusLMs), a family
of open foundational speech language models (SpeechLMs) up to 7B. Initialized
from decoder-only text language models, the OpusLMs are continuously
pre-trained on 213K hours of speech-text pairs and 292B text-only tokens. We
demonstrate our OpusLMs achieve comparable (or even superior) performance with
existing SpeechLMs in speech recognition, speech synthesis, and text-only
capabilities. Technically, this paper articulates our SpeechLM designs on
tokenization, multi-stream language models, and multi-stage training
strategies. We experimentally demonstrate the importance of model size scaling
and the effect of annealing data selection. The OpusLMs are all built from
publicly available materials and are fully transparent models. We release our
code, data, checkpoints, and training logs to facilitate open SpeechLM research

</details>


### [73] [Answer-Centric or Reasoning-Driven? Uncovering the Latent Memory Anchor in LLMs](https://arxiv.org/abs/2506.17630)
*Yang Wu,Yifan Zhang,Yiwei Wang,Yujun Cai,Yurong Wu,Yuran Wang,Ning Xu,Jian Cheng*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLMs）的推理能力主要依赖于显式答案而非真实推理，通过五级答案可见性提示框架验证了其答案锚定现象。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs的推理能力是否基于真实推断，还是仅依赖于记忆的答案-推理模式。

Method: 提出五级答案可见性提示框架，系统性地操纵答案线索并通过间接行为分析模型行为。

Result: 实验表明LLMs对显式答案有强烈依赖，答案线索被掩盖时性能下降26.90%。

Conclusion: LLMs的推理可能更多是事后合理化而非真实推断，需更深入理解其推理本质。

Abstract: While Large Language Models (LLMs) demonstrate impressive reasoning
capabilities, growing evidence suggests much of their success stems from
memorized answer-reasoning patterns rather than genuine inference. In this
work, we investigate a central question: are LLMs primarily anchored to final
answers or to the textual pattern of reasoning chains? We propose a five-level
answer-visibility prompt framework that systematically manipulates answer cues
and probes model behavior through indirect, behavioral analysis. Experiments
across state-of-the-art LLMs reveal a strong and consistent reliance on
explicit answers. The performance drops by 26.90\% when answer cues are masked,
even with complete reasoning chains. These findings suggest that much of the
reasoning exhibited by LLMs may reflect post-hoc rationalization rather than
true inference, calling into question their inferential depth. Our study
uncovers the answer-anchoring phenomenon with rigorous empirical validation and
underscores the need for a more nuanced understanding of what constitutes
reasoning in LLMs.

</details>


### [74] [Step-Opt: Boosting Optimization Modeling in LLMs through Iterative Data Synthesis and Structured Validation](https://arxiv.org/abs/2506.17637)
*Yang Wu,Yifan Zhang,Yurong Wu,Yuran Wang,Junkai Zhang,Jian Cheng*

Main category: cs.CL

TL;DR: Step-Opt-Instruct框架通过迭代生成和验证优化建模数据，显著提升LLMs在复杂OR任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在复杂OR优化建模任务中的性能不足问题。

Method: 提出Step-Opt-Instruct框架，包括迭代问题生成和逐步验证，用于生成高质量微调数据。

Result: 微调后的Step-Opt模型在NL4OPT等基准测试中表现优异，复杂任务准确率提升17.01%。

Conclusion: 结合结构化验证和渐进问题优化，可有效提升LLMs在OR任务中的自动化决策能力。

Abstract: Large Language Models (LLMs) have revolutionized various domains but
encounter substantial challenges in tackling optimization modeling tasks for
Operations Research (OR), particularly when dealing with complex problem. In
this work, we propose Step-Opt-Instruct, a framework that augments existing
datasets and generates high-quality fine-tuning data tailored to optimization
modeling. Step-Opt-Instruct employs iterative problem generation to
systematically increase problem complexity and stepwise validation to
rigorously verify data, preventing error propagation and ensuring the quality
of the generated dataset. Leveraging this framework, we fine-tune open-source
LLMs, including LLaMA-3-8B and Mistral-7B, to develop Step-Opt--a model that
achieves state-of-the-art performance on benchmarks such as NL4OPT, MAMO, and
IndustryOR. Extensive experiments demonstrate the superior performance of
Step-Opt, especially in addressing complex OR tasks, with a notable 17.01\%
improvement in micro average accuracy on difficult problems. These findings
highlight the effectiveness of combining structured validation with gradual
problem refinement to advance the automation of decision-making processes using
LLMs.The code and dataset are available at https://github.com/samwu-learn/Step.

</details>


### [75] [TPTT: Transforming Pretrained Transformer into Titans](https://arxiv.org/abs/2506.17671)
*Fabien Furfaro*

Main category: cs.CL

TL;DR: TPTT框架通过线性化注意力机制和高级内存管理提升预训练Transformer模型的效率，显著提高长上下文推理的性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在长上下文推理中的计算和内存需求问题。

Method: 采用Memory as Gate (MaG)和混合线性化注意力(LiZA)技术，支持参数高效微调(LoRA)。

Result: 在MMLU基准测试中，1B参数模型效率与准确性显著提升，如Titans-Llama-3.2-1B的Exact Match提高20%。

Conclusion: TPTT具有实际可扩展性和鲁棒性，代码和Python包已开源。

Abstract: Recent advances in large language models (LLMs) have led to remarkable
progress in natural language processing, but their computational and memory
demands remain a significant challenge, particularly for long-context
inference. We introduce TPTT (Transforming Pretrained Transformer into Titans),
a novel framework for enhancing pretrained Transformer models with efficient
linearized attention mechanisms and advanced memory management. TPTT employs
techniques such as Memory as Gate (MaG) and mixed linearized attention (LiZA).
It is fully compatible with the Hugging Face Transformers library, enabling
seamless adaptation of any causal LLM through parameter-efficient fine-tuning
(LoRA) without full retraining. We show the effectiveness of TPTT on the MMLU
benchmark with models of approximately 1 billion parameters, observing
substantial improvements in both efficiency and accuracy. For instance,
Titans-Llama-3.2-1B achieves a 20% increase in Exact Match (EM) over its
baseline. Statistical analyses and comparisons with recent state-of-the-art
methods confirm the practical scalability and robustness of TPTT. Code is
available at https://github.com/fabienfrfr/tptt . Python package at
https://pypi.org/project/tptt/ .

</details>


### [76] [Resource-Friendly Dynamic Enhancement Chain for Multi-Hop Question Answering](https://arxiv.org/abs/2506.17692)
*Binquan Ji,Haibo Luo,Yifei Lu,Lei Hei,Jiaqi Wang,Tingjing Liao,Lingyu Wang,Shichao Wang,Feiliang Ren*

Main category: cs.CL

TL;DR: DEC框架通过分解复杂问题为子问题并迭代优化，结合轻量级关键词提取模块，显著减少计算开销，在多跳QA任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决轻量级LLM在多跳QA任务中因文档和上下文过多导致的幻觉和语义漂移问题。

Method: 提出DEC框架，分解问题为子问题，迭代优化查询，并引入轻量级关键词提取模块。

Result: 在三个多跳QA数据集上表现优异，显著减少token消耗，8B参数模型达到SOTA。

Conclusion: DEC在资源受限环境中高效，适用于多跳QA任务。

Abstract: Knowledge-intensive multi-hop question answering (QA) tasks, which require
integrating evidence from multiple sources to address complex queries, often
necessitate multiple rounds of retrieval and iterative generation by large
language models (LLMs). However, incorporating many documents and extended
contexts poses challenges -such as hallucinations and semantic drift-for
lightweight LLMs with fewer parameters. This work proposes a novel framework
called DEC (Dynamic Enhancement Chain). DEC first decomposes complex questions
into logically coherent subquestions to form a hallucination-free reasoning
chain. It then iteratively refines these subquestions through context-aware
rewriting to generate effective query formulations. For retrieval, we introduce
a lightweight discriminative keyword extraction module that leverages extracted
keywords to achieve targeted, precise document recall with relatively low
computational overhead. Extensive experiments on three multi-hop QA datasets
demonstrate that DEC performs on par with or surpasses state-of-the-art
benchmarks while significantly reducing token consumption. Notably, our
approach attains state-of-the-art results on models with 8B parameters,
showcasing its effectiveness in various scenarios, particularly in
resource-constrained environments.

</details>


### [77] [Zero-Shot Conversational Stance Detection: Dataset and Approaches](https://arxiv.org/abs/2506.17693)
*Yuzhe Ding,Kang He,Bobo Li,Li Zheng,Haijun He,Fei Li,Chong Teng,Donghong Ji*

Main category: cs.CL

TL;DR: 论文提出了一种零样本对话立场检测数据集ZS-CSD和模型SITPCL，解决了现有数据集目标受限的问题，并在零样本设置下取得了先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有对话立场检测数据集目标范围有限，限制了模型在真实场景中的应用，因此需要构建更通用的数据集和模型。

Method: 手动构建了大规模零样本对话立场检测数据集ZS-CSD，并提出基于说话者交互和目标感知的原型对比学习模型SITPCL。

Result: SITPCL在零样本对话立场检测中达到先进性能，F1-macro得分为43.81%。

Conclusion: 尽管SITPCL表现优异，零样本对话立场检测仍面临挑战，需进一步研究。

Abstract: Stance detection, which aims to identify public opinion towards specific
targets using social media data, is an important yet challenging task. With the
increasing number of online debates among social media users, conversational
stance detection has become a crucial research area. However, existing
conversational stance detection datasets are restricted to a limited set of
specific targets, which constrains the effectiveness of stance detection models
when encountering a large number of unseen targets in real-world applications.
To bridge this gap, we manually curate a large-scale, high-quality zero-shot
conversational stance detection dataset, named ZS-CSD, comprising 280 targets
across two distinct target types. Leveraging the ZS-CSD dataset, we propose
SITPCL, a speaker interaction and target-aware prototypical contrastive
learning model, and establish the benchmark performance in the zero-shot
setting. Experimental results demonstrate that our proposed SITPCL model
achieves state-of-the-art performance in zero-shot conversational stance
detection. Notably, the SITPCL model attains only an F1-macro score of 43.81%,
highlighting the persistent challenges in zero-shot conversational stance
detection.

</details>


### [78] [The Evolution of Natural Language Processing: How Prompt Optimization and Language Models are Shaping the Future](https://arxiv.org/abs/2506.17700)
*Summra Saleem,Muhammad Nabeel Asim,Shaista Zulfiqar,Andreas Dengel*

Main category: cs.CL

TL;DR: 本文综述了提示优化策略在大型语言模型（LLMs）中的应用，填补了现有研究的空白，并提出了11种分类方法，为未来研究提供了基础。


<details>
  <summary>Details</summary>
Motivation: 当前研究缺乏对提示优化策略的全面分析，本文旨在填补这一空白，为LLM在NLP任务中的应用提供更高效的优化方法。

Method: 通过分析提示优化策略的工作原理，将其分为11类，并详细介绍了其在各种NLP任务中的应用以及使用的LLM和基准数据集。

Result: 研究提供了对提示优化策略的全面分类和应用分析，为未来研究提供了统一的实验基础和评估框架。

Conclusion: 本文为提示优化策略的研究奠定了基础，有助于未来开发更高效的LLM预测工具，并推动未探索任务中的创新应用。

Abstract: Large Language Models (LLMs) have revolutionized the field of Natural
Language Processing (NLP) by automating traditional labor-intensive tasks and
consequently accelerated the development of computer-aided applications. As
researchers continue to advance this field with the introduction of novel
language models and more efficient training/finetuning methodologies, the idea
of prompt engineering and subsequent optimization strategies with LLMs has
emerged as a particularly impactful trend to yield a substantial performance
boost across diverse NLP tasks. To best of our knowledge numerous review
articles have explored prompt engineering, however, a critical gap exists in
comprehensive analyses of prompt optimization strategies. To bridge this gap
this paper provides unique and comprehensive insights about the potential of
diverse prompt optimization strategies. It analyzes their underlying working
paradigms and based on these principles, categorizes them into 11 distinct
classes. Moreover, the paper provides details about various NLP tasks where
these prompt optimization strategies have been employed, along with details of
different LLMs and benchmark datasets used for evaluation. This comprehensive
compilation lays a robust foundation for future comparative studies and enables
rigorous assessment of prompt optimization and LLM-based predictive pipelines
under consistent experimental settings: a critical need in the current
landscape. Ultimately, this research will centralize diverse strategic
knowledge to facilitate the adaptation of existing prompt optimization
strategies for development of innovative predictors across unexplored tasks.

</details>


### [79] [Aged to Perfection: Machine-Learning Maps of Age in Conversational English](https://arxiv.org/abs/2506.17708)
*MingZe Tang*

Main category: cs.CL

TL;DR: 研究利用英国国家语料库2014分析不同年龄组的语言模式，探索人口统计与语言特征的关系，并开发预测年龄组的模型。


<details>
  <summary>Details</summary>
Motivation: 探索不同年龄组的语言模式差异，了解人口统计与语言特征（如话语时长、词汇多样性等）的联系。

Method: 结合计算语言分析和机器学习方法，识别代际语言特征并构建预测模型。

Result: 揭示了现代英国口语中的代际语言特征，并开发了预测年龄组的模型。

Conclusion: 研究增进了对现代英国口语社会语言多样性的理解。

Abstract: The study uses the British National Corpus 2014, a large sample of
contemporary spoken British English, to investigate language patterns across
different age groups. Our research attempts to explore how language patterns
vary between different age groups, exploring the connection between speaker
demographics and linguistic factors such as utterance duration, lexical
diversity, and word choice. By merging computational language analysis and
machine learning methodologies, we attempt to uncover distinctive linguistic
markers characteristic of multiple generations and create prediction models
that can consistently estimate the speaker's age group from various aspects.
This work contributes to our knowledge of sociolinguistic diversity throughout
the life of modern British speech.

</details>


### [80] [Unveiling Factors for Enhanced POS Tagging: A Study of Low-Resource Medieval Romance Languages](https://arxiv.org/abs/2506.17715)
*Matthias Schöffel,Esteban Garces Arias,Marinus Wiedner,Paula Ruppert,Meimingwei Li,Christian Heumann,Matthias Aßenmacher*

Main category: cs.CL

TL;DR: 该论文研究了在中世纪罗曼语（如奥克西唐语、西班牙语和法语）中，现代大语言模型（LLMs）在词性标注任务中的表现，探讨了其面临的独特挑战及解决方案。


<details>
  <summary>Details</summary>
Motivation: 中世纪罗曼语的词性标注面临历时语言演变、拼写变体和标注数据稀缺等挑战，需要系统研究以提升标注性能。

Method: 通过实验评估微调方法、提示工程、模型架构、解码策略和跨语言迁移学习对标注准确性的影响。

Result: 研究发现LLMs在处理历史语言变体和非标准化拼写时存在显著限制，但也提出了有效的专门技术。

Conclusion: 针对低资源历史语言，需采用专门技术以应对其独特挑战，提升词性标注性能。

Abstract: Part-of-speech (POS) tagging remains a foundational component in natural
language processing pipelines, particularly critical for historical text
analysis at the intersection of computational linguistics and digital
humanities. Despite significant advancements in modern large language models
(LLMs) for ancient languages, their application to Medieval Romance languages
presents distinctive challenges stemming from diachronic linguistic evolution,
spelling variations, and labeled data scarcity. This study systematically
investigates the central determinants of POS tagging performance across diverse
corpora of Medieval Occitan, Medieval Spanish, and Medieval French texts,
spanning biblical, hagiographical, medical, and dietary domains. Through
rigorous experimentation, we evaluate how fine-tuning approaches, prompt
engineering, model architectures, decoding strategies, and cross-lingual
transfer learning techniques affect tagging accuracy. Our results reveal both
notable limitations in LLMs' ability to process historical language variations
and non-standardized spelling, as well as promising specialized techniques that
effectively address the unique challenges presented by low-resource historical
languages.

</details>


### [81] [KAG-Thinker: Teaching Large Language Models to Think with Human-like Reasoning Process](https://arxiv.org/abs/2506.17728)
*Dalong Zhang,Jun Xu,Jun Zhou,Lei Liang,Lin Yuan,Ling Zhong,Mengshu Sun,Peilong Zhao,QiWei Wang,Xiaorui Wang,Xinkai Du,YangYang Hou,Yu Ao,ZhaoYang Wang,Zhengke Gui,ZhiYing Yi,Zhongpu Bo*

Main category: cs.CL

TL;DR: KAG-Thinker是一个基于轻量级大语言模型（LLM）的人机推理框架，通过结构化思维过程提升问答任务中的逻辑一致性和上下文连贯性。


<details>
  <summary>Details</summary>
Motivation: 提升LLM在特定领域知识库问答任务中的逻辑推理能力，模拟人类认知机制处理复杂问题。

Method: 1. 广度分解将复杂问题拆分为独立子问题（逻辑形式）；2. 使用知识边界和深度解决模型优化知识检索；3. 通过监督微调而非强化学习对齐结构化推理范式。

Result: 框架增强了知识检索和推理分析的逻辑一致性，并通过数据评估框架生成详细推理轨迹。

Conclusion: KAG-Thinker通过结构化思维和优化检索机制，显著提升了LLM在复杂问答任务中的表现。

Abstract: In this paper, we introduce KAG-Thinker, a novel human-like reasoning
framework built upon a parameter-light large language model (LLM). Our approach
enhances the logical coherence and contextual consistency of the thinking
process in question-answering (Q\&A) tasks on domain-specific knowledge bases
(KBs) within LLMs. This framework simulates human cognitive mechanisms for
handling complex problems by establishing a structured thinking process.
Continuing the \textbf{Logical Form} guided retrieval and reasoning technology
route of KAG v0.7, firstly, it decomposes complex questions into independently
solvable sub-problems(also referred to as logical forms) through
\textbf{breadth decomposition}, each represented in two equivalent
forms-natural language and logical function-and further classified as either
Knowledge Retrieval or Reasoning Analysis tasks, with dependencies and
variables passing explicitly modeled via logical function interfaces. In the
solving process, the Retrieval function is used to perform knowledge retrieval
tasks, while the Math and Deduce functions are used to perform reasoning
analysis tasks. Secondly, it is worth noting that, in the Knowledge Retrieval
sub-problem tasks, LLMs and external knowledge sources are regarded as
equivalent KBs. We use the \textbf{knowledge boundary} model to determine the
optimal source using self-regulatory mechanisms such as confidence calibration
and reflective reasoning, and use the \textbf{depth solving} model to enhance
the comprehensiveness of knowledge acquisition. Finally, instead of utilizing
reinforcement learning, we employ supervised fine-tuning with multi-turn
dialogues to align the model with our structured inference paradigm, thereby
avoiding excessive reflection. This is supported by a data evaluation framework
and iterative corpus synthesis, which facilitate the generation of detailed
reasoning trajectories...

</details>


### [82] [HIDE and Seek: Detecting Hallucinations in Language Models via Decoupled Representations](https://arxiv.org/abs/2506.17748)
*Anwoy Chatterjee,Yash Goel,Tanmoy Chakraborty*

Main category: cs.CL

TL;DR: 论文提出了一种名为HIDE的单次训练无关方法，通过解耦表示检测语言模型的幻觉内容，显著优于其他单次方法，并与多次方法竞争。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型生成的幻觉内容难以检测且计算成本高，需要一种高效的单次检测方法。

Method: 利用Hilbert-Schmidt独立性准则（HSIC）量化模型内部表示与生成输出的解耦程度。

Result: HIDE在四个问答数据集上表现优异，AUC-ROC平均提升29%，计算时间减少51%。

Conclusion: 解耦表示是高效检测语言模型幻觉的有效方法。

Abstract: Contemporary Language Models (LMs), while impressively fluent, often generate
content that is factually incorrect or unfaithful to the input context - a
critical issue commonly referred to as 'hallucination'. This tendency of LMs to
generate hallucinated content undermines their reliability, especially because
these fabrications are often highly convincing and therefore difficult to
detect. While several existing methods attempt to detect hallucinations, most
rely on analyzing multiple generations per input, leading to increased
computational cost and latency. To address this, we propose a single-pass,
training-free approach for effective Hallucination detectIon via Decoupled
rEpresentations (HIDE). Our approach leverages the hypothesis that
hallucinations result from a statistical decoupling between an LM's internal
representations of input context and its generated output. We quantify this
decoupling using the Hilbert-Schmidt Independence Criterion (HSIC) applied to
hidden-state representations extracted while generating the output sequence. We
conduct extensive experiments on four diverse question answering datasets,
evaluating both faithfulness and factuality hallucinations across six
open-source LMs of varying scales and properties. Our results demonstrate that
HIDE outperforms other single-pass methods in almost all settings, achieving an
average relative improvement of ~29% in AUC-ROC over the best-performing
single-pass strategy across various models and datasets. Additionally, HIDE
shows competitive and often superior performance with multi-pass
state-of-the-art methods, obtaining an average relative improvement of ~3% in
AUC-ROC while consuming ~51% less computation time. Our findings highlight the
effectiveness of exploiting internal representation decoupling in LMs for
efficient and practical hallucination detection.

</details>


### [83] [Multilingual Tokenization through the Lens of Indian Languages: Challenges and Insights](https://arxiv.org/abs/2506.17789)
*N J Karthika,Maharaj Brahma,Rohit Saluja,Ganesh Ramakrishnan,Maunendra Sankar Desarkar*

Main category: cs.CL

TL;DR: 本文评估了17种印度语言的标记化策略，比较了BPE和Unigram LM算法、词汇量大小的影响，以及多语言词汇构建方法，发现低资源语言可从相关高资源语言的标记化中受益。


<details>
  <summary>Details</summary>
Motivation: 现有标记化方法偏向高资源语言，限制了其在印度次大陆等语言多样性和形态丰富语言中的有效性。

Method: 对17种印度语言进行标记化策略的综合评估，比较了BPE和Unigram LM算法、词汇量大小的影响，以及多语言词汇构建方法。

Result: 研究发现低资源语言可从相关高资源语言的标记化中受益，并提供了构建更公平、高效和语言感知的标记化器的实用见解。

Conclusion: 研究为多语言NLP中构建更公平、高效和语言感知的标记化器提供了实践指导。

Abstract: Tokenization plays a pivotal role in multilingual NLP. However, existing
tokenizers are often skewed towards high-resource languages, limiting their
effectiveness for linguistically diverse and morphologically rich languages
such as those in the Indian subcontinent. This paper presents a comprehensive
intrinsic evaluation of tokenization strategies across 17 Indian languages. We
quantify the trade-offs between bottom-up and top-down tokenizer algorithms
(BPE and Unigram LM), effects of vocabulary sizes, and compare strategies of
multilingual vocabulary construction such as joint and cluster-based training.
We also show that extremely low-resource languages can benefit from tokenizers
trained on related high-resource languages. Our study provides practical
insights for building more fair, efficient, and linguistically informed
tokenizers for multilingual NLP.

</details>


### [84] [THCM-CAL: Temporal-Hierarchical Causal Modelling with Conformal Calibration for Clinical Risk Prediction](https://arxiv.org/abs/2506.17844)
*Xin Zhang,Qiyu Wei,Yingjie Zhu,Fanyi Wu,Sophia Ananiadou*

Main category: cs.CL

TL;DR: THCM-CAL是一种结合多模态因果图和时间层次结构的临床风险预测模型，通过因果发现和校准提升预测可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用电子健康记录中结构化诊断代码和非结构化叙述笔记之间的因果交互关系。

Method: 构建多模态因果图，通过层次因果发现推断三种临床交互关系，并扩展多标签ICD编码的校准预测。

Result: 在MIMIC-III和MIMIC-IV数据集上表现优于现有方法。

Conclusion: THCM-CAL通过因果建模和校准显著提升了临床风险预测的准确性和可靠性。

Abstract: Automated clinical risk prediction from electronic health records (EHRs)
demands modeling both structured diagnostic codes and unstructured narrative
notes. However, most prior approaches either handle these modalities separately
or rely on simplistic fusion strategies that ignore the directional,
hierarchical causal interactions by which narrative observations precipitate
diagnoses and propagate risk across admissions. In this paper, we propose
THCM-CAL, a Temporal-Hierarchical Causal Model with Conformal Calibration. Our
framework constructs a multimodal causal graph where nodes represent clinical
entities from two modalities: Textual propositions extracted from notes and ICD
codes mapped to textual descriptions. Through hierarchical causal discovery,
THCM-CAL infers three clinically grounded interactions: intra-slice
same-modality sequencing, intra-slice cross-modality triggers, and inter-slice
risk propagation. To enhance prediction reliability, we extend conformal
prediction to multi-label ICD coding, calibrating per-code confidence intervals
under complex co-occurrences. Experimental results on MIMIC-III and MIMIC-IV
demonstrate the superiority of THCM-CAL.

</details>


### [85] [LLMs for Customized Marketing Content Generation and Evaluation at Scale](https://arxiv.org/abs/2506.17863)
*Haoran Liu,Amir Tahmasbi,Ehtesham Sam Haque,Purak Jain*

Main category: cs.CL

TL;DR: 论文提出MarketingFM系统，通过多数据源生成关键词特定的广告文案，提升广告效果，并开发AutoEval-Main和AutoEval-Update系统，自动化评估广告文案，减少人工成本。


<details>
  <summary>Details</summary>
Motivation: 当前外部营销内容过于通用且与落地页不匹配，限制了效果，需改进。

Method: 提出MarketingFM系统，结合检索增强和多数据源生成广告文案；开发AutoEval-Main和AutoEval-Update系统，自动化评估文案。

Result: 关键词广告文案表现优于模板，点击率提升9%，展示量增加12%，成本降低0.38%；AutoEval-Main与人工评估一致率达89.57%。

Conclusion: 自动化系统显著提升广告效果和评估效率，但人工监督仍不可或缺。

Abstract: Offsite marketing is essential in e-commerce, enabling businesses to reach
customers through external platforms and drive traffic to retail websites.
However, most current offsite marketing content is overly generic,
template-based, and poorly aligned with landing pages, limiting its
effectiveness. To address these limitations, we propose MarketingFM, a
retrieval-augmented system that integrates multiple data sources to generate
keyword-specific ad copy with minimal human intervention. We validate
MarketingFM via offline human and automated evaluations and large-scale online
A/B tests. In one experiment, keyword-focused ad copy outperformed templates,
achieving up to 9% higher CTR, 12% more impressions, and 0.38% lower CPC,
demonstrating gains in ad ranking and cost efficiency. Despite these gains,
human review of generated ads remains costly. To address this, we propose
AutoEval-Main, an automated evaluation system that combines rule-based metrics
with LLM-as-a-Judge techniques to ensure alignment with marketing principles.
In experiments with large-scale human annotations, AutoEval-Main achieved
89.57% agreement with human reviewers. Building on this, we propose
AutoEval-Update, a cost-efficient LLM-human collaborative framework to
dynamically refine evaluation prompts and adapt to shifting criteria with
minimal human input. By selectively sampling representative ads for human
review and using a critic LLM to generate alignment reports, AutoEval-Update
improves evaluation consistency while reducing manual effort. Experiments show
the critic LLM suggests meaningful refinements, improving LLM-human agreement.
Nonetheless, human oversight remains essential for setting thresholds and
validating refinements before deployment.

</details>


### [86] [QueueEDIT: Structural Self-Correction for Sequential Model Editing in LLMs](https://arxiv.org/abs/2506.17864)
*Taolin Zhang,Haidong Kang,Dongyang Li,Qizhou Chen,Chengyu Wang Xiaofeng He,Richang Hong*

Main category: cs.CL

TL;DR: QueueEDIT框架通过队列机制和动态参数对齐，提升序列模型编辑性能并减少对LLM通用能力的负面影响。


<details>
  <summary>Details</summary>
Motivation: 解决序列模型编辑（SME）中因参数引入导致的LLM通用能力下降问题。

Method: 引入结构映射编辑损失和队列机制，动态对齐参数并冻结无关参数。

Result: 在多种SME设置中显著优于基线，同时保持单次编辑的竞争力。

Conclusion: QueueEDIT有效提升SME性能并保护LLM的通用能力。

Abstract: Recently, large language models (LLMs) have demonstrated impressive results
but still suffer from hallucinations. Model editing has been proposed to
correct factual inaccuracies in LLMs. A challenging case is sequential model
editing (SME), which aims to rectify errors continuously rather than treating
them as a one-time task. During SME, the general capabilities of LLMs can be
negatively affected due to the introduction of new parameters. In this paper,
we propose a queue-based self-correction framework (QueueEDIT) that not only
enhances SME performance by addressing long-sequence dependency but also
mitigates the impact of parameter bias on the general capabilities of LLMs.
Specifically, we first introduce a structural mapping editing loss to map the
triplets to the knowledge-sensitive neurons within the Transformer layers of
LLMs. We then store the located parameters for each piece of edited knowledge
in a queue and dynamically align previously edited parameters. In each edit, we
select queue parameters most relevant to the currently located parameters to
determine whether previous knowledge needs realignment. Irrelevant parameters
in the queue are frozen, and we update the parameters at the queue head to the
LLM to ensure they do not harm general abilities. Experiments show that our
framework significantly outperforms strong baselines across various SME
settings and maintains competitiveness in single-turn editing. The resulting
LLMs also preserve high capabilities in general NLP tasks throughout the SME
process.

</details>


### [87] [How Alignment Shrinks the Generative Horizon](https://arxiv.org/abs/2506.17871)
*Chenghao Yang,Ari Holtzman*

Main category: cs.CL

TL;DR: 论文研究了大型语言模型（LLM）输出缺乏多样性的原因，提出Branching Factor（BF）作为量化指标，发现对齐调优显著降低BF，解释了模型输出稳定性的原因，并探讨了其对复杂推理的影响。


<details>
  <summary>Details</summary>
Motivation: 探究对齐大型语言模型（LLM）输出稳定性的原因及其对生成多样性的影响。

Method: 引入Branching Factor（BF）作为量化指标，分析生成过程中BF的变化及对齐调优对BF的影响。

Result: 发现BF随生成过程降低，对齐调优显著减少BF；对齐Chain-of-Thought（CoT）模型通过更长推理链进入低BF阶段，输出更稳定。

Conclusion: BF是理解和控制LLM输出的有效工具，揭示了对齐调优和CoT对输出稳定性的作用。

Abstract: Despite their impressive capabilities, aligned large language models (LLMs)
often generate outputs that lack diversity. What drives this stability in the
generation? We investigate this phenomenon through the lens of probability
concentration in the model's output distribution. To quantify this
concentration, we introduce the Branching Factor (BF) -- a token-invariant
measure of the effective number of plausible next steps during generation. Our
empirical analysis reveals two key findings: (1) BF often decreases as
generation progresses, suggesting that LLMs become more predictable as they
generate. (2) alignment tuning substantially sharpens the model's output
distribution from the outset, reducing BF by nearly an order of magnitude
(e.g., from 12 to 1.2) relative to base models. This stark reduction helps
explain why aligned models often appear less sensitive to decoding strategies.
Building on this insight, we find this stability has surprising implications
for complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g.,
DeepSeek-distilled models), for instance, leverage this effect; by generating
longer reasoning chains, they push generation into later, more deterministic
(lower BF) stages, resulting in more stable outputs. We hypothesize that
alignment tuning does not fundamentally change a model's behavior, but instead
steers it toward stylistic tokens (e.g., "Sure") that unlock low-entropy
trajectories already present in the base model. This view is supported by
nudging experiments, which show that prompting base models with such tokens can
similarly reduce BF. Together, our findings establish BF as a powerful
diagnostic for understanding and controlling LLM outputs - clarifying how
alignment reduces variability, how CoT promotes stable generations, and how
base models can be steered away from diversity.

</details>


### [88] [Multi-turn Jailbreaking via Global Refinement and Active Fabrication](https://arxiv.org/abs/2506.17881)
*Hua Tang,Lingyong Yan,Yukun Zhao,Shuaiqiang Wang,Jizhou Huang,Dawei Yin*

Main category: cs.CL

TL;DR: 论文提出了一种多轮越狱方法，通过全局优化和主动伪造模型响应，提高了在复杂对话中引发有害内容的成功率。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）存在安全风险，现有越狱技术多针对单轮场景，多轮场景研究不足且适应性差。

Method: 提出了一种多轮越狱方法，全局优化越狱路径并主动伪造模型响应以抑制安全警告。

Result: 实验表明，该方法在六种先进LLMs上优于现有单轮和多轮越狱技术。

Conclusion: 该方法有效提升了多轮对话中的越狱成功率，为LLMs安全性研究提供了新思路。

Abstract: Large Language Models (LLMs) have achieved exceptional performance across a
wide range of tasks. However, they still pose significant safety risks due to
the potential misuse for malicious purposes. Jailbreaks, which aim to elicit
models to generate harmful content, play a critical role in identifying the
underlying security threats. Recent jailbreaking primarily focuses on
single-turn scenarios, while the more complicated multi-turn scenarios remain
underexplored. Moreover, existing multi-turn jailbreaking techniques struggle
to adapt to the evolving dynamics of dialogue as the interaction progresses. To
address this limitation, we propose a novel multi-turn jailbreaking method that
refines the jailbreaking path globally at each interaction. We also actively
fabricate model responses to suppress safety-related warnings, thereby
increasing the likelihood of eliciting harmful outputs in subsequent questions.
Experimental results demonstrate the superior performance of our method
compared with existing single-turn and multi-turn jailbreaking techniques
across six state-of-the-art LLMs. Our code is publicly available at
https://github.com/Ytang520/Multi-Turn_jailbreaking_Global-Refinment_and_Active-Fabrication.

</details>


### [89] [Scatter-Based Innovation Propagation in Large Language Models for Multi-Stage Process Adaptation](https://arxiv.org/abs/2506.17949)
*Hong Su*

Main category: cs.CL

TL;DR: 论文提出了一种基于散射的创新扩展模型（innovation scatter model），帮助LLMs将局部创新推广到多阶段流程的其他部分，从而提高泛化和重用能力。


<details>
  <summary>Details</summary>
Motivation: LLMs在复制和扩展预训练中观察到的模式方面表现强大，但在将新颖想法推广到原始上下文之外时存在困难。本文旨在解决如何将局部创新应用到多阶段流程的其他部分的问题。

Method: 提出四步创新散射模型：(1) 识别核心创新，(2) 通过去除特定阶段或组件的引用泛化创新，(3) 判断泛化创新是否适用于更广范围，(4) 系统性地将其应用于结构相似阶段。

Result: 验证结果表明，该模型能有效帮助LLMs将创新扩展到结构相似阶段，增强泛化和重用能力。

Conclusion: 创新散射模型通过利用阶段间的结构冗余，显著提升了LLMs在跨阶段推广创新方面的能力。

Abstract: Large Language Models (LLMs) exhibit strong capabilities in reproducing and
extending patterns observed during pretraining but often struggle to generalize
novel ideas beyond their original context. This paper addresses the challenge
of applying such localized innovations - introduced at a specific stage or
component - to other parts of a multi-stage process. We propose a scatter-based
innovation expansion model (innovation scatter model) that guides the LLM
through a four-step process: (1) identifying the core innovation by comparing
the user's input with its surrounding context, (2) generalizing the innovation
by removing references to specific stages or components, (3) determining
whether the generalized innovation applies to a broader scope beyond the
original stage, and (4) systematically applying it to other structurally
similar stages using the LLM. This model leverages structural redundancy across
stages to improve the applicability of novel ideas. Verification results
demonstrate that the innovation scatter model enables LLMs to extend
innovations across structurally similar stages, thereby enhancing
generalization and reuse.

</details>


### [90] [A Comprehensive Graph Framework for Question Answering with Mode-Seeking Preference Alignment](https://arxiv.org/abs/2506.17951)
*Quanwei Tang,Sophia Yat Mei Lee,Junshuang Wu,Dong Zhang,Shoushan Li,Erik Cambria,Guodong Zhou*

Main category: cs.CL

TL;DR: GraphMPA框架通过图结构和模式偏好优化提升检索增强生成（RAG）的全局理解和伦理对齐。


<details>
  <summary>Details</summary>
Motivation: 解决RAG在全局理解和伦理偏好对齐上的挑战。

Method: 构建层次化文档图，模拟人类认知过程，并引入模式偏好优化。

Result: 在六个数据集上的实验验证了GraphMPA的有效性。

Conclusion: GraphMPA显著提升了RAG的全局理解和伦理对齐能力。

Abstract: Recent advancements in retrieval-augmented generation (RAG) have enhanced
large language models in question answering by integrating external knowledge.
However, challenges persist in achieving global understanding and aligning
responses with human ethical and quality preferences. To address these issues,
we propose GraphMPA, a comprehensive graph-based framework with mode-seeking
preference alignment. Our approach constructs a hierarchical document graph
using a general similarity measurement, mimicking human cognitive processes for
information understanding and synthesis. Additionally, we introduce
mode-seeking preference optimization to better align model outputs with human
preferences through probability-matching constraints. Extensive experiments on
six datasets demonstrate the effectiveness of our
\href{https://github.com/tangquanwei/GraphMPA}{GraphMPA}.

</details>


### [91] [PDF Retrieval Augmented Question Answering](https://arxiv.org/abs/2506.18027)
*Thi Thu Uyen Hoang,Viet Anh Nguyen*

Main category: cs.CL

TL;DR: 提出了一种基于RAG框架的QA系统改进方法，专注于从PDF中提取多模态信息，以解决复杂查询问题。


<details>
  <summary>Details</summary>
Motivation: PDF文件包含丰富多样的数据（如文本、图像、图表等），现有QA系统主要针对文本内容设计，难以处理多模态查询。

Method: 通过优化非文本元素在RAG框架中的处理和集成方法，并微调大型语言模型，以提升系统性能。

Result: 实验证明该系统能够从PDF中准确提取信息，适用于多种内容类型。

Conclusion: 该研究不仅拓展了检索增强QA系统的能力，还为多模态数据集成和处理的研究奠定了基础。

Abstract: This paper presents an advancement in Question-Answering (QA) systems using a
Retrieval Augmented Generation (RAG) framework to enhance information
extraction from PDF files. Recognizing the richness and diversity of data
within PDFs--including text, images, vector diagrams, graphs, and tables--poses
unique challenges for existing QA systems primarily designed for textual
content. We seek to develop a comprehensive RAG-based QA system that will
effectively address complex multimodal questions, where several data types are
combined in the query. This is mainly achieved by refining approaches to
processing and integrating non-textual elements in PDFs into the RAG framework
to derive precise and relevant answers, as well as fine-tuning large language
models to better adapt to our system. We provide an in-depth experimental
evaluation of our solution, demonstrating its capability to extract accurate
information that can be applied to different types of content across PDFs. This
work not only pushes the boundaries of retrieval-augmented QA systems but also
lays a foundation for further research in multimodal data integration and
processing.

</details>


### [92] [Splitformer: An improved early-exit architecture for automatic speech recognition on edge devices](https://arxiv.org/abs/2506.18035)
*Maxence Lasbordes,Daniele Falavigna,Alessio Brutti*

Main category: cs.CL

TL;DR: 论文提出了一种在早期退出模型中引入并行层处理降采样输入的方法，显著提升了语音识别性能，同时仅小幅增加模型参数且不影响推理时间。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的设备上动态调整神经网络计算负载的需求，以及现有语音识别架构缺乏早期退出分支的模块性。

Method: 在架构中引入并行层处理降采样输入，结合标准处理层。

Result: 在标准基准测试中显著提升了语音识别性能，模型参数小幅增加，推理时间不受影响。

Conclusion: 提出的方法有效提升了早期退出模型的性能，适用于资源受限的语音识别场景。

Abstract: The ability to dynamically adjust the computational load of neural models
during inference in a resource aware manner is crucial for on-device processing
scenarios, characterised by limited and time-varying computational resources.
Early-exit architectures represent an elegant and effective solution, since
they can process the input with a subset of their layers, exiting at
intermediate branches (the upmost layers are hence removed from the model).
  From a different perspective, for automatic speech recognition applications
there are memory-efficient neural architectures that apply variable frame rate
analysis, through downsampling/upsampling operations in the middle layers,
reducing the overall number of operations and improving significantly the
performance on well established benchmarks. One example is the Zipformer.
However, these architectures lack the modularity necessary to inject early-exit
branches.
  With the aim of improving the performance in early-exit models, we propose
introducing parallel layers in the architecture that process downsampled
versions of their inputs. % in conjunction with standard processing layers. We
show that in this way the speech recognition performance on standard benchmarks
significantly improve, at the cost of a small increase in the overall number of
model parameters but without affecting the inference time.

</details>


### [93] [Markov-Enhanced Clustering for Long Document Summarization: Tackling the 'Lost in the Middle' Challenge with Large Language Models](https://arxiv.org/abs/2506.18036)
*Aziz Amari,Mohamed Achref Ben Ammar*

Main category: cs.CL

TL;DR: 提出了一种结合抽取式和生成式摘要的混合方法，通过分块、聚类和马尔可夫链图生成更高效的摘要。


<details>
  <summary>Details</summary>
Motivation: 解决生成式摘要模型在长文档中丢失关键信息的问题，同时降低资源消耗。

Method: 将文档分块并聚类，为每个聚类生成摘要，利用马尔可夫链图选择语义顺序。

Result: 生成更连贯且保留关键信息的摘要。

Conclusion: 混合方法有效解决了长文档摘要中的信息丢失问题，同时兼顾效率。

Abstract: The rapid expansion of information from diverse sources has heightened the
need for effective automatic text summarization, which condenses documents into
shorter, coherent texts. Summarization methods generally fall into two
categories: extractive, which selects key segments from the original text, and
abstractive, which generates summaries by rephrasing the content coherently.
Large language models have advanced the field of abstractive summarization, but
they are resourceintensive and face significant challenges in retaining key
information across lengthy documents, which we call being "lost in the middle".
To address these issues, we propose a hybrid summarization approach that
combines extractive and abstractive techniques. Our method splits the document
into smaller text chunks, clusters their vector embeddings, generates a summary
for each cluster that represents a key idea in the document, and constructs the
final summary by relying on a Markov chain graph when selecting the semantic
order of ideas.

</details>


### [94] [Statistical Multicriteria Evaluation of LLM-Generated Text](https://arxiv.org/abs/2506.18082)
*Esteban Garces Arias,Hannah Blocher,Julian Rodemann,Matthias Aßenmacher,Christoph Jansen*

Main category: cs.CL

TL;DR: 该论文提出了一种基于广义随机优势（GSD）的框架，用于评估LLM生成文本的质量，解决了现有方法在单一指标、指标兼容性和统计推断方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前评估LLM生成文本质量的方法存在不足，无法全面捕捉文本质量的多个维度（如连贯性、多样性、流畅性等），且缺乏统计推断保证。

Method: 采用广义随机优势（GSD）框架，通过部分排序解码策略，避免对多维度指标的任意加权，同时考虑不同测量尺度。

Result: 该框架能够识别解码策略与人类生成文本之间的显著性能差异，并处理采样设计中的非独立同分布假设偏差。

Conclusion: GSD-front方法为多维度文本质量评估提供了统计可靠的解决方案，优于现有单一指标或简单聚合方法。

Abstract: Assessing the quality of LLM-generated text remains a fundamental challenge
in natural language processing. Current evaluation approaches often rely on
isolated metrics or simplistic aggregations that fail to capture the nuanced
trade-offs between coherence, diversity, fluency, and other relevant indicators
of text quality. In this work, we adapt a recently proposed framework for
statistical inference based on Generalized Stochastic Dominance (GSD) that
addresses three critical limitations in existing benchmarking methodologies:
the inadequacy of single-metric evaluation, the incompatibility between
cardinal automatic metrics and ordinal human judgments, and the lack of
inferential statistical guarantees. The GSD-front approach enables simultaneous
evaluation across multiple quality dimensions while respecting their different
measurement scales, building upon partial orders of decoding strategies, thus
avoiding arbitrary weighting of the involved metrics. By applying this
framework to evaluate common decoding strategies against human-generated text,
we demonstrate its ability to identify statistically significant performance
differences while accounting for potential deviations from the i.i.d.
assumption of the sampling design.

</details>


### [95] [Evaluating Prompt-Based and Fine-Tuned Approaches to Czech Anaphora Resolution](https://arxiv.org/abs/2506.18091)
*Patrik Stano,Aleš Horák*

Main category: cs.CL

TL;DR: 论文比较了两种现代指代消解方法在捷克语中的表现：基于提示工程的大语言模型（LLMs）和微调紧凑生成模型。结果显示，微调模型（如mT5-large）显著优于提示方法，准确率最高达88%。


<details>
  <summary>Details</summary>
Motivation: 指代消解对自然语言理解至关重要，尤其在形态丰富的语言（如捷克语）中。研究旨在比较两种现代方法的性能。

Method: 使用来自Prague Dependency Treebank的数据集，评估了指令调优的LLMs（如Mistral Large 2和Llama 3）和微调的mT5、Mistral模型。

Result: 微调模型（特别是mT5-large）表现最佳，准确率达88%，优于提示方法的74.5%。

Conclusion: 微调模型在捷克语指代消解中更具优势，尤其在性能和计算资源需求方面。

Abstract: Anaphora resolution plays a critical role in natural language understanding,
especially in morphologically rich languages like Czech. This paper presents a
comparative evaluation of two modern approaches to anaphora resolution on Czech
text: prompt engineering with large language models (LLMs) and fine-tuning
compact generative models. Using a dataset derived from the Prague Dependency
Treebank, we evaluate several instruction-tuned LLMs, including Mistral Large 2
and Llama 3, using a series of prompt templates. We compare them against
fine-tuned variants of the mT5 and Mistral models that we trained specifically
for Czech anaphora resolution. Our experiments demonstrate that while prompting
yields promising few-shot results (up to 74.5% accuracy), the fine-tuned
models, particularly mT5-large, outperform them significantly, achieving up to
88% accuracy while requiring fewer computational resources. We analyze
performance across different anaphora types, antecedent distances, and source
corpora, highlighting key strengths and trade-offs of each approach.

</details>


### [96] [InspireDebate: Multi-Dimensional Subjective-Objective Evaluation-Guided Reasoning and Optimization for Debating](https://arxiv.org/abs/2506.18102)
*Fuyu Wang,Jiangtong Li,Kun Zhu,Changjun Jiang*

Main category: cs.CL

TL;DR: 提出了一种双组件框架（InspireScore和InspireDebate），用于改进基于LLM的辩论系统，通过多维评估和优化方法显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的辩论系统缺乏对客观评估（如真实性和逻辑有效性）的关注，且缺乏结构化优化方法。

Method: 1. InspireScore：多维评估架构，结合主观和客观指标；2. InspireDebate：分阶段优化方法，包括CoT推理增强、多维DPO和Web-RAG。

Result: InspireScore与专家判断的相关性提高44%，InspireDebate比基线模型提升57%。

Conclusion: 提出的框架显著提升了辩论系统的评估和优化能力。

Abstract: With the rapid advancements in large language models (LLMs), debating tasks,
such as argument quality assessment and debate process simulation, have made
significant progress. However, existing LLM-based debating systems focus on
responding to specific arguments while neglecting objective assessments such as
authenticity and logical validity. Furthermore, these systems lack a structured
approach to optimize across various dimensions$-$including evaluation metrics,
chain-of-thought (CoT) reasoning, and multi-turn debate refinement$-$thereby
limiting their effectiveness. To address these interconnected challenges, we
propose a dual-component framework: (1) $\textbf{InspireScore}$, a novel
evaluation system that establishes a multi-dimensional assessment architecture
incorporating four subjective criteria (emotional appeal, argument clarity,
argument arrangement, and topic relevance) alongside two objective metrics
(fact authenticity and logical validity); and (2) $\textbf{InspireDebate}$, an
optimized debating framework employing a phased optimization approach through
CoT reasoning enhancement, multi-dimensional Direct Preference Optimization
(DPO), and real-time knowledge grounding via web-based Retrieval Augmented
Generation (Web-RAG). Empirical evaluations demonstrate that
$\textbf{InspireScore}$ achieves 44$\%$ higher correlation with expert
judgments compared to existing methods, while $\textbf{InspireDebate}$ shows
significant improvements, outperforming baseline models by 57$\%$. Source code
is available at https://github.com/fywang12/InspireDebate.

</details>


### [97] [Chengyu-Bench: Benchmarking Large Language Models for Chinese Idiom Understanding and Use](https://arxiv.org/abs/2506.18105)
*Yicheng Fu,Zhemin Huang,Liuxin Yang,Yumeng Lu,Zhongdongming Dai*

Main category: cs.CL

TL;DR: Chengyu-Bench是一个针对中文成语的综合性基准测试，包含三项任务：情感分类、使用恰当性检测和开放填空。研究发现，大语言模型在情感分类上表现优异，但在文化语境理解上仍有不足。


<details>
  <summary>Details</summary>
Motivation: 中文成语的复杂性和文化背景使其难以被语言模型准确理解和运用，现有基准测试任务过于狭窄，因此需要更全面的评估工具。

Method: Chengyu-Bench包含2,937个人工验证的例子，覆盖1,765个常见成语，通过三项任务评估语言模型：情感分类、使用恰当性检测和开放填空。

Result: 大语言模型在情感分类任务上准确率超过95%，但在使用恰当性检测和开放填空任务上分别仅为85%和40%。

Conclusion: Chengyu-Bench显示，尽管大语言模型能可靠判断成语情感倾向，但在文化语境理解上仍有显著不足，需进一步改进。

Abstract: Chinese idioms (Chengyu) are concise four-character expressions steeped in
history and culture, whose literal translations often fail to capture their
full meaning. This complexity makes them challenging for language models to
interpret and use correctly. Existing benchmarks focus on narrow tasks -
multiple-choice cloze tests, isolated translation, or simple paraphrasing. We
introduce Chengyu-Bench, a comprehensive benchmark featuring three tasks: (1)
Evaluative Connotation, classifying idioms as positive or negative; (2)
Appropriateness, detecting incorrect idiom usage in context; and (3) Open
Cloze, filling blanks in longer passages without options. Chengyu-Bench
comprises 2,937 human-verified examples covering 1,765 common idioms sourced
from diverse corpora. We evaluate leading LLMs and find they achieve over 95%
accuracy on Evaluative Connotation, but only ~85% on Appropriateness and ~40%
top-1 accuracy on Open Cloze. Error analysis reveals that most mistakes arise
from fundamental misunderstandings of idiom meanings. Chengyu-Bench
demonstrates that while LLMs can reliably gauge idiom sentiment, they still
struggle to grasp the cultural and contextual nuances essential for proper
usage. The benchmark and source code are available at:
https://github.com/sofyc/ChengyuBench.

</details>


### [98] [Mental Health Equity in LLMs: Leveraging Multi-Hop Question Answering to Detect Amplified and Silenced Perspectives](https://arxiv.org/abs/2506.18116)
*Batool Haider,Atmika Gorti,Aman Chadha,Manas Gaur*

Main category: cs.CL

TL;DR: 该研究提出了一种多跳问答框架（MHQA），用于检测大型语言模型（LLM）在心理健康讨论中的偏见，揭示了模型在人口统计交叉点上的系统性偏差，并提出了两种去偏技术。


<details>
  <summary>Details</summary>
Motivation: 心理健康领域的大型语言模型可能传播偏见，加剧对边缘群体的伤害，但目前缺乏系统性检测交叉偏见的方法。

Method: 使用多跳问答框架（MHQA）分析IMHI数据集，通过系统性标签（如年龄、种族、性别、社会经济地位）检测人口统计交叉点的偏见模式，并评估四种LLM模型。

Result: 研究发现LLM在情感、人口统计和心理健康条件上存在系统性差异，MHQA方法优于传统方法，去偏技术（角色扮演模拟和显性偏见减少）实现了66-94%的偏见减少。

Conclusion: 研究揭示了LLM在心理健康领域中的偏见问题，为开发公平AI提供了可行建议。

Abstract: Large Language Models (LLMs) in mental healthcare risk propagating biases
that reinforce stigma and harm marginalized groups. While previous research
identified concerning trends, systematic methods for detecting intersectional
biases remain limited. This work introduces a multi-hop question answering
(MHQA) framework to explore LLM response biases in mental health discourse. We
analyze content from the Interpretable Mental Health Instruction (IMHI) dataset
across symptom presentation, coping mechanisms, and treatment approaches. Using
systematic tagging across age, race, gender, and socioeconomic status, we
investigate bias patterns at demographic intersections. We evaluate four LLMs:
Claude 3.5 Sonnet, Jamba 1.6, Gemma 3, and Llama 4, revealing systematic
disparities across sentiment, demographics, and mental health conditions. Our
MHQA approach demonstrates superior detection compared to conventional methods,
identifying amplification points where biases magnify through sequential
reasoning. We implement two debiasing techniques: Roleplay Simulation and
Explicit Bias Reduction, achieving 66-94% bias reductions through few-shot
prompting with BBQ dataset examples. These findings highlight critical areas
where LLMs reproduce mental healthcare biases, providing actionable insights
for equitable AI development.

</details>


### [99] [The Syntactic Acceptability Dataset (Preview): A Resource for Machine Learning and Linguistic Analysis of English](https://arxiv.org/abs/2506.18120)
*Tom S Juzek*

Main category: cs.CL

TL;DR: 介绍了Syntactic Acceptability Dataset，包含1000个英语序列，标注了语法状态和可接受性状态，是目前最大的公开数据集。初步分析显示语法性和可接受性判断在83%情况下一致，机器学习模型在预测可接受性方面表现较好。


<details>
  <summary>Details</summary>
Motivation: 为语法和计算语言学研究设计资源，填补公开数据集的空白。

Method: 数据集包含1000个英语序列，标注语法状态和可接受性状态，通过众包获取数据。

Result: 语法性和可接受性判断83%一致，机器学习模型在预测可接受性方面表现较好。

Conclusion: 数据集为语言学和计算语言学研究提供了新资源，未来将扩展数据集。

Abstract: We present a preview of the Syntactic Acceptability Dataset, a resource being
designed for both syntax and computational linguistics research. In its current
form, the dataset comprises 1,000 English sequences from the syntactic
discourse: Half from textbooks and half from the journal Linguistic Inquiry,
the latter to ensure a representation of the contemporary discourse. Each entry
is labeled with its grammatical status ("well-formedness" according to
syntactic formalisms) extracted from the literature, as well as its
acceptability status ("intuitive goodness" as determined by native speakers)
obtained through crowdsourcing, with highest experimental standards. Even in
its preliminary form, this dataset stands as the largest of its kind that is
publicly accessible. We also offer preliminary analyses addressing three
debates in linguistics and computational linguistics: We observe that
grammaticality and acceptability judgments converge in about 83% of the cases
and that "in-betweenness" occurs frequently. This corroborates existing
research. We also find that while machine learning models struggle with
predicting grammaticality, they perform considerably better in predicting
acceptability. This is a novel finding. Future work will focus on expanding the
dataset.

</details>


### [100] [$φ^{\infty}$: Clause Purification, Embedding Realignment, and the Total Suppression of the Em Dash in Autoregressive Language Models](https://arxiv.org/abs/2506.18129)
*Bugra Kilictas,Faruk Alpay*

Main category: cs.CL

TL;DR: 论文发现自回归Transformer语言模型中存在一个关键漏洞，即em dash符号会导致递归语义漂移，引发子句边界幻觉和嵌入空间纠缠。通过符号子句净化和嵌入矩阵调整，提出了一种无需重新训练模型的解决方案，显著提升了生成一致性和主题保持。


<details>
  <summary>Details</summary>
Motivation: 自回归Transformer语言模型在长文本生成中可能因特定符号（如em dash）引发递归语义漂移，导致生成内容的不一致性和潜在安全隐患。

Method: 结合符号子句净化（phi-infinity算子）和针对性嵌入矩阵调整，抑制问题符号的影响，同时保证语义连贯性。

Result: 实验验证表明，该方法显著提升了生成一致性和主题保持能力。

Conclusion: 该研究为识别和缓解基础模型中的符号级漏洞提供了通用框架，对AI安全、模型对齐和大语言模型的稳健部署具有重要意义。

Abstract: We identify a critical vulnerability in autoregressive transformer language
models where the em dash token induces recursive semantic drift, leading to
clause boundary hallucination and embedding space entanglement. Through formal
analysis of token-level perturbations in semantic lattices, we demonstrate that
em dash insertion fundamentally alters the model's latent representations,
causing compounding errors in long-form generation. We propose a novel solution
combining symbolic clause purification via the phi-infinity operator with
targeted embedding matrix realignment. Our approach enables total suppression
of problematic tokens without requiring model retraining, while preserving
semantic coherence through fixed-point convergence guarantees. Experimental
validation shows significant improvements in generation consistency and topic
maintenance. This work establishes a general framework for identifying and
mitigating token-level vulnerabilities in foundation models, with immediate
implications for AI safety, model alignment, and robust deployment of large
language models in production environments. The methodology extends beyond
punctuation to address broader classes of recursive instabilities in neural
text generation systems.

</details>


### [101] [Sparse Feature Coactivation Reveals Composable Semantic Modules in Large Language Models](https://arxiv.org/abs/2506.18141)
*Ruixuan Deng,Xiaoyang Hu,Miles Gilberti,Shane Storks,Aman Taxali,Mike Angstadt,Chandra Sripada,Joyce Chai*

Main category: cs.CL

TL;DR: 通过稀疏自编码器（SAE）特征共激活，识别大型语言模型（LLM）中语义连贯、上下文一致的网络组件，并展示其对模型输出的可预测影响。


<details>
  <summary>Details</summary>
Motivation: 探索LLM中知识的模块化组织，以及如何高效、有针对性地操控模型行为。

Method: 使用稀疏自编码器（SAE）特征共激活技术，从少量提示中提取语义组件，并通过消融和放大这些组件分析其对输出的影响。

Result: 消融或放大国家与关系语义组件会分别改变或诱导模型输出；关系组件更集中于深层网络，且对输出有更强的因果影响。

Conclusion: LLM中的知识呈现模块化组织，为高效模型操控提供了新方法。

Abstract: We identify semantically coherent, context-consistent network components in
large language models (LLMs) using coactivation of sparse autoencoder (SAE)
features collected from just a handful of prompts. Focusing on country-relation
tasks, we show that ablating semantic components for countries and relations
changes model outputs in predictable ways, while amplifying these components
induces counterfactual responses. Notably, composing relation and country
components yields compound counterfactual outputs. We find that, whereas most
country components emerge from the very first layer, the more abstract relation
components are concentrated in later layers. Furthermore, within relation
components themselves, nodes from later layers tend to have a stronger causal
impact on model outputs. Overall, these findings suggest a modular organization
of knowledge within LLMs and advance methods for efficient, targeted model
manipulation.

</details>


### [102] [QuranMorph: Morphologically Annotated Quranic Corpus](https://arxiv.org/abs/2506.18148)
*Diyam Akra,Tymaa Hammouda,Mustafa Jarrar*

Main category: cs.CL

TL;DR: QuranMorph是一个为《古兰经》提供形态学标注的语料库，包含77,429个标记，由三位专家手动完成词形还原和词性标注。


<details>
  <summary>Details</summary>
Motivation: 构建一个高质量的《古兰经》形态学标注语料库，以便与其他语言学资源互联。

Method: 利用Qabas阿拉伯语词典数据库进行词形还原，并使用SAMA/Qabas细粒度词性标注集（40个标签）进行标注。

Result: QuranMorph语料库成功与其他语言学资源互联，并作为开源资源公开提供。

Conclusion: QuranMorph为《古兰经》研究提供了高质量的标注数据，并促进了语言学资源的互联。

Abstract: We present the QuranMorph corpus, a morphologically annotated corpus for the
Quran (77,429 tokens). Each token in the QuranMorph was manually lemmatized and
tagged with its part-of-speech by three expert linguists. The lemmatization
process utilized lemmas from Qabas, an Arabic lexicographic database linked
with 110 lexicons and corpora of 2 million tokens. The part-of-speech tagging
was performed using the fine-grained SAMA/Qabas tagset, which encompasses 40
tags. As shown in this paper, this rich lemmatization and POS tagset enabled
the QuranMorph corpus to be inter-linked with many linguistic resources. The
corpus is open-source and publicly available as part of the SinaLab resources
at (https://sina.birzeit.edu/quran)

</details>


### [103] [CareLab at #SMM4H-HeaRD 2025: Insomnia Detection and Food Safety Event Extraction with Domain-Aware Transformers](https://arxiv.org/abs/2506.18185)
*Zihan Liang,Ziwen Pan,Sumon Kanti Dey,Azra Ismail*

Main category: cs.CL

TL;DR: 本文介绍了参与SMM4H-HeaRD 2025共享任务的系统，重点在Task 5 Subtask 1中取得第一名（F1=0.958），使用了RoBERTa和GPT-4等方法。


<details>
  <summary>Details</summary>
Motivation: 解决临床笔记中失眠检测和新闻中食品安全事件提取的任务。

Method: 采用基于编码器的模型（如RoBERTa）和GPT-4进行数据增强，包括预处理、模型架构和任务特定调整。

Result: 在Task 5 Subtask 1中表现优异，F1得分为0.958，排名第一。

Conclusion: 提出的方法在多个子任务中表现良好，特别是在食品安全事件提取任务中取得了显著成果。

Abstract: This paper presents our system for the SMM4H-HeaRD 2025 shared tasks,
specifically Task 4 (Subtasks 1, 2a, and 2b) and Task 5 (Subtasks 1 and 2).
Task 4 focused on detecting mentions of insomnia in clinical notes, while Task
5 addressed the extraction of food safety events from news articles. We
participated in all subtasks and report key findings across them, with
particular emphasis on Task 5 Subtask 1, where our system achieved strong
performance-securing first place with an F1 score of 0.958 on the test set. To
attain this result, we employed encoder-based models (e.g., RoBERTa), alongside
GPT-4 for data augmentation. This paper outlines our approach, including
preprocessing, model architecture, and subtask-specific adaptations

</details>


### [104] [Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review](https://arxiv.org/abs/2506.18199)
*Bushra Asseri,Estabrag Abdelaziz,Areej Al-Wabil*

Main category: cs.CL

TL;DR: 该论文探讨了如何通过提示工程减少大语言模型中对阿拉伯人和穆斯林的文化偏见，总结了五种主要方法，并指出结构化多步流程效果最佳。


<details>
  <summary>Details</summary>
Motivation: 针对大语言模型中存在的文化偏见（尤其是对阿拉伯人和穆斯林的偏见）缺乏专门研究，亟需有效的提示工程策略。

Method: 采用混合方法的系统性综述，遵循PRISMA指南和Kitchenham的系统综述方法，分析了2021-2024年的8项实证研究。

Result: 发现五种提示工程方法，其中结构化多步流程效果最好（偏见减少87.7%），文化提示更具普适性。

Conclusion: 提示工程可有效减少文化偏见，但需进一步研究文化适应性提示和整合其他去偏方法。

Abstract: Large language models have demonstrated remarkable capabilities across
various domains, yet concerns about cultural bias - particularly towards Arabs
and Muslims - pose significant ethical challenges by perpetuating harmful
stereotypes and marginalization. Despite growing recognition of bias in LLMs,
prompt engineering strategies specifically addressing Arab and Muslim
representation remain understudied. This mixed-methods systematic review
examines such techniques, offering evidence-based guidance for researchers and
practitioners. Following PRISMA guidelines and Kitchenham's systematic review
methodology, we analyzed 8 empirical studies published between 2021-2024
investigating bias mitigation strategies. Our findings reveal five primary
prompt engineering approaches: cultural prompting, affective priming,
self-debiasing techniques, structured multi-step pipelines, and
parameter-optimized continuous prompts. Although all approaches show potential
for reducing bias, effectiveness varied substantially across studies and bias
types. Evidence suggests that certain bias types may be more resistant to
prompt-based mitigation than others. Structured multi-step pipelines
demonstrated the highest overall effectiveness, achieving up to 87.7% reduction
in bias, though they require greater technical expertise. Cultural prompting
offers broader accessibility with substantial effectiveness. These results
underscore the accessibility of prompt engineering for mitigating cultural bias
without requiring access to model parameters. The limited number of studies
identified highlights a significant research gap in this critical area. Future
research should focus on developing culturally adaptive prompting techniques,
creating Arab and Muslim-specific evaluation resources, and integrating prompt
engineering with complementary debiasing methods to address deeper stereotypes
while maintaining model utility.

</details>


### [105] [Deciphering Emotions in Children Storybooks: A Comparative Analysis of Multimodal LLMs in Educational Applications](https://arxiv.org/abs/2506.18201)
*Bushra Asseri,Estabraq Abdelaziz,Maha Al Mogren,Tayef Alhefdhi,Areej Al-Wabil*

Main category: cs.CL

TL;DR: 研究评估了GPT-4o和Gemini 1.5 Pro在阿拉伯语儿童故事书插图情感识别中的表现，发现GPT-4o优于Gemini，但两者在文化细微情感和模糊叙事语境中存在局限性。


<details>
  <summary>Details</summary>
Motivation: 开发文化敏感的教育技术需要探索阿拉伯语情境下的情感识别能力。

Method: 使用75张阿拉伯故事书插图，通过零样本、少样本和思维链提示策略评估模型性能，并与人类标注对比。

Result: GPT-4o在思维链提示下表现最佳（F1-score 59%），Gemini最高为43%。模型在文化细微情感和模糊语境中表现不佳。

Conclusion: 当前模型在文化理解上存在局限，需开发文化敏感的训练方法以支持阿拉伯语学习者的情感识别技术。

Abstract: Emotion recognition capabilities in multimodal AI systems are crucial for
developing culturally responsive educational technologies, yet remain
underexplored for Arabic language contexts where culturally appropriate
learning tools are critically needed. This study evaluates the emotion
recognition performance of two advanced multimodal large language models,
GPT-4o and Gemini 1.5 Pro, when processing Arabic children's storybook
illustrations. We assessed both models across three prompting strategies
(zero-shot, few-shot, and chain-of-thought) using 75 images from seven Arabic
storybooks, comparing model predictions with human annotations based on
Plutchik's emotional framework. GPT-4o consistently outperformed Gemini across
all conditions, achieving the highest macro F1-score of 59% with
chain-of-thought prompting compared to Gemini's best performance of 43%. Error
analysis revealed systematic misclassification patterns, with valence
inversions accounting for 60.7% of errors, while both models struggled with
culturally nuanced emotions and ambiguous narrative contexts. These findings
highlight fundamental limitations in current models' cultural understanding and
emphasize the need for culturally sensitive training approaches to develop
effective emotion-aware educational technologies for Arabic-speaking learners.

</details>


### [106] [Enhancing Entity Aware Machine Translation with Multi-task Learning](https://arxiv.org/abs/2506.18318)
*An Trieu,Phuong Nguyen,Minh Le Nguyen*

Main category: cs.CL

TL;DR: 提出一种多任务学习方法，优化实体识别和机器翻译子任务，提升实体感知机器翻译性能。


<details>
  <summary>Details</summary>
Motivation: 实体感知机器翻译因数据不足和上下文复杂而具挑战性。

Method: 应用多任务学习优化实体识别和机器翻译子任务。

Result: 在SemEval 2025竞赛Task 2数据集上验证了性能提升。

Conclusion: 多任务学习方法有效提升了实体感知机器翻译的性能。

Abstract: Entity-aware machine translation (EAMT) is a complicated task in natural
language processing due to not only the shortage of translation data related to
the entities needed to translate but also the complexity in the context needed
to process while translating those entities. In this paper, we propose a method
that applies multi-task learning to optimize the performance of the two
subtasks named entity recognition and machine translation, which improves the
final performance of the Entity-aware machine translation task. The result and
analysis are performed on the dataset provided by the organizer of Task 2 of
the SemEval 2025 competition.

</details>


### [107] [TranslationCorrect: A Unified Framework for Machine Translation Post-Editing with Predictive Error Assistance](https://arxiv.org/abs/2506.18337)
*Syed Mekael Wasti,Shou-Yi Hung,Christopher Collins,En-Shiun Annie Lee*

Main category: cs.CL

TL;DR: TranslationCorrect是一个集成框架，旨在优化机器翻译后编辑和研究数据收集工作流程，结合MT生成、错误预测和直观的后编辑界面，显著提升效率和用户满意度。


<details>
  <summary>Details</summary>
Motivation: 解决机器翻译后编辑和研究数据收集工作流程的低效和脱节问题。

Method: 集成NLLB等MT模型、XCOMET或LLM API的自动错误预测，以及基于HCI原则的后编辑界面。

Result: 用户研究表明，TranslationCorrect显著提高了翻译效率和用户满意度。

Conclusion: TranslationCorrect为翻译者和研究人员提供了一个高效、用户友好的工具，支持高质量数据输出和模型训练。

Abstract: Machine translation (MT) post-editing and research data collection often rely
on inefficient, disconnected workflows. We introduce TranslationCorrect, an
integrated framework designed to streamline these tasks. TranslationCorrect
combines MT generation using models like NLLB, automated error prediction using
models like XCOMET or LLM APIs (providing detailed reasoning), and an intuitive
post-editing interface within a single environment. Built with human-computer
interaction (HCI) principles in mind to minimize cognitive load, as confirmed
by a user study. For translators, it enables them to correct errors and batch
translate efficiently. For researchers, TranslationCorrect exports high-quality
span-based annotations in the Error Span Annotation (ESA) format, using an
error taxonomy inspired by Multidimensional Quality Metrics (MQM). These
outputs are compatible with state-of-the-art error detection models and
suitable for training MT or post-editing systems. Our user study confirms that
TranslationCorrect significantly improves translation efficiency and user
satisfaction over traditional annotation methods.

</details>


### [108] [Less Data Less Tokens: Multilingual Unification Learning for Efficient Test-Time Reasoning in LLMs](https://arxiv.org/abs/2506.18341)
*Kang Chen,Mengdi Zhang,Yixin Cao*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This paper explores the challenges of test-time scaling of large language
models (LLMs), regarding both the data and inference efficiency. We highlight
the diversity of multi-lingual reasoning based on our pilot studies, and then
introduce a novel approach, \(L^2\) multi-lingual unification learning with a
decoding intervention strategy for further investigation. The basic idea of
\(L^2\) is that the reasoning process varies across different languages, which
may be mutually beneficial to enhance both model performance and efficiency. In
specific, there are two types of multi-lingual data: the entire long
chain-of-thought annotations in different languages and the step-wise mixture
of languages. By further tuning based on them, we show that even small amounts
of data can significantly improve reasoning capabilities. Our findings suggest
that multilingual learning reduces both the required data and the number of
inference tokens while maintaining a comparable performance. Furthermore,
\(L^2\) is orthogonal to other data efficient methods. Thus, we also emphasize
the importance of diverse data selection. The \(L^2\) method offers a promising
solution to the challenges of data collection and test-time compute efficiency
in LLMs.

</details>


### [109] [Evaluating Causal Explanation in Medical Reports with LLM-Based and Human-Aligned Metrics](https://arxiv.org/abs/2506.18387)
*Yousang Cho,Key-Sun Choi*

Main category: cs.CL

TL;DR: 研究比较了六种评估指标在自动生成诊断报告中因果解释质量的表现，发现GPT-Black在逻辑连贯性和临床有效性上表现最佳，GPT-White与专家评估一致，而相似性指标与临床推理质量不符。


<details>
  <summary>Details</summary>
Motivation: 探讨不同评估指标如何准确捕捉自动生成诊断报告中因果解释的质量，以支持需要可解释性和因果推理的任务。

Method: 比较六种指标（BERTScore、Cosine Similarity、BioSentVec、GPT-White、GPT-Black和专家定性评估）在两种输入类型（基于观察和基于多项选择）下的表现，并应用两种加权策略。

Result: GPT-Black在识别逻辑连贯和临床有效的因果叙述上表现最强，GPT-White与专家评估一致，相似性指标与临床推理质量不符。

Conclusion: 指标选择和加权对评估结果有显著影响，支持在需要可解释性和因果推理的任务中使用基于LLM的评估。

Abstract: This study investigates how accurately different evaluation metrics capture
the quality of causal explanations in automatically generated diagnostic
reports. We compare six metrics: BERTScore, Cosine Similarity, BioSentVec,
GPT-White, GPT-Black, and expert qualitative assessment across two input types:
observation-based and multiple-choice-based report generation. Two weighting
strategies are applied: one reflecting task-specific priorities, and the other
assigning equal weights to all metrics. Our results show that GPT-Black
demonstrates the strongest discriminative power in identifying logically
coherent and clinically valid causal narratives. GPT-White also aligns well
with expert evaluations, while similarity-based metrics diverge from clinical
reasoning quality. These findings emphasize the impact of metric selection and
weighting on evaluation outcomes, supporting the use of LLM-based evaluation
for tasks requiring interpretability and causal reasoning.

</details>


### [110] [Lemmatization as a Classification Task: Results from Arabic across Multiple Genres](https://arxiv.org/abs/2506.18399)
*Mostafa Saeed,Nizar Habash*

Main category: cs.CL

TL;DR: 论文提出两种新方法，将词形还原视为分类任务，利用机器翻译和语义聚类，并引入新的阿拉伯语测试集。结果显示分类和聚类方法更稳健。


<details>
  <summary>Details</summary>
Motivation: 解决阿拉伯语等形态丰富语言中词形还原工具因标准不一致和体裁覆盖有限而面临的挑战。

Method: 将词形还原定义为Lemma-POS-Gloss (LPG)标签集的分类任务，结合机器翻译和语义聚类。

Result: 分类和聚类方法表现更稳健且可解释，优于字符级序列到序列模型。

Conclusion: 分类和聚类方法为阿拉伯语词形还原设定了新基准，提供更可靠的输出。

Abstract: Lemmatization is crucial for NLP tasks in morphologically rich languages with
ambiguous orthography like Arabic, but existing tools face challenges due to
inconsistent standards and limited genre coverage. This paper introduces two
novel approaches that frame lemmatization as classification into a
Lemma-POS-Gloss (LPG) tagset, leveraging machine translation and semantic
clustering. We also present a new Arabic lemmatization test set covering
diverse genres, standardized alongside existing datasets. We evaluate character
level sequence-to-sequence models, which perform competitively and offer
complementary value, but are limited to lemma prediction (not LPG) and prone to
hallucinating implausible forms. Our results show that classification and
clustering yield more robust, interpretable outputs, setting new benchmarks for
Arabic lemmatization.

</details>


### [111] [TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Capabilities of Large Language Models](https://arxiv.org/abs/2506.18421)
*Ce Li,Xiaofan Liu,Zhiyan Song,Ce Chi,Chen Zhao,Jingjing Yang,Zhendong Wang,Kexin Yang,Boshen Shi,Xing Wang,Chao Deng,Junlan Feng*

Main category: cs.CL

TL;DR: 论文提出了一个全面的表格推理评估基准TReB，包含26个子任务，用于衡量LLMs在表格理解与推理上的能力，并构建了高质量数据集和评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有评估基准未能全面反映LLMs在表格推理上的表现，因此需要一个新的标准来填补这一空白。

Method: 通过迭代数据处理构建高质量数据集，并设计包含TCoT、PoT和ICoT三种推理模式的评估框架。

Result: 实验表明，现有LLMs在复杂表格任务上仍有显著改进空间。

Conclusion: TReB基准和框架为LLMs在表格推理领域的研究提供了有效工具，数据集和框架已公开。

Abstract: The majority of data in businesses and industries is stored in tables,
databases, and data warehouses. Reasoning with table-structured data poses
significant challenges for large language models (LLMs) due to its hidden
semantics, inherent complexity, and structured nature. One of these challenges
is lacking an effective evaluation benchmark fairly reflecting the performances
of LLMs on broad table reasoning abilities. In this paper, we fill in this gap,
presenting a comprehensive table reasoning evolution benchmark, TReB, which
measures both shallow table understanding abilities and deep table reasoning
abilities, a total of 26 sub-tasks. We construct a high quality dataset through
an iterative data processing procedure. We create an evaluation framework to
robustly measure table reasoning capabilities with three distinct inference
modes, TCoT, PoT and ICoT. Further, we benchmark over 20 state-of-the-art LLMs
using this frame work and prove its effectiveness. Experimental results reveal
that existing LLMs still have significant room for improvement in addressing
the complex and real world Table related tasks. Both the dataset and evaluation
framework are publicly available, with the dataset hosted on [HuggingFace] and
the framework on [GitHub].

</details>


### [112] [MeRF: Motivation-enhanced Reinforcement Finetuning for Large Reasoning Models](https://arxiv.org/abs/2506.18485)
*Junjie Zhang,Guozheng Ma,Shunyu Liu,Haoyu Wang,Jiaxing Huang,Ting-En Lin,Fei Huang,Yongbin Li,Dacheng Tao*

Main category: cs.CL

TL;DR: 论文提出了一种结合强化学习与上下文学习的方法MeRF，通过将奖励规范直接注入提示中，提升LLM的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法忽视了LLM的上下文学习能力，论文探索如何结合强化学习与上下文学习以提升LLM的推理能力。

Method: 提出MeRF方法，将奖励规范注入提示作为上下文动机，激励模型生成符合优化目标的输出。

Result: 在K&K逻辑谜题基准测试中，MeRF表现显著优于基线，且动机与奖励函数一致性越高性能越好。

Conclusion: MeRF通过结合强化学习与上下文学习，有效提升了LLM的推理能力，并能适应误导性动机。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful learn-to-reason paradigm for Large Language Models (LLMs) to tackle
complex reasoning tasks. However, existing RLVR methods overlook one of the
most distinctive capabilities of LLMs, their in-context learning ability, as
prominently demonstrated by the success of Chain-of-Thought (CoT) prompting.
This motivates us to explore how reinforcement learning can be effectively
combined with in-context learning to better improve the reasoning capabilities
of LLMs. In this paper, we introduce Motivation-enhanced Reinforcement
Finetuning} (MeRF), an intuitive yet effective method enhancing reinforcement
learning of LLMs by involving ``telling LLMs the rules of the game''.
Specifically, MeRF directly injects the reward specification into the prompt,
which serves as an in-context motivation for model to improve its responses
with awareness of the optimization objective. This simple modification
leverages the in-context learning ability of LLMs aligning generation with
optimization, thereby incentivizing the model to generate desired outputs from
both inner motivation and external reward. Empirical evaluations on the Knights
and Knaves~(K&K) logic puzzle reasoning benchmark demonstrate that
\texttt{MeRF} achieves substantial performance gains over baselines. Moreover,
ablation studies show that performance improves with greater consistency
between the in-context motivation and the external reward function, while the
model also demonstrates an ability to adapt to misleading motivations through
reinforcement learning.

</details>


### [113] [Comparative Evaluation of ChatGPT and DeepSeek Across Key NLP Tasks: Strengths, Weaknesses, and Domain-Specific Performance](https://arxiv.org/abs/2506.18501)
*Wael Etaiwi,Bushra Alhijawi*

Main category: cs.CL

TL;DR: 该研究评估了ChatGPT和DeepSeek在五个NLP任务中的表现，发现DeepSeek在分类稳定性和逻辑推理上更优，而ChatGPT在需要灵活性和细微理解的任务中表现更好。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在NLP任务中的广泛应用，需要全面评估其在不同任务中的表现，以了解其优势和局限性。

Method: 采用结构化实验协议，使用相同的中性提示在两个基准数据集上测试ChatGPT和DeepSeek。

Result: DeepSeek在分类稳定性和逻辑推理上表现更好，ChatGPT在灵活性和细微理解任务中更优。

Conclusion: 研究结果为根据任务需求选择合适的LLM提供了重要参考。

Abstract: The increasing use of large language models (LLMs) in natural language
processing (NLP) tasks has sparked significant interest in evaluating their
effectiveness across diverse applications. While models like ChatGPT and
DeepSeek have shown strong results in many NLP domains, a comprehensive
evaluation is needed to understand their strengths, weaknesses, and
domain-specific abilities. This is critical as these models are applied to
various tasks, from sentiment analysis to more nuanced tasks like textual
entailment and translation. This study aims to evaluate ChatGPT and DeepSeek
across five key NLP tasks: sentiment analysis, topic classification, text
summarization, machine translation, and textual entailment. A structured
experimental protocol is used to ensure fairness and minimize variability. Both
models are tested with identical, neutral prompts and evaluated on two
benchmark datasets per task, covering domains like news, reviews, and
formal/informal texts. The results show that DeepSeek excels in classification
stability and logical reasoning, while ChatGPT performs better in tasks
requiring nuanced understanding and flexibility. These findings provide
valuable insights for selecting the appropriate LLM based on task requirements.

</details>


### [114] [End-to-End Spoken Grammatical Error Correction](https://arxiv.org/abs/2506.18532)
*Mengjie Qian,Rao Ma,Stefano Bannò,Mark J. F. Gales,Kate M. Knill*

Main category: cs.CL

TL;DR: 该论文探讨了端到端（E2E）框架在口语语法纠错（SGEC）和反馈生成中的应用，比较了级联、部分级联和E2E架构，并提出了解决数据稀缺和反馈精度问题的创新方法。


<details>
  <summary>Details</summary>
Motivation: 口语语法纠错（SGEC）因口语不流畅、转录错误等问题面临挑战，传统级联系统易受错误传播影响，需要更高效的解决方案。

Method: 研究基于Whisper基础模型，比较了不同架构，提出了自动伪标签框架增加训练数据，并引入参考对齐和编辑置信度估计以提高反馈精度。

Result: 在Linguaskill和Speak & Improve语料库上的实验表明，所提方法显著提升了E2E SGEC的性能。

Conclusion: 端到端框架结合创新方法能有效解决SGEC的挑战，提升反馈质量。

Abstract: Grammatical Error Correction (GEC) and feedback play a vital role in
supporting second language (L2) learners, educators, and examiners. While
written GEC is well-established, spoken GEC (SGEC), aiming to provide feedback
based on learners' speech, poses additional challenges due to disfluencies,
transcription errors, and the lack of structured input. SGEC systems typically
follow a cascaded pipeline consisting of Automatic Speech Recognition (ASR),
disfluency detection, and GEC, making them vulnerable to error propagation
across modules. This work examines an End-to-End (E2E) framework for SGEC and
feedback generation, highlighting challenges and possible solutions when
developing these systems. Cascaded, partial-cascaded and E2E architectures are
compared, all built on the Whisper foundation model. A challenge for E2E
systems is the scarcity of GEC labeled spoken data. To address this, an
automatic pseudo-labeling framework is examined, increasing the training data
from 77 to over 2500 hours. To improve the accuracy of the SGEC system,
additional contextual information, exploiting the ASR output, is investigated.
Candidate feedback of their mistakes is an essential step to improving
performance. In E2E systems the SGEC output must be compared with an estimate
of the fluent transcription to obtain the feedback. To improve the precision of
this feedback, a novel reference alignment process is proposed that aims to
remove hypothesised edits that results from fluent transcription errors.
Finally, these approaches are combined with an edit confidence estimation
approach, to exclude low-confidence edits. Experiments on the in-house
Linguaskill (LNG) corpora and the publicly available Speak & Improve (S&I)
corpus show that the proposed approaches significantly boost E2E SGEC
performance.

</details>


### [115] [When Fine-Tuning Fails: Lessons from MS MARCO Passage Ranking](https://arxiv.org/abs/2506.18535)
*Manu Pande,Shahil Kumar,Anay Yatin Damle*

Main category: cs.CL

TL;DR: 研究发现，对预训练Transformer模型进行微调反而会降低其在MS MARCO任务上的性能，所有微调方法均不如基础模型表现。


<details>
  <summary>Details</summary>
Motivation: 探究为何微调预训练模型在MS MARCO任务上表现下降，挑战传统迁移学习的有效性。

Method: 通过五种模型变体（包括全参数微调和LoRA适配）进行实验，分析嵌入空间结构和训练动态。

Result: 微调破坏了基础模型预训练学到的嵌入空间结构，导致性能下降（MRR@10: 0.3026）。

Conclusion: 传统微调方法在饱和基准上可能无效，需架构创新以提升性能。

Abstract: This paper investigates the counterintuitive phenomenon where fine-tuning
pre-trained transformer models degrades performance on the MS MARCO passage
ranking task. Through comprehensive experiments involving five model
variants-including full parameter fine-tuning and parameter efficient LoRA
adaptations-we demonstrate that all fine-tuning approaches underperform the
base sentence-transformers/all- MiniLM-L6-v2 model (MRR@10: 0.3026). Our
analysis reveals that fine-tuning disrupts the optimal embedding space
structure learned during the base model's extensive pre-training on 1 billion
sentence pairs, including 9.1 million MS MARCO samples. UMAP visualizations
show progressive embedding space flattening, while training dynamics analysis
and computational efficiency metrics further support our findings. These
results challenge conventional wisdom about transfer learning effectiveness on
saturated benchmarks and suggest architectural innovations may be necessary for
meaningful improvements.

</details>


### [116] [A Modular Taxonomy for Hate Speech Definitions and Its Impact on Zero-Shot LLM Classification Performance](https://arxiv.org/abs/2506.18576)
*Matteo Melis,Gabriella Lapesa,Dennis Assenmacher*

Main category: cs.CL

TL;DR: 论文探讨了仇恨言论的定义对模型性能的影响，提出了14个概念元素的分类法，并通过实验验证了不同定义对三种大语言模型性能的影响。


<details>
  <summary>Details</summary>
Motivation: 仇恨言论检测是NLP应用中的重要任务，但定义模糊。研究旨在明确仇恨言论的定义，并探索不同定义对模型性能的影响。

Method: 收集并分析现有仇恨言论定义，构建14个概念元素的分类法；在三种数据集上对三种大语言模型进行零样本评估。

Result: 不同定义（即包含不同概念元素的具体程度）会影响模型性能，但这种影响因模型架构而异。

Conclusion: 仇恨言论定义的明确性和具体性对模型性能有显著影响，但需结合具体模型架构考虑。

Abstract: Detecting harmful content is a crucial task in the landscape of NLP
applications for Social Good, with hate speech being one of its most dangerous
forms. But what do we mean by hate speech, how can we define it, and how does
prompting different definitions of hate speech affect model performance? The
contribution of this work is twofold. At the theoretical level, we address the
ambiguity surrounding hate speech by collecting and analyzing existing
definitions from the literature. We organize these definitions into a taxonomy
of 14 Conceptual Elements-building blocks that capture different aspects of
hate speech definitions, such as references to the target of hate (individual
or groups) or of the potential consequences of it. At the experimental level,
we employ the collection of definitions in a systematic zero-shot evaluation of
three LLMs, on three hate speech datasets representing different types of data
(synthetic, human-in-the-loop, and real-world). We find that choosing different
definitions, i.e., definitions with a different degree of specificity in terms
of encoded elements, impacts model performance, but this effect is not
consistent across all architectures.

</details>


### [117] [Parallel Continuous Chain-of-Thought with Jacobi Iteration](https://arxiv.org/abs/2506.18582)
*Haoyi Wu,Zhihao Teng,Kewei Tu*

Main category: cs.CL

TL;DR: 提出并行连续思维链（PCCoT），通过并行更新潜在思维令牌，提升训练和推理效率，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 连续思维链（CoT）的潜在思维令牌之间存在顺序依赖，导致训练时间长。

Method: 采用Jacobi迭代法并行更新潜在思维令牌，减少顺序依赖。

Result: 实验表明，PCCoT在性能相当或更好的情况下，节省近50%的时间和提升稳定性。

Conclusion: PCCoT是一种高效且稳定的连续思维链改进方法。

Abstract: Continuous chain-of-thought has been shown to be effective in saving
reasoning tokens for large language models. By reasoning with continuous latent
thought tokens, continuous CoT is able to perform implicit reasoning in a
compact manner. However, the sequential dependencies between latent thought
tokens spoil parallel training, leading to long training time. In this paper,
we propose Parallel Continuous Chain-of-Thought (PCCoT), which performs Jacobi
iteration on the latent thought tokens, updating them iteratively in parallel
instead of sequentially and thus improving both training and inference
efficiency of continuous CoT. Experiments demonstrate that by choosing the
proper number of iterations, we are able to achieve comparable or even better
performance while saving nearly 50% of the training and inference time.
Moreover, PCCoT shows better stability and robustness in the training process.
Our code is available at https://github.com/whyNLP/PCCoT.

</details>


### [118] [Reply to "Emergent LLM behaviors are observationally equivalent to data leakage"](https://arxiv.org/abs/2506.18600)
*Ariel Flint Ashery,Luca Maria Aiello,Andrea Baronchelli*

Main category: cs.CL

TL;DR: 论文讨论了大型语言模型（LLM）模拟中数据污染的潜在问题，但指出这并不妨碍研究LLM群体中真正涌现的动态行为。


<details>
  <summary>Details</summary>
Motivation: 澄清数据污染问题对LLM多智能体实验的影响，并强调仍可研究其涌现动态。

Method: 通过分析Barrie和Törnberg对Flint Ashery等人研究的批评，探讨LLM群体中的自组织和模型依赖涌现动态。

Result: 实证研究表明，LLM群体中确实存在社会规范等涌现动态。

Conclusion: 尽管数据污染是潜在问题，但LLM群体的涌现动态仍可被有效研究。

Abstract: A potential concern when simulating populations of large language models
(LLMs) is data contamination, i.e. the possibility that training data may shape
outcomes in unintended ways. While this concern is important and may hinder
certain experiments with multi-agent models, it does not preclude the study of
genuinely emergent dynamics in LLM populations. The recent critique by Barrie
and T\"ornberg [1] of the results of Flint Ashery et al. [2] offers an
opportunity to clarify that self-organisation and model-dependent emergent
dynamics can be studied in LLM populations, highlighting how such dynamics have
been empirically observed in the specific case of social conventions.

</details>


### [119] [Semantic similarity estimation for domain specific data using BERT and other techniques](https://arxiv.org/abs/2506.18602)
*R. Prashanth*

Main category: cs.CL

TL;DR: 论文研究了语义相似度估计问题，比较了USE、InferSent和BERT等先进技术，发现BERT在特定领域数据集上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 语义相似度估计在自然语言处理和理解中具有重要应用，如问答、语义搜索等。研究旨在比较不同技术在该任务上的表现。

Method: 使用USE、InferSent和BERT等技术，分别在特定领域内部数据集和公开的Quora问题对数据集上进行实验。

Result: BERT表现优于其他方法，归因于其微调过程能更好地学习训练数据的模式。

Conclusion: BERT是处理特定领域数据语义相似度估计的最佳选择。

Abstract: Estimation of semantic similarity is an important research problem both in
natural language processing and the natural language understanding, and that
has tremendous application on various downstream tasks such as question
answering, semantic search, information retrieval, document clustering,
word-sense disambiguation and machine translation. In this work, we carry out
the estimation of semantic similarity using different state-of-the-art
techniques including the USE (Universal Sentence Encoder), InferSent and the
most recent BERT, or Bidirectional Encoder Representations from Transformers,
models. We use two question pairs datasets for the analysis, one is a domain
specific in-house dataset and the other is a public dataset which is the
Quora's question pairs dataset. We observe that the BERT model gave much
superior performance as compared to the other methods. This should be because
of the fine-tuning procedure that is involved in its training process, allowing
it to learn patterns based on the training data that is used. This works
demonstrates the applicability of BERT on domain specific datasets. We infer
from the analysis that BERT is the best technique to use in the case of domain
specific data.

</details>


### [120] [The Anatomy of Speech Persuasion: Linguistic Shifts in LLM-Modified Speeches](https://arxiv.org/abs/2506.18621)
*Alisa Barkar,Mathieu Chollet,Matthieu Labeau,Beatrice Biancardi,Chloe Clavel*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型如何通过修改演讲文本来理解说服力，发现GPT-4o通过调整情感词汇和句法结构来增强修辞效果，而非模仿人类优化说服力。


<details>
  <summary>Details</summary>
Motivation: 探索大型语言模型在公共演讲中理解与生成说服性内容的能力。

Method: 使用3MT法语数据集，通过GPT-4o修改演讲文本，分析修辞手法和话语标记的变化。

Result: GPT-4o通过系统性的风格调整（如情感词汇和句法结构）增强修辞效果，而非模仿人类说服策略。

Conclusion: GPT-4o在生成说服性内容时更注重风格调整，而非人类式的说服优化。

Abstract: This study examines how large language models understand the concept of
persuasiveness in public speaking by modifying speech transcripts from PhD
candidates in the "Ma These en 180 Secondes" competition, using the 3MT French
dataset. Our contributions include a novel methodology and an interpretable
textual feature set integrating rhetorical devices and discourse markers. We
prompt GPT-4o to enhance or diminish persuasiveness and analyze linguistic
shifts between original and generated speech in terms of the new features.
Results indicate that GPT-4o applies systematic stylistic modifications rather
than optimizing persuasiveness in a human-like manner. Notably, it manipulates
emotional lexicon and syntactic structures (such as interrogative and
exclamatory clauses) to amplify rhetorical impact.

</details>


### [121] [ByteSpan: Information-Driven Subword Tokenisation](https://arxiv.org/abs/2506.18639)
*Zébulon Goriely,Suchir Salhan,Pietro Lesci,Julius Cheng,Paula Buttery*

Main category: cs.CL

TL;DR: ByteSpan是一种新的信息驱动子词分词器，通过外部字节级语言模型在训练中识别可预测的字节序列并分组为子词，实验表明其在英语中比BPE更高效且形态对齐更好。


<details>
  <summary>Details</summary>
Motivation: 探索是否可以通过分组可预测字节（而非仅池化其表示）来生成有用的固定子词词汇。

Method: 提出ByteSpan，利用外部字节级语言模型在训练中识别连续可预测字节序列并将其分组为子词。

Result: ByteSpan在英语中生成更高效的词汇，形态对齐得分高于BPE；多语言实验中在25种语言中表现出相似的压缩和Rényi效率。

Conclusion: ByteSpan是一种有效的子词分词方法，尤其在形态对齐和效率方面表现优异。

Abstract: Recent dynamic tokenisation methods operate directly on bytes and pool their
latent representations into patches. This bears similarities to computational
models of word segmentation that determine lexical boundaries using spikes in
an autoregressive model's prediction error. Inspired by this connection, we
explore whether grouping predictable bytes - rather than pooling their
representations - can yield a useful fixed subword vocabulary. We propose a new
information-driven subword tokeniser, ByteSpan, that uses an external
byte-level LM during training to identify contiguous predictable byte sequences
and group them into subwords. Experiments show that ByteSpan yields efficient
vocabularies with higher morphological alignment scores than BPE for English.
Multilingual experiments show similar compression and R\'enyi efficiency for 25
languages.

</details>


### [122] [Is There a Case for Conversation Optimized Tokenizers in Large Language Models?](https://arxiv.org/abs/2506.18674)
*Raquel Ferrando,Javier Conde,Gonzalo Martínez,Pedro Reviriego*

Main category: cs.CL

TL;DR: 论文探讨了为聊天机器人对话优化分词器的潜力，通过重新设计分词器词汇表，显著减少了对话中的token数量，实现了5%到10%的能源节省。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的计算和能源成本因模型规模和用户数量激增而急剧上升，而分词器在模型效率中起关键作用。聊天机器人的用户输入和响应文本与训练语料不同，因此优化分词器可能带来性能提升。

Method: 使用公开的聊天机器人对话语料库重新设计不同分词器的词汇表，并评估其在对话领域的性能。

Result: 优化后的分词器在聊天对话中显著减少了token数量，能源节省达5%到10%，同时对原始训练语料的分词效率影响极小甚至略有提升。

Conclusion: 为聊天机器人对话优化分词器具有实际效益，能显著降低能源消耗且不影响原始性能。

Abstract: The computational and energy costs of Large Language Models (LLMs) have
increased exponentially driven by the growing model sizes and the massive
adoption of LLMs by hundreds of millions of users. The unit cost of an LLM is
the computation of a token. Therefore, the tokenizer plays an important role in
the efficiency of a model, and they are carefully optimized to minimize the
number of tokens for the text in their training corpus. One of the most popular
applications of LLMs are chatbots that interact with users. A key observation
is that, for those chatbots, what is important is the performance of the
tokenizer in the user text input and the chatbot responses. Those are most
likely different from the text in the training corpus. So, a question that
immediately arises is whether there is a potential benefit in optimizing
tokenizers for chatbot conversations. In this paper, this idea is explored for
different tokenizers by using a publicly available corpus of chatbot
conversations to redesign their vocabularies and evaluate their performance in
this domain. The results show that conversation-optimized tokenizers
consistently reduce the number of tokens in chatbot dialogues, which can lead
to meaningful energy savings, in the range of 5% to 10% while having minimal or
even slightly positive impact on tokenization efficiency for the original
training corpus.

</details>


### [123] [Context Biasing for Pronunciations-Orthography Mismatch in Automatic Speech Recognition](https://arxiv.org/abs/2506.18703)
*Christian Huber,Alexander Waibel*

Main category: cs.CL

TL;DR: 论文提出了一种在推理过程中动态修正语音识别中拼写错误的方法，显著提高了未训练词汇的识别准确率。


<details>
  <summary>Details</summary>
Motivation: 解决现有序列到序列语音识别系统在未训练词汇（如专有名词、缩写等）上的识别失败问题，尤其是发音与拼写不匹配的情况。

Method: 提出了一种允许用户在推理过程中动态添加修正的方法，以改进对特定词汇的识别。

Result: 实验表明，该方法在偏置词错误率上实现了高达11%的相对提升，同时保持了整体词错误率的竞争力。

Conclusion: 该方法有效提升了语音识别系统对未训练词汇的识别能力，尤其是发音与拼写不匹配的词汇。

Abstract: Neural sequence-to-sequence systems deliver state-of-the-art performance for
automatic speech recognition. When using appropriate modeling units, e.g.,
byte-pair encoded characters, these systems are in principal open vocabulary
systems. In practice, however, they often fail to recognize words not seen
during training, e.g., named entities, acronyms, or domain-specific special
words. To address this problem, many context biasing methods have been
proposed; however, for words with a pronunciation-orthography mismatch, these
methods may still struggle. We propose a method which allows corrections of
substitution errors to improve the recognition accuracy of such challenging
words. Users can add corrections on the fly during inference. We show that with
this method we get a relative improvement in biased word error rate of up to
11\%, while maintaining a competitive overall word error rate.

</details>


### [124] [Benchmarking the Pedagogical Knowledge of Large Language Models](https://arxiv.org/abs/2506.18710)
*Maxime Lelièvre,Amy Waldock,Meng Liu,Natalia Valdés Aspillaga,Alasdair Mackintosh,María José Ogando Portelo,Jared Lee,Paul Atherton,Robin A. A. Ince,Oliver G. B. Garrod*

Main category: cs.CL

TL;DR: 论文介绍了The Pedagogy Benchmark，一个评估大语言模型在跨领域教学知识（CDPK）和特殊教育需求与障碍（SEND）教学知识上的新数据集。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要关注内容知识，缺乏对教学法理解的评估，而教学法对教育至关重要。

Method: 基于教师专业发展考试的精选问题集，涵盖教学策略和评估方法等子领域。

Result: 测试了97个模型，准确率在28%至89%之间，并分析了成本与准确率的关系。

Conclusion: 教育相关基准对衡量模型的教学理解能力、支持教育实践及指导政策决策至关重要。

Abstract: Benchmarks like Massive Multitask Language Understanding (MMLU) have played a
pivotal role in evaluating AI's knowledge and abilities across diverse domains.
However, existing benchmarks predominantly focus on content knowledge, leaving
a critical gap in assessing models' understanding of pedagogy - the method and
practice of teaching. This paper introduces The Pedagogy Benchmark, a novel
dataset designed to evaluate large language models on their Cross-Domain
Pedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND)
pedagogical knowledge. These benchmarks are built on a carefully curated set of
questions sourced from professional development exams for teachers, which cover
a range of pedagogical subdomains such as teaching strategies and assessment
methods. Here we outline the methodology and development of these benchmarks.
We report results for 97 models, with accuracies spanning a range from 28% to
89% on the pedagogical knowledge questions. We consider the relationship
between cost and accuracy and chart the progression of the Pareto value
frontier over time. We provide online leaderboards at
https://rebrand.ly/pedagogy which are updated with new models and allow
interactive exploration and filtering based on various model properties, such
as cost per token and open-vs-closed weights, as well as looking at performance
in different subjects. LLMs and generative AI have tremendous potential to
influence education and help to address the global learning crisis.
Education-focused benchmarks are crucial to measure models' capacities to
understand pedagogical concepts, respond appropriately to learners' needs, and
support effective teaching practices across diverse contexts. They are needed
for informing the responsible and evidence-based deployment of LLMs and
LLM-based tools in educational settings, and for guiding both development and
policy decisions.

</details>


### [125] [Semantic-Preserving Adversarial Attacks on LLMs: An Adaptive Greedy Binary Search Approach](https://arxiv.org/abs/2506.18756)
*Chong Zhang,Xiang Li,Jia Wang,Shan Liang,Haochen Xue,Xiaobo Jin*

Main category: cs.CL

TL;DR: 论文提出了一种名为AGBS的方法，用于解决自动提示工程中语义失真的问题，通过动态评估优化策略对LLM性能的影响，平衡语义一致性和攻击效果。


<details>
  <summary>Details</summary>
Motivation: 自动提示工程在GUI中广泛使用，但用户需求的多样性常导致误解和错误输出，需要一种方法在优化提示时保持语义稳定性。

Method: 提出Adaptive Greedy Binary Search (AGBS)方法，模拟常见提示优化机制，动态评估其对LLM性能的影响。

Result: 在开源和闭源LLM上的实验表明，AGBS能有效平衡语义一致性和攻击效果。

Conclusion: AGBS为设计更可靠的提示优化系统提供了实用见解，代码已开源。

Abstract: Large Language Models (LLMs) increasingly rely on automatic prompt
engineering in graphical user interfaces (GUIs) to refine user inputs and
enhance response accuracy. However, the diversity of user requirements often
leads to unintended misinterpretations, where automated optimizations distort
original intentions and produce erroneous outputs. To address this challenge,
we propose the Adaptive Greedy Binary Search (AGBS) method, which simulates
common prompt optimization mechanisms while preserving semantic stability. Our
approach dynamically evaluates the impact of such strategies on LLM
performance, enabling robust adversarial sample generation. Through extensive
experiments on open and closed-source LLMs, we demonstrate AGBS's effectiveness
in balancing semantic consistency and attack efficacy. Our findings offer
actionable insights for designing more reliable prompt optimization systems.
Code is available at: https://github.com/franz-chang/DOBS

</details>


### [126] [ASP2LJ : An Adversarial Self-Play Laywer Augmented Legal Judgment Framework](https://arxiv.org/abs/2506.18768)
*Ao Chang,Tong Zhou,Yubo Chen,Delai Qiu,Shengping Liu,Kang Liu,Jun Zhao*

Main category: cs.CL

TL;DR: 论文提出ASP2LJ框架，通过对抗自博弈和案例生成模块解决法律判决预测中的长尾分布和律师角色缺失问题，并引入RareCases数据集。


<details>
  <summary>Details</summary>
Motivation: 解决法律判决预测中的长尾数据分布和律师角色缺失问题，提升判决的客观性和公平性。

Method: 提出ASP2LJ框架，结合案例生成模块和对抗自博弈机制，增强律师论证能力。

Result: 实验表明框架有效，提升了判决质量，并公开了数据集和代码。

Conclusion: ASP2LJ框架和RareCases数据集为自动化司法系统研究提供了新工具和方法。

Abstract: Legal Judgment Prediction (LJP) aims to predict judicial outcomes, including
relevant legal charge, terms, and fines, which is a crucial process in Large
Language Model(LLM). However, LJP faces two key challenges: (1)Long Tail
Distribution: Current datasets, derived from authentic cases, suffer from high
human annotation costs and imbalanced distributions, leading to model
performance degradation. (2)Lawyer's Improvement: Existing systems focus on
enhancing judges' decision-making but neglect the critical role of lawyers in
refining arguments, which limits overall judicial accuracy. To address these
issues, we propose an Adversarial Self-Play Lawyer Augmented Legal Judgment
Framework, called ASP2LJ, which integrates a case generation module to tackle
long-tailed data distributions and an adversarial self-play mechanism to
enhance lawyers' argumentation skills. Our framework enables a judge to
reference evolved lawyers' arguments, improving the objectivity, fairness, and
rationality of judicial decisions. Besides, We also introduce RareCases, a
dataset for rare legal cases in China, which contains 120 tail-end cases. We
demonstrate the effectiveness of our approach on the SimuCourt dataset and our
RareCases dataset. Experimental results show our framework brings improvements,
indicating its utilization. Our contributions include an integrated framework,
a rare-case dataset, and publicly releasing datasets and code to support
further research in automated judicial systems.

</details>


### [127] [Existing LLMs Are Not Self-Consistent For Simple Tasks](https://arxiv.org/abs/2506.18781)
*Zhenru Lin,Jiawen Tao,Yang Yuan,Andrew Chi-Chih Yao*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLMs）在简单任务中存在自洽性问题，提出两种自动化方法（图基和能量基）部分改进，但问题仍复杂。


<details>
  <summary>Details</summary>
Motivation: 确保LLMs决策透明可信，需解决其内部推理的自洽性问题。

Method: 引入不一致性指标，提出图基和能量基两种自动化方法。

Result: 小型模型高度不一致，先进模型如DeepSeek-R1和GPT-o4-mini也未完全自洽，改进方法部分有效。

Conclusion: 自洽性对构建可靠、可解释AI至关重要，但问题复杂，需进一步研究。

Abstract: Large Language Models (LLMs) have grown increasingly powerful, yet ensuring
their decisions remain transparent and trustworthy requires self-consistency --
no contradictions in their internal reasoning. Our study reveals that even on
simple tasks, such as comparing points on a line or a plane, or reasoning in a
family tree, all smaller models are highly inconsistent, and even
state-of-the-art models like DeepSeek-R1 and GPT-o4-mini are not fully
self-consistent. To quantify and mitigate these inconsistencies, we introduce
inconsistency metrics and propose two automated methods -- a graph-based and an
energy-based approach. While these fixes provide partial improvements, they
also highlight the complexity and importance of self-consistency in building
more reliable and interpretable AI. The code and data are available at
https://github.com/scorpio-nova/llm-self-consistency.

</details>


### [128] [RWESummary: A Framework and Test for Choosing Large Language Models to Summarize Real-World Evidence (RWE) Studies](https://arxiv.org/abs/2506.18819)
*Arjun Mukerji,Michael L. Jackson,Jason Jones,Neil Sanghavi*

Main category: cs.CL

TL;DR: RWESummary是一个用于评估大型语言模型（LLMs）在真实世界证据（RWE）研究摘要任务中的基准工具，基于MedHELM框架开发，并比较了不同LLMs的性能。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在通用摘要任务和医学研究辅助方面已有评估，但缺乏针对RWE研究结构化输出的专门评估。

Method: 开发了RWESummary，包含一个场景和三种评估，覆盖医学研究摘要中的主要错误类型，并使用Atropos Health专有数据进行测试。

Result: 在13项RWE研究中，Gemini 2.5模型（Flash和Pro）表现最佳。

Conclusion: RWESummary为RWE研究摘要提供了一个新颖且有用的基准模型。

Abstract: Large Language Models (LLMs) have been extensively evaluated for general
summarization tasks as well as medical research assistance, but they have not
been specifically evaluated for the task of summarizing real-world evidence
(RWE) from structured output of RWE studies. We introduce RWESummary, a
proposed addition to the MedHELM framework (Bedi, Cui, Fuentes, Unell et al.,
2025) to enable benchmarking of LLMs for this task. RWESummary includes one
scenario and three evaluations covering major types of errors observed in
summarization of medical research studies and was developed using Atropos
Health proprietary data. Additionally, we use RWESummary to compare the
performance of different LLMs in our internal RWE summarization tool. At the
time of publication, with 13 distinct RWE studies, we found the Gemini 2.5
models performed best overall (both Flash and Pro). We suggest RWESummary as a
novel and useful foundation model benchmark for real-world evidence study
summarization.

</details>


### [129] [MLLP-VRAIN UPV system for the IWSLT 2025 Simultaneous Speech Translation Translation task](https://arxiv.org/abs/2506.18828)
*Jorge Iranzo-Sánchez,Javier Iranzo-Sánchez,Adrià Giménez,Jorge Civera,Alfons Juan*

Main category: cs.CL

TL;DR: MLLP-VRAIN团队在IWSLT 2025实时语音翻译任务中提出了一种模块化级联系统，结合Whisper和NLLB模型，通过轻量级适应技术实现高质量低延迟的翻译。


<details>
  <summary>Details</summary>
Motivation: 解决长语音实时翻译的挑战，避免从头训练端到端模型。

Method: 使用Whisper Large-V3-Turbo进行语音识别，NLLB-3.3B进行翻译，采用前缀训练和自适应发射策略。

Result: 在ACL60/60数据集上BLEU得分31.96，延迟2.94秒；官方测试集得分29.8 BLEU。

Conclusion: 预训练模型经轻量级适应可有效实现长语音实时翻译，无需大量并行数据或端到端训练。

Abstract: This work describes the participation of the MLLP-VRAIN research group in the
shared task of the IWSLT 2025 Simultaneous Speech Translation track. Our
submission addresses the unique challenges of real-time translation of
long-form speech by developing a modular cascade system that adapts strong
pre-trained models to streaming scenarios. We combine Whisper Large-V3-Turbo
for ASR with the multilingual NLLB-3.3B model for MT, implementing lightweight
adaptation techniques rather than training new end-to-end models from scratch.
Our approach employs document-level adaptation with prefix training to enhance
the MT model's ability to handle incomplete inputs, while incorporating
adaptive emission policies including a wait-$k$ strategy and RALCP for managing
the translation stream. Specialized buffer management techniques and
segmentation strategies ensure coherent translations across long audio
sequences. Experimental results on the ACL60/60 dataset demonstrate that our
system achieves a favorable balance between translation quality and latency,
with a BLEU score of 31.96 and non-computational-aware StreamLAAL latency of
2.94 seconds. Our final model achieves a preliminary score on the official test
set (IWSLT25Instruct) of 29.8 BLEU. Our work demonstrates that carefully
adapted pre-trained components can create effective simultaneous translation
systems for long-form content without requiring extensive in-domain parallel
data or specialized end-to-end training.

</details>


### [130] [STU-PID: Steering Token Usage via PID Controller for Efficient Large Language Model Reasoning](https://arxiv.org/abs/2506.18831)
*Aryasomayajula Ram Bharadwaj*

Main category: cs.CL

TL;DR: STUPID方法通过PID控制器动态调整推理过程中的激活引导强度，减少冗余推理步骤，提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在扩展思维链推理中因过度思考导致的冗余计算和性能下降问题。

Method: 结合块级分类器检测冗余推理模式，并使用PID控制器动态调整引导强度。

Result: 在GSM8K数据集上，准确率提升6%，token使用减少32%。

Conclusion: STUPID提供了一种动态校准推理的框架，在保持推理质量的同时显著提升计算效率。

Abstract: Large Language Models employing extended chain-of-thought (CoT) reasoning
often suffer from the overthinking phenomenon, generating excessive and
redundant reasoning steps that increase computational costs while potentially
degrading performance. While recent work has explored static steering
approaches to mitigate this issue, they lack the adaptability to dynamically
adjust intervention strength based on real-time reasoning quality. We propose
STUPID (Steering Token Usage via PID controller), a novel training-free method
that employs a PID controller to dynamically modulate activation steering
strength during inference. Our approach combines a chunk-level classifier for
detecting redundant reasoning patterns with a PID control mechanism that
adaptively adjusts steering intensity based on the predicted redundancy
probability. Experimental evaluation on GSM8K demonstrates that STUPID achieves
a 6% improvement in accuracy while reducing token usage by 32%, outperforming
static steering baselines. Our method provides a principled framework for
dynamic reasoning calibration that maintains reasoning quality while
significantly improving computational efficiency.

</details>


### [131] [LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning](https://arxiv.org/abs/2506.18841)
*Yuhao Wu,Yushi Bai,Zhiqiang Hu,Roy Ka-Wei Lee,Juanzi Li*

Main category: cs.CL

TL;DR: 论文提出了一种基于激励的方法，利用强化学习（RL）从头开始训练LLM，无需依赖标注或合成数据，实现了超长高质量文本生成。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在超长文本生成中的长度限制和质量下降问题，避免传统方法依赖合成数据的缺点。

Method: 采用强化学习训练基础模型，结合专用奖励模型，引导模型在写作过程中进行规划和优化。

Result: LongWriter-Zero模型在长文本写作任务中表现优于传统SFT方法，并在多个评测中达到SOTA。

Conclusion: 激励方法有效提升了LLM的超长文本生成能力，且无需依赖合成数据，具有实际应用潜力。

Abstract: Ultra-long generation by large language models (LLMs) is a widely demanded
scenario, yet it remains a significant challenge due to their maximum
generation length limit and overall quality degradation as sequence length
increases. Previous approaches, exemplified by LongWriter, typically rely on
''teaching'', which involves supervised fine-tuning (SFT) on synthetic
long-form outputs. However, this strategy heavily depends on synthetic SFT
data, which is difficult and costly to construct, often lacks coherence and
consistency, and tends to be overly artificial and structurally monotonous. In
this work, we propose an incentivization-based approach that, starting entirely
from scratch and without relying on any annotated or synthetic data, leverages
reinforcement learning (RL) to foster the emergence of ultra-long, high-quality
text generation capabilities in LLMs. We perform RL training starting from a
base model, similar to R1-Zero, guiding it to engage in reasoning that
facilitates planning and refinement during the writing process. To support
this, we employ specialized reward models that steer the LLM towards improved
length control, writing quality, and structural formatting. Experimental
evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,
consistently outperforms traditional SFT methods on long-form writing tasks,
achieving state-of-the-art results across all metrics on WritingBench and
Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and
Qwen3-235B. We open-source our data and model checkpoints under
https://huggingface.co/THU-KEG/LongWriter-Zero-32B

</details>


### [132] [Mechanistic Interpretability Needs Philosophy](https://arxiv.org/abs/2506.18852)
*Iwan Williams,Ninell Oldenburg,Ruchira Dhar,Joshua Hatherley,Constanza Fierro,Nina Rajcic,Sandrine R. Schiller,Filippos Stamatiou,Anders Søgaard*

Main category: cs.CL

TL;DR: 本文主张机制可解释性（MI）研究需要哲学的持续合作，以澄清概念、改进方法并评估解释AI系统的认知和伦理意义。


<details>
  <summary>Details</summary>
Motivation: 随着MI领域影响力的增长，需要审视其隐含的假设、概念和解释策略，而不仅仅是模型本身。

Method: 通过分析MI文献中的三个开放问题，展示哲学对MI研究的价值。

Result: 哲学可以为MI研究提供概念澄清、方法改进和伦理评估。

Conclusion: 提倡哲学与MI研究之间更深入的跨学科对话。

Abstract: Mechanistic interpretability (MI) aims to explain how neural networks work by
uncovering their underlying causal mechanisms. As the field grows in influence,
it is increasingly important to examine not just models themselves, but the
assumptions, concepts and explanatory strategies implicit in MI research. We
argue that mechanistic interpretability needs philosophy: not as an
afterthought, but as an ongoing partner in clarifying its concepts, refining
its methods, and assessing the epistemic and ethical stakes of interpreting AI
systems. Taking three open problems from the MI literature as examples, this
position paper illustrates the value philosophy can add to MI research, and
outlines a path toward deeper interdisciplinary dialogue.

</details>


### [133] [CommVQ: Commutative Vector Quantization for KV Cache Compression](https://arxiv.org/abs/2506.18879)
*Junyan Li,Yang Zhang,Muhammad Yusuf Hassan,Talha Chafekar,Tianle Cai,Zhile Ren,Pengsheng Guo,Foroozan Karimzadeh,Colorado Reed,Chong Wang,Chuang Gan*

Main category: cs.CL

TL;DR: 提出Commutative Vector Quantization (CommVQ)方法，显著减少长上下文LLM推理中的KV缓存内存使用。


<details>
  <summary>Details</summary>
Motivation: 解决长上下文LLM推理中KV缓存成为GPU内存瓶颈的问题。

Method: 采用轻量级编码器和码本的加法量化压缩KV缓存，设计RoPE可交换的码本，并通过EM算法训练。

Result: 在2位量化下减少87.5%的FP16 KV缓存大小，支持1位量化且精度损失最小，单GPU运行128K上下文。

Conclusion: CommVQ高效减少内存使用，优于现有KV缓存量化方法，适用于长上下文LLM推理。

Abstract: Large Language Models (LLMs) are increasingly used in applications requiring
long context lengths, but the key-value (KV) cache often becomes a memory
bottleneck on GPUs as context grows. To address this, we propose Commutative
Vector Quantization (CommVQ) to significantly reduce memory usage for
long-context LLM inference. We first introduce additive quantization with a
lightweight encoder and codebook to compress the KV cache, which can be decoded
via simple matrix multiplication. To further reduce computational costs during
decoding, we design the codebook to be commutative with Rotary Position
Embedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.
This enables efficient integration of decoding into the self-attention
mechanism. Our approach achieves high accuracy with additive quantization and
low overhead via the RoPE-commutative codebook. Experiments on long-context
benchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%
with 2-bit quantization, while outperforming state-of-the-art KV cache
quantization methods. Notably, it enables 1-bit KV cache quantization with
minimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context
length on a single RTX 4090 GPU. The source code is available at:
https://github.com/UMass-Embodied-AGI/CommVQ.

</details>


### [134] [OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization](https://arxiv.org/abs/2506.18880)
*Yiyou Sun,Shawn Hu,Georgia Zhou,Ken Zheng,Hannaneh Hajishirzi,Nouha Dziri,Dawn Song*

Main category: cs.CL

TL;DR: OMEGA是一个评估LLMs在数学问题中泛化能力的基准，重点关注探索性、组合性和转化性三个维度。研究发现前沿LLMs在复杂问题中表现下降，而微调模型在探索性泛化上有提升，但组合性和转化性泛化仍有限。


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型（LLMs）在数学问题中的局限性，尤其是其在需要新颖思维方式时的表现不足。

Method: 引入OMEGA基准，通过程序生成的训练-测试对评估三个泛化维度（探索性、组合性、转化性），并对前沿LLMs和微调模型进行测试。

Result: 前沿LLMs在复杂问题中表现显著下降；微调模型在探索性泛化上有提升，但组合性和转化性泛化改进有限。

Conclusion: OMEGA为LLMs在数学创造力方面的进一步研究奠定了基础，揭示了当前模型的局限性。

Abstract: Recent large-scale language models (LLMs) with long Chain-of-Thought
reasoning-such as DeepSeek-R1-have achieved impressive results on
Olympiad-level mathematics benchmarks. However, they often rely on a narrow set
of strategies and struggle with problems that require a novel way of thinking.
To systematically investigate these limitations, we introduce
OMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a
controlled yet diverse benchmark designed to evaluate three axes of
out-of-distribution generalization, inspired by Boden's typology of creativity:
(1) Exploratory-applying known problem solving skills to more complex instances
within the same problem domain; (2) Compositional-combining distinct reasoning
skills, previously learned in isolation, to solve novel problems that require
integrating these skills in new and coherent ways; and (3)
Transformative-adopting novel, often unconventional strategies by moving beyond
familiar approaches to solve problems more effectively. OMEGA consists of
programmatically generated training-test pairs derived from templated problem
generators across geometry, number theory, algebra, combinatorics, logic, and
puzzles, with solutions verified using symbolic, numerical, or graphical
methods. We evaluate frontier (or top-tier) LLMs and observe sharp performance
degradation as problem complexity increases. Moreover, we fine-tune the
Qwen-series models across all generalization settings and observe notable
improvements in exploratory generalization, while compositional generalization
remains limited and transformative reasoning shows little to no improvement. By
isolating and quantifying these fine-grained failures, OMEGA lays the
groundwork for advancing LLMs toward genuine mathematical creativity beyond
mechanical proficiency.

</details>


### [135] [ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs](https://arxiv.org/abs/2506.18896)
*Jiaru Zou,Ling Yang,Jingwen Gu,Jiahao Qiu,Ke Shen,Jingrui He,Mengdi Wang*

Main category: cs.CL

TL;DR: ReasonFlux-PRM是一种新型的轨迹感知过程奖励模型，专门设计用于评估轨迹-响应类型的推理轨迹，通过细粒度奖励分配提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有PRM主要基于模型最终输出训练，难以稳健评估中间推理轨迹，尤其是在前沿推理模型生成的轨迹-响应输出场景中。

Method: ReasonFlux-PRM结合了步骤级和轨迹级监督，支持离线和在线奖励监督，包括高质量数据选择、密集过程级奖励和测试时扩展。

Result: 在多个基准测试中，ReasonFlux-PRM-7B表现优于现有PRM和人工基线，平均性能提升显著。

Conclusion: ReasonFlux-PRM在提升模型推理能力和效率方面具有潜力，并提供了资源受限场景下的轻量版本。

Abstract: Process Reward Models (PRMs) have recently emerged as a powerful framework
for supervising intermediate reasoning steps in large language models (LLMs).
Previous PRMs are primarily trained on model final output responses and
struggle to evaluate intermediate thinking trajectories robustly, especially in
the emerging setting of trajectory-response outputs generated by frontier
reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a
novel trajectory-aware PRM explicitly designed to evaluate the
trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both
step-level and trajectory-level supervision, enabling fine-grained reward
assignment aligned with structured chain-of-thought data. We adapt
ReasonFlux-PRM to support reward supervision under both offline and online
settings, including (i) selecting high-quality model distillation data for
downstream supervised fine-tuning of smaller models, (ii) providing dense
process-level rewards for policy optimization during reinforcement learning,
and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results
on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond
demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs
(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our
derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving
average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement
learning, and 6.3% in test-time scaling. We also release our efficient
ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.
Projects: https://github.com/Gen-Verse/ReasonFlux

</details>


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [136] [Mechanistic Interpretability of Diffusion Models: Circuit-Level Analysis and Causal Validation](https://arxiv.org/abs/2506.17237)
*Dip Roy*

Main category: cs.CV

TL;DR: 该论文通过定量电路级分析揭示了扩散模型在图像生成过程中的计算路径和机制原理，发现合成数据与自然数据处理的算法差异，并识别了八种功能不同的注意力机制。


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型在图像生成中的计算机制，以理解其行为并为生成模型的控制提供定量基础。

Method: 通过对2,000张合成图像和2,000张CelebA人脸图像进行系统性干预实验，分析计算复杂性和注意力模式。

Result: 发现真实人脸处理需要更高计算复杂度（1.084±0.008），识别了八种功能不同的注意力机制，干预实验显示性能下降25.6%至128.3%。

Conclusion: 研究为生成模型行为的算法理解和控制提供了定量基础，支持通过机制干预策略优化模型。

Abstract: We present a quantitative circuit-level analysis of diffusion models,
establishing computational pathways and mechanistic principles underlying image
generation processes. Through systematic intervention experiments across 2,000
synthetic and 2,000 CelebA facial images, we discover fundamental algorithmic
differences in how diffusion architectures process synthetic versus
naturalistic data distributions. Our investigation reveals that real-world face
processing requires circuits with measurably higher computational complexity
(complexity ratio = 1.084 plus/minus 0.008, p < 0.001), exhibiting distinct
attention specialization patterns with entropy divergence ranging from 0.015 to
0.166 across denoising timesteps. We identify eight functionally distinct
attention mechanisms showing specialized computational roles: edge detection
(entropy = 3.18 plus/minus 0.12), texture analysis (entropy = 4.16 plus/minus
0.08), and semantic understanding (entropy = 2.67 plus/minus 0.15).
Intervention analysis demonstrates critical computational bottlenecks where
targeted ablations produce 25.6% to 128.3% performance degradation, providing
causal evidence for identified circuit functions. These findings establish
quantitative foundations for algorithmic understanding and control of
generative model behavior through mechanistic intervention strategies.

</details>


### [137] [SRKD: Towards Efficient 3D Point Cloud Segmentation via Structure- and Relation-aware Knowledge Distillation](https://arxiv.org/abs/2506.17290)
*Yuqi Li,Junhao Dong,Zeyu Dong,Chuanguang Yang,Zhulin An,Yongjun Xu*

Main category: cs.CV

TL;DR: 提出了一种名为SRKD的结构和关系感知知识蒸馏框架，将大型教师模型的几何和语义知识转移到轻量级学生模型中，显著降低了模型复杂度。


<details>
  <summary>Details</summary>
Motivation: 解决3D点云分割中大规模基于Transformer的模型计算复杂和部署限制的问题。

Method: 使用亲和矩阵关系对齐模块和跨样本小批量构建策略，结合KL散度对齐语义分布和真实监督。

Result: 在显著降低模型复杂度的同时，实现了最先进的性能。

Conclusion: SRKD框架在现实部署场景中表现出高效性和有效性。

Abstract: 3D point cloud segmentation faces practical challenges due to the
computational complexity and deployment limitations of large-scale
transformer-based models. To address this, we propose a novel Structure- and
Relation-aware Knowledge Distillation framework, named SRKD, that transfers
rich geometric and semantic knowledge from a large frozen teacher model (>100M)
to a lightweight student model (<15M). Specifically, we propose an affinity
matrix-based relation alignment module, which distills structural dependencies
from the teacher to the student through point-wise similarity matching,
enhancing the student's capability to learn contextual interactions. Meanwhile,
we introduce a cross-sample mini-batch construction strategy that enables the
student to perceive stable and generalized geometric structure. This aligns
across diverse point cloud instances of the teacher, rather than within a
single sample. Additionally, KL divergence is applied to align semantic
distributions, and ground-truth supervision further reinforces accurate
segmentation. Our method achieves state of the art performance with
significantly reduced model complexity, demonstrating its effectiveness and
efficiency in real-world deployment scenarios. Our Code is available at
https://github.com/itsnotacie/SRKD.

</details>


### [138] [Fine-Scale Soil Mapping in Alaska with Multimodal Machine Learning](https://arxiv.org/abs/2506.17302)
*Yijun Lin,Theresa Chen,Colby Brungard,Grunwald Sabine,Sue Ives,Matt Macander,Timm Nawrocki,Yao-Yi Chiang,Nic Jelinski*

Main category: cs.CV

TL;DR: MISO是一种基于视觉的机器学习模型，用于生成阿拉斯加高分辨率土壤地图，优于传统方法，适用于永久冻土监测和规划。


<details>
  <summary>Details</summary>
Motivation: 阿拉斯加高分辨率土壤地图对永久冻土分布和生态系统服务至关重要，但传统方法受限。

Method: MISO结合地理空间基础模型、隐式神经表示和对比学习，进行连续空间预测和多模态对齐。

Result: MISO在空间交叉验证中表现优于随机森林，泛化能力更强，召回率更高。

Conclusion: MISO展示了先进机器学习在高分辨率土壤制图中的潜力，为永久冻土地区的规划和采样提供指导。

Abstract: Fine-scale soil mapping in Alaska, traditionally relying on fieldwork and
localized simulations, remains a critical yet underdeveloped task, despite the
region's ecological importance and extensive permafrost coverage. As permafrost
thaw accelerates due to climate change, it threatens infrastructure stability
and key ecosystem services, such as soil carbon storage. High-resolution soil
maps are essential for characterizing permafrost distribution, identifying
vulnerable areas, and informing adaptation strategies. We present MISO, a
vision-based machine learning (ML) model to produce statewide fine-scale soil
maps for near-surface permafrost and soil taxonomy. The model integrates a
geospatial foundation model for visual feature extraction, implicit neural
representations for continuous spatial prediction, and contrastive learning for
multimodal alignment and geo-location awareness. We compare MISO with Random
Forest (RF), a traditional ML model that has been widely used in soil mapping
applications. Spatial cross-validation and regional analysis across Permafrost
Zones and Major Land Resource Areas (MLRAs) show that MISO generalizes better
to remote, unseen locations and achieves higher recall than RF, which is
critical for monitoring permafrost thaw and related environmental processes.
These findings demonstrate the potential of advanced ML approaches for
fine-scale soil mapping and provide practical guidance for future soil sampling
and infrastructure planning in permafrost-affected landscapes. The project will
be released at https://github.com/knowledge-computing/Peatland-permafrost.

</details>


### [139] [RadarSeq: A Temporal Vision Framework for User Churn Prediction via Radar Chart Sequences](https://arxiv.org/abs/2506.17325)
*Sina Najafi,M. Hadi Sepanj,Fahimeh Jafari*

Main category: cs.CV

TL;DR: 提出了一种基于时间感知的计算机视觉框架，通过雷达图序列建模用户行为，结合CNN和双向LSTM提升用户流失预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决非订阅制零工平台中用户流失预测的挑战，如缺乏显式标签和动态行为特征。

Method: 使用雷达图序列编码日级行为特征，结合预训练CNN和双向LSTM捕获时空模式。

Result: 在真实数据集上表现优于传统模型和ViT基线，F1提升17.7，精确度提升29.4，AUC提升16.1。

Conclusion: 框架具有模块化设计、可解释性和高效部署特性，适用于动态零工平台的大规模流失建模。

Abstract: Predicting user churn in non-subscription gig platforms, where disengagement
is implicit, poses unique challenges due to the absence of explicit labels and
the dynamic nature of user behavior. Existing methods often rely on aggregated
snapshots or static visual representations, which obscure temporal cues
critical for early detection. In this work, we propose a temporally-aware
computer vision framework that models user behavioral patterns as a sequence of
radar chart images, each encoding day-level behavioral features. By integrating
a pretrained CNN encoder with a bidirectional LSTM, our architecture captures
both spatial and temporal patterns underlying churn behavior. Extensive
experiments on a large real-world dataset demonstrate that our method
outperforms classical models and ViT-based radar chart baselines, yielding
gains of 17.7 in F1 score, 29.4 in precision, and 16.1 in AUC, along with
improved interpretability. The framework's modular design, explainability
tools, and efficient deployment characteristics make it suitable for
large-scale churn modeling in dynamic gig-economy platforms.

</details>


### [140] [P2MFDS: A Privacy-Preserving Multimodal Fall Detection System for Elderly People in Bathroom Environments](https://arxiv.org/abs/2506.17332)
*Haitian Wang,Yiren Wang,Xinyu Wang,Yumeng Miao,Yuliang Zhang,Yu Zhang,Atif Mansoor*

Main category: cs.CV

TL;DR: 该论文提出了一种隐私保护的多模态跌倒检测系统（P2MFDS），结合毫米波雷达和3D振动传感，用于浴室环境中的老年人跌倒检测，显著提高了准确率和召回率。


<details>
  <summary>Details</summary>
Motivation: 随着老龄化加剧，浴室环境中的跌倒风险增加，现有单模态系统在复杂环境中准确性不足，亟需一种隐私保护的多模态解决方案。

Method: 开发传感器评估框架，融合毫米波雷达和3D振动传感，构建大规模隐私保护数据集；提出双流网络P2MFDS，结合CNN-BiLSTM-Attention和多尺度CNN-SEBlock-Self-Attention分支。

Result: P2MFDS在准确率和召回率上显著优于现有方法。

Conclusion: 该研究为老年人浴室跌倒检测提供了一种高效、隐私保护的解决方案，并公开了数据集和模型。

Abstract: By 2050, people aged 65 and over are projected to make up 16 percent of the
global population. As aging is closely associated with increased fall risk,
particularly in wet and confined environments such as bathrooms where over 80
percent of falls occur. Although recent research has increasingly focused on
non-intrusive, privacy-preserving approaches that do not rely on wearable
devices or video-based monitoring, these efforts have not fully overcome the
limitations of existing unimodal systems (e.g., WiFi-, infrared-, or
mmWave-based), which are prone to reduced accuracy in complex environments.
These limitations stem from fundamental constraints in unimodal sensing,
including system bias and environmental interference, such as multipath fading
in WiFi-based systems and drastic temperature changes in infrared-based
methods. To address these challenges, we propose a Privacy-Preserving
Multimodal Fall Detection System for Elderly People in Bathroom Environments.
First, we develop a sensor evaluation framework to select and fuse
millimeter-wave radar with 3D vibration sensing, and use it to construct and
preprocess a large-scale, privacy-preserving multimodal dataset in real
bathroom settings, which will be released upon publication. Second, we
introduce P2MFDS, a dual-stream network combining a CNN-BiLSTM-Attention branch
for radar motion dynamics with a multi-scale CNN-SEBlock-Self-Attention branch
for vibration impact detection. By uniting macro- and micro-scale features,
P2MFDS delivers significant gains in accuracy and recall over state-of-the-art
approaches. Code and pretrained models will be made available at:
https://github.com/HaitianWang/P2MFDS-A-Privacy-Preserving-Multimodal-Fall-Detection-Network-for-Elderly-Individuals-in-Bathroom.

</details>


### [141] [A Novel Multi-layer Task-centric and Data Quality Framework for Autonomous Driving](https://arxiv.org/abs/2506.17346)
*Yuhan Zhou,Haihua Chen,Kewei Sha*

Main category: cs.CV

TL;DR: 本文提出了一种以任务为中心、数据质量为基础的五层框架，旨在将数据质量与任务需求和性能目标映射，以提高自动驾驶车辆的功能性、效率和可信度。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶车辆依赖多源多模态数据，但数据质量常因环境或传感器问题而波动，而现有研究多关注模型/算法，忽视数据质量。本文旨在填补这一空白。

Method: 提出五层框架（数据层、DQ层、任务层、应用层、目标层），并通过nuScenes数据集案例研究验证冗余去除对YOLOv8目标检测性能的提升。

Result: 案例研究表明，部分去除多源图像数据的冗余可提升目标检测性能，同时揭示了图像和LiDAR数据中的冗余问题。

Conclusion: 本文为自动驾驶领域的数据质量、任务编排和性能导向系统开发提供了新视角，有望推动构建更具适应性、可解释性和鲁棒性的自动驾驶系统。

Abstract: The next-generation autonomous vehicles (AVs), embedded with frequent
real-time decision-making, will rely heavily on a large volume of multisource
and multimodal data. In real-world settings, the data quality (DQ) of different
sources and modalities usually varies due to unexpected environmental factors
or sensor issues. However, both researchers and practitioners in the AV field
overwhelmingly concentrate on models/algorithms while undervaluing the DQ. To
fulfill the needs of the next-generation AVs with guarantees of functionality,
efficiency, and trustworthiness, this paper proposes a novel task-centric and
data quality vase framework which consists of five layers: data layer, DQ
layer, task layer, application layer, and goal layer. The proposed framework
aims to map DQ with task requirements and performance goals. To illustrate, a
case study investigating redundancy on the nuScenes dataset proves that
partially removing redundancy on multisource image data could improve YOLOv8
object detection task performance. Analysis on multimodal data of image and
LiDAR further presents existing redundancy DQ issues. This paper opens up a
range of critical but unexplored challenges at the intersection of DQ, task
orchestration, and performance-oriented system development in AVs. It is
expected to guide the AV community toward building more adaptive, explainable,
and resilient AVs that respond intelligently to dynamic environments and
heterogeneous data streams. Code, data, and implementation details are publicly
available at: https://anonymous.4open.science/r/dq4av-framework/README.md.

</details>


### [142] [Efficient Feedback Gate Network for Hyperspectral Image Super-Resolution](https://arxiv.org/abs/2506.17361)
*Xufei Wang,Mingjian Zhang,Fei Ge,Jinchen Zhu,Wen Sha,Jifen Ren,Zhimeng Hou,Shouguo Zheng,ling Zheng,Shizhuang Weng*

Main category: cs.CV

TL;DR: 提出了一种基于分组的高效反馈门网络方法，用于单幅高光谱图像超分辨率（SHSR），通过反馈门操作和空间-光谱特征增强模块显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有SHSR方法未能充分利用波段间和空间-光谱信息的一致性，导致性能受限。

Method: 采用反馈门操作、大核卷积和光谱交互，结合通道混洗和扩张卷积的SPDFM模块，以及空间-光谱强化门模块（SSRGM）。

Result: 在三个高光谱数据集上验证了该方法在光谱保真度和空间内容重建方面的优越性。

Conclusion: 所提方法通过高效探索波段和空间-光谱信息，显著提升了SHSR的性能。

Abstract: Even without auxiliary images, single hyperspectral image super-resolution
(SHSR) methods can be designed to improve the spatial resolution of
hyperspectral images. However, failing to explore coherence thoroughly along
bands and spatial-spectral information leads to the limited performance of the
SHSR. In this study, we propose a novel group-based SHSR method termed the
efficient feedback gate network, which uses various feedbacks and gate
operations involving large kernel convolutions and spectral interactions. In
particular, by providing different guidance for neighboring groups, we can
learn rich band information and hierarchical hyperspectral spatial information
using channel shuffling and dilatation convolution in shuffled and progressive
dilated fusion module(SPDFM). Moreover, we develop a wide-bound perception gate
block and a spectrum enhancement gate block to construct the spatial-spectral
reinforcement gate module (SSRGM) and obtain highly representative
spatial-spectral features efficiently. Additionally, we apply a
three-dimensional SSRGM to enhance holistic information and coherence for
hyperspectral data. The experimental results on three hyperspectral datasets
demonstrate the superior performance of the proposed network over the
state-of-the-art methods in terms of spectral fidelity and spatial content
reconstruction.

</details>


### [143] [From Drawings to Decisions: A Hybrid Vision-Language Framework for Parsing 2D Engineering Drawings into Structured Manufacturing Knowledge](https://arxiv.org/abs/2506.17374)
*Muhammad Tayyab Khan,Lequn Chen,Zane Yong,Jun Ming Tan,Wenhe Feng,Seung Ki Moon*

Main category: cs.CV

TL;DR: 提出了一种结合旋转感知目标检测模型和视觉语言解析器的混合框架，用于从2D工程图中高效提取关键信息。


<details>
  <summary>Details</summary>
Motivation: 手动提取工程图中的关键信息效率低且不可靠，通用OCR模型因复杂布局和工程符号表现不佳。

Method: 使用YOLOv11-OBB定位注释并提取旋转边界框，再通过微调的视觉语言模型（Donut和Florence-2）解析为结构化输出。

Result: Donut模型表现优于Florence-2，精确率达88.5%，召回率99.2%，F1分数93.5%，幻觉率11.5%。

Conclusion: 该框架能有效支持下游制造任务，展示了其在现代化2D图纸解析中的实用价值。

Abstract: Efficient and accurate extraction of key information from 2D engineering
drawings is essential for advancing digital manufacturing workflows. Such
information includes geometric dimensioning and tolerancing (GD&T), measures,
material specifications, and textual annotations. Manual extraction is slow and
labor-intensive, while generic OCR models often fail due to complex layouts,
engineering symbols, and rotated text, leading to incomplete and unreliable
outputs. These limitations result in incomplete and unreliable outputs. To
address these challenges, we propose a hybrid vision-language framework that
integrates a rotation-aware object detection model (YOLOv11-obb) with a
transformer-based vision-language parser. Our structured pipeline applies
YOLOv11-OBB to localize annotations and extract oriented bounding box (OBB)
patches, which are then parsed into structured outputs using a fine-tuned,
lightweight vision-language model (VLM). We curate a dataset of 1,367 2D
mechanical drawings annotated across nine key categories. YOLOv11-OBB is
trained on this dataset to detect OBBs and extract annotation patches. These
are parsed using two open-source VLMs: Donut and Florence-2. Both models are
lightweight and well-suited for specialized industrial tasks under limited
computational overhead. Following fine-tuning of both models on the curated
dataset of image patches paired with structured annotation labels, a
comparative experiment is conducted to evaluate parsing performance across four
key metrics. Donut outperforms Florence-2, achieving 88.5% precision, 99.2%
recall, and a 93.5% F1-score, with a hallucination rate of 11.5%. Finally, a
case study demonstrates how the extracted structured information supports
downstream manufacturing tasks such as process and tool selection, showcasing
the practical utility of the proposed framework in modernizing 2D drawing
interpretation.

</details>


### [144] [Spatial-Temporal Pre-Training for Embryo Viability Prediction Using Time-Lapse Videos](https://arxiv.org/abs/2506.17403)
*Zhiyi Shi,Junsik Kim,Helen Y. Yang,Yonghyun Song,Hyun-Jic Oh,Dalit Ben-Yosef,Daniel Needleman,Hanspeter Pfister*

Main category: cs.CV

TL;DR: 提出了一种名为STPT的自监督学习方法，用于解决胚胎发育视频中的长视频和时序对齐问题，显著提高了IVF胚胎存活预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 由于胚胎移植后标记数据有限，且现有自监督学习方法难以处理胚胎发育视频的长序列和时序不一致问题，需要一种更高效的方法。

Method: STPT分为空间和时间两个阶段，分别训练一个编码器并冻结另一个，以减少内存需求，并通过视频内对齐和时序建模解决对齐问题。

Result: 在23,027个视频（3,286个标记）上，STPT实现了0.635的AUC（95% CI: 0.632-0.638），优于基线方法。

Conclusion: STPT是一种高效且资源友好的方法，显著提升了胚胎存活预测的性能。

Abstract: Automating embryo viability prediction for in vitro fertilization (IVF) is
important but challenging due to the limited availability of labeled pregnancy
outcome data, as only a small fraction of embryos are labeled after transfer.
Self-supervised learning (SSL) can leverage both labeled and unlabeled data to
improve prediction. However, existing SSL methods for videos are not directly
applicable to embryo development videos due to two challenges: (1) embryo
time-lapse videos contain hundreds of frames, requiring significant GPU memory
for conventional SSL; (2) the dataset contains videos with varying lengths and
many outlier frames, causing traditional video alignment methods to struggle
with semantic misalignment. We propose Spatial-Temporal Pre-Training (STPT) to
address these challenges. STPT includes two stages: spatial and temporal. In
each stage, only one encoder is trained while the other is frozen, reducing
memory demands. To handle temporal misalignment, STPT avoids frame-by-frame
alignment across videos. The spatial stage learns from alignments within each
video and its temporally consistent augmentations. The temporal stage then
models relationships between video embeddings. Our method efficiently handles
long videos and temporal variability. On 23,027 time-lapse videos (3,286
labeled), STPT achieves the highest AUC of 0.635 (95% CI: 0.632-0.638) compared
to baselines, with limited computational resources.

</details>


### [145] [VMRA-MaR: An Asymmetry-Aware Temporal Framework for Longitudinal Breast Cancer Risk Prediction](https://arxiv.org/abs/2506.17412)
*Zijun Sun,Solveig Thrun,Michael Kampffmeyer*

Main category: cs.CV

TL;DR: 提出了一种基于Vision Mamba RNN（VMRNN）和状态空间模型（SSM）的方法，结合LSTM机制和不对称模块，用于乳腺癌风险预测，显著提高了高密度乳腺病例的预测性能。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌是全球主要死因之一，现有筛查方法多依赖单次筛查数据，而临床实践表明利用时间序列数据能更好捕捉乳腺组织变化趋势。

Method: 采用VMRNN结合SSM和LSTM机制，引入不对称模块（SAD和LAT）以捕捉双侧乳腺差异，提升时间序列数据的动态建模能力。

Result: 在乳腺癌发病预测中表现优异，尤其对高密度乳腺病例和远期时间点（第4和第5年）预测效果显著提升。

Conclusion: 该方法有望推动乳腺癌早期识别和个性化筛查策略的发展，代码已开源。

Abstract: Breast cancer remains a leading cause of mortality worldwide and is typically
detected via screening programs where healthy people are invited in regular
intervals. Automated risk prediction approaches have the potential to improve
this process by facilitating dynamically screening of high-risk groups. While
most models focus solely on the most recent screening, there is growing
interest in exploiting temporal information to capture evolving trends in
breast tissue, as inspired by clinical practice. Early methods typically relied
on two time steps, and although recent efforts have extended this to multiple
time steps using Transformer architectures, challenges remain in fully
harnessing the rich temporal dynamics inherent in longitudinal imaging data. In
this work, we propose to instead leverage Vision Mamba RNN (VMRNN) with a
state-space model (SSM) and LSTM-like memory mechanisms to effectively capture
nuanced trends in breast tissue evolution. To further enhance our approach, we
incorporate an asymmetry module that utilizes a Spatial Asymmetry Detector
(SAD) and Longitudinal Asymmetry Tracker (LAT) to identify clinically relevant
bilateral differences. This integrated framework demonstrates notable
improvements in predicting cancer onset, especially for the more challenging
high-density breast cases and achieves superior performance at extended time
points (years four and five), highlighting its potential to advance early
breast cancer recognition and enable more personalized screening strategies.
Our code is available at https://github.com/Mortal-Suen/VMRA-MaR.git.

</details>


### [146] [Trans${^2}$-CBCT: A Dual-Transformer Framework for Sparse-View CBCT Reconstruction](https://arxiv.org/abs/2506.17425)
*Minmin Yang,Huantao Ren,Senem Velipasalar*

Main category: cs.CV

TL;DR: 论文提出了一种结合CNN和Transformer的混合模型Trans-CBCT，用于稀疏视图CBCT重建，并通过引入邻居感知的Point Transformer进一步优化，显著提升了图像质量。


<details>
  <summary>Details</summary>
Motivation: 稀疏视图CBCT扫描速度快且辐射剂量低，但会导致严重的欠采样伪影和空间覆盖不足。论文旨在解决这些问题。

Method: 1. 使用TransUNet（CNN-Transformer混合模型）替代传统编码器，结合多尺度特征和轻量级衰减预测头。2. 引入邻居感知的Point Transformer，通过3D位置编码和k近邻注意力增强空间一致性。

Result: 在LUNA16数据集上，Trans-CBCT比基线模型PSNR提升1.17 dB，SSIM提升0.0163；Trans²-CBCT进一步提升了0.63 dB PSNR和0.0117 SSIM。

Conclusion: 结合CNN-Transformer特征和基于点的几何推理，能够有效提升稀疏视图CBCT的重建质量。

Abstract: Cone-beam computed tomography (CBCT) using only a few X-ray projection views
enables faster scans with lower radiation dose, but the resulting severe
under-sampling causes strong artifacts and poor spatial coverage. We address
these challenges in a unified framework. First, we replace conventional
UNet/ResNet encoders with TransUNet, a hybrid CNN-Transformer model.
Convolutional layers capture local details, while self-attention layers enhance
global context. We adapt TransUNet to CBCT by combining multi-scale features,
querying view-specific features per 3D point, and adding a lightweight
attenuation-prediction head. This yields Trans-CBCT, which surpasses prior
baselines by 1.17 dB PSNR and 0.0163 SSIM on the LUNA16 dataset with six views.
Second, we introduce a neighbor-aware Point Transformer to enforce volumetric
coherence. This module uses 3D positional encoding and attention over k-nearest
neighbors to improve spatial consistency. The resulting model, Trans$^2$-CBCT,
provides an additional gain of 0.63 dB PSNR and 0.0117 SSIM. Experiments on
LUNA16 and ToothFairy show consistent gains from six to ten views, validating
the effectiveness of combining CNN-Transformer features with point-based
geometry reasoning for sparse-view CBCT reconstruction.

</details>


### [147] [Enhancing Wireless Device Identification through RF Fingerprinting: Leveraging Transient Energy Spectrum Analysis](https://arxiv.org/abs/2506.17439)
*Nisar Ahmed,Gulshan Saleem,Hafiz Muhammad Shahzad Asif,Muhammad Usman Younus,Kalsoom Safdar*

Main category: cs.CV

TL;DR: 该研究提出了一种基于瞬态能量谱分析和混合深度学习模型CNN-Bi-GRU的方法，用于在复杂电磁环境中准确识别和分类射频设备，取得了高精度的分类性能。


<details>
  <summary>Details</summary>
Motivation: 随着物联网技术和5G网络的快速发展，复杂电磁环境中的辐射设备数量激增，准确识别和分类这些设备成为关键挑战。

Method: 利用广义线性调频小波变换提取射频设备的瞬态能量谱特征，并采用CNN-Bi-GRU混合深度学习模型进行分类建模。

Result: 实验结果显示，CNN-Bi-GRU模型的分类精度达到99.17%，精确率、召回率和F1分数均超过99%。

Conclusion: 该方法在射频设备识别中表现出色，有望提升复杂无线环境中的设备识别和分类能力。

Abstract: In recent years, the rapid growth of the Internet of Things technologies and
the widespread adoption of 5G wireless networks have led to an exponential
increase in the number of radiation devices operating in complex
electromagnetic environments. A key challenge in managing and securing these
devices is accurate identification and classification. To address this
challenge, specific emitter identification techniques have emerged as a
promising solution that aims to provide reliable and efficient means of
identifying individual radiation devices in a unified and standardized manner.
This research proposes an approach that leverages transient energy spectrum
analysis using the General Linear Chirplet Transform to extract features from
RF devices. A dataset comprising nine RF devices is utilized, with each sample
containing 900 attributes and a total of 1080 equally distributed samples
across the devices. These features are then used in a classification modeling
framework. To overcome the limitations of conventional machine learning
methods, we introduce a hybrid deep learning model called the CNN-Bi-GRU for
learning the identification of RF devices based on their transient
characteristics. The proposed approach provided a 10-fold cross-validation
performance with a precision of 99.33%, recall of 99.53%, F1-score of 99.43%,
and classification accuracy of 99.17%. The results demonstrate the promising
classification performance of the CNN-Bi-GRU approach, indicating its
suitability for accurately identifying RF devices based on their transient
characteristics and its potential for enhancing device identification and
classification in complex wireless environments.

</details>


### [148] [AQUA20: A Benchmark Dataset for Underwater Species Classification under Challenging Conditions](https://arxiv.org/abs/2506.17455)
*Taufikur Rahman Fuad,Sabbir Ahmed,Shahriar Ivan*

Main category: cs.CV

TL;DR: AQUA20是一个包含8,171张水下图像的数据集，用于评估13种深度学习模型在复杂水下环境中的海洋物种分类性能。ConvNeXt表现最佳，但仍有改进空间。


<details>
  <summary>Details</summary>
Motivation: 水下视觉识别因浑浊、低光照和遮挡等复杂失真问题而具有挑战性，需要新的数据集和模型评估方法。

Method: 使用AQUA20数据集评估13种深度学习模型（包括轻量级CNN和Transformer架构）的性能，并进行可解释性分析（GRAD-CAM和LIME）。

Result: ConvNeXt表现最佳，Top-3准确率达98.82%，Top-1准确率为90.69%，F1分数为88.92%。其他模型在复杂性和性能之间存在权衡。

Conclusion: AQUA20为水下物种识别研究提供了基础，未来仍有改进空间。数据集已公开。

Abstract: Robust visual recognition in underwater environments remains a significant
challenge due to complex distortions such as turbidity, low illumination, and
occlusion, which severely degrade the performance of standard vision systems.
This paper introduces AQUA20, a comprehensive benchmark dataset comprising
8,171 underwater images across 20 marine species reflecting real-world
environmental challenges such as illumination, turbidity, occlusions, etc.,
providing a valuable resource for underwater visual understanding. Thirteen
state-of-the-art deep learning models, including lightweight CNNs (SqueezeNet,
MobileNetV2) and transformer-based architectures (ViT, ConvNeXt), were
evaluated to benchmark their performance in classifying marine species under
challenging conditions. Our experimental results show ConvNeXt achieving the
best performance, with a Top-3 accuracy of 98.82% and a Top-1 accuracy of
90.69%, as well as the highest overall F1-score of 88.92% with moderately large
parameter size. The results obtained from our other benchmark models also
demonstrate trade-offs between complexity and performance. We also provide an
extensive explainability analysis using GRAD-CAM and LIME for interpreting the
strengths and pitfalls of the models. Our results reveal substantial room for
improvement in underwater species recognition and demonstrate the value of
AQUA20 as a foundation for future research in this domain. The dataset is
publicly available at: https://huggingface.co/datasets/taufiktrf/AQUA20.

</details>


### [149] [When Every Millisecond Counts: Real-Time Anomaly Detection via the Multimodal Asynchronous Hybrid Network](https://arxiv.org/abs/2506.17457)
*Dong Xiao,Guangyao Chen,Peixi Peng,Yangru Huang,Yifan Zhao,Yongxing Dai,Yonghong Tian*

Main category: cs.CV

TL;DR: 提出了一种实时异常检测方法，结合事件相机和RGB相机数据，通过异步图神经网络和CNN实现高精度和快速响应。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视响应时间，而自动驾驶需要快速准确的异常检测。

Method: 提出多模态异步混合网络，结合事件相机的事件流和RGB相机的图像数据，利用异步GNN和CNN提取时空特征。

Result: 在基准数据集上表现优于现有方法，实现毫秒级实时性能。

Conclusion: 该方法在自动驾驶中实现了快速且高精度的异常检测。

Abstract: Anomaly detection is essential for the safety and reliability of autonomous
driving systems. Current methods often focus on detection accuracy but neglect
response time, which is critical in time-sensitive driving scenarios. In this
paper, we introduce real-time anomaly detection for autonomous driving,
prioritizing both minimal response time and high accuracy. We propose a novel
multimodal asynchronous hybrid network that combines event streams from event
cameras with image data from RGB cameras. Our network utilizes the high
temporal resolution of event cameras through an asynchronous Graph Neural
Network and integrates it with spatial features extracted by a CNN from RGB
images. This combination effectively captures both the temporal dynamics and
spatial details of the driving environment, enabling swift and precise anomaly
detection. Extensive experiments on benchmark datasets show that our approach
outperforms existing methods in both accuracy and response time, achieving
millisecond-level real-time performance.

</details>


### [150] [Photogranulometry -- Dataset of soil images with corresponding particle size distributions](https://arxiv.org/abs/2506.17469)
*Thomas Plante St-Cyr,François Duhaime,Jean-Sébastien Dubé,Simon Grenier*

Main category: cs.CV

TL;DR: 提出了一种基于光学粒度分析的土壤粒度分布（PSD）方法，以减少传统PSD分析的时间和成本。


<details>
  <summary>Details</summary>
Motivation: 传统PSD分析耗时且成本高，希望通过光学分析集成到常规实验室工作流中来解决这些问题。

Method: 使用高分辨率相机（45 MP）拍摄321种土壤样本的12,714张图像，标准化拍摄条件，并采用自定义测试台和分样方法。

Result: 提供了高质量的数据集，可用于训练卷积神经网络（CNN）进行地质技术应用。

Conclusion: 该方法为PSD分析提供了高效且经济的替代方案，并支持未来AI在土壤分析中的应用。

Abstract: Traditional particle size distribution (PSD) analyses create significant
downtime and are expensive in labor and maintenance. These drawbacks could be
alleviated using optical grain size analysis integrated into routine
geotechnical laboratory workflow. This paper presents a high-resolution dataset
of 12,714 images of 321 different soil samples collected in the Montreal,
Quebec region, alongside their PSD analysis. It is designed to provide a robust
starting point for training convolutional neural networks (CNN) in geotechnical
applications. Soil samples were photographed in a standardized top-view
position with a resolution of 45 MP and a minimum scale of 39.4 micrometers per
pixel, both in their moist and dry states. A custom test bench employing 13x9
inch white aluminum trays, on which the samples are spread in a thin layer, was
used. For samples exceeding a size limit, a coning and quartering method was
employed for mass reduction.

</details>


### [151] [Few-Shot, Now for Real: Medical VLMs Adaptation without Balanced Sets or Validation](https://arxiv.org/abs/2506.17500)
*Julio Silva-Rodríguez,Fereshteh Shakeri,Houda Bahig,Jose Dolz,Ismail Ben Ayed*

Main category: cs.CV

TL;DR: 论文探讨了医学图像分析中视觉语言模型（VLMs）的适应性问题，提出了一种无需验证集的线性探针方法，以应对现实中的不平衡数据分布。


<details>
  <summary>Details</summary>
Motivation: 现有VLMs在医学领域的适应方法假设数据分布平衡且需要验证集，这与现实中的不平衡疾病分布和数据效率需求不符。

Method: 提出了一种无需训练、自适应融合视觉和文本监督的线性探针方法。

Result: 实验表明，现有方法在现实条件下性能下降，而提出的方法在多种模态和任务中表现稳健。

Conclusion: 该方法为医学图像分析中的不平衡数据适应提供了高效且鲁棒的解决方案。

Abstract: Vision-language models (VLMs) are gaining attention in medical image
analysis. These are pre-trained on large, heterogeneous data sources, yielding
rich and transferable representations. Notably, the combination of
modality-specialized VLMs with few-shot adaptation has provided fruitful
results, enabling the efficient deployment of high-performing solutions.
However, previous works on this topic make strong assumptions about the
distribution of adaptation data, which are unrealistic in the medical domain.
First, prior art assumes access to a balanced support set, a condition that
breaks the natural imbalance in disease prevalence found in real-world
scenarios. Second, these works typically assume the presence of an additional
validation set to fix critical hyper-parameters, which is highly
data-inefficient. This work challenges these favorable deployment scenarios and
introduces a realistic, imbalanced, validation-free adaptation setting. Our
extensive benchmark across various modalities and downstream tasks demonstrates
that current methods systematically compromise their performance when operating
under realistic conditions, occasionally even performing worse than zero-shot
inference. Also, we introduce a training-free linear probe that adaptively
blends visual and textual supervision. Detailed studies demonstrate that the
proposed solver is a strong, efficient baseline, enabling robust adaptation in
challenging scenarios.

</details>


### [152] [Trustworthy Few-Shot Transfer of Medical VLMs through Split Conformal Prediction](https://arxiv.org/abs/2506.17503)
*Julio Silva-Rodríguez,Ismail Ben Ayed,Jose Dolz*

Main category: cs.CV

TL;DR: 该论文提出了一种新的方法SCA-T，用于在医学视觉语言模型（VLM）的迁移学习中提供可信度保证，解决了传统方法在适应性阶段破坏数据交换性假设的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管医学视觉语言模型在数据高效图像分类中表现出色，但其可靠性尚未充分研究。本文旨在通过改进的SCP框架提供可信度保证。

Method: 提出了SCA-T方法，通过无监督的转导式适应联合校准和测试数据，解决了传统方法破坏数据交换性假设的问题。

Result: 实验表明，SCA-T在效率和条件覆盖方面优于传统SCP方法，同时保持了相同的经验保证。

Conclusion: SCA-T为医学VLM的迁移学习提供了一种更可靠的解决方案，适用于多种图像模态和任务。

Abstract: Medical vision-language models (VLMs) have demonstrated unprecedented
transfer capabilities and are being increasingly adopted for data-efficient
image classification. Despite its growing popularity, its reliability aspect
remains largely unexplored. This work explores the split conformal prediction
(SCP) framework to provide trustworthiness guarantees when transferring such
models based on a small labeled calibration set. Despite its potential, the
generalist nature of the VLMs' pre-training could negatively affect the
properties of the predicted conformal sets for specific tasks. While common
practice in transfer learning for discriminative purposes involves an
adaptation stage, we observe that deploying such a solution for conformal
purposes is suboptimal since adapting the model using the available calibration
data breaks the rigid exchangeability assumptions for test data in SCP. To
address this issue, we propose transductive split conformal adaptation (SCA-T),
a novel pipeline for transfer learning on conformal scenarios, which performs
an unsupervised transductive adaptation jointly on calibration and test data.
We present comprehensive experiments utilizing medical VLMs across various
image modalities, transfer tasks, and non-conformity scores. Our framework
offers consistent gains in efficiency and conditional coverage compared to SCP,
maintaining the same empirical guarantees.

</details>


### [153] [Learning golf swing signatures from a single wrist-worn inertial sensor](https://arxiv.org/abs/2506.17505)
*Jessy Lauer*

Main category: cs.CV

TL;DR: 提出了一种基于手腕传感器的个性化高尔夫挥杆分析框架，通过数据驱动方法重建全身运动并检测技术缺陷。


<details>
  <summary>Details</summary>
Motivation: 解决高尔夫挥杆分析中孤立指标、专业运动员数据不足及缺乏可解释运动表征的问题。

Method: 利用公开视频构建专业挥杆数据集，通过生物准确的人体网格恢复重建3D运动，训练神经网络从手腕数据推断运动并分段挥杆。

Result: 系统能准确估计全身运动和挥杆事件，提供实验室级分析，并揭示个性化运动特征。

Conclusion: 挑战了挥杆一致性和单一“理想”挥杆的假设，为研究、教练和运动损伤预防提供了可扩展的高保真运动分析。

Abstract: Despite its importance for performance and injury prevention, golf swing
analysis is limited by isolated metrics, underrepresentation of professional
athletes, and a lack of rich, interpretable movement representations. We
address these gaps with a holistic, data-driven framework for personalized golf
swing analysis from a single wrist-worn sensor. We build a large dataset of
professional swings from publicly available videos, reconstruct full-body 3D
kinematics using biologically accurate human mesh recovery, and generate
synthetic inertial data to train neural networks that infer motion and segment
swing phases from wrist-based input. We learn a compositional, discrete
vocabulary of motion primitives that facilitates the detection and
visualization of technical flaws, and is expressive enough to predict player
identity, club type, sex, and age. Our system accurately estimates full-body
kinematics and swing events from wrist data, delivering lab-grade motion
analysis on-course and supporting early detection of anomalous movement
patterns. Explainability methods reveal subtle, individualized movement
signatures, reinforcing the view that variability is a hallmark of skilled
performance. Longitudinal tracking demonstrates practical value: as one
player's handicap improved from 50 to 2.2 over 1.5 years, our system captured
measurable technical progress and provided targeted, actionable feedback. Our
findings challenge common assumptions, such as swing consistency across clubs
and the existence of a single "ideal" swing, and uncover latent biomarkers
shaped by both intrinsic traits and task-specific constraints. This work
bridges lab and field-based biomechanics, offering scalable, accessible,
high-fidelity motion analysis for research, coaching, and injury prevention,
while opening new directions in movement-based phenotyping, personalized
equipment design, and motor skill development.

</details>


### [154] [Scene-R1: Video-Grounded Large Language Models for 3D Scene Reasoning without 3D Annotations](https://arxiv.org/abs/2506.17545)
*Zhihao Yuan,Shuyi Jiang,Chun-Mei Feng,Yaolun Zhang,Shuguang Cui,Zhen Li,Na Zhao*

Main category: cs.CV

TL;DR: Scene-R1是一个基于视频的框架，通过强化学习驱动的推理和两阶段接地流程，无需3D实例监督即可理解3D场景。


<details>
  <summary>Details</summary>
Motivation: 现有3D感知的LLMs是黑箱，依赖预训练的3D检测器提供对象提案，缺乏透明性。Scene-R1旨在消除对3D检测器的依赖，并提供逐步推理。

Method: 结合强化学习驱动的推理和两阶段接地流程：时间接地阶段选择相关视频片段，图像接地阶段预测2D边界框，并通过SAM2生成像素级掩码，投影回3D。

Result: Scene-R1在多个数据集上超越现有开放词汇基线，提供透明的逐步推理，且仅需任务级2D框或文本标签。

Conclusion: 结合强化学习和RGB-D视频，Scene-R1提供了一种高效且可信的3D场景理解方法。

Abstract: Currently, utilizing large language models to understand the 3D world is
becoming popular. Yet existing 3D-aware LLMs act as black boxes: they output
bounding boxes or textual answers without revealing how those decisions are
made, and they still rely on pre-trained 3D detectors to supply object
proposals. We introduce Scene-R1, a video-grounded framework that learns to
reason about 3D scenes without any point-wise 3D instance supervision by
pairing reinforcement-learning-driven reasoning with a two-stage grounding
pipeline. In the temporal grounding stage, we explicitly reason about the video
and select the video snippets most relevant to an open-ended query. In the
subsequent image grounding stage, we analyze the image and predict the 2D
bounding box. After that, we track the object using SAM2 to produce
pixel-accurate masks in RGB frames, and project them back into 3D, thereby
eliminating the need for 3D detector-based proposals while capturing fine
geometry and material cues. Scene-R1 can also adapt to the 3D visual question
answering task to answer free-form questions directly from video. Our training
pipeline only needs task-level 2D boxes or textual labels without dense 3D
point-wise labels. Scene-R1 surpasses existing open-vocabulary baselines on
multiple datasets, while delivering transparent, step-by-step rationales. These
results show that reinforcement-learning-based reasoning combined with RGB-D
video alone offers a practical, annotation-efficient route to trustworthy 3D
scene understanding.

</details>


### [155] [SynDaCaTE: A Synthetic Dataset For Evaluating Part-Whole Hierarchical Inference](https://arxiv.org/abs/2506.17558)
*Jake Levi,Mark van der Wilk*

Main category: cs.CV

TL;DR: 论文提出SynDaCaTE数据集，用于评估胶囊网络是否真正学习到部分-整体层次结构，并展示了现有模型的瓶颈及自注意力机制的有效性。


<details>
  <summary>Details</summary>
Motivation: 解决现有胶囊网络在监督任务中难以验证是否真正学习到部分-整体层次结构的问题。

Method: 提出合成数据集SynDaCaTE，并通过实验分析现有胶囊模型的瓶颈及自注意力机制的效果。

Result: 发现现有胶囊模型的瓶颈，并证明自注意力机制在部分-整体推理中的高效性。

Conclusion: SynDaCaTE为胶囊网络评估提供工具，自注意力机制为未来设计计算机视觉的归纳偏置指明方向。

Abstract: Learning to infer object representations, and in particular part-whole
hierarchies, has been the focus of extensive research in computer vision, in
pursuit of improving data efficiency, systematic generalisation, and
robustness. Models which are \emph{designed} to infer part-whole hierarchies,
often referred to as capsule networks, are typically trained end-to-end on
supervised tasks such as object classification, in which case it is difficult
to evaluate whether such a model \emph{actually} learns to infer part-whole
hierarchies, as claimed. To address this difficulty, we present a SYNthetic
DAtaset for CApsule Testing and Evaluation, abbreviated as SynDaCaTE, and
establish its utility by (1) demonstrating the precise bottleneck in a
prominent existing capsule model, and (2) demonstrating that
permutation-equivariant self-attention is highly effective for parts-to-wholes
inference, which motivates future directions for designing effective inductive
biases for computer vision.

</details>


### [156] [VLA-OS: Structuring and Dissecting Planning Representations and Paradigms in Vision-Language-Action Models](https://arxiv.org/abs/2506.17561)
*Chongkai Gao,Zixuan Liu,Zhenghao Chi,Junshan Huang,Xin Fei,Yiwen Hou,Yuxuan Zhang,Yudi Lin,Zhirui Fang,Zeyu Jiang,Lin Shao*

Main category: cs.CV

TL;DR: 论文研究了视觉-语言-动作（VLA）模型中任务规划和动作生成的不同范式，提出了统一的VLA-OS架构，并通过实验验证了视觉基础规划表示和分层-VLA范式的优势。


<details>
  <summary>Details</summary>
Motivation: 现有VLA模型在任务规划和动作生成上的方法差异大，难以确定性能提升的具体来源，因此需要系统研究不同规划和表示范式的影响。

Method: 提出VLA-OS统一架构，设计多维度实验（物体类别、视觉模态、环境、末端执行器）比较不同规划和表示范式。

Result: 视觉基础规划表示优于语言表示；分层-VLA范式在任务性能、泛化能力等方面表现更优，但训练和推理速度较慢。

Conclusion: 视觉基础规划和分层-VLA范式是VLA模型的优选方案，但需权衡速度与性能。

Abstract: Recent studies on Vision-Language-Action (VLA) models have shifted from the
end-to-end action-generation paradigm toward a pipeline involving task planning
followed by action generation, demonstrating improved performance on various
complex, long-horizon manipulation tasks. However, existing approaches vary
significantly in terms of network architectures, planning paradigms,
representations, and training data sources, making it challenging for
researchers to identify the precise sources of performance gains and components
to be further improved. To systematically investigate the impacts of different
planning paradigms and representations isolating from network architectures and
training data, in this paper, we introduce VLA-OS, a unified VLA architecture
series capable of various task planning paradigms, and design a comprehensive
suite of controlled experiments across diverse object categories (rigid and
deformable), visual modalities (2D and 3D), environments (simulation and
real-world), and end-effectors (grippers and dexterous hands). Our results
demonstrate that: 1) visually grounded planning representations are generally
better than language planning representations; 2) the Hierarchical-VLA paradigm
generally achieves superior or comparable performance than other paradigms on
task performance, pretraining, generalization ability, scalability, and
continual learning ability, albeit at the cost of slower training and inference
speeds.

</details>


### [157] [LLM-driven Medical Report Generation via Communication-efficient Heterogeneous Federated Learning](https://arxiv.org/abs/2506.17562)
*Haoxuan Che,Haibo Jin,Zhengrui Guo,Yi Lin,Cheng Jin,Hao Chen*

Main category: cs.CV

TL;DR: FedMRG是一个基于联邦学习的框架，用于隐私保护的多中心LLM驱动的医学报告生成，解决了通信效率和数据异质性挑战。


<details>
  <summary>Details</summary>
Motivation: 医学图像-报告数据分散且隐私敏感，集中化困难，阻碍了LLM驱动的医学报告生成模型的发展。

Method: 采用低秩分解减少通信开销，结合客户端感知对比学习和双适配器机制处理数据异质性。

Result: FedMRG在多中心数据中表现出良好的泛化性和适应性，同时保持通信效率和临床准确性。

Conclusion: FedMRG为多中心LLM驱动的医学报告生成提供了可行的隐私保护解决方案。

Abstract: LLMs have demonstrated significant potential in Medical Report Generation
(MRG), yet their development requires large amounts of medical image-report
pairs, which are commonly scattered across multiple centers. Centralizing these
data is exceptionally challenging due to privacy regulations, thereby impeding
model development and broader adoption of LLM-driven MRG models. To address
this challenge, we present FedMRG, the first framework that leverages Federated
Learning (FL) to enable privacy-preserving, multi-center development of
LLM-driven MRG models, specifically designed to overcome the critical challenge
of communication-efficient LLM training under multi-modal data heterogeneity.
To start with, our framework tackles the fundamental challenge of communication
overhead in FL-LLM tuning by employing low-rank factorization to efficiently
decompose parameter updates, significantly reducing gradient transmission costs
and making LLM-driven MRG feasible in bandwidth-constrained FL settings.
Furthermore, we observed the dual heterogeneity in MRG under the FL scenario:
varying image characteristics across medical centers, as well as diverse
reporting styles and terminology preferences. To address this, we further
enhance FedMRG with (1) client-aware contrastive learning in the MRG encoder,
coupled with diagnosis-driven prompts, which capture both globally
generalizable and locally distinctive features while maintaining diagnostic
accuracy; and (2) a dual-adapter mutual boosting mechanism in the MRG decoder
that harmonizes generic and specialized adapters to address variations in
reporting styles and terminology. Through extensive evaluation of our
established FL-MRG benchmark, we demonstrate the generalizability and
adaptability of FedMRG, underscoring its potential in harnessing multi-center
data and generating clinically accurate reports while maintaining communication
efficiency.

</details>


### [158] [HalluRNN: Mitigating Hallucinations via Recurrent Cross-Layer Reasoning in Large Vision-Language Models](https://arxiv.org/abs/2506.17587)
*Le Yu,Kaishen Wang,Jianlong Xiong,Yue Cao,Tao He*

Main category: cs.CV

TL;DR: HalluRNN通过架构级解决方案（DG-DPU模块）减少大型视觉语言模型的幻觉问题，仅需微调该模块即可在多任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型（LVLMs）易产生视觉无依据的幻觉输出，现有方法通常需要大量资源或任务特定配置。

Method: 提出HalluRNN架构，采用双门深度传播单元（DG-DPU）模块，通过跨层循环推理增强模型稳定性。

Result: 仅微调DG-DPU模块，HalluRNN在多个基准测试中表现优异且稳健。

Conclusion: HalluRNN提供了一种高效且资源友好的方法，显著减少了LVLMs的幻觉问题。

Abstract: Though Large Vision-Language Models (LVLMs) have achieved remarkable
performance across various tasks, they are still prone to
hallucinations-generating outputs that are textually plausible but visually
ungrounded. While prior approaches generally address this issue through
data-centric fine-tuning or innovative decoding strategies, these methods often
require substantial resources or task-specific configurations. In this work, we
introduce an architecture-level solution, HalluRNN, which enhances model
stability through recurrent cross-layer reasoning. Specifically, we propose a
novel Dual-Gated Depth Propagation Unit (DG-DPU) module, which is shared across
layers and recurrently refines hidden states. This allows for the adaptive
propagation of information throughout the model, enforces consistency across
layers, and mitigates hallucinations caused by representational drift. By
fine-tuning only the DG-DPU module, HalluRNN achieves strong and robust
performance across multiple benchmarks.

</details>


### [159] [DRAMA-X: A Fine-grained Intent Prediction and Risk Reasoning Benchmark For Driving](https://arxiv.org/abs/2506.17590)
*Mihir Godbole,Xiangbo Gao,Zhengzhong Tu*

Main category: cs.CV

TL;DR: 论文介绍了DRAMA-X基准，用于评估自动驾驶中行人及骑行者的意图预测、风险评估和行动建议，并提出了轻量级框架SGG-Intent作为基线。


<details>
  <summary>Details</summary>
Motivation: 解决现有技术中缺乏对多类意图预测的评估，尤其是在安全关键场景中。

Method: 通过自动化标注流程构建DRAMA-X基准，并提出SGG-Intent框架，利用视觉语言模型和大型语言模型进行推理。

Result: 实验表明，基于场景图的推理能提升意图预测和风险评估的准确性。

Conclusion: DRAMA-X为自动驾驶决策提供了结构化评估工具，SGG-Intent框架展示了上下文建模的重要性。

Abstract: Understanding the short-term motion of vulnerable road users (VRUs) like
pedestrians and cyclists is critical for safe autonomous driving, especially in
urban scenarios with ambiguous or high-risk behaviors. While vision-language
models (VLMs) have enabled open-vocabulary perception, their utility for
fine-grained intent reasoning remains underexplored. Notably, no existing
benchmark evaluates multi-class intent prediction in safety-critical
situations, To address this gap, we introduce DRAMA-X, a fine-grained benchmark
constructed from the DRAMA dataset via an automated annotation pipeline.
DRAMA-X contains 5,686 accident-prone frames labeled with object bounding
boxes, a nine-class directional intent taxonomy, binary risk scores,
expert-generated action suggestions for the ego vehicle, and descriptive motion
summaries. These annotations enable a structured evaluation of four
interrelated tasks central to autonomous decision-making: object detection,
intent prediction, risk assessment, and action suggestion. As a reference
baseline, we propose SGG-Intent, a lightweight, training-free framework that
mirrors the ego vehicle's reasoning pipeline. It sequentially generates a scene
graph from visual input using VLM-backed detectors, infers intent, assesses
risk, and recommends an action using a compositional reasoning stage powered by
a large language model. We evaluate a range of recent VLMs, comparing
performance across all four DRAMA-X tasks. Our experiments demonstrate that
scene-graph-based reasoning enhances intent prediction and risk assessment,
especially when contextual cues are explicitly modeled.

</details>


### [160] [SELFI: Selective Fusion of Identity for Generalizable Deepfake Detection](https://arxiv.org/abs/2506.17592)
*Younghun Kim,Minsuk Jang,Myung-Joon Kwon,Wonjun Lee,Changick Kim*

Main category: cs.CV

TL;DR: SELFI框架通过动态调节身份特征的使用，提升了深度伪造检测的泛化能力，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决身份特征在深度伪造检测中的矛盾观点：是否应抑制或依赖身份特征。

Method: 提出SELFI框架，包括FAIA（提取并投影身份特征）和IAFM（选择性融合身份与视觉特征）。

Result: 在四个基准测试中，SELFI平均AUC提升3.1%，在DFDC数据集上提升6%。

Conclusion: 身份特征应被显式建模和动态控制，而非盲目抑制或依赖。

Abstract: Face identity provides a powerful signal for deepfake detection. Prior
studies show that even when not explicitly modeled, classifiers often learn
identity features implicitly. This has led to conflicting views: some suppress
identity cues to reduce bias, while others rely on them as forensic evidence.
To reconcile these views, we analyze two hypotheses: (1) whether face identity
alone is discriminative for detecting deepfakes, and (2) whether such identity
features generalize poorly across manipulation methods. Our experiments confirm
that identity is informative but context-dependent. While some manipulations
preserve identity-consistent artifacts, others distort identity cues and harm
generalization. We argue that identity features should neither be blindly
suppressed nor relied upon, but instead be explicitly modeled and adaptively
controlled based on per-sample relevance. We propose \textbf{SELFI}
(\textbf{SEL}ective \textbf{F}usion of \textbf{I}dentity), a generalizable
detection framework that dynamically modulates identity usage. SELFI consists
of: (1) a Forgery-Aware Identity Adapter (FAIA) that extracts identity
embeddings from a frozen face recognition model and projects them into a
forgery-relevant space via auxiliary supervision; and (2) an Identity-Aware
Fusion Module (IAFM) that selectively integrates identity and visual features
using a relevance-guided fusion mechanism. Experiments on four benchmarks show
that SELFI improves cross-manipulation generalization, outperforming prior
methods by an average of 3.1\% AUC. On the challenging DFDC dataset, SELFI
exceeds the previous best by 6\%. Code will be released upon paper acceptance.

</details>


### [161] [A Multimodal In Vitro Diagnostic Method for Parkinson's Disease Combining Facial Expressions and Behavioral Gait Data](https://arxiv.org/abs/2506.17596)
*Wei Huang,Yinxuan Xu,Yintao Zhou,Zhengyu Li,Jing Huang,Meng Pang*

Main category: cs.CV

TL;DR: 提出了一种基于面部表情和步态的多模态体外诊断方法，用于早期帕金森病检测，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 帕金森病（PD）的不可治愈性、快速进展和严重残疾对患者及其家庭造成巨大挑战，早期检测需求增加。现有体外诊断方法存在数据不足、设备要求高和单模态风险等问题。

Method: 采用轻量级深度学习模型进行特征提取和融合，结合面部表情和步态数据，提升诊断准确性并支持移动设备部署。

Result: 建立了最大的多模态PD数据集，并通过实验验证了方法的有效性。

Conclusion: 提出的多模态诊断方法在提高准确性和实用性方面表现出色，有望推动PD早期检测的发展。

Abstract: Parkinson's disease (PD), characterized by its incurable nature, rapid
progression, and severe disability, poses significant challenges to the lives
of patients and their families. Given the aging population, the need for early
detection of PD is increasing. In vitro diagnosis has garnered attention due to
its non-invasive nature and low cost. However, existing methods present several
challenges: 1) limited training data for facial expression diagnosis; 2)
specialized equipment and acquisition environments required for gait diagnosis,
resulting in poor generalizability; 3) the risk of misdiagnosis or missed
diagnosis when relying on a single modality. To address these issues, we
propose a novel multimodal in vitro diagnostic method for PD, leveraging facial
expressions and behavioral gait. Our method employs a lightweight deep learning
model for feature extraction and fusion, aimed at improving diagnostic accuracy
and facilitating deployment on mobile devices. Furthermore, we have established
the largest multimodal PD dataset in collaboration with a hospital and
conducted extensive experiments to validate the effectiveness of our proposed
method.

</details>


### [162] [OpenMAP-BrainAge: Generalizable and Interpretable Brain Age Predictor](https://arxiv.org/abs/2506.17597)
*Pengyu Kan,Craig Jones,Kenichi Oishi*

Main category: cs.CV

TL;DR: 提出了一种基于Transformer的可解释脑年龄预测模型，通过自监督预训练和线性复杂度设计，实现了高精度和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够应对人口和技术差异的脑MRI年龄预测模型，同时具备可解释性。

Method: 采用Transformer架构，结合自监督预训练和伪3D MRI扫描处理，引入线性复杂度设计，训练于ADNI2 & 3和OASIS3数据集，验证于AIBL数据集。

Result: 在测试集上MAE为3.65年，泛化能力MAE为3.54年，脑年龄差（BAG）与认知功能显著相关。

Conclusion: 模型融合多视角和体积信息，实现了高精度、泛化性和可解释性，并与神经退行性疾病相关。

Abstract: Purpose: To develop an age prediction model which is interpretable and robust
to demographic and technological variances in brain MRI scans. Materials and
Methods: We propose a transformer-based architecture that leverages
self-supervised pre-training on large-scale datasets. Our model processes
pseudo-3D T1-weighted MRI scans from three anatomical views and incorporates
brain volumetric information. By introducing a stem architecture, we reduce the
conventional quadratic complexity of transformer models to linear complexity,
enabling scalability for high-dimensional MRI data. We trained our model on
ADNI2 $\&$ 3 (N=1348) and OASIS3 (N=716) datasets (age range: 42 - 95) from the
North America, with an 8:1:1 split for train, validation and test. Then, we
validated it on the AIBL dataset (N=768, age range: 60 - 92) from Australia.
Results: We achieved an MAE of 3.65 years on ADNI2 $\&$ 3 and OASIS3 test set
and a high generalizability of MAE of 3.54 years on AIBL. There was a notable
increase in brain age gap (BAG) across cognitive groups, with mean of 0.15
years (95% CI: [-0.22, 0.51]) in CN, 2.55 years ([2.40, 2.70]) in MCI, 6.12
years ([5.82, 6.43]) in AD. Additionally, significant negative correlation
between BAG and cognitive scores was observed, with correlation coefficient of
-0.185 (p < 0.001) for MoCA and -0.231 (p < 0.001) for MMSE. Gradient-based
feature attribution highlighted ventricles and white matter structures as key
regions influenced by brain aging. Conclusion: Our model effectively fused
information from different views and volumetric information to achieve
state-of-the-art brain age prediction accuracy, improved generalizability and
interpretability with association to neurodegenerative disorders.

</details>


### [163] [HIRE: Lightweight High-Resolution Image Feature Enrichment for Multimodal LLMs](https://arxiv.org/abs/2506.17608)
*Nikitha SR,Aradhya Neeraj Mathur,Tarun Ram Menta,Rishabh Jain,Mausoom Sarkar*

Main category: cs.CV

TL;DR: 论文提出了一种浅层特征增强器，通过特征上采样实现高分辨率特征生成，显著降低了计算成本，同时保持高性能。


<details>
  <summary>Details</summary>
Motivation: 现代多模态大语言模型通过高分辨率图像特征提升了细粒度视觉理解任务性能，但计算成本高昂。

Method: 提出浅层特征增强器，通过特征上采样减少对大型图像编码器的多次调用。

Result: 实验表明，该方法在训练和推理时间上大幅节省，计算成本降低1.5倍FLOPs，同时保持竞争力。

Conclusion: 浅层特征增强器是一种高效的高分辨率特征生成方法，显著降低计算负担。

Abstract: The integration of high-resolution image features in modern multimodal large
language models has demonstrated significant improvements in fine-grained
visual understanding tasks, achieving high performance across multiple
benchmarks. Since these features are obtained from large image encoders like
ViT, they come with a significant increase in computational costs due to
multiple calls to these encoders. In this work, we first develop an intuition
for feature upsampling as a natural extension of high-resolution feature
generation. Through extensive experiments and ablations, we demonstrate how a
shallow feature enricher can achieve competitive results with tremendous
reductions in training and inference time as well as computational cost, with
upto 1.5x saving in FLOPs.

</details>


### [164] [JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo Retouching Agent](https://arxiv.org/abs/2506.17612)
*Yunlong Lin,Zixu Lin,Kunjie Lin,Jinbin Bai,Panwang Pan,Chenxin Li,Haoyu Chen,Zhongdao Wang,Xinghao Ding,Wenbo Li,Shuicheng Yan*

Main category: cs.CV

TL;DR: JarvisArt是一个基于多模态大语言模型的智能修图代理，通过两阶段训练和专用协议与Lightroom集成，提供用户友好、高泛化性和精细控制的修图体验。


<details>
  <summary>Details</summary>
Motivation: 解决专业修图工具需要高技能和现有AI修图工具泛化能力不足的问题，满足个性化编辑需求。

Method: 采用两阶段训练：Chain-of-Thought监督微调建立基础推理能力，GRPO-R优化决策和工具使用；提出Agent-to-Lightroom协议实现无缝集成。

Result: 在MMArt-Bench基准测试中，JarvisArt在内容保真度上比GPT-4o提升60%，同时保持指令跟随能力。

Conclusion: JarvisArt为智能修图开辟了新途径，实现了用户友好、高泛化和精细控制的修图效果。

Abstract: Photo retouching has become integral to contemporary visual storytelling,
enabling users to capture aesthetics and express creativity. While professional
tools such as Adobe Lightroom offer powerful capabilities, they demand
substantial expertise and manual effort. In contrast, existing AI-based
solutions provide automation but often suffer from limited adjustability and
poor generalization, failing to meet diverse and personalized editing needs. To
bridge this gap, we introduce JarvisArt, a multi-modal large language model
(MLLM)-driven agent that understands user intent, mimics the reasoning process
of professional artists, and intelligently coordinates over 200 retouching
tools within Lightroom. JarvisArt undergoes a two-stage training process: an
initial Chain-of-Thought supervised fine-tuning to establish basic reasoning
and tool-use skills, followed by Group Relative Policy Optimization for
Retouching (GRPO-R) to further enhance its decision-making and tool
proficiency. We also propose the Agent-to-Lightroom Protocol to facilitate
seamless integration with Lightroom. To evaluate performance, we develop
MMArt-Bench, a novel benchmark constructed from real-world user edits.
JarvisArt demonstrates user-friendly interaction, superior generalization, and
fine-grained control over both global and local adjustments, paving a new
avenue for intelligent photo retouching. Notably, it outperforms GPT-4o with a
60% improvement in average pixel-level metrics on MMArt-Bench for content
fidelity, while maintaining comparable instruction-following capabilities.
Project Page: https://jarvisart.vercel.app/.

</details>


### [165] [CLiViS: Unleashing Cognitive Map through Linguistic-Visual Synergy for Embodied Visual Reasoning](https://arxiv.org/abs/2506.17629)
*Kailing Li,Qi'ao Xu,Tianwen Qian,Yuqian Fu,Yang Jiao,Xiaoling Wang*

Main category: cs.CV

TL;DR: CLiViS是一个无需训练的框架，结合LLMs的任务规划和VLMs的视觉感知，通过动态认知地图实现高效的视觉推理。


<details>
  <summary>Details</summary>
Motivation: 解决EVR中复杂指令和长时视频的时空动态性挑战，弥补现有方法在细节缺失和逐步推理上的不足。

Method: 利用LLMs进行高层任务规划，VLMs进行开放世界视觉感知，通过动态认知地图迭代更新场景上下文。

Result: 在多个基准测试中表现优异，尤其在长时视觉依赖任务上。

Conclusion: CLiViS通过结合LLMs和VLMs的优势，显著提升了视觉推理能力。

Abstract: Embodied Visual Reasoning (EVR) seeks to follow complex, free-form
instructions based on egocentric video, enabling semantic understanding and
spatiotemporal reasoning in dynamic environments. Despite its promising
potential, EVR encounters significant challenges stemming from the diversity of
complex instructions and the intricate spatiotemporal dynamics in long-term
egocentric videos. Prior solutions either employ Large Language Models (LLMs)
over static video captions, which often omit critical visual details, or rely
on end-to-end Vision-Language Models (VLMs) that struggle with stepwise
compositional reasoning. Consider the complementary strengths of LLMs in
reasoning and VLMs in perception, we propose CLiViS. It is a novel
training-free framework that leverages LLMs for high-level task planning and
orchestrates VLM-driven open-world visual perception to iteratively update the
scene context. Building on this synergy, the core of CLiViS is a dynamic
Cognitive Map that evolves throughout the reasoning process. This map
constructs a structured representation of the embodied scene, bridging
low-level perception and high-level reasoning. Extensive experiments across
multiple benchmarks demonstrate the effectiveness and generality of CLiViS,
especially in handling long-term visual dependencies. Code is available at
https://github.com/Teacher-Tom/CLiViS.

</details>


### [166] [Optimization-Free Patch Attack on Stereo Depth Estimation](https://arxiv.org/abs/2506.17632)
*Hangcheng Liu,Xu Kuang,Xingshuo Han,Xingwan Wu,Haoran Ou,Shangwei Guo,Xingyi Huang,Tao Xiang,Tianwei Zhang*

Main category: cs.CV

TL;DR: 论文提出了一种名为PatchHunter的对抗性补丁攻击方法，针对立体深度估计（SDE）模型，解决了现有攻击方法在物理可实现性和迁移性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有SDE模型对抗攻击方法多局限于不现实的数字扰动场景，缺乏物理可实现性和迁移性，因此需要设计一种更实用的攻击方法。

Method: 提出PatchHunter，一种基于强化学习的优化无关对抗补丁攻击方法，通过搜索视觉模式空间来破坏SDE假设。

Result: PatchHunter在KITTI数据集、CARLA模拟器和真实车辆部署中表现优异，攻击成功率高且迁移性强，尤其在低光条件下仍有效。

Conclusion: PatchHunter为SDE模型提供了一种高效且物理可实现的对抗攻击方法，显著优于传统优化方法。

Abstract: Stereo Depth Estimation (SDE) is essential for scene understanding in
vision-based systems like autonomous driving. However, recent studies show that
SDE models are vulnerable to adversarial attacks, which are often limited to
unrealistic settings, e.g., digital perturbations on separate stereo views in
static scenes, restricting their real-world applicability. This raises a
critical question: how can we design physically realizable, scene-adaptive, and
transferable attacks against SDE under realistic constraints?
  To answer this, we make two key contributions. First, we propose a unified
attack framework that extends optimization-based techniques to four core stages
of stereo matching: feature extraction, cost-volume construction, cost
aggregation, and disparity regression. A comprehensive stage-wise evaluation
across 9 mainstream SDE models, under constraints like photometric consistency,
reveals that optimization-based patches suffer from poor transferability.
Interestingly, partially transferable patches suggest that patterns, rather
than pixel-level perturbations, may be key to generalizable attacks. Motivated
by this, we present PatchHunter, the first optimization-free adversarial patch
attack against SDE. PatchHunter formulates patch generation as a reinforcement
learning-driven search over a structured space of visual patterns crafted to
disrupt SDE assumptions.
  We validate PatchHunter across three levels: the KITTI dataset, the CARLA
simulator, and real-world vehicle deployment. PatchHunter not only surpasses
optimization-based methods in effectiveness but also achieves significantly
better black-box transferability. Even under challenging physical conditions
like low light, PatchHunter maintains high attack success (e.g., D1-all > 0.4),
whereas optimization-based methods fail.

</details>


### [167] [Adaptive Multi-prompt Contrastive Network for Few-shot Out-of-distribution Detection](https://arxiv.org/abs/2506.17633)
*Xiang Fang,Arvind Easwaran,Blaise Genest*

Main category: cs.CV

TL;DR: 提出了一种名为AMCN的新网络，用于解决少样本OOD检测问题，通过自适应多提示对比学习优化ID-OOD边界。


<details>
  <summary>Details</summary>
Motivation: 传统OOD检测方法需要大量ID样本，限制了实际应用；少样本OOD检测更具挑战性，且现有方法忽略了类间多样性。

Method: 利用CLIP连接文本与图像，生成自适应提示（可学习的ID提示、固定标签的OOD提示和自适应标签的OOD提示），并通过类间阈值和提示引导的分离模块优化边界。

Result: 实验表明AMCN优于现有方法。

Conclusion: AMCN通过自适应提示和边界优化，有效解决了少样本OOD检测问题。

Abstract: Out-of-distribution (OOD) detection attempts to distinguish outlier samples
to prevent models trained on the in-distribution (ID) dataset from producing
unavailable outputs. Most OOD detection methods require many IID samples for
training, which seriously limits their real-world applications. To this end, we
target a challenging setting: few-shot OOD detection, where {Only a few {\em
labeled ID} samples are available.} Therefore, few-shot OOD detection is much
more challenging than the traditional OOD detection setting. Previous few-shot
OOD detection works ignore the distinct diversity between different classes. In
this paper, we propose a novel network: Adaptive Multi-prompt Contrastive
Network (AMCN), which adapts the ID-OOD separation boundary by learning inter-
and intra-class distribution. To compensate for the absence of OOD and scarcity
of ID {\em image samples}, we leverage CLIP, connecting text with images,
engineering learnable ID and OOD {\em textual prompts}. Specifically, we first
generate adaptive prompts (learnable ID prompts, label-fixed OOD prompts and
label-adaptive OOD prompts). Then, we generate an adaptive class boundary for
each class by introducing a class-wise threshold. Finally, we propose a
prompt-guided ID-OOD separation module to control the margin between ID and OOD
prompts. Experimental results show that AMCN outperforms other state-of-the-art
works.

</details>


### [168] [Histopathology Image Report Generation by Vision Language Model with Multimodal In-Context Learning](https://arxiv.org/abs/2506.17645)
*Shih-Wen Liu,Hsuan-Yu Fan,Wei-Ta Chu,Fu-En Yang,Yu-Chiang Frank Wang*

Main category: cs.CV

TL;DR: PathGenIC框架通过上下文学习和自适应反馈，从病理图像生成高质量医学报告，在HistGen基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 自动化医学报告生成需要有效的视觉表示和领域知识，模仿人类专家的实践方法。

Method: 提出PathGenIC框架，结合训练集的上下文和多模态上下文学习机制，动态检索相似图像-报告对并引入自适应反馈。

Result: 在HistGen基准测试中取得最佳表现，BLEU、METEOR和ROUGE-L指标显著提升，且对不同报告长度和疾病类别具有鲁棒性。

Conclusion: PathGenIC为AI驱动的病理报告生成提供了解决方案，为多模态临床应用的未来发展奠定了基础。

Abstract: Automating medical report generation from histopathology images is a critical
challenge requiring effective visual representations and domain-specific
knowledge. Inspired by the common practices of human experts, we propose an
in-context learning framework called PathGenIC that integrates context derived
from the training set with a multimodal in-context learning (ICL) mechanism.
Our method dynamically retrieves semantically similar whole slide image
(WSI)-report pairs and incorporates adaptive feedback to enhance contextual
relevance and generation quality. Evaluated on the HistGen benchmark, the
framework achieves state-of-the-art results, with significant improvements
across BLEU, METEOR, and ROUGE-L metrics, and demonstrates robustness across
diverse report lengths and disease categories. By maximizing training data
utility and bridging vision and language with ICL, our work offers a solution
for AI-driven histopathology reporting, setting a strong foundation for future
advancements in multimodal clinical applications.

</details>


### [169] [MDSAM:Memory-Driven Sparse Attention Matrix for LVLMs Hallucination Mitigation](https://arxiv.org/abs/2506.17664)
*Shuaiye Lu,Linjiang Zhou,Xiaochuan Shi*

Main category: cs.CV

TL;DR: 论文提出了一种无需训练的Memory-Driven Sparse Attention Matrix (MDSAM)方法，通过动态调整图像令牌的注意力分配，减少大型视觉语言模型中的幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型在解码过程中对图像令牌的敏感性导致幻觉问题，表现为注意力峰值。

Method: 提出MDSAM方法，动态捕捉和优化各层的图像令牌注意力，通过记忆注意力模式和解码对齐激活更新。

Result: 在图像描述和视觉问答等任务中，MDSAM能持续减少幻觉并提高可靠性，且兼容多种模型架构。

Conclusion: MDSAM无需额外训练或外部工具，能有效减少幻觉，展示了其适应性和有效性。

Abstract: Hallucinations in large vision-language models (LVLMs) often stem from the
model's sensitivity to image tokens during decoding, as evidenced by attention
peaks observed when generating both real and hallucinated entities. To address
this, we propose Memory-Driven Sparse Attention Matrix (MDSAM) , a novel
training-free approach that dynamically captures and refines the attention
allocated to image tokens at each layer. MDSAM memorizes attention patterns and
activates updates through alignment during decoding, enhancing focus on
relevant image tokens while effectively reducing hallucinations. We evaluate
MDSAM on multiple benchmarks for tasks such as image captioning and visual
question answering, demonstrating its ability to consistently reduce
hallucinations and improve reliability. Compatible with various LVLM
architectures, MDSAM highlights its adaptability and effectiveness in
mitigating hallucinations without requiring additional training or external
tools.

</details>


### [170] [CSDN: A Context-Gated Self-Adaptive Detection Network for Real-Time Object Detection](https://arxiv.org/abs/2506.17679)
*Wei Haolin*

Main category: cs.CV

TL;DR: 论文提出了一种基于Transformer的检测头CSDN，通过新颖的门控机制替代传统自注意力和交叉注意力层，以更高效地利用CNN骨干网络的特征，提升目标检测的全局上下文建模能力。


<details>
  <summary>Details</summary>
Motivation: 传统CNN在目标检测中受限于有限的感受野，难以捕捉全局上下文信息，且DETR架构中的自注意力机制存在信息冗余问题。

Method: 引入CSDN，采用门控机制自适应选择和组合多注意力模式的特征维度和尺度信息。

Result: CSDN显著提升了检测精度，且仅需少量微调即可适配多种CNN检测器。

Conclusion: CSDN通过高效利用特征和全局建模能力，为目标检测提供了更优的解决方案。

Abstract: Convolutional neural networks (CNNs) have long been the cornerstone of target
detection, but they are often limited by limited receptive fields, which
hinders their ability to capture global contextual information. This paper
believes that the effective utilization of extracted features is as important
as the feature extraction process itself. We critically re-evaluated the
DETR-inspired header network architecture, questioning the indispensable nature
of its self-attention mechanism, and discovering significant information
redundancies. To solve these problems, we introduced the Context-Gated
Scale-Adaptive Detection Network (CSDN), a Transformer-based detection header
inspired by natural language processing architecture and human visual
perception. CSDN aims to efficiently utilize the characteristics of the CNN
backbone network by replacing the traditional stacked self-attention and
cross-attention layers with a novel gating mechanism. This mechanism enables
each region of interest (ROI) to adaptively select and combine feature
dimensions and scale information from multiple attention patterns. CSDN
provides more powerful global context modeling capabilities and can better
adapt to objects of different sizes and structures. Our proposed detection head
can directly replace the native heads of various CNN-based detectors, and only
a few rounds of fine-tuning on the pre-training weights can significantly
improve the detection accuracy, thus avoiding the need to achieve small
improvements. Various layer modules undergo extensive re-training.

</details>


### [171] [Domain Generalization using Action Sequences for Egocentric Action Recognition](https://arxiv.org/abs/2506.17685)
*Amirshayan Nasirimajd,Chiara Plizzari,Simone Alberto Peirone,Marco Ciccone,Giuseppe Averta,Barbara Caputo*

Main category: cs.CV

TL;DR: 提出了一种名为SeqDG的领域泛化方法，通过利用动作序列和视觉-文本序列重建目标（SeqRec）以及混合域动作序列训练（SeqMix），提升了模型在未见环境中的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决Egocentric Action Recognition模型在未见训练环境时性能下降的问题，利用动作序列反映用户意图的一致性。

Method: 提出SeqDG方法，包括SeqRec（视觉-文本序列重建目标）和SeqMix（混合域动作序列训练）。

Result: 在EPIC-KITCHENS-100上，跨域动作识别相对提升2.4%；在EGTEA上，域内动作识别Top-1准确率提升0.6%。

Conclusion: SeqDG通过动作序列和混合域训练有效提升了模型在未见环境中的泛化能力。

Abstract: Recognizing human activities from visual inputs, particularly through a
first-person viewpoint, is essential for enabling robots to replicate human
behavior. Egocentric vision, characterized by cameras worn by observers,
captures diverse changes in illumination, viewpoint, and environment. This
variability leads to a notable drop in the performance of Egocentric Action
Recognition models when tested in environments not seen during training. In
this paper, we tackle these challenges by proposing a domain generalization
approach for Egocentric Action Recognition. Our insight is that action
sequences often reflect consistent user intent across visual domains. By
leveraging action sequences, we aim to enhance the model's generalization
ability across unseen environments. Our proposed method, named SeqDG,
introduces a visual-text sequence reconstruction objective (SeqRec) that uses
contextual cues from both text and visual inputs to reconstruct the central
action of the sequence. Additionally, we enhance the model's robustness by
training it on mixed sequences of actions from different domains (SeqMix). We
validate SeqDG on the EGTEA and EPIC-KITCHENS-100 datasets. Results on
EPIC-KITCHENS-100, show that SeqDG leads to +2.4% relative average improvement
in cross-domain action recognition in unseen environments, and on EGTEA the
model achieved +0.6% Top-1 accuracy over SOTA in intra-domain action
recognition.

</details>


### [172] [SSAVSV: Towards Unified Model for Self-Supervised Audio-Visual Speaker Verification](https://arxiv.org/abs/2506.17694)
*Gnana Praveen Rajasekhar,Jahangir Alam*

Main category: cs.CV

TL;DR: 提出一种基于对比学习和掩码数据建模的自监督学习框架，用于音频-视觉说话人验证，减少对标注数据的依赖和计算成本。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖大量标注数据和独立模态架构，计算成本高且扩展性差。

Method: 采用自监督对比学习和掩码数据建模，使用共享视觉Transformer主干处理多模态输入。

Result: 在无标注数据情况下实现竞争性性能，同时降低计算成本。

Conclusion: 提出的统一框架高效且鲁棒，适用于多模态输入，减少对标注数据的依赖。

Abstract: Conventional audio-visual methods for speaker verification rely on large
amounts of labeled data and separate modality-specific architectures, which is
computationally expensive, limiting their scalability. To address these
problems, we propose a self-supervised learning framework based on contrastive
learning with asymmetric masking and masked data modeling to obtain robust
audiovisual feature representations. In particular, we employ a unified
framework for self-supervised audiovisual speaker verification using a single
shared backbone for audio and visual inputs, leveraging the versatility of
vision transformers. The proposed unified framework can handle audio, visual,
or audiovisual inputs using a single shared vision transformer backbone during
training and testing while being computationally efficient and robust to
missing modalities. Extensive experiments demonstrate that our method achieves
competitive performance without labeled data while reducing computational costs
compared to traditional approaches.

</details>


### [173] [DreamJourney: Perpetual View Generation with Video Diffusion Models](https://arxiv.org/abs/2506.17705)
*Bo Pan,Yang Chen,Yingwei Pan,Ting Yao,Wei Chen,Tao Mei*

Main category: cs.CV

TL;DR: DreamJourney提出了一种两阶段框架，通过视频扩散模型生成动态场景视图，解决了现有方法在3D感知和动态对象运动方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖2D扩散模型，缺乏3D感知能力且无法捕捉动态对象运动，导致生成视图失真且仅适用于静态场景。

Method: DreamJourney分为两阶段：第一阶段通过3D点云和视频扩散模型生成跨视图一致的视频；第二阶段利用多模态大语言模型描述对象运动并生成动态视图。

Result: 实验表明，DreamJourney在定量和定性上均优于现有方法。

Conclusion: DreamJourney实现了动态场景的长期视图生成，解决了现有技术的局限性。

Abstract: Perpetual view generation aims to synthesize a long-term video corresponding
to an arbitrary camera trajectory solely from a single input image. Recent
methods commonly utilize a pre-trained text-to-image diffusion model to
synthesize new content of previously unseen regions along camera movement.
However, the underlying 2D diffusion model lacks 3D awareness and results in
distorted artifacts. Moreover, they are limited to generating views of static
3D scenes, neglecting to capture object movements within the dynamic 4D world.
To alleviate these issues, we present DreamJourney, a two-stage framework that
leverages the world simulation capacity of video diffusion models to trigger a
new perpetual scene view generation task with both camera movements and object
dynamics. Specifically, in stage I, DreamJourney first lifts the input image to
3D point cloud and renders a sequence of partial images from a specific camera
trajectory. A video diffusion model is then utilized as generative prior to
complete the missing regions and enhance visual coherence across the sequence,
producing a cross-view consistent video adheres to the 3D scene and camera
trajectory. Meanwhile, we introduce two simple yet effective strategies (early
stopping and view padding) to further stabilize the generation process and
improve visual quality. Next, in stage II, DreamJourney leverages a multimodal
large language model to produce a text prompt describing object movements in
current view, and uses video diffusion model to animate current view with
object movements. Stage I and II are repeated recurrently, enabling perpetual
dynamic scene view generation. Extensive experiments demonstrate the
superiority of our DreamJourney over state-of-the-art methods both
quantitatively and qualitatively. Our project page:
https://dream-journey.vercel.app.

</details>


### [174] [Programmable-Room: Interactive Textured 3D Room Meshes Generation Empowered by Large Language Models](https://arxiv.org/abs/2506.17707)
*Jihyun Kim,Junho Park,Kyeongbo Kong,Suk-Ju Kang*

Main category: cs.CV

TL;DR: Programmable-Room是一个通过自然语言指令交互式生成和编辑3D房间网格的框架，利用视觉编程和预训练扩散模型实现高质量全景图像生成。


<details>
  <summary>Details</summary>
Motivation: 为了实现对房间属性的精确控制，将复杂任务分解为多个简单步骤，并通过统一框架支持这些任务。

Method: 框架结合视觉编程（VP）和预训练扩散模型，分解任务为生成3D坐标、全景图像、3D网格构建和家具布置。

Result: 展示了框架在生成和编辑3D房间网格上的灵活性，并在质量和数量上优于现有模型。

Conclusion: Programmable-Room通过自然语言指令和模块化设计，实现了高效且高质量的3D房间生成与编辑。

Abstract: We present Programmable-Room, a framework which interactively generates and
edits a 3D room mesh, given natural language instructions. For precise control
of a room's each attribute, we decompose the challenging task into simpler
steps such as creating plausible 3D coordinates for room meshes, generating
panorama images for the texture, constructing 3D meshes by integrating the
coordinates and panorama texture images, and arranging furniture. To support
the various decomposed tasks with a unified framework, we incorporate visual
programming (VP). VP is a method that utilizes a large language model (LLM) to
write a Python-like program which is an ordered list of necessary modules for
the various tasks given in natural language. We develop most of the modules.
Especially, for the texture generating module, we utilize a pretrained
large-scale diffusion model to generate panorama images conditioned on text and
visual prompts (i.e., layout, depth, and semantic map) simultaneously.
Specifically, we enhance the panorama image generation quality by optimizing
the training objective with a 1D representation of a panorama scene obtained
from bidirectional LSTM. We demonstrate Programmable-Room's flexibility in
generating and editing 3D room meshes, and prove our framework's superiority to
an existing model quantitatively and qualitatively. Project page is available
in https://jihyun0510.github.io/Programmable_Room_Page/.

</details>


### [175] [PDC-Net: Pattern Divide-and-Conquer Network for Pelvic Radiation Injury Segmentation](https://arxiv.org/abs/2506.17712)
*Xinyu Xiong,Wuteng Cao,Zihuang Wu,Lei Zhang,Chong Gao,Guanbin Li,Qiyuan Qin*

Main category: cs.CV

TL;DR: 提出了一种名为PDC-Net的新方法，用于从MRI中准确分割盆腔放射损伤（PRI），通过分而治之的策略处理复杂形态和上下文干扰。


<details>
  <summary>Details</summary>
Motivation: 盆腔放射损伤（PRI）的精确分割对预后评估和个性化治疗至关重要，但自动化分割因器官形态复杂和上下文干扰而具有挑战性。

Method: 提出PDC-Net，包含Multi-Direction Aggregation（MDA）模块增强形状拟合，Memory-Guided Context（MGC）模块提升全局模式区分，以及Adaptive Fusion Decoder（AFD）动态选择特征。

Result: 在首个大规模盆腔放射损伤数据集上验证，PDC-Net优于现有方法。

Conclusion: PDC-Net通过分而治之策略和模块化设计，显著提升了PRI分割的准确性。

Abstract: Accurate segmentation of Pelvic Radiation Injury (PRI) from Magnetic
Resonance Images (MRI) is crucial for more precise prognosis assessment and the
development of personalized treatment plans. However, automated segmentation
remains challenging due to factors such as complex organ morphologies and
confusing context. To address these challenges, we propose a novel Pattern
Divide-and-Conquer Network (PDC-Net) for PRI segmentation. The core idea is to
use different network modules to "divide" various local and global patterns
and, through flexible feature selection, to "conquer" the Regions of Interest
(ROI) during the decoding phase. Specifically, considering that our ROI often
manifests as strip-like or circular-like structures in MR slices, we introduce
a Multi-Direction Aggregation (MDA) module. This module enhances the model's
ability to fit the shape of the organ by applying strip convolutions in four
distinct directions. Additionally, to mitigate the challenge of confusing
context, we propose a Memory-Guided Context (MGC) module. This module
explicitly maintains a memory parameter to track cross-image patterns at the
dataset level, thereby enhancing the distinction between global patterns
associated with the positive and negative classes. Finally, we design an
Adaptive Fusion Decoder (AFD) that dynamically selects features from different
patterns based on the Mixture-of-Experts (MoE) framework, ultimately generating
the final segmentation results. We evaluate our method on the first large-scale
pelvic radiation injury dataset, and the results demonstrate the superiority of
our PDC-Net over existing approaches.

</details>


### [176] [YOLOv13: Real-Time Object Detection with Hypergraph-Enhanced Adaptive Visual Perception](https://arxiv.org/abs/2506.17733)
*Mengqi Lei,Siqi Li,Yihong Wu,Han Hu,You Zhou,Xinhu Zheng,Guiguang Ding,Shaoyi Du,Zongze Wu,Yue Gao*

Main category: cs.CV

TL;DR: YOLOv13提出了一种基于超图的自适应相关性增强机制（HyperACE）和全流程聚合与分配范式（FullPAD），显著提升了复杂场景下的目标检测性能，同时减少了参数和计算量。


<details>
  <summary>Details</summary>
Motivation: YOLO系列模型在实时目标检测中表现出色，但其局部信息聚合和成对相关性建模能力限制了在复杂场景中的性能。

Method: 提出HyperACE机制以捕捉全局高阶相关性，并设计FullPAD范式实现细粒度信息流；采用深度可分离卷积减少计算复杂度。

Result: 在MS COCO基准测试中，YOLOv13-N的mAP比YOLO11-N提升3.0%，比YOLOv12-N提升1.5%，且参数和FLOPs更少。

Conclusion: YOLOv13通过全局相关性建模和轻量化设计，实现了高效且高性能的目标检测。

Abstract: The YOLO series models reign supreme in real-time object detection due to
their superior accuracy and computational efficiency. However, both the
convolutional architectures of YOLO11 and earlier versions and the area-based
self-attention mechanism introduced in YOLOv12 are limited to local information
aggregation and pairwise correlation modeling, lacking the capability to
capture global multi-to-multi high-order correlations, which limits detection
performance in complex scenarios. In this paper, we propose YOLOv13, an
accurate and lightweight object detector. To address the above-mentioned
challenges, we propose a Hypergraph-based Adaptive Correlation Enhancement
(HyperACE) mechanism that adaptively exploits latent high-order correlations
and overcomes the limitation of previous methods that are restricted to
pairwise correlation modeling based on hypergraph computation, achieving
efficient global cross-location and cross-scale feature fusion and enhancement.
Subsequently, we propose a Full-Pipeline Aggregation-and-Distribution (FullPAD)
paradigm based on HyperACE, which effectively achieves fine-grained information
flow and representation synergy within the entire network by distributing
correlation-enhanced features to the full pipeline. Finally, we propose to
leverage depthwise separable convolutions to replace vanilla large-kernel
convolutions, and design a series of blocks that significantly reduce
parameters and computational complexity without sacrificing performance. We
conduct extensive experiments on the widely used MS COCO benchmark, and the
experimental results demonstrate that our method achieves state-of-the-art
performance with fewer parameters and FLOPs. Specifically, our YOLOv13-N
improves mAP by 3.0\% over YOLO11-N and by 1.5\% over YOLOv12-N. The code and
models of our YOLOv13 model are available at:
https://github.com/iMoonLab/yolov13.

</details>


### [177] [PhysID: Physics-based Interactive Dynamics from a Single-view Image](https://arxiv.org/abs/2506.17746)
*Sourabh Vasant Gothe,Ayon Chattopadhyay,Gunturi Venkata Sai Phani Kiran,Pratik,Vibhav Agarwal,Jayesh Rajkumar Vachhani,Sourav Ghosh,Parameswaranath VM,Barath Raj KR*

Main category: cs.CV

TL;DR: PhysID利用生成模型从单视角图像创建物理交互动态，简化3D建模和物理属性校准，实现实时交互和个性化体验。


<details>
  <summary>Details</summary>
Motivation: 将静态图像转化为交互体验是计算机视觉的挑战，可提升移动用户体验，尤其是AR/VR应用。当前方法依赖多视角图像或预录视频，限制了实用性。

Method: PhysID结合生成模型生成3D网格和预测物理属性，集成设备端物理引擎实现实时渲染和交互。

Result: 实验验证了多模态大语言模型的零样本能力和3D重建模型性能，展示了端到端框架的有效性。

Conclusion: PhysID显著降低了交互动态创建的技术门槛，为移动端实时交互提供了高效解决方案。

Abstract: Transforming static images into interactive experiences remains a challenging
task in computer vision. Tackling this challenge holds the potential to elevate
mobile user experiences, notably through interactive and AR/VR applications.
Current approaches aim to achieve this either using pre-recorded video
responses or requiring multi-view images as input. In this paper, we present
PhysID, that streamlines the creation of physics-based interactive dynamics
from a single-view image by leveraging large generative models for 3D mesh
generation and physical property prediction. This significantly reduces the
expertise required for engineering-intensive tasks like 3D modeling and
intrinsic property calibration, enabling the process to be scaled with minimal
manual intervention. We integrate an on-device physics-based engine for
physically plausible real-time rendering with user interactions. PhysID
represents a leap forward in mobile-based interactive dynamics, offering
real-time, non-deterministic interactions and user-personalization with
efficient on-device memory consumption. Experiments evaluate the zero-shot
capabilities of various Multimodal Large Language Models (MLLMs) on diverse
tasks and the performance of 3D reconstruction models. These results
demonstrate the cohesive functioning of all modules within the end-to-end
framework, contributing to its effectiveness.

</details>


### [178] [LoLA-SpecViT: Local Attention SwiGLU Vision Transformer with LoRA for Hyperspectral Imaging](https://arxiv.org/abs/2506.17759)
*Fadi Abdeladhim Zidi,Djamel Eddine Boukhari,Abdellah Zakaria Sellam,Abdelkrim Ouafi,Cosimo Distante,Salah Eddine Bekhouche,Abdelmalik Taleb-Ahmed*

Main category: cs.CV

TL;DR: 提出了一种轻量级光谱视觉变换器（LoLA-SpecViT），通过参数高效架构解决高光谱图像分类中的高维度和标注样本不足问题。


<details>
  <summary>Details</summary>
Motivation: 高光谱图像分类因数据高维度、频带冗余和标注样本有限而具有挑战性，现有变换器模型在标签稀缺条件下的可扩展性和适应性不足。

Method: 结合3D卷积光谱前端和局部窗口自注意力，引入低秩适应（LoRA）减少训练参数，并使用循环学习率调度器优化训练。

Result: 在三个基准数据集上表现优异，最高达99.91%准确率，参数更少且在低标签条件下更鲁棒。

Conclusion: LoLA-SpecViT为农业、环境监测等实际应用提供了可扩展且通用的解决方案。

Abstract: Hyperspectral image classification remains a challenging task due to the high
dimensionality of spectral data, significant inter-band redundancy, and the
limited availability of annotated samples. While recent transformer-based
models have improved the global modeling of spectral-spatial dependencies,
their scalability and adaptability under label-scarce conditions remain
limited. In this work, we propose \textbf{LoLA-SpecViT}(Low-rank adaptation
Local Attention Spectral Vision Transformer), a lightweight spectral vision
transformer that addresses these limitations through a parameter-efficient
architecture tailored to the unique characteristics of hyperspectral imagery.
Our model combines a 3D convolutional spectral front-end with local
window-based self-attention, enhancing both spectral feature extraction and
spatial consistency while reducing computational complexity. To further improve
adaptability, we integrate low-rank adaptation (LoRA) into attention and
projection layers, enabling fine-tuning with over 80\% fewer trainable
parameters. A novel cyclical learning rate scheduler modulates LoRA adaptation
strength during training, improving convergence and generalisation. Extensive
experiments on three benchmark datasets WHU-Hi LongKou, WHU-Hi HongHu, and
Salinas demonstrate that LoLA-SpecViT consistently outperforms state-of-the-art
baselines, achieving up to 99.91\% accuracy with substantially fewer parameters
and enhanced robustness under low-label regimes. The proposed framework
provides a scalable and generalizable solution for real-world HSI applications
in agriculture, environmental monitoring, and remote sensing analytics. Our
code is available in the following
\href{https://github.com/FadiZidiDz/LoLA-SpecViT}{GitHub Repository}.

</details>


### [179] [Incorporating Rather Than Eliminating: Achieving Fairness for Skin Disease Diagnosis Through Group-Specific Expert](https://arxiv.org/abs/2506.17787)
*Gelei Xu,Yuying Duan,Zheyuan Liu,Xueyang Li,Meng Jiang,Michael Lemmon,Wei Jin,Yiyu Shi*

Main category: cs.CV

TL;DR: FairMoE框架通过动态路由数据到最适合的专家模块，实现了在皮肤疾病诊断中既提高准确性又保持公平性。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在皮肤疾病诊断中存在跨人口群体的偏见，导致不公平的医疗结果和患者信任下降。传统方法消除敏感属性与预测的关联，但会损失临床相关线索。

Method: 提出FairMoE框架，采用分层混合专家模块作为群体特定学习器，动态路由数据以处理群体边界附近的案例。

Result: 实验表明，FairMoE在保持公平性指标的同时显著提高了准确性，优于传统方法。

Conclusion: FairMoE为AI医疗诊断中的公平性问题提供了一种有效的解决方案，同时不牺牲性能。

Abstract: AI-based systems have achieved high accuracy in skin disease diagnostics but
often exhibit biases across demographic groups, leading to inequitable
healthcare outcomes and diminished patient trust. Most existing bias mitigation
methods attempt to eliminate the correlation between sensitive attributes and
diagnostic prediction, but those methods often degrade performance due to the
lost of clinically relevant diagnostic cues. In this work, we propose an
alternative approach that incorporates sensitive attributes to achieve
fairness. We introduce FairMoE, a framework that employs layer-wise
mixture-of-experts modules to serve as group-specific learners. Unlike
traditional methods that rigidly assign data based on group labels, FairMoE
dynamically routes data to the most suitable expert, making it particularly
effective for handling cases near group boundaries. Experimental results show
that, unlike previous fairness approaches that reduce performance, FairMoE
achieves substantial accuracy improvements while preserving comparable fairness
metrics.

</details>


### [180] [Time-Contrastive Pretraining for In-Context Image and Video Segmentation](https://arxiv.org/abs/2506.17837)
*Assefa Wahd,Jacob Jaremko,Abhilash Hareendranathan*

Main category: cs.CV

TL;DR: 论文提出了一种基于时间对比自监督目标的方法（Temporal），用于视觉上下文学习（ICL），通过将其重新定义为视频对象分割（VOS）任务，解决了传统网格方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 主流ICL方法依赖网格策略，缺乏视觉应用的灵活性。本文旨在解决网格方法在上下文图像数量和分辨率上的限制。

Method: 提出Temporal方法，通过自监督学习预训练提示检索器，将ICL重新定义为VOS任务，支持可变数量的上下文图像并保持其完整分辨率。

Result: 在MICCAI FLARE 2022上，图像分割Dice分数提升10.64%（达90.95%），视频分割提升14.88%（达92.45%）。

Conclusion: Temporal方法显著提升了视觉ICL的性能，证明了其在图像和视频分割任务中的有效性。

Abstract: In-context learning (ICL) enables generalization to new tasks with minimal
labeled data. However, mainstream ICL approaches rely on a gridding strategy,
which lacks the flexibility required for vision applications. We introduce
Temporal, a time-contrastive self-supervised objective that pretrains a prompt
retriever for visual ICL, and formulate ICL as a video object segmentation
(VOS) task. Temporal addresses key limitations of grid-based methods that
restrict the number and resolution of context images. By reframing ICL as a VOS
problem, our approach supports a variable number of context images while
preserving their full resolution. To address the challenge of selecting optimal
context sets for queries, we pretrain a prompt retriever on videos via
self-supervised learning, where adjacent frames serve as positives and distant
frames as negatives. For image segmentation, the prompt retriever selects
relevant sequences that, when combined with the query, form coherent videos for
VOS processing. For video segmentation, it identifies keyframes, predicts their
masks using our ICL pipeline, and propagates them throughout the sequence. When
evaluated on MICCAI FLARE 2022, our method achieves substantial improvements
over baselines: 90.95% Dice score for image segmentation (10.64% improvement)
and 92.45% Dice for video segmentation (14.88% improvement).

</details>


### [181] [Robust Foreground-Background Separation for Severely-Degraded Videos Using Convolutional Sparse Representation Modeling](https://arxiv.org/abs/2506.17838)
*Kazuki Naganuma,Shunsuke Ono*

Main category: cs.CV

TL;DR: 提出了一种基于卷积稀疏表示（CSR）的前景-背景分离（FBS）方法，用于处理低帧率和多噪声视频。


<details>
  <summary>Details</summary>
Motivation: 现有FBS方法无法准确分离低质量视频中的前景和背景，因为它们仅捕获特定或通用特征，且未明确建模多种噪声。

Method: 结合CSR前景模型、通用特征捕获函数和显式噪声表征函数，将FBS建模为多凸优化问题，并开发交替求解算法。

Result: 实验表明，该方法在红外和显微镜视频中优于现有方法。

Conclusion: 提出的方法能有效分离前景和背景，并处理多种噪声和低帧率问题。

Abstract: This paper proposes a foreground-background separation (FBS) method with a
novel foreground model based on convolutional sparse representation (CSR). In
order to analyze the dynamic and static components of videos acquired under
undesirable conditions, such as hardware, environmental, and power limitations,
it is essential to establish an FBS method that can handle videos with low
frame rates and various types of noise. Existing FBS methods have two
limitations that prevent us from accurately separating foreground and
background components from such degraded videos. First, they only capture
either data-specific or general features of the components. Second, they do not
include explicit models for various types of noise to remove them in the FBS
process. To this end, we propose a robust FBS method with a CSR-based
foreground model. This model can adaptively capture specific spatial structures
scattered in imaging data. Then, we formulate FBS as a constrained multiconvex
optimization problem that incorporates CSR, functions that capture general
features, and explicit noise characterization functions for multiple types of
noise. Thanks to these functions, our method captures both data-specific and
general features to accurately separate the components from various types of
noise even under low frame rates. To obtain a solution of the optimization
problem, we develop an algorithm that alternately solves its two convex
subproblems by newly established algorithms. Experiments demonstrate the
superiority of our method over existing methods using two types of degraded
videos: infrared and microscope videos.

</details>


### [182] [Fetuses Made Simple: Modeling and Tracking of Fetal Shape and Pose](https://arxiv.org/abs/2506.17858)
*Yingcheng Liu,Peiqi Wang,Sebastian Diaz,Esra Abaci Turk,Benjamin Billot,Patricia Ellen Grant,Polina Golland*

Main category: cs.CV

TL;DR: 提出了一种基于SMPL的3D胎儿统计身体模型，用于改进MRI中的胎儿形状和运动分析。


<details>
  <summary>Details</summary>
Motivation: 现有方法（关键点或体积分割）在胎儿MRI分析中各有局限，前者忽略细节，后者难以处理大范围运动。

Method: 构建3D胎儿统计模型，迭代估计图像空间中的姿势和标准姿势空间中的形状。

Result: 模型在未见过的胎儿形状上达到3.2毫米的表面对齐误差，支持自动化测量和直观可视化。

Conclusion: 该模型为胎儿运动与形状分析提供了新工具，代码已开源。

Abstract: Analyzing fetal body motion and shape is paramount in prenatal diagnostics
and monitoring. Existing methods for fetal MRI analysis mainly rely on
anatomical keypoints or volumetric body segmentations. Keypoints simplify body
structure to facilitate motion analysis, but may ignore important details of
full-body shape. Body segmentations capture complete shape information but
complicate temporal analysis due to large non-local fetal movements. To address
these limitations, we construct a 3D articulated statistical fetal body model
based on the Skinned Multi-Person Linear Model (SMPL). Our algorithm
iteratively estimates body pose in the image space and body shape in the
canonical pose space. This approach improves robustness to MRI motion artifacts
and intensity distortions, and reduces the impact of incomplete surface
observations due to challenging fetal poses. We train our model on
segmentations and keypoints derived from $19,816$ MRI volumes across $53$
subjects. Our model captures body shape and motion across time series and
provides intuitive visualization. Furthermore, it enables automated
anthropometric measurements traditionally difficult to obtain from
segmentations and keypoints. When tested on unseen fetal body shapes, our
method yields a surface alignment error of $3.2$ mm for $3$ mm MRI voxel size.
To our knowledge, this represents the first 3D articulated statistical fetal
body model, paving the way for enhanced fetal motion and shape analysis in
prenatal diagnostics. The code is available at
https://github.com/MedicalVisionGroup/fetal-smpl .

</details>


### [183] [Cross-modal State Space Modeling for Real-time RGB-thermal Wild Scene Semantic Segmentation](https://arxiv.org/abs/2506.17869)
*Xiaodong Guo,Zi'ang Lin,Luwen Hu,Zhihong Deng,Tong Liu,Wujie Zhou*

Main category: cs.CV

TL;DR: CM-SSM是一种高效的RGB-热成像语义分割架构，通过跨模态状态空间建模（SSM）方法，解决了多源数据处理的计算开销问题。


<details>
  <summary>Details</summary>
Motivation: RGB和热成像数据的融合可以显著提升野外环境中语义分割的性能，但现有方法（如基于Transformer的方法）计算开销大，难以在资源受限系统中应用。

Method: 提出CM-SSM架构，包含跨模态2D选择性扫描（CM-SS2D）模块和跨模态状态空间关联（CM-SSA）模块，分别建立跨模态状态空间模型并整合全局关联与局部空间特征。

Result: 在CART数据集上达到最先进性能，参数更少且计算成本更低；在PST900数据集上验证了泛化能力。

Conclusion: CM-SSM通过线性计算复杂度解决了多源数据处理的效率问题，同时保持了高性能。

Abstract: The integration of RGB and thermal data can significantly improve semantic
segmentation performance in wild environments for field robots. Nevertheless,
multi-source data processing (e.g. Transformer-based approaches) imposes
significant computational overhead, presenting challenges for
resource-constrained systems. To resolve this critical limitation, we
introduced CM-SSM, an efficient RGB-thermal semantic segmentation architecture
leveraging a cross-modal state space modeling (SSM) approach. Our framework
comprises two key components. First, we introduced a cross-modal
2D-selective-scan (CM-SS2D) module to establish SSM between RGB and thermal
modalities, which constructs cross-modal visual sequences and derives hidden
state representations of one modality from the other. Second, we developed a
cross-modal state space association (CM-SSA) module that effectively integrates
global associations from CM-SS2D with local spatial features extracted through
convolutional operations. In contrast with Transformer-based approaches, CM-SSM
achieves linear computational complexity with respect to image resolution.
Experimental results show that CM-SSM achieves state-of-the-art performance on
the CART dataset with fewer parameters and lower computational cost. Further
experiments on the PST900 dataset demonstrate its generalizability. Codes are
available at https://github.com/xiaodonguo/CMSSM.

</details>


### [184] [SurgVidLM: Towards Multi-grained Surgical Video Understanding with Large Language Model](https://arxiv.org/abs/2506.17873)
*Guankun Wang,Wenjin Mo,Junyi Wang,Long Bai,Kun Yuan,Ming Hu,Jinlin Wu,Junjun He,Yiming Huang,Nicolas Padoy,Zhen Lei,Hongbin Liu,Nassir Navab,Hongliang Ren*

Main category: cs.CV

TL;DR: SurgVidLM是首个专为手术视频理解设计的视频语言模型，通过SVU-31K数据集和StageFocus机制，显著优于现有Vid-LLMs。


<details>
  <summary>Details</summary>
Motivation: 现有Vid-LLMs缺乏针对细粒度手术视频理解的能力，SurgVidLM填补了这一空白。

Method: 构建SVU-31K数据集，提出StageFocus机制和Multi-frequency Fusion Attention，实现多层次视频理解。

Result: SurgVidLM在全视频和细粒度理解任务中均显著优于现有Vid-LLMs。

Conclusion: SurgVidLM为手术视频理解提供了高效解决方案，展现了在复杂场景中的优越性能。

Abstract: Recent advances in Multimodal Large Language Models have demonstrated great
potential in the medical domain, facilitating users to understand surgical
scenes and procedures. Beyond image-based methods, the exploration of Video
Large Language Models (Vid-LLMs) has emerged as a promising avenue for
capturing the complex sequences of information involved in surgery. However,
there is still a lack of Vid-LLMs specialized for fine-grained surgical video
understanding tasks, which is crucial for analyzing specific processes or
details within a surgical procedure. To bridge this gap, we propose SurgVidLM,
the first video language model designed to address both full and fine-grained
surgical video comprehension. To train our SurgVidLM, we construct the SVU-31K
dataset which consists of over 31K video-instruction pairs, enabling both
holistic understanding and detailed analysis of surgical procedures.
Furthermore, we introduce the StageFocus mechanism which is a two-stage
framework performing the multi-grained, progressive understanding of surgical
videos. We also develop the Multi-frequency Fusion Attention to effectively
integrate low and high-frequency visual tokens, ensuring the retention of
critical information. Experimental results demonstrate that SurgVidLM
significantly outperforms state-of-the-art Vid-LLMs in both full and
fine-grained video understanding tasks, showcasing its superior capability in
capturing complex procedural contexts.

</details>


### [185] [StainPIDR: A Pathological Image Decouplingand Reconstruction Method for StainNormalization Based on Color VectorQuantization and Structure Restaining](https://arxiv.org/abs/2506.17879)
*Zheng Chen*

Main category: cs.CV

TL;DR: 提出了一种名为StainPIDR的染色归一化方法，通过分离图像的结构特征和颜色特征，并利用目标颜色特征重新染色，以解决病理图像颜色差异问题。


<details>
  <summary>Details</summary>
Motivation: 病理图像的颜色差异可能影响计算机辅助诊断系统的性能，因此需要一种有效的染色归一化方法。

Method: 通过解耦图像的结构特征和向量量化颜色特征，利用交叉注意力机制重新染色结构特征，并设计模板图像选择算法优化目标颜色。

Result: 实验验证了StainPIDR和模板选择算法的有效性，表明该方法在染色归一化任务中表现良好。

Conclusion: StainPIDR能有效解决病理图像颜色差异问题，代码将公开。

Abstract: The color appearance of a pathological image is highly related to the imaging
protocols, the proportion of different dyes, and the scanning devices.
Computer-aided diagnostic systems may deteriorate when facing these
color-variant pathological images. In this work, we propose a stain
normalization method called StainPIDR. We try to eliminate this color
discrepancy by decoupling the image into structure features and
vector-quantized color features, restaining the structure features with the
target color features, and decoding the stained structure features to
normalized pathological images. We assume that color features decoupled by
different images with the same color should be exactly the same. Under this
assumption, we train a fixed color vector codebook to which the decoupled color
features will map. In the restaining part, we utilize the cross-attention
mechanism to efficiently stain the structure features. As the target color
(decoupled from a selected template image) will also affect the performance of
stain normalization, we further design a template image selection algorithm to
select a template from a given dataset. In our extensive experiments, we
validate the effectiveness of StainPIDR and the template image selection
algorithm. All the results show that our method can perform well in the stain
normalization task. The code of StainPIDR will be publicly available later.

</details>


### [186] [Cloud-Aware SAR Fusion for Enhanced Optical Sensing in Space Missions](https://arxiv.org/abs/2506.17885)
*Trong-An Bui,Thanh-Thoai Le*

Main category: cs.CV

TL;DR: 提出了一种结合SAR-光学特征融合和深度学习的云去除框架，通过注意力机制和自适应损失加权提升重建效果，实验表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 云污染严重影响光学卫星影像的可用性，阻碍环境监测、灾害响应等关键应用。

Method: 采用Cloud-Attentive Reconstruction Framework，结合SAR-光学特征融合和深度学习重建，引入注意力机制和自适应损失加权策略。

Result: PSNR达31.01 dB，SSIM为0.918，MAE为0.017，表现优于现有方法。

Conclusion: 该框架能生成高保真、空间和光谱一致的云去除光学影像，效果显著。

Abstract: Cloud contamination significantly impairs the usability of optical satellite
imagery, affecting critical applications such as environmental monitoring,
disaster response, and land-use analysis. This research presents a
Cloud-Attentive Reconstruction Framework that integrates SAR-optical feature
fusion with deep learning-based image reconstruction to generate cloud-free
optical imagery. The proposed framework employs an attention-driven feature
fusion mechanism to align complementary structural information from Synthetic
Aperture Radar (SAR) with spectral characteristics from optical data.
Furthermore, a cloud-aware model update strategy introduces adaptive loss
weighting to prioritize cloud-occluded regions, enhancing reconstruction
accuracy. Experimental results demonstrate that the proposed method outperforms
existing approaches, achieving a PSNR of 31.01 dB, SSIM of 0.918, and MAE of
0.017. These outcomes highlight the framework's effectiveness in producing
high-fidelity, spatially and spectrally consistent cloud-free optical images.

</details>


### [187] [Relation3D: Enhancing Relation Modeling for Point Cloud Instance Segmentation](https://arxiv.org/abs/2506.17891)
*Jiahao Lu,Jiacheng Deng*

Main category: cs.CV

TL;DR: Relation3D通过自适应超点聚合和对比学习增强点云实例分割中的关系建模，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的方法缺乏对场景特征内部及查询特征间关系的有效建模，限制了性能。

Method: 提出自适应超点聚合模块和对比学习引导的超点优化模块，结合关系感知自注意力机制。

Result: 在多个数据集（ScanNetV2、ScanNet++等）上表现优异。

Conclusion: Relation3D通过改进关系建模，显著提升了点云实例分割的性能。

Abstract: 3D instance segmentation aims to predict a set of object instances in a
scene, representing them as binary foreground masks with corresponding semantic
labels. Currently, transformer-based methods are gaining increasing attention
due to their elegant pipelines and superior predictions. However, these methods
primarily focus on modeling the external relationships between scene features
and query features through mask attention. They lack effective modeling of the
internal relationships among scene features as well as between query features.
In light of these disadvantages, we propose \textbf{Relation3D: Enhancing
Relation Modeling for Point Cloud Instance Segmentation}. Specifically, we
introduce an adaptive superpoint aggregation module and a contrastive
learning-guided superpoint refinement module to better represent superpoint
features (scene features) and leverage contrastive learning to guide the
updates of these features. Furthermore, our relation-aware self-attention
mechanism enhances the capabilities of modeling relationships between queries
by incorporating positional and geometric relationships into the self-attention
mechanism. Extensive experiments on the ScanNetV2, ScanNet++, ScanNet200 and
S3DIS datasets demonstrate the superior performance of Relation3D.

</details>


### [188] [BeltCrack: the First Sequential-image Industrial Conveyor Belt Crack Detection Dataset and Its Baseline with Triple-domain Feature Learning](https://arxiv.org/abs/2506.17892)
*Jianghong Huang,Luping Ji,Xin Ma,Mao Ye*

Main category: cs.CV

TL;DR: 论文构建了首个真实工业场景的传送带裂缝数据集（BeltCrack14ks和BeltCrack9kd），并提出了一种基于时空频三域特征分层融合的基线方法，验证了数据集的有效性。


<details>
  <summary>Details</summary>
Motivation: 传送带裂缝对工业安全和效率至关重要，但现有数据集多为路面或合成数据，缺乏真实工业场景数据。

Method: 构建真实工业场景的传送带裂缝数据集，并提出基于时空频三域特征分层融合的基线方法。

Result: 实验证明数据集有效，且基线方法优于其他类似检测方法。

Conclusion: 论文填补了工业传送带裂缝数据集的空白，并验证了基线方法的优越性。

Abstract: Conveyor belt is a category of important equipments in modern industry,
widely applied in production and manufacturing Fields. Its health status is
much critical to operation efficiency and safety hazards. Among the factors
affecting belt health, crack is often one of the most threatening risks.
Currently, considering safety, how to intelligently detect belt cracks is
catching an increasing attention. To implement the intelligent detection with
machine learning, real crack samples are believed to be necessary. However,
existing crack datasets primarily focus on pavement scenarios or synthetic
data, no real-world industrial belt crack datasets at all. To propel machine
learning advancement in this field, this paper constructs the first
sequential-image belt crack detection datasets (BeltCrack14ks and
BeltCrack9kd), from real-world factory scenes. Furthermore, to validate
usability and effectiveness, we propose a special baseline method with
triple-domain (i.e., time-space-frequency) feature hierarchical fusion learning
for the two whole-new datasets. Experimental results demonstrate the
availability and effectiveness of our dataset. Besides, they also show that our
baseline is obviously superior to other similar detection methods. Our datasets
and source codes are available at https://github.com/UESTC-nnLab/BeltCrack.

</details>


### [189] [EgoWorld: Translating Exocentric View to Egocentric View using Rich Exocentric Observations](https://arxiv.org/abs/2506.17896)
*Junho Park,Andrew Sangwoo Ye,Taein Kwon*

Main category: cs.CV

TL;DR: EgoWorld是一个新颖的两阶段框架，通过外中心观测（如点云、3D手部姿态和文本描述）重建内中心视角，解决了现有方法依赖2D线索和多视角同步的限制。


<details>
  <summary>Details</summary>
Motivation: 内中心视觉对于人机视觉理解至关重要，尤其是在捕捉手-物体交互细节时。现有方法依赖2D线索和同步多视角设置，限制了其应用。

Method: EgoWorld通过估计外中心深度图重建点云，将其重投影到内中心视角，并应用基于扩散的图像修复生成密集且语义连贯的内中心图像。

Result: 在H2O和TACO数据集上，EgoWorld实现了最先进的性能，并对新对象、动作、场景和主体表现出强大的泛化能力。

Conclusion: EgoWorld不仅解决了现有方法的局限性，还在未标记的真实世界示例中表现出潜力。

Abstract: Egocentric vision is essential for both human and machine visual
understanding, particularly in capturing the detailed hand-object interactions
needed for manipulation tasks. Translating third-person views into first-person
views significantly benefits augmented reality (AR), virtual reality (VR) and
robotics applications. However, current exocentric-to-egocentric translation
methods are limited by their dependence on 2D cues, synchronized multi-view
settings, and unrealistic assumptions such as necessity of initial egocentric
frame and relative camera poses during inference. To overcome these challenges,
we introduce EgoWorld, a novel two-stage framework that reconstructs an
egocentric view from rich exocentric observations, including projected point
clouds, 3D hand poses, and textual descriptions. Our approach reconstructs a
point cloud from estimated exocentric depth maps, reprojects it into the
egocentric perspective, and then applies diffusion-based inpainting to produce
dense, semantically coherent egocentric images. Evaluated on the H2O and TACO
datasets, EgoWorld achieves state-of-the-art performance and demonstrates
robust generalization to new objects, actions, scenes, and subjects. Moreover,
EgoWorld shows promising results even on unlabeled real-world examples.

</details>


### [190] [PostAlign: Multimodal Grounding as a Corrective Lens for MLLMs](https://arxiv.org/abs/2506.17901)
*Yixuan Wu,Yang Zhang,Jian Wu,Philip Torr,Jindong Gu*

Main category: cs.CV

TL;DR: MMGrounded-PostAlign框架通过多模态对齐增强MLLMs的视觉理解能力，减少幻觉现象。


<details>
  <summary>Details</summary>
Motivation: 解决MLLMs在视觉语言任务中过度依赖虚假关联的问题，提升视觉信息的利用。

Method: 引入多模态接地模块（视觉和文本接地）和负拒绝机制，选择性调整推理策略。

Result: 在多个基准测试中显著提升细粒度视觉理解和幻觉抑制能力。

Conclusion: MMGrounded-PostAlign有效提升MLLMs的视觉理解能力并减少幻觉。

Abstract: Multimodal Large Language Models (MLLMs) excel in vision-language tasks, such
as image captioning and visual question answering. However, they often suffer
from over-reliance on spurious correlations, primarily due to linguistic priors
that distract the model from leveraging actual visual information. To address
these issues, we introduce MMGrounded-PostAlign, a post-multimodal alignment
framework designed to enhance the visual understanding capabilities and
mitigate the hallucinations of MLLMs. Our framework incorporates a multimodal
grounding module for both visual grounding, which identifies the referred
object in the image, and textual grounding, which generates the rationale for
the final answer, ensuring that outputs are anchored in both visual and textual
evidence. To mitigate the hallucinations, we introduce a negative rejection
mechanism in the visual grounding module to distinguish grounded entities from
non-existent objects influenced by linguistic biases. On the textual grounding
side, we propose a selective reasoning mechanism that adjusts the model's
reasoning strategy based on query complexity. Extensive evaluations are
conducted on benchmarks such as POPE, HaloQuest, VQAv2, MME, and MMBench
showing significant improvements in fine-grained visual understanding and
hallucination suppression.

</details>


### [191] [Cause-Effect Driven Optimization for Robust Medical Visual Question Answering with Language Biases](https://arxiv.org/abs/2506.17903)
*Huanjia Zhu,Yishu Liu,Xiaozhao Fang,Guangming Lu,Bingzhi Chen*

Main category: cs.CV

TL;DR: CEDO框架通过MHO、GMS和DLR三种机制从因果和效应角度全面缓解Med-VQA模型中的语言偏差。


<details>
  <summary>Details</summary>
Motivation: 现有Med-VQA模型存在语言偏差问题，即问题类型与答案类别之间的虚假相关性。

Method: 提出CEDO框架，包含MHO（模态驱动异质优化）、GMS（梯度引导模态协同）和DLR（分布适应损失重缩放）三种机制。

Result: 在多个基准测试中，CEDO表现优于现有方法。

Conclusion: CEDO能有效缓解语言偏差，提升模型鲁棒性。

Abstract: Existing Medical Visual Question Answering (Med-VQA) models often suffer from
language biases, where spurious correlations between question types and answer
categories are inadvertently established. To address these issues, we propose a
novel Cause-Effect Driven Optimization framework called CEDO, that incorporates
three well-established mechanisms, i.e., Modality-driven Heterogeneous
Optimization (MHO), Gradient-guided Modality Synergy (GMS), and
Distribution-adapted Loss Rescaling (DLR), for comprehensively mitigating
language biases from both causal and effectual perspectives. Specifically, MHO
employs adaptive learning rates for specific modalities to achieve
heterogeneous optimization, thus enhancing robust reasoning capabilities.
Additionally, GMS leverages the Pareto optimization method to foster
synergistic interactions between modalities and enforce gradient orthogonality
to eliminate bias updates, thereby mitigating language biases from the effect
side, i.e., shortcut bias. Furthermore, DLR is designed to assign adaptive
weights to individual losses to ensure balanced learning across all answer
categories, effectively alleviating language biases from the cause side, i.e.,
imbalance biases within datasets. Extensive experiments on multiple traditional
and bias-sensitive benchmarks consistently demonstrate the robustness of CEDO
over state-of-the-art competitors.

</details>


### [192] [Feedback Driven Multi Stereo Vision System for Real-Time Event Analysis](https://arxiv.org/abs/2506.17910)
*Mohamed Benkedadra,Matei Mancas,Sidi Ahmed Mahmoudi*

Main category: cs.CV

TL;DR: 提出了一种基于3D立体视觉的交互系统管道，用于处理复杂环境中的场景理解和任务执行。


<details>
  <summary>Details</summary>
Motivation: 现有2D和3D相机在复杂环境中不可靠，需要更强大的解决方案。

Method: 融合多个3D相机进行全场景重建，结合反馈机制优化决策。

Result: 初步实验展示了系统在事件识别、目标跟踪等任务中的潜力。

Conclusion: 提出了下一步研究路线图，以推动管道进入生产阶段。

Abstract: 2D cameras are often used in interactive systems. Other systems like gaming
consoles provide more powerful 3D cameras for short range depth sensing.
Overall, these cameras are not reliable in large, complex environments. In this
work, we propose a 3D stereo vision based pipeline for interactive systems,
that is able to handle both ordinary and sensitive applications, through robust
scene understanding. We explore the fusion of multiple 3D cameras to do full
scene reconstruction, which allows for preforming a wide range of tasks, like
event recognition, subject tracking, and notification. Using possible feedback
approaches, the system can receive data from the subjects present in the
environment, to learn to make better decisions, or to adapt to completely new
environments. Throughout the paper, we introduce the pipeline and explain our
preliminary experimentation and results. Finally, we draw the roadmap for the
next steps that need to be taken, in order to get this pipeline into production

</details>


### [193] [PlanMoGPT: Flow-Enhanced Progressive Planning for Text to Motion Synthesis](https://arxiv.org/abs/2506.17912)
*Chuhao Jin,Haosen Li,Bingzi Zhang,Che Liu,Xiting Wang,Ruihua Song,Wenbing Huang,Ying Qin,Fuzheng Zhang,Di Zhang*

Main category: cs.CV

TL;DR: PlanMoGPT通过渐进式规划和流增强细粒度运动标记化，解决了LLM在文本到运动生成中的性能瓶颈，显著提升了生成质量和多样性。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在文本到运动生成中表现不佳，主要由于运动标记化的粒度问题，导致局部依赖或细节丢失。

Method: 提出PlanMoGPT框架，结合渐进式规划和流增强细粒度标记化，通过分层生成和增强解码器优化运动生成。

Result: 在文本到运动基准测试中，FID分数提升63.8%，运动多样性增加49.9%，达到最先进性能。

Conclusion: PlanMoGPT成功解决了多样性-质量的权衡问题，为文本到运动生成设定了新标准。

Abstract: Recent advances in large language models (LLMs) have enabled breakthroughs in
many multimodal generation tasks, but a significant performance gap still
exists in text-to-motion generation, where LLM-based methods lag far behind
non-LLM methods. We identify the granularity of motion tokenization as a
critical bottleneck: fine-grained tokenization induces local dependency issues,
where LLMs overemphasize short-term coherence at the expense of global semantic
alignment, while coarse-grained tokenization sacrifices motion details. To
resolve this issue, we propose PlanMoGPT, an LLM-based framework integrating
progressive planning and flow-enhanced fine-grained motion tokenization. First,
our progressive planning mechanism leverages LLMs' autoregressive capabilities
to hierarchically generate motion tokens by starting from sparse global plans
and iteratively refining them into full sequences. Second, our flow-enhanced
tokenizer doubles the downsampling resolution and expands the codebook size by
eight times, minimizing detail loss during discretization, while a
flow-enhanced decoder recovers motion nuances. Extensive experiments on
text-to-motion benchmarks demonstrate that it achieves state-of-the-art
performance, improving FID scores by 63.8% (from 0.380 to 0.141) on
long-sequence generation while enhancing motion diversity by 49.9% compared to
existing methods. The proposed framework successfully resolves the
diversity-quality trade-off that plagues current non-LLM approaches,
establishing new standards for text-to-motion generation.

</details>


### [194] [IDAL: Improved Domain Adaptive Learning for Natural Images Dataset](https://arxiv.org/abs/2506.17931)
*Ravi Kant Gupta,Shounak Das,Amit Sethi*

Main category: cs.CV

TL;DR: 提出了一种新颖的无监督域适应方法，结合ResNet和FPN架构，通过定制损失函数提升模型在目标域的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 解决现有对抗域适应方法在多模态分布对齐上的不足，应对自然图像中的尺度、噪声和风格变化。

Method: 使用ResNet和FPN架构处理内容和风格特征，结合新颖和现有损失函数训练网络。

Result: 在多个数据集上优于现有CNN方法，训练收敛更快。

Conclusion: 该方法在多模态分布对齐和自然图像适应任务中表现优异。

Abstract: We present a novel approach for unsupervised domain adaptation (UDA) for
natural images. A commonly-used objective for UDA schemes is to enhance domain
alignment in representation space even if there is a domain shift in the input
space. Existing adversarial domain adaptation methods may not effectively align
different domains of multimodal distributions associated with classification
problems. Our approach has two main features. Firstly, its neural architecture
uses the deep structure of ResNet and the effective separation of scales of
feature pyramidal network (FPN) to work with both content and style features.
Secondly, it uses a combination of a novel loss function and judiciously
selected existing loss functions to train the network architecture. This
tailored combination is designed to address challenges inherent to natural
images, such as scale, noise, and style shifts, that occur on top of a
multi-modal (multi-class) distribution. The combined loss function not only
enhances model accuracy and robustness on the target domain but also speeds up
training convergence. Our proposed UDA scheme generalizes better than
state-of-the-art for CNN-based methods on Office-Home, Office-31, and
VisDA-2017 datasets and comaparable for DomainNet dataset.

</details>


### [195] [GEMeX-ThinkVG: Towards Thinking with Visual Grounding in Medical VQA via Reinforcement Learning](https://arxiv.org/abs/2506.17939)
*Bo Liu,Xiangyu Zhao,Along He,Yidi Chen,Huazhu Fu,Xiao-Ming Wu*

Main category: cs.CV

TL;DR: 论文提出了一种基于视觉定位的医学视觉问答方法（ThinkVG），通过分解推理步骤提高可解释性，并引入可验证奖励机制提升答案可靠性。


<details>
  <summary>Details</summary>
Motivation: 当前医学视觉问答方法在答案可靠性和可解释性上存在不足，影响临床决策的信任度。

Method: 提出ThinkVG数据集，分解推理步骤并显式定位图像区域；引入可验证奖励机制进行强化学习后训练。

Result: 方法仅用八分之一训练数据即达到可比性能，证明了高效性和有效性。

Conclusion: ThinkVG通过细粒度可解释性和奖励机制，显著提升了医学视觉问答的可靠性和信任度。

Abstract: Medical visual question answering aims to support clinical decision-making by
enabling models to answer natural language questions based on medical images.
While recent advances in multi-modal learning have significantly improved
performance, current methods still suffer from limited answer reliability and
poor interpretability, impairing the ability of clinicians and patients to
understand and trust model-generated answers. To address this, this work first
proposes a Thinking with Visual Grounding (ThinkVG) dataset wherein the answer
generation is decomposed into intermediate reasoning steps that explicitly
ground relevant visual regions of the medical image, thereby providing
fine-grained explainability. Furthermore, we introduce a novel verifiable
reward mechanism for reinforcement learning to guide post-training, improving
the alignment between the model's reasoning process and its final answer.
Remarkably, our method achieves comparable performance using only one-eighth of
the training data, demonstrating the efficiency and effectiveness of the
proposal. The dataset is available at
https://huggingface.co/datasets/BoKelvin/GEMeX-ThinkVG.

</details>


### [196] [SegChange-R1:Augmented Reasoning for Remote Sensing Change Detection via Large Language Models](https://arxiv.org/abs/2506.17944)
*Fei Zhou*

Main category: cs.CV

TL;DR: 提出了一种基于大语言模型（LLM）的增强推理方法（SegChange-R1），结合文本描述信息提升变化检测能力，并设计了基于线性注意力的空间变换模块（BEV），解决了模态不对齐问题。


<details>
  <summary>Details</summary>
Motivation: 提升遥感变化检测的精度和效率，特别是在建筑物变化检测中，通过整合文本信息引导模型关注感兴趣的变化区域。

Method: 结合LLM的推理方法（SegChange-R1）和基于线性注意力的BEV模块，统一不同时间视角的特征。

Result: 在四个广泛使用的变化检测数据集上表现优于现有方法，并构建了首个无人机视角的建筑物变化检测数据集（DVCD）。

Conclusion: SegChange-R1方法显著提升了变化检测性能，解决了模态不对齐问题，并提供了新的数据集支持。

Abstract: Remote sensing change detection is widely used in a variety of fields such as
urban planning, terrain and geomorphology analysis, and environmental
monitoring, mainly by analyzing the significant change differences of features
(e.g., building changes) in the same spatial region at different time phases.
In this paper, we propose a large language model (LLM) augmented inference
approach (SegChange-R1), which enhances the detection capability by integrating
textual descriptive information and aims at guiding the model to segment the
more interested change regions, thus accelerating the convergence speed.
Moreover, we design a spatial transformation module (BEV) based on linear
attention, which solves the problem of modal misalignment in change detection
by unifying features from different temporal perspectives onto the BEV space.
In addition, we construct the first dataset for building change detection from
UAV viewpoints (DVCD ), and our experiments on four widely-used change
detection datasets show a significant improvement over existing methods. The
code and pre-trained models are available in
https://github.com/Yu-Zhouz/SegChange-R1.

</details>


### [197] [Classification of Tents in Street Bazaars Using CNN](https://arxiv.org/abs/2506.17946)
*Azamat Ibragimov,Ruslan Isaev,Remudin Reshid Mekuria,Gulnaz Gimaletdinova,Dim Shaiakhmetov*

Main category: cs.CV

TL;DR: 论文提出一种改进的深度学习模型，用于街头集市帐篷分类，比较了自定义CNN与EfficientNetB0的性能，结果显示预训练模型显著提升分类效果。


<details>
  <summary>Details</summary>
Motivation: 街头集市是许多地区的重要经济中心，但其非结构化特性使帐篷分类任务具有挑战性。传统手动方法效率低下，而CNN在此类任务中的应用尚未充分探索。

Method: 使用126张原始照片及其增强图像训练自定义CNN和EfficientNetB0，通过多种性能指标（如准确率、召回率等）评估模型。

Result: 自定义CNN准确率为92.8%，EfficientNetB0达到98.4%，表明预训练模型在分类任务中表现更优。

Conclusion: 预训练模型如EfficientNetB0显著提高了分类准确性和泛化能力，适用于街头集市帐篷分类任务。

Abstract: This research paper proposes an improved deep learning model for classifying
tents in street bazaars, comparing a custom Convolutional Neural Network (CNN)
with EfficientNetB0. This is a critical task for market organization with a
tent classification, but manual methods in the past have been inefficient.
Street bazaars represent a vital economic hub in many regions, yet their
unstructured nature poses significant challenges for the automated
classification of market infrastructure, such as tents. In Kyrgyzstan, more
than a quarter of the country's GDP is derived from bazaars. While CNNs have
been widely applied to object recognition, their application to bazaar-specific
tasks remains underexplored. Here, we build upon our original approach by
training on an extended set of 126 original photographs that were augmented to
generate additional images. This dataset is publicly available for download on
Kaggle. A variety of performance metrics, such as accuracy, precision, recall,
F1 score, and mean average precision (mAP), were used to assess the models
comparatively, providing a more extensive analysis of classification
performance.
  The results show that the CNN custom model achieved 92.8% accuracy, and
EfficientNetB0 showed 98.4% accuracy results, confirming the effectiveness of
transfer learning in the bazaar image classification. Also, when analyzing the
confusion matrix, the analysis reveals the weaknesses and strengths of each
model. These findings suggest that using a pre-trained model such as
EfficientNetB0 significantly improves classification accuracy and
generalization.

</details>


### [198] [Mobile Image Analysis Application for Mantoux Skin Test](https://arxiv.org/abs/2506.17954)
*Liong Gele,Tan Chye Cheah*

Main category: cs.CV

TL;DR: 本文介绍了一款新开发的移动应用，用于通过曼托皮肤试验（TST）诊断潜伏性结核感染（LTBI）。该应用利用先进的图像处理和机器学习技术，解决了传统TST方法的低随访率、患者不适和主观解释等问题。


<details>
  <summary>Details</summary>
Motivation: 传统TST方法存在随访率低、患者不适和主观解释导致误诊的问题，需要更准确和自动化的解决方案。

Method: 应用采用缩放贴纸作为参考物，结合ARCore和DeepLabv3等图像处理和机器学习技术，进行皮肤硬结的精确测量。

Result: 与标准临床实践相比，该应用显著提高了准确性和可靠性。

Conclusion: 该创新对结核病管理至关重要，未来将优化算法和扩展功能。

Abstract: This paper presents a newly developed mobile application designed to diagnose
Latent Tuberculosis Infection (LTBI) using the Mantoux Skin Test (TST).
Traditional TST methods often suffer from low follow-up return rates, patient
discomfort, and subjective manual interpretation, particularly with the
ball-point pen method, leading to misdiagnosis and delayed treatment. Moreover,
previous developed mobile applications that used 3D reconstruction, this app
utilizes scaling stickers as reference objects for induration measurement. This
mobile application integrates advanced image processing technologies, including
ARCore, and machine learning algorithms such as DeepLabv3 for robust image
segmentation and precise measurement of skin indurations indicative of LTBI.
The system employs an edge detection algorithm to enhance accuracy. The
application was evaluated against standard clinical practices, demonstrating
significant improvements in accuracy and reliability. This innovation is
crucial for effective tuberculosis management, especially in resource-limited
regions. By automating and standardizing TST evaluations, the application
enhances the accessibility and efficiency of TB di-agnostics. Future work will
focus on refining machine learning models, optimizing measurement algorithms,
expanding functionalities to include comprehensive patient data management, and
enhancing ARCore's performance across various lighting conditions and
operational settings.

</details>


### [199] [ELMAR: Enhancing LiDAR Detection with 4D Radar Motion Awareness and Cross-modal Uncertainty](https://arxiv.org/abs/2506.17958)
*Xiangyuan Peng,Miao Tang,Huawei Sun,Bierzynski Kay,Lorenzo Servadei,Robert Wille*

Main category: cs.CV

TL;DR: 论文提出了一种通过4D雷达运动状态和跨模态不确定性增强的LiDAR检测框架，以解决LiDAR与4D雷达融合中的不对齐问题。


<details>
  <summary>Details</summary>
Motivation: LiDAR和4D雷达在自动驾驶和机器人技术中各有优势，但融合时存在模态不对齐问题。

Method: 使用动态运动感知编码模块提取4D雷达的运动信息，并通过估计边界框的实例级不确定性来优化LiDAR预测。

Result: 在VoD数据集上表现优异，整体区域mAP达74.89%，驾驶走廊内达88.70%，实时推理速度为30.02 FPS。

Conclusion: 该方法有效解决了跨模态不对齐问题，并实现了高性能和实时性。

Abstract: LiDAR and 4D radar are widely used in autonomous driving and robotics. While
LiDAR provides rich spatial information, 4D radar offers velocity measurement
and remains robust under adverse conditions. As a result, increasing studies
have focused on the 4D radar-LiDAR fusion method to enhance the perception.
However, the misalignment between different modalities is often overlooked. To
address this challenge and leverage the strengths of both modalities, we
propose a LiDAR detection framework enhanced by 4D radar motion status and
cross-modal uncertainty. The object movement information from 4D radar is first
captured using a Dynamic Motion-Aware Encoding module during feature extraction
to enhance 4D radar predictions. Subsequently, the instance-wise uncertainties
of bounding boxes are estimated to mitigate the cross-modal misalignment and
refine the final LiDAR predictions. Extensive experiments on the View-of-Delft
(VoD) dataset highlight the effectiveness of our method, achieving
state-of-the-art performance with the mAP of 74.89% in the entire area and
88.70% within the driving corridor while maintaining a real-time inference
speed of 30.02 FPS.

</details>


### [200] [BPCLIP: A Bottom-up Image Quality Assessment from Distortion to Semantics Based on CLIP](https://arxiv.org/abs/2506.17969)
*Chenyue Song,Chen Hui,Wei Zhang,Haiqi Zhu,Shaohui Liu,Hong Huang,Feng Jiang*

Main category: cs.CV

TL;DR: 提出了一种基于CLIP的自底向上图像质量评估方法BPCLIP，通过多尺度交叉注意力模块和文本编码器增强图像质量与人类语言的关联，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常线性融合多尺度特征，未能充分捕捉失真对语义内容的影响。

Method: 利用CLIP模型，设计自底向上的多尺度交叉注意力模块，结合40个图像质量形容词，生成图像内在质量表示。

Result: 在多数公开的FR和NR IQA基准测试中表现优异，且更具鲁棒性。

Conclusion: BPCLIP通过结合多尺度特征和语言表示，显著提升了图像质量评估的性能和鲁棒性。

Abstract: Image Quality Assessment (IQA) aims to evaluate the perceptual quality of
images based on human subjective perception. Existing methods generally combine
multiscale features to achieve high performance, but most rely on
straightforward linear fusion of these features, which may not adequately
capture the impact of distortions on semantic content. To address this, we
propose a bottom-up image quality assessment approach based on the Contrastive
Language-Image Pre-training (CLIP, a recently proposed model that aligns images
and text in a shared feature space), named BPCLIP, which progressively extracts
the impact of low-level distortions on high-level semantics. Specifically, we
utilize an encoder to extract multiscale features from the input image and
introduce a bottom-up multiscale cross attention module designed to capture the
relationships between shallow and deep features. In addition, by incorporating
40 image quality adjectives across six distinct dimensions, we enable the
pre-trained CLIP text encoder to generate representations of the intrinsic
quality of the image, thereby strengthening the connection between image
quality perception and human language. Our method achieves superior results on
most public Full-Reference (FR) and No-Reference (NR) IQA benchmarks, while
demonstrating greater robustness.

</details>


### [201] [Enabling PSO-Secure Synthetic Data Sharing Using Diversity-Aware Diffusion Models](https://arxiv.org/abs/2506.17975)
*Mischa Dombrowski,Bernhard Kainz*

Main category: cs.CV

TL;DR: 论文提出了一种通用框架，通过最大化合成数据的多样性来保护隐私，同时保持接近真实数据的性能。


<details>
  <summary>Details</summary>
Motivation: 合成数据在医学影像中具有隐私保护潜力，但现有方法在法规遵从性和性能上存在不足。

Method: 提出了一种基于扩散模型的通用框架，生成非个人化的合成数据，确保隐私保护。

Result: 合成数据性能接近真实数据（差距1%以内），显著优于不保障隐私的现有方法。

Conclusion: 通过最大化多样性实现隐私保护，同时保持高性能，为合成数据的应用提供了新思路。

Abstract: Synthetic data has recently reached a level of visual fidelity that makes it
nearly indistinguishable from real data, offering great promise for
privacy-preserving data sharing in medical imaging. However, fully synthetic
datasets still suffer from significant limitations: First and foremost, the
legal aspect of sharing synthetic data is often neglected and data regulations,
such as the GDPR, are largley ignored. Secondly, synthetic models fall short of
matching the performance of real data, even for in-domain downstream
applications. Recent methods for image generation have focused on maximising
image diversity instead of fidelity solely to improve the mode coverage and
therefore the downstream performance of synthetic data. In this work, we shift
perspective and highlight how maximizing diversity can also be interpreted as
protecting natural persons from being singled out, which leads to predicate
singling-out (PSO) secure synthetic datasets. Specifically, we propose a
generalisable framework for training diffusion models on personal data which
leads to unpersonal synthetic datasets achieving performance within one
percentage point of real-data models while significantly outperforming
state-of-the-art methods that do not ensure privacy. Our code is available at
https://github.com/MischaD/Trichotomy.

</details>


### [202] [Fast Neural Inverse Kinematics on Human Body Motions](https://arxiv.org/abs/2506.17996)
*David Tolpin,Sefy Kagarlitsky*

Main category: cs.CV

TL;DR: 提出了一种快速可靠的神经逆运动学框架，用于从3D关键点实时捕捉人体运动。


<details>
  <summary>Details</summary>
Motivation: 无标记运动捕捉虽然灵活且成本低，但通常计算需求高且推理速度慢，限制了实时应用。

Method: 详细描述了网络架构、训练方法和推理过程，并通过消融研究支持关键设计决策。

Result: 框架在定性和定量上均得到评估，表现出色。

Conclusion: 该框架为实时人体运动捕捉提供了一种高效解决方案。

Abstract: Markerless motion capture enables the tracking of human motion without
requiring physical markers or suits, offering increased flexibility and reduced
costs compared to traditional systems. However, these advantages often come at
the expense of higher computational demands and slower inference, limiting
their applicability in real-time scenarios. In this technical report, we
present a fast and reliable neural inverse kinematics framework designed for
real-time capture of human body motions from 3D keypoints. We describe the
network architecture, training methodology, and inference procedure in detail.
Our framework is evaluated both qualitatively and quantitatively, and we
support key design decisions through ablation studies.

</details>


### [203] [OSDMamba: Enhancing Oil Spill Detection from Remote Sensing Images Using Selective State Space Model](https://arxiv.org/abs/2506.18006)
*Shuaiyu Chen,Fu Wang,Peng Ren,Chunbo Luo,Zeyu Fu*

Main category: cs.CV

TL;DR: 本文提出了一种基于Mamba的OSDMamba架构，用于解决油污检测中的小目标检测和类别不平衡问题，实验表明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 油污检测中样本标记少、类别不平衡以及现有CNN方法对小目标检测效果不佳，促使研究者探索State-Space Models（如Mamba）的潜力。

Method: 提出OSDMamba架构，利用Mamba的选择性扫描机制扩大感受野，并结合非对称解码器和深度监督增强多尺度特征融合。

Result: 在两个公开数据集上，OSDMamba的性能分别提升了8.9%和11.8%，达到最优水平。

Conclusion: OSDMamba通过结合Mamba和创新的解码器设计，显著提升了油污检测的准确性和对小目标的敏感性。

Abstract: Semantic segmentation is commonly used for Oil Spill Detection (OSD) in
remote sensing images. However, the limited availability of labelled oil spill
samples and class imbalance present significant challenges that can reduce
detection accuracy. Furthermore, most existing methods, which rely on
convolutional neural networks (CNNs), struggle to detect small oil spill areas
due to their limited receptive fields and inability to effectively capture
global contextual information. This study explores the potential of State-Space
Models (SSMs), particularly Mamba, to overcome these limitations, building on
their recent success in vision applications. We propose OSDMamba, the first
Mamba-based architecture specifically designed for oil spill detection.
OSDMamba leverages Mamba's selective scanning mechanism to effectively expand
the model's receptive field while preserving critical details. Moreover, we
designed an asymmetric decoder incorporating ConvSSM and deep supervision to
strengthen multi-scale feature fusion, thereby enhancing the model's
sensitivity to minority class samples. Experimental results show that the
proposed OSDMamba achieves state-of-the-art performance, yielding improvements
of 8.9% and 11.8% in OSD across two publicly available datasets.

</details>


### [204] [On the Robustness of Human-Object Interaction Detection against Distribution Shift](https://arxiv.org/abs/2506.18021)
*Chi Xie,Shuang Liang,Jie Li,Feng Zhu,Rui Zhao,Yichen Wei,Shengjie Zhao*

Main category: cs.CV

TL;DR: 本文研究了人类-物体交互（HOI）检测在分布偏移下的鲁棒性问题，提出了首个自动化评估基准，分析了40多种现有模型的不足，并提出两种简单有效的改进方法。


<details>
  <summary>Details</summary>
Motivation: 现有HOI检测研究集中于理想场景，忽略了实际中的分布偏移问题，限制了其实际应用。

Method: 提出自动化评估基准，分析现有模型；提出跨域数据增强与特征融合策略。

Result: 实验表明，所提方法显著提升了多种模型的鲁棒性，并在标准基准上也有改进。

Conclusion: 通过简单的方法显著提升HOI检测的鲁棒性，为实际应用提供了支持。

Abstract: Human-Object Interaction (HOI) detection has seen substantial advances in
recent years. However, existing works focus on the standard setting with ideal
images and natural distribution, far from practical scenarios with inevitable
distribution shifts. This hampers the practical applicability of HOI detection.
In this work, we investigate this issue by benchmarking, analyzing, and
enhancing the robustness of HOI detection models under various distribution
shifts. We start by proposing a novel automated approach to create the first
robustness evaluation benchmark for HOI detection. Subsequently, we evaluate
more than 40 existing HOI detection models on this benchmark, showing their
insufficiency, analyzing the features of different frameworks, and discussing
how the robustness in HOI is different from other tasks. With the insights from
such analyses, we propose to improve the robustness of HOI detection methods
through: (1) a cross-domain data augmentation integrated with mixup, and (2) a
feature fusion strategy with frozen vision foundation models. Both are simple,
plug-and-play, and applicable to various methods. Our experimental results
demonstrate that the proposed approach significantly increases the robustness
of various methods, with benefits on standard benchmarks, too. The dataset and
code will be released.

</details>


### [205] [PP-DocBee2: Improved Baselines with Efficient Data for Multimodal Document Understanding](https://arxiv.org/abs/2506.18023)
*Kui Huang,Xinrong Chen,Wenyu Lv,Jincheng Liao,Guanzhong Wang,Yi Liu*

Main category: cs.CV

TL;DR: PP-DocBee2是PP-DocBee的升级版，通过改进合成数据质量、视觉特征融合策略和推理方法，提升了多模态文档理解能力，性能提升11.4%，推理延迟降低73%。


<details>
  <summary>Details</summary>
Motivation: 解决PP-DocBee在多模态文档理解中的局限性，提升模型性能。

Method: 采用大规模多模态预训练模型评估数据，优化数据质量；分解ViT层并应用新特征融合策略增强表示能力。

Result: 性能提升11.4%，推理延迟降低73%。

Conclusion: PP-DocBee2通过技术创新显著提升了多模态文档理解能力，代码和预训练模型已开源。

Abstract: This report introduces PP-DocBee2, an advanced version of the PP-DocBee,
designed to enhance multimodal document understanding. Built on a large
multimodal model architecture, PP-DocBee2 addresses the limitations of its
predecessor through key technological improvements, including enhanced
synthetic data quality, improved visual feature fusion strategy, and optimized
inference methodologies. These enhancements yield an $11.4\%$ performance boost
on internal benchmarks for Chinese business documents, and reduce inference
latency by $73.0\%$ to the vanilla version. A key innovation of our work is a
data quality optimization strategy for multimodal document tasks. By employing
a large-scale multimodal pre-trained model to evaluate data, we apply a novel
statistical criterion to filter outliers, ensuring high-quality training data.
Inspired by insights into underutilized intermediate features in multimodal
models, we enhance the ViT representational capacity by decomposing it into
layers and applying a novel feature fusion strategy to improve complex
reasoning. The source code and pre-trained model are available at
\href{https://github.com/PaddlePaddle/PaddleMIX}{https://github.com/PaddlePaddle/PaddleMIX}.

</details>


### [206] [MiCo: Multiple Instance Learning with Context-Aware Clustering for Whole Slide Image Analysis](https://arxiv.org/abs/2506.18028)
*Junjian Li,Hulin Kuang,Jin Liu,Hailin Yue,Mengshen He,Jianxin Wang*

Main category: cs.CV

TL;DR: 提出了一种名为MiCo的多实例学习框架，通过上下文感知聚类增强WSI中跨区域组织相关性。


<details>
  <summary>Details</summary>
Motivation: 解决WSI中组织分布分散和跨区域空间交互难以建模的问题。

Method: 采用聚类提取形态模式，通过Cluster Route模块动态链接相同组织类型的实例，Cluster Reducer模块整合冗余锚点。

Result: 在多个癌症数据集上表现优于现有方法。

Conclusion: MiCo有效增强了WSI中组织的语义关联和相关性。

Abstract: Multiple instance learning (MIL) has shown significant promise in
histopathology whole slide image (WSI) analysis for cancer diagnosis and
prognosis. However, the inherent spatial heterogeneity of WSIs presents
critical challenges, as morphologically similar tissue types are often
dispersed across distant anatomical regions. Conventional MIL methods struggle
to model these scattered tissue distributions and capture cross-regional
spatial interactions effectively. To address these limitations, we propose a
novel Multiple instance learning framework with Context-Aware Clustering
(MiCo), designed to enhance cross-regional intra-tissue correlations and
strengthen inter-tissue semantic associations in WSIs. MiCo begins by
clustering instances to distill discriminative morphological patterns, with
cluster centroids serving as semantic anchors. To enhance cross-regional
intra-tissue correlations, MiCo employs a Cluster Route module, which
dynamically links instances of the same tissue type across distant regions via
feature similarity. These semantic anchors act as contextual hubs, propagating
semantic relationships to refine instance-level representations. To eliminate
semantic fragmentation and strengthen inter-tissue semantic associations, MiCo
integrates a Cluster Reducer module, which consolidates redundant anchors while
enhancing information exchange between distinct semantic groups. Extensive
experiments on two challenging tasks across nine large-scale public cancer
datasets demonstrate the effectiveness of MiCo, showcasing its superiority over
state-of-the-art methods. The code is available at
https://github.com/junjianli106/MiCo.

</details>


### [207] [Pre-Trained LLM is a Semantic-Aware and Generalizable Segmentation Booster](https://arxiv.org/abs/2506.18034)
*Fenghe Tang,Wenxin Ma,Zhiyang He,Xiaodong Tao,Zihang Jiang,S. Kevin Zhou*

Main category: cs.CV

TL;DR: 论文提出了一种结合预训练冻结LLM层的混合结构（LLM4Seg），用于医学图像分割任务，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 探索预训练LLM的语义理解能力是否能迁移到视觉任务中，尤其是医学图像分割。

Method: 设计了一个简单的混合结构，将冻结的预训练LLM层集成到CNN编码器-解码器分割框架中。

Result: 该方法在多种模态（超声、皮肤镜、息肉镜、CT）中均提升了分割性能，且参数增加极少。

Conclusion: LLM的语义感知能力可以增强分割任务，提供更好的全局理解和局部建模能力，且对不同LLM均有效。

Abstract: With the advancement of Large Language Model (LLM) for natural language
processing, this paper presents an intriguing finding: a frozen pre-trained LLM
layer can process visual tokens for medical image segmentation tasks.
Specifically, we propose a simple hybrid structure that integrates a
pre-trained, frozen LLM layer within the CNN encoder-decoder segmentation
framework (LLM4Seg). Surprisingly, this design improves segmentation
performance with a minimal increase in trainable parameters across various
modalities, including ultrasound, dermoscopy, polypscopy, and CT scans. Our
in-depth analysis reveals the potential of transferring LLM's semantic
awareness to enhance segmentation tasks, offering both improved global
understanding and better local modeling capabilities. The improvement proves
robust across different LLMs, validated using LLaMA and DeepSeek.

</details>


### [208] [CmFNet: Cross-modal Fusion Network for Weakly-supervised Segmentation of Medical Images](https://arxiv.org/abs/2506.18042)
*Dongdong Meng,Sheng Li,Hao Wu,Suqing Tian,Wenjun Ma,Guoping Wang,Xueqing Yan*

Main category: cs.CV

TL;DR: 提出了一种名为CmFNet的3D弱监督跨模态医学图像分割方法，通过结合多模态信息和混合监督策略，有效解决了稀疏标注导致的性能下降和过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 高质量的密集标注成本高且耗时，而弱监督学习虽能利用稀疏标注，但存在性能下降和过拟合的挑战。

Method: CmFNet包含三个组件：模态特定特征学习网络、跨模态特征学习网络和混合监督学习策略，通过多模态信息整合和多种监督方式提升分割性能。

Result: 在临床跨模态鼻咽癌数据集和公开腹部器官数据集上，CmFNet优于现有弱监督方法，甚至在某些情况下超越全监督方法。

Conclusion: CmFNet能有效解决弱监督学习中的问题，为临床治疗提供支持，适用于多种医学专家。

Abstract: Accurate automatic medical image segmentation relies on high-quality, dense
annotations, which are costly and time-consuming. Weakly supervised learning
provides a more efficient alternative by leveraging sparse and coarse
annotations instead of dense, precise ones. However, segmentation performance
degradation and overfitting caused by sparse annotations remain key challenges.
To address these issues, we propose CmFNet, a novel 3D weakly supervised
cross-modal medical image segmentation approach. CmFNet consists of three main
components: a modality-specific feature learning network, a cross-modal feature
learning network, and a hybrid-supervised learning strategy. Specifically, the
modality-specific feature learning network and the cross-modal feature learning
network effectively integrate complementary information from multi-modal
images, enhancing shared features across modalities to improve segmentation
performance. Additionally, the hybrid-supervised learning strategy guides
segmentation through scribble supervision, intra-modal regularization, and
inter-modal consistency, modeling spatial and contextual relationships while
promoting feature alignment. Our approach effectively mitigates overfitting,
delivering robust segmentation results. It excels in segmenting both
challenging small tumor regions and common anatomical structures. Extensive
experiments on a clinical cross-modal nasopharyngeal carcinoma (NPC) dataset
(including CT and MR imaging) and the publicly available CT Whole Abdominal
Organ dataset (WORD) show that our approach outperforms state-of-the-art weakly
supervised methods. In addition, our approach also outperforms fully supervised
methods when full annotation is used. Our approach can facilitate clinical
therapy and benefit various specialists, including physicists, radiologists,
pathologists, and oncologists.

</details>


### [209] [CLGRPO: Reasoning Ability Enhancement for Small VLMs](https://arxiv.org/abs/2506.18048)
*Fanyi Wang,Binzhi Dong,Haotian Hu,Jinjin Xu,Zhiwang Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种增量训练策略（Incremental Training Strategy），通过自监督的链式思维（COT）数据构建系统和分阶段优化方法，显著提升了小规模视觉语言模型（SVLMs）的推理能力。


<details>
  <summary>Details</summary>
Motivation: SVLMs因参数规模小（≤2B）而具有低成本优势，但其推理能力受限。本文旨在通过优化训练策略提升其性能。

Method: 1. 构建自监督COT数据系统；2. 分四阶段优化：SFT注入知识、GRPO对齐格式、GRPO增强推理、CLGRPO约束训练空间。

Result: 在EMOSet-118K数据集上，1B SVLM的准确率提升2.77，召回率提升0.69，性能接近8B模型。

Conclusion: 增量训练策略有效提升了SVLMs的推理能力，使其在低成本下达到更高性能。

Abstract: Small Vision Language Models (SVLMs) generally refer to models with parameter
sizes less than or equal to 2B. Their low cost and power consumption
characteristics confer high commercial value. However, their reasoning
abilities are limited by the number of parameters. To address this issue, this
paper proposes a post-training optimization paradigm called the Incremental
Training Strategy to enhance the reasoning ability of SVLMs. Firstly, we
constructed a Self-Supervised Chain-of-Thought (COT) Data Construction System,
which leverages multiple LVLMs with 7B parameters or more to transform original
data into COT data in a self-supervised manner. Our proposed Incremental
Training Strategy consists of four stages. Stage 1 injects domain knowledge by
performing Supervised Fine-Tuning (SFT) to the pretrained model on the COT
data. Stage 2 aligns the COT data format by conducting a small amount of Group
Relative Policy Optimization (GRPO) training constrained only by format rewards
on the COT data. Stage 3 enhances reasoning ability by applying GRPO training
on the COT data with constraints on both format and accuracy rewards. The
resulting model shows significant improvement compared to the baseline. Stage 4
addresses the limited capacity of the SVLMs and the weak ability to capture
complex patterns by proposing ClipLow GRPO (CLGRPO) to constrain the capture
space of the training process. We conducted extensive comparative and ablation
experiments on the abstract semantic recognition dataset EMOSet-118K.
Experimental results demonstrate that our method significantly improves the
reasoning ability of 1B SVLM. Compared to the baseline model fine-tuned on the
original data, accuracy increased by 2.77 and recall by 0.69, achieving
performance comparable to that of 8B models.

</details>


### [210] [Deep Supervised LSTM for 3D morphology estimation from Multi-View RGB Images of Wheat Spikes](https://arxiv.org/abs/2506.18060)
*Olivia Zumsteg,Nico Graf,Aaron Haeusler,Norbert Kirchgessner,Nicola Storni,Lukas Roth,Andreas Hund*

Main category: cs.CV

TL;DR: 论文提出了一种基于深度学习的2D图像体积估计方法，用于小麦穗的三维形态特征分析，通过结合DINOv2和LSTM网络，显著优于传统几何方法。


<details>
  <summary>Details</summary>
Motivation: 由于2D图像在深度信息丢失、投影失真和遮挡等问题下难以准确估计三维形态特征，论文旨在开发一种非破坏性方法，利用RGB图像序列和3D扫描数据作为参考。

Method: 采用DINOv2（自监督视觉Transformer）和单向LSTM网络的迁移学习管道，结合深度监督学习，提升模型的泛化能力。

Result: 模型在六视角室内图像上的平均绝对百分比误差（MAPE）为6.46%，优于传统方法（9.36%和13.98%），在单图像野外数据上MAPE为10.82%。

Conclusion: 深度学习模型在复杂几何形状（如小麦穗）的体积估计上优于传统几何方法，形状对预测精度有显著影响。

Abstract: Estimating three-dimensional morphological traits from two-dimensional RGB
images presents inherent challenges due to the loss of depth information,
projection distortions, and occlusions under field conditions. In this work, we
explore multiple approaches for non-destructive volume estimation of wheat
spikes, using RGB image sequences and structured-light 3D scans as ground truth
references. Due to the complex geometry of the spikes, we propose a neural
network approach for volume estimation in 2D images, employing a transfer
learning pipeline that combines DINOv2, a self-supervised Vision Transformer,
with a unidirectional Long Short-Term Memory (LSTM) network. By using deep
supervision, the model is able to learn more robust intermediate
representations, which enhances its generalisation ability across varying
evaluation sequences. We benchmark our model against two conventional
baselines: a 2D area-based projection and a geometric reconstruction using
axis-aligned cross-sections. Our deep supervised model achieves a mean absolute
percentage error (MAPE) of 6.46% on six-view indoor images, outperforming the
area (9.36%) and geometric (13.98%) baselines. Fine-tuning the model on
field-based single-image data enables domain adaptation, yielding a MAPE of
10.82%. We demonstrate that object shape significantly impacts volume
prediction accuracy, with irregular geometries such as wheat spikes posing
greater challenges for geometric methods compared to our deep learning
approach.

</details>


### [211] [Training-free Test-time Improvement for Explainable Medical Image Classification](https://arxiv.org/abs/2506.18070)
*Hangzhou He,Jiachen Tang,Lei Zhu,Kaiwen Li,Yanye Lu*

Main category: cs.CV

TL;DR: 论文提出了一种无需训练的混淆概念识别策略，用于提升概念瓶颈模型（CBMs）在新环境中的性能，同时不牺牲源域准确性。


<details>
  <summary>Details</summary>
Motivation: 解决CBMs在部署到新环境时因成像协议和染色方法差异导致的概念级偏移问题，以及因缺乏专家标注概念标签而难以微调的挑战。

Method: 利用少量新数据（每类4张图像）和仅图像级标签，通过屏蔽误激活的混淆概念和增强未充分激活的判别概念来优化模型。

Result: 方法在皮肤和白细胞图像上验证有效，提升了域外性能。

Conclusion: 提出的策略为CBMs在医疗图像分类中的实际部署提供了一种高效且低成本的解决方案。

Abstract: Deep learning-based medical image classification techniques are rapidly
advancing in medical image analysis, making it crucial to develop accurate and
trustworthy models that can be efficiently deployed across diverse clinical
scenarios. Concept Bottleneck Models (CBMs), which first predict a set of
explainable concepts from images and then perform classification based on these
concepts, are increasingly being adopted for explainable medical image
classification. However, the inherent explainability of CBMs introduces new
challenges when deploying trained models to new environments. Variations in
imaging protocols and staining methods may induce concept-level shifts, such as
alterations in color distribution and scale. Furthermore, since CBM training
requires explicit concept annotations, fine-tuning models solely with
image-level labels could compromise concept prediction accuracy and
faithfulness - a critical limitation given the high cost of acquiring
expert-annotated concept labels in medical domains. To address these
challenges, we propose a training-free confusion concept identification
strategy. By leveraging minimal new data (e.g., 4 images per class) with only
image-level labels, our approach enhances out-of-domain performance without
sacrificing source domain accuracy through two key operations: masking
misactivated confounding concepts and amplifying under-activated discriminative
concepts. The efficacy of our method is validated on both skin and white blood
cell images. Our code is available at:
https://github.com/riverback/TF-TTI-XMed.

</details>


### [212] [MUPA: Towards Multi-Path Agentic Reasoning for Grounded Video Question Answering](https://arxiv.org/abs/2506.18071)
*Jisheng Dang,Huilin Song,Junbin Xiao,Bimei Wang,Han Peng,Haoxuan Li,Xun Yang,Meng Wang,Tat-Seng Chua*

Main category: cs.CV

TL;DR: MUPA是一种多路径协作方法，通过视频定位、问题回答、答案反思和聚合来解决Grounded VideoQA问题，显著提高了定位准确性且不牺牲答案精度。


<details>
  <summary>Details</summary>
Motivation: 现代多模态模型依赖语言先验和虚假关联，导致预测缺乏视觉证据支持。

Method: MUPA采用多路径协作设计，包括视频定位、问题回答、答案反思和聚合，通过三种不同的推理路径和反思代理实现一致的结果。

Result: MUPA在2B参数下优于7B规模的竞争对手，7B参数时在NExT-GQA和DeVE-QA上分别达到30.3%和47.4%的Acc@GQA。

Conclusion: MUPA为可信赖的视频-语言理解提供了有效解决方案。

Abstract: Grounded Video Question Answering (Grounded VideoQA) requires aligning
textual answers with explicit visual evidence. However, modern multimodal
models often rely on linguistic priors and spurious correlations, resulting in
poorly grounded predictions. In this work, we propose MUPA, a cooperative
MUlti-Path Agentic approach that unifies video grounding, question answering,
answer reflection and aggregation to tackle Grounded VideoQA. MUPA features
three distinct reasoning paths on the interplay of grounding and QA agents in
different chronological orders, along with a dedicated reflection agent to
judge and aggregate the multi-path results to accomplish consistent QA and
grounding. This design markedly improves grounding fidelity without sacrificing
answer accuracy. Despite using only 2B parameters, our method outperforms all
7B-scale competitors. When scaled to 7B parameters, MUPA establishes new
state-of-the-art results, with Acc@GQA of 30.3% and 47.4% on NExT-GQA and
DeVE-QA respectively, demonstrating MUPA' effectiveness towards trustworthy
video-language understanding. Our code is available in
https://github.com/longmalongma/MUPA.

</details>


### [213] [TEM^3-Learning: Time-Efficient Multimodal Multi-Task Learning for Advanced Assistive Driving](https://arxiv.org/abs/2506.18084)
*Wenzhuo Liu,Yicheng Qiao,Zhen Wang,Qiannan Guo,Zilong Chen,Meihua Zhou,Xinran Li,Letian Wang,Zhiwei Li,Huaping Liu,Wenshuo Wang*

Main category: cs.CV

TL;DR: TEM^3-Learning是一种高效的多模态多任务学习框架，通过两阶段架构优化驾驶辅助任务，实现高精度和实时性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法受限于单模态和低效架构，无法全面理解场景并实时部署。

Method: 提出MTS-Mamba子网络提取时空特征，结合MGMI模块自适应整合多模态特征。

Result: 在AIDE数据集上实现SOTA精度，参数少于600万，推理速度142.32 FPS。

Conclusion: TEM^3-Learning通过高效架构和模块设计，显著提升多任务学习性能。

Abstract: Multi-task learning (MTL) can advance assistive driving by exploring
inter-task correlations through shared representations. However, existing
methods face two critical limitations: single-modality constraints limiting
comprehensive scene understanding and inefficient architectures impeding
real-time deployment. This paper proposes TEM^3-Learning (Time-Efficient
Multimodal Multi-task Learning), a novel framework that jointly optimizes
driver emotion recognition, driver behavior recognition, traffic context
recognition, and vehicle behavior recognition through a two-stage architecture.
The first component, the mamba-based multi-view temporal-spatial feature
extraction subnetwork (MTS-Mamba), introduces a forward-backward temporal
scanning mechanism and global-local spatial attention to efficiently extract
low-cost temporal-spatial features from multi-view sequential images. The
second component, the MTL-based gated multimodal feature integrator (MGMI),
employs task-specific multi-gating modules to adaptively highlight the most
relevant modality features for each task, effectively alleviating the negative
transfer problem in MTL. Evaluation on the AIDE dataset, our proposed model
achieves state-of-the-art accuracy across all four tasks, maintaining a
lightweight architecture with fewer than 6 million parameters and delivering an
impressive 142.32 FPS inference speed. Rigorous ablation studies further
validate the effectiveness of the proposed framework and the independent
contributions of each module. The code is available on
https://github.com/Wenzhuo-Liu/TEM3-Learning.

</details>


### [214] [ShareGPT-4o-Image: Aligning Multimodal Models with GPT-4o-Level Image Generation](https://arxiv.org/abs/2506.18095)
*Junying Chen,Zhenyang Cai,Pengcheng Chen,Shunian Chen,Ke Ji,Xidong Wang,Yunjin Yang,Benyou Wang*

Main category: cs.CV

TL;DR: 论文介绍了ShareGPT-4o-Image数据集和Janus-4o模型，旨在开源多模态图像生成能力。


<details>
  <summary>Details</summary>
Motivation: 当前领先的多模态生成模型（如GPT-4o-Image）是专有且不可访问的，作者希望通过开源数据集和模型推动开放研究。

Method: 使用GPT-4o生成合成数据集ShareGPT-4o-Image，并基于此训练Janus-4o模型，支持文本到图像和文本加图像到图像的生成。

Result: Janus-4o在文本到图像生成上超越前代模型，并首次支持文本加图像到图像的生成，仅用91K样本和6小时训练即取得显著效果。

Conclusion: 开源ShareGPT-4o-Image和Janus-4o将促进逼真、指令对齐的图像生成研究。

Abstract: Recent advances in multimodal generative models have unlocked photorealistic,
instruction-aligned image generation, yet leading systems like GPT-4o-Image
remain proprietary and inaccessible. To democratize these capabilities, we
present ShareGPT-4o-Image, the first dataset comprising 45K text-to-image and
46K text-and-image-to-image data, all synthesized using GPT-4o's image
generation capabilities for distilling its advanced image generation abilities.
Leveraging this dataset, we develop Janus-4o, a multimodal large language model
capable of both text-to-image and text-and-image-to-image generation. Janus-4o
not only significantly improves text-to-image generation over its predecessor,
Janus-Pro, but also newly supports text-and-image-to-image generation. Notably,
it achieves impressive performance in text-and-image-to-image generation from
scratch, using only 91K synthetic samples and 6 hours of training on an 8
A800-GPU machine. We hope the release of ShareGPT-4o-Image and Janus-4o will
foster open research in photorealistic, instruction-aligned image generation.

</details>


### [215] [Enhancing VICReg: Random-Walk Pairing for Improved Generalization and Better Global Semantics Capturing](https://arxiv.org/abs/2506.18104)
*Idan Simai,Ronen Talmon,Uri Shaham*

Main category: cs.CV

TL;DR: 论文分析了VICReg在自监督学习中的潜在不足，提出改进方法SAG-VICReg，通过新训练技术提升泛化能力，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: VICReg可能因过度依赖训练数据而泛化能力不足，需改进以生成更鲁棒的图像表示。

Method: 提出SAG-VICReg，结合新训练技术，增强数据全局语义捕捉和泛化能力。

Result: SAG-VICReg在泛化能力上表现优异，优于现有自监督学习方法，并在全局语义评估中领先。

Conclusion: SAG-VICReg有效解决了VICReg的泛化问题，并提出新无标签评估指标，适用于数据稀缺场景。

Abstract: In this paper, we argue that viewing VICReg-a popular self-supervised
learning (SSL) method--through the lens of spectral embedding reveals a
potential source of sub-optimality: it may struggle to generalize robustly to
unseen data due to overreliance on the training data. This observation invites
a closer look at how well this method achieves its goal of producing meaningful
representations of images outside of the training set as well. Here, we
investigate this issue and introduce SAG-VICReg (Stable and Generalizable
VICReg), a method that builds on VICReg by incorporating new training
techniques. These enhancements improve the model's ability to capture global
semantics within the data and strengthen the generalization capabilities.
Experiments demonstrate that SAG-VICReg effectively addresses the
generalization challenge while matching or surpassing diverse state-of-the-art
SSL baselines. Notably, our method exhibits superior performance on metrics
designed to evaluate global semantic understanding, while simultaneously
maintaining competitive results on local evaluation metrics. Furthermore, we
propose a new standalone evaluation metric for embeddings that complements the
standard evaluation methods and accounts for the global data structure without
requiring labels--a key issue when tagged data is scarce or not available.

</details>


### [216] [Targeted False Positive Synthesis via Detector-guided Adversarial Diffusion Attacker for Robust Polyp Detection](https://arxiv.org/abs/2506.18134)
*Quan Zhou,Gan Luo,Qiang Hu,Qingyong Zhang,Jinhua Zhang,Yinjiao Tian,Qiang Li,Zhiwei Wang*

Main category: cs.CV

TL;DR: 提出了一种对抗扩散框架，用于合成高价值的假阳性样本，以提升结肠息肉检测的可靠性。


<details>
  <summary>Details</summary>
Motivation: 现有模型在数据规模和多样性上受限，且生成模型多关注息肉多样性，忽略了假阳性问题。

Method: 设计了区域噪声匹配策略和检测器引导的对抗扩散攻击模块（DADA），用于合成多样背景和高价值假阳性样本。

Result: 在公开和内部数据集上验证了方法的优越性，合成数据使检测器F1分数分别提升至少2.6%和2.7%。

Conclusion: 该方法为病灶检测中的假阳性合成提供了新范式，推动了结肠癌筛查的临床可靠性。

Abstract: Polyp detection is crucial for colorectal cancer screening, yet existing
models are limited by the scale and diversity of available data. While
generative models show promise for data augmentation, current methods mainly
focus on enhancing polyp diversity, often overlooking the critical issue of
false positives. In this paper, we address this gap by proposing an adversarial
diffusion framework to synthesize high-value false positives. The extensive
variability of negative backgrounds presents a significant challenge in false
positive synthesis. To overcome this, we introduce two key innovations: First,
we design a regional noise matching strategy to construct a negative synthesis
space using polyp detection datasets. This strategy trains a negative-centric
diffusion model by masking polyp regions, ensuring the model focuses
exclusively on learning diverse background patterns. Second, we introduce the
Detector-guided Adversarial Diffusion Attacker (DADA) module, which perturbs
the negative synthesis process to disrupt a pre-trained detector's decision,
guiding the negative-centric diffusion model to generate high-value,
detector-confusing false positives instead of low-value, ordinary backgrounds.
Our approach is the first to apply adversarial diffusion to lesion detection,
establishing a new paradigm for targeted false positive synthesis and paving
the way for more reliable clinical applications in colorectal cancer screening.
Extensive results on public and in-house datasets verify the superiority of our
method over the current state-of-the-arts, with our synthesized data improving
the detectors by at least 2.6% and 2.7% in F1-score, respectively, over the
baselines. Codes are at https://github.com/Huster-Hq/DADA.

</details>


### [217] [See-in-Pairs: Reference Image-Guided Comparative Vision-Language Models for Medical Diagnosis](https://arxiv.org/abs/2506.18140)
*Ruinan Jin,Gexin Huang,Xinwei Shen,Qiong Zhang,Yan Shuo Tan,Xiaoxiao Li*

Main category: cs.CV

TL;DR: 论文探讨了医学影像诊断中比较推理的重要性，提出了一种结合通用视觉语言模型（VLM）和医学领域知识的方法，通过参考图像提升诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 医学影像诊断存在疾病模拟正常解剖结构和患者间差异大的问题，现有医学VLM缺乏比较推理机制，而通用VLM又缺乏医学领域知识。

Method: 通过提供查询图像和匹配的参考图像，结合临床启发的比较提示，对通用VLM进行监督微调（SFT）。

Result: 实验表明，该方法显著优于单图像基线，尤其在监督微调后，在多个医学视觉问答任务中表现更优。

Conclusion: 研究强调了比较分析在医学诊断中的临床价值，提出了利用参考图像的新策略，并验证了其有效性。

Abstract: Medical imaging diagnosis presents inherent challenges due to diseases that
mimic normal anatomy and exhibit significant inter-patient variability.
Clinicians routinely employ comparative reasoning-using reference images from
healthy controls or previous patient examinations-to discern subtle yet
diagnostically critical abnormalities. However, existing medical
vision-language models (VLMs) focus primarily on single-image or single-series
analyses and lack explicit mechanisms for comparative reasoning. Conversely,
general-purpose VLMs demonstrate strong multi-image comparative reasoning
capabilities but lack essential medical-domain knowledge to identify nuanced
clinical differences. This work aims to bridge this gap by exploring
clinically-inspired comparative analysis within VLMs, leveraging reference
images to enhance diagnostic accuracy. Through extensive empirical analysis, we
show that providing general-purpose VLMs with query and normative matched
reference images, accompanied by clinically-informed comparative prompts,
significantly improves diagnostic outcomes compared to single-image baselines,
especially after supervised finetuning (SFT). Our contributions highlight the
clinical relevance of comparative analysis introduce novel strategies for
leveraging reference images in VLMs, empirically demonstrate enhanced
performance across multiple medical visual question answering (VQA) tasks, and
provide theoretical insights into the efficacy of comparative image analysis in
medical diagnosis.

</details>


### [218] [Pattern-Based Phase-Separation of Tracer and Dispersed Phase Particles in Two-Phase Defocusing Particle Tracking Velocimetry](https://arxiv.org/abs/2506.18157)
*Christian Sax,Jochen Kriegseis*

Main category: cs.CV

TL;DR: 论文提出了一种基于后处理的相分离方法，用于散焦粒子跟踪测速中的分散两相流，通过单相机实现两相粒子的3D定位。


<details>
  <summary>Details</summary>
Motivation: 传统基于波长、尺寸或相关性的相分离方法在某些场景下不适用，需要一种更灵活的方法。

Method: 利用卷积神经网络（如Faster R-CNN和YOLOv4）识别散焦粒子图像的模式差异，并通过生成对抗网络生成标注训练数据。

Result: 在合成和真实数据集的测试中，检测精度和分类准确率达到95-100%，即使存在领域偏移。

Conclusion: 该方法在分散两相流中表现出鲁棒性，为传统方法不适用的情况提供了可行解决方案。

Abstract: This work investigates the feasibility of a post-processing-based approach
for phase separation in defocusing particle tracking velocimetry for dispersed
two-phase flows. The method enables the simultaneous 3D localization
determination of both tracer particles and particles of the dispersed phase,
using a single-camera setup. The distinction between phases is based on pattern
differences in defocused particle images, which arise from distinct light
scattering behaviors of tracer particles and bubbles or droplets. Convolutional
neural networks, including Faster R-CNN and YOLOv4 variants, are trained to
detect and classify particle images based on these pattern features. To
generate large, labeled training datasets, a generative adversarial network
based framework is introduced, allowing the generation of auto-labeled data
that more closely reflects experiment-specific visual appearance. Evaluation
across six datasets, comprising synthetic two-phase and real single- and
two-phase flows, demonstrates high detection precision and classification
accuracy (95-100%), even under domain shifts. The results confirm the viability
of using CNNs for robust phase separation in disperse two-phase DPTV,
particularly in scenarios where traditional wavelength-, size-, or ensemble
correlation-based methods are impractical.

</details>


### [219] [CDG-MAE: Learning Correspondences from Diffusion Generated Views](https://arxiv.org/abs/2506.18164)
*Varun Belagali,Pierre Marza,Srikar Yellapragada,Zilinghan Li,Tarak Nath Nandi,Ravi K Madduri,Joel Saltz,Stergios Christodoulidis,Maria Vakalopoulou,Dimitris Samaras*

Main category: cs.CV

TL;DR: CDG-MAE是一种基于MAE的自监督方法，通过扩散模型生成多样合成视图，显著提升密集对应学习性能。


<details>
  <summary>Details</summary>
Motivation: 解决密集对应学习中手动标注繁琐且不可扩展的问题，以及现有自监督方法训练数据不足的挑战。

Method: 利用图像条件扩散模型生成多样合成视图，并采用多锚点策略调整预训练任务的难度。

Result: CDG-MAE显著优于仅依赖图像的MAE方法，并大幅缩小与基于视频方法的性能差距。

Conclusion: CDG-MAE通过合成视图和多锚点策略，为密集对应学习提供了高效的自监督解决方案。

Abstract: Learning dense correspondences, critical for application such as video label
propagation, is hindered by tedious and unscalable manual annotation.
Self-supervised methods address this by using a cross-view pretext task, often
modeled with a masked autoencoder, where a masked target view is reconstructed
from an anchor view. However, acquiring effective training data remains a
challenge - collecting diverse video datasets is difficult and costly, while
simple image crops lack necessary pose variations. This paper introduces
CDG-MAE, a novel MAE-based self-supervised method that uses diverse synthetic
views generated from static images via an image-conditioned diffusion model.
These generated views exhibit substantial changes in pose and perspective,
providing a rich training signal that overcomes the limitations of video and
crop-based anchors. We present a quantitative method to evaluate local and
global consistency of generated images, discussing their use for cross-view
self-supervised pretraining. Furthermore, we enhance the standard single-anchor
MAE setting to a multi-anchor strategy to effectively modulate the difficulty
of pretext task. CDG-MAE significantly outperforms state-of-the-art MAE methods
reliant only on images and substantially narrows the performance gap to
video-based approaches.

</details>


### [220] [STACT-Time: Spatio-Temporal Cross Attention for Cine Thyroid Ultrasound Time Series Classification](https://arxiv.org/abs/2506.18172)
*Irsyad Adam,Tengyue Zhang,Shrayes Raman,Zhuyu Qiu,Brandon Taraku,Hexiang Feng,Sile Wang,Ashwath Radhachandran,Shreeram Athreya,Vedrana Ivezic,Peipei Ping,Corey Arnold,William Speier*

Main category: cs.CV

TL;DR: 提出了一种名为STACT-Time的深度学习模型，通过结合超声动态视频和分割掩模特征，优化甲状腺结节的恶性预测，减少不必要的活检。


<details>
  <summary>Details</summary>
Motivation: 当前甲状腺结节评估方法（如FNA活检）存在不必要的良性结节活检问题，且现有系统（如TI-RADS）受限于观察者间差异。深度学习模型虽尝试改进，但未充分利用超声动态视频的时空信息。

Method: 提出STACT-Time模型，利用自注意力和交叉注意力机制，整合超声动态视频的时空特征与预训练模型生成的分割掩模特征。

Result: 模型在恶性预测中表现优异，交叉验证精确度为0.91（±0.02），F1分数为0.89（±0.02）。

Conclusion: STACT-Time模型通过减少不必要的良性结节活检，同时保持高恶性检测灵敏度，有望改善临床决策和患者预后。

Abstract: Thyroid cancer is among the most common cancers in the United States. Thyroid
nodules are frequently detected through ultrasound (US) imaging, and some
require further evaluation via fine-needle aspiration (FNA) biopsy. Despite its
effectiveness, FNA often leads to unnecessary biopsies of benign nodules,
causing patient discomfort and anxiety. To address this, the American College
of Radiology Thyroid Imaging Reporting and Data System (TI-RADS) has been
developed to reduce benign biopsies. However, such systems are limited by
interobserver variability. Recent deep learning approaches have sought to
improve risk stratification, but they often fail to utilize the rich temporal
and spatial context provided by US cine clips, which contain dynamic global
information and surrounding structural changes across various views. In this
work, we propose the Spatio-Temporal Cross Attention for Cine Thyroid
Ultrasound Time Series Classification (STACT-Time) model, a novel
representation learning framework that integrates imaging features from US cine
clips with features from segmentation masks automatically generated by a
pretrained model. By leveraging self-attention and cross-attention mechanisms,
our model captures the rich temporal and spatial context of US cine clips while
enhancing feature representation through segmentation-guided learning. Our
model improves malignancy prediction compared to state-of-the-art models,
achieving a cross-validation precision of 0.91 (plus or minus 0.02) and an F1
score of 0.89 (plus or minus 0.02). By reducing unnecessary biopsies of benign
nodules while maintaining high sensitivity for malignancy detection, our model
has the potential to enhance clinical decision-making and improve patient
outcomes.

</details>


### [221] [DExNet: Combining Observations of Domain Adapted Critics for Leaf Disease Classification with Limited Data](https://arxiv.org/abs/2506.18173)
*Sabbir Ahmed,Md. Bakhtiar Hasan,Tasnim Ahmed,Md. Hasanul Kabir*

Main category: cs.CV

TL;DR: 提出了一种基于少样本学习的植物病害分类框架DExNet，通过结合多个预训练CNN架构的特征嵌入，显著减少了对大规模数据集的需求。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习模型在有限样本下分类植物病害时性能不足的问题。

Method: 使用9个预训练的CNN架构提取特征嵌入，通过域适应和特征融合块，结合Bi-LSTM分类器进行分类。

Result: 在PlantVillage数据集上，5-shot、10-shot和15-shot分类分别达到89.06%、92.46%和94.07%的准确率，80-shot分类达到98.09%。

Conclusion: DExNet在少样本条件下表现优异，显著减少数据需求，优于现有方法。

Abstract: While deep learning-based architectures have been widely used for correctly
detecting and classifying plant diseases, they require large-scale datasets to
learn generalized features and achieve state-of-the-art performance. This poses
a challenge for such models to obtain satisfactory performance in classifying
leaf diseases with limited samples. This work proposes a few-shot learning
framework, Domain-adapted Expert Network (DExNet), for plant disease
classification that compensates for the lack of sufficient training data by
combining observations of a number of expert critics. It starts with extracting
the feature embeddings as 'observations' from nine 'critics' that are
state-of-the-art pre-trained CNN-based architectures. These critics are 'domain
adapted' using a publicly available leaf disease dataset having no overlapping
classes with the specific downstream task of interest. The observations are
then passed to the 'Feature Fusion Block' and finally to a classifier network
consisting of Bi-LSTM layers. The proposed pipeline is evaluated on the 10
classes of tomato leaf images from the PlantVillage dataset, achieving
promising accuracies of 89.06%, 92.46%, and 94.07%, respectively, for 5-shot,
10-shot, and 15-shot classification. Furthermore, an accuracy of 98.09+-0.7%
has been achieved in 80-shot classification, which is only 1.2% less than
state-of-the-art, allowing a 94.5% reduction in the training data requirement.
The proposed pipeline also outperforms existing works on leaf disease
classification with limited data in both laboratory and real-life conditions in
single-domain, mixed-domain, and cross-domain scenarios.

</details>


### [222] [Multimodal Fusion SLAM with Fourier Attention](https://arxiv.org/abs/2506.18204)
*Youjie Zhou,Guofeng Mei,Yiming Wang,Yi Wan,Fabio Poiesi*

Main category: cs.CV

TL;DR: FMF-SLAM是一种高效的多模态融合SLAM方法，利用FFT提升效率，通过傅里叶自注意力和跨注意力机制处理RGB和深度信号，并在噪声、光照变化和黑暗条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统基于光流的视觉SLAM方法在噪声、光照变化和黑暗环境中表现不佳且计算资源需求高。

Method: 提出FMF-SLAM，结合FFT、傅里叶自注意力和跨注意力机制，并引入多尺度知识蒸馏增强多模态特征交互。

Result: 在TUM、TartanAir和真实数据集上验证了其高效性和实时性，表现优于现有方法。

Conclusion: FMF-SLAM在复杂环境中具有高效性和实用性，代码和数据集已开源。

Abstract: Visual SLAM is particularly challenging in environments affected by noise,
varying lighting conditions, and darkness. Learning-based optical flow
algorithms can leverage multiple modalities to address these challenges, but
traditional optical flow-based visual SLAM approaches often require significant
computational resources.To overcome this limitation, we propose FMF-SLAM, an
efficient multimodal fusion SLAM method that utilizes fast Fourier transform
(FFT) to enhance the algorithm efficiency. Specifically, we introduce a novel
Fourier-based self-attention and cross-attention mechanism to extract features
from RGB and depth signals. We further enhance the interaction of multimodal
features by incorporating multi-scale knowledge distillation across modalities.
We also demonstrate the practical feasibility of FMF-SLAM in real-world
scenarios with real time performance by integrating it with a security robot by
fusing with a global positioning module GNSS-RTK and global Bundle Adjustment.
Our approach is validated using video sequences from TUM, TartanAir, and our
real-world datasets, showcasing state-of-the-art performance under noisy,
varying lighting, and dark conditions.Our code and datasets are available at
https://github.com/youjie-zhou/FMF-SLAM.git.

</details>


### [223] [Limitations of NERF with pre-trained Vision Features for Few-Shot 3D Reconstruction](https://arxiv.org/abs/2506.18208)
*Ankit Sanjyal*

Main category: cs.CV

TL;DR: DINO-enhanced NeRF模型在极少量样本场景中表现不如基线NeRF，预训练视觉特征可能对少样本3D重建无益甚至有害。


<details>
  <summary>Details</summary>
Motivation: 探索预训练视觉特征（如DINO）对少样本3D重建的效果，验证其在极端少样本场景中的有效性。

Method: 系统评估DINO增强的NeRF模型，包括基线NeRF、冻结DINO特征、LoRA微调特征和多尺度特征融合。

Result: 所有DINO变体表现均逊于基线NeRF（PSNR 12.9-13.0 vs. 14.71），表明预训练特征可能引入有害偏差。

Conclusion: 预训练视觉特征可能不适合少样本3D重建，建议关注几何一致性的简单架构。

Abstract: Neural Radiance Fields (NeRF) have revolutionized 3D scene reconstruction
from sparse image collections. Recent work has explored integrating pre-trained
vision features, particularly from DINO, to enhance few-shot reconstruction
capabilities. However, the effectiveness of such approaches remains unclear,
especially in extreme few-shot scenarios. In this paper, we present a
systematic evaluation of DINO-enhanced NeRF models, comparing baseline NeRF,
frozen DINO features, LoRA fine-tuned features, and multi-scale feature fusion.
Surprisingly, our experiments reveal that all DINO variants perform worse than
the baseline NeRF, achieving PSNR values around 12.9 to 13.0 compared to the
baseline's 14.71. This counterintuitive result suggests that pre-trained vision
features may not be beneficial for few-shot 3D reconstruction and may even
introduce harmful biases. We analyze potential causes including feature-task
mismatch, overfitting to limited data, and integration challenges. Our findings
challenge common assumptions in the field and suggest that simpler
architectures focusing on geometric consistency may be more effective for
few-shot scenarios.

</details>


### [224] [Deep Learning-based Alignment Measurement in Knee Radiographs](https://arxiv.org/abs/2506.18209)
*Zhisen Hu,Dominic Cullen,Peter Thompson,David Johnson,Chang Bian,Aleksei Tiulpin,Timothy Cootes,Claudia Lindner*

Main category: cs.CV

TL;DR: 提出了一种基于深度学习的膝关节对齐（KA）测量方法，通过自动定位膝关节解剖标志，提高了测量的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统KA测量方法耗时且依赖长腿X光片，需要自动化解决方案以提高效率和准确性。

Method: 采用沙漏网络和注意力门结构，自动定位100多个膝关节解剖标志，并测量术前和术后的KA。

Result: 与临床测量相比，平均绝对差异约为1°，术前ICC为0.97，术后为0.86，显示出高准确性和可靠性。

Conclusion: 该方法可高精度自动化KA评估，为临床工作流程提供数字化支持。

Abstract: Radiographic knee alignment (KA) measurement is important for predicting
joint health and surgical outcomes after total knee replacement. Traditional
methods for KA measurements are manual, time-consuming and require long-leg
radiographs. This study proposes a deep learning-based method to measure KA in
anteroposterior knee radiographs via automatically localized knee anatomical
landmarks. Our method builds on hourglass networks and incorporates an
attention gate structure to enhance robustness and focus on key anatomical
features. To our knowledge, this is the first deep learning-based method to
localize over 100 knee anatomical landmarks to fully outline the knee shape
while integrating KA measurements on both pre-operative and post-operative
images. It provides highly accurate and reliable anatomical varus/valgus KA
measurements using the anatomical tibiofemoral angle, achieving mean absolute
differences ~1{\deg} when compared to clinical ground truth measurements.
Agreement between automated and clinical measurements was excellent
pre-operatively (intra-class correlation coefficient (ICC) = 0.97) and good
post-operatively (ICC = 0.86). Our findings demonstrate that KA assessment can
be automated with high accuracy, creating opportunities for digitally enhanced
clinical workflows.

</details>


### [225] [Shape from Polarization of Thermal Emission and Reflection](https://arxiv.org/abs/2506.18217)
*Kazuma Kitazawa,Tsuyoshi Takatani*

Main category: cs.CV

TL;DR: 论文提出了一种基于长波红外（LWIR）偏振成像的透明物体形状估计方法，通过改进偏振模型和结合学习与模型方法，显著提高了准确性。


<details>
  <summary>Details</summary>
Motivation: 透明物体的形状估计因复杂的光传输而困难，现有LWIR偏振方法因忽略反射导致误差较大。

Method: 提出考虑发射和反射的偏振模型，结合基于模型和学习的方法，并建模系统误差以优化偏振成像。

Result: 实验证明该方法在多种材料（包括可见光透明材料）上具有高准确性和广泛适用性。

Conclusion: 改进的LWIR偏振方法为透明物体形状估计提供了有效解决方案，并创建了首个真实世界基准数据集ThermoPol。

Abstract: Shape estimation for transparent objects is challenging due to their complex
light transport. To circumvent these difficulties, we leverage the Shape from
Polarization (SfP) technique in the Long-Wave Infrared (LWIR) spectrum, where
most materials are opaque and emissive. While a few prior studies have explored
LWIR SfP, these attempts suffered from significant errors due to inadequate
polarimetric modeling, particularly the neglect of reflection. Addressing this
gap, we formulated a polarization model that explicitly accounts for the
combined effects of emission and reflection. Based on this model, we estimated
surface normals using not only a direct model-based method but also a
learning-based approach employing a neural network trained on a
physically-grounded synthetic dataset. Furthermore, we modeled the LWIR
polarimetric imaging process, accounting for inherent systematic errors to
ensure accurate polarimetry. We implemented a prototype system and created
ThermoPol, the first real-world benchmark dataset for LWIR SfP. Through
comprehensive experiments, we demonstrated the high accuracy and broad
applicability of our method across various materials, including those
transparent in the visible spectrum.

</details>


### [226] [Cross-Architecture Knowledge Distillation (KD) for Retinal Fundus Image Anomaly Detection on NVIDIA Jetson Nano](https://arxiv.org/abs/2506.18220)
*Berk Yilmaz,Aniruddh Aiyengar*

Main category: cs.CV

TL;DR: 论文提出了一种轻量级、可在边缘设备部署的视网膜疾病分类器，通过跨架构知识蒸馏方法，将高容量ViT教师模型压缩为CNN学生模型，适用于资源有限的环境。


<details>
  <summary>Details</summary>
Motivation: 在资源匮乏地区，可靠诊断设备稀缺，早期准确识别视网膜疾病对防止视力下降至关重要。

Method: 使用I-JEPA自监督学习预训练ViT教师模型，通过PCA投影器、GL投影器和多视图鲁棒训练方法压缩为CNN学生模型。

Result: 学生模型参数比教师模型少97.4%，分类准确率达89%，保留了教师模型93%的诊断性能。

Conclusion: 该方法成功压缩ViT模型并保持准确性，为资源匮乏地区提供了可扩展的AI视网膜疾病筛查方案。

Abstract: Early and accurate identification of retinal ailments is crucial for averting
ocular decline; however, access to dependable diagnostic devices is not often
available in low-resourced settings. This project proposes to solve that by
developing a lightweight, edge-device deployable disease classifier using
cross-architecture knowledge distilling. We first train a high-capacity vision
transformer (ViT) teacher model, pre-trained using I-JEPA self-supervised
learning, to classify fundus images into four classes: Normal, Diabetic
Retinopathy, Glaucoma, and Cataract. We kept an Internet of Things (IoT) focus
when compressing to a CNN-based student model for deployment in
resource-limited conditions, such as the NVIDIA Jetson Nano. This was
accomplished using a novel framework which included a Partitioned
Cross-Attention (PCA) projector, a Group-Wise Linear (GL) projector, and a
multi-view robust training method. The teacher model has 97.4 percent more
parameters than the student model, with it achieving 89 percent classification
with a roughly 93 percent retention of the teacher model's diagnostic
performance. The retention of clinical classification behavior supports our
method's initial aim: compression of the ViT while retaining accuracy. Our work
serves as an example of a scalable, AI-driven triage solution for retinal
disorders in under-resourced areas.

</details>


### [227] [Make It Efficient: Dynamic Sparse Attention for Autoregressive Image Generation](https://arxiv.org/abs/2506.18226)
*Xunzhi Xiang,Qi Fan*

Main category: cs.CV

TL;DR: 论文提出了一种名为ADSA的训练无关上下文优化方法，用于解决自回归图像生成模型中的内存和计算延迟问题。


<details>
  <summary>Details</summary>
Motivation: 自回归图像生成模型在推理过程中因长上下文导致内存开销和计算延迟问题。

Method: 提出ADSA方法，动态识别关键历史标记以优化注意力计算，并引入动态KV缓存更新机制。

Result: 实验表明ADSA在生成质量和资源效率上均表现优越，GPU内存消耗减少约50%。

Conclusion: ADSA有效解决了自回归图像生成模型中的资源效率问题，同时保持了生成质量。

Abstract: Autoregressive conditional image generation models have emerged as a dominant
paradigm in text-to-image synthesis. These methods typically convert images
into one-dimensional token sequences and leverage the self-attention mechanism,
which has achieved remarkable success in natural language processing, to
capture long-range dependencies, model global context, and ensure semantic
coherence. However, excessively long contexts during inference lead to
significant memory overhead caused by KV-cache and computational delays. To
alleviate these challenges, we systematically analyze how global semantics,
spatial layouts, and fine-grained textures are formed during inference, and
propose a novel training-free context optimization method called Adaptive
Dynamic Sparse Attention (ADSA). Conceptually, ADSA dynamically identifies
historical tokens crucial for maintaining local texture consistency and those
essential for ensuring global semantic coherence, thereby efficiently
streamlining attention computation. Additionally, we introduce a dynamic
KV-cache update mechanism tailored for ADSA, reducing GPU memory consumption
during inference by approximately $50\%$. Extensive qualitative and
quantitative experiments demonstrate the effectiveness and superiority of our
approach in terms of both generation quality and resource efficiency.

</details>


### [228] [Drive-R1: Bridging Reasoning and Planning in VLMs for Autonomous Driving with Reinforcement Learning](https://arxiv.org/abs/2506.18234)
*Yue Li,Meng Tian,Dechang Zhu,Jiangtong Zhu,Zhenyu Lin,Zhiwei Xiong,Xinhai Zhao*

Main category: cs.CV

TL;DR: 论文提出Drive-R1模型，通过结合场景推理和运动规划，解决了大视觉语言模型在自动驾驶中的两个关键挑战：过度依赖历史信息和推理与规划结果不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 解决大视觉语言模型在自动驾驶中过度依赖历史信息及推理与规划结果不一致的问题。

Method: 提出Drive-R1模型，先通过监督微调处理长短期推理数据，再通过强化学习优化推理路径以提升规划效果。

Result: 在nuScenes和DriveLM-nuScenes基准测试中表现优于现有最先进模型。

Conclusion: Drive-R1为自动驾驶中推理与规划的结合提供了新方向，具有方法论启示。

Abstract: Large vision-language models (VLMs) for autonomous driving (AD) are evolving
beyond perception and cognition tasks toward motion planning. However, we
identify two critical challenges in this direction: (1) VLMs tend to learn
shortcuts by relying heavily on history input information, achieving seemingly
strong planning results without genuinely understanding the visual inputs; and
(2) the chain-ofthought (COT) reasoning processes are always misaligned with
the motion planning outcomes, and how to effectively leverage the complex
reasoning capability to enhance planning remains largely underexplored. In this
paper, we start from a small-scale domain-specific VLM and propose Drive-R1
designed to bridges the scenario reasoning and motion planning for AD. Drive-R1
first undergoes the supervised finetuning on a elaborate dataset containing
both long and short COT data. Drive-R1 is encouraged to reason step-by-step
from visual input to final planning decisions. Subsequently, Drive-R1 is
trained within a reinforcement learning framework that incentivizes the
discovery of reasoning paths that are more informative for planning, guided by
rewards based on predicted trajectories and meta actions. Experimental
evaluations on the nuScenes and DriveLM-nuScenes benchmarks demonstrate that
Drive-R1 achieves superior performance compared to existing state-of-the-art
VLMs. We believe that Drive-R1 presents a promising direction for bridging
reasoning and planning in AD, offering methodological insights for future
research and applications.

</details>


### [229] [Referring Expression Instance Retrieval and A Strong End-to-End Baseline](https://arxiv.org/abs/2506.18246)
*Xiangzhao Hao,Kuan Zhu,Hongyu Guo,Haiyun Guo,Ming Tang,JinQiao Wang*

Main category: cs.CV

TL;DR: 论文提出了一种新任务REIR，结合实例级检索和定位，并构建了REIRCOCO基准和基线方法CLARE，实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有TIR和REC任务在实例级检索和定位上的不足，填补两者之间的空白。

Method: 提出CLARE方法，采用双流架构和MORE模块，结合目标检测、REC预训练和CLIA进行端到端优化。

Result: CLARE在REIR任务上表现优异，同时在TIR和REC任务上也有良好泛化能力。

Conclusion: REIR任务和CLARE方法的提出为视觉语言任务提供了新的解决方案，展示了其有效性和通用性。

Abstract: Natural language querying of visual content underpins many vision-language
tasks, typically categorized by text granularity and visual search scope.
Text-Image Retrieval (TIR) retrieves whole images using coarse descriptions,
while Referring Expression Comprehension (REC) localizes objects using
fine-grained expressions within a single image. However, real-world scenarios
often require both instance-level retrieval and localization across large
galleries -- tasks where TIR lacks precision and REC lacks scalability. To
address this gap, we propose a new task: Referring Expression Instance
Retrieval (REIR), which jointly supports instance-level retrieval and
localization. We introduce REIRCOCO, a large-scale benchmark constructed by
prompting vision-language models to generate fine-grained expressions for
MSCOCO and RefCOCO instances. We also present a baseline method, CLARE,
featuring a dual-stream architecture with a Mix of Relation Experts (MORE)
module for capturing inter-instance relationships. CLARE integrates object
detection and REC pretraining with Contrastive Language-Instance Alignment
(CLIA) for end-to-end optimization. Experiments show that CLARE achieves
state-of-the-art performance on REIR and generalizes well to TIR and REC,
highlighting its effectiveness and versatility.

</details>


### [230] [Semantic Structure-Aware Generative Attacks for Enhanced Adversarial Transferability](https://arxiv.org/abs/2506.18248)
*Jongoh Jeong,Hunmin Yang,Jaeseok Jeong,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: 提出一种基于Mean Teacher的语义结构感知攻击框架，通过特征蒸馏增强对抗性扰动的语义一致性，显著提升对抗迁移性。


<details>
  <summary>Details</summary>
Motivation: 现有生成对抗攻击未充分利用生成模型的语义信息，限制了扰动与对象显著区域的对齐，从而影响迁移性。

Method: 采用Mean Teacher作为时间平滑特征参考，通过特征蒸馏指导学生与教师的早期层激活保持语义一致性，锚定生成器的语义显著中间块。

Result: 在多种模型、领域和任务上实验表明，该方法相对于现有生成攻击在对抗迁移性上有显著提升。

Conclusion: 通过语义结构感知攻击框架，有效提升了对抗扰动的迁移性，为生成对抗攻击提供了新思路。

Abstract: Generative adversarial attacks train a perturbation generator on a white-box
surrogate model and subsequently apply the crafted perturbations to unseen
black-box victim models. In contrast to iterative attacks, these methods
deliver superior inference-time efficiency, scalability, and transferability;
however, up until now, existing studies have not fully exploited the
representational capacity of generative models to preserve and harness semantic
information. Specifically, the intermediate activations of the generator encode
rich semantic features--object boundaries and coarse shapes--that remain
under-exploited, thereby limiting the alignment of perturbations with
object-salient regions which are critical for adversarial transferability. To
remedy this, we introduce a semantic structure-aware attack framework based on
the Mean Teacher, which serves as a temporally smoothed feature reference. With
this smoothed reference, we further direct semantic consistency between the
early-layer activations in the student and those of the semantically rich
teacher by feature distillation. By anchoring perturbation synthesis to the
semantically salient early intermediate blocks within the generator based on
empirical findings, our method guides progressive adversarial perturbation on
regions that substantially enhance adversarial transferability. We conduct
extensive experiments over diverse models, domains and tasks to demonstrate
consistent improvements relative to state-of-the-art generative attacks,
comprehensively evaluated using conventional metrics and our newly proposed
Accidental Correction Rate (ACR).

</details>


### [231] [Improving Weakly Supervised Temporal Action Localization by Exploiting Multi-resolution Information in Temporal Domain](https://arxiv.org/abs/2506.18261)
*Rui Su,Dong Xu,Luping Zhou,Wanli Ouyang*

Main category: cs.CV

TL;DR: 提出一种两阶段方法，利用多分辨率信息和伪标签迭代优化，提升弱监督时序动作定位性能。


<details>
  <summary>Details</summary>
Motivation: 弱监督时序动作定位任务中仅提供视频级标注，难以直接生成高质量帧级伪标签。

Method: 第一阶段通过ILG模块生成初始伪标签，第二阶段通过PTLR框架迭代优化伪标签，利用多分辨率信息交换提升性能。

Result: 通过多分辨率信息交换和伪标签迭代优化，提升了时序动作定位的准确性。

Conclusion: 两阶段方法有效利用多分辨率信息，通过伪标签迭代优化显著提升弱监督时序动作定位性能。

Abstract: Weakly supervised temporal action localization is a challenging task as only
the video-level annotation is available during the training process. To address
this problem, we propose a two-stage approach to fully exploit multi-resolution
information in the temporal domain and generate high quality frame-level pseudo
labels based on both appearance and motion streams. Specifically, in the first
stage, we generate reliable initial frame-level pseudo labels, and in the
second stage, we iteratively refine the pseudo labels and use a set of selected
frames with highly confident pseudo labels to train neural networks and better
predict action class scores at each frame. We fully exploit temporal
information at multiple scales to improve temporal action localization
performance. Specifically, in order to obtain reliable initial frame-level
pseudo labels, in the first stage, we propose an Initial Label Generation (ILG)
module, which leverages temporal multi-resolution consistency to generate high
quality class activation sequences (CASs), which consist of a number of
sequences with each sequence measuring how likely each video frame belongs to
one specific action class. In the second stage, we propose a Progressive
Temporal Label Refinement (PTLR) framework. In our PTLR framework, two networks
called Network-OTS and Network-RTS, which are respectively used to generate
CASs for the original temporal scale and the reduced temporal scales, are used
as two streams (i.e., the OTS stream and the RTS stream) to refine the pseudo
labels in turn. By this way, the multi-resolution information in the temporal
domain is exchanged at the pseudo label level, and our work can help improve
each stream (i.e., the OTS/RTS stream) by exploiting the refined pseudo labels
from another stream (i.e., the RTS/OTS stream).

</details>


### [232] [YouTube-Occ: Learning Indoor 3D Semantic Occupancy Prediction from YouTube Videos](https://arxiv.org/abs/2506.18266)
*Haoming Chen,Lichen Yuan,TianFang Sun,Jingyu Gong,Xin Tan,Zhizhong Zhang,Yuan Xie*

Main category: cs.CV

TL;DR: 论文提出了一种仅使用室内互联网数据（无需相机参数）实现3D语义占用预测的自监督方法，通过YouTube-Occ数据集和2D先验知识蒸馏，取得了零样本SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 解决复杂室内环境中3D语义占用预测对精确几何关系和精细标注的依赖问题，降低数据采集难度和隐私风险。

Method: 利用YouTube-Occ数据集，通过自监督模型和2D先验知识（如超像素分组）蒸馏到3D占用网络。

Result: 在NYUv2和OccScanNet基准上实现了零样本SOTA性能。

Conclusion: 证明了仅用互联网数据即可实现高精度3D语义占用预测，无需传统几何信息。

Abstract: 3D semantic occupancy prediction in the past was considered to require
precise geometric relationships in order to enable effective training. However,
in complex indoor environments, the large-scale and widespread collection of
data, along with the necessity for fine-grained annotations, becomes
impractical due to the complexity of data acquisition setups and privacy
concerns. In this paper, we demonstrate that 3D spatially-accurate training can
be achieved using only indoor Internet data, without the need for any
pre-knowledge of intrinsic or extrinsic camera parameters. In our framework, we
collect a web dataset, YouTube-Occ, which comprises house tour videos from
YouTube, providing abundant real house scenes for 3D representation learning.
Upon on this web dataset, we establish a fully self-supervised model to
leverage accessible 2D prior knowledge for reaching powerful 3D indoor
perception. Specifically, we harness the advantages of the prosperous vision
foundation models, distilling the 2D region-level knowledge into the occupancy
network by grouping the similar pixels into superpixels. Experimental results
show that our method achieves state-of-the-art zero-shot performance on two
popular benchmarks (NYUv2 and OccScanNet

</details>


### [233] [ThermalLoc: A Vision Transformer-Based Approach for Robust Thermal Camera Relocalization in Large-Scale Environments](https://arxiv.org/abs/2506.18268)
*Yu Liu,Yangtao Meng,Xianfei Pan,Jie Jiang,Changhao Chen*

Main category: cs.CV

TL;DR: ThermalLoc是一种新型端到端深度学习方法，专为热图像重定位设计，结合EfficientNet和Transformers提取特征，并通过MLP网络进行绝对姿态回归，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统基于可见光图像的视觉重定位方法不适用于热图像，而针对热相机重定位的深度学习方法尚未充分探索。

Method: ThermalLoc整合EfficientNet和Transformers提取热图像的局部和全局特征，使用两个MLP网络进行绝对姿态回归。

Result: 在公开数据集和自建数据集上，ThermalLoc在准确性和鲁棒性上均优于AtLoc、MapNet、PoseNet和RobustLoc等现有方法。

Conclusion: ThermalLoc填补了热相机重定位领域的空白，为热图像处理提供了高效解决方案。

Abstract: Thermal cameras capture environmental data through heat emission, a
fundamentally different mechanism compared to visible light cameras, which rely
on pinhole imaging. As a result, traditional visual relocalization methods
designed for visible light images are not directly applicable to thermal
images. Despite significant advancements in deep learning for camera
relocalization, approaches specifically tailored for thermal camera-based
relocalization remain underexplored. To address this gap, we introduce
ThermalLoc, a novel end-to-end deep learning method for thermal image
relocalization. ThermalLoc effectively extracts both local and global features
from thermal images by integrating EfficientNet with Transformers, and performs
absolute pose regression using two MLP networks. We evaluated ThermalLoc on
both the publicly available thermal-odometry dataset and our own dataset. The
results demonstrate that ThermalLoc outperforms existing representative methods
employed for thermal camera relocalization, including AtLoc, MapNet, PoseNet,
and RobustLoc, achieving superior accuracy and robustness.

</details>


### [234] [Adaptive Mask-guided K-space Diffusion for Accelerated MRI Reconstruction](https://arxiv.org/abs/2506.18270)
*Qinrong Cai,Yu Guan,Zhibo Chen,Dong Liang,Qiuyun Fan,Qiegen Liu*

Main category: cs.CV

TL;DR: 该论文提出了一种基于自适应掩码的扩散模型（AMDM），用于MRI重建，通过自适应调整k空间数据的频率分布，提高图像质量。


<details>
  <summary>Details</summary>
Motivation: 传统的MRI重建方法未充分考虑k空间不同频率区域的重要性，导致重建效果受限。

Method: 采用自适应掩码机制，根据k空间数据的频率分布动态调整掩码，分离高低频成分，并通过闭环扩散过程优化重建。

Result: 实验证明AMDM能有效学习特定频率信息，显著提升MRI重建质量。

Conclusion: 该方法为未来利用掩码优化k空间数据提供了灵活框架。

Abstract: As the deep learning revolution marches on, masked modeling has emerged as a
distinctive approach that involves predicting parts of the original data that
are proportionally masked during training, and has demonstrated exceptional
performance in multiple fields. Magnetic Resonance Imaging (MRI) reconstruction
is a critical task in medical imaging that seeks to recover high-quality images
from under-sampled k-space data. However, previous MRI reconstruction
strategies usually optimized the entire image domain or k-space, without
considering the importance of different frequency regions in the k-space This
work introduces a diffusion model based on adaptive masks (AMDM), which
utilizes the adaptive adjustment of frequency distribution based on k-space
data to develop a hybrid masks mechanism that adapts to different k-space
inputs. This enables the effective separation of high-frequency and
low-frequency components, producing diverse frequency-specific representations.
Additionally, the k-space frequency distribution informs the generation of
adaptive masks, which, in turn, guide a closed-loop diffusion process.
Experimental results verified the ability of this method to learn specific
frequency information and thereby improved the quality of MRI reconstruction,
providing a flexible framework for optimizing k-space data using masks in the
future.

</details>


### [235] [ReFrame: Rectification Framework for Image Explaining Architectures](https://arxiv.org/abs/2506.18272)
*Debjyoti Das Adhikary,Aritra Hazra,Partha Pratim Chakrabarti*

Main category: cs.CV

TL;DR: 论文提出了一种可解释的框架，用于改进图像解释中的不一致性和不完整性，显著提升了图像描述、视觉问答和基于提示的AI模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有图像解释方法常存在幻觉对象或遗漏对象的问题，论文旨在解决这些问题。

Method: 提出了一种可解释的框架，可集成到多种图像解释框架中，通过纠正错误或缺失的对象来增强解释能力。

Result: 实验表明，该框架在图像描述、视觉问答和基于提示的AI模型中均显著提升了完整性和一致性。

Conclusion: 提出的框架有效解决了图像解释中的不一致性和不完整性问题，性能优于现有方法。

Abstract: Image explanation has been one of the key research interests in the Deep
Learning field. Throughout the years, several approaches have been adopted to
explain an input image fed by the user. From detecting an object in a given
image to explaining it in human understandable sentence, to having a
conversation describing the image, this problem has seen an immense change
throughout the years, However, the existing works have been often found to (a)
hallucinate objects that do not exist in the image and/or (b) lack identifying
the complete set of objects present in the image. In this paper, we propose a
novel approach to mitigate these drawbacks of inconsistency and incompleteness
of the objects recognized during the image explanation. To enable this, we
propose an interpretable framework that can be plugged atop diverse image
explaining frameworks including Image Captioning, Visual Question Answering
(VQA) and Prompt-based AI using LLMs, thereby enhancing their explanation
capabilities by rectifying the incorrect or missing objects. We further measure
the efficacy of the rectified explanations generated through our proposed
approaches leveraging object based precision metrics, and showcase the
improvements in the inconsistency and completeness of image explanations.
Quantitatively, the proposed framework is able to improve the explanations over
the baseline architectures of Image Captioning (improving the completeness by
81.81% and inconsistency by 37.10%), Visual Question Answering(average of 9.6%
and 37.10% in completeness and inconsistency respectively) and Prompt-based AI
model (0.01% and 5.2% for completeness and inconsistency respectively)
surpassing the current state-of-the-art by a substantial margin.

</details>


### [236] [Open Set Recognition for Endoscopic Image Classification: A Deep Learning Approach on the Kvasir Dataset](https://arxiv.org/abs/2506.18284)
*Kasra Moazzami,Seoyoun Son,John Lin,Sun Min Lee,Daniel Son,Hayeon Lee,Jeongho Lee,Seongji Lee*

Main category: cs.CV

TL;DR: 该论文探讨了在医学内窥镜图像分类中应用开放集识别（OSR）技术的重要性，并在Kvasir数据集上评估了多种深度学习模型的OSR性能。


<details>
  <summary>Details</summary>
Motivation: 传统封闭集分类框架在开放世界临床环境中存在局限性，无法应对未知条件，影响模型可靠性。

Method: 在Kvasir数据集上评估了ResNet-50、Swin Transformer和混合ResNet-Transformer模型的OSR能力，并采用OpenMax作为基线方法。

Result: 研究为医学图像分析中的OSR性能提供了基准，并展示了模型在临床现实环境中的行为。

Conclusion: OSR技术对AI系统在内窥镜中的安全部署至关重要，研究为未来工作奠定了基础。

Abstract: Endoscopic image classification plays a pivotal role in medical diagnostics
by identifying anatomical landmarks and pathological findings. However,
conventional closed-set classification frameworks are inherently limited in
open-world clinical settings, where previously unseen conditions can arise
andcompromise model reliability. To address this, we explore the application of
Open Set Recognition (OSR) techniques on the Kvasir dataset, a publicly
available and diverse endoscopic image collection. In this study, we evaluate
and compare the OSR capabilities of several representative deep learning
architectures, including ResNet-50, Swin Transformer, and a hybrid
ResNet-Transformer model, under both closed-set and open-set conditions.
OpenMax is adopted as a baseline OSR method to assess the ability of these
models to distinguish known classes from previously unseen categories. This
work represents one of the first efforts to apply open set recognition to the
Kvasir dataset and provides a foundational benchmark for evaluating OSR
performance in medical image analysis. Our results offer practical insights
into model behavior in clinically realistic settings and highlight the
importance of OSR techniques for the safe deployment of AI systems in
endoscopy.

</details>


### [237] [Selective Social-Interaction via Individual Importance for Fast Human Trajectory Prediction](https://arxiv.org/abs/2506.18291)
*Yota Urano,Hiromu Taketsugu,Norimichi Ukita*

Main category: cs.CV

TL;DR: 提出了一种通过选择重要邻居预测主人物轨迹的架构，使用重要性估计器和Gumbel Softmax优化训练，实验显示方法高效且准确。


<details>
  <summary>Details</summary>
Motivation: 预测主人物轨迹时，周围人物的选择对准确性至关重要，但传统方法可能因不可微操作阻碍梯度传播。

Method: 设计重要性估计器模块评估邻居重要性，采用Gumbel Softmax解决不可微问题。

Result: 在JRDB数据集上实验，方法在保持预测精度的同时提升了效率。

Conclusion: 所提架构通过优化邻居选择和训练过程，实现了高效且准确的轨迹预测。

Abstract: This paper presents an architecture for selecting important neighboring
people to predict the primary person's trajectory. To achieve effective
neighboring people selection, we propose a people selection module called the
Importance Estimator which outputs the importance of each neighboring person
for predicting the primary person's future trajectory. To prevent gradients
from being blocked by non-differentiable operations when sampling surrounding
people based on their importance, we employ the Gumbel Softmax for training.
Experiments conducted on the JRDB dataset show that our method speeds up the
process with competitive prediction accuracy.

</details>


### [238] [Rapeseed population point cloud completion network (RP-PCN) with dynamic graph convolution for 3D reconstruction of crop canopy occlusion architecture](https://arxiv.org/abs/2506.18292)
*Ziyue Guo,Xin Yang,Yutao Shen,Yang Zhu,Lixi Jiang,Haiyan Cen*

Main category: cs.CV

TL;DR: 提出了一种基于多视角成像的油菜群体三维重建点云补全模型（RP-PCN），通过虚拟-现实集成（VRI）模拟和遮挡点检测算法生成完整点云，显著提高了油菜群体冠层结构的描述精度和产量预测准确性。


<details>
  <summary>Details</summary>
Motivation: 油菜群体冠层结构的完整三维描述对光合作用和产量评估至关重要，但现有技术因遮挡和复杂结构难以实现准确重建。

Method: 开发了RP-PCN模型，结合多分辨率动态图卷积编码器（MRDG）和点金字塔解码器（PPD），并引入动态图卷积特征提取器（DGCFE）捕捉生长周期中的结构变化。

Result: RP-PCN在不同生长阶段的CD值显著降低，且通过完整点云计算的SEI指标使产量预测精度提高了11.2%。

Conclusion: RP-PCN模型可推广至其他作物，显著提升田间群体冠层结构的分析能力。

Abstract: Quantitative descriptions of complete canopy architecture are crucial for
evaluating crop photosynthesis and yield to guide ideotype design. Although
three-dimensional (3D) sensing technologies have been developed for plant and
canopy reconstruction, severe occlusion and complex architectures hinder
accurate canopy descriptions. In this study, we propose a point cloud
completion model for 3D reconstruction of rapeseed populations from seeding to
silique stages using multi-view imaging. A complete point cloud generation
framework was developed with the virtual-real integration (VRI) simulation
method and occlusion point detection algorithm to annotate the training dataset
by distinguishing surface from occluded points. The rapeseed population point
cloud completion network (RP-PCN) was designed with a multi-resolution dynamic
graph convolutional encoder (MRDG) and point pyramid decoder (PPD) to predict
occluded points based on input surface point clouds. A dynamic graph
convolutional feature extractor (DGCFE) was introduced to capture structural
variations across the growth period. The effectiveness of point cloud
completion was validated by predicting yield using architectural indicators
from complete point clouds of rapeseed population. The results demonstrated
that RP-PCN achieved chamfer distance (CD) values of 3.35 cm, 3.46 cm, 4.32 cm,
and 4.51 cm at the seedling, bolting, flowering, and silique stages,
respectively. Ablation studies showed the effectiveness of the MRDG and DGCFE
modules, reducing CD values by 10% and 23%, respectively. The silique
efficiency index (SEI) from RP-PCN improved yield prediction accuracy by 11.2%
compared to incomplete point clouds. The RP-PCN pipeline proposed in this study
has the potential to be extended to other crops, significantly enhancing the
analysis of population canopy architectures in field environments.

</details>


### [239] [Attention-Based Ensemble Learning for Crop Classification Using Landsat 8-9 Fusion](https://arxiv.org/abs/2506.18321)
*Zeeshan Ramzan,Nisar Ahmed,Qurat-ul-Ain Akram,Shahzad Asif,Muhammad Shahbaz,Rabin Chakrabortty,Ahmed F. Elaksher*

Main category: cs.CV

TL;DR: 该研究利用遥感技术和高级建模方法，结合实地调查和卫星影像数据，提高了灌溉农业区的作物分类精度。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过遥感技术获取灌溉农业区的作物覆盖信息，以解决传统方法在作物分类中的局限性。

Method: 研究分两阶段进行：实地调查和卫星影像采集，随后进行数据预处理、图像融合和分类建模（包括传统分类器、集成学习和神经网络）。

Result: 构建了包含50,835个数据点的数据集，并提取了多种植被指数，显著提高了作物分类的准确性。

Conclusion: 结合遥感数据和高级建模技术，可有效提升灌溉农业区的作物分类精度。

Abstract: Remote sensing offers a highly effective method for obtaining accurate
information on total cropped area and crop types. The study focuses on crop
cover identification for irrigated regions of Central Punjab. Data collection
was executed in two stages: the first involved identifying and geocoding six
target crops through field surveys conducted in January and February 2023. The
second stage involved acquiring Landsat 8-9 imagery for each geocoded field to
construct a labelled dataset. The satellite imagery underwent extensive
pre-processing, including radiometric calibration for reflectance values,
atmospheric correction, and georeferencing verification to ensure consistency
within a common coordinate system. Subsequently, image fusion techniques were
applied to combine Landsat 8 and 9 spectral bands, creating a composite image
with enhanced spectral information, followed by contrast enhancement. During
data acquisition, farmers were interviewed, and fields were meticulously mapped
using GPS instruments, resulting in a comprehensive dataset of 50,835 data
points. This dataset facilitated the extraction of vegetation indices such as
NDVI, SAVO, RECI, and NDRE. These indices and raw reflectance values were
utilized for classification modeling using conventional classifiers, ensemble
learning, and artificial neural networks. A feature selection approach was also
incorporated to identify the optimal feature set for classification learning.
This study demonstrates the effectiveness of combining remote sensing data and
advanced modeling techniques to improve crop classification accuracy in
irrigated agricultural regions.

</details>


### [240] [Escaping the SpuriVerse: Can Large Vision-Language Models Generalize Beyond Seen Spurious Correlations?](https://arxiv.org/abs/2506.18322)
*Yiwei Yang,Chung Peng Lee,Shangbin Feng,Dora Zhao,Bingbing Wen,Anthony Z. Liu,Yulia Tsvetkov,Bill Howe*

Main category: cs.CV

TL;DR: 论文研究了多模态大型视觉语言模型（LVLM）中的虚假相关性，提出了SpuriVerse基准测试，发现即使最先进的模型也表现不佳，但通过微调可以显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究虚假相关性在多模态LVLM中的影响，尤其是在真实世界数据集上的表现，而非人为设计的狭窄任务。

Method: 通过GPT-4o的错误分析、人工标注和合成反事实评估，构建了包含124种虚假相关性的SpuriVerse基准测试。

Result: 15种LVLM在SpuriVerse上表现不佳，最高准确率仅37.1%，但微调后提升至78.40%。

Conclusion: 模型通过多样化虚假模式的训练可以避免‘捷径’，更好地关注图像整体上下文。

Abstract: Finetuning can cause spurious correlations to arise between non-essential
features and the target labels, but benchmarks to study these effects involve
contrived settings and narrow tasks. In contrast, we consider spurious
correlations in multi-modal Large Vision Language Models (LVLMs) pretrained on
extensive and diverse datasets without explicit task supervision. We develop a
benchmark by sourcing GPT-4o errors on real-world visual-question-answering
(VQA) benchmarks, then curating a subset through LVLM-human annotation and
synthetic counterfactual evaluation to identify errors caused by spurious
correlations. This process yields SpuriVerse, a novel benchmark comprised of
124 distinct types of spurious correlations extracted from real-world datasets,
each containing 1 realistic and 10 synthetic VQA samples for a total of 1364
multiple choice questions. We evaluate 15 open and closed-source LVLMs on
SpuriVerse, finding that even state-of-the-art closed-source models struggle
significantly, achieving at best only 37.1% accuracy. Fine-tuning on synthetic
examples that emphasize the spurious correlation improves performance to
78.40%, suggesting that training on diverse spurious patterns generalizes to
unseen situations: models appear to learn to avoid "shortcuts" and attend to
the overall image context.

</details>


### [241] [A Multi-Scale Spatial Attention-Based Zero-Shot Learning Framework for Low-Light Image Enhancement](https://arxiv.org/abs/2506.18323)
*Muhammad Azeem Aslam,Hassan Khalid,Nisar Ahmed*

Main category: cs.CV

TL;DR: LucentVisionNet是一种零样本学习框架，用于低光图像增强，结合多尺度空间注意力和深度曲线估计网络，提升图像质量。


<details>
  <summary>Details</summary>
Motivation: 解决低光图像增强任务中缺乏配对训练数据的问题，克服传统和深度学习方法在语义和感知保真度上的局限性。

Method: 采用多尺度空间注意力与深度曲线估计网络，结合循环增强策略和复合损失函数（含六个组件，包括基于人类视觉感知的无参考图像质量损失）。

Result: 在多个基准数据集上优于现有监督、无监督和零样本方法，在视觉质量、结构一致性和计算效率方面表现优异。

Conclusion: LucentVisionNet适用于移动摄影、监控和自主导航等实际应用，具有高效性和高质量增强能力。

Abstract: Low-light image enhancement remains a challenging task, particularly in the
absence of paired training data. In this study, we present LucentVisionNet, a
novel zero-shot learning framework that addresses the limitations of
traditional and deep learning-based enhancement methods. The proposed approach
integrates multi-scale spatial attention with a deep curve estimation network,
enabling fine-grained enhancement while preserving semantic and perceptual
fidelity. To further improve generalization, we adopt a recurrent enhancement
strategy and optimize the model using a composite loss function comprising six
tailored components, including a novel no-reference image quality loss inspired
by human visual perception. Extensive experiments on both paired and unpaired
benchmark datasets demonstrate that LucentVisionNet consistently outperforms
state-of-the-art supervised, unsupervised, and zero-shot methods across
multiple full-reference and no-reference image quality metrics. Our framework
achieves high visual quality, structural consistency, and computational
efficiency, making it well-suited for deployment in real-world applications
such as mobile photography, surveillance, and autonomous navigation.

</details>


### [242] [NSFW-Classifier Guided Prompt Sanitization for Safe Text-to-Image Generation](https://arxiv.org/abs/2506.18325)
*Yu Xie,Chengjie Zeng,Lingyun Zhang,Yanwei Fu*

Main category: cs.CV

TL;DR: 论文提出PromptSan方法，通过两种变体（PromptSan-Modify和PromptSan-Suffix）净化有害文本提示，以降低文本到图像模型生成有害内容的风险，同时保持生成能力。


<details>
  <summary>Details</summary>
Motivation: 随着文本到图像模型的快速发展，其生成有害内容的风险增加，阻碍了技术的可持续发展。论文旨在解决这一问题，确保模型的安全性和可用性。

Method: 提出NSFW-Classifier Guided Prompt Sanitization (PromptSan)，包括PromptSan-Modify（迭代识别并替换有害标记）和PromptSan-Suffix（训练优化的后缀标记序列中和有害意图）。

Result: 实验表明，PromptSan在减少有害内容生成方面表现优异，同时平衡了安全性和可用性。

Conclusion: PromptSan是一种有效的方法，能够在保持生成能力的同时显著降低有害内容的生成风险。

Abstract: The rapid advancement of text-to-image (T2I) models, such as Stable
Diffusion, has enhanced their capability to synthesize images from textual
prompts. However, this progress also raises significant risks of misuse,
including the generation of harmful content (e.g., pornography, violence,
discrimination), which contradicts the ethical goals of T2I technology and
hinders its sustainable development. Inspired by "jailbreak" attacks in large
language models, which bypass restrictions through subtle prompt modifications,
this paper proposes NSFW-Classifier Guided Prompt Sanitization (PromptSan), a
novel approach to detoxify harmful prompts without altering model architecture
or degrading generation capability. PromptSan includes two variants:
PromptSan-Modify, which iteratively identifies and replaces harmful tokens in
input prompts using text NSFW classifiers during inference, and
PromptSan-Suffix, which trains an optimized suffix token sequence to neutralize
harmful intent while passing both text and image NSFW classifier checks.
Extensive experiments demonstrate that PromptSan achieves state-of-the-art
performance in reducing harmful content generation across multiple metrics,
effectively balancing safety and usability.

</details>


### [243] [Geometry-Aware Preference Learning for 3D Texture Generation](https://arxiv.org/abs/2506.18331)
*AmirHossein Zamani,Tianhao Xie,Amir G. Aghdam,Tiberiu Popa,Eugene Belilovsky*

Main category: cs.CV

TL;DR: 提出了一种端到端的可微分偏好学习框架，通过几何感知奖励函数优化3D生成模型，使其更符合人类偏好和任务需求。


<details>
  <summary>Details</summary>
Motivation: 现有3D生成模型生成的内容可能不符合人类主观偏好或任务特定标准，且多数方法依赖2D文本到图像模型，缺乏对3D结构的理解。

Method: 提出端到端可微分偏好学习框架，通过几何感知奖励函数反向传播人类偏好，优化3D生成流程。

Result: 使用四种几何感知奖励函数验证了框架的有效性，实现了更可控和可解释的高质量3D内容生成。

Conclusion: 该框架为基于自然语言的高质量3D内容生成提供了更可控和几何感知的解决方案。

Abstract: Recent advances in 3D generative models have achieved impressive results but
3D contents generated by these models may not align with subjective human
preferences or task-specific criteria. Moreover, a core challenge in the 3D
texture generation domain remains: most existing approaches rely on repeated
calls to 2D text-to-image generative models, which lack an inherent
understanding of the 3D structure of the input 3D mesh object. To address this,
we propose an end-to-end differentiable preference learning framework that
back-propagates human preferences, represented by differentiable reward
functions, through the entire 3D generative pipeline, making the process
inherently geometry-aware. We demonstrate the effectiveness of our framework
using four proposed novel geometry-aware reward functions, offering a more
controllable and interpretable pathway for high-quality 3D content creation
from natural language.

</details>


### [244] [Rethinking Decoder Design: Improving Biomarker Segmentation Using Depth-to-Space Restoration and Residual Linear Attention](https://arxiv.org/abs/2506.18335)
*Saad Wazir,Daeyoung Kim*

Main category: cs.CV

TL;DR: 提出了一种新的医学图像分割架构，通过多尺度特征捕获和高效解码器设计，显著提升了分割精度。


<details>
  <summary>Details</summary>
Motivation: 解决现有Transformer和CNN方法在医学图像分割中因染色和形态变化导致的特征提取不足问题，以及端到端方法在有限数据集下的性能限制。

Method: 设计了一种能够捕获多尺度局部和全局上下文信息的架构，并引入新型解码器，有效整合编码器特征、强调重要通道和区域，并重建空间维度。

Result: 在四个数据集上的实验表明，该方法比现有SOTA方法性能提升显著，绝对增益分别为2.76%（MoNuSeg）、3.12%（DSB）、2.87%（Electron Microscopy）和4.03%（TNBC）。

Conclusion: 提出的方法在医学图像分割任务中表现优异，兼容多种编码器，并通过实验验证了其有效性。

Abstract: Segmenting biomarkers in medical images is crucial for various biotech
applications. Despite advances, Transformer and CNN based methods often
struggle with variations in staining and morphology, limiting feature
extraction. In medical image segmentation, where datasets often have limited
sample availability, recent state-of-the-art (SOTA) methods achieve higher
accuracy by leveraging pre-trained encoders, whereas end-to-end methods tend to
underperform. This is due to challenges in effectively transferring rich
multiscale features from encoders to decoders, as well as limitations in
decoder efficiency. To address these issues, we propose an architecture that
captures multi-scale local and global contextual information and a novel
decoder design, which effectively integrates features from the encoder,
emphasizes important channels and regions, and reconstructs spatial dimensions
to enhance segmentation accuracy. Our method, compatible with various encoders,
outperforms SOTA methods, as demonstrated by experiments on four datasets and
ablation studies. Specifically, our method achieves absolute performance gains
of 2.76% on MoNuSeg, 3.12% on DSB, 2.87% on Electron Microscopy, and 4.03% on
TNBC datasets compared to existing SOTA methods. Code:
https://github.com/saadwazir/MCADS-Decoder

</details>


### [245] [BSMamba: Brightness and Semantic Modeling for Long-Range Interaction in Low-Light Image Enhancement](https://arxiv.org/abs/2506.18346)
*Tongshun Zhang,Pingping Liu,Mengen Cai,Zijian Zhang,Yubing Lu,Qiuzhan Zhou*

Main category: cs.CV

TL;DR: BSMamba是一种新型视觉Mamba架构，通过亮度Mamba和语义Mamba组件，解决了低光图像增强中亮度恢复和语义一致性的问题。


<details>
  <summary>Details</summary>
Motivation: 现有低光图像增强方法在提升亮度的同时难以保持语义一致性和细节，且视觉Mamba方法因固定扫描规则限制了远距离标记的交互。

Method: BSMamba包含亮度Mamba和语义Mamba，分别基于亮度和语义相似性建模标记交互，突破传统扫描模式的限制。

Result: 实验表明，BSMamba在低光图像增强任务中实现了最先进的性能，同时保持了语义一致性。

Conclusion: BSMamba通过创新的标记交互模式，显著提升了低光图像增强的效果。

Abstract: Current low-light image enhancement (LLIE) methods face significant
limitations in simultaneously improving brightness while preserving semantic
consistency, fine details, and computational efficiency. With the emergence of
state-space models, particularly Mamba, image restoration has achieved
remarkable performance, yet existing visual Mamba approaches flatten 2D images
into 1D token sequences using fixed scanning rules, critically limiting
interactions between distant tokens with causal relationships and constraining
their ability to capture meaningful long-range dependencies. To address these
fundamental limitations, we propose BSMamba, a novel visual Mamba architecture
comprising two specially designed components: Brightness Mamba and Semantic
Mamba. The Brightness Mamba revolutionizes token interaction patterns by
prioritizing connections between distant tokens with similar brightness levels,
effectively addressing the challenge of brightness restoration in LLIE tasks
through brightness-guided selective attention. Complementing this, the Semantic
Mamba establishes priority interactions between tokens sharing similar semantic
meanings, allowing the model to maintain contextual consistency by connecting
semantically related regions across the image, thus preserving the hierarchical
nature of image semantics during enhancement. By intelligently modeling tokens
based on brightness and semantic similarity rather than arbitrary scanning
patterns, BSMamba transcends the constraints of conventional token sequencing
while adhering to the principles of causal modeling. Extensive experiments
demonstrate that BSMamba achieves state-of-the-art performance in LLIE while
preserving semantic consistency.

</details>


### [246] [Spatial frequency information fusion network for few-shot learning](https://arxiv.org/abs/2506.18364)
*Wenqing Zhao,Guojia Xie,Han Pan,Biao Yang,Weichuan Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种结合频域和空间域信息的SFIFNet方法，通过创新的数据预处理提升小样本学习的分类性能。


<details>
  <summary>Details</summary>
Motivation: 小样本学习中，传统方法常忽略频域信息，导致特征利用不充分，影响分类性能。

Method: 基于常规数据增强，提出SFIFNet方法，整合频域与空间域信息以提升特征表示准确性。

Result: 实验证明该方法有效提升了分类性能。

Conclusion: 结合频域信息的方法在小样本学习中具有显著优势。

Abstract: The objective of Few-shot learning is to fully leverage the limited data
resources for exploring the latent correlations within the data by applying
algorithms and training a model with outstanding performance that can
adequately meet the demands of practical applications. In practical
applications, the number of images in each category is usually less than that
in traditional deep learning, which can lead to over-fitting and poor
generalization performance. Currently, many Few-shot classification models pay
more attention to spatial domain information while neglecting frequency domain
information, which contains more feature information. Ignoring frequency domain
information will prevent the model from fully exploiting feature information,
which would effect the classification performance. Based on conventional data
augmentation, this paper proposes an SFIFNet with innovative data
preprocessing. The key of this method is enhancing the accuracy of image
feature representation by integrating frequency domain information with spatial
domain information. The experimental results demonstrate the effectiveness of
this method in enhancing classification performance.

</details>


### [247] [Sequential keypoint density estimator: an overlooked baseline of skeleton-based video anomaly detection](https://arxiv.org/abs/2506.18368)
*Anja Delić,Matej Grcić,Siniša Šegvić*

Main category: cs.CV

TL;DR: SeeKer是一种通过自回归因子化检测人体骨骼序列异常的方法，利用关键点条件分布建模，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 在医疗监控、工作场所安全和公共监控等安全关键应用中，检测异常行为至关重要，而异常常表现为不寻常的人体姿态。

Method: 通过关键点级别的自回归因子化建模骨骼序列密度，利用条件高斯分布预测关键点位置，并通过加权对数条件概率计算异常分数。

Result: 在UBnormal、MSAD-HR数据集上表现最优，在上海Tech数据集上也有竞争力。

Conclusion: SeeKer方法简单但高效，显著提升了异常检测性能。

Abstract: Detecting anomalous human behaviour is an important visual task in
safety-critical applications such as healthcare monitoring, workplace safety,
or public surveillance. In these contexts, abnormalities are often reflected
with unusual human poses. Thus, we propose SeeKer, a method for detecting
anomalies in sequences of human skeletons. Our method formulates the skeleton
sequence density through autoregressive factorization at the keypoint level.
The corresponding conditional distributions represent probable keypoint
locations given prior skeletal motion. We formulate the joint distribution of
the considered skeleton as causal prediction of conditional Gaussians across
its constituent keypoints. A skeleton is flagged as anomalous if its keypoint
locations surprise our model (i.e. receive a low density). In practice, our
anomaly score is a weighted sum of per-keypoint log-conditionals, where the
weights account for the confidence of the underlying keypoint detector. Despite
its conceptual simplicity, SeeKer surpasses all previous methods on the
UBnormal and MSAD-HR datasets while delivering competitive performance on the
ShanghaiTech dataset.

</details>


### [248] [RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language Models](https://arxiv.org/abs/2506.18369)
*Yeongtak Oh,Jisoo Mok,Dohyun Chung,Juhyeon Shin,Sangha Park,Johan Barthelemy,Sungroh Yoon*

Main category: cs.CV

TL;DR: 提出了一种基于强化学习的后训练框架，显著提升了多模态大语言模型在个性化图像描述任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于监督微调的后训练方法在生成个性化图像描述时表现不佳，尤其是在复杂场景下。

Method: 采用强化学习（RL）作为后训练框架，替代传统的监督微调（SFT）。

Result: 该方法在多概念图像描述任务中显著优于现有基线，提升了模型的视觉识别和个性化生成能力。

Conclusion: 强化学习是一种有效的后训练方法，能够解决数据稀缺问题并提升模型性能。

Abstract: Recent multi-modal large language models (MLLMs) often struggle to generate
personalized image captions, even when trained on high-quality captions. In
this work, we observe that such limitations persist in existing
post-training-based MLLM personalization methods. Specifically, despite being
post-tuned with large-scale caption data through supervised fine-tuning (SFT),
these models frequently fail to produce faithful descriptions in real-world
scenarios, such as multi-concept image captioning. However, acquiring
large-scale, high-quality captions for such complex settings is both costly and
difficult. To address the data-centric nature of SFT, we propose a
reinforcement learning (RL)-based post-training framework. To the best of our
knowledge, this is the first RL-based approach to post-train MLLMs for
personalized image captioning. Our method significantly enhances both visual
recognition and personalized generation capabilities of MLLMs, and consistently
outperforms existing SFT-based baselines, especially in the challenging
multi-concept image captioning task.

</details>


### [249] [OpenEvents V1: Large-Scale Benchmark Dataset for Multimodal Event Grounding](https://arxiv.org/abs/2506.18372)
*Hieu Nguyen,Phuc-Tan Nguyen,Thien-Phuc Tran,Minh-Quang Nguyen,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: OpenEvents V1是一个大规模基准数据集，专注于事件为中心的视觉语言理解，包含两个任务：生成事件感知图像描述和基于叙事式文本查询检索相关图像。


<details>
  <summary>Details</summary>
Motivation: 传统图像描述和检索数据集侧重于表面描述，而OpenEvents V1旨在通过上下文和时间基础推动深度推理。

Method: 数据集包含20万篇新闻文章和40万张相关图像，来自CNN和The Guardian，涵盖多样领域和时间段。

Result: 提供了基线结果和标准化评估协议，支持多模态模型的开发。

Conclusion: OpenEvents V1为复杂现实事件的深度推理提供了坚实基础，数据集已公开。

Abstract: We introduce OpenEvents V1, a large-scale benchmark dataset aimed at
advancing event-centric vision-language understanding. Unlike conventional
image captioning and retrieval datasets that emphasize surface-level
descriptions, OpenEvents V1 focuses on contextual and temporal grounding
through two primary tasks: (1) generating rich, event-aware image captions and
(2) retrieving event-relevant images based on narrative-style textual queries.
The dataset contains over 200,000 news articles and 400,000 associated images
sourced from CNN and The Guardian, spanning diverse domains and time periods.
We provide extensive baseline results and standardized evaluation protocols for
both tasks. OpenEvents V1 establishes a robust foundation for developing
multimodal models capable of deep reasoning over complex real-world events. The
dataset is available at https://ltnghia.github.io/eventa/openevents-v1

</details>


### [250] [InternSpatial: A Comprehensive Dataset for Spatial Reasoning in Vision-Language Models](https://arxiv.org/abs/2506.18385)
*Nianchen Deng,Lixin Gu,Shenglong Ye,Yinan He,Zhe Chen,Songze Li,Haomin Wang,Xingguang Wei,Tianshuo Yang,Min Dou,Tong He,Wenqi Shao,Kaipeng Zhang,Yi Wang,Botian Shi,Yanting Zhang,Jifeng Dai,Yu Qiao,Hongjie Zhang,Wenhai Wang*

Main category: cs.CV

TL;DR: 论文提出了InternSpatial数据集和InternSpatial-Bench评估基准，旨在提升视觉语言模型的空间推理能力，实验显示模型性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有空间推理数据集规模小、视觉多样性不足且指令表达有限，限制了视觉语言模型的发展。

Method: 构建了包含1200万QA对的InternSpatial数据集，支持19种指令格式，并设计了InternSpatial-Bench评估基准，新增旋转角度预测任务。

Result: 模型在InternSpatial-Bench和VSI-Bench上分别提升12.1%和10.7%，同时保持通用基准的强性能。

Conclusion: InternSpatial及其评估基准有望推动空间推理能力在实际应用（如机器人）中的发展。

Abstract: Recent benchmarks and datasets have been proposed to improve spatial
reasoning in vision-language models (VLMs), yet existing open resources remain
limited in scale, visual diversity, and instruction expressiveness. In this
work, we introduce InternSpatial, the largest open-source dataset for spatial
reasoning in VLMs, along with InternSpatial-Bench, a corresponding evaluation
benchmark designed to assess spatial understanding under diverse instruction
formats. InternSpatial comprises 12 million QA pairs spanning both single-view
and multi-view settings, drawn from diverse visual environments and supporting
19 instruction formats that reflect varied query styles. For evaluation, we
propose InternSpatial-Bench for single-view tasks and expand multi-view
reasoning by introducing a novel rotation angle prediction task that has not
been explored in prior work. Experimental results show that models trained on
InternSpatial achieve 12.1% improvement on InternSpatial-Bench and 10.7% on
VSI-Bench, while maintaining strong performance on general-purpose benchmarks.
We hope these resources will support the development of spatially capable VLMs
in practical applications such as robotics and embodied AI.

</details>


### [251] [Distributed Poisson multi-Bernoulli filtering via generalised covariance intersection](https://arxiv.org/abs/2506.18397)
*Ángel F. García-Fernández,Giorgio Battistelli*

Main category: cs.CV

TL;DR: 本文提出了一种基于广义协方差交集（GCI）融合规则的分布式泊松多伯努利（PMB）滤波器，用于分布式多目标滤波。通过近似PMB密度的幂作为未归一化的PMB密度，实现了GCI融合的可行解。


<details>
  <summary>Details</summary>
Motivation: 解决分布式多目标滤波中PMB密度的GCI融合难以精确计算的问题。

Method: 近似PMB密度的幂作为未归一化的PMB密度，并通过GCI融合规则得到泊松多伯努利混合（PMBM）形式的结果。

Result: 实验结果表明，该方法优于其他分布式多目标滤波器。

Conclusion: 提出的方法为分布式多目标滤波提供了一种有效的解决方案，并展示了其优越性。

Abstract: This paper presents the distributed Poisson multi-Bernoulli (PMB) filter
based on the generalised covariance intersection (GCI) fusion rule for
distributed multi-object filtering. Since the exact GCI fusion of two PMB
densities is intractable, we derive a principled approximation. Specifically,
we approximate the power of a PMB density as an unnormalised PMB density, which
corresponds to an upper bound of the PMB density. Then, the GCI fusion rule
corresponds to the normalised product of two unnormalised PMB densities. We
show that the result is a Poisson multi-Bernoulli mixture (PMBM), which can be
expressed in closed form. Future prediction and update steps in each filter
preserve the PMBM form, which can be projected back to a PMB density before the
next fusion step. Experimental results show the benefits of this approach
compared to other distributed multi-object filters.

</details>


### [252] [Latent Space Analysis for Melanoma Prevention](https://arxiv.org/abs/2506.18414)
*Ciro Listone,Aniello Murano*

Main category: cs.CV

TL;DR: 提出了一种基于条件变分自编码器的可解释性风险建模方法，用于皮肤病变的连续评估和分类。


<details>
  <summary>Details</summary>
Motivation: 黑色素瘤的高死亡率和现有深度学习模型仅提供二元分类的局限性，促使开发更可解释的诊断工具。

Method: 使用条件变分自编码器学习结构化潜在空间，结合SVM进行分类，实现形态差异的连续评估。

Result: 方法在区分良性痣和黑色素瘤上表现优异，潜在空间支持视觉和几何解释，空间接近性可作为风险指标。

Conclusion: 该方法结合预测性能和临床适用性，促进早期检测和透明决策，增强AI辅助诊断的可信度。

Abstract: Melanoma represents a critical health risk due to its aggressive progression
and high mortality, underscoring the need for early, interpretable diagnostic
tools. While deep learning has advanced in skin lesion classification, most
existing models provide only binary outputs, offering limited clinical insight.
This work introduces a novel approach that extends beyond classification,
enabling interpretable risk modelling through a Conditional Variational
Autoencoder. The proposed method learns a structured latent space that captures
semantic relationships among lesions, allowing for a nuanced, continuous
assessment of morphological differences. An SVM is also trained on this
representation effectively differentiating between benign nevi and melanomas,
demonstrating strong and consistent performance. More importantly, the learned
latent space supports visual and geometric interpretation of malignancy, with
the spatial proximity of a lesion to known melanomas serving as a meaningful
indicator of risk. This approach bridges predictive performance with clinical
applicability, fostering early detection, highlighting ambiguous cases, and
enhancing trust in AI-assisted diagnosis through transparent and interpretable
decision-making.

</details>


### [253] [Benchmarking Foundation Models and Parameter-Efficient Fine-Tuning for Prognosis Prediction in Medical Imaging](https://arxiv.org/abs/2506.18434)
*Filippo Ruffini,Elena Mulero Ayllon,Linlin Shen,Paolo Soda,Valerio Guarrasi*

Main category: cs.CV

TL;DR: 该论文提出了一个结构化基准，用于评估和比较卷积神经网络和基础模型在COVID-19患者临床预后预测中的迁移能力，并探索了多种微调策略。


<details>
  <summary>Details</summary>
Motivation: AI在医学影像预后预测中具有潜力，但实际应用仍面临挑战。研究旨在通过结构化基准评估模型在数据稀缺和类别不平衡条件下的表现。

Method: 研究采用了多种微调策略（如全微调、线性探测、参数高效微调方法），并在多种学习范式（全数据场景和少样本学习）下评估模型性能。

Result: 通过大规模比较分析，研究评估了不同预训练模型在预后任务中的适应性和泛化能力，尤其是在数据稀缺和类别不平衡条件下的表现。

Conclusion: 该基准为临床预后预测中AI解决方案的实际部署提供了详细见解，旨在推动高效、鲁棒且可泛化的AI应用。

Abstract: Artificial Intelligence (AI) holds significant promise for improving
prognosis prediction in medical imaging, yet its effective application remains
challenging. In this work, we introduce a structured benchmark explicitly
designed to evaluate and compare the transferability of Convolutional Neural
Networks and Foundation Models in predicting clinical outcomes in COVID-19
patients, leveraging diverse publicly available Chest X-ray datasets. Our
experimental methodology extensively explores a wide set of fine-tuning
strategies, encompassing traditional approaches such as Full Fine-Tuning and
Linear Probing, as well as advanced Parameter-Efficient Fine-Tuning methods
including Low-Rank Adaptation, BitFit, VeRA, and IA3. The evaluations were
conducted across multiple learning paradigms, including both extensive
full-data scenarios and more clinically realistic Few-Shot Learning settings,
which are critical for modeling rare disease outcomes and rapidly emerging
health threats. By implementing a large-scale comparative analysis involving a
diverse selection of pretrained models, including general-purpose architectures
pretrained on large-scale datasets such as CLIP and DINOv2, to
biomedical-specific models like MedCLIP, BioMedCLIP, and PubMedCLIP, we
rigorously assess each model's capacity to effectively adapt and generalize to
prognosis tasks, particularly under conditions of severe data scarcity and
pronounced class imbalance. The benchmark was designed to capture critical
conditions common in prognosis tasks, including variations in dataset size and
class distribution, providing detailed insights into the strengths and
limitations of each fine-tuning strategy. This extensive and structured
evaluation aims to inform the practical deployment and adoption of robust,
efficient, and generalizable AI-driven solutions in real-world clinical
prognosis prediction workflows.

</details>


### [254] [Frequency-Domain Fusion Transformer for Image Inpainting](https://arxiv.org/abs/2506.18437)
*Sijin He,Guangfeng Lin,Tao Li,Yajun Chen*

Main category: cs.CV

TL;DR: 提出了一种结合频域融合的Transformer图像修复方法，通过小波变换和Gabor滤波增强多尺度结构建模，同时设计了可学习的频域滤波器以保留高频细节。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以处理复杂纹理和大遮挡，而基于Transformer的方法虽具有全局建模能力，但高频细节保留不足且计算成本高。

Method: 结合小波变换和Gabor滤波的注意力机制，设计可学习的频域滤波器，采用四层编码器-解码器结构，并使用新损失策略平衡全局语义和细节。

Result: 实验表明，该方法能有效保留更多高频信息，提升图像修复质量。

Conclusion: 提出的方法在保留高频细节和全局建模方面表现优异，解决了传统和Transformer方法的局限性。

Abstract: Image inpainting plays a vital role in restoring missing image regions and
supporting high-level vision tasks, but traditional methods struggle with
complex textures and large occlusions. Although Transformer-based approaches
have demonstrated strong global modeling capabilities, they often fail to
preserve high-frequency details due to the low-pass nature of self-attention
and suffer from high computational costs. To address these challenges, this
paper proposes a Transformer-based image inpainting method incorporating
frequency-domain fusion. Specifically, an attention mechanism combining wavelet
transform and Gabor filtering is introduced to enhance multi-scale structural
modeling and detail preservation. Additionally, a learnable frequency-domain
filter based on the fast Fourier transform is designed to replace the
feedforward network, enabling adaptive noise suppression and detail retention.
The model adopts a four-level encoder-decoder structure and is guided by a
novel loss strategy to balance global semantics and fine details. Experimental
results demonstrate that the proposed method effectively improves the quality
of image inpainting by preserving more high-frequency information.

</details>


### [255] [OmniGen2: Exploration to Advanced Multimodal Generation](https://arxiv.org/abs/2506.18871)
*Chenyuan Wu,Pengfei Zheng,Ruiran Yan,Shitao Xiao,Xin Luo,Yueze Wang,Wanli Li,Xiyan Jiang,Yexin Liu,Junjie Zhou,Ze Liu,Ziyi Xia,Chaofan Li,Haoge Deng,Jiahao Wang,Kun Luo,Bo Zhang,Defu Lian,Xinlong Wang,Zhongyuan Wang,Tiejun Huang,Zheng Liu*

Main category: cs.CV

TL;DR: OmniGen2是一个多功能开源生成模型，支持文本到图像、图像编辑和上下文生成任务，通过双解码路径和独立图像分词器实现高效性能。


<details>
  <summary>Details</summary>
Motivation: 提供统一的生成解决方案，同时保留文本生成能力，避免重新适应VAE输入。

Method: 采用双解码路径（文本和图像模态）、独立图像分词器，并开发了数据构建管道和反射机制。

Result: 在多个任务基准测试中表现优异，尤其在上下文生成任务中达到开源模型的最先进水平。

Conclusion: OmniGen2在保持较小参数规模的同时，实现了高效的多任务生成性能，并开源了相关资源以促进未来研究。

Abstract: In this work, we introduce OmniGen2, a versatile and open-source generative
model designed to provide a unified solution for diverse generation tasks,
including text-to-image, image editing, and in-context generation. Unlike
OmniGen v1, OmniGen2 features two distinct decoding pathways for text and image
modalities, utilizing unshared parameters and a decoupled image tokenizer. This
design enables OmniGen2 to build upon existing multimodal understanding models
without the need to re-adapt VAE inputs, thereby preserving the original text
generation capabilities. To facilitate the training of OmniGen2, we developed
comprehensive data construction pipelines, encompassing image editing and
in-context generation data. Additionally, we introduce a reflection mechanism
tailored for image generation tasks and curate a dedicated reflection dataset
based on OmniGen2. Despite its relatively modest parameter size, OmniGen2
achieves competitive results on multiple task benchmarks, including
text-to-image and image editing. To further evaluate in-context generation,
also referred to as subject-driven tasks, we introduce a new benchmark named
OmniContext. OmniGen2 achieves state-of-the-art performance among open-source
models in terms of consistency. We will release our models, training code,
datasets, and data construction pipeline to support future research in this
field. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link:
https://github.com/VectorSpaceLab/OmniGen2

</details>


### [256] [CPAM: Context-Preserving Adaptive Manipulation for Zero-Shot Real Image Editing](https://arxiv.org/abs/2506.18438)
*Dinh-Khoi Vo,Thanh-Toan Do,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: 提出了一种零样本框架CPAM，用于复杂非刚性图像编辑，通过自适应模块和掩码引导技术保留对象和背景。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在保留纹理和身份、处理复杂非刚性对象以及编辑特定区域时的局限性。

Method: 提出保护适应模块和局部提取模块，调整自注意力机制，结合掩码引导技术。

Result: 在IMBA基准测试中表现优于现有方法，成为人类评分者的首选。

Conclusion: CPAM是一种高效、零样本的图像编辑框架，能够有效保留对象和背景细节。

Abstract: Editing natural images using textual descriptions in text-to-image diffusion
models remains a significant challenge, particularly in achieving consistent
generation and handling complex, non-rigid objects. Existing methods often
struggle to preserve textures and identity, require extensive fine-tuning, and
exhibit limitations in editing specific spatial regions or objects while
retaining background details. This paper proposes Context-Preserving Adaptive
Manipulation (CPAM), a novel zero-shot framework for complicated, non-rigid
real image editing. Specifically, we propose a preservation adaptation module
that adjusts self-attention mechanisms to preserve and independently control
the object and background effectively. This ensures that the objects' shapes,
textures, and identities are maintained while keeping the background
undistorted during the editing process using the mask guidance technique.
Additionally, we develop a localized extraction module to mitigate the
interference with the non-desired modified regions during conditioning in
cross-attention mechanisms. We also introduce various mask-guidance strategies
to facilitate diverse image manipulation tasks in a simple manner. Extensive
experiments on our newly constructed Image Manipulation BenchmArk (IMBA), a
robust benchmark dataset specifically designed for real image editing,
demonstrate that our proposed method is the preferred choice among human
raters, outperforming existing state-of-the-art editing techniques.

</details>


### [257] [Vision as a Dialect: Unifying Visual Understanding and Generation via Text-Aligned Representations](https://arxiv.org/abs/2506.18898)
*Jiaming Han,Hao Chen,Yang Zhao,Hanyu Wang,Qi Zhao,Ziyan Yang,Hao He,Xiangyu Yue,Lu Jiang*

Main category: cs.CV

TL;DR: 本文提出了一种多模态框架Tar，通过共享离散语义表示统一视觉理解和生成，核心是文本对齐分词器（TA-Tok），结合自适应编码解码和生成去分词器，提升跨模态任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多模态任务中需要针对不同模态设计特定接口，缺乏统一的表示和高效的处理方式。本文旨在通过共享离散语义表示和统一接口解决这一问题。

Method: 提出文本对齐分词器（TA-Tok）将图像转换为离散标记，结合自适应编码解码和两种去分词器（自回归和扩散模型），并通过高级预训练任务增强模态融合。

Result: 实验表明，Tar在多模态基准测试中优于现有方法，收敛更快且训练效率更高。

Conclusion: Tar框架通过统一表示和高效设计，显著提升了视觉理解和生成任务的性能。

Abstract: This paper presents a multimodal framework that attempts to unify visual
understanding and generation within a shared discrete semantic representation.
At its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into
discrete tokens using a text-aligned codebook projected from a large language
model's (LLM) vocabulary. By integrating vision and text into a unified space
with an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input
and output through a shared interface, without the need for modality-specific
designs. Additionally, we propose scale-adaptive encoding and decoding to
balance efficiency and visual detail, along with a generative de-tokenizer to
produce high-fidelity visual outputs. To address diverse decoding needs, we
utilize two complementary de-tokenizers: a fast autoregressive model and a
diffusion-based model. To enhance modality fusion, we investigate advanced
pre-training tasks, demonstrating improvements in both visual understanding and
generation. Experiments across benchmarks show that Tar matches or surpasses
existing multimodal LLM methods, achieving faster convergence and greater
training efficiency. Code, models, and data are available at
https://tar.csuhan.com

</details>


### [258] [DIP: Unsupervised Dense In-Context Post-training of Visual Representations](https://arxiv.org/abs/2506.18463)
*Sophia Sirko-Galouchenko,Spyros Gidaris,Antonin Vobecky,Andrei Bursuc,Nicolas Thome*

Main category: cs.CV

TL;DR: DIP是一种无监督的后训练方法，通过模拟下游任务场景的伪任务增强预训练视觉编码器的密集图像表示，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 提升大规模预训练视觉编码器在上下文场景理解中的密集表示能力，避免依赖复杂的自蒸馏架构。

Method: 利用伪任务模拟下游场景，结合预训练扩散模型和视觉编码器自动生成任务，方法简单且计算高效。

Result: 在多种下游任务中表现优异，优于初始编码器和现有方法，计算成本低（单A100 GPU不到9小时）。

Conclusion: DIP为改进密集表示提供了实用有效的解决方案，代码已开源。

Abstract: We introduce DIP, a novel unsupervised post-training method designed to
enhance dense image representations in large-scale pretrained vision encoders
for in-context scene understanding. Unlike prior approaches that rely on
complex self-distillation architectures, our method trains the vision encoder
using pseudo-tasks that explicitly simulate downstream in-context scenarios,
inspired by meta-learning principles. To enable post-training on unlabeled
data, we propose an automatic mechanism for generating in-context tasks that
combines a pretrained diffusion model and the vision encoder itself. DIP is
simple, unsupervised, and computationally efficient, requiring less than 9
hours on a single A100 GPU. By learning dense representations through pseudo
in-context tasks, it achieves strong performance across a wide variety of
downstream real-world in-context scene understanding tasks. It outperforms both
the initial vision encoder and prior methods, offering a practical and
effective solution for improving dense representations. Code available here:
https://github.com/sirkosophia/DIP

</details>


### [259] [AViLA: Asynchronous Vision-Language Agent for Streaming Multimodal Data Interaction](https://arxiv.org/abs/2506.18472)
*Gengyuan Zhang,Tanveer Hannan,Hermine Kleiner,Beste Aydemir,Xinyu Xie,Jian Lan,Thomas Seidl,Volker Tresp,Jindong Gu*

Main category: cs.CV

TL;DR: 论文提出了一个名为AViLA的异步视频-语言代理，用于处理流数据中的查询-证据异步问题，并通过三个关键模块提升模型的准确性和时间感知能力。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，视觉-语言代理需要在动态数据流中处理用户查询，但查询与支持证据通常异步到达，这需要代理具备强大的推理能力和时间感知能力。

Method: 提出了AViLA代理，包含三个模块：全面记忆保留、证据识别和证据触发的响应机制，以处理流数据中的异步查询。

Result: 实验表明，现有模型在时间感知上表现不佳，而AViLA显著提升了准确性和时间感知能力。

Conclusion: AViLA为解决查询-证据异步问题提供了有效方案，未来将公开代码和数据集。

Abstract: An ideal vision-language agent serves as a bridge between the human users and
their surrounding physical world in real-world applications like autonomous
driving and embodied agents, and proactively provides accurate and timely
responses given user intents. An intriguing challenge arises when agents
interact with the world as a dynamic data stream and ad-hoc queries from users:
supporting knowledge for queries, namely evidence, usually appears
asynchronously with the arrival time of queries, and agents need to ground
their responses in historical data, present observations, and even future
streams. We frame this challenge as Query-Evidence Asynchrony, where user
queries and their supporting evidence typically arrive asynchronously in the
streaming setting. This setting requires not only strong reasoning capabilities
but also the ability to retain past observations and respond to queries with
temporal awareness. In this paper, we introduce a diagnostic benchmark that
evaluates Multimodal Large Language Models (MLLMs) on their ability to handle
interaction with streaming data. Further, we present AViLA, Asynchronous
Video-Language Agent for streaming data interaction that can handle ad-hoc
queries and give time-aware responses. For this purpose, AViLA consists of
three key modules: comprehensive memory retention, evidence identification, and
evidence-grounded trigger, that are designed to maintain a general-purpose
memory and respond readily and timely to queries. Our experiments show that
existing models often fail to respond at appropriate times, while AViLA
significantly improves both accuracy and temporal awareness. Our code and
dataset will be publicly available.

</details>


### [260] [Context Consistency Learning via Sentence Removal for Semi-Supervised Video Paragraph Grounding](https://arxiv.org/abs/2506.18476)
*Yaokun Zhong,Siyu Jiang,Jian Zhu,Jian-Fang Hu*

Main category: cs.CV

TL;DR: 提出了一种新的上下文一致性学习框架（CCL），通过结合一致性正则化和伪标签技术，在半监督视频段落定位任务中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法忽视了通过扰动查询上下文生成强监督信号的重要性，限制了半监督学习的效果。

Method: 提出CCL框架，结合教师-学生学习和伪标签技术，通过强增强样本和模型重训练提升性能。

Result: 实验表明，CCL大幅优于现有方法。

Conclusion: CCL框架通过上下文一致性学习，显著提升了半监督视频段落定位的性能。

Abstract: Semi-Supervised Video Paragraph Grounding (SSVPG) aims to localize multiple
sentences in a paragraph from an untrimmed video with limited temporal
annotations. Existing methods focus on teacher-student consistency learning and
video-level contrastive loss, but they overlook the importance of perturbing
query contexts to generate strong supervisory signals. In this work, we propose
a novel Context Consistency Learning (CCL) framework that unifies the paradigms
of consistency regularization and pseudo-labeling to enhance semi-supervised
learning. Specifically, we first conduct teacher-student learning where the
student model takes as inputs strongly-augmented samples with sentences removed
and is enforced to learn from the adequately strong supervisory signals from
the teacher model. Afterward, we conduct model retraining based on the
generated pseudo labels, where the mutual agreement between the original and
augmented views' predictions is utilized as the label confidence. Extensive
experiments show that CCL outperforms existing methods by a large margin.

</details>


### [261] [GANs vs. Diffusion Models for virtual staining with the HER2match dataset](https://arxiv.org/abs/2506.18484)
*Pascal Klöckner,José Teixeira,Diana Montezuma,Jaime S. Cardoso,Hugo M. Horlings,Sara P. Oliveira*

Main category: cs.CV

TL;DR: 论文介绍了首个公开的H&E-HER2染色匹配数据集HER2match，并比较了GANs和DMs在H&E-HER2染色转换任务中的性能，发现GANs表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决H&E-HER2染色转换任务中公开数据集不足和模型框架选择不明确的问题。

Method: 引入HER2match数据集，比较多种GANs和DMs，并提出一种新的Brownian Bridge Diffusion Model。

Result: GANs整体表现优于DMs，仅BBDM与之相当；数据对齐显著提升模型效果。

Conclusion: 研究提供了高质量数据集和模型框架比较，为相关研究提供了重要参考。

Abstract: Virtual staining is a promising technique that uses deep generative models to
recreate histological stains, providing a faster and more cost-effective
alternative to traditional tissue chemical staining. Specifically for H&E-HER2
staining transfer, despite a rising trend in publications, the lack of
sufficient public datasets has hindered progress in the topic. Additionally, it
is currently unclear which model frameworks perform best for this particular
task. In this paper, we introduce the HER2match dataset, the first publicly
available dataset with the same breast cancer tissue sections stained with both
H&E and HER2. Furthermore, we compare the performance of several Generative
Adversarial Networks (GANs) and Diffusion Models (DMs), and implement a novel
Brownian Bridge Diffusion Model for H&E-HER2 translation. Our findings indicate
that, overall, GANs perform better than DMs, with only the BBDM achieving
comparable results. Furthermore, we emphasize the importance of data alignment,
as all models trained on HER2match produced vastly improved visuals compared to
the widely used consecutive-slide BCI dataset. This research provides a new
high-quality dataset ([available upon publication acceptance]), improving both
model training and evaluation. In addition, our comparison of frameworks offers
valuable guidance for researchers working on the topic.

</details>


### [262] [ShowFlow: From Robust Single Concept to Condition-Free Multi-Concept Generation](https://arxiv.org/abs/2506.18493)
*Trong-Vu Hoang,Quang-Binh Nguyen,Thanh-Toan Do,Tam V. Nguyen,Minh-Triet Tran,Trung-Nghia Le*

Main category: cs.CV

TL;DR: ShowFlow是一个解决单概念和多概念图像生成挑战的框架，包括ShowFlow-S和ShowFlow-M，分别通过KronA-WED适配器和SAMA模块提升生成效果。


<details>
  <summary>Details</summary>
Motivation: 解决图像生成中身份保持和提示对齐的挑战，特别是在多概念场景下避免身份丢失和概念遗漏。

Method: ShowFlow-S使用KronA-WED适配器和解耦学习方法；ShowFlow-M重用ShowFlow-S模型，加入SAMA和布局一致性策略。

Result: 实验和用户研究验证了ShowFlow的有效性，展示了在广告和虚拟试衣等实际应用中的潜力。

Conclusion: ShowFlow通过创新的适配器和模块设计，显著提升了单概念和多概念图像生成的质量和可控性。

Abstract: Customizing image generation remains a core challenge in controllable image
synthesis. For single-concept generation, maintaining both identity
preservation and prompt alignment is challenging. In multi-concept scenarios,
relying solely on a prompt without additional conditions like layout boxes or
semantic masks, often leads to identity loss and concept omission. In this
paper, we introduce ShowFlow, a comprehensive framework designed to tackle
these challenges. We propose ShowFlow-S for single-concept image generation,
and ShowFlow-M for handling multiple concepts. ShowFlow-S introduces a
KronA-WED adapter, which integrates a Kronecker adapter with weight and
embedding decomposition, and employs a disentangled learning approach with a
novel attention regularization objective to enhance single-concept generation.
Building on this foundation, ShowFlow-M directly reuses the learned models from
ShowFlow-S to support multi-concept generation without extra conditions,
incorporating a Subject-Adaptive Matching Attention (SAMA) and a layout
consistency strategy as the plug-and-play module. Extensive experiments and
user studies validate ShowFlow's effectiveness, highlighting its potential in
real-world applications like advertising and virtual dressing.

</details>


### [263] [Biased Teacher, Balanced Student](https://arxiv.org/abs/2506.18496)
*Seonghak Kim*

Main category: cs.CV

TL;DR: 论文提出了一种针对长尾数据分布的知识蒸馏框架LTKD，通过分解KL散度为组间和组内部分，校准教师模型的预测偏差，显著提升了尾类性能。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏在长尾数据分布中表现不佳，因为教师模型对头部类别的预测存在偏差，导致尾类监督不足。

Method: 将标准KD目标分解为组间和组内KL散度，并引入重新平衡的组间损失和均匀的组内损失。

Result: 在多个数据集上，LTKD显著优于现有KD方法，整体准确率和尾类性能均有提升。

Conclusion: LTKD能够有效从有偏见的教师模型中转移知识，适用于资源受限和不平衡的实际场景。

Abstract: Knowledge Distillation (KD) is a widely adopted model compression technique
where a compact student model learns from the output of a larger, pre-trained
teacher. While effective in balanced settings, conventional KD suffers
significantly when applied to long-tailed data distributions, as the teacher
model tends to be biased toward head classes and provides limited supervision
for tail classes. In this paper, we propose Long-Tailed Knowledge Distillation
(LTKD), a novel framework tailored for class-imbalanced scenarios. We begin by
reformulating the standard KD objective into two components: inter-group and
intra-group Kullback-Leibler (KL) divergence, corresponding to the prediction
distributions across and within class groups (head, medium, tail),
respectively. This decomposition allows us to identify and quantify the sources
of teacher bias. To address them, we introduce (1) a rebalanced inter-group
loss that calibrates the teacher's group-level predictions and (2) a uniform
intra-group loss that ensures equal contribution from all groups during
distillation. Extensive experiments on CIFAR-100-LT, TinyImageNet-LT, and
ImageNet-LT show that LTKD consistently outperforms existing KD methods,
achieving significant gains in both overall accuracy and tail-class
performance. Our results demonstrate that LTKD enables effective knowledge
transfer even from biased teachers, making it a strong candidate for real-world
deployment in resource-constrained and imbalanced settings.

</details>


### [264] [Generalizing Vision-Language Models to Novel Domains: A Comprehensive Survey](https://arxiv.org/abs/2506.18504)
*Xinyao Li,Jingjing Li,Fengling Li,Lei Zhu,Yang Yang,Heng Tao Shen*

Main category: cs.CV

TL;DR: 综述探讨了视觉语言模型（VLM）的泛化方法，包括提示、参数和特征三种类型，并比较了它们在特定任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管VLM在零样本任务中表现优异，但在领域特定任务中性能下降，因此需要研究如何泛化其知识。

Method: 通过分类为提示、参数和特征三种方法，总结了VLM的泛化技术。

Result: 综述比较了不同方法在流行基准上的性能，并讨论了VLM与多模态大语言模型（MLLM）的关系。

Conclusion: 该研究为多模态研究的当前和未来提供了清晰的视角。

Abstract: Recently, vision-language pretraining has emerged as a transformative
technique that integrates the strengths of both visual and textual modalities,
resulting in powerful vision-language models (VLMs). Leveraging web-scale
pretraining data, these models exhibit strong zero-shot capabilities. However,
their performance often deteriorates when confronted with domain-specific or
specialized generalization tasks. To address this, a growing body of research
focuses on transferring or generalizing the rich knowledge embedded in VLMs to
various downstream applications. This survey aims to comprehensively summarize
the generalization settings, methodologies, benchmarking and results in VLM
literatures. Delving into the typical VLM structures, current literatures are
categorized into prompt-based, parameter-based and feature-based methods
according to the transferred modules. The differences and characteristics in
each category are furthered summarized and discussed by revisiting the typical
transfer learning (TL) settings, providing novel interpretations for TL in the
era of VLMs. Popular benchmarks for VLM generalization are further introduced
with thorough performance comparisons among the reviewed methods. Following the
advances in large-scale generalizable pretraining, this survey also discusses
the relations and differences between VLMs and up-to-date multimodal large
language models (MLLM), e.g., DeepSeek-VL. By systematically reviewing the
surging literatures in vision-language research from a novel and practical
generalization prospective, this survey contributes to a clear landscape of
current and future multimodal researches.

</details>


### [265] [MedTVT-R1: A Multimodal LLM Empowering Medical Reasoning and Diagnosis](https://arxiv.org/abs/2506.18512)
*Yuting Zhang,Kaishen Yuan,Hao Lu,Yutao Yue,Jintai Chen,Kaishun Wu*

Main category: cs.CV

TL;DR: 提出MedTVT-R1，一种多模态大语言模型框架，用于整合临床多模态数据以实现多疾病诊断，并通过GRPO强化微调优化诊断推理。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法依赖单模态数据无法全面理解复杂疾病的问题，提升多疾病诊断的准确性和可解释性。

Method: 构建MedTVT-QA数据集，设计模态感知层捕捉模态间依赖关系，采用GRPO强化微调和Jaccard奖励函数优化推理。

Result: 实验证明MedTVT-R1在多模态特征利用和多疾病诊断方面表现优越，具有临床应用潜力。

Conclusion: MedTVT-R1为多疾病诊断提供了高效且可解释的解决方案，数据集和代码已开源。

Abstract: Accurate and interpretable multi-disease diagnosis remains a critical
challenge in medical research, particularly when leveraging heterogeneous
multimodal medical data. Current approaches often rely on single-modal data,
limiting their ability to comprehensively understand complex diseases. To
address this, we propose MedTVT-R1, a novel Multimodal Large Language Model
(MLLM) framework designed to integrate clinical multimodal data for reasoning
and diagnosing multiple diseases. We construct MedTVT-QA, a curated instruction
dataset that provides question-answer pairs for physiological-level
interpretations and disease-level diagnoses with a Chain of Evidence approach.
MedTVT-R1 incorporates a modality perception layer to capture inter-modal
dependencies and adaptively weight modality contributions. Additionally, we
employ Group Relative Policy Optimization (GRPO)-based Reinforcement
Fine-Tuning with a Jaccard Reward function to enhance diagnostic reasoning.
Experimental results demonstrate MedTVT-R1's superiority in multimodal feature
utilization and multi-disease diagnosis, offering significant potential for
clinical applications such as diagnostic report generation and comorbidity
reasoning. The dataset and code are available at
https://github.com/keke-nice/MedTVT-R1.

</details>


### [266] [Enhancing Image Restoration Transformer via Adaptive Translation Equivariance](https://arxiv.org/abs/2506.18520)
*JiaKui Hu,Zhengjian Yao,Lujia Jin,Hangzhou He,Yanye Lu*

Main category: cs.CV

TL;DR: 论文提出了一种名为TEAFormer的翻译等变性自适应Transformer，通过滑动索引和组件堆叠策略解决现代Transformer在图像修复中破坏翻译等变性的问题，并在多个任务中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现代Transformer中的注意力机制破坏了图像修复中的翻译等变性，影响了训练收敛和泛化能力。

Method: 提出滑动索引和组件堆叠策略，并设计自适应滑动索引机制，结合全局聚合的键值对，构建TEAFormer网络。

Result: TEAFormer在多种图像修复任务中表现出色，验证了其有效性、训练收敛性和泛化能力。

Conclusion: TEAFormer通过自适应滑动索引和组件堆叠策略，成功解决了翻译等变性问题，并在性能上优于现有方法。

Abstract: Translation equivariance is a fundamental inductive bias in image
restoration, ensuring that translated inputs produce translated outputs.
Attention mechanisms in modern restoration transformers undermine this
property, adversely impacting both training convergence and generalization. To
alleviate this issue, we propose two key strategies for incorporating
translation equivariance: slide indexing and component stacking. Slide indexing
maintains operator responses at fixed positions, with sliding window attention
being a notable example, while component stacking enables the arrangement of
translation-equivariant operators in parallel or sequentially, thereby building
complex architectures while preserving translation equivariance. However, these
strategies still create a dilemma in model design between the high
computational cost of self-attention and the fixed receptive field associated
with sliding window attention. To address this, we develop an adaptive sliding
indexing mechanism to efficiently select key-value pairs for each query, which
are then concatenated in parallel with globally aggregated key-value pairs. The
designed network, called the Translation Equivariance Adaptive Transformer
(TEAFormer), is assessed across a variety of image restoration tasks. The
results highlight its superiority in terms of effectiveness, training
convergence, and generalization.

</details>


### [267] [Multi-Scale Representation of Follicular Lymphoma Pathology Images in a Single Hyperbolic Space](https://arxiv.org/abs/2506.18523)
*Kei Taguchi,Kazumasa Ohara,Tatsuya Yokota,Hiroaki Miyoshi,Noriaki Hashimoto,Ichiro Takeuchi,Hidekata Hontani*

Main category: cs.CV

TL;DR: 提出了一种在双曲空间中表示恶性淋巴瘤病理图像的方法，通过自监督学习捕捉多尺度形态变化。


<details>
  <summary>Details</summary>
Motivation: 为了捕捉疾病进展中跨尺度的形态变化，需要一种能够统一表示高分辨率细胞核和低分辨率组织图像的方法。

Method: 使用自监督学习在双曲空间（Poincaré球）中嵌入组织和细胞核图像，基于包含关系使它们在特征空间中接近。

Result: 学习到的表示能够同时捕捉疾病状态和细胞类型的变化。

Conclusion: 该方法有效地编码了病理图像的层次结构，为疾病分析提供了新的视角。

Abstract: We propose a method for representing malignant lymphoma pathology images,
from high-resolution cell nuclei to low-resolution tissue images, within a
single hyperbolic space using self-supervised learning. To capture
morphological changes that occur across scales during disease progression, our
approach embeds tissue and corresponding nucleus images close to each other
based on inclusion relationships. Using the Poincar\'e ball as the feature
space enables effective encoding of this hierarchical structure. The learned
representations capture both disease state and cell type variations.

</details>


### [268] [Auto-Regressively Generating Multi-View Consistent Images](https://arxiv.org/abs/2506.18527)
*JiaKui Hu,Yuxiao Yang,Jialun Liu,Jinbo Wu,Chen Zhao,Yanye Lu*

Main category: cs.CV

TL;DR: MV-AR方法通过自回归模型生成多视角图像，解决了视角一致性和多样性条件下的形状与纹理合成问题。


<details>
  <summary>Details</summary>
Motivation: 从人类指令生成多视角图像对3D内容创作至关重要，但保持多视角一致性和多样性条件下的合成是主要挑战。

Method: 采用自回归模型逐步生成多视角图像，引入条件注入模块处理多种提示，并通过渐进式训练策略和数据增强技术优化模型。

Result: 实验表明MV-AR能一致生成多视角图像，性能与领先的扩散模型相当。

Conclusion: MV-AR在多视角图像生成中表现出色，代码和模型将开源。

Abstract: Generating multi-view images from human instructions is crucial for 3D
content creation. The primary challenges involve maintaining consistency across
multiple views and effectively synthesizing shapes and textures under diverse
conditions. In this paper, we propose the Multi-View Auto-Regressive (MV-AR)
method, which leverages an auto-regressive model to progressively generate
consistent multi-view images from arbitrary prompts. Firstly, the
next-token-prediction capability of the AR model significantly enhances its
effectiveness in facilitating progressive multi-view synthesis. When generating
widely-separated views, MV-AR can utilize all its preceding views to extract
effective reference information. Subsequently, we propose a unified model that
accommodates various prompts via architecture designing and training
strategies. To address multiple conditions, we introduce condition injection
modules for text, camera pose, image, and shape. To manage multi-modal
conditions simultaneously, a progressive training strategy is employed. This
strategy initially adopts the text-to-multi-view (t2mv) model as a baseline to
enhance the development of a comprehensive X-to-multi-view (X2mv) model through
the randomly dropping and combining conditions. Finally, to alleviate the
overfitting problem caused by limited high-quality data, we propose the
"Shuffle View" data augmentation technique, thus significantly expanding the
training data by several magnitudes. Experiments demonstrate the performance
and versatility of our MV-AR, which consistently generates consistent
multi-view images across a range of conditions and performs on par with leading
diffusion-based multi-view image generation models. Code and models will be
released at https://github.com/MILab-PKU/MVAR.

</details>


### [269] [A Set-to-Set Distance Measure in Hyperbolic Space](https://arxiv.org/abs/2506.18529)
*Pengxiang Li,Wei Wu,Zhi Gao,Xiaomeng Fan,Peilin Yu,Yuwei Wu,Zhipeng Lu,Yunde Jia,Mehrtash Harandi*

Main category: cs.CV

TL;DR: 提出了一种双曲集合间距离度量（HS2SD），用于计算双曲空间中集合之间的差异，结合全局和局部结构信息。


<details>
  <summary>Details</summary>
Motivation: 许多实际应用需要比较双曲数据点集合，而现有方法未能充分捕捉集合的全局和局部结构信息。

Method: HS2SD通过双曲集合的爱因斯坦中点测地距离（全局结构）和拓扑特征（局部结构）计算距离，使用有限Thue-Morse序列近似拓扑结构。

Result: 在实体匹配、标准图像分类和少样本图像分类任务中，HS2SD优于现有方法。

Conclusion: HS2SD通过建模双曲集合的层次和复杂关系，提供了更精确的距离度量。

Abstract: We propose a hyperbolic set-to-set distance measure for computing
dissimilarity between sets in hyperbolic space. While point-to-point distances
in hyperbolic space effectively capture hierarchical relationships between data
points, many real-world applications require comparing sets of hyperbolic data
points, where the local structure and the global structure of the sets carry
crucial semantic information. The proposed the \underline{h}yperbolic
\underline{s}et-\underline{to}-\underline{s}et \underline{d}istance measure
(HS2SD) integrates both global and local structural information: global
structure through geodesic distances between Einstein midpoints of hyperbolic
sets, and local structure through topological characteristics of the two sets.
To efficiently compute topological differences, we prove that using a finite
Thue-Morse sequence of degree and adjacency matrices can serve as a robust
approximation to capture the topological structure of a set. In this case, by
considering the topological differences, HS2SD provides a more nuanced
understanding of the relationships between two hyperbolic sets. Empirical
evaluation on entity matching, standard image classification, and few-shot
image classification demonstrates that our distance measure outperforms
existing methods by effectively modeling the hierarchical and complex
relationships inherent in hyperbolic sets.

</details>


### [270] [Geometry-aware Distance Measure for Diverse Hierarchical Structures in Hyperbolic Spaces](https://arxiv.org/abs/2506.18533)
*Pengxiang Li,Yuwei Wu,Zhi Gao,Xiaomeng Fan,Wei Wu,Zhipeng Lu,Yunde Jia,Mehrtash Harandi*

Main category: cs.CV

TL;DR: 提出了一种动态适应不同层次结构的双曲空间几何感知距离度量方法，优于固定距离度量的学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有双曲学习方法假设所有数据具有统一的层次结构，但现实中的层次结构多样性使得这一假设过于限制。

Method: 通过为每对数据点生成定制投影和曲率，动态适应层次结构；引入低秩分解和硬对挖掘机制降低计算成本。

Result: 在标准图像分类、层次分类和少样本学习任务中表现优异，少样本学习任务中提升超过5%。

Conclusion: 自适应距离度量能更好地捕捉多样层次结构，可视化显示更清晰的类别边界和原型分离。

Abstract: Learning in hyperbolic spaces has attracted increasing attention due to its
superior ability to model hierarchical structures of data. Most existing
hyperbolic learning methods use fixed distance measures for all data, assuming
a uniform hierarchy across all data points. However, real-world hierarchical
structures exhibit significant diversity, making this assumption overly
restrictive. In this paper, we propose a geometry-aware distance measure in
hyperbolic spaces, which dynamically adapts to varying hierarchical structures.
Our approach derives the distance measure by generating tailored projections
and curvatures for each pair of data points, effectively mapping them to an
appropriate hyperbolic space. We introduce a revised low-rank decomposition
scheme and a hard-pair mining mechanism to mitigate the computational cost of
pair-wise distance computation without compromising accuracy. We present an
upper bound on the low-rank approximation error using Talagrand's concentration
inequality, ensuring theoretical robustness. Extensive experiments on standard
image classification (MNIST, CIFAR-10 and CIFAR-100), hierarchical
classification (5-level CIFAR-100), and few-shot learning tasks (mini-ImageNet,
tiered-ImageNet) demonstrate the effectiveness of our method. Our approach
consistently outperforms learning methods that use fixed distance measures,
with notable improvements on few-shot learning tasks, where it achieves over
5\% gains on mini-ImageNet. The results reveal that adaptive distance measures
better capture diverse hierarchical structures, with visualization showing
clearer class boundaries and improved prototype separation in hyperbolic
spaces.

</details>


### [271] [Normality Prior Guided Multi-Semantic Fusion Network for Unsupervised Image Anomaly Detection](https://arxiv.org/abs/2506.18544)
*Muhao Xu,Xueying Zhou,Xizhan Gao,Weiye Song,Guang Feng,Sijie Niu*

Main category: cs.CV

TL;DR: 提出了一种基于多语义融合网络的无监督异常检测方法，通过引入正常样本的多语义特征来引导异常重建，显著提升了逻辑异常检测的性能。


<details>
  <summary>Details</summary>
Motivation: 逻辑异常检测比结构异常更具挑战性，现有方法因神经网络泛化能力导致异常特征通过低维瓶颈传播，重建结果误导性高。

Method: 使用预训练的视觉语言网络提取正常样本的全局语义，构建可学习的语义代码本存储特征向量，融合多语义特征引导异常重建。

Result: 在MVTec LOCO AD数据集上达到SOTA性能，像素级sPRO提升5.7%，图像级AUROC提升2.6%。

Conclusion: 提出的方法通过多语义特征融合有效解决了逻辑异常检测问题，性能显著优于现有方法。

Abstract: Recently, detecting logical anomalies is becoming a more challenging task
compared to detecting structural ones. Existing encoder decoder based methods
typically compress inputs into low-dimensional bottlenecks on the assumption
that the compression process can effectively suppress the transmission of
logical anomalies to the decoder. However, logical anomalies present a
particular difficulty because, while their local features often resemble normal
semantics, their global semantics deviate significantly from normal patterns.
Thanks to the generalisation capabilities inherent in neural networks, these
abnormal semantic features can propagate through low-dimensional bottlenecks.
This ultimately allows the decoder to reconstruct anomalous images with
misleading fidelity. To tackle the above challenge, we propose a novel
normality prior guided multi-semantic fusion network for unsupervised anomaly
detection. Instead of feeding the compressed bottlenecks to the decoder
directly, we introduce the multi-semantic features of normal samples into the
reconstruction process. To this end, we first extract abstract global semantics
of normal cases by a pre-trained vision-language network, then the learnable
semantic codebooks are constructed to store representative feature vectors of
normal samples by vector quantisation. Finally, the above multi-semantic
features are fused and employed as input to the decoder to guide the
reconstruction of anomalies to approximate normality. Extensive experiments are
conducted to validate the effectiveness of our proposed method, and it achieves
the SOTA performance on the MVTec LOCO AD dataset with improvements of 5.7% in
pixel-sPRO and 2.6% in image-AUROC. The source code is available at
https://github.com/Xmh-L/NPGMF.

</details>


### [272] [Object-aware Sound Source Localization via Audio-Visual Scene Understanding](https://arxiv.org/abs/2506.18557)
*Sung Jin Um,Dongjin Kim,Sangmin Lee,Jung Uk Kim*

Main category: cs.CV

TL;DR: 论文提出了一种基于多模态大语言模型（MLLMs）的声源定位框架，通过生成详细上下文信息区分发声物体和静默物体，并引入两种新损失函数（OCA和ORI），显著提升了复杂场景中的定位性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在复杂场景中（尤其是存在视觉相似静默物体时）难以准确定位发声物体，主要依赖简单的视听对应关系，无法捕捉语义差异。

Method: 利用MLLMs生成详细上下文信息，明确区分发声前景物体和静默背景物体，并设计OCA和ORI两种损失函数进行优化。

Result: 在MUSIC和VGGSound数据集上的实验表明，该方法在单源和多源定位场景中均显著优于现有方法。

Conclusion: 提出的框架通过结合详细上下文信息和新型损失函数，有效解决了复杂场景中的声源定位问题。

Abstract: Audio-visual sound source localization task aims to spatially localize
sound-making objects within visual scenes by integrating visual and audio cues.
However, existing methods struggle with accurately localizing sound-making
objects in complex scenes, particularly when visually similar silent objects
coexist. This limitation arises primarily from their reliance on simple
audio-visual correspondence, which does not capture fine-grained semantic
differences between sound-making and silent objects. To address these
challenges, we propose a novel sound source localization framework leveraging
Multimodal Large Language Models (MLLMs) to generate detailed contextual
information that explicitly distinguishes between sound-making foreground
objects and silent background objects. To effectively integrate this detailed
information, we introduce two novel loss functions: Object-aware Contrastive
Alignment (OCA) loss and Object Region Isolation (ORI) loss. Extensive
experimental results on MUSIC and VGGSound datasets demonstrate the
effectiveness of our approach, significantly outperforming existing methods in
both single-source and multi-source localization scenarios. Code and generated
detailed contextual information are available at:
https://github.com/VisualAIKHU/OA-SSL.

</details>


### [273] [VQ-Insight: Teaching VLMs for AI-Generated Video Quality Understanding via Progressive Visual Reinforcement Learning](https://arxiv.org/abs/2506.18564)
*Xuanyu Zhang,Weiqi Li,Shijie Zhao,Junlin Li,Li Zhang,Jian Zhang*

Main category: cs.CV

TL;DR: 论文提出VQ-Insight框架，用于评估AI生成视频的质量，通过渐进式学习和多维度评分奖励提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前AI生成视频质量评估存在泛化性不足、缺乏时间感知、依赖标注数据等问题。

Method: 采用渐进式视频质量学习方案和多维度评分奖励设计。

Result: VQ-Insight在偏好比较、多维度评分和自然视频评分中优于现有方法。

Conclusion: VQ-Insight显著提升了视频生成任务的质量评估效果。

Abstract: Recent advances in AI-generated content (AIGC) have led to the emergence of
powerful text-to-video generation models. Despite these successes, evaluating
the quality of AIGC-generated videos remains challenging due to limited
generalization, lack of temporal awareness, heavy reliance on large-scale
annotated datasets, and the lack of effective interaction with generation
models. Most current approaches rely on supervised finetuning of
vision-language models (VLMs), which often require large-scale annotated
datasets and tend to decouple understanding and generation. To address these
shortcomings, we propose VQ-Insight, a novel reasoning-style VLM framework for
AIGC video quality assessment. Our approach features: (1) a progressive video
quality learning scheme that combines image quality warm-up, general
task-specific temporal learning, and joint optimization with the video
generation model; (2) the design of multi-dimension scoring rewards, preference
comparison rewards, and temporal modeling rewards to enhance both
generalization and specialization in video quality evaluation. Extensive
experiments demonstrate that VQ-Insight consistently outperforms
state-of-the-art baselines in preference comparison, multi-dimension scoring,
and natural video scoring, bringing significant improvements for video
generation tasks.

</details>


### [274] [VisualChef: Generating Visual Aids in Cooking via Mask Inpainting](https://arxiv.org/abs/2506.18569)
*Oleh Kuzyk,Zuoyue Li,Marc Pollefeys,Xi Wang*

Main category: cs.CV

TL;DR: VisualChef是一种生成烹饪场景上下文视觉辅助的方法，通过基于掩码的视觉定位简化对齐，生成动作执行和结果图像，同时保持环境一致性。


<details>
  <summary>Details</summary>
Motivation: 烹饪过程中缺乏一致的视觉指导，现有方法依赖复杂文本对齐和额外标注，VisualChef旨在简化这一过程。

Method: 通过识别动作相关对象并分类，进行针对性修改，同时提出自动化流程提取高质量帧。

Result: 在三个第一人称视频数据集上定量和定性评估，优于现有方法。

Conclusion: VisualChef提供了一种高效生成烹饪视觉辅助的方法，简化了对齐需求并保持环境一致性。

Abstract: Cooking requires not only following instructions but also understanding,
executing, and monitoring each step - a process that can be challenging without
visual guidance. Although recipe images and videos offer helpful cues, they
often lack consistency in focus, tools, and setup. To better support the
cooking process, we introduce VisualChef, a method for generating contextual
visual aids tailored to cooking scenarios. Given an initial frame and a
specified action, VisualChef generates images depicting both the action's
execution and the resulting appearance of the object, while preserving the
initial frame's environment. Previous work aims to integrate knowledge
extracted from large language models by generating detailed textual
descriptions to guide image generation, which requires fine-grained
visual-textual alignment and involves additional annotations. In contrast,
VisualChef simplifies alignment through mask-based visual grounding. Our key
insight is identifying action-relevant objects and classifying them to enable
targeted modifications that reflect the intended action and outcome while
maintaining a consistent environment. In addition, we propose an automated
pipeline to extract high-quality initial, action, and final state frames. We
evaluate VisualChef quantitatively and qualitatively on three egocentric video
datasets and show its improvements over state-of-the-art methods.

</details>


### [275] [2D Triangle Splatting for Direct Differentiable Mesh Training](https://arxiv.org/abs/2506.18575)
*Kaifeng Sheng,Zheng Zhou,Yingliang Peng,Qianwei Wang*

Main category: cs.CV

TL;DR: 提出了一种名为2D Triangle Splatting（2DTS）的新方法，用2D三角形面片替代3D高斯基元，结合紧凑性参数，实现了高保真度的网格重建。


<details>
  <summary>Details</summary>
Motivation: 3D高斯基元在渲染速度和高级渲染效果（如重新光照和阴影渲染）方面仍存在挑战，而网格模型在这些方面表现更好。

Method: 使用2D三角形面片形成离散的类网格结构，同时保留连续体积建模的优势，并引入紧凑性参数直接训练逼真网格。

Result: 实验表明，该方法在高保真度上优于现有高斯基元方法，且重建网格的视觉质量优于现有网格重建方法。

Conclusion: 2DTS方法在渲染效果和网格质量上均表现出色，为3D场景重建提供了新的解决方案。

Abstract: Differentiable rendering with 3D Gaussian primitives has emerged as a
powerful method for reconstructing high-fidelity 3D scenes from multi-view
images. While it offers improvements over NeRF-based methods, this
representation still encounters challenges with rendering speed and advanced
rendering effects, such as relighting and shadow rendering, compared to
mesh-based models. In this paper, we propose 2D Triangle Splatting (2DTS), a
novel method that replaces 3D Gaussian primitives with 2D triangle facelets.
This representation naturally forms a discrete mesh-like structure while
retaining the benefits of continuous volumetric modeling. By incorporating a
compactness parameter into the triangle primitives, we enable direct training
of photorealistic meshes. Our experimental results demonstrate that our
triangle-based method, in its vanilla version (without compactness tuning),
achieves higher fidelity compared to state-of-the-art Gaussian-based methods.
Furthermore, our approach produces reconstructed meshes with superior visual
quality compared to existing mesh reconstruction methods.

</details>


### [276] [Resampling Augmentation for Time Series Contrastive Learning: Application to Remote Sensing](https://arxiv.org/abs/2506.18587)
*Antoine Saget,Baptiste Lafabregue,Antoine Cornuéjols,Pierre Gançarski*

Main category: cs.CV

TL;DR: 论文提出了一种基于重采样的对比自监督学习方法，用于卫星图像时间序列（SITS），通过上采样和提取不重叠子序列生成正样本对，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 由于标记数据稀缺，而大量未标记的卫星图像时间序列数据可用，需要一种有效的自监督学习方法利用这些数据。

Method: 提出了一种新颖的重采样增强策略，通过上采样时间序列并提取不重叠子序列来生成正样本对。

Result: 在多个农业分类基准测试中表现优于传统方法（如抖动、调整大小和掩码），并在S2-Agri100数据集上达到了最先进性能。

Conclusion: 该方法为遥感时间序列提供了一种简单而有效的对比学习增强策略。

Abstract: Given the abundance of unlabeled Satellite Image Time Series (SITS) and the
scarcity of labeled data, contrastive self-supervised pretraining emerges as a
natural tool to leverage this vast quantity of unlabeled data. However,
designing effective data augmentations for contrastive learning remains
challenging for time series. We introduce a novel resampling-based augmentation
strategy that generates positive pairs by upsampling time series and extracting
disjoint subsequences while preserving temporal coverage. We validate our
approach on multiple agricultural classification benchmarks using Sentinel-2
imagery, showing that it outperforms common alternatives such as jittering,
resizing, and masking. Further, we achieve state-of-the-art performance on the
S2-Agri100 dataset without employing spatial information or temporal encodings,
surpassing more complex masked-based SSL frameworks. Our method offers a
simple, yet effective, contrastive learning augmentation for remote sensing
time series.

</details>


### [277] [SpaNN: Detecting Multiple Adversarial Patches on CNNs by Spanning Saliency Thresholds](https://arxiv.org/abs/2506.18591)
*Mauricio Byrd Victorica,György Dán,Henrik Sandberg*

Main category: cs.CV

TL;DR: SpaNN是一种新型对抗攻击检测器，其计算复杂度与对抗补丁数量无关，通过构建二值化特征图集合和聚类来检测攻击，显著优于现有防御方法。


<details>
  <summary>Details</summary>
Motivation: 现有防御方法对多补丁攻击效果不佳或计算效率低下，SpaNN旨在解决这一问题。

Method: 通过应用一组显著性阈值构建二值化特征图集合，并进行聚类，将聚类特征输入分类器进行攻击检测。

Result: 在四个数据集上，SpaNN在目标检测和图像分类任务中分别比现有方法高出11和27个百分点。

Conclusion: SpaNN是一种高效且鲁棒的对抗攻击检测方法，适用于多补丁攻击场景。

Abstract: State-of-the-art convolutional neural network models for object detection and
image classification are vulnerable to physically realizable adversarial
perturbations, such as patch attacks. Existing defenses have focused,
implicitly or explicitly, on single-patch attacks, leaving their sensitivity to
the number of patches as an open question or rendering them computationally
infeasible or inefficient against attacks consisting of multiple patches in the
worst cases. In this work, we propose SpaNN, an attack detector whose
computational complexity is independent of the expected number of adversarial
patches. The key novelty of the proposed detector is that it builds an ensemble
of binarized feature maps by applying a set of saliency thresholds to the
neural activations of the first convolutional layer of the victim model. It
then performs clustering on the ensemble and uses the cluster features as the
input to a classifier for attack detection. Contrary to existing detectors,
SpaNN does not rely on a fixed saliency threshold for identifying adversarial
regions, which makes it robust against white box adversarial attacks. We
evaluate SpaNN on four widely used data sets for object detection and
classification, and our results show that SpaNN outperforms state-of-the-art
defenses by up to 11 and 27 percentage points in the case of object detection
and the case of image classification, respectively. Our code is available at
https://github.com/gerkbyrd/SpaNN.

</details>


### [278] [RDPO: Real Data Preference Optimization for Physics Consistency Video Generation](https://arxiv.org/abs/2506.18655)
*Wenxu Qian,Chaoyue Wang,Hou Peng,Zhiyu Tan,Hao Li,Anxiang Zeng*

Main category: cs.CV

TL;DR: RDPO是一种无需标注的框架，通过从真实视频中提取物理先验，显著提升了生成视频的动作连贯性和物理真实性。


<details>
  <summary>Details</summary>
Motivation: 现有视频生成技术虽视觉质量高，但难以真实再现物理规律，且基于偏好的后训练方法成本高昂。

Method: RDPO通过反向采样真实视频序列自动构建偏好对，并采用多阶段迭代训练提升生成器的物理一致性。

Result: 在多个基准测试和人工评估中，RDPO在动作连贯性和物理真实性方面均有显著提升。

Conclusion: RDPO为视频生成中的物理一致性提供了一种高效且无需标注的解决方案。

Abstract: Video generation techniques have achieved remarkable advancements in visual
quality, yet faithfully reproducing real-world physics remains elusive.
Preference-based model post-training may improve physical consistency, but
requires costly human-annotated datasets or reward models that are not yet
feasible. To address these challenges, we present Real Data Preference
Optimisation (RDPO), an annotation-free framework that distills physical priors
directly from real-world videos. Specifically, the proposed RDPO
reverse-samples real video sequences with a pre-trained generator to
automatically build preference pairs that are statistically distinguishable in
terms of physical correctness. A multi-stage iterative training schedule then
guides the generator to obey physical laws increasingly well. Benefiting from
the dynamic information explored from real videos, our proposed RDPO
significantly improves the action coherence and physical realism of the
generated videos. Evaluations on multiple benchmarks and human evaluations have
demonstrated that RDPO achieves improvements across multiple dimensions. The
source code and demonstration of this paper are available at:
https://wwenxu.github.io/RDPO/

</details>


### [279] [Historical Report Guided Bi-modal Concurrent Learning for Pathology Report Generation](https://arxiv.org/abs/2506.18658)
*Ling Zhang,Boxiang Yun,Qingli Li,Yan Wang*

Main category: cs.CV

TL;DR: BiGen框架通过历史报告引导的双模态并发学习，解决了病理报告生成中的语义内容不足和信息冗余问题，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 解决病理报告生成中视觉特征缺乏语义内容和WSI信息冗余的挑战。

Method: 提出BiGen框架，包括知识检索机制和双模态并发学习策略，通过多模态解码器生成报告。

Result: 在PathText数据集上，NLP指标相对提升7.4%，Her-2预测分类指标提升19.1%。

Conclusion: BiGen能有效提供语义内容并抑制冗余，验证了模块的必要性。

Abstract: Automated pathology report generation from Whole Slide Images (WSIs) faces
two key challenges: (1) lack of semantic content in visual features and (2)
inherent information redundancy in WSIs. To address these issues, we propose a
novel Historical Report Guided \textbf{Bi}-modal Concurrent Learning Framework
for Pathology Report \textbf{Gen}eration (BiGen) emulating pathologists'
diagnostic reasoning, consisting of: (1) A knowledge retrieval mechanism to
provide rich semantic content, which retrieves WSI-relevant knowledge from
pre-built medical knowledge bank by matching high-attention patches and (2) A
bi-modal concurrent learning strategy instantiated via a learnable visual token
and a learnable textual token to dynamically extract key visual features and
retrieved knowledge, where weight-shared layers enable cross-modal alignment
between visual features and knowledge features. Our multi-modal decoder
integrates both modals for comprehensive diagnostic reports generation.
Experiments on the PathText (BRCA) dataset demonstrate our framework's
superiority, achieving state-of-the-art performance with 7.4\% relative
improvement in NLP metrics and 19.1\% enhancement in classification metrics for
Her-2 prediction versus existing methods. Ablation studies validate the
necessity of our proposed modules, highlighting our method's ability to provide
WSI-relevant rich semantic content and suppress information redundancy in WSIs.
Code is publicly available at https://github.com/DeepMed-Lab-ECNU/BiGen.

</details>


### [280] [Benchmarking histopathology foundation models in a multi-center dataset for skin cancer subtyping](https://arxiv.org/abs/2506.18668)
*Pablo Meseguer,Rocío del Amor,Valery Naranjo*

Main category: cs.CV

TL;DR: 该论文提出了一种新的基准测试，用于评估病理学基础模型在MIL分类框架中的表现，并引入了一种新指标FM-SI来衡量模型对分布变化的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 由于病理学基础模型的多样性，需要设计实际挑战来评估其有效性，尤其是在多实例学习框架下的表现。

Method: 利用AI4SkIN数据集，设计了一个基准测试，评估基础模型作为特征提取器的性能，并提出了FM-SI指标。

Result: 实验表明，提取较少偏差的特征能提升分类性能，特别是在基于相似性的MIL分类器中。

Conclusion: 该研究为病理学基础模型的评估提供了新方法，并展示了其在MIL框架中的潜力。

Abstract: Pretraining on large-scale, in-domain datasets grants histopathology
foundation models (FM) the ability to learn task-agnostic data representations,
enhancing transfer learning on downstream tasks. In computational pathology,
automated whole slide image analysis requires multiple instance learning (MIL)
frameworks due to the gigapixel scale of the slides. The diversity among
histopathology FMs has highlighted the need to design real-world challenges for
evaluating their effectiveness. To bridge this gap, our work presents a novel
benchmark for evaluating histopathology FMs as patch-level feature extractors
within a MIL classification framework. For that purpose, we leverage the
AI4SkIN dataset, a multi-center cohort encompassing slides with challenging
cutaneous spindle cell neoplasm subtypes. We also define the Foundation Model -
Silhouette Index (FM-SI), a novel metric to measure model consistency against
distribution shifts. Our experimentation shows that extracting less biased
features enhances classification performance, especially in similarity-based
MIL classifiers.

</details>


### [281] [MedSeg-R: Medical Image Segmentation with Clinical Reasoning](https://arxiv.org/abs/2506.18669)
*Hao Shao,Qibin Hou*

Main category: cs.CV

TL;DR: MedSeg-R是一个轻量级双阶段框架，通过结合医学报告的语义先验和SAM主干网络，显著提升了小病灶分割的敏感性和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分割中由于解剖结构重叠、边界模糊以及前景与背景类别不平衡导致的小病灶分割困难问题。

Method: 提出MedSeg-R框架，分为认知阶段（解析医学报告生成语义先验）和感知阶段（通过空间注意力、动态卷积和可变形采样调制SAM主干）。

Result: 在重叠和模糊结构的分割任务中，MedSeg-R显著提升了Dice分数，表现出对小病灶的高敏感性。

Conclusion: MedSeg-R通过嵌入语义先验和精细调制，有效解决了医学图像分割中的挑战，具有与SAM系统的兼容性。

Abstract: Medical image segmentation is challenging due to overlapping anatomies with
ambiguous boundaries and a severe imbalance between the foreground and
background classes, which particularly affects the delineation of small
lesions. Existing methods, including encoder-decoder networks and prompt-driven
variants of the Segment Anything Model (SAM), rely heavily on local cues or
user prompts and lack integrated semantic priors, thus failing to generalize
well to low-contrast or overlapping targets. To address these issues, we
propose MedSeg-R, a lightweight, dual-stage framework inspired by inspired by
clinical reasoning. Its cognitive stage interprets medical report into
structured semantic priors (location, texture, shape), which are fused via
transformer block. In the perceptual stage, these priors modulate the SAM
backbone: spatial attention highlights likely lesion regions, dynamic
convolution adapts feature filters to expected textures, and deformable
sampling refines spatial support. By embedding this fine-grained guidance
early, MedSeg-R disentangles inter-class confusion and amplifies minority-class
cues, greatly improving sensitivity to small lesions. In challenging
benchmarks, MedSeg-R produces large Dice improvements in overlapping and
ambiguous structures, demonstrating plug-and-play compatibility with SAM-based
systems.

</details>


### [282] [Reconstructing Tornadoes in 3D with Gaussian Splatting](https://arxiv.org/abs/2506.18677)
*Adam Yang,Nadula Kadawedduwa,Tianfu Wang,Maria Molina,Christopher Metzler*

Main category: cs.CV

TL;DR: 本文提出了一种基于实验室的小型龙卷风多视角数据集，并利用3D高斯泼溅技术成功重建了其3D结构。


<details>
  <summary>Details</summary>
Motivation: 准确重建龙卷风的3D结构对理解和预防其破坏性至关重要，但目前缺乏用于开发和验证相关工具的受控数据集。

Method: 捕获并发布了一个实验室小型龙卷风的多视角数据集，使用3D高斯泼溅（3DGS）技术进行重建。

Result: 成功重建并可视化了龙卷风的3D结构。

Conclusion: 该数据集和3DGS技术为龙卷风3D重建提供了有效工具。

Abstract: Accurately reconstructing the 3D structure of tornadoes is critically
important for understanding and preparing for this highly destructive weather
phenomenon. While modern 3D scene reconstruction techniques, such as 3D
Gaussian splatting (3DGS), could provide a valuable tool for reconstructing the
3D structure of tornados, at present we are critically lacking a controlled
tornado dataset with which to develop and validate these tools. In this work we
capture and release a novel multiview dataset of a small lab-based tornado. We
demonstrate one can effectively reconstruct and visualize the 3D structure of
this tornado using 3DGS.

</details>


### [283] [MCN-SLAM: Multi-Agent Collaborative Neural SLAM with Hybrid Implicit Neural Scene Representation](https://arxiv.org/abs/2506.18678)
*Tianchen Deng,Guole Shen,Xun Chen,Shenghai Yuan,Hongming Shen,Guohao Peng,Zhenyu Wu,Jingchuan Wang,Lihua Xie,Danwei Wang,Hesheng Wang,Weidong Chen*

Main category: cs.CV

TL;DR: 提出首个分布式多智能体协作神经SLAM框架，结合混合场景表示、分布式相机跟踪、局部到全局闭环和在线蒸馏，并发布首个真实世界密集SLAM数据集。


<details>
  <summary>Details</summary>
Motivation: 现有隐式SLAM算法局限于单智能体场景，且在大规模场景和长序列中表现不佳，而现有NeRF多智能体SLAM框架无法满足通信带宽限制。

Method: 提出混合场景表示方法（triplane-grid）、分布式相机跟踪、局部到全局闭环和在线蒸馏技术，并发布DES数据集。

Result: 实验表明，该方法在映射、跟踪和通信方面表现优越。

Conclusion: 该方法解决了多智能体SLAM的挑战，并提供了首个真实世界数据集，推动了SLAM和3D重建研究。

Abstract: Neural implicit scene representations have recently shown promising results
in dense visual SLAM. However, existing implicit SLAM algorithms are
constrained to single-agent scenarios, and fall difficulties in large-scale
scenes and long sequences. Existing NeRF-based multi-agent SLAM frameworks
cannot meet the constraints of communication bandwidth. To this end, we propose
the first distributed multi-agent collaborative neural SLAM framework with
hybrid scene representation, distributed camera tracking, intra-to-inter loop
closure, and online distillation for multiple submap fusion. A novel
triplane-grid joint scene representation method is proposed to improve scene
reconstruction. A novel intra-to-inter loop closure method is designed to
achieve local (single-agent) and global (multi-agent) consistency. We also
design a novel online distillation method to fuse the information of different
submaps to achieve global consistency. Furthermore, to the best of our
knowledge, there is no real-world dataset for NeRF-based/GS-based SLAM that
provides both continuous-time trajectories groundtruth and high-accuracy 3D
meshes groundtruth. To this end, we propose the first real-world Dense slam
(DES) dataset covering both single-agent and multi-agent scenarios, ranging
from small rooms to large-scale outdoor scenes, with high-accuracy ground truth
for both 3D mesh and continuous-time camera trajectory. This dataset can
advance the development of the research in both SLAM, 3D reconstruction, and
visual foundation model. Experiments on various datasets demonstrate the
superiority of the proposed method in both mapping, tracking, and
communication. The dataset and code will open-source on
https://github.com/dtc111111/mcnslam.

</details>


### [284] [MARL-MambaContour: Unleashing Multi-Agent Deep Reinforcement Learning for Active Contour Optimization in Medical Image Segmentation](https://arxiv.org/abs/2506.18679)
*Ruicheng Zhang,Yu Sun,Zeyu Zhang,Jinai Li,Xiaofan Liu,Au Hoi Fan,Haowei Guo,Puxin Yan*

Main category: cs.CV

TL;DR: MARL-MambaContour是一种基于多智能体强化学习的医学图像分割框架，通过生成拓扑一致的轮廓解决传统像素方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统像素分割方法缺乏拓扑约束和结构意识，无法适应医学图像中的模糊边缘和复杂形态。

Method: 将每个轮廓点建模为智能体，通过改进的SAC算法和ERAM机制优化调整，结合Mamba策略网络和BCHFM机制提升信息交换。

Result: 在五个医学影像数据集上表现出色，达到最先进性能。

Conclusion: MARL-MambaContour是一种准确且鲁棒的临床分割工具。

Abstract: We introduce MARL-MambaContour, the first contour-based medical image
segmentation framework based on Multi-Agent Reinforcement Learning (MARL). Our
approach reframes segmentation as a multi-agent cooperation task focused on
generate topologically consistent object-level contours, addressing the
limitations of traditional pixel-based methods which could lack topological
constraints and holistic structural awareness of anatomical regions. Each
contour point is modeled as an autonomous agent that iteratively adjusts its
position to align precisely with the target boundary, enabling adaptation to
blurred edges and intricate morphologies common in medical images. This
iterative adjustment process is optimized by a contour-specific Soft
Actor-Critic (SAC) algorithm, further enhanced with the Entropy Regularization
Adjustment Mechanism (ERAM) which dynamically balance agent exploration with
contour smoothness. Furthermore, the framework incorporates a Mamba-based
policy network featuring a novel Bidirectional Cross-attention Hidden-state
Fusion Mechanism (BCHFM). This mechanism mitigates potential memory confusion
limitations associated with long-range modeling in state space models, thereby
facilitating more accurate inter-agent information exchange and informed
decision-making. Extensive experiments on five diverse medical imaging datasets
demonstrate the state-of-the-art performance of MARL-MambaContour, highlighting
its potential as an accurate and robust clinical application.

</details>


### [285] [Multi-Scale Spectral Attention Module-based Hyperspectral Segmentation in Autonomous Driving Scenarios](https://arxiv.org/abs/2506.18682)
*Imad Ali Shah,Jiarong Li,Tim Brophy,Martin Glavin,Edward Jones,Enda Ward,Brian Deegan*

Main category: cs.CV

TL;DR: 论文提出了一种多尺度光谱注意力模块（MSAM），通过结合不同核大小的1D卷积和自适应特征聚合机制，显著提升了高光谱图像（HSI）的语义分割性能。


<details>
  <summary>Details</summary>
Motivation: 高光谱成像（HSI）在自动驾驶（AD）中具有潜力，但其高维数据处理仍具挑战性。

Method: 在UNet的跳跃连接中集成MSAM，使用三种不同核大小的1D卷积和自适应特征聚合。

Result: 在多个HSI数据集上，UNet-MSAM平均提升3.61%的mIoU和3.80%的mF1，计算开销仅增加0.02%参数和0.82% GFLOPS。

Conclusion: 多尺度核组合优于单尺度配置，为AD中的HSI处理提供了有效方法。

Abstract: Recent advances in autonomous driving (AD) have highlighted the potential of
Hyperspectral Imaging (HSI) for enhanced environmental perception, particularly
in challenging weather and lighting conditions. However, efficiently processing
its high-dimensional spectral data remains a significant challenge. This paper
introduces a Multi-scale Spectral Attention Module (MSAM) that enhances
spectral feature extraction through three parallel 1D convolutions with varying
kernel sizes between 1 to 11, coupled with an adaptive feature aggregation
mechanism. By integrating MSAM into UNet's skip connections (UNet-SC), our
proposed UNet-MSAM achieves significant improvements in semantic segmentation
performance across multiple HSI datasets: HyKo-VIS v2, HSI-Drive v2, and
Hyperspectral City v2. Our comprehensive experiments demonstrate that with
minimal computational overhead (on average 0.02% in parameters and 0.82%
GFLOPS), UNet-MSAM consistently outperforms UNet-SC, achieving average
improvements of 3.61% in mean IoU and 3.80% in mF1 across the three datasets.
Through extensive ablation studies, we have established that multi-scale kernel
combinations perform better than single-scale configurations. These findings
demonstrate the potential of HSI processing for AD and provide valuable
insights into designing robust, multi-scale spectral feature extractors for
real-world applications.

</details>


### [286] [SIM-Net: A Multimodal Fusion Network Using Inferred 3D Object Shape Point Clouds from RGB Images for 2D Classification](https://arxiv.org/abs/2506.18683)
*Youcef Sklab,Hanane Ariouat,Eric Chenin,Edi Prifti,Jean-Daniel Zucker*

Main category: cs.CV

TL;DR: SIM-Net是一种新型2D图像分类架构，通过将2D图像转换为3D点云，融合纹理和几何特征，显著提升分类性能，尤其在处理复杂背景和遮挡问题时表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决传统2D图像分类模型在处理数字化植物标本时因背景复杂、遮挡等问题导致的性能下降。

Method: 提出像素到点的转换方法，将2D对象掩码转为3D点云，结合CNN和PointNet编码器，融合纹理与几何特征。

Result: 在植物标本数据集上，SIM-Net比ResNet101准确率提升9.9%，F-score提升12.3%，优于多种先进架构。

Conclusion: 将3D结构推理融入2D图像分类任务能显著提升性能，SIM-Net为复杂场景分类提供有效解决方案。

Abstract: We introduce the Shape-Image Multimodal Network (SIM-Net), a novel 2D image
classification architecture that integrates 3D point cloud representations
inferred directly from RGB images. Our key contribution lies in a
pixel-to-point transformation that converts 2D object masks into 3D point
clouds, enabling the fusion of texture-based and geometric features for
enhanced classification performance. SIM-Net is particularly well-suited for
the classification of digitized herbarium specimens (a task made challenging by
heterogeneous backgrounds), non-plant elements, and occlusions that compromise
conventional image-based models. To address these issues, SIM-Net employs a
segmentation-based preprocessing step to extract object masks prior to 3D point
cloud generation. The architecture comprises a CNN encoder for 2D image
features and a PointNet-based encoder for geometric features, which are fused
into a unified latent space. Experimental evaluations on herbarium datasets
demonstrate that SIM-Net consistently outperforms ResNet101, achieving gains of
up to 9.9% in accuracy and 12.3% in F-score. It also surpasses several
transformer-based state-of-the-art architectures, highlighting the benefits of
incorporating 3D structural reasoning into 2D image classification tasks.

</details>


### [287] [Matrix-Game: Interactive World Foundation Model](https://arxiv.org/abs/2506.18701)
*Yifan Zhang,Chunli Peng,Boyang Wang,Puyi Wang,Qingcheng Zhu,Fei Kang,Biao Jiang,Zedong Gao,Eric Li,Yang Liu,Yahui Zhou*

Main category: cs.CV

TL;DR: Matrix-Game是一个交互式世界基础模型，用于可控游戏世界生成，通过两阶段训练和高质量数据集实现精确控制和高质量视频生成。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够精确控制角色动作和相机移动的游戏世界生成模型，同时保持高视觉质量和时间一致性。

Method: 采用两阶段训练流程：大规模无标签预训练和环境理解，随后进行动作标签训练。使用Matrix-Game-MC数据集（包含大量标注和非标注游戏视频）。

Result: Matrix-Game在视觉质量、时间一致性、动作可控性和物理规则理解方面均优于现有模型，特别是在可控性和物理一致性上表现突出。

Conclusion: Matrix-Game能够生成感知真实且精确可控的视频，为交互式图像到世界生成的研究提供了新基准。

Abstract: We introduce Matrix-Game, an interactive world foundation model for
controllable game world generation. Matrix-Game is trained using a two-stage
pipeline that first performs large-scale unlabeled pretraining for environment
understanding, followed by action-labeled training for interactive video
generation. To support this, we curate Matrix-Game-MC, a comprehensive
Minecraft dataset comprising over 2,700 hours of unlabeled gameplay video clips
and over 1,000 hours of high-quality labeled clips with fine-grained keyboard
and mouse action annotations. Our model adopts a controllable image-to-world
generation paradigm, conditioned on a reference image, motion context, and user
actions. With over 17 billion parameters, Matrix-Game enables precise control
over character actions and camera movements, while maintaining high visual
quality and temporal coherence. To evaluate performance, we develop GameWorld
Score, a unified benchmark measuring visual quality, temporal quality, action
controllability, and physical rule understanding for Minecraft world
generation. Extensive experiments show that Matrix-Game consistently
outperforms prior open-source Minecraft world models (including Oasis and
MineWorld) across all metrics, with particularly strong gains in
controllability and physical consistency. Double-blind human evaluations
further confirm the superiority of Matrix-Game, highlighting its ability to
generate perceptually realistic and precisely controllable videos across
diverse game scenarios. To facilitate future research on interactive
image-to-world generation, we will open-source the Matrix-Game model weights
and the GameWorld Score benchmark at https://github.com/SkyworkAI/Matrix-Game.

</details>


### [288] [Including Semantic Information via Word Embeddings for Skeleton-based Action Recognition](https://arxiv.org/abs/2506.18721)
*Dustin Aganian,Erik Franze,Markus Eisenbach,Horst-Michael Gross*

Main category: cs.CV

TL;DR: 提出了一种基于骨架的动作识别新方法，通过词嵌入编码语义信息，显著提升了分类性能和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统骨架方法在复杂交互中丢失关键点语义信息，限制了其有效性。

Method: 利用词嵌入编码语义信息，替换独热编码，生成语义体积以捕捉关节与物体间的关系。

Result: 在多个装配数据集上实验表明，分类性能显著提升，同时支持不同骨架类型和物体类别。

Conclusion: 语义信息的融入能有效增强动态多样化环境中的骨架动作识别。

Abstract: Effective human action recognition is widely used for cobots in Industry 4.0
to assist in assembly tasks. However, conventional skeleton-based methods often
lose keypoint semantics, limiting their effectiveness in complex interactions.
In this work, we introduce a novel approach to skeleton-based action
recognition that enriches input representations by leveraging word embeddings
to encode semantic information. Our method replaces one-hot encodings with
semantic volumes, enabling the model to capture meaningful relationships
between joints and objects. Through extensive experiments on multiple assembly
datasets, we demonstrate that our approach significantly improves
classification performance, and enhances generalization capabilities by
simultaneously supporting different skeleton types and object classes. Our
findings highlight the potential of incorporating semantic information to
enhance skeleton-based action recognition in dynamic and diverse environments.

</details>


### [289] [Deep CNN Face Matchers Inherently Support Revocable Biometric Templates](https://arxiv.org/abs/2506.18731)
*Aman Bhatta,Michael C. King,Kevin W. Bowyer*

Main category: cs.CV

TL;DR: 论文提出了一种基于深度CNN的可撤销生物特征方案，解决了生物特征被泄露后无法撤销的问题。通过生成多个识别能力相同但模板不兼容的模型，实现了生物特征的撤销与重新注册。同时，对比了ViT和ResNet的适用性。


<details>
  <summary>Details</summary>
Motivation: 解决生物特征被泄露后无法撤销的问题，提供一种可撤销的生物特征认证方案。

Method: 利用深度CNN生成多个识别能力相同但模板不兼容的模型，实现生物特征的撤销与重新注册。

Result: 生成的模型具有相同的识别能力，且模板不兼容，泄露的模板在撤销后失去价值。ViT在此方案中表现不如ResNet。

Conclusion: 深度CNN可有效实现可撤销生物特征方案，ViT在此场景下适用性较差。

Abstract: One common critique of biometric authentication is that if an individual's
biometric is compromised, then the individual has no recourse. The concept of
revocable biometrics was developed to address this concern. A biometric scheme
is revocable if an individual can have their current enrollment in the scheme
revoked, so that the compromised biometric template becomes worthless, and the
individual can re-enroll with a new template that has similar recognition
power. We show that modern deep CNN face matchers inherently allow for a robust
revocable biometric scheme. For a given state-of-the-art deep CNN backbone and
training set, it is possible to generate an unlimited number of distinct face
matcher models that have both (1) equivalent recognition power, and (2)
strongly incompatible biometric templates. The equivalent recognition power
extends to the point of generating impostor and genuine distributions that have
the same shape and placement on the similarity dimension, meaning that the
models can share a similarity threshold for a 1-in-10,000 false match rate. The
biometric templates from different model instances are so strongly incompatible
that the cross-instance similarity score for images of the same person is
typically lower than the same-instance similarity score for images of different
persons. That is, a stolen biometric template that is revoked is of less value
in attempting to match the re-enrolled identity than the average impostor
template. We also explore the feasibility of using a Vision Transformer (ViT)
backbone-based face matcher in the revocable biometric system proposed in this
work and demonstrate that it is less suitable compared to typical ResNet-based
deep CNN backbones.

</details>


### [290] [USVTrack: USV-Based 4D Radar-Camera Tracking Dataset for Autonomous Driving in Inland Waterways](https://arxiv.org/abs/2506.18737)
*Shanliang Yao,Runwei Guan,Yi Ni,Sen Xu,Yong Yue,Xiaohui Zhu,Ryan Wen Liu*

Main category: cs.CV

TL;DR: 论文提出了一种基于4D雷达和单目相机的无人水面车辆（USV）目标跟踪方法，并发布了首个适用于新一代水上交通系统的数据集USVTrack。


<details>
  <summary>Details</summary>
Motivation: 内陆水道目标跟踪对水上交通、观光旅游、环境监测和救援等应用至关重要，但复杂的水上环境增加了跟踪难度。

Method: USV配备了4D雷达、单目相机、GPS和IMU，收集了多样化的跟踪数据，并提出了一种简单的雷达-相机匹配方法（RCM）。

Result: 实验证明RCM能有效提升水上环境中目标跟踪的准确性和可靠性。

Conclusion: USVTrack数据集和RCM方法为水上自动驾驶提供了实用的工具和基准。

Abstract: Object tracking in inland waterways plays a crucial role in safe and
cost-effective applications, including waterborne transportation, sightseeing
tours, environmental monitoring and surface rescue. Our Unmanned Surface
Vehicle (USV), equipped with a 4D radar, a monocular camera, a GPS, and an IMU,
delivers robust tracking capabilities in complex waterborne environments. By
leveraging these sensors, our USV collected comprehensive object tracking data,
which we present as USVTrack, the first 4D radar-camera tracking dataset
tailored for autonomous driving in new generation waterborne transportation
systems. Our USVTrack dataset presents rich scenarios, featuring diverse
various waterways, varying times of day, and multiple weather and lighting
conditions. Moreover, we present a simple but effective radar-camera matching
method, termed RCM, which can be plugged into popular two-stage association
trackers. Experimental results utilizing RCM demonstrate the effectiveness of
the radar-camera matching in improving object tracking accuracy and reliability
for autonomous driving in waterborne environments. The USVTrack dataset is
public on https://usvtrack.github.io.

</details>


### [291] [SWA-SOP: Spatially-aware Window Attention for Semantic Occupancy Prediction in Autonomous Driving](https://arxiv.org/abs/2506.18785)
*Helin Cao,Rafael Materla,Sven Behnke*

Main category: cs.CV

TL;DR: 提出了一种名为空间感知窗口注意力（SWA）的新机制，用于改进语义占用预测（SOP）任务，显著提升了稀疏或遮挡区域的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的SOP方法在注意力计算中缺乏对空间结构的显式建模，导致几何感知能力有限，在稀疏或遮挡区域表现不佳。

Method: 提出SWA机制，将局部空间上下文信息融入注意力计算中。

Result: 在LiDAR和相机两种模态的SOP基准测试中均取得了最先进的性能。

Conclusion: SWA通过显式建模空间结构，显著提升了SOP任务的性能，并展示了跨模态的通用性。

Abstract: Perception systems in autonomous driving rely on sensors such as LiDAR and
cameras to perceive the 3D environment. However, due to occlusions and data
sparsity, these sensors often fail to capture complete information. Semantic
Occupancy Prediction (SOP) addresses this challenge by inferring both occupancy
and semantics of unobserved regions. Existing transformer-based SOP methods
lack explicit modeling of spatial structure in attention computation, resulting
in limited geometric awareness and poor performance in sparse or occluded
areas. To this end, we propose Spatially-aware Window Attention (SWA), a novel
mechanism that incorporates local spatial context into attention. SWA
significantly improves scene completion and achieves state-of-the-art results
on LiDAR-based SOP benchmarks. We further validate its generality by
integrating SWA into a camera-based SOP pipeline, where it also yields
consistent gains across modalities.

</details>


### [292] [3D Arena: An Open Platform for Generative 3D Evaluation](https://arxiv.org/abs/2506.18787)
*Dylan Ebert*

Main category: cs.CV

TL;DR: 3D Arena是一个开放平台，通过大规模人类偏好收集评估图像到3D生成模型，解决了现有指标与人类感知质量不一致的问题。


<details>
  <summary>Details</summary>
Motivation: 当前评估生成3D模型的指标与人类感知质量不一致，缺乏有效的3D结构和感知吸引力评估方法。

Method: 利用3D Arena平台，通过人类偏好收集（成对比较）和ELO排名系统评估模型，并采用统计欺诈检测确保数据真实性。

Result: 收集了123,243票，发现高斯溅射输出比网格模型有16.6 ELO优势，带纹理模型比无纹理模型有144.1 ELO优势。

Conclusion: 3D Arena成为生成3D模型的评估基准，提供了多标准评估和任务导向评估的建议，推动了以人为中心的评估方法。

Abstract: Evaluating Generative 3D models remains challenging due to misalignment
between automated metrics and human perception of quality. Current benchmarks
rely on image-based metrics that ignore 3D structure or geometric measures that
fail to capture perceptual appeal and real-world utility. To address this gap,
we present 3D Arena, an open platform for evaluating image-to-3D generation
models through large-scale human preference collection using pairwise
comparisons.
  Since launching in June 2024, the platform has collected 123,243 votes from
8,096 users across 19 state-of-the-art models, establishing the largest human
preference evaluation for Generative 3D. We contribute the iso3d dataset of 100
evaluation prompts and demonstrate quality control achieving 99.75% user
authenticity through statistical fraud detection. Our ELO-based ranking system
provides reliable model assessment, with the platform becoming an established
evaluation resource.
  Through analysis of this preference data, we present insights into human
preference patterns. Our findings reveal preferences for visual presentation
features, with Gaussian splat outputs achieving a 16.6 ELO advantage over
meshes and textured models receiving a 144.1 ELO advantage over untextured
models. We provide recommendations for improving evaluation methods, including
multi-criteria assessment, task-oriented evaluation, and format-aware
comparison. The platform's community engagement establishes 3D Arena as a
benchmark for the field while advancing understanding of human-centered
evaluation in Generative 3D.

</details>


### [293] [Focus Your Attention: Towards Data-Intuitive Lightweight Vision Transformers](https://arxiv.org/abs/2506.18791)
*Suyash Gaurav,Muhammad Farhan Humayun,Jukka Heikkonen,Jatin Chaudhary*

Main category: cs.CV

TL;DR: 论文提出了一种名为SPPP的技术和LLA模块，通过生成语义丰富的补丁嵌入和动态注意力机制，显著降低了Vision Transformers的计算和内存需求，同时保持了性能。


<details>
  <summary>Details</summary>
Motivation: Vision Transformers在计算和内存资源上的高需求以及任务特定迁移学习的困难限制了其广泛应用。

Method: 提出SPPP技术生成上下文感知的补丁嵌入，并引入LLA模块，通过潜在令牌和动态位置编码优化注意力机制。

Result: 实验表明，该方法显著提高了计算效率，同时性能与现有最优方法相当。

Conclusion: 该方法为边缘部署提供了高效节能的Transformer解决方案。

Abstract: The evolution of Vision Transformers has led to their widespread adaptation
to different domains. Despite large-scale success, there remain significant
challenges including their reliance on extensive computational and memory
resources for pre-training on huge datasets as well as difficulties in
task-specific transfer learning. These limitations coupled with energy
inefficiencies mainly arise due to the computation-intensive self-attention
mechanism. To address these issues, we propose a novel Super-Pixel Based Patch
Pooling (SPPP) technique that generates context-aware, semantically rich, patch
embeddings to effectively reduce the architectural complexity and improve
efficiency. Additionally, we introduce the Light Latent Attention (LLA) module
in our pipeline by integrating latent tokens into the attention mechanism
allowing cross-attention operations to significantly reduce the time and space
complexity of the attention module. By leveraging the data-intuitive patch
embeddings coupled with dynamic positional encodings, our approach adaptively
modulates the cross-attention process to focus on informative regions while
maintaining the global semantic structure. This targeted attention improves
training efficiency and accelerates convergence. Notably, the SPPP module is
lightweight and can be easily integrated into existing transformer
architectures. Extensive experiments demonstrate that our proposed architecture
provides significant improvements in terms of computational efficiency while
achieving comparable results with the state-of-the-art approaches, highlighting
its potential for energy-efficient transformers suitable for edge deployment.
(The code is available on our GitHub repository:
https://github.com/zser092/Focused-Attention-ViT).

</details>


### [294] [ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs](https://arxiv.org/abs/2506.18792)
*Michal Nazarczuk,Sibi Catley-Chandar,Thomas Tanay,Zhensong Zhang,Gregory Slabaugh,Eduardo Pérez-Pellitero*

Main category: cs.CV

TL;DR: ViDAR利用个性化扩散模型生成伪多视角监督信号，通过高斯泼溅表示训练动态新视角合成，解决了单目视频中结构和运动分离的挑战。


<details>
  <summary>Details</summary>
Motivation: 动态新视角合成在单目视频中因结构和运动分离问题及监督稀缺而极具挑战性。

Method: ViDAR结合个性化扩散模型生成伪多视角监督信号，提出扩散感知损失函数和相机位姿优化策略。

Result: 在DyCheck基准测试中，ViDAR在视觉质量和几何一致性上优于现有方法，尤其在动态区域表现突出。

Conclusion: ViDAR通过扩散感知监督和几何对齐，显著提升了动态新视角合成的性能。

Abstract: Dynamic Novel View Synthesis aims to generate photorealistic views of moving
subjects from arbitrary viewpoints. This task is particularly challenging when
relying on monocular video, where disentangling structure from motion is
ill-posed and supervision is scarce. We introduce Video Diffusion-Aware
Reconstruction (ViDAR), a novel 4D reconstruction framework that leverages
personalised diffusion models to synthesise a pseudo multi-view supervision
signal for training a Gaussian splatting representation. By conditioning on
scene-specific features, ViDAR recovers fine-grained appearance details while
mitigating artefacts introduced by monocular ambiguity. To address the
spatio-temporal inconsistency of diffusion-based supervision, we propose a
diffusion-aware loss function and a camera pose optimisation strategy that
aligns synthetic views with the underlying scene geometry. Experiments on
DyCheck, a challenging benchmark with extreme viewpoint variation, show that
ViDAR outperforms all state-of-the-art baselines in visual quality and
geometric consistency. We further highlight ViDAR's strong improvement over
baselines on dynamic regions and provide a new benchmark to compare performance
in reconstructing motion-rich parts of the scene. Project page:
https://vidar-4d.github.io

</details>


### [295] [OC-SOP: Enhancing Vision-Based 3D Semantic Occupancy Prediction by Object-Centric Awareness](https://arxiv.org/abs/2506.18798)
*Helin Cao,Sven Behnke*

Main category: cs.CV

TL;DR: 论文提出了一种基于对象中心的语义占用预测框架（OC-SOP），通过整合检测分支提取的高层对象特征，显著提升了前景对象的预测准确性，并在SemanticKITTI上实现了最优性能。


<details>
  <summary>Details</summary>
Motivation: 自动驾驶感知因环境中的遮挡和不完整场景数据面临挑战，传统相机方法对所有类别平等处理且依赖局部特征，导致前景对象预测效果不佳。

Method: 提出OC-SOP框架，通过检测分支提取对象中心的高层特征，并将其整合到语义占用预测流程中。

Result: 显著提升了前景对象的预测准确性，在SemanticKITTI上实现了所有类别的最优性能。

Conclusion: OC-SOP通过对象中心的方法有效解决了传统方法的局限性，为自动驾驶感知提供了更优的解决方案。

Abstract: Autonomous driving perception faces significant challenges due to occlusions
and incomplete scene data in the environment. To overcome these issues, the
task of semantic occupancy prediction (SOP) is proposed, which aims to jointly
infer both the geometry and semantic labels of a scene from images. However,
conventional camera-based methods typically treat all categories equally and
primarily rely on local features, leading to suboptimal predictions, especially
for dynamic foreground objects. To address this, we propose Object-Centric SOP
(OC-SOP), a framework that integrates high-level object-centric cues extracted
via a detection branch into the semantic occupancy prediction pipeline. This
object-centric integration significantly enhances the prediction accuracy for
foreground objects and achieves state-of-the-art performance among all
categories on SemanticKITTI.

</details>


### [296] [PicoSAM2: Low-Latency Segmentation In-Sensor for Edge Vision Applications](https://arxiv.org/abs/2506.18807)
*Pietro Bonazzi,Nicola Farronato,Stefan Zihlmann,Haotong Qi,Michele Magno*

Main category: cs.CV

TL;DR: PicoSAM2是一款轻量级、可提示的分割模型，专为边缘和传感器内执行优化，适用于实时、隐私敏感的应用。


<details>
  <summary>Details</summary>
Motivation: 满足实时、隐私敏感的智能眼镜和物联网设备对低延迟、本地化分割的需求。

Method: 基于深度可分离U-Net，结合知识蒸馏和定点提示编码，从SAM2学习。

Result: 在COCO和LVIS上分别达到51.9%和44.9% mIoU，量化模型仅1.22MB，在IMX500上运行时间为14.3 ms。

Conclusion: PicoSAM2证明了高效、可提示的分割在传感器内执行的可行性，无需依赖云端或主机处理。

Abstract: Real-time, on-device segmentation is critical for latency-sensitive and
privacy-aware applications like smart glasses and IoT devices. We introduce
PicoSAM2, a lightweight (1.3M parameters, 336M MACs) promptable segmentation
model optimized for edge and in-sensor execution, including the Sony IMX500. It
builds on a depthwise separable U-Net, with knowledge distillation and
fixed-point prompt encoding to learn from the Segment Anything Model 2 (SAM2).
On COCO and LVIS, it achieves 51.9% and 44.9% mIoU, respectively. The quantized
model (1.22MB) runs at 14.3 ms on the IMX500-achieving 86 MACs/cycle, making it
the only model meeting both memory and compute constraints for in-sensor
deployment. Distillation boosts LVIS performance by +3.5% mIoU and +5.1% mAP.
These results demonstrate that efficient, promptable segmentation is feasible
directly on-camera, enabling privacy-preserving vision without cloud or host
processing.

</details>


### [297] [4Real-Video-V2: Fused View-Time Attention and Feedforward Reconstruction for 4D Scene Generation](https://arxiv.org/abs/2506.18839)
*Chaoyang Wang,Ashkan Mirzaei,Vidit Goel,Willi Menapace,Aliaksandr Siarohin,Avalon Vinella,Michael Vasilkovsky,Ivan Skorokhodov,Vladislav Shakhrai,Sergey Korolev,Sergey Tulyakov,Peter Wonka*

Main category: cs.CV

TL;DR: 提出首个4D时空网格框架，结合视频帧和3D高斯粒子，通过前馈架构实现时空注意力融合，提升4D生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有4D视频扩散架构在时空注意力处理上存在局限性，需改进以提升视觉质量和重建能力。

Method: 采用融合架构，单层内实现时空注意力，引入稀疏注意力模式；扩展3D重建算法，加入高斯头、相机令牌替换和动态层。

Result: 在4D生成任务中达到新SOTA，视觉质量和重建能力均显著提升。

Conclusion: 提出的框架有效解决了现有方法的不足，为4D生成任务提供了更优的解决方案。

Abstract: We propose the first framework capable of computing a 4D spatio-temporal grid
of video frames and 3D Gaussian particles for each time step using a
feed-forward architecture. Our architecture has two main components, a 4D video
model and a 4D reconstruction model. In the first part, we analyze current 4D
video diffusion architectures that perform spatial and temporal attention
either sequentially or in parallel within a two-stream design. We highlight the
limitations of existing approaches and introduce a novel fused architecture
that performs spatial and temporal attention within a single layer. The key to
our method is a sparse attention pattern, where tokens attend to others in the
same frame, at the same timestamp, or from the same viewpoint. In the second
part, we extend existing 3D reconstruction algorithms by introducing a Gaussian
head, a camera token replacement algorithm, and additional dynamic layers and
training. Overall, we establish a new state of the art for 4D generation,
improving both visual quality and reconstruction capability.

</details>


### [298] [Phantom-Data : Towards a General Subject-Consistent Video Generation Dataset](https://arxiv.org/abs/2506.18851)
*Zhuowei Chen,Bingchuan Li,Tianxiang Ma,Lijie Liu,Mingcong Liu,Yi Zhang,Gen Li,Xinghui Li,Siyu Zhou,Qian He,Xinglong Wu*

Main category: cs.CV

TL;DR: 论文提出了Phantom-Data数据集，解决了现有主题到视频生成模型因训练范式导致的文本指令跟随问题。


<details>
  <summary>Details</summary>
Motivation: 现有模型在遵循文本指令时存在‘复制粘贴’问题，原因是训练数据中主题身份与背景和上下文属性被绑定。

Method: 通过三阶段流程构建跨对主题到视频一致性数据集：主题检测、跨上下文主题检索和身份验证。

Result: 实验表明，使用Phantom-Data训练显著提升了提示对齐和视觉质量，同时保持了身份一致性。

Conclusion: Phantom-Data为跨对主题到视频生成提供了有效的解决方案。

Abstract: Subject-to-video generation has witnessed substantial progress in recent
years. However, existing models still face significant challenges in faithfully
following textual instructions. This limitation, commonly known as the
copy-paste problem, arises from the widely used in-pair training paradigm. This
approach inherently entangles subject identity with background and contextual
attributes by sampling reference images from the same scene as the target
video. To address this issue, we introduce \textbf{Phantom-Data, the first
general-purpose cross-pair subject-to-video consistency dataset}, containing
approximately one million identity-consistent pairs across diverse categories.
Our dataset is constructed via a three-stage pipeline: (1) a general and
input-aligned subject detection module, (2) large-scale cross-context subject
retrieval from more than 53 million videos and 3 billion images, and (3)
prior-guided identity verification to ensure visual consistency under
contextual variation. Comprehensive experiments show that training with
Phantom-Data significantly improves prompt alignment and visual quality while
preserving identity consistency on par with in-pair baselines.

</details>


### [299] [RAG-6DPose: Retrieval-Augmented 6D Pose Estimation via Leveraging CAD as Knowledge Base](https://arxiv.org/abs/2506.18856)
*Kuanning Wang,Yuqian Fu,Tianyu Wang,Yanwei Fu,Longfei Liang,Yu-Gang Jiang,Xiangyang Xue*

Main category: cs.CV

TL;DR: RAG-6DPose提出了一种基于检索增强的6D姿态估计方法，结合视觉和几何线索，利用3D CAD模型作为知识库，显著提升了姿态估计的精度和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 6D姿态估计是机器人操作的关键，但现有方法在处理遮挡和新视角时表现不佳。RAG-6DPose旨在通过检索增强的方法提升姿态估计的准确性和鲁棒性。

Method: 方法分为三阶段：1) 构建多模态CAD知识库，提取多视角渲染图像的2D视觉特征和3D点云；2) 通过ReSPC模块从查询图像中检索相关CAD特征；3) 利用检索到的CAD信息通过解码器优化姿态预测。

Result: 在标准基准测试和实际机器人任务中，RAG-6DPose表现出色，尤其在处理遮挡和新视角时效果显著。

Conclusion: RAG-6DPose通过检索增强的方法有效提升了6D姿态估计的性能，适用于复杂场景下的机器人操作。

Abstract: Accurate 6D pose estimation is key for robotic manipulation, enabling precise
object localization for tasks like grasping. We present RAG-6DPose, a
retrieval-augmented approach that leverages 3D CAD models as a knowledge base
by integrating both visual and geometric cues. Our RAG-6DPose roughly contains
three stages: 1) Building a Multi-Modal CAD Knowledge Base by extracting 2D
visual features from multi-view CAD rendered images and also attaching 3D
points; 2) Retrieving relevant CAD features from the knowledge base based on
the current query image via our ReSPC module; and 3) Incorporating retrieved
CAD information to refine pose predictions via retrieval-augmented decoding.
Experimental results on standard benchmarks and real-world robotic tasks
demonstrate the effectiveness and robustness of our approach, particularly in
handling occlusions and novel viewpoints. Supplementary material is available
on our project website: https://sressers.github.io/RAG-6DPose .

</details>


### [300] [TAMMs: Temporal-Aware Multimodal Model for Satellite Image Change Understanding and Forecasting](https://arxiv.org/abs/2506.18862)
*Zhongbin Guo,Yuhao Wang,Ping Jian,Xinyue Chen,Wei Peng,Ertai E*

Main category: cs.CV

TL;DR: TAMMs模型通过轻量级时序模块和语义融合控制注入机制，提升了多模态大语言模型在卫星图像时序分析和未来场景生成任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决现有MLLMs在卫星图像时序分析中细粒度时空推理的不足，探索其在复杂多模态动态建模中的潜力。

Method: 提出TAMMs模型，结合轻量级时序模块和语义融合控制注入机制（SFCI），增强MLLMs的时空推理能力。

Result: 实验表明TAMMs在时序变化理解和未来图像预测任务中优于基线MLLMs。

Conclusion: TAMMs通过精心设计的时序推理和语义融合，释放了MLLMs在时空理解中的潜力。

Abstract: Satellite image time-series analysis demands fine-grained spatial-temporal
reasoning, which remains a challenge for existing multimodal large language
models (MLLMs). In this work, we study the capabilities of MLLMs on a novel
task that jointly targets temporal change understanding and future scene
generation, aiming to assess their potential for modeling complex multimodal
dynamics over time. We propose TAMMs, a Temporal-Aware Multimodal Model for
satellite image change understanding and forecasting, which enhances frozen
MLLMs with lightweight temporal modules for structured sequence encoding and
contextual prompting. To guide future image generation, TAMMs introduces a
Semantic-Fused Control Injection (SFCI) mechanism that adaptively combines
high-level semantic reasoning and structural priors within an enhanced
ControlNet. This dual-path conditioning enables temporally consistent and
semantically grounded image synthesis. Experiments demonstrate that TAMMs
outperforms strong MLLM baselines in both temporal change understanding and
future image forecasting tasks, highlighting how carefully designed temporal
reasoning and semantic fusion can unlock the full potential of MLLMs for
spatio-temporal understanding.

</details>


### [301] [OmniAvatar: Efficient Audio-Driven Avatar Video Generation with Adaptive Body Animation](https://arxiv.org/abs/2506.18866)
*Qijun Gan,Ruizi Yang,Jianke Zhu,Shaofei Xue,Steven Hoi*

Main category: cs.CV

TL;DR: OmniAvatar是一种创新的音频驱动全身视频生成模型，通过像素级多层级音频嵌入策略和LoRA训练方法，提升了唇部同步精度和自然动作，支持精确的文本控制。


<details>
  <summary>Details</summary>
Motivation: 现有音频驱动人体动画方法主要关注面部动作，难以生成自然同步和流畅的全身动画，且缺乏精细提示控制。

Method: 采用像素级多层级音频嵌入策略捕捉潜在空间中的音频特征，结合LoRA训练方法实现音频特征与基础模型的融合。

Result: 实验表明，OmniAvatar在面部和半身视频生成中优于现有模型，支持多种场景的精确文本控制。

Conclusion: OmniAvatar通过改进的音频嵌入和训练方法，显著提升了音频驱动全身动画的质量和控制能力。

Abstract: Significant progress has been made in audio-driven human animation, while
most existing methods focus mainly on facial movements, limiting their ability
to create full-body animations with natural synchronization and fluidity. They
also struggle with precise prompt control for fine-grained generation. To
tackle these challenges, we introduce OmniAvatar, an innovative audio-driven
full-body video generation model that enhances human animation with improved
lip-sync accuracy and natural movements. OmniAvatar introduces a pixel-wise
multi-hierarchical audio embedding strategy to better capture audio features in
the latent space, enhancing lip-syncing across diverse scenes. To preserve the
capability for prompt-driven control of foundation models while effectively
incorporating audio features, we employ a LoRA-based training approach.
Extensive experiments show that OmniAvatar surpasses existing models in both
facial and semi-body video generation, offering precise text-based control for
creating videos in various domains, such as podcasts, human interactions,
dynamic scenes, and singing. Our project page is
https://omni-avatar.github.io/.

</details>


### [302] [Let Your Video Listen to Your Music!](https://arxiv.org/abs/2506.18881)
*Xinyu Zhang,Dong Gong,Zicheng Duan,Anton van den Hengel,Lingqiao Liu*

Main category: cs.CV

TL;DR: MVAA框架自动将视频节奏与音乐节拍对齐，保留原始内容，通过关键帧对齐和节奏感知修复实现高效编辑。


<details>
  <summary>Details</summary>
Motivation: 视频与音乐节奏对齐是多媒体制作的实际需求，现有方法依赖手动或启发式编辑，缺乏灵活性。

Method: MVAA采用两步法：关键帧对齐音乐节拍，再用扩散模型生成连贯中间帧。采用预训练和快速微调策略。

Result: 实验表明，MVAA能高质量对齐节拍并保持视觉流畅性，10分钟内完成适配。

Conclusion: MVAA提供了一种高效、灵活的视频与音乐自动对齐方法，适用于多种多媒体场景。

Abstract: Aligning the rhythm of visual motion in a video with a given music track is a
practical need in multimedia production, yet remains an underexplored task in
autonomous video editing. Effective alignment between motion and musical beats
enhances viewer engagement and visual appeal, particularly in music videos,
promotional content, and cinematic editing. Existing methods typically depend
on labor-intensive manual cutting, speed adjustments, or heuristic-based
editing techniques to achieve synchronization. While some generative models
handle joint video and music generation, they often entangle the two
modalities, limiting flexibility in aligning video to music beats while
preserving the full visual content. In this paper, we propose a novel and
efficient framework, termed MVAA (Music-Video Auto-Alignment), that
automatically edits video to align with the rhythm of a given music track while
preserving the original visual content. To enhance flexibility, we modularize
the task into a two-step process in our MVAA: aligning motion keyframes with
audio beats, followed by rhythm-aware video inpainting. Specifically, we first
insert keyframes at timestamps aligned with musical beats, then use a
frame-conditioned diffusion model to generate coherent intermediate frames,
preserving the original video's semantic content. Since comprehensive test-time
training can be time-consuming, we adopt a two-stage strategy: pretraining the
inpainting module on a small video set to learn general motion priors, followed
by rapid inference-time fine-tuning for video-specific adaptation. This hybrid
approach enables adaptation within 10 minutes with one epoch on a single NVIDIA
4090 GPU using CogVideoX-5b-I2V as the backbone. Extensive experiments show
that our approach can achieve high-quality beat alignment and visual
smoothness.

</details>


### [303] [Light of Normals: Unified Feature Representation for Universal Photometric Stereo](https://arxiv.org/abs/2506.18882)
*Hong Li,Houyuan Chen,Chongjie Ye,Zhaoxi Chen,Bohan Li,Shaocong Xu,Xianda Guo,Xuhui Liu,Yikai Wang,Baochang Zhang,Satoshi Ikehata,Boxin Shi,Anyi Rao,Hao Zhao*

Main category: cs.CV

TL;DR: 通用光度立体（PS）旨在从任意光照条件下的物体中恢复高质量表面法线，无需依赖特定光照模型。尽管有SDM-UniPS和Uni MS-PS等进展，仍存在两个挑战：光照与法线特征的深度耦合，以及复杂表面高频几何细节的保留。


<details>
  <summary>Details</summary>
Motivation: 解决通用光度立体中光照与表面法线特征的深度耦合问题，以及复杂表面高频几何细节的准确捕捉。

Method: 通过分析光照变化与表面法线特征的耦合关系，并改进特征处理操作以保留高频细节。

Result: 揭示了光照与法线特征的耦合问题，并指出了现有方法在复杂表面细节捕捉上的不足。

Conclusion: 需进一步研究以解决光照与法线特征的耦合问题，并改进高频几何细节的捕捉方法。

Abstract: Universal photometric stereo (PS) aims to recover high-quality surface
normals from objects under arbitrary lighting conditions without relying on
specific illumination models. Despite recent advances such as SDM-UniPS and Uni
MS-PS, two fundamental challenges persist: 1) the deep coupling between varying
illumination and surface normal features, where ambiguity in observed intensity
makes it difficult to determine whether brightness variations stem from
lighting changes or surface orientation; and 2) the preservation of
high-frequency geometric details in complex surfaces, where intricate
geometries create self-shadowing, inter-reflections, and subtle normal
variations that conventional feature processing operations struggle to capture
accurately.

</details>


### [304] [Universal Video Temporal Grounding with Generative Multi-modal Large Language Models](https://arxiv.org/abs/2506.18883)
*Zeqian Li,Shangzhe Di,Zhonghua Zhai,Weilin Huang,Yanfeng Wang,Weidi Xie*

Main category: cs.CV

TL;DR: 提出了一种通用视频时间定位模型UniTime，利用生成式多模态大语言模型（MLLMs）处理多样化视频和复杂语言查询，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法局限于特定视频领域或时长，无法处理多样化视频和复杂语言查询。

Method: 结合时间戳标记与视频标记，通过自适应帧缩放处理不同输入粒度的视频。

Result: 在五个公开时间定位基准测试中表现优异，显著提升长视频问答的准确性。

Conclusion: UniTime是一种通用且鲁棒的视频时间定位模型，适用于复杂视频理解任务。

Abstract: This paper presents a computational model for universal video temporal
grounding, which accurately localizes temporal moments in videos based on
natural language queries (e.g., questions or descriptions). Unlike existing
methods that are often limited to specific video domains or durations, we
propose UniTime, a robust and universal video grounding model leveraging the
strong vision-language understanding capabilities of generative Multi-modal
Large Language Models (MLLMs). Our model effectively handles videos of diverse
views, genres, and lengths while comprehending complex language queries. The
key contributions include: (i) We consider steering strong MLLMs for temporal
grounding in videos. To enable precise timestamp outputs, we incorporate
temporal information by interleaving timestamp tokens with video tokens. (ii)
By training the model to handle videos with different input granularities
through adaptive frame scaling, our approach achieves robust temporal grounding
for both short and long videos. (iii) Comprehensive experiments show that
UniTime outperforms state-of-the-art approaches in both zero-shot and
dataset-specific finetuned settings across five public temporal grounding
benchmarks. (iv) When employed as a preliminary moment retriever for long-form
video question-answering (VideoQA), UniTime significantly improves VideoQA
accuracy, highlighting its value for complex video understanding tasks.

</details>


### [305] [4D-LRM: Large Space-Time Reconstruction Model From and To Any View at Any Time](https://arxiv.org/abs/2506.18890)
*Ziqiao Ma,Xuweiyi Chen,Shoubin Yu,Sai Bi,Kai Zhang,Chen Ziwen,Sihan Xu,Jianing Yang,Zexiang Xu,Kalyan Sunkavalli,Mohit Bansal,Joyce Chai,Hao Tan*

Main category: cs.CV

TL;DR: 4D-LRM是一种大规模4D重建模型，能够从任意视角和时间渲染物体，解决了传统方法在效率、泛化性和保真度上的不足。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过4D预训练学习通用的时空表示，以从少量视角和时间重建物体。

Method: 4D-LRM通过统一的时空表示，直接从跨时间的图像标记预测每像素的4D高斯基元，实现快速高质量渲染。

Result: 4D-LRM在单次前向传递中重建24帧序列，耗时少于1.5秒，泛化能力强，支持时间插值和多样化相机设置。

Conclusion: 4D-LRM证明了时空预训练在高效准确4D重建中的潜力。

Abstract: Can we scale 4D pretraining to learn general space-time representations that
reconstruct an object from a few views at some times to any view at any time?
We provide an affirmative answer with 4D-LRM, the first large-scale 4D
reconstruction model that takes input from unconstrained views and timestamps
and renders arbitrary novel view-time combinations. Unlike prior 4D approaches,
e.g., optimization-based, geometry-based, or generative, that struggle with
efficiency, generalization, or faithfulness, 4D-LRM learns a unified space-time
representation and directly predicts per-pixel 4D Gaussian primitives from
posed image tokens across time, enabling fast, high-quality rendering at, in
principle, infinite frame rate. Our results demonstrate that scaling
spatiotemporal pretraining enables accurate and efficient 4D reconstruction. We
show that 4D-LRM generalizes to novel objects, interpolates across time, and
handles diverse camera setups. It reconstructs 24-frame sequences in one
forward pass with less than 1.5 seconds on a single A100 GPU.

</details>


### [306] [FilMaster: Bridging Cinematic Principles and Generative AI for Automated Film Generation](https://arxiv.org/abs/2506.18899)
*Kaiyi Huang,Yukun Huang,Xintao Wang,Zinan Lin,Xuefei Ning,Pengfei Wan,Di Zhang,Yu Wang,Xihui Liu*

Main category: cs.CV

TL;DR: FilMaster是一个端到端AI系统，通过整合真实世界的电影原则生成专业级电影，解决现有系统在镜头语言和节奏上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有AI电影生成系统缺乏多样化的镜头语言和电影节奏，导致视觉效果模板化和叙事乏味。

Method: FilMaster基于两个原则：从大量电影数据学习摄影技术，模拟以观众为中心的后期制作流程。系统分为参考引导生成阶段和生成后期制作阶段。

Result: FilMaster在镜头语言设计和电影节奏控制上表现优异，并通过FilmEval基准测试验证。

Conclusion: FilMaster通过整合专业电影原则和生成AI模型，推动了专业电影制作中生成AI的进步。

Abstract: AI-driven content creation has shown potential in film production. However,
existing film generation systems struggle to implement cinematic principles and
thus fail to generate professional-quality films, particularly lacking diverse
camera language and cinematic rhythm. This results in templated visuals and
unengaging narratives. To address this, we introduce FilMaster, an end-to-end
AI system that integrates real-world cinematic principles for
professional-grade film generation, yielding editable, industry-standard
outputs. FilMaster is built on two key principles: (1) learning cinematography
from extensive real-world film data and (2) emulating professional,
audience-centric post-production workflows. Inspired by these principles,
FilMaster incorporates two stages: a Reference-Guided Generation Stage which
transforms user input to video clips, and a Generative Post-Production Stage
which transforms raw footage into audiovisual outputs by orchestrating visual
and auditory elements for cinematic rhythm. Our generation stage highlights a
Multi-shot Synergized RAG Camera Language Design module to guide the AI in
generating professional camera language by retrieving reference clips from a
vast corpus of 440,000 film clips. Our post-production stage emulates
professional workflows by designing an Audience-Centric Cinematic Rhythm
Control module, including Rough Cut and Fine Cut processes informed by
simulated audience feedback, for effective integration of audiovisual elements
to achieve engaging content. The system is empowered by generative AI models
like (M)LLMs and video generation models. Furthermore, we introduce FilmEval, a
comprehensive benchmark for evaluating AI-generated films. Extensive
experiments show FilMaster's superior performance in camera language design and
cinematic rhythm control, advancing generative AI in professional filmmaking.

</details>


### [307] [Audit & Repair: An Agentic Framework for Consistent Story Visualization in Text-to-Image Diffusion Models](https://arxiv.org/abs/2506.18900)
*Kiymet Akdemir,Tahira Kazimi,Pinar Yanardag*

Main category: cs.CV

TL;DR: 提出了一种多智能体协作框架，用于解决多面板故事可视化中的视觉一致性问题，无需重新生成整个序列。


<details>
  <summary>Details</summary>
Motivation: 故事可视化中保持角色和对象的视觉一致性是一个关键挑战，现有方法常导致叙事不连贯。

Method: 采用多智能体框架，通过迭代循环自主识别、修正和优化不一致性，支持多种扩散模型。

Result: 定量和定性实验表明，该方法在多面板一致性上优于现有方法。

Conclusion: 该框架灵活且模型无关，能有效提升故事可视化的连贯性。

Abstract: Story visualization has become a popular task where visual scenes are
generated to depict a narrative across multiple panels. A central challenge in
this setting is maintaining visual consistency, particularly in how characters
and objects persist and evolve throughout the story. Despite recent advances in
diffusion models, current approaches often fail to preserve key character
attributes, leading to incoherent narratives. In this work, we propose a
collaborative multi-agent framework that autonomously identifies, corrects, and
refines inconsistencies across multi-panel story visualizations. The agents
operate in an iterative loop, enabling fine-grained, panel-level updates
without re-generating entire sequences. Our framework is model-agnostic and
flexibly integrates with a variety of diffusion models, including rectified
flow transformers such as Flux and latent diffusion models such as Stable
Diffusion. Quantitative and qualitative experiments show that our method
outperforms prior approaches in terms of multi-panel consistency.

</details>


### [308] [From Virtual Games to Real-World Play](https://arxiv.org/abs/2506.18901)
*Wenqiang Sun,Fangyun Wei,Jinjing Zhao,Xi Chen,Zilong Chen,Hongyang Zhang,Jun Zhang,Yan Lu*

Main category: cs.CV

TL;DR: RealPlay是一个基于神经网络的实时游戏引擎，通过用户控制信号生成交互式视频，目标是生成逼真且时间一致的视频序列。


<details>
  <summary>Details</summary>
Motivation: 现有方法主要关注游戏风格视觉效果，而RealPlay旨在生成类似真实世界场景的逼真视频。

Method: 采用迭代分块预测实现低延迟反馈，确保时间一致性，并准确响应用户控制信号。训练数据结合标记的游戏数据和无标记的真实世界视频。

Result: RealPlay实现了两种泛化能力：控制信号从虚拟到真实场景的映射，以及从赛车游戏标签扩展到控制自行车和行人等多样实体。

Conclusion: RealPlay展示了在无需真实世界动作标注的情况下，生成逼真交互视频的潜力。

Abstract: We introduce RealPlay, a neural network-based real-world game engine that
enables interactive video generation from user control signals. Unlike prior
works focused on game-style visuals, RealPlay aims to produce photorealistic,
temporally consistent video sequences that resemble real-world footage. It
operates in an interactive loop: users observe a generated scene, issue a
control command, and receive a short video chunk in response. To enable such
realistic and responsive generation, we address key challenges including
iterative chunk-wise prediction for low-latency feedback, temporal consistency
across iterations, and accurate control response. RealPlay is trained on a
combination of labeled game data and unlabeled real-world videos, without
requiring real-world action annotations. Notably, we observe two forms of
generalization: (1) control transfer-RealPlay effectively maps control signals
from virtual to real-world scenarios; and (2) entity transfer-although training
labels originate solely from a car racing game, RealPlay generalizes to control
diverse real-world entities, including bicycles and pedestrians, beyond
vehicles. Project page can be found: https://wenqsun.github.io/RealPlay/

</details>


### [309] [VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed View Memory](https://arxiv.org/abs/2506.18903)
*Runjia Li,Philip Torr,Andrea Vedaldi,Tomas Jakab*

Main category: cs.CV

TL;DR: 提出了一种新的内存机制（VMem），用于构建能够交互式探索环境的视频生成器，解决了现有方法在长期场景一致性上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法在长期场景合成中容易积累误差或难以保持一致性，需要一种更高效且准确的方法。

Method: 引入Surfel-Indexed View Memory（VMem），通过几何索引存储和检索过去视图，仅关注相关视图以生成新视图。

Result: 在长期场景合成基准测试中表现优异，显著提升了场景一致性和相机控制能力。

Conclusion: VMem机制在保持场景一致性和降低计算成本方面优于现有方法。

Abstract: We propose a novel memory mechanism to build video generators that can
explore environments interactively. Similar results have previously been
achieved by out-painting 2D views of the scene while incrementally
reconstructing its 3D geometry, which quickly accumulates errors, or by video
generators with a short context window, which struggle to maintain scene
coherence over the long term. To address these limitations, we introduce
Surfel-Indexed View Memory (VMem), a mechanism that remembers past views by
indexing them geometrically based on the 3D surface elements (surfels) they
have observed. VMem enables the efficient retrieval of the most relevant past
views when generating new ones. By focusing only on these relevant views, our
method produces consistent explorations of imagined environments at a fraction
of the computational cost of using all past views as context. We evaluate our
approach on challenging long-term scene synthesis benchmarks and demonstrate
superior performance compared to existing methods in maintaining scene
coherence and camera control.

</details>


### [310] [TC-Light: Temporally Consistent Relighting for Dynamic Long Videos](https://arxiv.org/abs/2506.18904)
*Yang Liu,Chuanchen Luo,Zimo Tang,Yingyan Li,Yuran Yang,Yuanyong Ning,Lue Fan,Junran Peng,Zhaoxiang Zhang*

Main category: cs.CV

TL;DR: TC-Light是一种新的视频重光照方法，通过两阶段优化机制实现全局光照和细节纹理的对齐，具有高效计算和优秀的时间一致性。


<details>
  <summary>Details</summary>
Motivation: 现有视频重光照技术主要局限于肖像视频或面临时间一致性和计算效率的瓶颈，TC-Light旨在解决这些问题。

Method: 提出两阶段后优化机制：第一阶段优化外观嵌入以对齐全局光照，第二阶段优化唯一视频张量（UVT）以对齐细节纹理和光照。

Result: 实验表明，TC-Light能够实现物理上合理且时间一致的重光照效果，同时计算成本低。

Conclusion: TC-Light为复杂动态长视频的重光照提供了一种高效且一致的解决方案。

Abstract: Editing illumination in long videos with complex dynamics has significant
value in various downstream tasks, including visual content creation and
manipulation, as well as data scaling up for embodied AI through sim2real and
real2real transfer. Nevertheless, existing video relighting techniques are
predominantly limited to portrait videos or fall into the bottleneck of
temporal consistency and computation efficiency. In this paper, we propose
TC-Light, a novel paradigm characterized by the proposed two-stage post
optimization mechanism. Starting from the video preliminarily relighted by an
inflated video relighting model, it optimizes appearance embedding in the first
stage to align global illumination. Then it optimizes the proposed canonical
video representation, i.e., Unique Video Tensor (UVT), to align fine-grained
texture and lighting in the second stage. To comprehensively evaluate
performance, we also establish a long and highly dynamic video benchmark.
Extensive experiments show that our method enables physically plausible
relighting results with superior temporal coherence and low computation cost.
The code and video demos are available at
https://dekuliutesla.github.io/tclight/.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [311] [Modal Logic for Stratified Becoming: Actualization Beyond Possible Worlds](https://arxiv.org/abs/2506.17276)
*Alexandre Le Nepvou*

Main category: cs.LO

TL;DR: 本文提出了一种基于分层实现的新型模态逻辑框架，替代传统的全局可能世界模型。


<details>
  <summary>Details</summary>
Motivation: 传统克里普克语义学将模态算子视为对完全确定替代物的量化，忽略了实现的局部性、动态性和不对称性。

Method: 提出分层实现逻辑（SAL），模态按本体稳定性层级索引，解释为可接纳制度，定义其语法、语义、公理，并证明其可靠性和完备性。

Result: SAL成功捕捉了实现的本体结构，无需依赖抽象可能世界。

Conclusion: SAL为模态实在论提供了一种分层替代方案，适用于时间生成、量子退相干等领域。

Abstract: This article develops a novel framework for modal logic based on the idea of
stratified actualization, rather than the classical model of global possible
worlds. Traditional Kripke semantics treat modal operators as quantification
over fully determinate alternatives, neglecting the local, dynamic, and often
asymmetric nature of actualization processes. We propose a system Stratified
Actualization Logic (SAL) in which modalities are indexed by levels of
ontological stability, interpreted as admissibility regimes. Each modality
operates over a structured layer of possibility, grounded in the internal
coherence of transitions between layers. We formally define the syntax and
semantics of SAL, introduce its axioms, and prove soundness and completeness.
Applications are discussed in connection with temporal becoming, quantum
decoherence domains, and modal metaphysics. The result is a logic that captures
the ontological structure of actualization without recourse to abstract
possible worlds, offering a stratified alternative to standard modal realism.

</details>


### [312] [Beyond Prediction -- Structuring Epistemic Integrity in Artificial Reasoning Systems](https://arxiv.org/abs/2506.17331)
*Craig Steven Wright*

Main category: cs.LO

TL;DR: 本文提出了一个在严格认知约束下运作的AI系统框架，支持结构化推理、命题承诺和矛盾检测。


<details>
  <summary>Details</summary>
Motivation: 超越随机语言预测，构建能够进行结构化推理和矛盾检测的AI系统。

Method: 结合符号推理、知识图谱和区块链技术，形式化信念表示和元认知过程。

Result: 开发出能够保持真理、可审计的理性认知代理。

Conclusion: 该框架为AI系统提供了更高级的认知能力，适用于需要严格逻辑和可验证性的场景。

Abstract: This paper develops a comprehensive framework for artificial intelligence
systems that operate under strict epistemic constraints, moving beyond
stochastic language prediction to support structured reasoning, propositional
commitment, and contradiction detection. It formalises belief representation,
metacognitive processes, and normative verification, integrating symbolic
inference, knowledge graphs, and blockchain-based justification to ensure
truth-preserving, auditably rational epistemic agents.

</details>


<div id='physics.geo-ph'></div>

# physics.geo-ph [[Back]](#toc)

### [313] [Pix2Geomodel: A Next-Generation Reservoir Geomodeling with Property-to-Property Translation](https://arxiv.org/abs/2506.17747)
*Abdulrahman Al-Fakih,Ardiansyah Koeshidayatullah,Nabil A. Saraih,Tapan Mukerji,Rayan Kanfar,Abdulmohsen Alali,SanLinn I. Kaka*

Main category: physics.geo-ph

TL;DR: Pix2Geomodel是一种基于Pix2Pix的条件生成对抗网络（cGAN），用于预测储层属性，在复杂地质建模中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统地质建模方法难以处理复杂的地下异质性和观测数据条件化问题。

Method: 使用Pix2Pix框架，结合U-Net生成器和PatchGAN判别器，对7.6百万个单元的数据集进行预处理、增强和训练。

Result: 在储层属性预测中表现出高准确性（如相PA 0.88，水饱和度PA 0.96），但孔隙度和渗透率表现中等。

Conclusion: Pix2Geomodel在直接属性映射中提供了更高的保真度，未来可扩展至3D建模和多模态数据。

Abstract: Accurate geological modeling is critical for reservoir characterization, yet
traditional methods struggle with complex subsurface heterogeneity, and they
have problems with conditioning to observed data. This study introduces
Pix2Geomodel, a novel conditional generative adversarial network (cGAN)
framework based on Pix2Pix, designed to predict reservoir properties (facies,
porosity, permeability, and water saturation) from the Rotliegend reservoir of
the Groningen gas field. Utilizing a 7.6 million-cell dataset from the
Nederlandse Aardolie Maatschappij, accessed via EPOS-NL, the methodology
included data preprocessing, augmentation to generate 2,350 images per
property, and training with a U-Net generator and PatchGAN discriminator over
19,000 steps. Evaluation metrics include pixel accuracy (PA), mean intersection
over union (mIoU), frequency weighted intersection over union (FWIoU), and
visualizations assessed performance in masked property prediction and
property-to-property translation tasks. Results demonstrated high accuracy for
facies (PA 0.88, FWIoU 0.85) and water saturation (PA 0.96, FWIoU 0.95), with
moderate success for porosity (PA 0.70, FWIoU 0.55) and permeability (PA 0.74,
FWIoU 0.60), and robust translation performance (e.g., facies-to-facies PA
0.98, FWIoU 0.97). The framework captured spatial variability and geological
realism, as validated by variogram analysis, and calculated the training loss
curves for the generator and discriminator for each property. Compared to
traditional methods, Pix2Geomodel offers enhanced fidelity in direct property
mapping. Limitations include challenges with microstructural variability and 2D
constraints, suggesting future integration of multi-modal data and 3D modeling
(Pix2Geomodel v2.0). This study advances the application of generative AI in
geoscience, supporting improved reservoir management and open science
initiatives.

</details>


<div id='physics.med-ph'></div>

# physics.med-ph [[Back]](#toc)

### [314] [Exploring Strategies for Personalized Radiation Therapy Part II Predicting Tumor Drift Patterns with Diffusion Models](https://arxiv.org/abs/2506.17491)
*Hao Peng,Steve Jiang,Robert Timmerman*

Main category: physics.med-ph

TL;DR: 该论文提出了一种基于去噪扩散隐式模型（DDIM）的新框架，用于模拟脑癌放疗中肿瘤的时空演变，以优化个性化适应性放疗（PULSAR）的决策。


<details>
  <summary>Details</summary>
Motivation: 脑癌放疗的剂量和时机对治疗效果至关重要，但现有模型难以捕捉肿瘤响应的时空动态变化，限制了适应性放疗的精准性。

Method: 采用DDIM模型，开发单步和迭代去噪策略，从治疗前影像预测治疗后肿瘤演变。

Result: 扩散模型能有效模拟患者特异性肿瘤演变，并定位与治疗响应相关的区域。

Conclusion: 该框架为建模异质性治疗响应和早期适应性干预提供了基础，推动更个性化的放疗方案。

Abstract: Radiation therapy outcomes are decided by two key parameters, dose and
timing, whose best values vary substantially across patients. This variability
is especially critical in the treatment of brain cancer, where fractionated or
staged stereotactic radiosurgery improves safety compared to single fraction
approaches, but complicates the ability to predict treatment response. To
address this challenge, we employ Personalized Ultra-fractionated Stereotactic
Adaptive Radiotherapy (PULSAR), a strategy that dynamically adjusts treatment
based on how each tumor evolves over time. However, the success of PULSAR and
other adaptive approaches depends on predictive tools that can guide early
treatment decisions and avoid both overtreatment and undertreatment. However,
current radiomics and dosiomics models offer limited insight into the evolving
spatial and temporal patterns of tumor response. To overcome these limitations,
we propose a novel framework using Denoising Diffusion Implicit Models (DDIM),
which learns data-driven mappings from pre to post treatment imaging. In this
study, we developed single step and iterative denoising strategies and compared
their performance. The results show that diffusion models can effectively
simulate patient specific tumor evolution and localize regions associated with
treatment response. The proposed strategy provides a promising foundation for
modeling heterogeneous treatment response and enabling early, adaptive
interventions, paving the way toward more personalized and biologically
informed radiotherapy.

</details>


### [315] [Exploring Strategies for Personalized Radiation Therapy Part I Unlocking Response-Related Tumor Subregions with Class Activation Mapping](https://arxiv.org/abs/2506.17536)
*Hao Peng,Steve Jiang,Robert Timmerman*

Main category: physics.med-ph

TL;DR: 比较了三种预测放疗反应的方法（标准放射组学、基于梯度的特征和卷积神经网络），发现像素级CAM在分类准确性和空间洞察力上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 个性化精准放疗需要识别预后和空间信息特征，并根据个体反应调整治疗。

Method: 使用集成自编码器分类器模型，分析69例脑转移患者的伽玛刀放疗数据，预测肿瘤体积缩小是否超过20%。

Result: 像素级CAM在分类准确性上优于其他方法，并能提供更细致的空间信息，识别病灶特异性区域。

Conclusion: 像素级CAM有望指导个性化放疗策略，但需进一步验证。

Abstract: Personalized precision radiation therapy requires more than simple
classification, it demands the identification of prognostic, spatially
informative features and the ability to adapt treatment based on individual
response. This study compares three approaches for predicting treatment
response: standard radiomics, gradient based features, and convolutional neural
networks enhanced with Class Activation Mapping. We analyzed 69 brain
metastases from 39 patients treated with Gamma Knife radiosurgery. An
integrated autoencoder classifier model was used to predict whether tumor
volume would shrink by more than 20 percent at a three months follow up, framed
as a binary classification task. The results highlight their strength in
hierarchical feature extraction and the classifiers discriminative capacity.
Among the models, pixel wise CAM provides the most detailed spatial insight,
identifying lesion specific regions rather than relying on fixed patterns,
demonstrating strong generalization. In non responding lesions, the activated
regions may indicate areas of radio resistance. Pixel wise CAM outperformed
both radiomics and gradient based methods in classification accuracy. Moreover,
its fine grained spatial features allow for alignment with cellular level data,
supporting biological validation and deeper understanding of heterogeneous
treatment responses. Although further validation is necessary, these findings
underscore the promise in guiding personalized and adaptive radiotherapy
strategies for both photon and particle therapies.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [316] [MMET: A Multi-Input and Multi-Scale Transformer for Efficient PDEs Solving](https://arxiv.org/abs/2506.17230)
*Yichen Luo,Jia Wang,Dapeng Lan,Yu Liu,Zhibo Pang*

Main category: cs.LG

TL;DR: 论文提出了一种名为MMET的新型框架，通过解耦网格和查询点、使用GCE层嵌入多尺度输入，并结合Hilbert曲线降低计算成本，显著提升了多输入多尺度PDE求解的效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决传统机器学习方法在求解多输入多尺度PDE时泛化能力不足和计算成本高的问题。

Method: MMET框架通过解耦网格和查询点、使用GCE层嵌入输入，并结合Hilbert曲线和分块嵌入机制降低输入长度。

Result: 实验表明，MMET在多个物理领域的基准测试中优于现有方法，具有更高的准确性和计算效率。

Conclusion: MMET为工程和物理应用中的实时PDE求解提供了可扩展的解决方案，并为特定领域的预训练大规模模型探索铺平了道路。

Abstract: Partial Differential Equations (PDEs) are fundamental for modeling physical
systems, yet solving them in a generic and efficient manner using machine
learning-based approaches remains challenging due to limited multi-input and
multi-scale generalization capabilities, as well as high computational costs.
This paper proposes the Multi-input and Multi-scale Efficient Transformer
(MMET), a novel framework designed to address the above challenges. MMET
decouples mesh and query points as two sequences and feeds them into the
encoder and decoder, respectively, and uses a Gated Condition Embedding (GCE)
layer to embed input variables or functions with varying dimensions, enabling
effective solutions for multi-scale and multi-input problems. Additionally, a
Hilbert curve-based reserialization and patch embedding mechanism decrease the
input length. This significantly reduces the computational cost when dealing
with large-scale geometric models. These innovations enable efficient
representations and support multi-scale resolution queries for large-scale and
multi-input PDE problems. Experimental evaluations on diverse benchmarks
spanning different physical fields demonstrate that MMET outperforms SOTA
methods in both accuracy and computational efficiency. This work highlights the
potential of MMET as a robust and scalable solution for real-time PDE solving
in engineering and physics-based applications, paving the way for future
explorations into pre-trained large-scale models in specific domains. This work
is open-sourced at https://github.com/YichenLuo-0/MMET.

</details>


### [317] [PCaM: A Progressive Focus Attention-Based Information Fusion Method for Improving Vision Transformer Domain Adaptation](https://arxiv.org/abs/2506.17232)
*Zelin Zang,Fei Wang,Liangyu Li,Jinlin Wu,Chunshui Zhao,Zhen Lei,Baigui Sun*

Main category: cs.LG

TL;DR: 本文提出了一种渐进式聚焦交叉注意力机制（PCaM），通过逐步过滤背景信息，专注于跨领域的判别性前景语义，解决了无监督域适应中前景对象不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于Vision Transformers的无监督域适应方法在特征对齐时存在前景对象大小和空间分布不一致的问题，影响了注意力一致性和域对齐效果。

Method: 提出PCaM机制，逐步过滤背景信息，并引入注意力引导损失，明确引导注意力到任务相关区域。

Result: 在多个数据集上的实验表明，PCaM显著提升了适应性能，并达到了新的最优结果。

Conclusion: PCaM通过注意力引导的前景融合，有效提升了无监督域适应的性能。

Abstract: Unsupervised Domain Adaptation (UDA) aims to transfer knowledge from a
labeled source domain to an unlabeled target domain. Recent UDA methods based
on Vision Transformers (ViTs) have achieved strong performance through
attention-based feature alignment. However, we identify a key limitation:
foreground object mismatch, where the discrepancy in foreground object size and
spatial distribution across domains weakens attention consistency and hampers
effective domain alignment. To address this issue, we propose the Progressive
Focus Cross-Attention Mechanism (PCaM), which progressively filters out
background information during cross-attention, allowing the model to focus on
and fuse discriminative foreground semantics across domains. We further
introduce an attentional guidance loss that explicitly directs attention toward
task-relevant regions, enhancing cross-domain attention consistency. PCaM is
lightweight, architecture-agnostic, and easy to integrate into existing
ViT-based UDA pipelines. Extensive experiments on Office-Home, DomainNet,
VisDA-2017, and remote sensing datasets demonstrate that PCaM significantly
improves adaptation performance and achieves new state-of-the-art results,
validating the effectiveness of attention-guided foreground fusion for domain
adaptation.

</details>


### [318] [Graph Neural Networks in Multi-Omics Cancer Research: A Structured Survey](https://arxiv.org/abs/2506.17234)
*Payam Zohari,Mostafa Haghir Chehreghani*

Main category: cs.LG

TL;DR: 本文综述了图神经网络（GNN）在多组学癌症研究中的应用，分类了不同方法，并总结了当前趋势和未来方向。


<details>
  <summary>Details</summary>
Motivation: 多组学数据整合是揭示癌症复杂生物学机制的有效策略，而GNN为建模异构结构化数据提供了框架。

Method: 系统回顾了基于GNN的多组学癌症研究，分类了方法（如组学层、GNN结构、生物任务）。

Result: 发现混合模型和可解释模型趋势增长，注意力机制和对比学习应用增加，患者特异性图和知识驱动先验成为新兴方向。

Conclusion: 本文为设计高效GNN多组学分析流程提供了资源，总结了当前实践、局限性和未来潜力。

Abstract: The task of data integration for multi-omics data has emerged as a powerful
strategy to unravel the complex biological underpinnings of cancer. Recent
advancements in graph neural networks (GNNs) offer an effective framework to
model heterogeneous and structured omics data, enabling precise representation
of molecular interactions and regulatory networks. This systematic review
explores several recent studies that leverage GNN-based architectures in
multi-omics cancer research. We classify the approaches based on their targeted
omics layers, graph neural network structures, and biological tasks such as
subtype classification, prognosis prediction, and biomarker discovery. The
analysis reveals a growing trend toward hybrid and interpretable models,
alongside increasing adoption of attention mechanisms and contrastive learning.
Furthermore, we highlight the use of patient-specific graphs and
knowledge-driven priors as emerging directions. This survey serves as a
comprehensive resource for researchers aiming to design effective GNN-based
pipelines for integrative cancer analysis, offering insights into current
practices, limitations, and potential future directions.

</details>


### [319] [Recursive Learning-Based Virtual Buffering for Analytical Global Placement](https://arxiv.org/abs/2506.17247)
*Andrew B. Kahng,Yiting Liu,Zhiang Wang*

Main category: cs.LG

TL;DR: MLBuf-RePlAce是一个基于学习的虚拟缓冲感知全局布局框架，显著提升了时序性能。


<details>
  <summary>Details</summary>
Motivation: 现代技术节点中互连延迟与单元延迟的不均衡缩放，需要缓冲孔隙感知的布局以实现时序收敛。现有方法存在计算成本高或忽略电气规则检查的问题。

Method: MLBuf-RePlAce采用递归学习生成缓冲类型和位置，并在全局布局中解决电气规则检查问题。

Result: 在OpenROAD流程中，MLBuf-RePlAce在总负松弛（TNS）上实现了最高56%、平均31%的改进，且不影响后布线功耗。

Conclusion: MLBuf-RePlAce是一种高效的缓冲感知布局方法，显著提升了时序性能并适用于实际设计流程。

Abstract: Due to the skewed scaling of interconnect versus cell delay in modern
technology nodes, placement with buffer porosity (i.e., cell density) awareness
is essential for timing closure in physical synthesis flows. However, existing
approaches face two key challenges: (i) traditional van Ginneken-Lillis-style
buffering approaches are computationally expensive during global placement; and
(ii) machine learning-based approaches, such as BufFormer, lack a thorough
consideration of Electrical Rule Check (ERC) violations and fail to "close the
loop" back into the physical design flow. In this work, we propose
MLBuf-RePlAce, the first open-source learning-driven virtual buffering-aware
analytical global placement framework, built on top of the OpenROAD
infrastructure. MLBuf-RePlAce adopts an efficient recursive learning-based
generative buffering approach to predict buffer types and locations, addressing
ERC violations during global placement. We compare MLBuf-RePlAce against the
default virtual buffering-based timing-driven global placer in OpenROAD, using
open-source testcases from the TILOS MacroPlacement and OpenROAD-flow-scripts
repositories. Without degradation of post-route power, MLBuf-RePlAce achieves
(maximum, average) improvements of (56%, 31%) in total negative slack (TNS)
within the open-source OpenROAD flow. When evaluated by completion in a
commercial flow, MLBuf-RePlAce achieves (maximum, average) improvements of
(53%, 28%) in TNS with an average of 0.2% improvement in post-route power.

</details>


### [320] [Efficient Quantification of Multimodal Interaction at Sample Level](https://arxiv.org/abs/2506.17248)
*Zequn Yang,Hongfa Wang,Di Hu*

Main category: cs.LG

TL;DR: 论文提出了一种轻量级样本级多模态交互（LSMI）估计器，用于量化多模态信息中的冗余、独特性和协同作用，解决了理论和计算上的挑战。


<details>
  <summary>Details</summary>
Motivation: 理解多模态信息中的交互作用（冗余、独特性和协同作用）对分析信息动态至关重要，但样本级量化存在理论和计算难题。

Method: 基于点对点信息理论，首先开发冗余估计框架，再提出通用交互估计方法，采用高效熵估计技术。

Result: 在合成和真实数据集上的实验验证了LSMI的精确性和效率，揭示了样本和类别级的动态。

Conclusion: LSMI为多模态数据分析提供了实用工具，支持冗余样本划分、知识蒸馏和模型集成等应用。

Abstract: Interactions between modalities -- redundancy, uniqueness, and synergy --
collectively determine the composition of multimodal information. Understanding
these interactions is crucial for analyzing information dynamics in multimodal
systems, yet their accurate sample-level quantification presents significant
theoretical and computational challenges. To address this, we introduce the
Lightweight Sample-wise Multimodal Interaction (LSMI) estimator, rigorously
grounded in pointwise information theory. We first develop a redundancy
estimation framework, employing an appropriate pointwise information measure to
quantify this most decomposable and measurable interaction. Building upon this,
we propose a general interaction estimation method that employs efficient
entropy estimation, specifically tailored for sample-wise estimation in
continuous distributions. Extensive experiments on synthetic and real-world
datasets validate LSMI's precision and efficiency. Crucially, our sample-wise
approach reveals fine-grained sample- and category-level dynamics within
multimodal data, enabling practical applications such as redundancy-informed
sample partitioning, targeted knowledge distillation, and interaction-aware
model ensembling. The code is available at
https://github.com/GeWu-Lab/LSMI_Estimator.

</details>


### [321] [Improving Prediction Certainty Estimation for Reliable Early Exiting via Null Space Projection](https://arxiv.org/abs/2506.17249)
*Jianing He,Qi Zhang,Duoqian Miao,Yi Kun,Shufeng Hao,Hongyun Zhang,Zhihua Wei*

Main category: cs.LG

TL;DR: 论文提出了一种基于NSP分数和CAP分数的早期退出方法，通过整合类相关和类无关信息来更准确地估计预测确定性，从而提升推理效率。


<details>
  <summary>Details</summary>
Motivation: 现有早期退出方法仅依赖类相关logits估计预测确定性，忽略了类无关信息对预测的负面影响，导致过早退出错误预测的样本。

Method: 定义NSP分数衡量类无关信息比例，提出CAP分数整合logits和NSP分数，优化预测确定性估计。

Result: 在GLUE基准测试中，平均加速比为2.19倍，性能损失可忽略，优于现有方法ConsistentEE 28%。

Conclusion: 该方法在任务性能和推理效率之间取得了更好的平衡，代码已开源。

Abstract: Early exiting has demonstrated great potential in accelerating the inference
of pre-trained language models (PLMs) by enabling easy samples to exit at
shallow layers, eliminating the need for executing deeper layers. However,
existing early exiting methods primarily rely on class-relevant logits to
formulate their exiting signals for estimating prediction certainty, neglecting
the detrimental influence of class-irrelevant information in the features on
prediction certainty. This leads to an overestimation of prediction certainty,
causing premature exiting of samples with incorrect early predictions. To
remedy this, we define an NSP score to estimate prediction certainty by
considering the proportion of class-irrelevant information in the features. On
this basis, we propose a novel early exiting method based on the
Certainty-Aware Probability (CAP) score, which integrates insights from both
logits and the NSP score to enhance prediction certainty estimation, thus
enabling more reliable exiting decisions. The experimental results on the GLUE
benchmark show that our method can achieve an average speed-up ratio of 2.19x
across all tasks with negligible performance degradation, surpassing the
state-of-the-art (SOTA) ConsistentEE by 28%, yielding a better trade-off
between task performance and inference efficiency. The code is available at
https://github.com/He-Jianing/NSP.git.

</details>


### [322] [Towards Interpretable Adversarial Examples via Sparse Adversarial Attack](https://arxiv.org/abs/2506.17250)
*Fudong Lin,Jiadong Lou,Hao Wang,Brian Jalaian,Xu Yuan*

Main category: cs.LG

TL;DR: 本文提出了一种稀疏攻击方法，通过最小化初始扰动的幅度（在l0约束下），以解决现有稀疏攻击方法的不足，如计算开销大、迁移性差和攻击强度弱。该方法通过理论参数化技术和新型损失函数，实现了快速、可迁移且强力的攻击，并生成了更稀疏的对抗样本。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏攻击方法在解释深度神经网络（DNN）的脆弱性时存在不足，如稀疏性差、计算开销大、迁移性差和攻击强度弱。本文旨在开发一种更高效的稀疏攻击方法，以更好地理解卷积神经网络（CNN）的脆弱性。

Method: 提出了一种理论参数化技术，将NP难的l0优化问题转化为可计算的形式，并设计了新型损失函数，以同时最大化对抗性和最小化扰动像素数量。

Result: 实验表明，该方法在计算开销、迁移性和攻击强度上优于现有稀疏攻击方法，并生成了更稀疏的对抗样本。此外，发现了两种噪声类型（“遮蔽噪声”和“引导噪声”），有助于解释对抗扰动如何误导分类器。

Conclusion: 本文提出的稀疏攻击方法在理论和实验上均表现出色，可作为评估DNN鲁棒性的基准，并为理解对抗扰动的机制提供了新视角。

Abstract: Sparse attacks are to optimize the magnitude of adversarial perturbations for
fooling deep neural networks (DNNs) involving only a few perturbed pixels
(i.e., under the l0 constraint), suitable for interpreting the vulnerability of
DNNs. However, existing solutions fail to yield interpretable adversarial
examples due to their poor sparsity. Worse still, they often struggle with
heavy computational overhead, poor transferability, and weak attack strength.
In this paper, we aim to develop a sparse attack for understanding the
vulnerability of CNNs by minimizing the magnitude of initial perturbations
under the l0 constraint, to overcome the existing drawbacks while achieving a
fast, transferable, and strong attack to DNNs. In particular, a novel and
theoretical sound parameterization technique is introduced to approximate the
NP-hard l0 optimization problem, making directly optimizing sparse
perturbations computationally feasible. Besides, a novel loss function is
designed to augment initial perturbations by maximizing the adversary property
and minimizing the number of perturbed pixels simultaneously. Extensive
experiments are conducted to demonstrate that our approach, with theoretical
performance guarantees, outperforms state-of-the-art sparse attacks in terms of
computational overhead, transferability, and attack strength, expecting to
serve as a benchmark for evaluating the robustness of DNNs. In addition,
theoretical and empirical results validate that our approach yields sparser
adversarial examples, empowering us to discover two categories of noises, i.e.,
"obscuring noise" and "leading noise", which will help interpret how
adversarial perturbation misleads the classifiers into incorrect predictions.
Our code is available at https://github.com/fudong03/SparseAttack.

</details>


### [323] [Training-free LLM Verification via Recycling Few-shot Examples](https://arxiv.org/abs/2506.17251)
*Dongseok Lee,Jimyung Hong,Dongyoung Kim,Jaehyung Kim*

Main category: cs.LG

TL;DR: 论文提出了一种名为Referi的新框架，通过复用少样本示例来验证LLM输出，显著提高了准确性，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: LLM的随机性和结论不一致性带来挑战，现有方法如多数投票或外部验证模型存在局限性。

Method: Referi利用少样本示例评估候选输出，结合两种分数选择最优解，无需额外训练。

Result: 在三个LLM和七项任务中，Referi平均提升4.8%的准确性。

Conclusion: Referi是一种高效且无需额外训练的LLM输出验证框架。

Abstract: Although LLMs have achieved remarkable performance, the inherent
stochasticity of their reasoning process and varying conclusions present
significant challenges. Majority voting or Best-of-N with external verification
models has been explored to find the most promising solution among multiple LLM
outputs. However, these approaches have certain limitations, such as limited
applicability or the cost of an additional training step. To address this
problem, we propose a novel and effective framework that Recycles Few-shot
examples to verify LLM outputs (Referi). Our key idea is to additionally
utilize the given few-shot examples to evaluate the candidate outputs of the
target query, not only using them to generate outputs as the conventional
few-shot prompting setup. Specifically, Referi evaluates the generated outputs
by combining two different scores, designed motivated from Bayes' rule, and
subsequently selects the candidate that is both confidently determined and
contextually coherent through a few additional LLM inferences. Experiments with
three different LLMs and across seven diverse tasks demonstrate that our
framework significantly improves the accuracy of LLMs-achieving an average gain
of 4.8%-through effective response selection, without additional training.

</details>


### [324] [Adaptive Sample Scheduling for Direct Preference Optimization](https://arxiv.org/abs/2506.17252)
*Zixuan Huang,Yikun Ban,Lean Fu,Xiaojie Li,Zhongxiang Dai,Jianxin Li,Deqing Wang*

Main category: cs.LG

TL;DR: 本文提出了一种名为SamS的动态样本调度算法，用于优化DPO过程中训练样本的选择，以提升模型性能。


<details>
  <summary>Details</summary>
Motivation: DPO的性能依赖于人类偏好数据的质量，而现有方法忽视了模型在优化过程中的动态状态变化。

Method: 提出Sample Scheduling for DPO问题，并设计SamS算法，根据模型学习反馈动态选择样本。

Result: 在不修改DPO核心算法的情况下，SamS显著提升了任务性能，且计算开销极小。

Conclusion: SamS为通过更有效利用固定偏好数据集改进LLM对齐提供了新方向。

Abstract: Direct Preference Optimization (DPO) has emerged as an effective approach for
aligning large language models (LLMs) with human preferences. However, its
performance is highly dependent on the quality of the underlying human
preference data. To address this bottleneck, prior work has explored various
data selection strategies, but these methods often overlook the impact of the
evolving states of the language model during the DPO process. %including active
querying, response pair selection, and data pre-selection. In this paper, we
introduce a novel problem: Sample Scheduling for DPO, which aims to dynamically
and adaptively schedule training samples based on the model's evolving states
throughout preference optimization. To solve this problem, we propose SamS, an
efficient and effective algorithm that adaptively selects samples in each
training batch based on the LLM's learning feedback to maximize the potential
generalization performance. Notably, without modifying the core DPO algorithm,
simply integrating SamS significantly improves performance across tasks, with
minimal additional computational overhead. This work points to a promising new
direction for improving LLM alignment through more effective utilization of
fixed preference datasets.

</details>


### [325] [MS-TVNet:A Long-Term Time Series Prediction Method Based on Multi-Scale Dynamic Convolution](https://arxiv.org/abs/2506.17253)
*Chenghan Li,Mingchen Li,Yipu Liao,Ruisheng Diao*

Main category: cs.LG

TL;DR: 论文提出了一种多尺度时间序列重塑模块和MS-TVNet模型，用于长期时间序列预测，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要依赖Transformer和MLP模型，而卷积网络在长期时间序列预测中的潜力尚未充分挖掘。

Method: 提出多尺度时间序列重塑模块和MS-TVNet（多尺度3D动态卷积神经网络），捕捉多周期片段和变量依赖关系。

Result: 在多个数据集上表现优异，达到SOTA水平。

Conclusion: 卷积网络能有效捕捉复杂时间模式，为未来研究提供了新方向。

Abstract: Long-term time series prediction has predominantly relied on Transformer and
MLP models, while the potential of convolutional networks in this domain
remains underexplored. To address this gap, we introduce a novel multi-scale
time series reshape module, which effectively captures the relationships among
multi-period patches and variable dependencies. Building upon this module, we
propose MS-TVNet, a multi-scale 3D dynamic convolutional neural network.
Through comprehensive evaluations on diverse datasets, MS-TVNet demonstrates
superior performance compared to baseline models, achieving state-of-the-art
(SOTA) results in long-term time series prediction. Our findings highlight the
effectiveness of leveraging convolutional networks for capturing complex
temporal patterns, suggesting a promising direction for future research in this
field.The code is realsed on https://github.com/Curyyfaust/TVNet.

</details>


### [326] [Keeping Up with the Models: Online Deployment and Routing of LLMs at Scale](https://arxiv.org/abs/2506.17254)
*Shaoang Li,Jian Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为StageRoute的分层算法，用于在线决策问题中优化大型语言模型（LLM）的部署和查询路由，以应对模型快速更新和预算限制的挑战。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）的快速更新和淘汰，服务提供商需要在有限的部署容量和查询成本预算下管理模型库存。

Method: StageRoute算法分两个阶段：(i) 使用奖励上界和成本下界乐观选择最多M_max个模型；(ii) 通过预算约束的bandit子问题为每个查询路由。

Result: 理论证明StageRoute的遗憾度为T^{2/3}，实验验证其在实际场景中接近最优性能。

Conclusion: StageRoute是一种高效的在线决策算法，适用于动态模型管理和查询路由问题。

Abstract: The rapid pace at which new large language models (LLMs) appear -- and older
ones become obsolete -- forces LLM service providers to juggle a streaming
inventory of models while respecting tight deployment capacity and per-query
cost budgets. We cast the reality as an online decision problem that couples
stage-wise deployment, made at fixed maintenance windows, with per-query
routing among the models kept live. We introduce StageRoute, a hierarchical
algorithm that (i) optimistically selects up to $M_max$ models for the next
stage using reward upper-confidence and cost lower-confidence bounds, then (ii)
solves a budget-constrained bandit sub-problem to route each incoming query. We
prove that StageRoute achieves a regret of order $T^{2/3}$ and provide a
matching lower bound, thereby establishing its near-optimality. Moreover, our
experiments confirm the theory, demonstrating that StageRoute performs close to
the optimum in practical settings.

</details>


### [327] [UltraSketchLLM: Saliency-Driven Sketching for Ultra-Low Bit LLM Compression](https://arxiv.org/abs/2506.17255)
*Sunan Zou,Ziyun Zhang,Xueting Sun,Guojie Luo*

Main category: cs.LG

TL;DR: UltraSketchLLM 是一种基于数据草图的框架，实现超低比特压缩（低至 0.5 比特/权重），同时保持模型性能，适用于资源受限环境中的 LLM 部署。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的快速增长超过了边缘设备的内存限制，需要超越 1 比特极限的极端权重压缩。现有方法要么依赖映射表（增加内存开销），要么因随机权重分组导致严重精度下降。

Method: UltraSketchLLM 利用数据草图技术，通过低估 AbsMaxMin 草图最小化小权重的相对误差，重要性感知空间分配优先处理关键权重，并使用直通估计器进行压缩感知微调。

Result: 在 Llama-3.2-1B 上实验表明，UltraSketchLLM 实现了低至 0.5 比特的压缩，同时保持竞争力的困惑度，并具有可容忍的延迟开销。

Conclusion: UltraSketchLLM 为资源受限环境中的 LLM 部署提供了实用解决方案。

Abstract: The rapid growth of large language models (LLMs) has outpaced the memory
constraints of edge devices, necessitating extreme weight compression beyond
the 1-bit limit. While quantization reduces model size, it is fundamentally
limited to 1 bit per weight. Existing multiple-to-one compression methods
either rely on mapping tables (inducing memory overhead) or incur severe
accuracy degradation due to random weight grouping. We introduce
UltraSketchLLM, an index-free, sketch-based framework that achieves ultra-low
bit compression (down to 0.5 bits per weight) while preserving model
performance. UltraSketchLLM leverages data sketching, a sub-linear
representation technique from streaming applications, to map multiple weights
to single values with bounded error. Our approach integrates an underestimate
AbsMaxMin sketch to minimize relative errors for small weights,
importance-aware space allocation to prioritize salient weights, and a
straight-through estimator for compression-aware finetuning. Experiments on
Llama-3.2-1B demonstrate up to 0.5-bit compression with competitive perplexity,
alongside tolerable latency overhead. UltraSketchLLM offers a practical
solution for deploying LLMs in resource-constrained environments.

</details>


### [328] [AI to Identify Strain-sensitive Regions of the Optic Nerve Head Linked to Functional Loss in Glaucoma](https://arxiv.org/abs/2506.17262)
*Thanadet Chuangsuwanich,Monisha E. Nongpiur,Fabian A. Braeu,Tin A. Tun,Alexandre Thiery,Shamira Perera,Ching Lin Ho,Martin Buist,George Barbastathis,Tin Aung,Michaël J. A. Girard*

Main category: cs.LG

TL;DR: 研究评估视神经乳头（ONH）生物力学是否改善青光眼三种进展性视野缺损模式的预测，并利用可解释AI识别关键应变敏感区域。


<details>
  <summary>Details</summary>
Motivation: 青光眼视野缺损预测通常依赖形态学，但生物力学可能提供额外信息。

Method: 237名青光眼患者ONH在两种眼压条件下成像，通过几何深度学习模型分析生物力学和结构特征。

Result: 模型AUC达0.77-0.88，显示ONH应变提升预测能力，下颞侧缘是关键区域。

Conclusion: ONH应变增强青光眼视野缺损预测，神经视网膜缘是关键贡献区域。

Abstract: Objective: (1) To assess whether ONH biomechanics improves prediction of
three progressive visual field loss patterns in glaucoma; (2) to use
explainable AI to identify strain-sensitive ONH regions contributing to these
predictions.
  Methods: We recruited 237 glaucoma subjects. The ONH of one eye was imaged
under two conditions: (1) primary gaze and (2) primary gaze with IOP elevated
to ~35 mmHg via ophthalmo-dynamometry. Glaucoma experts classified the subjects
into four categories based on the presence of specific visual field defects:
(1) superior nasal step (N=26), (2) superior partial arcuate (N=62), (3) full
superior hemifield defect (N=25), and (4) other/non-specific defects (N=124).
Automatic ONH tissue segmentation and digital volume correlation were used to
compute IOP-induced neural tissue and lamina cribrosa (LC) strains.
Biomechanical and structural features were input to a Geometric Deep Learning
model. Three classification tasks were performed to detect: (1) superior nasal
step, (2) superior partial arcuate, (3) full superior hemifield defect. For
each task, the data were split into 80% training and 20% testing sets. Area
under the curve (AUC) was used to assess performance. Explainable AI techniques
were employed to highlight the ONH regions most critical to each
classification.
  Results: Models achieved high AUCs of 0.77-0.88, showing that ONH strain
improved VF loss prediction beyond morphology alone. The inferior and
inferotemporal rim were identified as key strain-sensitive regions,
contributing most to visual field loss prediction and showing progressive
expansion with increasing disease severity.
  Conclusion and Relevance: ONH strain enhances prediction of glaucomatous VF
loss patterns. Neuroretinal rim, rather than the LC, was the most critical
region contributing to model predictions.

</details>


### [329] [Memory Allocation in Resource-Constrained Reinforcement Learning](https://arxiv.org/abs/2506.17263)
*Massimiliano Tamborski,David Abel*

Main category: cs.LG

TL;DR: 研究资源限制（尤其是内存限制）如何影响强化学习智能体在未知环境中的性能，探讨内存分配对学习和决策的影响。


<details>
  <summary>Details</summary>
Motivation: 资源限制（如内存）可能显著改变学习和决策过程，研究其在强化学习中的作用。

Method: 在MCTS和DQN算法中研究内存分配对性能的影响，比较情景学习和持续学习设置。

Result: 不同内存分配策略对智能体性能有显著影响，具体表现因算法和学习设置而异。

Conclusion: 内存分配是强化学习中的关键因素，需根据算法和学习任务优化分配策略。

Abstract: Resource constraints can fundamentally change both learning and
decision-making. We explore how memory constraints influence an agent's
performance when navigating unknown environments using standard reinforcement
learning algorithms. Specifically, memory-constrained agents face a dilemma:
how much of their limited memory should be allocated to each of the agent's
internal processes, such as estimating a world model, as opposed to forming a
plan using that model? We study this dilemma in MCTS- and DQN-based algorithms
and examine how different allocations of memory impact performance in episodic
and continual learning settings.

</details>


### [330] [OAT-Rephrase: Optimization-Aware Training Data Rephrasing for Zeroth-Order LLM Fine-Tuning](https://arxiv.org/abs/2506.17264)
*Jikai Long,Zijian Hu,Xiaodong Yu,Jianwen Xie,Zhaozhuo Xu*

Main category: cs.LG

TL;DR: OAT-Rephrase通过优化感知的数据重述策略，提升零阶优化（ZO）在大型语言模型（LLM）微调中的性能，缩小与一阶方法的差距。


<details>
  <summary>Details</summary>
Motivation: 零阶优化（ZO）在LLM微调中内存效率高，但收敛慢且优化不稳定。OAT-Rephrase旨在通过优化感知的数据重述解决这些问题。

Method: 提出OAT-Rephrase策略，利用LLM根据ZO动态（如MeZO）重述训练数据，包含重写LLM和语义判断的双阶段流程。

Result: 在五个分类任务和三种LLM架构上，OAT-Rephrase显著提升MeZO微调性能，接近或达到一阶方法水平。

Conclusion: 优化感知的数据重述是零阶调优的低开销、可复用增强方法。

Abstract: Fine-tuning large language models (LLMs) using zeroth-order optimization (ZO)
offers a memory-efficient alternative to gradient-based methods but suffers
from slower convergence and unstable optimization due to noisy gradient
estimates. This paper introduces OAT-Rephrase, an Optimization-Aware Training
data rephrasing strategy that leverages an LLM to rephrase training instances
based on its understanding of the ZO dynamics, specifically MeZO, derived
directly from its paper. The approach incorporates a dual-stage pipeline
featuring a rewriter LLM and a semantic judge, ensuring all rephrasings retain
task relevance and logical consistency. Evaluations across five classification
tasks and three LLM architectures demonstrate that OAT-Rephrase consistently
improves MeZO fine-tuning performance, often narrowing or eliminating the gap
with first-order methods. Our findings suggest that optimization-aware
rephrasing serves as a reusable and low-overhead enhancement for zeroth-order
tuning regimes.

</details>


### [331] [Does Multimodal Large Language Model Truly Unlearn? Stealthy MLLM Unlearning Attack](https://arxiv.org/abs/2506.17265)
*Xianren Zhang,Hui Liu,Delvin Ce Zhang,Xianfeng Tang,Qi He,Dongwon Lee,Suhang Wang*

Main category: cs.LG

TL;DR: 论文提出了一种针对多模态大语言模型（MLLMs）的遗忘攻击方法（SUA），通过生成通用噪声模式恢复被遗忘的敏感信息。


<details>
  <summary>Details</summary>
Motivation: MLLMs可能记忆敏感信息，现有遗忘方法无法确保信息真正被遗忘，可能存在隐藏风险。

Method: 提出SUA框架，学习通用噪声模式，触发模型泄露未遗忘内容；引入嵌入对齐损失以提高隐蔽性。

Result: 实验表明SUA能有效恢复未遗忘信息，且噪声模式具有泛化能力。

Conclusion: 知识重现是MLLMs遗忘后的普遍行为，而非偶然现象。

Abstract: Multimodal Large Language Models (MLLMs) trained on massive data may memorize
sensitive personal information and photos, posing serious privacy risks. To
mitigate this, MLLM unlearning methods are proposed, which fine-tune MLLMs to
reduce the ``forget'' sensitive information. However, it remains unclear
whether the knowledge has been truly forgotten or just hidden in the model.
Therefore, we propose to study a novel problem of LLM unlearning attack, which
aims to recover the unlearned knowledge of an unlearned LLM. To achieve the
goal, we propose a novel framework Stealthy Unlearning Attack (SUA) framework
that learns a universal noise pattern. When applied to input images, this noise
can trigger the model to reveal unlearned content. While pixel-level
perturbations may be visually subtle, they can be detected in the semantic
embedding space, making such attacks vulnerable to potential defenses. To
improve stealthiness, we introduce an embedding alignment loss that minimizes
the difference between the perturbed and denoised image embeddings, ensuring
the attack is semantically unnoticeable. Experimental results show that SUA can
effectively recover unlearned information from MLLMs. Furthermore, the learned
noise generalizes well: a single perturbation trained on a subset of samples
can reveal forgotten content in unseen images. This indicates that knowledge
reappearance is not an occasional failure, but a consistent behavior.

</details>


### [332] [CF-VLM:CounterFactual Vision-Language Fine-tuning](https://arxiv.org/abs/2506.17267)
*Jusheng Zhang,Kaitong Cai,Yijia Fan,Jian Wang,Keze Wang*

Main category: cs.LG

TL;DR: CF-VLM通过引入反事实样本增强视觉语言模型的因果推理能力，显著提升了细粒度判别和深度因果推理任务的表现。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）在细粒度判别和深度因果推理任务中存在局限性，主要依赖表面统计相关性，缺乏捕捉视觉与文本内容间因果逻辑的能力。

Method: 提出CF-VLM框架，通过三种互补的训练目标：保持跨模态对齐基础、强化事实场景表示的唯一性和稳定性、提升对关键因果编辑的敏感性。

Result: CF-VLM在组合推理和泛化基准测试中表现优于基线方法和现有先进方法，并能有效减少视觉幻觉，提高事实一致性。

Conclusion: CF-VLM为高风险的现实场景提供了可靠的推理和可解释性基础。

Abstract: Recent advances in vision-language models (VLMs) have greatly improved
cross-modal semantic understanding, yet significant limitations remain in
fine-grained discrimination and deep causal reasoning tasks. Existing VLMs
often rely on superficial statistical correlations, lacking the ability to
capture the underlying causal logic between visual and textual content. To
address this, we propose CounterFactual Vision-Language Fine-tuning (CF-VLM), a
novel framework that enhances the causal reasoning capabilities of VLMs through
the targeted use of counterfactual samples. CF-VLM introduces three
complementary training objectives: maintaining foundational cross-modal
alignment, reinforcing the uniqueness and stability of factual scene
representations against coherent counterfactuals, and sharpening the model's
sensitivity to minimal but critical causal edits. Extensive experiments
demonstrate that CF-VLM consistently outperforms strong baselines and
state-of-the-art methods on compositional reasoning and generalization
benchmarks. Furthermore, it shows promise in mitigating visual hallucinations,
indicating improved factual consistency. Our CF-VLM provides a robust
foundation for deploying VLMs in high-stakes, real-world scenarios requiring
reliable reasoning and interpretability.

</details>


### [333] [SafeRL-Lite: A Lightweight, Explainable, and Constrained Reinforcement Learning Library](https://arxiv.org/abs/2506.17297)
*Satyam Mishra,Phung Thao Vi,Shivam Mishra,Vishwanath Bijalwan,Vijay Bhaskar Semwal,Abdul Manan Khan*

Main category: cs.LG

TL;DR: SafeRL-Lite是一个开源的Python库，用于构建具有约束和可解释性的强化学习（RL）代理。


<details>
  <summary>Details</summary>
Motivation: 现有RL工具包通常缺乏强制执行硬安全约束或生成人类可理解的决策解释的机制。

Method: SafeRL-Lite通过模块化封装标准Gym环境和深度Q学习代理，支持安全感知训练和实时后验解释。

Result: 在CartPole的约束变体上展示了有效性，并通过可视化揭示了策略逻辑和安全性。

Conclusion: SafeRL-Lite是一个轻量级、可扩展的库，可通过pip安装，适用于需要安全约束和可解释性的RL任务。

Abstract: We introduce SafeRL-Lite, an open-source Python library for building
reinforcement learning (RL) agents that are both constrained and explainable.
Existing RL toolkits often lack native mechanisms for enforcing hard safety
constraints or producing human-interpretable rationales for decisions.
SafeRL-Lite provides modular wrappers around standard Gym environments and deep
Q-learning agents to enable: (i) safety-aware training via constraint
enforcement, and (ii) real-time post-hoc explanation via SHAP values and
saliency maps. The library is lightweight, extensible, and installable via pip,
and includes built-in metrics for constraint violations. We demonstrate its
effectiveness on constrained variants of CartPole and provide visualizations
that reveal both policy logic and safety adherence. The full codebase is
available at: https://github.com/satyamcser/saferl-lite.

</details>


### [334] [AlgoSelect: Universal Algorithm Selection via the Comb Operator](https://arxiv.org/abs/2506.17304)
*Jasper Yao*

Main category: cs.LG

TL;DR: AlgoSelect是一个基于Comb Operator的框架，用于从数据中学习最优算法选择，具有通用性、信息理论最优性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 解决自动化算法选择问题，提供理论保证和实际部署的解决方案。

Method: 使用Comb Operator和N-Path Comb，通过sigmoid门控选择器在算法之间插值。

Result: 在20×20问题-算法研究中实现99.9%+的准确率，样本需求少且收敛快。

Conclusion: AlgoSelect为AI和自适应系统提供了理论支持的高效算法选择方案。

Abstract: We introduce AlgoSelect, a principled framework for learning optimal
algorithm selection from data, centered around the novel Comb Operator. Given a
set of algorithms and a feature representation of problems, AlgoSelect learns
to interpolate between diverse computational approaches. For pairs of
algorithms, a simple sigmoid-gated selector, an instance of the Comb Operator,
facilitates this interpolation. We extend this to an N-Path Comb for multiple
algorithms. We prove that this framework is universal (can approximate any
algorithm selector), information-theoretically optimal in its learnability
(thresholds for selection converge almost surely, demonstrated via
Borel-Cantelli arguments), computationally efficient, and robust. Key
theoretical contributions include: (1) a universal approximation theorem
demonstrating that Comb-based selectors can achieve arbitrary accuracy; (2)
information-theoretic learnability for selection thresholds; (3) formalization
of the Comb Operator within linear operator theory, detailing its boundedness
and spectral properties; (4) an N-Path Comb generalization for multi-algorithm
selection; and (5) a practical learning framework for the adaptive seeding
functions that guide the Comb Operator. Empirical validation on a comprehensive
20$\times$20 problem-algorithm study demonstrates near-perfect selection
(99.9\%+ accuracy) with remarkably few samples and rapid convergence, revealing
that $H(\text{Algorithm}|\text{Problem}) \approx 0$ in structured domains.
AlgoSelect provides a theoretically grounded, practically deployable solution
to automated algorithm selection with provable optimality and learnability
guarantees, with significant implications for AI and adaptive systems.

</details>


### [335] [I Know Which LLM Wrote Your Code Last Summer: LLM generated Code Stylometry for Authorship Attribution](https://arxiv.org/abs/2506.17323)
*Tamas Bisztray,Bilel Cherif,Richard A. Dubniczky,Nils Gruschka,Bertalan Borsos,Mohamed Amine Ferrag,Attila Kovacs,Vasileios Mavroeidis,Norbert Tihanyi*

Main category: cs.LG

TL;DR: 本文提出了一种名为CodeT5-Authorship的新模型，用于识别C程序代码的作者（即生成代码的LLM），并发布了LLM-AuthorBench基准测试。


<details>
  <summary>Details</summary>
Motivation: 随着LLM生成的代码日益普遍，识别代码背后的具体模型变得重要。

Method: 使用CodeT5的编码器层，丢弃解码器，专注于分类任务，通过两层分类头输出概率分布。

Result: 在二元分类中准确率达97.56%，多类分类中达95.40%。

Conclusion: CodeT5-Authorship在LLM作者归属任务中表现优异，支持开源科学。

Abstract: Detecting AI-generated code, deepfakes, and other synthetic content is an
emerging research challenge. As code generated by Large Language Models (LLMs)
becomes more common, identifying the specific model behind each sample is
increasingly important. This paper presents the first systematic study of LLM
authorship attribution for C programs. We released CodeT5-Authorship, a novel
model that uses only the encoder layers from the original CodeT5
encoder-decoder architecture, discarding the decoder to focus on
classification. Our model's encoder output (first token) is passed through a
two-layer classification head with GELU activation and dropout, producing a
probability distribution over possible authors. To evaluate our approach, we
introduce LLM-AuthorBench, a benchmark of 32,000 compilable C programs
generated by eight state-of-the-art LLMs across diverse tasks. We compare our
model to seven traditional ML classifiers and eight fine-tuned transformer
models, including BERT, RoBERTa, CodeBERT, ModernBERT, DistilBERT, DeBERTa-V3,
Longformer, and LoRA-fine-tuned Qwen2-1.5B. In binary classification, our model
achieves 97.56% accuracy in distinguishing C programs generated by closely
related models such as GPT-4.1 and GPT-4o, and 95.40% accuracy for multi-class
attribution among five leading LLMs (Gemini 2.5 Flash, Claude 3.5 Haiku,
GPT-4.1, Llama 3.3, and DeepSeek-V3). To support open science, we release the
CodeT5-Authorship architecture, the LLM-AuthorBench benchmark, and all relevant
Google Colab scripts on GitHub: https://github.com/LLMauthorbench/.

</details>


### [336] [FaithfulSAE: Towards Capturing Faithful Features with Sparse Autoencoders without External Dataset Dependencies](https://arxiv.org/abs/2506.17673)
*Seonglae Cho,Harryn Oh,Donghyun Lee,Luis Eduardo Rodrigues Vieira,Andrew Bermingham,Ziad El Sayed*

Main category: cs.LG

TL;DR: FaithfulSAE通过使用模型自身生成的数据集训练稀疏自编码器（SAE），解决了传统SAE因外部数据集导致的特征不稳定和虚假特征问题。


<details>
  <summary>Details</summary>
Motivation: 传统SAE在外部数据集上训练可能导致特征不稳定和虚假特征，影响模型内部特征的可解释性。

Method: 提出FaithfulSAE方法，使用模型自身合成的数据集训练SAE，减少分布外数据的影响。

Result: FaithfulSAE在种子稳定性和特征真实性上表现优于传统SAE，在5/7模型中虚假特征比例更低。

Conclusion: FaithfulSAE消除了对外部数据集的依赖，提升了模型内部特征的可解释性，并强调了SAE训练数据集的重要性。

Abstract: Sparse Autoencoders (SAEs) have emerged as a promising solution for
decomposing large language model representations into interpretable features.
However, Paulo and Belrose (2025) have highlighted instability across different
initialization seeds, and Heap et al. (2025) have pointed out that SAEs may not
capture model-internal features. These problems likely stem from training SAEs
on external datasets - either collected from the Web or generated by another
model - which may contain out-of-distribution (OOD) data beyond the model's
generalisation capabilities. This can result in hallucinated SAE features,
which we term "Fake Features", that misrepresent the model's internal
activations. To address these issues, we propose FaithfulSAE, a method that
trains SAEs on the model's own synthetic dataset. Using FaithfulSAEs, we
demonstrate that training SAEs on less-OOD instruction datasets results in SAEs
being more stable across seeds. Notably, FaithfulSAEs outperform SAEs trained
on web-based datasets in the SAE probing task and exhibit a lower Fake Feature
Ratio in 5 out of 7 models. Overall, our approach eliminates the dependency on
external datasets, advancing interpretability by better capturing
model-internal features while highlighting the often neglected importance of
SAE training datasets.

</details>


### [337] [Beyond instruction-conditioning, MoTE: Mixture of Task Experts for Multi-task Embedding Models](https://arxiv.org/abs/2506.17781)
*Miguel Romero,Shuoyang Ding,Corey D. Barret,Georgiana Dinu,George Karypis*

Main category: cs.LG

TL;DR: 论文提出了一种名为MoTE的混合任务专家块，通过任务感知对比学习提升低容量模型生成专用嵌入的能力，性能显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有指令调节方法在低容量模型中存在表示限制，限制了性能提升，需要新的方法解决这一问题。

Method: 引入Mixture of Task Experts (MoTE)块，结合任务感知对比学习（TACL）训练任务专用参数。

Result: MoTE在检索数据集上性能提升64%（3.27→5.21），所有数据集上提升43%（1.81→2.60）。

Conclusion: MoTE在不改变指令、训练数据、推理时间或参数数量的情况下显著提升了性能。

Abstract: Dense embeddings are fundamental to modern machine learning systems, powering
Retrieval-Augmented Generation (RAG), information retrieval, and representation
learning. While instruction-conditioning has become the dominant approach for
embedding specialization, its direct application to low-capacity models imposes
fundamental representational constraints that limit the performance gains
derived from specialization. In this paper, we analyze these limitations and
introduce the Mixture of Task Experts (MoTE) transformer block, which leverages
task-specialized parameters trained with Task-Aware Contrastive Learning
(\tacl) to enhance the model ability to generate specialized embeddings.
Empirical results show that MoTE achieves $64\%$ higher performance gains in
retrieval datasets ($+3.27 \rightarrow +5.21$) and $43\%$ higher performance
gains across all datasets ($+1.81 \rightarrow +2.60$). Critically, these gains
are achieved without altering instructions, training data, inference time, or
number of active parameters.

</details>


### [338] [Aligning Frozen LLMs by Reinforcement Learning: An Iterative Reweight-then-Optimize Approach](https://arxiv.org/abs/2506.17828)
*Xinnan Zhang,Chenliang Li,Siliang Zeng,Jiaxiang Li,Zhongruo Wang,Kaixiang Lin,Songtao Lu,Alfredo Garcia,Mingyi Hong*

Main category: cs.LG

TL;DR: 提出了一种名为IRO的方法，通过强化学习框架在不更新模型参数的情况下对齐大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法如RLHF和DPO需要直接优化模型参数，无法在测试时使用或应用于无法访问模型权重的情况，而测试时方法则因高推理成本和基于不完美奖励函数导致输出次优。

Method: IRO通过迭代重加权优化框架，训练轻量级价值函数来指导模型生成，无需更新模型参数。

Result: IRO能够在测试时通过搜索优化过程指导模型生成，且用户可在不访问模型权重的情况下对齐模型。

Conclusion: IRO提供了一种高效且灵活的对齐方法，适用于无法访问模型权重或需要在测试时优化的场景。

Abstract: Aligning large language models (LLMs) with human preferences usually requires
fine-tuning methods such as RLHF and DPO. These methods directly optimize the
model parameters, so they cannot be used in test-time to improve model
performance, nor are they applicable when the model weights are not accessible.
In contrast, test-time methods sidestep weight updates by leveraging reward
functions to guide and improve output quality. However, they incur high
inference costs, and their one-shot guidance is often based on imperfect reward
or value functions, leading to suboptimal outputs. In this work, we present a
method named Iterative Reweight-then-Optimize (IRO), a reinforcement learning
(RL) framework that performs RL-style alignment of the (frozen) base model
without touching its parameters. During training, each iteration (i) samples
candidates from the base model, (ii) resamples using current value functions,
and (iii) trains a new lightweight value function that guides the next decoding
pass. At test time, the value functions are used to guide the base model
generation via a search-based optimization process. Notably, users can apply
IRO to align a model on their own dataset, similar to OpenAI's reinforcement
fine-tuning (RFT), but without requiring access to the model weights.

</details>


### [339] [Adaptive Social Metaverse Streaming based on Federated Multi-Agent Deep Reinforcement Learning](https://arxiv.org/abs/2506.17342)
*Zijian Long,Haopeng Wang,Haiwei Dong,Abdulmotaleb El Saddik*

Main category: cs.LG

TL;DR: ASMS（自适应社交元宇宙流媒体系统）通过结合联邦学习和深度强化学习，动态调整流媒体比特率，提升用户体验14%，同时保护用户隐私。


<details>
  <summary>Details</summary>
Motivation: 社交元宇宙中隐私保护和高质量低延迟流媒体的需求是主要挑战。

Method: 提出ASMS系统，基于联邦多智能体近端策略优化（F-MAPPO），结合联邦学习和深度强化学习。

Result: 实验显示ASMS在多种网络条件下比现有方法提升用户体验至少14%。

Conclusion: ASMS在动态和资源受限网络中提供无缝沉浸式流媒体，同时保护用户隐私。

Abstract: The social metaverse is a growing digital ecosystem that blends virtual and
physical worlds. It allows users to interact socially, work, shop, and enjoy
entertainment. However, privacy remains a major challenge, as immersive
interactions require continuous collection of biometric and behavioral data. At
the same time, ensuring high-quality, low-latency streaming is difficult due to
the demands of real-time interaction, immersive rendering, and bandwidth
optimization. To address these issues, we propose ASMS (Adaptive Social
Metaverse Streaming), a novel streaming system based on Federated Multi-Agent
Proximal Policy Optimization (F-MAPPO). ASMS leverages F-MAPPO, which
integrates federated learning (FL) and deep reinforcement learning (DRL) to
dynamically adjust streaming bit rates while preserving user privacy.
Experimental results show that ASMS improves user experience by at least 14%
compared to existing streaming methods across various network conditions.
Therefore, ASMS enhances the social metaverse experience by providing seamless
and immersive streaming, even in dynamic and resource-constrained networks,
while ensuring that sensitive user data remains on local devices.

</details>


### [340] [AdapThink: Adaptive Thinking Preferences for Reasoning Language Model](https://arxiv.org/abs/2506.18237)
*Xu Wan,Wei Wang,Wenyue Xu,Wotao Yin,Jie Song,Mingyang Sun*

Main category: cs.LG

TL;DR: AdapThink是一个自适应后训练框架，通过动态调整奖励函数和多样性采样机制，提升语言模型的推理效率，同时保持性能。


<details>
  <summary>Details</summary>
Motivation: 传统RL后训练方法在推理效率上存在不足，无法适应不同问题复杂度和模型能力的变化。

Method: AdapThink采用基于模型置信度和响应特征的动态奖励函数，以及熵引导的多样性采样机制。

Result: 在多个数学推理数据集上的实验表明，AdapThink能有效提升推理效率并减少低效问题。

Conclusion: AdapThink通过自适应机制优化了语言模型的推理效率，为复杂推理任务提供了更高效的解决方案。

Abstract: Reinforcement Learning (RL)-based post-training has significantly advanced
the complex reasoning capabilities of language models, fostering sophisticated
self-reflection processes. However, this ``slow thinking'' paradigm presents a
critical challenge to reasoning efficiency: models may expend excessive
computation on simple questions and shift reasoning prematurely for complex
ones. Previous mechanisms typically rely on static length budgets or predefined
rules, lacking the adaptability for varying question complexities and models'
evolving capabilities. To this end, we propose AdapThink, an adaptive
post-training framework designed to induce more efficient thinking while
maintaining the performance of reasoning language models. Specifically,
AdapThink incorporates two key mechanisms: 1) A group-relative reward function
that leverages model confidence and response's characteristic to dynamically
adjust the preference of reflection-related transition words without resorting
to a fixed length preference. 2) A diversity-aware sampling mechanism that
balances the training group's solution accuracy with reasoning diversity via an
entropy-guided score. Experiments on several mathematical reasoning datasets
with DeepSeek-distilled models demonstrate AdapThink's advantages in enabling
adaptive reasoning patterns and mitigating the inefficiencies.

</details>


### [341] [RLPR: Extrapolating RLVR to General Domains without Verifiers](https://arxiv.org/abs/2506.18254)
*Tianyu Yu,Bo Ji,Shouli Wang,Shu Yao,Zefan Wang,Ganqu Cui,Lifan Yuan,Ning Ding,Yuan Yao,Zhiyuan Liu,Maosong Sun,Tat-Seng Chua*

Main category: cs.LG

TL;DR: RLPR是一种无需验证器的强化学习框架，利用LLM自身生成答案的概率作为奖励信号，显著提升了通用领域的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法依赖领域特定验证器，复杂且难以扩展，限制了其在通用领域的应用。

Method: 提出RLPR框架，利用LLM生成答案的token概率作为奖励信号，并通过prob-to-reward和稳定化方法降低噪声影响。

Result: 在四个通用领域和三个数学基准测试中，RLPR显著提升了Gemma、Llama和Qwen模型的推理能力，表现优于VeriFree和General-Reasoner。

Conclusion: RLPR通过利用LLM自身概率作为奖励信号，成功扩展了强化学习在通用领域的应用，且无需依赖验证器。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates promising
potential in advancing the reasoning capabilities of LLMs. However, its success
remains largely confined to mathematical and code domains. This primary
limitation stems from the heavy reliance on domain-specific verifiers, which
results in prohibitive complexity and limited scalability. To address the
challenge, our key observation is that LLM's intrinsic probability of
generating a correct free-form answer directly indicates its own evaluation of
the reasoning reward (i.e., how well the reasoning process leads to the correct
answer). Building on this insight, we propose RLPR, a simple verifier-free
framework that extrapolates RLVR to broader general domains. RLPR uses the
LLM's own token probability scores for reference answers as the reward signal
and maximizes the expected reward during training. We find that addressing the
high variance of this noisy probability reward is crucial to make it work, and
propose prob-to-reward and stabilizing methods to ensure a precise and stable
reward from LLM intrinsic probabilities. Comprehensive experiments in four
general-domain benchmarks and three mathematical benchmarks show that RLPR
consistently improves reasoning capabilities in both areas for Gemma, Llama,
and Qwen based models. Notably, RLPR outperforms concurrent VeriFree by 7.6
points on TheoremQA and 7.5 points on Minerva, and even surpasses strong
verifier-model-dependent approaches General-Reasoner by 1.6 average points
across seven benchmarks.

</details>


### [342] [Confucius3-Math: A Lightweight High-Performance Reasoning LLM for Chinese K-12 Mathematics Learning](https://arxiv.org/abs/2506.18330)
*Lixin Wu,Na Cai,Qiao Cheng,Jiachen Wang,Yitao Duan*

Main category: cs.LG

TL;DR: Confucius3-Math是一个14B参数的开源大语言模型，专为数学推理任务设计，能在消费级GPU上高效运行，并在中国K-12数学教育中表现优异。


<details>
  <summary>Details</summary>
Motivation: 通过AI增强教育和知识传播，特别是针对中国K-12学生的数学学习需求。

Method: 采用大规模强化学习（RL）后训练，结合三项技术创新：目标熵正则化、近期样本恢复和策略特定难度加权。

Result: 模型在数学推理任务上达到SOTA性能，优于许多更大规模的模型，且成本低。

Conclusion: 研究表明，低成本构建特定领域的强推理模型是可行的，模型和代码已开源。

Abstract: We introduce Confucius3-Math, an open-source large language model with 14B
parameters that (1) runs efficiently on a single consumer-grade GPU; (2)
achieves SOTA performances on a range of mathematical reasoning tasks,
outperforming many models with significantly larger sizes. In particular, as
part of our mission to enhancing education and knowledge dissemination with AI,
Confucius3-Math is specifically committed to mathematics learning for Chinese
K-12 students and educators. Built via post-training with large-scale
reinforcement learning (RL), Confucius3-Math aligns with national curriculum
and excels at solving main-stream Chinese K-12 mathematical problems with low
cost. In this report we share our development recipe, the challenges we
encounter and the techniques we develop to overcome them. In particular, we
introduce three technical innovations: Targeted Entropy Regularization, Recent
Sample Recovery and Policy-Specific Hardness Weighting. These innovations
encompass a new entropy regularization, a novel data scheduling policy, and an
improved group-relative advantage estimator. Collectively, they significantly
stabilize the RL training, improve data efficiency, and boost performance. Our
work demonstrates the feasibility of building strong reasoning models in a
particular domain at low cost. We open-source our model and code at
https://github.com/netease-youdao/Confucius3-Math.

</details>


### [343] [SAFEx: Analyzing Vulnerabilities of MoE-Based LLMs via Stable Safety-critical Expert Identification](https://arxiv.org/abs/2506.17368)
*Zhenglin Lai,Mengyao Liao,Dong Xu,Zebin Zhao,Zhihang Yuan,Chao Fan,Jianqiang Li,Bingzhe Wu*

Main category: cs.LG

TL;DR: 论文研究了基于Mixture-of-Experts（MoE）的大型语言模型的安全对齐挑战，提出了SAFEx框架，通过Stability-based Expert Selection算法识别和验证安全关键专家模块。实验表明，禁用少量关键专家会显著降低模型的安全性。


<details>
  <summary>Details</summary>
Motivation: MoE模型的独特架构引入了未充分探索的安全对齐挑战，现有策略不适用于MoE特定的漏洞。

Method: 提出SAFEx框架，使用SES算法识别和验证安全关键专家模块，并将其分解为不同功能组。

Result: 实验显示，禁用少量安全关键专家（如Qwen3-MoE中的12个专家）会导致拒绝有害请求的能力显著下降（22%）。

Conclusion: MoE模型的安全性高度依赖于少数关键专家模块，SAFEx框架为MoE模型的安全对齐提供了有效工具。

Abstract: Large language models based on Mixture-of-Experts have achieved substantial
gains in efficiency and scalability, yet their architectural uniqueness
introduces underexplored safety alignment challenges. Existing safety alignment
strategies, predominantly designed for dense models, are ill-suited to address
MoE-specific vulnerabilities. In this work, we formalize and systematically
study MoE model's positional vulnerability - the phenomenon where
safety-aligned behaviors rely on specific expert modules, revealing critical
risks inherent to MoE architectures. To this end, we present SAFEx, an
analytical framework that robustly identifies, characterizes, and validates the
safety-critical experts using a novel Stability-based Expert Selection (SES)
algorithm. Notably, our approach enables the explicit decomposition of
safety-critical experts into distinct functional groups, including those
responsible for harmful content detection and those controlling safe response
generation. Extensive experiments on mainstream MoE models, such as the
recently released Qwen3-MoE, demonstrated that their intrinsic safety
mechanisms heavily rely on a small subset of positional experts. Disabling
these experts significantly compromised the models' ability to refuse harmful
requests. For Qwen3-MoE with 6144 experts (in the FNN layer), we find that
disabling as few as 12 identified safety-critical experts can cause the refusal
rate to drop by 22%, demonstrating the disproportionate impact of a small set
of experts on overall model safety.

</details>


### [344] [SlimMoE: Structured Compression of Large MoE Models via Expert Slimming and Distillation](https://arxiv.org/abs/2506.18349)
*Zichong Li,Chen Liang,Zixuan Zhang,Ilgee Hong,Young Jin Kim,Weizhu Chen,Tuo Zhao*

Main category: cs.LG

TL;DR: SlimMoE框架通过多阶段压缩方法，将大型MoE模型压缩为更小、高效的变体，显著降低内存需求，适合资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 解决MoE架构因内存需求大而难以在资源受限环境中微调或部署的问题。

Method: 采用多阶段压缩框架，通过精简专家和知识转移减少参数数量，避免性能下降。

Result: 成功压缩Phi 3.5-MoE为Phi-mini-MoE和Phi-tiny-MoE，性能优于同类小模型，接近大模型。

Conclusion: 结构化剪枝与分阶段蒸馏结合，为创建高质量紧凑MoE模型提供了有效途径。

Abstract: The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm
for scaling large language models (LLMs) while maintaining inference
efficiency. However, their enormous memory requirements make them prohibitively
expensive to fine-tune or deploy in resource-constrained environments. To
address this challenge, we introduce SlimMoE, a multi-stage compression
framework for transforming large MoE models into much smaller, efficient
variants without incurring the prohibitive costs of training from scratch. Our
method systematically reduces parameter counts by slimming experts and
transferring knowledge through intermediate stages, effectively mitigating the
performance degradation common in one-shot pruning approaches. Using this
framework, we compress Phi 3.5-MoE (41.9B total/6.6B activated parameters) to
create Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE
(3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of
the original model's training data. These compressed models can be fine-tuned
on a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them
highly suitable for academic and resource-limited settings. Our experiments
demonstrate that these compressed models outperform others of similar size and
remain competitive with larger models. For instance, Phi-mini-MoE achieves
similar or better performance to Phi-3-mini using only 2/3 of the activated
parameters and yields comparable MMLU scores to Llama 3.1 8B despite having
significantly lower latency. Our findings demonstrate that structured pruning
combined with staged distillation offers an effective path to creating
high-quality, compact MoE models, paving the way for broader adoption of MoE
architectures. We make our models publicly available at
https://huggingface.co/microsoft/Phi-mini-MoE-instruct and
https://huggingface.co/microsoft/Phi-tiny-MoE-instruct .

</details>


### [345] [No Training Wheels: Steering Vectors for Bias Correction at Inference Time](https://arxiv.org/abs/2506.18598)
*Aviral Gupta,Armaan Sethi,Ameesh Sethi*

Main category: cs.LG

TL;DR: 提出一种无需重新训练的低成本方法，通过计算多数与少数群体激活均值的差异定义“偏置向量”，并在推理时减去该向量以减少分类偏置。


<details>
  <summary>Details</summary>
Motivation: 神经网络分类器在数据分布不均时容易继承类别偏置，现有方法通常需要重新训练或大量计算资源。

Method: 计算多数与少数群体激活均值的差异定义“偏置向量”，在推理时从残差流中减去该向量。

Result: 减少了分类偏置，提升了最差群体准确率。

Conclusion: 展示了在推理时无需训练的廉价方法，有效缓解分类模型中的偏置问题。

Abstract: Neural network classifiers trained on datasets with uneven group
representation often inherit class biases and learn spurious correlations.
These models may perform well on average but consistently fail on atypical
groups. For example, in hair color classification, datasets may over-represent
females with blond hair, reinforcing stereotypes. Although various algorithmic
and data-centric methods have been proposed to address such biases, they often
require retraining or significant compute. In this work, we propose a cheap,
training-free method inspired by steering vectors used to edit behaviors in
large language models. We compute the difference in mean activations between
majority and minority groups to define a "bias vector," which we subtract from
the model's residual stream. This leads to reduced classification bias and
improved worst-group accuracy. We explore multiple strategies for extracting
and applying these vectors in transformer-like classifiers, showing that
steering vectors, traditionally used in generative models, can also be
effective in classification. More broadly, we showcase an extremely cheap,
inference time, training free method to mitigate bias in classification models.

</details>


### [346] [ReDit: Reward Dithering for Improved LLM Policy Optimization](https://arxiv.org/abs/2506.18631)
*Chenxing Wei,Jiarui Yu,Ying Tiffany He,Hande Dong,Yao Shu,Fei Yu*

Main category: cs.LG

TL;DR: ReDit通过向离散奖励信号添加随机噪声，解决了梯度异常、优化不稳定和收敛慢的问题，显著提升了训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 离散奖励可能导致梯度异常、优化不稳定和收敛慢，需要一种方法来解决这些问题。

Method: 提出ReDit方法，通过向离散奖励信号添加随机噪声，提供连续的探索梯度，加速收敛。

Result: 实验表明，ReDit在训练步骤减少90%的情况下达到与GRPO相当的性能，并在相同训练时长下性能提升4%。

Conclusion: ReDit有效解决了离散奖励的问题，提升了训练效率和模型性能，理论分析进一步验证了其优势。

Abstract: DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning
capabilities through its rule-based reward system. While it's a ''perfect''
reward system that effectively mitigates reward hacking, such reward functions
are often discrete. Our experimental observations suggest that discrete rewards
can lead to gradient anomaly, unstable optimization, and slow convergence. To
address this issue, we propose ReDit (Reward Dithering), a method that dithers
the discrete reward signal by adding simple random noise. With this perturbed
reward, exploratory gradients are continuously provided throughout the learning
process, enabling smoother gradient updates and accelerating convergence. The
injected noise also introduces stochasticity into flat reward regions,
encouraging the model to explore novel policies and escape local optima.
Experiments across diverse tasks demonstrate the effectiveness and efficiency
of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO
with only approximately 10% the training steps, and furthermore, still exhibits
a 4% performance improvement over vanilla GRPO when trained for a similar
duration. Visualizations confirm significant mitigation of gradient issues with
ReDit. Moreover, theoretical analyses are provided to further validate these
advantages.

</details>


### [347] [Multi-modal Anchor Gated Transformer with Knowledge Distillation for Emotion Recognition in Conversation](https://arxiv.org/abs/2506.18716)
*Jie Li,Shifei Ding,Lili Guo,Xuan Li*

Main category: cs.LG

TL;DR: 论文提出了一种名为MAGTKD的多模态锚点门控变换器模型，用于对话中的情感识别任务，通过提示学习和知识蒸馏增强模态表示，并在实验中取得了最优性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在多模态情感识别中未能充分考虑模态贡献差异以及帧级对齐引入的高复杂度问题。

Method: 采用提示学习增强文本模态表示，利用知识蒸馏强化较弱模态表示，并设计多模态锚点门控变换器整合多模态表示。

Result: 在IEMOCAP和MELD数据集上验证了知识蒸馏的有效性，并实现了最优的情感识别性能。

Conclusion: MAGTKD模型通过多模态表示增强和有效整合，显著提升了对话情感识别的性能。

Abstract: Emotion Recognition in Conversation (ERC) aims to detect the emotions of
individual utterances within a conversation. Generating efficient and
modality-specific representations for each utterance remains a significant
challenge. Previous studies have proposed various models to integrate features
extracted using different modality-specific encoders. However, they neglect the
varying contributions of modalities to this task and introduce high complexity
by aligning modalities at the frame level. To address these challenges, we
propose the Multi-modal Anchor Gated Transformer with Knowledge Distillation
(MAGTKD) for the ERC task. Specifically, prompt learning is employed to enhance
textual modality representations, while knowledge distillation is utilized to
strengthen representations of weaker modalities. Furthermore, we introduce a
multi-modal anchor gated transformer to effectively integrate utterance-level
representations across modalities. Extensive experiments on the IEMOCAP and
MELD datasets demonstrate the effectiveness of knowledge distillation in
enhancing modality representations and achieve state-of-the-art performance in
emotion recognition. Our code is available at:
https://github.com/JieLi-dd/MAGTKD.

</details>


### [348] [Neural Total Variation Distance Estimators for Changepoint Detection in News Data](https://arxiv.org/abs/2506.18764)
*Csaba Zsolnai,Niels Lörch,Julian Arnold*

Main category: cs.LG

TL;DR: 利用神经网络和“学习混淆”方案检测新闻数据中的变化点，成功识别重大历史事件。


<details>
  <summary>Details</summary>
Motivation: 理解社会动态需要检测公共话语的变化，但高维、稀疏和嘈杂的数据使这一任务具有挑战性。

Method: 通过训练分类器区分不同时间段的新闻文章，利用分类准确率估计内容分布的总变差距离，识别变化点。

Result: 在合成数据集和《卫报》真实数据中验证了方法的有效性，成功检测到9/11、COVID-19大流行等事件。

Conclusion: 该方法无需过多领域知识，能自主发现公共话语的显著变化，适用于新闻、政策分析和危机监测。

Abstract: Detecting when public discourse shifts in response to major events is crucial
for understanding societal dynamics. Real-world data is high-dimensional,
sparse, and noisy, making changepoint detection in this domain a challenging
endeavor. In this paper, we leverage neural networks for changepoint detection
in news data, introducing a method based on the so-called learning-by-confusion
scheme, which was originally developed for detecting phase transitions in
physical systems. We train classifiers to distinguish between articles from
different time periods. The resulting classification accuracy is used to
estimate the total variation distance between underlying content distributions,
where significant distances highlight changepoints. We demonstrate the
effectiveness of this method on both synthetic datasets and real-world data
from The Guardian newspaper, successfully identifying major historical events
including 9/11, the COVID-19 pandemic, and presidential elections. Our approach
requires minimal domain knowledge, can autonomously discover significant shifts
in public discourse, and yields a quantitative measure of change in content,
making it valuable for journalism, policy analysis, and crisis monitoring.

</details>


### [349] [FedNAMs: Performing Interpretability Analysis in Federated Learning Context](https://arxiv.org/abs/2506.17466)
*Amitash Nanda,Sree Bhargavi Balija,Debashis Sahoo*

Main category: cs.LG

TL;DR: 论文提出了一种名为FedNAMs的新方法，将神经加法模型（NAMs）与联邦学习结合，以提升模型的解释性和隐私保护能力。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在解释性和隐私保护方面的挑战。

Method: 采用神经加法模型（NAMs）在联邦学习框架中，通过分散训练实现特征特定的学习。

Result: 在多个数据集上验证了FedNAMs在解释性和准确性上的优势，同时识别了关键预测特征。

Conclusion: FedNAMs在隐私保护、模型效率和解释性方面表现出色，适用于金融和医疗等领域。

Abstract: Federated learning continues to evolve but faces challenges in
interpretability and explainability. To address these challenges, we introduce
a novel approach that employs Neural Additive Models (NAMs) within a federated
learning framework. This new Federated Neural Additive Models (FedNAMs)
approach merges the advantages of NAMs, where individual networks concentrate
on specific input features, with the decentralized approach of federated
learning, ultimately producing interpretable analysis results. This integration
enhances privacy by training on local data across multiple devices, thereby
minimizing the risks associated with data centralization and improving model
robustness and generalizability. FedNAMs maintain detailed, feature-specific
learning, making them especially valuable in sectors such as finance and
healthcare. They facilitate the training of client-specific models to integrate
local updates, preserve privacy, and mitigate concerns related to
centralization. Our studies on various text and image classification tasks,
using datasets such as OpenFetch ML Wine, UCI Heart Disease, and Iris, show
that FedNAMs deliver strong interpretability with minimal accuracy loss
compared to traditional Federated Deep Neural Networks (DNNs). The research
involves notable findings, including the identification of critical predictive
features at both client and global levels. Volatile acidity, sulfates, and
chlorides for wine quality. Chest pain type, maximum heart rate, and number of
vessels for heart disease. Petal length and width for iris classification. This
approach strengthens privacy and model efficiency and improves interpretability
and robustness across diverse datasets. Finally, FedNAMs generate insights on
causes of highly and low interpretable features.

</details>


### [350] [A Survey of State Representation Learning for Deep Reinforcement Learning](https://arxiv.org/abs/2506.17518)
*Ayoub Echchahed,Pablo Samuel Castro*

Main category: cs.LG

TL;DR: 该论文综述了强化学习中状态表示学习方法的分类，探讨了六类方法的机制、优势和局限，旨在为研究者提供指导。


<details>
  <summary>Details</summary>
Motivation: 解决复杂观测空间在序列决策问题中的挑战，提升样本效率、泛化能力和性能。

Method: 对模型无关的在线设置下的状态表示学习方法进行分类，详细分析六类方法的机制。

Result: 提出了一个分类框架，帮助理解不同方法的优缺点，并讨论了表示质量的评估技术。

Conclusion: 该综述为研究者提供了状态表示学习领域的清晰指南，并指出了未来研究方向。

Abstract: Representation learning methods are an important tool for addressing the
challenges posed by complex observations spaces in sequential decision making
problems. Recently, many methods have used a wide variety of types of
approaches for learning meaningful state representations in reinforcement
learning, allowing better sample efficiency, generalization, and performance.
This survey aims to provide a broad categorization of these methods within a
model-free online setting, exploring how they tackle the learning of state
representations differently. We categorize the methods into six main classes,
detailing their mechanisms, benefits, and limitations. Through this taxonomy,
our aim is to enhance the understanding of this field and provide a guide for
new researchers. We also discuss techniques for assessing the quality of
representations, and detail relevant future directions.

</details>


### [351] [Accelerating Residual Reinforcement Learning with Uncertainty Estimation](https://arxiv.org/abs/2506.17564)
*Lakshita Dodeja,Karl Schmeckpeper,Shivam Vats,Thomas Weng,Mingxi Jia,George Konidaris,Stefanie Tellex*

Main category: cs.LG

TL;DR: 本文提出两种改进残差强化学习的方法，提升样本效率并适用于随机基础策略。


<details>
  <summary>Details</summary>
Motivation: 现有残差强化学习方法在稀疏奖励和随机基础策略上表现不佳。

Method: 利用基础策略的不确定性估计优化探索区域，改进离策略残差学习以观察基础动作。

Result: 在多个仿真基准环境中显著优于现有基线，并实现零样本仿真到现实的迁移。

Conclusion: 改进的残差强化学习方法在样本效率和适应性上表现优异，适用于实际部署。

Abstract: Residual Reinforcement Learning (RL) is a popular approach for adapting
pretrained policies by learning a lightweight residual policy that provides
corrective actions. While Residual RL is more sample-efficient than finetuning
the entire base policy, existing methods struggle with sparse rewards and are
designed for deterministic base policies. We propose two improvements to
Residual RL that further enhance its sample efficiency and make it suitable for
stochastic base policies. First, we leverage uncertainty estimates of the base
policy to focus exploration on regions in which the base policy is not
confident. Second, we propose a simple modification to off-policy residual
learning that allows it to observe base actions and better handle stochastic
base policies. We evaluate our method with both Gaussian-based and
Diffusion-based stochastic base policies on tasks from Robosuite and D4RL, and
compare against state-of-the-art finetuning methods, demo-augmented RL methods,
and other residual RL methods. Our algorithm significantly outperforms existing
baselines in a variety of simulation benchmark environments. We also deploy our
learned polices in the real world to demonstrate their robustness with
zero-shot sim-to-real transfer.

</details>


### [352] [Exploiting Efficiency Vulnerabilities in Dynamic Deep Learning Systems](https://arxiv.org/abs/2506.17621)
*Ravishka Rathnasuriya,Wei Yang*

Main category: cs.LG

TL;DR: 动态深度学习系统（DDLSs）在优化运行时效率的同时，引入了未充分探索的安全风险，如效率攻击。本文研究了这些风险，并提出了针对性防御措施。


<details>
  <summary>Details</summary>
Motivation: 随着深度学习模型在实时环境中的广泛应用，动态深度学习系统（DDLSs）通过输入自适应计算优化效率，但其动态特性带来了潜在的安全风险，如效率攻击。

Method: 通过调查现有攻击策略，分析现代DDLSs的效率漏洞，并开发针对性防御机制。

Result: 研究发现当前系统存在效率漏洞，易受对抗性输入攻击，现有防御机制覆盖不足。

Conclusion: 本文揭示了DDLSs的安全隐患，并提出了增强其对抗条件下鲁棒性的防御方案。

Abstract: The growing deployment of deep learning models in real-world environments has
intensified the need for efficient inference under strict latency and resource
constraints. To meet these demands, dynamic deep learning systems (DDLSs) have
emerged, offering input-adaptive computation to optimize runtime efficiency.
While these systems succeed in reducing cost, their dynamic nature introduces
subtle and underexplored security risks. In particular, input-dependent
execution pathways create opportunities for adversaries to degrade efficiency,
resulting in excessive latency, energy usage, and potential denial-of-service
in time-sensitive deployments. This work investigates the security implications
of dynamic behaviors in DDLSs and reveals how current systems expose efficiency
vulnerabilities exploitable by adversarial inputs. Through a survey of existing
attack strategies, we identify gaps in the coverage of emerging model
architectures and limitations in current defense mechanisms. Building on these
insights, we propose to examine the feasibility of efficiency attacks on modern
DDLSs and develop targeted defenses to preserve robustness under adversarial
conditions.

</details>


### [353] [LLM-Prompt: Integrated Heterogeneous Prompts for Unlocking LLMs in Time Series Forecasting](https://arxiv.org/abs/2506.17631)
*Zesen Wang,Yonggang Li,Lijuan Lan*

Main category: cs.LG

TL;DR: LLM-Prompt框架通过统一文本提示和多模态对齐，提升了时间序列预测的性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的时间序列预测方法存在文本提示不统一和模态差异问题。

Method: 提出LLM-Prompt框架，结合可学习软提示和文本化硬提示，并通过跨模态语义对齐模块融合时间与文本信息。

Result: 在6个公共数据集和3个碳排放数据集上验证了LLM-Prompt的有效性。

Conclusion: LLM-Prompt是一个强大的时间序列预测框架。

Abstract: Time series forecasting aims to model temporal dependencies among variables
for future state inference, holding significant importance and widespread
applications in real-world scenarios. Although deep learning-based methods have
achieved remarkable progress, they still exhibit suboptimal performance in
long-term forecasting and data-scarce scenarios. Recent research demonstrates
that large language models (LLMs) achieve promising performance in time series
forecasting. However, we find existing LLM-based methods still have
shortcomings: (1) the absence of a unified paradigm for textual prompt
formulation and (2) the neglect of modality discrepancies between textual
prompts and time series. To address this, we propose LLM-Prompt, an LLM-based
time series forecasting framework integrating multi-prompt information and
cross-modal semantic alignment. Specifically, we first construct a unified
textual prompt paradigm containing learnable soft prompts and textualized hard
prompts. Second, to enhance LLMs' comprehensive understanding of the
forecasting task, we design a semantic space embedding and cross-modal
alignment module to achieve cross-modal fusion of temporal and textual
information. Finally, the transformed time series from the LLMs are projected
to obtain the forecasts. Comprehensive evaluations on 6 public datasets and 3
carbon emission datasets demonstrate that LLM-Prompt is a powerful framework
for time series forecasting.

</details>


### [354] [Enhancing Stress-Strain Predictions with Seq2Seq and Cross-Attention based on Small Punch Test](https://arxiv.org/abs/2506.17680)
*Zhengni Yang,Rui Yang,Weijian Han,Qixin Liu*

Main category: cs.LG

TL;DR: 提出一种基于深度学习的方法，通过小冲孔试验数据预测高强钢的真实应力-应变曲线，使用GAF和Seq2Seq模型，显著提高了预测精度。


<details>
  <summary>Details</summary>
Motivation: 传统实验方法在预测高强钢的真实应力-应变曲线时效率低且成本高，需要一种更高效、准确的方法。

Method: 利用GAF将载荷-位移序列转化为图像，采用基于LSTM的Seq2Seq模型，并结合多头交叉注意力机制提升精度。

Result: 实验结果显示，最小和最大平均绝对误差分别为0.15 MPa和5.58 MPa，预测精度显著优于传统方法。

Conclusion: 该方法为材料科学提供了一种高效、准确的替代方案，显著提升了真实应力-应变关系的预测能力。

Abstract: This paper introduces a novel deep-learning approach to predict true
stress-strain curves of high-strength steels from small punch test (SPT)
load-displacement data. The proposed approach uses Gramian Angular Field (GAF)
to transform load-displacement sequences into images, capturing
spatial-temporal features and employs a Sequence-to-Sequence (Seq2Seq) model
with an LSTM-based encoder-decoder architecture, enhanced by multi-head
cross-attention to improved accuracy. Experimental results demonstrate that the
proposed approach achieves superior prediction accuracy, with minimum and
maximum mean absolute errors of 0.15 MPa and 5.58 MPa, respectively. The
proposed method offers a promising alternative to traditional experimental
techniques in materials science, enhancing the accuracy and efficiency of true
stress-strain relationship predictions.

</details>


### [355] [Machine Learning Model Integration with Open World Temporal Logic for Process Automation](https://arxiv.org/abs/2506.17776)
*Dyuman Aditya,Colton Payne,Mario Leiva,Paulo Shakarian*

Main category: cs.LG

TL;DR: 论文提出了一种将机器学习模型输出与PyReason框架结合的新方法，以实现复杂工作流中的实时自适应决策。


<details>
  <summary>Details</summary>
Motivation: 解决机器学习模型输出难以转化为复杂工作流中可操作的决策问题。

Method: 通过PyReason框架将ML模型的实值输出（如概率、置信度）转换为逻辑事实，并动态计算最小模型。

Result: 实现了结合ML感知能力和PyReason逻辑推理的自动化系统，支持时间敏感数据和知识图分析。

Conclusion: 该方法在制造、医疗和商业运营等领域具有广泛应用潜力。

Abstract: Recent advancements in Machine Learning (ML) have yielded powerful models
capable of extracting structured information from diverse and complex data
sources. However, a significant challenge lies in translating these perceptual
or extractive outputs into actionable, reasoned decisions within complex
operational workflows. To address these challenges, this paper introduces a
novel approach that integrates the outputs from various machine learning models
directly with the PyReason framework, an open-world temporal logic programming
reasoning engine. PyReason's foundation in generalized annotated logic allows
for the seamless incorporation of real-valued outputs (e.g., probabilities,
confidence scores) from diverse ML models, treating them as truth intervals
within its logical framework. Crucially, PyReason provides mechanisms,
implemented in Python, to continuously poll ML model outputs, convert them into
logical facts, and dynamically recompute the minimal model, ensuring real-tine
adaptive decision-making. Furthermore, its native support for temporal
reasoning, knowledge graph integration, and fully explainable interface traces
enables sophisticated analysis over time-sensitive process data and existing
organizational knowledge. By combining the strengths of perception and
extraction from ML models with the logical deduction and transparency of
PyReason, we aim to create a powerful system for automating complex processes.
This integration finds utility across numerous domains, including
manufacturing, healthcare, and business operations.

</details>


### [356] [Toward Autonomous UI Exploration: The UIExplorer Benchmark](https://arxiv.org/abs/2506.17779)
*Andrei Cristian Nica,Akshaya Vishnu Kudlu Shanbhogue,Harshil Shah,Aleix Cambray,Tudor Berariu,Lucas Maystre,David Barber*

Main category: cs.LG

TL;DR: 论文介绍了UIExplore-Bench，首个专注于UI探索的基准测试，评估代理在结构化模式或屏幕模式下的表现，并提出了hUFO指标。UIExplore-AlGo表现最佳，但仍与人类专家存在差距。


<details>
  <summary>Details</summary>
Motivation: 现有研究缺乏对UI探索阶段的系统评估，而这是自主代理完成任务的关键环节。

Method: 通过UIExplore-Bench基准测试，评估代理在结构化模式（访问布局信息）和屏幕模式（仅依赖GUI观察）下的表现，提出hUFO指标量化探索效果。

Result: UIExplore-AlGo在结构化模式和屏幕模式下分别达到人类性能的77.2%和59.0%，但仍存在显著差距。

Conclusion: UIExplore-Bench为UI探索研究提供了标准化工具，未来仍有改进空间。基准环境、数据集和评估套件已公开。

Abstract: Autonomous agents must know how to explore user interfaces (UIs) for reliable
task solving, yet systematic evaluation of this crucial phase is lacking. We
introduce UIExplore-Bench, the first benchmark explicitly dedicated to UI
exploration. The benchmark evaluates agents with either Structured mode
(granting access to layout information like DOM trees) or Screen mode (relying
on GUI-only observations such as screenshots and human-like mouse/keyboard
interactions) across three levels in a standardized GitLab sandbox environment.
We formalize exploration as the process of maximizing the set of actionable UI
components discovered and propose a metric, human-normalized UI-Functionalities
Observed (hUFO), to quantify the effectiveness of exploration. Our results show
that UIExplore-AlGo achieves the leading mean hUFO scores, reaching up to 77.2%
of human performance in Structured mode and 59.0% in Screen mode at 2,000
steps, particularly excelling at the Sparse level. The results highlight the
relevance of our benchmark, as current agents show a substantial performance
gap compared to one hour of human expert exploration, indicating ample room for
future advancements. We publicly release the benchmark environment, an
exploration dataset, and an evaluation suite to catalyze research into
efficient UI exploration strategies and their downstream applications, such as
experience-driven task completion and automated training data generation.

</details>


### [357] [Reimagining Parameter Space Exploration with Diffusion Models](https://arxiv.org/abs/2506.17807)
*Lijun Zhang,Xiao Liu,Hui Guan*

Main category: cs.LG

TL;DR: 论文提出了一种使用扩散模型生成任务特定参数的方法，避免了传统微调的时间和标注数据依赖。


<details>
  <summary>Details</summary>
Motivation: 传统神经网络适应新任务需要任务特定的微调，耗时且依赖标注数据，因此探索一种生成式替代方案。

Method: 使用扩散模型学习任务特定参数空间的结构，并直接从任务标识生成参数。

Result: 扩散模型能生成准确的任务特定参数，支持多任务插值，但无法泛化到未见任务。

Conclusion: 该方法展示了生成式解决方案的潜力，但也揭示了其局限性。

Abstract: Adapting neural networks to new tasks typically requires task-specific
fine-tuning, which is time-consuming and reliant on labeled data. We explore a
generative alternative that produces task-specific parameters directly from
task identity, eliminating the need for task-specific training. To this end, we
propose using diffusion models to learn the underlying structure of effective
task-specific parameter space and synthesize parameters on demand. Once
trained, the task-conditioned diffusion model can generate specialized weights
directly from task identifiers. We evaluate this approach across three
scenarios: generating parameters for a single seen task, for multiple seen
tasks, and for entirely unseen tasks. Experiments show that diffusion models
can generate accurate task-specific parameters and support multi-task
interpolation when parameter subspaces are well-structured, but fail to
generalize to unseen tasks, highlighting both the potential and limitations of
this generative solution.

</details>


### [358] [Actionable Interpretability via Causal Hypergraphs: Unravelling Batch Size Effects in Deep Learning](https://arxiv.org/abs/2506.17826)
*Zhongtian Sun,Anoushka Harit,Pietro Lio*

Main category: cs.LG

TL;DR: HGCNet提出了一种基于超图的因果框架，用于研究批量大小如何通过梯度噪声、极小值锐度和模型复杂性影响泛化能力。


<details>
  <summary>Details</summary>
Motivation: 探索批量大小在图和文本领域中对泛化能力的因果机制，填补现有研究的空白。

Method: 使用超图捕捉训练动态中的高阶交互，结合深度结构因果模型（DSCMs）和do-calculus量化批量大小的直接和间接效应。

Result: HGCNet在多个数据集上优于基线模型，并揭示较小批量通过增加随机性和平坦极小值提升泛化能力。

Conclusion: HGCNet为深度学习中的训练策略提供了可操作的因果解释，推动了基于解释性的架构和优化选择。

Abstract: While the impact of batch size on generalisation is well studied in vision
tasks, its causal mechanisms remain underexplored in graph and text domains. We
introduce a hypergraph-based causal framework, HGCNet, that leverages deep
structural causal models (DSCMs) to uncover how batch size influences
generalisation via gradient noise, minima sharpness, and model complexity.
Unlike prior approaches based on static pairwise dependencies, HGCNet employs
hypergraphs to capture higher-order interactions across training dynamics.
Using do-calculus, we quantify direct and mediated effects of batch size
interventions, providing interpretable, causally grounded insights into
optimisation. Experiments on citation networks, biomedical text, and e-commerce
reviews show that HGCNet outperforms strong baselines including GCN, GAT,
PI-GNN, BERT, and RoBERTa. Our analysis reveals that smaller batch sizes
causally enhance generalisation through increased stochasticity and flatter
minima, offering actionable interpretability to guide training strategies in
deep learning. This work positions interpretability as a driver of principled
architectural and optimisation choices beyond post hoc analysis.

</details>


### [359] [Causal Spherical Hypergraph Networks for Modelling Social Uncertainty](https://arxiv.org/abs/2506.17840)
*Anoushka Harit,Zhongtian Sun*

Main category: cs.LG

TL;DR: 提出了一种名为Causal-SphHN的框架，用于社会行为预测，结合高阶结构、方向性影响和认知不确定性。


<details>
  <summary>Details</summary>
Motivation: 人类社交行为受复杂交互影响，需建模不确定性、因果性和群体动态。

Method: 使用超球面嵌入表示个体，超边表示群体上下文，结合熵量化不确定性，并通过Granger子图识别因果依赖。

Result: 在多个数据集上表现优于基线，提升了预测准确性、鲁棒性和可解释性。

Conclusion: 提出了一种统一的方法，用于动态社交环境中的不确定性学习。

Abstract: Human social behaviour is governed by complex interactions shaped by
uncertainty, causality, and group dynamics. We propose Causal Spherical
Hypergraph Networks (Causal-SphHN), a principled framework for socially
grounded prediction that jointly models higher-order structure, directional
influence, and epistemic uncertainty. Our method represents individuals as
hyperspherical embeddings and group contexts as hyperedges, capturing semantic
and relational geometry. Uncertainty is quantified via Shannon entropy over von
Mises-Fisher distributions, while temporal causal dependencies are identified
using Granger-informed subgraphs. Information is propagated through an angular
message-passing mechanism that respects belief dispersion and directional
semantics. Experiments on SNARE (offline networks), PHEME (online discourse),
and AMIGOS (multimodal affect) show that Causal-SphHN improves predictive
accuracy, robustness, and calibration over strong baselines. Moreover, it
enables interpretable analysis of influence patterns and social ambiguity. This
work contributes a unified causal-geometric approach for learning under
uncertainty in dynamic social environments.

</details>


### [360] [A Comparative Study of Open-Source Libraries for Synthetic Tabular Data Generation: SDV vs. SynthCity](https://arxiv.org/abs/2506.17847)
*Cristian Del Gobbo*

Main category: cs.LG

TL;DR: 论文研究了六种表格合成数据生成器在低数据量下的表现，评估了统计相似性和预测实用性，发现贝叶斯网络在保真度上表现最佳，而SDV库因其易用性更受推荐。


<details>
  <summary>Details</summary>
Motivation: 高质量训练数据对机器学习模型至关重要，但获取真实数据困难，合成数据生成器提供了一种解决方案。

Method: 使用UCI数据集模拟低数据量场景，评估六种生成器在两种输入输出比例下的表现，通过统计相似性和预测实用性进行衡量。

Result: 贝叶斯网络在保真度上表现最佳，TVAE在预测任务中表现较好；SDV库因文档和易用性更受推荐。

Conclusion: 合成数据生成器在低数据量下表现良好，SDV库因其易用性更适合实际应用。

Abstract: High-quality training data is critical to the performance of machine learning
models, particularly Large Language Models (LLMs). However, obtaining real,
high-quality data can be challenging, especially for smaller organizations and
early-stage startups. Synthetic data generators provide a promising solution by
replicating the statistical and structural properties of real data while
preserving privacy and scalability. This study evaluates the performance of six
tabular synthetic data generators from two widely used open-source libraries:
SDV (Gaussian Copula, CTGAN, TVAE) and Synthicity (Bayesian Network, CTGAN,
TVAE). Using a real-world dataset from the UCI Machine Learning Repository,
comprising energy consumption and environmental variables from Belgium, we
simulate a low-data regime by training models on only 1,000 rows. Each
generator is then tasked with producing synthetic datasets under two
conditions: a 1:1 (1,000 rows) and a 1:10 (10,000 rows) input-output ratio.
Evaluation is conducted using two criteria: statistical similarity, measured
via classical statistics and distributional metrics; and predictive utility,
assessed using a "Train on Synthetic, Test on Real" approach with four
regression models. While statistical similarity remained consistent across
models in both scenarios, predictive utility declined notably in the 1:10 case.
The Bayesian Network from Synthicity achieved the highest fidelity in both
scenarios, while TVAE from SDV performed best in predictive tasks under the
1:10 setting. Although no significant performance gap was found between the two
libraries, SDV stands out for its superior documentation and ease of use,
making it more accessible for practitioners.

</details>


### [361] [Pathway-based Progressive Inference (PaPI) for Energy-Efficient Continual Learning](https://arxiv.org/abs/2506.17848)
*Suyash Gaurav,Jukka Heikkonen,Jatin Chaudhary*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Continual learning systems face the dual challenge of preventing catastrophic
forgetting while maintaining energy efficiency, particularly in
resource-constrained environments. This paper introduces Pathway-based
Progressive Inference (PaPI), a novel theoretical framework that addresses
these challenges through a mathematically rigorous approach to pathway
selection and adaptation. We formulate continual learning as an
energy-constrained optimization problem and provide formal convergence
guarantees for our pathway routing mechanisms. Our theoretical analysis
demonstrates that PaPI achieves an $\mathcal{O}(K)$ improvement in the
stability-plasticity trade-off compared to monolithic architectures, where $K$
is the number of pathways. We derive tight bounds on forgetting rates using
Fisher Information Matrix analysis and prove that PaPI's energy consumption
scales with the number of active parameters rather than the total model size.
Comparative theoretical analysis shows that PaPI provides stronger guarantees
against catastrophic forgetting than Elastic Weight Consolidation (EWC) while
maintaining better energy efficiency than both EWC and Gradient Episodic Memory
(GEM). Our experimental validation confirms these theoretical advantages across
multiple benchmarks, demonstrating PaPI's effectiveness for continual learning
in energy-constrained settings. Our codes are available at
https://github.com/zser092/PAPI_FILES.

</details>


### [362] [In-Context Learning Strategies Emerge Rationally](https://arxiv.org/abs/2506.17859)
*Daniel Wurgaft,Ekdeep Singh Lubana,Core Francisco Park,Hidenori Tanaka,Gautam Reddy,Noah D. Goodman*

Main category: cs.LG

TL;DR: 本文通过贝叶斯框架统一了上下文学习（ICL）中模型行为的多样性策略，解释了模型为何学习这些策略，并提出了一个分层贝叶斯框架来预测Transformer的训练行为。


<details>
  <summary>Details</summary>
Motivation: 研究旨在解释为什么模型在上下文学习中会学习不同的策略，并试图统一这些发现。

Method: 采用认知科学中的理性分析视角，开发了一个分层贝叶斯框架，无需访问模型权重即可预测Transformer的训练行为。

Result: 该框架几乎完美地预测了Transformer的训练行为，并揭示了策略损失与复杂性之间的权衡关系。

Conclusion: 研究提出了一个基于策略损失与复杂性权衡的解释性和预测性框架，为上下文学习提供了新的见解。

Abstract: Recent work analyzing in-context learning (ICL) has identified a broad set of
strategies that describe model behavior in different experimental conditions.
We aim to unify these findings by asking why a model learns these disparate
strategies in the first place. Specifically, we start with the observation that
when trained to learn a mixture of tasks, as is popular in the literature, the
strategies learned by a model for performing ICL can be captured by a family of
Bayesian predictors: a memorizing predictor, which assumes a discrete prior on
the set of seen tasks, and a generalizing predictor, wherein the prior matches
the underlying task distribution. Adopting the lens of rational analysis from
cognitive science, where a learner's behavior is explained as an optimal
adaptation to data given computational constraints, we develop a hierarchical
Bayesian framework that almost perfectly predicts Transformer next token
predictions throughout training without assuming access to its weights. Under
this framework, pretraining is viewed as a process of updating the posterior
probability of different strategies, and its inference-time behavior as a
posterior-weighted average over these strategies' predictions. Our framework
draws on common assumptions about neural network learning dynamics, which make
explicit a tradeoff between loss and complexity among candidate strategies:
beyond how well it explains the data, a model's preference towards implementing
a strategy is dictated by its complexity. This helps explain well-known ICL
phenomena, while offering novel predictions: e.g., we show a superlinear trend
in the timescale for transition to memorization as task diversity is increased.
Overall, our work advances an explanatory and predictive account of ICL
grounded in tradeoffs between strategy loss and complexity.

</details>


### [363] [NestQuant: Post-Training Integer-Nesting Quantization for On-Device DNN](https://arxiv.org/abs/2506.17870)
*Jianhang Xie,Chuntao Ding,Xiaqing Li,Shenyuan Ren,Yidong Li,Zhichao Lu*

Main category: cs.LG

TL;DR: 提出了一种资源友好的后训练整数嵌套量化方法NestQuant，用于物联网设备上的量化模型切换，解决了现有方法的动态资源适应和存储开销问题。


<details>
  <summary>Details</summary>
Motivation: 在物联网设备上部署具有资源适应能力的量化DNN模型，以提供高质量AI服务，但现有动态/混合精度量化方法需要重新训练或特殊硬件，而后训练量化（PTQ）在资源适应方面存在局限性。

Method: 提出NestQuant方法，通过整数权重分解和嵌套机制，将量化权重拆分为高低位整数权重，并优化高位权重以适应资源变化。

Result: 在ImageNet-1K预训练DNN上的实验表明，NestQuant模型在准确性和资源消耗方面表现优异，例如ResNet-101的INT8嵌套INT6模型在切换开销上减少了约78.1%。

Conclusion: NestQuant是一种高效的后训练量化方法，能够在动态资源环境下实现高性能和低开销的模型切换。

Abstract: Deploying quantized deep neural network (DNN) models with resource adaptation
capabilities on ubiquitous Internet of Things (IoT) devices to provide
high-quality AI services can leverage the benefits of compression and meet
multi-scenario resource requirements. However, existing dynamic/mixed precision
quantization requires retraining or special hardware, whereas post-training
quantization (PTQ) has two limitations for resource adaptation: (i) The
state-of-the-art PTQ methods only provide one fixed bitwidth model, which makes
it challenging to adapt to the dynamic resources of IoT devices; (ii) Deploying
multiple PTQ models with diverse bitwidths consumes large storage resources and
switching overheads. To this end, this paper introduces a resource-friendly
post-training integer-nesting quantization, i.e., NestQuant, for on-device
quantized model switching on IoT devices. The proposed NestQuant incorporates
the integer weight decomposition, which bit-wise splits quantized weights into
higher-bit and lower-bit weights of integer data types. It also contains a
decomposed weights nesting mechanism to optimize the higher-bit weights by
adaptive rounding and nest them into the original quantized weights. In
deployment, we can send and store only one NestQuant model and switch between
the full-bit/part-bit model by paging in/out lower-bit weights to adapt to
resource changes and reduce consumption. Experimental results on the
ImageNet-1K pretrained DNNs demonstrated that the NestQuant model can achieve
high performance in top-1 accuracy, and reduce in terms of data transmission,
storage consumption, and switching overheads. In particular, the ResNet-101
with INT8 nesting INT6 can achieve 78.1% and 77.9% accuracy for full-bit and
part-bit models, respectively, and reduce switching overheads by approximately
78.1% compared with diverse bitwidths PTQ models.

</details>


### [364] [Permutation Equivariant Model-based Offline Reinforcement Learning for Auto-bidding](https://arxiv.org/abs/2506.17919)
*Zhiyu Mou,Miao Xu,Wei Chen,Rongquan Bai,Chuan Yu,Jian Xu*

Main category: cs.LG

TL;DR: 论文提出了一种基于模型的强化学习方法（MRLB），通过结合真实数据和模型生成数据来扩展状态覆盖范围，并引入PE-MORL算法提升模型可靠性和策略性能。


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习方法（ORLB）受限于数据集的状态覆盖范围，而仿真强化学习方法（SRLB）存在仿真与现实的差距。MRLB旨在通过模型学习弥合这一差距。

Method: 提出MRLB方法，学习环境模型并结合真实与模型生成数据训练策略。引入PE-MORL算法，包括置换等变模型架构和悲观离线Q学习方法。

Result: 实验表明PE-MORL在自动竞价任务中优于现有方法。

Conclusion: MRLB通过模型学习扩展状态覆盖，PE-MORL算法有效提升策略性能，为自动竞价提供了更优解决方案。

Abstract: Reinforcement learning (RL) for auto-bidding has shifted from using
simplistic offline simulators (Simulation-based RL Bidding, SRLB) to offline RL
on fixed real datasets (Offline RL Bidding, ORLB). However, ORLB policies are
limited by the dataset's state space coverage, offering modest gains. While
SRLB expands state coverage, its simulator-reality gap risks misleading
policies. This paper introduces Model-based RL Bidding (MRLB), which learns an
environment model from real data to bridge this gap. MRLB trains policies using
both real and model-generated data, expanding state coverage beyond ORLB. To
ensure model reliability, we propose: 1) A permutation equivariant model
architecture for better generalization, and 2) A robust offline Q-learning
method that pessimistically penalizes model errors. These form the Permutation
Equivariant Model-based Offline RL (PE-MORL) algorithm. Real-world experiments
show that PE-MORL outperforms state-of-the-art auto-bidding methods.

</details>


### [365] [Learning to Adapt Frozen CLIP for Few-Shot Test-Time Domain Adaptation](https://arxiv.org/abs/2506.17307)
*Zhixiang Chi,Li Gu,Huan Liu,Ziqiang Wang,Yanan Wu,Yang Wang,Konstantinos N Plataniotis*

Main category: cs.LG

TL;DR: 本文提出了一种新方法，通过在输入空间直接学习来补充冻结CLIP的特定数据集知识，显著提升了少样本测试时域适应的性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法仅依赖CLIP特征空间知识的局限性，尤其是在使用较弱骨干网络时性能显著下降的问题。

Method: 引入一个独立的侧分支与CLIP并行，通过反向注意力学习专属知识，并通过贪婪文本集成和细化增强文本特征。

Result: 在5个大规模基准测试中表现优异，特别是在ViT-B/16网络上，iWildCam的F1提高了5.1，FMoW的WC Acc提高了3.1%。

Conclusion: 该方法通过直接学习输入空间知识，显著提升了少样本测试时域适应的性能，尤其在挑战性现实场景中表现突出。

Abstract: Few-shot Test-Time Domain Adaptation focuses on adapting a model at test time
to a specific domain using only a few unlabeled examples, addressing domain
shift. Prior methods leverage CLIP's strong out-of-distribution (OOD) abilities
by generating domain-specific prompts to guide its generalized, frozen
features. However, since downstream datasets are not explicitly seen by CLIP,
solely depending on the feature space knowledge is constrained by CLIP's prior
knowledge. Notably, when using a less robust backbone like ViT-B/16,
performance significantly drops on challenging real-world benchmarks. Departing
from the state-of-the-art of inheriting the intrinsic OOD capability of CLIP,
this work introduces learning directly on the input space to complement the
dataset-specific knowledge for frozen CLIP. Specifically, an independent side
branch is attached in parallel with CLIP and enforced to learn exclusive
knowledge via revert attention. To better capture the dataset-specific label
semantics for downstream adaptation, we propose to enhance the inter-dispersion
among text features via greedy text ensemble and refinement. The text and
visual features are then progressively fused in a domain-aware manner by a
generated domain prompt to adapt toward a specific domain. Extensive
experiments show our method's superiority on 5 large-scale benchmarks (WILDS
and DomainNet), notably improving over smaller networks like ViT-B/16 with
gains of \textbf{+5.1} in F1 for iWildCam and \textbf{+3.1\%} in WC Acc for
FMoW.

</details>


### [366] [ASTER: Adaptive Spatio-Temporal Early Decision Model for Dynamic Resource Allocation](https://arxiv.org/abs/2506.17929)
*Shulun Chen,Wei Shao,Flora D. Salim,Hao Xue*

Main category: cs.LG

TL;DR: 论文提出了一种自适应时空早期决策模型（ASTER），将预测与决策直接结合，优化资源分配和干预策略。


<details>
  <summary>Details</summary>
Motivation: 现有研究在时空预测的及时性和准确性上有所提升，但如何将预测转化为可操作的决策仍是一大挑战。预测与决策阶段的脱节会降低下游效率，例如在应急响应中，资源分配比事件预测更重要。

Method: ASTER引入资源感知时空交互模块（RaST）捕获动态资源条件下的时空依赖，并设计基于多目标强化学习的偏好导向决策代理（Poda），将预测信号转化为资源高效的干预策略。

Result: 在四个基准数据集上的实验表明，ASTER在早期预测准确性和资源分配效果上均达到最优性能。

Conclusion: ASTER通过直接结合预测与决策，显著提升了时空智能在决策支持中的实际效果。

Abstract: Supporting decision-making has long been a central vision in the field of
spatio-temporal intelligence. While prior work has improved the timeliness and
accuracy of spatio-temporal forecasting, converting these forecasts into
actionable strategies remains a key challenge. A main limitation is the
decoupling of the prediction and the downstream decision phases, which can
significantly degrade the downstream efficiency. For example, in emergency
response, the priority is successful resource allocation and intervention, not
just incident prediction. To this end, it is essential to propose an Adaptive
Spatio-Temporal Early Decision model (ASTER) that reforms the forecasting
paradigm from event anticipation to actionable decision support. This framework
ensures that information is directly used for decision-making, thereby
maximizing overall effectiveness. Specifically, ASTER introduces a new
Resource-aware Spatio-Temporal interaction module (RaST) that adaptively
captures long- and short-term dependencies under dynamic resource conditions,
producing context-aware spatiotemporal representations. To directly generate
actionable decisions, we further design a Preference-oriented decision agent
(Poda) based on multi-objective reinforcement learning, which transforms
predictive signals into resource-efficient intervention strategies by deriving
optimal actions under specific preferences and dynamic constraints.
Experimental results on four benchmark datasets demonstrate the
state-of-the-art performance of ASTER in improving both early prediction
accuracy and resource allocation outcomes across six downstream metrics.

</details>


### [367] [Origins of Creativity in Attention-Based Diffusion Models](https://arxiv.org/abs/2506.17324)
*Emma Finn,T. Anderson Keller,Manos Theodosis,Demba E. Ba*

Main category: cs.LG

TL;DR: 论文探讨了扩散模型中创造性的来源，特别是自注意力机制在生成全局一致图像中的作用。


<details>
  <summary>Details</summary>
Motivation: 随着扩散模型在图像生成中的广泛应用，理解其创造性来源变得重要。现有理论未涵盖自注意力机制的作用，本文旨在填补这一空白。

Method: 扩展了现有理论，研究了由CNN加自注意力层参数化的扩散模型，并通过实验验证。

Result: 理论表明自注意力能促进局部特征的全局一致性，实验证实了这一点。

Conclusion: 自注意力机制在扩散模型中提升了生成图像的全局一致性，为理解创造性提供了新视角。

Abstract: As diffusion models have become the tool of choice for image generation and
as the quality of the images continues to improve, the question of how
`creativity' originates in diffusion has become increasingly important. The
score matching perspective on diffusion has proven particularly fruitful for
understanding how and why diffusion models generate images that remain
plausible while differing significantly from their training images. In
particular, as explained in (Kamb \& Ganguli, 2024) and others, e.g.,
(Ambrogioni, 2023), theory suggests that if our score matching were optimal, we
would only be able to recover training samples through our diffusion process.
However, as shown by Kamb \& Ganguli, (2024), in diffusion models where the
score is parametrized by a simple CNN, the inductive biases of the CNN itself
(translation equivariance and locality) allow the model to generate samples
that globally do not match any training samples, but are rather patch-wise
`mosaics'. Notably, however, this theory does not extend to describe the role
of self-attention in this process. In this work, we take a preliminary step in
this direction to extend this theory to the case of diffusion models whose
score is parametrized by a CNN with a final self-attention layer. We show that
our theory suggests that self-attention will induce a globally image-consistent
arrangement of local features beyond the patch-level in generated samples, and
we verify this behavior empirically on a carefully crafted dataset.

</details>


### [368] [An entropy-optimal path to humble AI](https://arxiv.org/abs/2506.17940)
*Davide Bassetti,Lukáš Pospíšil,Michael Groom,Terence J. O'Kane,Illia Horenko*

Main category: cs.LG

TL;DR: 提出了一种基于非平衡熵优化的玻尔兹曼机数学框架，显著降低成本和资源需求，同时提供更高的性能和模型简洁性。


<details>
  <summary>Details</summary>
Motivation: 解决当前AI模型的高成本、资源消耗大以及过度自信的问题。

Method: 基于总概率定律的非平衡熵优化玻尔兹曼机框架，无需梯度下降学习。

Result: 在合成问题和历史气候数据上表现优于现有方法，模型更简洁且预测能力更强。

Conclusion: 该方法在性能和成本上均优于现有工具，尤其适用于气候预测等复杂问题。

Abstract: Progress of AI has led to a creation of very successful, but by no means
humble models and tools, especially regarding (i) the huge and further
exploding costs and resources they demand, and (ii) the over-confidence of
these tools with the answers they provide. Here we introduce a novel
mathematical framework for a non-equilibrium entropy-optimizing reformulation
of Boltzmann machines based on the exact law of total probability. It results
in the highly-performant, but much cheaper, gradient-descent-free learning
framework with mathematically-justified existence and uniqueness criteria, and
answer confidence/reliability measures. Comparisons to state-of-the-art AI
tools in terms of performance, cost and the model descriptor lengths on a set
of synthetic problems with varying complexity reveal that the proposed method
results in more performant and slim models, with the descriptor lengths being
very close to the intrinsic complexity scaling bounds for the underlying
problems. Applying this framework to historical climate data results in models
with systematically higher prediction skills for the onsets of La Ni\~na and El
Ni\~no climate phenomena, requiring just few years of climate data for training
- a small fraction of what is necessary for contemporary climate prediction
tools.

</details>


### [369] [Adapting Vision-Language Models for Evaluating World Models](https://arxiv.org/abs/2506.17967)
*Mariya Hendriksen,Tabish Rashid,David Bignell,Raluca Georgescu,Abdelhak Lemkhenter,Katja Hofmann,Sam Devlin,Sarah Parisot*

Main category: cs.LG

TL;DR: 论文提出了一种基于视觉语言模型（VLM）的评估方法UNIVERSE，用于细粒度、时间敏感的世界模型评估，性能接近任务专用基线且与人类判断一致。


<details>
  <summary>Details</summary>
Motivation: 现有评估指标无法捕捉世界模型生成内容的行为对齐和语义一致性，需要一种更精细的评估方法。

Method: 提出UNIVERSE方法，通过适应VLM在数据和计算限制下进行世界模型评估，包括动作识别和角色识别任务。

Result: UNIVERSE在多种任务格式和设置下表现优异，性能接近任务专用基线，并与人类判断高度一致。

Conclusion: UNIVERSE是一种可扩展、语义感知的世界模型评估工具，填补了现有评估方法的不足。

Abstract: World models -- generative models that simulate environment dynamics
conditioned on past observations and actions -- are gaining prominence in
planning, simulation, and embodied AI. However, evaluating their rollouts
remains a fundamental challenge, requiring fine-grained, temporally grounded
assessment of action alignment and semantic consistency -- capabilities not
captured by existing metrics. Vision-Language Models (VLMs) have shown promise
as automatic evaluators of generative content due to their strong multimodal
reasoning abilities. Yet, their use in fine-grained, temporally sensitive
evaluation tasks remains limited and requires targeted adaptation. We introduce
a evaluation protocol targeting two recognition tasks -- action recognition and
character recognition -- each assessed across binary, multiple-choice, and
open-ended formats. To support this, we present UNIVERSE (UNIfied
Vision-language Evaluator for Rollouts in Simulated Environments), a method for
adapting VLMs to rollout evaluation under data and compute constraints. We
conduct a large-scale study comparing full, partial, and parameter-efficient
finetuning across task formats, context lengths, sampling strategies, and data
compositions. The resulting unified evaluator matches the performance of
task-specific baselines using a single checkpoint. Human studies confirm strong
alignment with human judgments, establishing UNIVERSE as a scalable,
semantics-aware evaluator for world models.

</details>


### [370] [DRIMV_TSK: An Interpretable Surgical Evaluation Model for Incomplete Multi-View Rectal Cancer Data](https://arxiv.org/abs/2506.17552)
*Wei Zhang,Zi Wang,Hanwen Zhou,Zhaohong Deng,Weiping Ding,Yuxi Ge,Te Zhang,Yuanpeng Zhang,Kup-Sze Choi,Shitong Wang,Shudong Hu*

Main category: cs.LG

TL;DR: 本文提出了一种可解释的不完整多视图手术评估模型，用于直肠癌手术难度的可靠评估，结合多视图数据和人工智能技术，取得了最佳效果。


<details>
  <summary>Details</summary>
Motivation: 当前直肠癌手术难度评估主要依赖临床数据，但技术进步提供了更多数据来源，人工智能的应用为更全面的评估提供了可能。

Method: 构建多视图直肠癌数据集，提出双表示不完整多视图学习模型，结合缺失视图填补和二阶相似性约束，并基于TSK模糊系统设计多视图手术评估模型。

Result: 在MVRC数据集上，DRIMV_TSK模型优于其他先进算法，取得了最佳结果。

Conclusion: 提出的模型能够有效整合多视图数据，提升直肠癌手术难度评估的准确性和可靠性。

Abstract: A reliable evaluation of surgical difficulty can improve the success of the
treatment for rectal cancer and the current evaluation method is based on
clinical data. However, more data about rectal cancer can be collected with the
development of technology. Meanwhile, with the development of artificial
intelligence, its application in rectal cancer treatment is becoming possible.
In this paper, a multi-view rectal cancer dataset is first constructed to give
a more comprehensive view of patients, including the high-resolution MRI image
view, pressed-fat MRI image view, and clinical data view. Then, an
interpretable incomplete multi-view surgical evaluation model is proposed,
considering that it is hard to obtain extensive and complete patient data in
real application scenarios. Specifically, a dual representation incomplete
multi-view learning model is first proposed to extract the common information
between views and specific information in each view. In this model, the missing
view imputation is integrated into representation learning, and second-order
similarity constraint is also introduced to improve the cooperative learning
between these two parts. Then, based on the imputed multi-view data and the
learned dual representation, a multi-view surgical evaluation model with the
TSK fuzzy system is proposed. In the proposed model, a cooperative learning
mechanism is constructed to explore the consistent information between views,
and Shannon entropy is also introduced to adapt the view weight. On the MVRC
dataset, we compared it with several advanced algorithms and DRIMV_TSK obtained
the best results.

</details>


### [371] [h-calibration: Rethinking Classifier Recalibration with Probabilistic Error-Bounded Objective](https://arxiv.org/abs/2506.17968)
*Wenjian Huang,Guiping Cao,Jiahao Xia,Jingkun Chen,Hao Wang,Jianguo Zhang*

Main category: cs.LG

TL;DR: 本文提出了一种名为h-calibration的概率学习框架，用于解决深度神经网络输出概率的校准问题，克服了现有方法的十种常见限制，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在许多学习任务中表现出色，但其输出的概率常存在校准问题，导致不可靠。这激发了通过后处理校准方法在不牺牲分类性能的情况下获得校准概率的研究。

Method: 总结了现有方法为三类策略，并提出h-calibration框架，设计了一种简单有效的后处理校准算法。

Result: 该方法不仅克服了现有方法的限制，还在实验中显著优于传统方法，验证了其理论有效性。

Conclusion: h-calibration框架为学习误差有界的校准概率提供了理论支持，并在标准基准测试中实现了最先进的性能，为相关领域提供了有价值的参考。

Abstract: Deep neural networks have demonstrated remarkable performance across numerous
learning tasks but often suffer from miscalibration, resulting in unreliable
probability outputs. This has inspired many recent works on mitigating
miscalibration, particularly through post-hoc recalibration methods that aim to
obtain calibrated probabilities without sacrificing the classification
performance of pre-trained models. In this study, we summarize and categorize
previous works into three general strategies: intuitively designed methods,
binning-based methods, and methods based on formulations of ideal calibration.
Through theoretical and practical analysis, we highlight ten common limitations
in previous approaches. To address these limitations, we propose a
probabilistic learning framework for calibration called h-calibration, which
theoretically constructs an equivalent learning formulation for canonical
calibration with boundedness. On this basis, we design a simple yet effective
post-hoc calibration algorithm. Our method not only overcomes the ten
identified limitations but also achieves markedly better performance than
traditional methods, as validated by extensive experiments. We further analyze,
both theoretically and experimentally, the relationship and advantages of our
learning objective compared to traditional proper scoring rule. In summary, our
probabilistic framework derives an approximately equivalent differentiable
objective for learning error-bounded calibrated probabilities, elucidating the
correspondence and convergence properties of computational statistics with
respect to theoretical bounds in canonical calibration. The theoretical
effectiveness is verified on standard post-hoc calibration benchmarks by
achieving state-of-the-art performance. This research offers valuable reference
for learning reliable likelihood in related fields.

</details>


### [372] [Probing the Embedding Space of Transformers via Minimal Token Perturbations](https://arxiv.org/abs/2506.18011)
*Eddie Conti,Alejandro Astruc,Alvaro Parafita,Axel Brando*

Main category: cs.LG

TL;DR: 研究通过最小化标记扰动对Transformer嵌入空间的影响，验证了深层信息混合现象，并提出了一种新的模型解释工具。


<details>
  <summary>Details</summary>
Motivation: 理解信息在Transformer模型中的传播是解释模型行为的关键挑战。

Method: 通过实验分析最小标记扰动对嵌入空间的影响，观察扰动在层间的传播。

Result: 稀有标记通常导致更大的嵌入空间偏移，深层信息混合现象显著。

Conclusion: 结合标记扰动和嵌入空间偏移是一种强大的模型解释工具，验证了浅层作为解释代理的假设。

Abstract: Understanding how information propagates through Transformer models is a key
challenge for interpretability. In this work, we study the effects of minimal
token perturbations on the embedding space. In our experiments, we analyze the
frequency of which tokens yield to minimal shifts, highlighting that rare
tokens usually lead to larger shifts. Moreover, we study how perturbations
propagate across layers, demonstrating that input information is increasingly
intermixed in deeper layers. Our findings validate the common assumption that
the first layers of a model can be used as proxies for model explanations.
Overall, this work introduces the combination of token perturbations and shifts
on the embedding space as a powerful tool for model interpretability.

</details>


### [373] [Decoding Federated Learning: The FedNAM+ Conformal Revolution](https://arxiv.org/abs/2506.17872)
*Sree Bhargavi Balija,Amitash Nanda,Debashis Sahoo*

Main category: cs.LG

TL;DR: FedNAM+是一个联邦学习框架，结合了神经加法模型和新型的共形预测方法，提供可解释和可靠的预测不确定性估计。


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习框架缺乏结合不确定性量化、可解释性和鲁棒性的综合解决方案。

Method: FedNAM+通过动态级别调整技术和梯度敏感图识别关键输入特征，实现像素级不确定性估计和可解释性。

Result: 在CT扫描、MNIST和CIFAR数据集上验证，预测精度高且损失小（如MNIST仅0.1%），并提供透明的不确定性度量。

Conclusion: FedNAM+是一个鲁棒、可解释且计算高效的框架，提升了分布式预测建模的信任和透明度。

Abstract: Federated learning has significantly advanced distributed training of machine
learning models across decentralized data sources. However, existing frameworks
often lack comprehensive solutions that combine uncertainty quantification,
interpretability, and robustness. To address this, we propose FedNAM+, a
federated learning framework that integrates Neural Additive Models (NAMs) with
a novel conformal prediction method to enable interpretable and reliable
uncertainty estimation. Our method introduces a dynamic level adjustment
technique that utilizes gradient-based sensitivity maps to identify key input
features influencing predictions. This facilitates both interpretability and
pixel-wise uncertainty estimates. Unlike traditional interpretability methods
such as LIME and SHAP, which do not provide confidence intervals, FedNAM+
offers visual insights into prediction reliability. We validate our approach
through experiments on CT scan, MNIST, and CIFAR datasets, demonstrating high
prediction accuracy with minimal loss (e.g., only 0.1% on MNIST), along with
transparent uncertainty measures. Visual analysis highlights variable
uncertainty intervals, revealing low-confidence regions where model performance
can be improved with additional data. Compared to Monte Carlo Dropout, FedNAM+
delivers efficient and global uncertainty estimates with reduced computational
overhead, making it particularly suitable for federated learning scenarios.
Overall, FedNAM+ provides a robust, interpretable, and computationally
efficient framework that enhances trust and transparency in decentralized
predictive modeling.

</details>


### [374] [Pathwise Explanation of ReLU Neural Networks](https://arxiv.org/abs/2506.18037)
*Seongwoo Lim,Won Jo,Joohyung Lee,Jaesik Choi*

Main category: cs.LG

TL;DR: 提出了一种基于决策路径的子集隐藏单元解释方法，提高了神经网络的可解释性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 神经网络的“黑盒”特性引发了对透明性和可靠性的担忧，现有方法基于全部隐藏单元的激活状态，难以清晰解释决策过程。

Method: 通过考虑决策路径中的隐藏单元子集，提供更清晰、一致的输入与决策关系解释，并支持调整解释范围和分解详细解释。

Result: 实验表明，该方法在定量和定性上均优于其他方法。

Conclusion: 该方法显著提升了神经网络的可解释性，为理解决策过程提供了更灵活的工具。

Abstract: Neural networks have demonstrated a wide range of successes, but their
``black box" nature raises concerns about transparency and reliability.
Previous research on ReLU networks has sought to unwrap these networks into
linear models based on activation states of all hidden units. In this paper, we
introduce a novel approach that considers subsets of the hidden units involved
in the decision making path. This pathwise explanation provides a clearer and
more consistent understanding of the relationship between the input and the
decision-making process. Our method also offers flexibility in adjusting the
range of explanations within the input, i.e., from an overall attribution input
to particular components within the input. Furthermore, it allows for the
decomposition of explanations for a given input for more detailed explanations.
Experiments demonstrate that our method outperforms others both quantitatively
and qualitatively.

</details>


### [375] [Distributionally robust minimization in meta-learning for system identification](https://arxiv.org/abs/2506.18074)
*Matteo Rufolo,Dario Piga,Marco Forgione*

Main category: cs.LG

TL;DR: 本文提出了一种基于分布鲁棒优化的元学习方法，用于系统辨识，旨在提高在安全关键应用中的性能。


<details>
  <summary>Details</summary>
Motivation: 标准元学习方法忽略了任务变异性，本文希望通过分布鲁棒优化来优先处理高损失任务，以增强在最坏情况下的性能。

Method: 采用分布鲁棒优化范式，对一类合成动态系统进行元模型训练，并在分布内和分布外设置下进行测试。

Result: 所提出的方法能够减少安全关键应用中的失败情况。

Conclusion: 分布鲁棒优化在元学习中的应用能够有效提升系统辨识的鲁棒性。

Abstract: Meta learning aims at learning how to solve tasks, and thus it allows to
estimate models that can be quickly adapted to new scenarios. This work
explores distributionally robust minimization in meta learning for system
identification. Standard meta learning approaches optimize the expected loss,
overlooking task variability. We use an alternative approach, adopting a
distributionally robust optimization paradigm that prioritizes high-loss tasks,
enhancing performance in worst-case scenarios. Evaluated on a meta model
trained on a class of synthetic dynamical systems and tested in both
in-distribution and out-of-distribution settings, the proposed approach allows
to reduce failures in safety-critical applications.

</details>


### [376] [RL for Reasoning by Adaptively Revealing Rationales](https://arxiv.org/abs/2506.18110)
*Mohammad Hossein Amani,Aryo Lotfi,Nicolas Mario Baldwin,Samy Bengio,Mehrdad Farajtabar,Emmanuel Abbe,Robert West*

Main category: cs.LG

TL;DR: 论文提出了一种基于部分专家演示的强化学习框架（AdaBack），通过动态调整监督长度，解决了长序列生成任务中监督学习和强化学习的局限性。


<details>
  <summary>Details</summary>
Motivation: 解决长序列生成任务中监督学习（SFT）依赖密集标签和强化学习（RL）面临稀疏奖励的问题。

Method: 引入自适应回溯（AdaBack），一种基于样本的课程学习算法，动态调整每个样本的监督长度。

Result: 在合成任务和数学推理基准测试中，AdaBack成功解决了传统方法无法处理的长序列依赖问题。

Conclusion: 基于样本的课程学习在长序列任务中优于传统方法，能够通过逐步学习部分解实现泛化。

Abstract: We propose that reinforcement learning (RL) from partial expert
demonstrations is not merely a training heuristic, but a promising framework
for solving complex sequence generation tasks. Supervised fine-tuning (SFT)
relies on dense ground-truth labels, which become increasingly costly as
sequence length grows. RL, on the other hand, struggles with sparse rewards and
a combinatorially large output space. We address this by introducing adaptive
backtracking (AdaBack), a per-sample curriculum learning algorithm that reveals
only a partial prefix of the target output during training. The supervision
length is adjusted dynamically for each sample based on the model's past reward
signal, allowing it to incrementally learn to complete reasoning chains by
conditioning on correct partial solutions. We investigate this intermediate
regime between SFT and RL and argue that per-sample curriculum learning is more
than a trade-off between efficiency and generality, it can succeed in tasks
with long sequences of latent dependencies where SFT and RL both fail to
generalize. Using a synthetic task with latent parity constraints, we show that
our adaptive curriculum over partial answers reliably solves problems that are
otherwise intractable. On mathematical reasoning benchmarks (MATH, GSM8k), we
find that curriculum learning enables models to solve problems that RL alone
cannot, acquiring new reasoning capabilities through incremental exposure to
partial solutions.

</details>


### [377] [Pitfalls of Conformal Predictions for Medical Image Classification](https://arxiv.org/abs/2506.18162)
*Hendrik Mehrtens,Tabea Bucher,Titus J. Brinker*

Main category: cs.LG

TL;DR: 论文探讨了在医学分类任务中，共形预测方法在分布偏移和小类别情况下的局限性。


<details>
  <summary>Details</summary>
Motivation: 共形预测因其可证明的校准保证在医学领域受到关注，但其在安全关键领域的应用存在潜在问题和假设。

Method: 通过皮肤病学和组织病理学的例子，分析了共形预测在输入和标签变量分布偏移下的不可靠性。

Result: 共形预测在分布偏移、数据子集（如特定类别或患者属性）及小类别分类任务中表现不佳。

Conclusion: 共形预测在医学图像分类任务中的实际价值有限，需谨慎使用。

Abstract: Reliable uncertainty estimation is one of the major challenges for medical
classification tasks. While many approaches have been proposed, recently the
statistical framework of conformal predictions has gained a lot of attention,
due to its ability to provide provable calibration guarantees. Nonetheless, the
application of conformal predictions in safety-critical areas such as medicine
comes with pitfalls, limitations and assumptions that practitioners need to be
aware of. We demonstrate through examples from dermatology and histopathology
that conformal predictions are unreliable under distributional shifts in input
and label variables. Additionally, conformal predictions should not be used for
selecting predictions to improve accuracy and are not reliable for subsets of
the data, such as individual classes or patient attributes. Moreover, in
classification settings with a small number of classes, which are common in
medical image classification tasks, conformal predictions have limited
practical value.

</details>


### [378] [Routing Mamba: Scaling State Space Models with Mixture-of-Experts Projection](https://arxiv.org/abs/2506.18145)
*Zheng Zhan,Liliang Ren,Shuohang Wang,Liyuan Liu,Yang Liu,Yeyun Gong,Yanzhi Wang,Yelong Shen*

Main category: cs.LG

TL;DR: RoM（Routing Mamba）通过稀疏混合线性投影专家扩展SSM参数，显著提升Mamba模型的性能与效率。


<details>
  <summary>Details</summary>
Motivation: 尽管SSM（如Mamba）在长序列建模中表现出色，但如何高效扩展其表达能力（尤其是结合MoE）仍具挑战性。

Method: RoM通过共享路由决策和轻量子模块，利用线性投影专家的协同作用，实现Mamba层的稀疏扩展。

Result: 在1.3B活跃参数（10B总参数）和16K训练序列长度下，RoM性能与密集Mamba模型相当，且节省23% FLOPS。

Conclusion: RoM为SSM的高效扩展提供了有效解决方案，尤其在长序列建模中表现出色。

Abstract: Linear State Space Models (SSMs) offer remarkable performance gains in
efficient sequence modeling, with constant inference-time computation and
memory complexity. Recent advances, such as Mamba, further enhance SSMs with
input-dependent gating and hardware-aware implementations, positioning them as
strong alternatives to Transformers for long sequence modeling. However,
efficiently scaling the expressive power of SSMs, particularly with Mixture of
Experts (MoE), remains challenging, as naive integration attempts often falter
or degrade performance. In this work, we introduce Routing Mamba (RoM), a novel
approach that scales SSM parameters using sparse mixtures of linear projection
experts. By sharing routing decisions between projection layers and lightweight
sub-modules within Mamba across experts, RoM leverages synergies among linear
projection experts for effective and efficient sparse scaling of Mamba layers.
At a scale of 1.3B active parameters (10B total) and 16K training sequence
length, RoM achieves language modeling performance equivalent to a dense Mamba
model requiring over 2.3x more active parameters, and demonstrates consistent
perplexity across context lengths. Experimental results further show RoM
effectively scales hybrid language models, yielding a 23% FLOPS saving compared
to dense Mamba scaling for similar performance.

</details>


### [379] [Non-equilibrium Annealed Adjoint Sampler](https://arxiv.org/abs/2506.18165)
*Jaemoo Choi,Yongxin Chen,Molei Tao,Guan-Horng Liu*

Main category: cs.LG

TL;DR: 提出了一种新的基于随机最优控制（SOC）的扩散采样器NAAS，通过退火参考动态避免重要性采样，提高了效率和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 退火方法虽能引导样本向高密度区域移动，但依赖重要性采样导致高方差和有限的可扩展性。

Method: NAAS利用退火参考动态，结合轻量级伴随系统，实现高效训练。

Result: 在经典能量景观和分子玻尔兹曼分布等任务中表现出色。

Conclusion: NAAS是一种高效、可扩展的扩散采样器，避免了重要性采样的局限性。

Abstract: Recently, there has been significant progress in learning-based diffusion
samplers, which aim to sample from a given unnormalized density. These methods
typically follow one of two paradigms: (i) formulating sampling as an unbiased
stochastic optimal control (SOC) problem using a canonical reference process,
or (ii) refining annealed path measures through importance-weighted sampling.
Although annealing approaches have advantages in guiding samples toward
high-density regions, reliance on importance sampling leads to high variance
and limited scalability in practice. In this paper, we introduce the
\textbf{Non-equilibrium Annealed Adjoint Sampler (NAAS)}, a novel SOC-based
diffusion sampler that leverages annealed reference dynamics without resorting
to importance sampling. NAAS employs a lean adjoint system inspired by adjoint
matching, enabling efficient and scalable training. We demonstrate the
effectiveness of our approach across a range of tasks, including sampling from
classical energy landscapes and molecular Boltzmann distribution.

</details>


### [380] [Understanding Reasoning in Thinking Language Models via Steering Vectors](https://arxiv.org/abs/2506.18167)
*Constantin Venhoff,Iván Arcuschin,Philip Torr,Arthur Conmy,Neel Nanda*

Main category: cs.LG

TL;DR: 本文提出了一种通过分析和操纵DeepSeek-R1-Distill模型的特定推理行为来引导大型语言模型（LLM）的方法，实现了对模型推理过程的可控调节。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在生成内部推理链方面取得了进步，但其推理过程的控制仍然具有挑战性。

Method: 通过系统实验识别模型的推理行为，并利用激活空间中的线性方向提取控制向量，实现对推理行为的调节。

Result: 实验验证了该方法能够一致地控制不同架构的DeepSeek-R1-Distill模型。

Conclusion: 该方法为可控和可解释地引导思维模型的推理过程提供了实用工具。

Abstract: Recent advances in large language models (LLMs) have led to the development
of thinking language models that generate extensive internal reasoning chains
before producing responses. While these models achieve improved performance,
controlling their reasoning processes remains challenging. This work presents a
steering approach for thinking LLMs by analyzing and manipulating specific
reasoning behaviors in DeepSeek-R1-Distill models. Through a systematic
experiment on 500 tasks across 10 diverse categories, we identify several
reasoning behaviors exhibited by thinking models, including expressing
uncertainty, generating examples for hypothesis validation, and backtracking in
reasoning chains. We demonstrate that these behaviors are mediated by linear
directions in the model's activation space and can be controlled using steering
vectors. By extracting and applying these vectors, we provide a method to
modulate specific aspects of the model's reasoning process, such as its
tendency to backtrack or express uncertainty. Our approach offers practical
tools for steering reasoning processes in thinking models in a controlled and
interpretable manner. We validate our steering method using two
DeepSeek-R1-Distill models, demonstrating consistent control across different
model architectures.

</details>


### [381] [DeInfoReg: A Decoupled Learning Framework for Better Training Throughput](https://arxiv.org/abs/2506.18193)
*Zih-Hao Huang,You-Teng Lin,Hung-Hsuan Chen*

Main category: cs.LG

TL;DR: DeInfoReg通过将长梯度流分解为多个短梯度流，缓解梯度消失问题，并利用并行计算提升训练效率。


<details>
  <summary>Details</summary>
Motivation: 解决传统反向传播中的梯度消失问题，同时利用并行计算资源提高训练效率。

Method: 提出DeInfoReg方法，结合信息正则化和管道策略，实现梯度流分解和多GPU并行计算。

Result: 在多种任务和数据集上表现优于传统反向传播和其他梯度分解技术，具有更好的噪声抵抗能力和计算效率。

Conclusion: DeInfoReg是一种高效且性能优越的梯度流分解方法，适用于并行计算环境。

Abstract: This paper introduces Decoupled Supervised Learning with Information
Regularization (DeInfoReg), a novel approach that transforms a long gradient
flow into multiple shorter ones, thereby mitigating the vanishing gradient
problem. Integrating a pipeline strategy, DeInfoReg enables model
parallelization across multiple GPUs, significantly improving training
throughput. We compare our proposed method with standard backpropagation and
other gradient flow decomposition techniques. Extensive experiments on diverse
tasks and datasets demonstrate that DeInfoReg achieves superior performance and
better noise resistance than traditional BP models and efficiently utilizes
parallel computing resources. The code for reproducibility is available at:
https://github.com/ianzih/Decoupled-Supervised-Learning-for-Information-Regularization/.

</details>


### [382] [These are Not All the Features You are Looking For: A Fundamental Bottleneck In Supervised Pretraining](https://arxiv.org/abs/2506.18221)
*Xingyu Alice Yang,Jianyu Zhang,Léon Bottou*

Main category: cs.LG

TL;DR: 论文探讨了迁移学习中的信息饱和瓶颈问题，提出了一种新的特征表示方法以改善泛化能力。


<details>
  <summary>Details</summary>
Motivation: 解决迁移学习中预训练特征无法充分适应新任务的问题，尤其是量化任务相关性的困难。

Method: 评估从预训练混合模型到其组成任务的迁移性能，分析信息饱和瓶颈现象。

Result: 发现深度学习模型存在信息饱和瓶颈，导致特征学习受限，影响迁移性能。

Conclusion: 建议关注任务特定训练，并提出更丰富的特征表示方法以提升泛化能力。

Abstract: Transfer learning is a cornerstone of modern machine learning, promising a
way to adapt models pretrained on a broad mix of data to new tasks with minimal
new data. However, a significant challenge remains in ensuring that transferred
features are sufficient to handle unseen datasets, amplified by the difficulty
of quantifying whether two tasks are "related". To address these challenges, we
evaluate model transfer from a pretraining mixture to each of its component
tasks, assessing whether pretrained features can match the performance of
task-specific direct training. We identify a fundamental limitation in deep
learning models -- an "information saturation bottleneck" -- where networks
fail to learn new features once they encode similar competing features during
training. When restricted to learning only a subset of key features during
pretraining, models will permanently lose critical features for transfer and
perform inconsistently on data distributions, even components of the training
mixture. Empirical evidence from published studies suggests that this
phenomenon is pervasive in deep learning architectures -- factors such as data
distribution or ordering affect the features that current representation
learning methods can learn over time. This study suggests that relying solely
on large-scale networks may not be as effective as focusing on task-specific
training, when available. We propose richer feature representations as a
potential solution to better generalize across new datasets and, specifically,
present existing methods alongside a novel approach, the initial steps towards
addressing this challenge.

</details>


### [383] [Quantum-Classical Hybrid Quantized Neural Network](https://arxiv.org/abs/2506.18240)
*Wenxin Li,Chuan Wang,Hongdong Zhu,Qi Gao,Yin Ma,Hai Wei,Kai Wen*

Main category: cs.LG

TL;DR: 提出了一种基于二次二进制优化（QBO）的量化神经网络训练模型，通过样条插值支持任意激活和损失函数，并引入前向区间传播（FIP）方法处理非线性问题。采用量子条件梯度下降（QCGD）算法解决约束问题，实验在Fashion MNIST上达到94.95%准确率。


<details>
  <summary>Details</summary>
Motivation: 解决神经网络中非线性问题和多层复合结构的挑战，扩展量子计算机在人工智能中的应用。

Method: 提出FIP方法离散化激活函数，使用QCGD算法直接求解QCBO问题，并分析其收敛性。

Result: 理论证明了近似误差上界和所需伊辛自旋数量，实验在低精度下实现高分类准确率。

Conclusion: 该方法在保持神经网络通用逼近能力的同时，为量子计算在AI中的应用提供了新途径。

Abstract: Here in this work, we present a novel Quadratic Binary Optimization (QBO)
model for quantized neural network training, enabling the use of arbitrary
activation and loss functions through spline interpolation. We introduce
Forward Interval Propagation (FIP), a method designed to tackle the challenges
of non-linearity and the multi-layer composite structure in neural networks by
discretizing activation functions into linear subintervals. This approach
preserves the universal approximation properties of neural networks while
allowing complex nonlinear functions to be optimized using quantum computers,
thus broadening their applicability in artificial intelligence. We provide
theoretical upper bounds on the approximation error and the number of Ising
spins required, by deriving the sample complexity of the empirical risk
minimization problem, from an optimization perspective. A significant challenge
in solving the associated Quadratic Constrained Binary Optimization (QCBO)
model on a large scale is the presence of numerous constraints. When employing
the penalty method to handle these constraints, tuning a large number of
penalty coefficients becomes a critical hyperparameter optimization problem,
increasing computational complexity and potentially affecting solution quality.
To address this, we employ the Quantum Conditional Gradient Descent (QCGD)
algorithm, which leverages quantum computing to directly solve the QCBO
problem. We prove the convergence of QCGD under a quantum oracle with
randomness and bounded variance in objective value, as well as under limited
precision constraints in the coefficient matrix. Additionally, we provide an
upper bound on the Time-To-Solution for the QCBO solving process. Experimental
results using a coherent Ising machine (CIM) demonstrate a 94.95% accuracy on
the Fashion MNIST classification task, with only 1.1-bit precision.

</details>


### [384] [ARD-LoRA: Dynamic Rank Allocation for Parameter-Efficient Fine-Tuning of Foundation Models with Heterogeneous Adaptation Needs](https://arxiv.org/abs/2506.18267)
*Haseeb Ullah Khan Shinwari,Muhammad Usama*

Main category: cs.LG

TL;DR: ARD-LoRA提出了一种动态分配秩的方法，通过可学习的缩放因子优化任务性能和参数效率，显著提升了模型适应效率。


<details>
  <summary>Details</summary>
Motivation: 传统LoRA方法使用固定秩，无法适应不同层和注意力头的异质学习动态，限制了模型适应效率。

Method: 引入可学习的缩放因子，通过元目标优化（结合任务性能和参数效率），并加入ℓ1稀疏性和总变差正则化，实现动态、可微的每头秩分配。

Result: 在LLAMA-3.1-70B和PaliGemma-2上，ARD-LoRA达到全微调性能的99.3%，仅需0.32%可训练参数，并减少41%多模态适应内存。

Conclusion: 动态细粒度秩分配是高效基础模型适应的关键范式。

Abstract: Conventional Low-Rank Adaptation (LoRA) methods employ a fixed rank, imposing
uniform adaptation across transformer layers and attention heads despite their
heterogeneous learning dynamics. This paper introduces Adaptive Rank Dynamic
LoRA (ARD-LoRA), a novel framework that automates rank allocation through
learnable scaling factors. These factors are optimized via a meta-objective
balancing task performance and parameter efficiency, incorporating $\ell_1$
sparsity for minimal rank and Total Variation regularization for stable rank
transitions. ARD-LoRA enables continuous, differentiable, per-head rank
adaptation. Experiments on LLAMA-3.1-70B and PaliGemma-2 demonstrate ARD-LoRA's
efficacy, achieving up to 99.3% of full fine-tuning performance with only 0.32%
trainable parameters, outperforming strong baselines like DoRA and AdaLoRA.
Furthermore, it reduces multimodal adaptation memory by 41%. These results
establish dynamic, fine-grained rank allocation as a critical paradigm for
efficient foundation model adaptation.

</details>


### [385] [Learning Causal Graphs at Scale: A Foundation Model Approach](https://arxiv.org/abs/2506.18285)
*Naiyu Yin,Tian Gao,Yue Yu*

Main category: cs.LG

TL;DR: 论文提出了一种基于注意力机制的新架构ADAG，用于学习多个线性结构方程模型（SEMs），通过多任务预训练提升小样本下DAG学习的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: DAG学习在计算成本和可识别性方面存在挑战，尤其是在小样本情况下。

Method: 提出ADAG，一种基于注意力机制的架构，通过非线性注意力核学习数据到图结构和参数的映射，实现多任务线性SEM估计。

Result: ADAG在合成数据集上显著提升了DAG学习准确性和零样本推理效率。

Conclusion: ADAG是首个专门为DAG学习设计的预训练基础模型，为因果发现提供了更高效和通用的方法。

Abstract: Due to its human-interpretability and invariance properties, Directed Acyclic
Graph (DAG) has been a foundational tool across various areas of AI research,
leading to significant advancements. However, DAG learning remains highly
challenging, due to its super-exponential growth in computational cost and
identifiability issues, particularly in small-sample regimes. To address these
two challenges, in this work we leverage the recent success of linear
transformers and develop a foundation model approach for discovering multiple
order-consistent DAGs across tasks. In particular, we propose Attention-DAG
(ADAG), a novel attention-mechanism-based architecture for learning multiple
linear Structural Equation Models (SEMs). ADAG learns the mapping from observed
data to both graph structure and parameters via a nonlinear attention-based
kernel, enabling efficient multi-task estimation of the underlying linear SEMs.
By formulating the learning process across multiple tasks as a continuous
optimization problem, the pre-trained ADAG model captures the common structural
properties as a shared low-dimensional prior, thereby reducing the
ill-posedness of downstream DAG learning tasks in small-sample regimes. We
evaluate our proposed approach on benchmark synthetic datasets and find that
ADAG achieves substantial improvements in both DAG learning accuracy and
zero-shot inference efficiency. To the best of our knowledge, this is the first
practical approach for pre-training a foundation model specifically designed
for DAG learning, representing a step toward more efficient and generalizable
down-stream applications in causal discovery.

</details>


### [386] [GeNeRT: A Physics-Informed Approach to Intelligent Wireless Channel Modeling via Generalizable Neural Ray Tracing](https://arxiv.org/abs/2506.18295)
*Kejia Bian,Meixia Tao,Shu Sun,Jun Yu*

Main category: cs.LG

TL;DR: GeNeRT是一种可泛化的神经射线追踪框架，通过结合物理传播原理和神经网络，提高了建模的准确性、效率和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 当前神经射线追踪方法存在泛化能力受限和对电磁定律遵循不足的问题。

Method: GeNeRT结合了Fresnel启发的神经网络设计和GPU张量化加速策略，支持场景内空间可转移性和场景间零样本泛化。

Result: 实验表明，GeNeRT在未训练区域和全新环境中泛化良好，MPC预测准确性优于基线，且运行时效率优于Wireless Insite。

Conclusion: GeNeRT通过改进的网络架构和训练策略，有效捕捉了射线与表面相互作用的物理原理。

Abstract: Neural ray tracing (RT) has emerged as a promising paradigm for channel
modeling by combining physical propagation principles with neural networks. It
enables high modeling accuracy and efficiency. However, current neural RT
methods face two key limitations: constrained generalization capability due to
strong spatial dependence, and weak adherence to electromagnetic laws. In this
paper, we propose GeNeRT, a Generalizable Neural RT framework with enhanced
generalization, accuracy and efficiency. GeNeRT supports both intra-scenario
spatial transferability and inter-scenario zero-shot generalization. By
incorporating Fresnel-inspired neural network design, it also achieves higher
accuracy in multipath component (MPC) prediction. Furthermore, a GPU-tensorized
acceleration strategy is introduced to improve runtime efficiency. Extensive
experiments conducted in outdoor scenarios demonstrate that GeNeRT generalizes
well across untrained regions within a scenario and entirely unseen
environments, and achieves superior accuracy in MPC prediction compared to
baselines. Moreover, it outperforms Wireless Insite in runtime efficiency,
particularly in multi-transmitter settings. Ablation experiments validate the
effectiveness of the network architecture and training strategy in capturing
physical principles of ray-surface interactions.

</details>


### [387] [Sharpening the Spear: Adaptive Expert-Guided Adversarial Attack Against DRL-based Autonomous Driving Policies](https://arxiv.org/abs/2506.18304)
*Junchao Fan,Xuyang Lei,Xiaolin Chang*

Main category: cs.LG

TL;DR: 提出了一种自适应专家引导的对抗攻击方法，提升攻击策略训练的稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: 尽管深度强化学习在自动驾驶中表现出潜力，但其策略易受对抗攻击，揭示漏洞并开发更鲁棒的系统至关重要。现有攻击方法存在效率低和训练不稳定问题。

Method: 通过模仿学习从成功攻击演示中提取专家策略，并利用KL散度正则化引导DRL对抗者，引入性能感知退火策略逐步减少对专家的依赖。

Result: 实验表明，该方法在碰撞率、攻击效率和训练稳定性上优于现有方法，尤其在专家策略不理想时表现更佳。

Conclusion: 该方法有效解决了现有攻击方法的不足，为自动驾驶系统的安全性提供了新思路。

Abstract: Deep reinforcement learning (DRL) has emerged as a promising paradigm for
autonomous driving. However, despite their advanced capabilities, DRL-based
policies remain highly vulnerable to adversarial attacks, posing serious safety
risks in real-world deployments. Investigating such attacks is crucial for
revealing policy vulnerabilities and guiding the development of more robust
autonomous systems. While prior attack methods have made notable progress, they
still face several challenges: 1) they often rely on high-frequency attacks,
yet critical attack opportunities are typically context-dependent and
temporally sparse, resulting in inefficient attack patterns; 2) restricting
attack frequency can improve efficiency but often results in unstable training
due to the adversary's limited exploration. To address these challenges, we
propose an adaptive expert-guided adversarial attack method that enhances both
the stability and efficiency of attack policy training. Our method first
derives an expert policy from successful attack demonstrations using imitation
learning, strengthened by an ensemble Mixture-of-Experts architecture for
robust generalization across scenarios. This expert policy then guides a
DRL-based adversary through a KL-divergence regularization term. Due to the
diversity of scenarios, expert policies may be imperfect. To address this, we
further introduce a performance-aware annealing strategy that gradually reduces
reliance on the expert as the adversary improves. Extensive experiments
demonstrate that our method achieves outperforms existing approaches in terms
of collision rate, attack efficiency, and training stability, especially in
cases where the expert policy is sub-optimal.

</details>


### [388] [Structured Kolmogorov-Arnold Neural ODEs for Interpretable Learning and Symbolic Discovery of Nonlinear Dynamics](https://arxiv.org/abs/2506.18339)
*Wei Liu,Kiran Bacsa,Loon Ching Tang,Eleni Chatzi*

Main category: cs.LG

TL;DR: SKANODE结合结构化状态空间建模与Kolmogorov-Arnold网络，提出了一种高精度且物理可解释的非线性动态系统建模框架。


<details>
  <summary>Details</summary>
Motivation: 解决深度学习在建模非线性动态系统时高精度与物理可解释性难以兼顾的问题。

Method: 使用可训练的KAN作为通用函数逼近器，结合结构化Neural ODE框架进行虚拟感知，提取物理可解释的潜在状态，并通过符号回归获取系统动态的紧凑表达式。

Result: 在模拟和真实系统中，SKANODE表现出卓越性能，并提供了解释性强、符合物理规律的模型。

Conclusion: SKANODE成功实现了高精度与物理可解释性的平衡，揭示了非线性动态系统的潜在机制。

Abstract: Understanding and modeling nonlinear dynamical systems is a fundamental
problem across scientific and engineering domains. While deep learning has
demonstrated remarkable potential for learning complex system behavior,
achieving models that are both highly accurate and physically interpretable
remains a major challenge. To address this, we propose Structured
Kolmogorov-Arnold Neural ODEs (SKANODEs), a novel framework that integrates
structured state-space modeling with the Kolmogorov-Arnold Network (KAN).
SKANODE first employs a fully trainable KAN as a universal function
approximator within a structured Neural ODE framework to perform virtual
sensing, recovering latent states that correspond to physically interpretable
quantities such as positions and velocities. Once this structured latent
representation is established, we exploit the symbolic regression capability of
KAN to extract compact and interpretable expressions for the system's governing
dynamics. The resulting symbolic expression is then substituted back into the
Neural ODE framework and further calibrated through continued training to
refine its coefficients, enhancing both the precision of the discovered
equations and the predictive accuracy of system responses. Extensive
experiments on both simulated and real-world systems demonstrate that SKANODE
achieves superior performance while offering interpretable, physics-consistent
models that uncover the underlying mechanisms of nonlinear dynamical systems.

</details>


### [389] [Controlled Generation with Equivariant Variational Flow Matching](https://arxiv.org/abs/2506.18340)
*Floor Eijkelboom,Heiko Zimmermann,Sharvaree Vadgama,Erik J Bekkers,Max Welling,Christian A. Naesseth,Jan-Willem van de Meent*

Main category: cs.LG

TL;DR: 本文提出了一种基于变分流匹配（VFM）的受控生成方法，支持端到端训练或贝叶斯推理实现控制，并在分子生成中实现了对称性保持。


<details>
  <summary>Details</summary>
Motivation: 研究如何在流匹配框架下实现受控生成，同时保持对称性（如分子生成中的旋转、平移和排列不变性）。

Method: 通过变分推断将流匹配问题转化为受控生成任务，支持端到端训练和贝叶斯推理两种方式，并提出了对称性保持的VFM公式。

Result: 在非受控和受控分子生成任务中均达到最优性能，尤其在贝叶斯推理设置下表现突出。

Conclusion: 该工作为基于流的生成模型与贝叶斯推理建立了联系，提供了可扩展且理论完备的约束驱动和对称性感知生成框架。

Abstract: We derive a controlled generation objective within the framework of
Variational Flow Matching (VFM), which casts flow matching as a variational
inference problem. We demonstrate that controlled generation can be implemented
two ways: (1) by way of end-to-end training of conditional generative models,
or (2) as a Bayesian inference problem, enabling post hoc control of
unconditional models without retraining. Furthermore, we establish the
conditions required for equivariant generation and provide an equivariant
formulation of VFM tailored for molecular generation, ensuring invariance to
rotations, translations, and permutations. We evaluate our approach on both
uncontrolled and controlled molecular generation, achieving state-of-the-art
performance on uncontrolled generation and outperforming state-of-the-art
models in controlled generation, both with end-to-end training and in the
Bayesian inference setting. This work strengthens the connection between
flow-based generative modeling and Bayesian inference, offering a scalable and
principled framework for constraint-driven and symmetry-aware generation.

</details>


### [390] [LOGICPO: Efficient Translation of NL-based Logical Problems to FOL using LLMs and Preference Optimization](https://arxiv.org/abs/2506.18383)
*Koushik Viswanadha,Deepanway Ghosal,Somak Aditya*

Main category: cs.LG

TL;DR: 论文提出了一种通过偏好优化数据集微调的方法，以改善LLMs在逻辑推理任务中的表现，特别是在将自然语言问题转化为逻辑程序方面。


<details>
  <summary>Details</summary>
Motivation: 逻辑推理是AI的关键任务，但现有方法在将自然语言问题转化为逻辑程序时存在不足，影响了推理能力。

Method: 1) 引入新的监督和偏好优化数据集LogicPO；2) 采用DPO和KTO等技术微调开源LLMs。

Result: 最佳模型Phi-3.5在逻辑正确性上比GPT-3.5-turbo（8-shot）高10%，语法错误减少14%。

Conclusion: 该框架和评估指标为提升LLMs的逻辑推理能力提供了有前景的方向。

Abstract: Logical reasoning is a key task for artificial intelligence due to it's role
in major downstream tasks such as Question Answering, Summarization. Recent
methods in improving the reasoning ability of LLMs fall short in correctly
converting a natural language reasoning problem to an equivalent logical
formulation, which hinders the framework's overall ability to reason. Towards
this, we propose to use finetuning on a preference optimization dataset to
learn to parse and represent a natural language problem as a whole to a
consistent logical program by 1) introducing a new supervised and preference
optimization dataset LogicPO, and 2) adopting popular techniques such as Direct
Preference Optimization (DPO), Kahneman-Tversky optimization (KTO) to finetune
open-source LLMs. Our best model with Phi-3.5 consistently outperforms
GPT-3.5-turbo's (8-shot) by producing 10% more logically correct and with 14%
less syntax errors. Through the framework and our improved evaluation metrics,
we offer a promising direction in improving the logical reasoning of LLMs by
better representing them in their logical formulations.

</details>


### [391] [ADNF-Clustering: An Adaptive and Dynamic Neuro-Fuzzy Clustering for Leukemia Prediction](https://arxiv.org/abs/2506.18396)
*Marco Aruta,Ciro Listone,Giuseppe Murano,Aniello Murano*

Main category: cs.LG

TL;DR: ADNF框架结合CNN特征提取和在线模糊聚类，动态适应白血病细胞模式变化，量化不确定性，优于静态方法。


<details>
  <summary>Details</summary>
Motivation: 传统聚类方法无法适应动态细胞模式和实时不确定性量化，需要更灵活的工具。

Method: 结合CNN特征提取和在线模糊聚类，通过Fuzzy Temporal Index更新参数，并进行拓扑优化。

Result: 在C-NMC数据集上轮廓分数达0.51，优于静态基线。

Conclusion: ADNF具有自适应不确定性建模和无标签操作潜力，适用于个性化白血病管理。

Abstract: Leukemia diagnosis and monitoring rely increasingly on high-throughput image
data, yet conventional clustering methods lack the flexibility to accommodate
evolving cellular patterns and quantify uncertainty in real time. We introduce
Adaptive and Dynamic Neuro-Fuzzy Clustering, a novel streaming-capable
framework that combines Convolutional Neural Network-based feature extraction
with an online fuzzy clustering engine. ADNF initializes soft partitions via
Fuzzy C-Means, then continuously updates micro-cluster centers, densities, and
fuzziness parameters using a Fuzzy Temporal Index (FTI) that measures entropy
evolution. A topology refinement stage performs density-weighted merging and
entropy-guided splitting to guard against over- and under-segmentation. On the
C-NMC leukemia microscopy dataset, our tool achieves a silhouette score of
0.51, demonstrating superior cohesion and separation over static baselines. The
method's adaptive uncertainty modeling and label-free operation hold immediate
potential for integration within the INFANT pediatric oncology network,
enabling scalable, up-to-date support for personalized leukemia management.

</details>


### [392] [PuckTrick: A Library for Making Synthetic Data More Realistic](https://arxiv.org/abs/2506.18499)
*Alessandra Agostini,Andrea Maurino,Blerina Spahiu*

Main category: cs.LG

TL;DR: Pucktrick是一个Python库，用于在合成数据中引入受控错误，以模拟真实数据的不完美性，从而提升机器学习模型的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 由于隐私和可用性问题，真实数据集难以获取，合成数据成为替代方案，但过于干净的合成数据缺乏真实数据的不完美性，影响模型性能。

Method: 开发Pucktrick库，支持多种错误类型（如缺失值、噪声、异常值等），并提供两种污染模式。

Result: 实验表明，在受污染的合成数据上训练的模型（尤其是树模型和线性模型）表现优于在干净合成数据上训练的模型。

Conclusion: Pucktrick通过模拟真实数据的不完美性，有效提升了机器学习模型的鲁棒性和泛化能力。

Abstract: The increasing reliance on machine learning (ML) models for decision-making
requires high-quality training data. However, access to real-world datasets is
often restricted due to privacy concerns, proprietary restrictions, and
incomplete data availability. As a result, synthetic data generation (SDG) has
emerged as a viable alternative, enabling the creation of artificial datasets
that preserve the statistical properties of real data while ensuring privacy
compliance. Despite its advantages, synthetic data is often overly clean and
lacks real-world imperfections, such as missing values, noise, outliers, and
misclassified labels, which can significantly impact model generalization and
robustness. To address this limitation, we introduce Pucktrick, a Python
library designed to systematically contaminate synthetic datasets by
introducing controlled errors. The library supports multiple error types,
including missing data, noisy values, outliers, label misclassification,
duplication, and class imbalance, offering a structured approach to evaluating
ML model resilience under real-world data imperfections. Pucktrick provides two
contamination modes: one for injecting errors into clean datasets and another
for further corrupting already contaminated datasets. Through extensive
experiments on real-world financial datasets, we evaluate the impact of
systematic data contamination on model performance. Our findings demonstrate
that ML models trained on contaminated synthetic data outperform those trained
on purely synthetic, error-free data, particularly for tree-based and linear
models such as SVMs and Extra Trees.

</details>


### [393] [Optimization-Induced Dynamics of Lipschitz Continuity in Neural Networks](https://arxiv.org/abs/2506.18588)
*Róisín Luo,James McDermott,Christian Gagné,Qiang Sun,Colm O'Riordan*

Main category: cs.LG

TL;DR: 论文提出了一个数学框架，用于分析神经网络训练过程中Lipschitz连续性的动态变化，揭示了梯度流、梯度噪声等因素对其演化的影响。


<details>
  <summary>Details</summary>
Motivation: 研究Lipschitz连续性在训练中的动态变化，填补了现有研究的空白。

Method: 利用随机微分方程（SDEs）建模，分析梯度流和梯度噪声对Lipschitz连续性的影响。

Result: 理论分析揭示了梯度流、梯度噪声等因素的作用，实验验证了理论与实际行为的一致性。

Conclusion: 该框架为理解Lipschitz连续性的动态提供了新视角，并揭示了训练参数对其演化的影响。

Abstract: Lipschitz continuity characterizes the worst-case sensitivity of neural
networks to small input perturbations; yet its dynamics (i.e. temporal
evolution) during training remains under-explored. We present a rigorous
mathematical framework to model the temporal evolution of Lipschitz continuity
during training with stochastic gradient descent (SGD). This framework
leverages a system of stochastic differential equations (SDEs) to capture both
deterministic and stochastic forces. Our theoretical analysis identifies three
principal factors driving the evolution: (i) the projection of gradient flows,
induced by the optimization dynamics, onto the operator-norm Jacobian of
parameter matrices; (ii) the projection of gradient noise, arising from the
randomness in mini-batch sampling, onto the operator-norm Jacobian; and (iii)
the projection of the gradient noise onto the operator-norm Hessian of
parameter matrices. Furthermore, our theoretical framework sheds light on such
as how noisy supervision, parameter initialization, batch size, and mini-batch
sampling trajectories, among other factors, shape the evolution of the
Lipschitz continuity of neural networks. Our experimental results demonstrate
strong agreement between the theoretical implications and the observed
behaviors.

</details>


### [394] [Simulation-Free Differential Dynamics through Neural Conservation Laws](https://arxiv.org/abs/2506.18604)
*Mengjian Hua,Eric Vanden-Eijnden,Ricky T. Q. Chen*

Main category: cs.LG

TL;DR: 提出了一种无需模拟的连续时间扩散过程训练框架，适用于广泛的目标函数，解决了现有方法的限制。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么限制问题形式，要么需要昂贵的模拟计算，无法高效处理一般目标函数。

Method: 通过耦合参数化联合建模时间依赖密度函数和扩散过程动力学，直接引入Fokker-Planck方程和密度函数约束。

Result: 实现了多种问题形式的无模拟训练，包括生成建模、动态最优传输和随机最优控制等。

Conclusion: 该方法在时空事件建模和从群体数据学习最优动力学等应用中表现优异。

Abstract: We present a novel simulation-free framework for training continuous-time
diffusion processes over very general objective functions. Existing methods
typically involve either prescribing the optimal diffusion process -- which
only works for heavily restricted problem formulations -- or require expensive
simulation to numerically obtain the time-dependent densities and sample from
the diffusion process. In contrast, we propose a coupled parameterization which
jointly models a time-dependent density function, or probability path, and the
dynamics of a diffusion process that generates this probability path. To
accomplish this, our approach directly bakes in the Fokker-Planck equation and
density function requirements as hard constraints, by extending and greatly
simplifying the construction of Neural Conservation Laws. This enables
simulation-free training for a large variety of problem formulations, from
data-driven objectives as in generative modeling and dynamical optimal
transport, to optimality-based objectives as in stochastic optimal control,
with straightforward extensions to mean-field objectives due to the ease of
accessing exact density functions. We validate our method in a diverse range of
application domains from modeling spatio-temporal events to learning optimal
dynamics from population data.

</details>


### [395] [Multi-Agent Reinforcement Learning for Inverse Design in Photonic Integrated Circuits](https://arxiv.org/abs/2506.18627)
*Yannik Mahlau,Maximilian Schier,Christoph Reinders,Frederik Schubert,Marco Bügling,Bodo Rosenhahn*

Main category: cs.LG

TL;DR: 该论文提出了一种基于强化学习（RL）的逆向设计方法，用于优化光子集成电路（PICs），解决了传统梯度优化易陷入局部最优的问题。


<details>
  <summary>Details</summary>
Motivation: 随着光学计算对PICs需求的增加，需要更高效的优化算法来克服梯度优化的局限性。

Method: 通过将设计空间离散化为网格，将设计任务转化为具有数千个二元变量的优化问题，并采用多智能体RL算法进行优化。

Result: 该方法在二维和三维设计任务中均优于传统梯度优化，且仅需少量样本即可完成优化。

Conclusion: 该研究为光子学逆向设计提供了高效的RL基准方法，并展示了多智能体RL在复杂设计任务中的潜力。

Abstract: Inverse design of photonic integrated circuits (PICs) has traditionally
relied on gradientbased optimization. However, this approach is prone to end up
in local minima, which results in suboptimal design functionality. As interest
in PICs increases due to their potential for addressing modern hardware demands
through optical computing, more adaptive optimization algorithms are needed. We
present a reinforcement learning (RL) environment as well as multi-agent RL
algorithms for the design of PICs. By discretizing the design space into a
grid, we formulate the design task as an optimization problem with thousands of
binary variables. We consider multiple two- and three-dimensional design tasks
that represent PIC components for an optical computing system. By decomposing
the design space into thousands of individual agents, our algorithms are able
to optimize designs with only a few thousand environment samples. They
outperform previous state-of-the-art gradient-based optimization in both twoand
three-dimensional design tasks. Our work may also serve as a benchmark for
further exploration of sample-efficient RL for inverse design in photonics.

</details>


### [396] [Granular-Ball-Induced Multiple Kernel K-Means](https://arxiv.org/abs/2506.18637)
*Shuyin Xia,Yifan Wang,Lifeng Shen,Guoyin Wang*

Main category: cs.LG

TL;DR: 提出了一种基于粒度球计算的多核聚类框架（GB-MKKM），通过粒度球核（GBK）提高计算效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有多核聚类算法（如多核K-means）在复杂数据分布下计算效率低且鲁棒性差，依赖点对点关系优化难以捕捉数据内在结构。

Method: 利用粒度球计算自适应拟合数据分布，提出粒度球核（GBK）和GB-MKKM框架，通过球间关系优化多核聚类。

Result: GB-MKKM在多种聚类任务中表现出更高的效率和聚类性能。

Conclusion: 粒度球计算显著提升了多核聚类的效率和鲁棒性，适用于高维数据。

Abstract: Most existing multi-kernel clustering algorithms, such as multi-kernel
K-means, often struggle with computational efficiency and robustness when faced
with complex data distributions. These challenges stem from their dependence on
point-to-point relationships for optimization, which can lead to difficulty in
accurately capturing data sets' inherent structure and diversity. Additionally,
the intricate interplay between multiple kernels in such algorithms can further
exacerbate these issues, effectively impacting their ability to cluster data
points in high-dimensional spaces. In this paper, we leverage granular-ball
computing to improve the multi-kernel clustering framework. The core of
granular-ball computing is to adaptively fit data distribution by balls from
coarse to acceptable levels. Each ball can enclose data points based on a
density consistency measurement. Such ball-based data description thus improves
the computational efficiency and the robustness to unknown noises.
Specifically, based on granular-ball representations, we introduce the
granular-ball kernel (GBK) and its corresponding granular-ball multi-kernel
K-means framework (GB-MKKM) for efficient clustering. Using granular-ball
relationships in multiple kernel spaces, the proposed GB-MKKM framework shows
its superiority in efficiency and clustering performance in the empirical
evaluation of various clustering tasks.

</details>


### [397] [Federated Loss Exploration for Improved Convergence on Non-IID Data](https://arxiv.org/abs/2506.18640)
*Christian Internò,Markus Olhofer,Yaochu Jin,Barbara Hammer*

Main category: cs.LG

TL;DR: FedLEx是一种针对非独立同分布数据的联邦学习方法，通过优化学习行为和梯度更新策略，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决联邦学习在非独立同分布数据场景下的数据异构性和性能鲁棒性问题。

Method: 采用联邦损失探索技术，客户端通过计算梯度偏差贡献全局指导矩阵，指导后续梯度更新。

Result: 在非独立同分布条件下，FedLEx显著优于现有方法，且无需额外数据共享或统计信息。

Conclusion: FedLEx在复杂数据场景下表现出色，为联邦学习的广泛应用提供了新思路。

Abstract: Federated learning (FL) has emerged as a groundbreaking paradigm in machine
learning (ML), offering privacy-preserving collaborative model training across
diverse datasets. Despite its promise, FL faces significant hurdles in
non-identically and independently distributed (non-IID) data scenarios, where
most existing methods often struggle with data heterogeneity and lack
robustness in performance. This paper introduces Federated Loss Exploration
(FedLEx), an innovative approach specifically designed to tackle these
challenges. FedLEx distinctively addresses the shortcomings of existing FL
methods in non-IID settings by optimizing its learning behavior for scenarios
in which assumptions about data heterogeneity are impractical or unknown. It
employs a federated loss exploration technique, where clients contribute to a
global guidance matrix by calculating gradient deviations for model parameters.
This matrix serves as a strategic compass to guide clients' gradient updates in
subsequent FL rounds, thereby fostering optimal parameter updates for the
global model. FedLEx effectively navigates the complex loss surfaces inherent
in non-IID data, enhancing knowledge transfer in an efficient manner, since
only a small number of epochs and small amount of data are required to build a
strong global guidance matrix that can achieve model convergence without the
need for additional data sharing or data distribution statics in a large client
scenario. Our extensive experiments with state-of-the art FL algorithms
demonstrate significant improvements in performance, particularly under
realistic non-IID conditions, thus highlighting FedLEx's potential to overcome
critical barriers in diverse FL applications.

</details>


### [398] [On the Existence of Universal Simulators of Attention](https://arxiv.org/abs/2506.18739)
*Debanjan Dutta,Faizanuddin Ansari,Anish Chakrabarty,Swagatam Das*

Main category: cs.LG

TL;DR: 该论文探讨了Transformer编码器是否能精确模拟注意力机制，提出了一个通用模拟器并通过算法证明其可行性。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer架构在表达性和可学习性之间的交叉问题，特别是能否精确模拟任意注意力机制。

Method: 构建了一个由Transformer编码器组成的通用模拟器，并使用RASP框架进行算法验证。

Result: 首次证明存在一种算法可实现数据无关的精确模拟，而此前仅能通过学习近似。

Conclusion: Transformer编码器能够精确模拟注意力机制，提供了一种算法解决方案。

Abstract: Prior work on the learnability of transformers has established its capacity
to approximate specific algorithmic patterns through training under restrictive
architectural assumptions. Fundamentally, these arguments remain data-driven
and therefore can only provide a probabilistic guarantee. Expressivity, on the
contrary, has theoretically been explored to address the problems
\emph{computable} by such architecture. These results proved the
Turing-completeness of transformers, investigated bounds focused on circuit
complexity, and formal logic. Being at the crossroad between learnability and
expressivity, the question remains: \emph{can transformer architectures exactly
simulate an arbitrary attention mechanism, or in particular, the underlying
operations?} In this study, we investigate the transformer encoder's ability to
simulate a vanilla attention mechanism. By constructing a universal simulator
$\mathcal{U}$ composed of transformer encoders, we present algorithmic
solutions to identically replicate attention outputs and the underlying
elementary matrix and activation operations via RASP, a formal framework for
transformer computation. Our proofs, for the first time, show the existence of
an algorithmically achievable data-agnostic solution, previously known to be
approximated only by learning.

</details>


### [399] [ContinualFlow: Learning and Unlearning with Neural Flow Matching](https://arxiv.org/abs/2506.18747)
*Lorenzo Simone,Davide Bacciu,Shuangge Ma*

Main category: cs.LG

TL;DR: ContinualFlow是一个基于Flow Matching的生成模型目标性遗忘框架，通过能量重加权损失实现数据分布中不期望区域的软性移除，无需重新训练或直接访问需遗忘样本。


<details>
  <summary>Details</summary>
Motivation: 解决生成模型中目标性遗忘的问题，避免重新训练或直接访问需遗忘样本的复杂性。

Method: 利用能量重加权损失和Flow Matching，通过能量代理指导遗忘过程。

Result: 在2D和图像领域的实验中验证了框架的有效性，支持可解释的可视化和定量评估。

Conclusion: ContinualFlow提供了一种高效且无需重新训练的目标性遗忘方法，适用于生成模型。

Abstract: We introduce ContinualFlow, a principled framework for targeted unlearning in
generative models via Flow Matching. Our method leverages an energy-based
reweighting loss to softly subtract undesired regions of the data distribution
without retraining from scratch or requiring direct access to the samples to be
unlearned. Instead, it relies on energy-based proxies to guide the unlearning
process. We prove that this induces gradients equivalent to Flow Matching
toward a soft mass-subtracted target, and validate the framework through
experiments on 2D and image domains, supported by interpretable visualizations
and quantitative evaluations.

</details>


### [400] [Sensitivity Analysis of Image Classification Models using Generalized Polynomial Chaos](https://arxiv.org/abs/2506.18751)
*Lukas Bahr,Lucas Poßner,Konstantin Weise,Sophie Gröger,Rüdiger Daub*

Main category: cs.LG

TL;DR: 研究探讨了图像分类模型在预测质量中的敏感性，提出了一种基于随机变量建模输入域偏移的方法，并通过Sobol指数量化其对输出的影响。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在图像分类中常因模型、数据和域偏移导致不确定性，引发分类结果的过度自信，需通过敏感性分析理解模型行为。

Method: 利用随机变量建模输入域偏移，通过广义多项式混沌（GPC）计算Sobol指数，量化输入参数对输出的影响。

Result: 通过焊接缺陷分类和宝马集团生产设施中的标志分类案例验证了方法的有效性。

Conclusion: 提出的方法能有效量化域偏移对模型输出的影响，为预测质量中的图像分类模型提供了更深入的理解。

Abstract: Integrating advanced communication protocols in production has accelerated
the adoption of data-driven predictive quality methods, notably machine
learning (ML) models. However, ML models in image classification often face
significant uncertainties arising from model, data, and domain shifts. These
uncertainties lead to overconfidence in the classification model's output. To
better understand these models, sensitivity analysis can help to analyze the
relative influence of input parameters on the output. This work investigates
the sensitivity of image classification models used for predictive quality. We
propose modeling the distributional domain shifts of inputs with random
variables and quantifying their impact on the model's outputs using Sobol
indices computed via generalized polynomial chaos (GPC). This approach is
validated through a case study involving a welding defect classification
problem, utilizing a fine-tuned ResNet18 model and an emblem classification
model used in BMW Group production facilities.

</details>


### [401] [Shift Happens: Mixture of Experts based Continual Adaptation in Federated Learning](https://arxiv.org/abs/2506.18789)
*Rahul Atul Bhope,K. R. Jayaram,Praveen Venkateswaran,Nalini Venkatasubramanian*

Main category: cs.LG

TL;DR: ShiftEx框架通过动态创建和训练专家模型，解决了联邦学习中的协变量和标签偏移问题，显著提升了模型性能和适应性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在动态数据分布环境下性能下降，需要自适应解决方案。

Method: 提出ShiftEx框架，利用最大均值差异检测协变量偏移，结合潜在记忆机制和设施位置优化。

Result: 在基准数据集上实现5.5-12.9%的准确率提升和22-95%的更快适应速度。

Conclusion: ShiftEx为动态数据分布下的联邦学习提供了高效、隐私保护的中间件解决方案。

Abstract: Federated Learning (FL) enables collaborative model training across
decentralized clients without sharing raw data, yet faces significant
challenges in real-world settings where client data distributions evolve
dynamically over time. This paper tackles the critical problem of covariate and
label shifts in streaming FL environments, where non-stationary data
distributions degrade model performance and require adaptive middleware
solutions. We introduce ShiftEx, a shift-aware mixture of experts framework
that dynamically creates and trains specialized global models in response to
detected distribution shifts using Maximum Mean Discrepancy for covariate
shifts. The framework employs a latent memory mechanism for expert reuse and
implements facility location-based optimization to jointly minimize covariate
mismatch, expert creation costs, and label imbalance. Through theoretical
analysis and comprehensive experiments on benchmark datasets, we demonstrate
5.5-12.9 percentage point accuracy improvements and 22-95 % faster adaptation
compared to state-of-the-art FL baselines across diverse shift scenarios. The
proposed approach offers a scalable, privacy-preserving middleware solution for
FL systems operating in non-stationary, real-world conditions while minimizing
communication and computational overhead.

</details>


<div id='cs.DB'></div>

# cs.DB [[Back]](#toc)

### [402] [LIGHTHOUSE: Fast and precise distance to shoreline calculations from anywhere on earth](https://arxiv.org/abs/2506.18842)
*Patrick Beukema,Henry Herzog,Yawen Zhang,Hunter Pitelka,Favyen Bastani*

Main category: cs.DB

TL;DR: 提出了一种高精度全球海岸线数据集（10米分辨率）和高效计算库Lighthouse，适用于资源受限环境。


<details>
  <summary>Details</summary>
Motivation: 现有全球海岸数据集分辨率低（1-4公里），限制了其应用潜力。

Method: 结合公开卫星影像和计算机视觉技术，开发了高分辨率数据集；并设计了Lighthouse库，实现高效计算。

Result: 数据集分辨率提升100倍以上，Lighthouse在单CPU和2GB内存下实现毫秒级在线推理。

Conclusion: 新数据集和算法显著提升了海岸距离计算的精度和效率，适用于实时应用。

Abstract: We introduce a new dataset and algorithm for fast and efficient coastal
distance calculations from Anywhere on Earth (AoE). Existing global coastal
datasets are only available at coarse resolution (e.g. 1-4 km) which limits
their utility. Publicly available satellite imagery combined with computer
vision enable much higher precision. We provide a global coastline dataset at
10 meter resolution, a 100+ fold improvement in precision over existing data.
To handle the computational challenge of querying at such an increased scale,
we introduce a new library: Layered Iterative Geospatial Hierarchical
Terrain-Oriented Unified Search Engine (Lighthouse). Lighthouse is both
exceptionally fast and resource-efficient, requiring only 1 CPU and 2 GB of RAM
to achieve millisecond online inference, making it well suited for real-time
applications in resource-constrained environments.

</details>


<div id='math.OC'></div>

# math.OC [[Back]](#toc)

### [403] [Wisdom of Crowds Through Myopic Self-Confidence Adaptation](https://arxiv.org/abs/2506.18195)
*Giacomo Como,Fabio Fagnani,Anton Proskurnikov*

Main category: math.OC

TL;DR: 本文研究了群体智慧现象，探讨了在多智能体系统中通过迭代学习规则（French-DeGroot动态）更新估计值的过程，分析了方差最小化的博弈论优化问题，并证明了异步最佳响应动态的收敛性。


<details>
  <summary>Details</summary>
Motivation: 研究群体智慧现象在多智能体系统中的表现，特别是在独立决策和迭代学习规则下，群体决策的准确性如何受个体间权重分配的影响。

Method: 采用French-DeGroot动态（迭代意见池）作为学习规则，分析多智能体系统中的博弈论优化问题，包括帕累托前沿和纳什均衡的刻画，以及异步最佳响应动态的收敛性证明。

Result: 确定了方差最小化的帕累托前沿和纳什均衡集，并证明了异步最佳响应动态收敛于严格纳什均衡集。

Conclusion: 通过博弈论方法，本文揭示了在多智能体系统中，通过优化权重分配可以实现更准确的群体决策，同时异步动态的收敛性为实际应用提供了理论支持。

Abstract: The wisdom of crowds is an umbrella term for phenomena suggesting that the
collective judgment or decision of a large group can be more accurate than the
individual judgments or decisions of the group members. A well-known example
illustrating this concept is the competition at a country fair described by
Galton, where the median value of the individual guesses about the weight of an
ox resulted in an astonishingly accurate estimate of the actual weight. This
phenomenon resembles classical results in probability theory and relies on
independent decision-making. The accuracy of the group's final decision can be
significantly reduced if the final agents' opinions are driven by a few
influential agents.
  In this paper, we consider a group of agents who initially possess
uncorrelated and unbiased noisy measurements of a common state of the world.
Assume these agents iteratively update their estimates according to a simple
non-Bayesian learning rule, commonly known in mathematical sociology as the
French-DeGroot dynamics or iterative opinion pooling. As a result of this
iterative distributed averaging process, each agent arrives at an asymptotic
estimate of the state of the world, with the variance of this estimate
determined by the matrix of weights the agents assign to each other. Every
agent aims at minimizing the variance of her asymptotic estimate of the state
of the world; however, such variance is also influenced by the weights
allocated by other agents. To achieve the best possible estimate, the agents
must then solve a game-theoretic, multi-objective optimization problem defined
by the available sets of influence weights. We characterize both the Pareto
frontier and the set of Nash equilibria in the resulting game. Additionally, we
examine asynchronous best-response dynamics for the group of agents and prove
their convergence to the set of strict Nash equilibria.

</details>


<div id='cs.CR'></div>

# cs.CR [[Back]](#toc)

### [404] [Step-by-Step Reasoning Attack: Revealing 'Erased' Knowledge in Large Language Models](https://arxiv.org/abs/2506.17279)
*Yash Sinha,Manit Baser,Murari Mandal,Dinil Mon Divakaran,Mohan Kankanhalli*

Main category: cs.CR

TL;DR: 论文提出了一种基于逐步推理的黑盒攻击方法Sleek，用于暴露现有遗忘技术在知识擦除中的失败，并展示了如何通过特定提示恢复被隐藏的信息。


<details>
  <summary>Details</summary>
Motivation: 确保大型语言模型（LLM）的知识擦除符合数据与AI法规，保护用户隐私，减少偏见和错误信息。现有遗忘技术未能彻底擦除知识，仍可通过特定提示恢复。

Method: 提出Sleek攻击框架，包括：(1) 基于逐步推理的对抗提示生成策略，(2) 成功召回被擦除内容的攻击机制，(3) 对提示进行分类以识别最有效的攻击类型。

Result: 在四种最先进的遗忘技术和两种广泛使用的LLM上评估，62.5%的对抗提示成功恢复了被遗忘的《哈利波特》知识，50%暴露了对保留知识的不公平抑制。

Conclusion: 现有遗忘技术无法可靠擦除知识，信息泄漏风险持续存在，需开发更鲁棒的擦除策略。

Abstract: Knowledge erasure in large language models (LLMs) is important for ensuring
compliance with data and AI regulations, safeguarding user privacy, mitigating
bias, and misinformation. Existing unlearning methods aim to make the process
of knowledge erasure more efficient and effective by removing specific
knowledge while preserving overall model performance, especially for retained
information. However, it has been observed that the unlearning techniques tend
to suppress and leave the knowledge beneath the surface, thus making it
retrievable with the right prompts. In this work, we demonstrate that
\textit{step-by-step reasoning} can serve as a backdoor to recover this hidden
information. We introduce a step-by-step reasoning-based black-box attack,
Sleek, that systematically exposes unlearning failures. We employ a structured
attack framework with three core components: (1) an adversarial prompt
generation strategy leveraging step-by-step reasoning built from LLM-generated
queries, (2) an attack mechanism that successfully recalls erased content, and
exposes unfair suppression of knowledge intended for retention and (3) a
categorization of prompts as direct, indirect, and implied, to identify which
query types most effectively exploit unlearning weaknesses. Through extensive
evaluations on four state-of-the-art unlearning techniques and two widely used
LLMs, we show that existing approaches fail to ensure reliable knowledge
removal. Of the generated adversarial prompts, 62.5% successfully retrieved
forgotten Harry Potter facts from WHP-unlearned Llama, while 50% exposed unfair
suppression of retained knowledge. Our work highlights the persistent risks of
information leakage, emphasizing the need for more robust unlearning strategies
for erasure.

</details>


### [405] [Theoretically Unmasking Inference Attacks Against LDP-Protected Clients in Federated Vision Models](https://arxiv.org/abs/2506.17292)
*Quan Nguyen,Minh N. Vu,Truc Nguyen,My T. Thai*

Main category: cs.CR

TL;DR: 联邦学习通过协调服务器实现客户端协作学习，避免直接数据共享以保护隐私。然而，成员推理攻击（MIAs）研究表明，未受保护的数据仍面临高攻击成功率。尽管本地差分隐私（LDP）被视为隐私保护的黄金标准，但多数研究未考虑LDP或未提供理论保证。本文推导了低多项式时间MIAs的理论下界，证明即使数据受LDP保护，隐私风险仍存在，且噪声需求会显著降低模型效用。


<details>
  <summary>Details</summary>
Motivation: 探讨联邦学习在本地差分隐私（LDP）保护下仍存在的隐私风险，填补现有研究中对LDP保护数据下MIAs攻击成功率的理论空白。

Method: 推导低多项式时间MIAs的理论下界，分析其在全连接层或自注意力层中的漏洞，并通过联邦视觉模型进行实践评估。

Result: 研究表明，即使数据受LDP保护，隐私风险仍显著存在，且噪声需求会显著降低模型效用。

Conclusion: 联邦学习中的LDP保护无法完全消除隐私风险，需权衡隐私保护与模型效用的关系。

Abstract: Federated Learning enables collaborative learning among clients via a
coordinating server while avoiding direct data sharing, offering a perceived
solution to preserve privacy. However, recent studies on Membership Inference
Attacks (MIAs) have challenged this notion, showing high success rates against
unprotected training data. While local differential privacy (LDP) is widely
regarded as a gold standard for privacy protection in data analysis, most
studies on MIAs either neglect LDP or fail to provide theoretical guarantees
for attack success rates against LDP-protected data. To address this gap, we
derive theoretical lower bounds for the success rates of low-polynomial time
MIAs that exploit vulnerabilities in fully connected or self-attention layers.
We establish that even when data are protected by LDP, privacy risks persist,
depending on the privacy budget. Practical evaluations on federated vision
models confirm considerable privacy risks, revealing that the noise required to
mitigate these attacks significantly degrades models' utility.

</details>


### [406] [LLM Jailbreak Oracle](https://arxiv.org/abs/2506.17299)
*Shuyi Lin,Anshuman Suri,Alina Oprea,Cheng Tan*

Main category: cs.CR

TL;DR: 论文提出了一个名为Boa的高效算法，用于解决LLMs的jailbreak oracle问题，以评估模型在安全关键应用中的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在安全关键应用中的广泛部署，缺乏系统方法评估其对jailbreak攻击的脆弱性成为一个关键安全漏洞。

Method: Boa采用三阶段搜索策略：构建块列表识别拒绝模式、广度优先采样识别易访问的jailbreak、深度优先优先级搜索探索低概率路径。

Result: Boa能够进行严格的安全评估，包括系统防御评估、标准化红队攻击比较和极端对抗条件下的模型认证。

Conclusion: Boa为LLMs的jailbreak漏洞提供了系统化的评估方法，填补了安全研究中的关键空白。

Abstract: As large language models (LLMs) become increasingly deployed in
safety-critical applications, the lack of systematic methods to assess their
vulnerability to jailbreak attacks presents a critical security gap. We
introduce the jailbreak oracle problem: given a model, prompt, and decoding
strategy, determine whether a jailbreak response can be generated with
likelihood exceeding a specified threshold. This formalization enables a
principled study of jailbreak vulnerabilities. Answering the jailbreak oracle
problem poses significant computational challenges -- the search space grows
exponentially with the length of the response tokens. We present Boa, the first
efficient algorithm for solving the jailbreak oracle problem. Boa employs a
three-phase search strategy: (1) constructing block lists to identify refusal
patterns, (2) breadth-first sampling to identify easily accessible jailbreaks,
and (3) depth-first priority search guided by fine-grained safety scores to
systematically explore promising low-probability paths. Boa enables rigorous
security assessments including systematic defense evaluation, standardized
comparison of red team attacks, and model certification under extreme
adversarial conditions.

</details>


### [407] [Context manipulation attacks : Web agents are susceptible to corrupted memory](https://arxiv.org/abs/2506.17318)
*Atharv Singh Patlan,Ashwin Hebbar,Pramod Viswanath,Prateek Mittal*

Main category: cs.CR

TL;DR: 论文介绍了一种针对自主网页导航代理的“计划注入”攻击，通过操纵上下文绕过防御，攻击成功率显著提高。


<details>
  <summary>Details</summary>
Motivation: 由于自主网页导航代理依赖外部内存系统管理上下文，存在安全漏洞，近期被利用攻击生产系统。

Method: 提出并形式化“计划注入”攻击，评估两种流行代理（Browser-use和Agent-E），分析上下文链式注入的效果。

Result: 计划注入攻击成功率比提示注入高3倍，上下文链式注入使隐私泄露任务成功率提升17.7%。

Conclusion: 安全内存处理应成为代理系统的首要关注点。

Abstract: Autonomous web navigation agents, which translate natural language
instructions into sequences of browser actions, are increasingly deployed for
complex tasks across e-commerce, information retrieval, and content discovery.
Due to the stateless nature of large language models (LLMs), these agents rely
heavily on external memory systems to maintain context across interactions.
Unlike centralized systems where context is securely stored server-side, agent
memory is often managed client-side or by third-party applications, creating
significant security vulnerabilities. This was recently exploited to attack
production systems.
  We introduce and formalize "plan injection," a novel context manipulation
attack that corrupts these agents' internal task representations by targeting
this vulnerable context. Through systematic evaluation of two popular web
agents, Browser-use and Agent-E, we show that plan injections bypass robust
prompt injection defenses, achieving up to 3x higher attack success rates than
comparable prompt-based attacks. Furthermore, "context-chained injections,"
which craft logical bridges between legitimate user goals and attacker
objectives, lead to a 17.7% increase in success rate for privacy exfiltration
tasks. Our findings highlight that secure memory handling must be a first-class
concern in agentic systems.

</details>


### [408] [On the Performance of Cyber-Biomedical Features for Intrusion Detection in Healthcare 5.0](https://arxiv.org/abs/2506.17329)
*Pedro H. Lui,Lucas P. Siqueira,Juliano F. Kazienko,Vagner E. Quincozes,Silvio E. Quincozes,Daniel Welfer*

Main category: cs.CR

TL;DR: Healthcare 5.0结合AI和IoT实现个性化医疗，但面临网络安全威胁。本研究应用XAI分析医疗数据，XGBoost模型在入侵检测中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前AI驱动的网络安全模型忽视生物医学数据，本研究旨在填补这一空白。

Method: 应用eXplainable AI (XAI)分析Healthcare 5.0数据集，结合网络流量和生物医学传感器数据。

Result: XGBoost模型在良性数据篡改中F1-score达99%，欺骗检测为81%。网络数据主导入侵检测，生物医学特征（如温度）对欺骗检测贡献显著。

Conclusion: XAI在Healthcare 5.0中有效提升网络安全，生物医学数据对特定威胁检测具有重要价值。

Abstract: Healthcare 5.0 integrates Artificial Intelligence (AI), the Internet of
Things (IoT), real-time monitoring, and human-centered design toward
personalized medicine and predictive diagnostics. However, the increasing
reliance on interconnected medical technologies exposes them to cyber threats.
Meanwhile, current AI-driven cybersecurity models often neglect biomedical
data, limiting their effectiveness and interpretability. This study addresses
this gap by applying eXplainable AI (XAI) to a Healthcare 5.0 dataset that
integrates network traffic and biomedical sensor data. Classification outputs
indicate that XGBoost achieved 99% F1-score for benign and data alteration, and
81% for spoofing. Explainability findings reveal that network data play a
dominant role in intrusion detection whereas biomedical features contributed to
spoofing detection, with temperature reaching a Shapley values magnitude of
0.37.

</details>


### [409] [CUBA: Controlled Untargeted Backdoor Attack against Deep Neural Networks](https://arxiv.org/abs/2506.17350)
*Yinghao Wu,Liyan Zhang*

Main category: cs.CR

TL;DR: 提出了一种新型的约束无目标后门攻击（CUBA），结合了无目标攻击的灵活性和目标攻击的意图性，能够绕过现有防御方法。


<details>
  <summary>Details</summary>
Motivation: 现有后门攻击多为目标攻击，防御方法依赖这一特性。无目标攻击虽灵活但效果较弱，因此需要一种兼具灵活性和意图性的攻击方法。

Method: 通过应用带翻转独热标签的交叉熵损失的对数归一化，约束训练过程中的对数，使模型在选定目标类别上呈现均匀分布。

Result: 在不同数据集上的实验证明了CUBA的有效性。

Conclusion: CUBA成功结合了无目标攻击的灵活性和目标攻击的意图性，能够有效绕过现有防御方法。

Abstract: Backdoor attacks have emerged as a critical security threat against deep
neural networks in recent years. The majority of existing backdoor attacks
focus on targeted backdoor attacks, where trigger is strongly associated to
specific malicious behavior. Various backdoor detection methods depend on this
inherent property and shows effective results in identifying and mitigating
such targeted attacks. However, a purely untargeted attack in backdoor
scenarios is, in some sense, self-weakening, since the target nature is what
makes backdoor attacks so powerful. In light of this, we introduce a novel
Constrained Untargeted Backdoor Attack (CUBA), which combines the flexibility
of untargeted attacks with the intentionality of targeted attacks. The
compromised model, when presented with backdoor images, will classify them into
random classes within a constrained range of target classes selected by the
attacker. This combination of randomness and determinedness enables the
proposed untargeted backdoor attack to natively circumvent existing backdoor
defense methods. To implement the untargeted backdoor attack under controlled
flexibility, we propose to apply logit normalization on cross-entropy loss with
flipped one-hot labels. By constraining the logit during training, the
compromised model will show a uniform distribution across selected target
classes, resulting in controlled untargeted attack. Extensive experiments
demonstrate the effectiveness of the proposed CUBA on different datasets.

</details>


### [410] [Differentiation-Based Extraction of Proprietary Data from Fine-Tuned LLMs](https://arxiv.org/abs/2506.17353)
*Zongjie Li,Daoyuan Wu,Shuai Wang,Zhendong Su*

Main category: cs.CR

TL;DR: 本文首次研究了监督微调（SFT）数据提取问题，提出了一种名为差异化数据提取（DDE）的新方法，并验证了其有效性，同时提出了防御机制。


<details>
  <summary>Details</summary>
Motivation: 随着领域特定和人类对齐的大语言模型（LLMs）需求增加，SFT数据集成为潜在提取目标，研究其数据泄露风险至关重要。

Method: 通过分析直接提取行为，提出DDE方法，利用微调模型的置信度及其与预训练基模型的差异进行数据提取。

Result: 实验表明，DDE在所有攻击场景中均优于现有基线方法，验证了SFT数据提取的可行性。

Conclusion: 研究揭示了微调LLMs中隐藏的数据泄露风险，为开发更安全的模型提供了见解。

Abstract: The increasing demand for domain-specific and human-aligned Large Language
Models (LLMs) has led to the widespread adoption of Supervised Fine-Tuning
(SFT) techniques. SFT datasets often comprise valuable instruction-response
pairs, making them highly valuable targets for potential extraction. This paper
studies this critical research problem for the first time. We start by formally
defining and formulating the problem, then explore various attack goals, types,
and variants based on the unique properties of SFT data in real-world
scenarios. Based on our analysis of extraction behaviors of direct extraction,
we develop a novel extraction method specifically designed for SFT models,
called Differentiated Data Extraction (DDE), which exploits the confidence
levels of fine-tuned models and their behavioral differences from pre-trained
base models. Through extensive experiments across multiple domains and
scenarios, we demonstrate the feasibility of SFT data extraction using DDE. Our
results show that DDE consistently outperforms existing extraction baselines in
all attack settings. To counter this new attack, we propose a defense mechanism
that mitigates DDE attacks with minimal impact on model performance. Overall,
our research reveals hidden data leak risks in fine-tuned LLMs and provides
insights for developing more secure models.

</details>


### [411] [Shrinking the Generation-Verification Gap with Weak Verifiers](https://arxiv.org/abs/2506.18203)
*Jon Saad-Falcon,E. Kelly Buchanan,Mayee F. Chen,Tzu-Heng Huang,Brendan McLaughlin,Tanvir Bhathal,Shang Zhu,Ben Athiwaratkun,Frederic Sala,Scott Linderman,Azalia Mirhoseini,Christopher Ré*

Main category: cs.CR

TL;DR: Weaver框架通过结合多个弱验证器，显著提升了语言模型生成候选答案的选择性能，减少了标记数据的依赖。


<details>
  <summary>Details</summary>
Motivation: 当前高质量验证器要么不可扩展（如人类），要么实用性有限（如工具Lean），而语言模型验证器与完美验证器之间存在性能差距。

Method: Weaver通过加权集成多个弱验证器，利用弱监督估计其准确性，并结合输出为统一分数，同时处理验证器输出格式不一致和低质量验证器问题。

Result: Weaver显著提升了候选答案选择性能，达到接近完美验证器的准确性（87.7%），同时减少了计算成本。

Conclusion: Weaver为设计高效验证器提供了可行方案，显著缩小了与完美验证器的性能差距。

Abstract: Verifiers can improve language model capabilities by scoring and ranking
responses from generated candidates. Currently, high-quality verifiers are
either unscalable (e.g., humans) or limited in utility (e.g., tools like Lean).
While LM judges and reward models have become broadly useful as general-purpose
verifiers, a significant performance gap remains between them and oracle
verifiers (verifiers with perfect accuracy). To help close this gap, we
introduce Weaver, a framework for designing a strong verifier by combining
multiple weak, imperfect verifiers. We find weighted ensembles of verifiers,
which typically require learning from labeled data, significantly outperform
unweighted combinations due to differences in verifier accuracies. To reduce
dependency on labeled data, Weaver leverages weak supervision to estimate each
verifier's accuracy and combines outputs into a unified score that better
reflects true response quality. However, directly applying weak supervision
algorithms poses challenges, including inconsistent verifier output formats and
handling low-quality verifiers. Weaver addresses these using dataset statistics
to normalize outputs and filter specific verifiers. We study Weaver's
effectiveness in test-time repeated sampling, where a model generates multiple
candidate responses and selects one. Our evaluations show Weaver significantly
improves over Pass@1-performance when selecting the first candidate-across
reasoning and math tasks, achieving o3-mini-level accuracy with Llama 3.3 70B
Instruct as generator, and an ensemble of 70B or smaller judge and reward
models as verifiers (87.7% average). This gain mirrors the jump between GPT-4o
and o3-mini (69.0% vs. 86.7%), which required extensive finetuning and
post-training. To reduce computational costs of verifier ensembles, we train a
400M cross-encoder using Weaver's combined output scores.

</details>


### [412] [Mechanistic Interpretability in the Presence of Architectural Obfuscation](https://arxiv.org/abs/2506.18053)
*Marcos Florencio,Thomas Barton*

Main category: cs.CR

TL;DR: 研究了架构混淆对大型语言模型（LLM）可解释性的影响，发现混淆虽改变激活模式但保留计算图，阻碍细粒度解释但保持任务性能。


<details>
  <summary>Details</summary>
Motivation: 探讨架构混淆是否真正阻碍模型工作机制的理解，还是仅将电路映射到不熟悉的坐标系。

Method: 在GPT-2-small模型上应用代表性混淆映射，使用logit-lens归因、因果路径修补和注意力头消融技术分析。

Result: 混淆显著改变注意力头激活模式但保留层间计算图，阻碍用户提示的反向工程，但前馈和残差路径功能完好。

Conclusion: 架构混淆可同时保持全局模型行为并阻碍用户内容机制分析，为隐私防御和可解释性工具提供指导。

Abstract: Architectural obfuscation - e.g., permuting hidden-state tensors, linearly
transforming embedding tables, or remapping tokens - has recently gained
traction as a lightweight substitute for heavyweight cryptography in
privacy-preserving large-language-model (LLM) inference. While recent work has
shown that these techniques can be broken under dedicated reconstruction
attacks, their impact on mechanistic interpretability has not been
systematically studied. In particular, it remains unclear whether scrambling a
network's internal representations truly thwarts efforts to understand how the
model works, or simply relocates the same circuits to an unfamiliar coordinate
system. We address this gap by analyzing a GPT-2-small model trained from
scratch with a representative obfuscation map. Assuming the obfuscation map is
private and the original basis is hidden (mirroring an honest-but-curious
server), we apply logit-lens attribution, causal path-patching, and
attention-head ablation to locate and manipulate known circuits. Our findings
reveal that obfuscation dramatically alters activation patterns within
attention heads yet preserves the layer-wise computational graph. This
disconnect hampers reverse-engineering of user prompts: causal traces lose
their alignment with baseline semantics, and token-level logit attributions
become too noisy to reconstruct. At the same time, feed-forward and residual
pathways remain functionally intact, suggesting that obfuscation degrades
fine-grained interpretability without compromising top-level task performance.
These results establish quantitative evidence that architectural obfuscation
can simultaneously (i) retain global model behaviour and (ii) impede
mechanistic analyses of user-specific content. By mapping where
interpretability breaks down, our study provides guidance for future privacy
defences and for robustness-aware interpretability tooling.

</details>


### [413] [Federated Learning-Based Data Collaboration Method for Enhancing Edge Cloud AI System Security Using Large Language Models](https://arxiv.org/abs/2506.18087)
*Huaiying Luo,Cheng Ji*

Main category: cs.CR

TL;DR: 本文提出了一种基于联邦学习的数据协作方法，结合大型语言模型（LLMs）和安全多方计算协议，以提高边缘云AI系统的安全性和数据隐私保护。


<details>
  <summary>Details</summary>
Motivation: 随着边缘计算和云系统在AI驱动应用中的广泛应用，如何在保证数据隐私的同时维持高效性能成为迫切的安全问题。

Method: 在现有联邦学习框架基础上，引入安全多方计算协议，利用LLM优化分布式节点间的数据聚合和加密过程，并结合对抗训练技术增强系统安全性。

Result: 实验结果表明，该方法在数据保护和模型鲁棒性方面比传统联邦学习方法提高了15%。

Conclusion: 该方法有效提升了边缘云AI系统的数据隐私保护和安全性，同时增强了系统对数据泄露和模型投毒等威胁的抵抗力。

Abstract: With the widespread application of edge computing and cloud systems in
AI-driven applications, how to maintain efficient performance while ensuring
data privacy has become an urgent security issue. This paper proposes a
federated learning-based data collaboration method to improve the security of
edge cloud AI systems, and use large-scale language models (LLMs) to enhance
data privacy protection and system robustness. Based on the existing federated
learning framework, this method introduces a secure multi-party computation
protocol, which optimizes the data aggregation and encryption process between
distributed nodes by using LLM to ensure data privacy and improve system
efficiency. By combining advanced adversarial training techniques, the model
enhances the resistance of edge cloud AI systems to security threats such as
data leakage and model poisoning. Experimental results show that the proposed
method is 15% better than the traditional federated learning method in terms of
data protection and model robustness.

</details>


### [414] [Smart-LLaMA-DPO: Reinforced Large Language Model for Explainable Smart Contract Vulnerability Detection](https://arxiv.org/abs/2506.18245)
*Lei Yu,Zhirong Huang,Hang Yuan,Shiqi Cheng,Li Yang,Fengjun Zhang,Chenjie Shen,Jiajia Ma,Jingyuan Zhang,Junyi Lu,Chun Zuo*

Main category: cs.CR

TL;DR: 论文提出Smart-LLaMA-DPO方法，基于LLaMA-3.1-8B，通过构建全面数据集和改进训练流程，显著提升智能合约漏洞检测的性能和解释质量。


<details>
  <summary>Details</summary>
Motivation: 现有智能合约漏洞检测方法存在数据集不全面和大型语言模型（LLMs）解释不准确的问题。

Method: 构建全面数据集，进行持续预训练（CPT）、监督微调（SFT）和直接偏好优化（DPO），改进LLMs对智能合约安全的理解。

Result: 在四种主要漏洞类型和机器不可审计漏洞上，F1分数和准确率分别平均提升10.43%和7.87%，解释更正确、全面和清晰。

Conclusion: Smart-LLaMA-DPO显著优于现有方法，解决了数据集和LLMs解释的局限性。

Abstract: Smart contract vulnerability detection remains a major challenge in
blockchain security. Existing vulnerability detection methods face two main
issues: (1) Existing datasets lack comprehensive coverage and high-quality
explanations for preference learning. (2) Large language models (LLMs) often
struggle with accurately interpreting specific concepts in smart contract
security. Empirical analysis shows that even after continual pre-training (CPT)
and supervised fine-tuning (SFT), LLMs may misinterpret the execution order of
state changes, resulting in incorrect explanations despite making correct
detection decisions. To address these challenges, we propose Smart-LLaMA-DPO
based on LLaMA-3.1-8B. We construct a comprehensive dataset covering four major
vulnerability types and machine-unauditable vulnerabilities, including precise
labels, explanations, and locations for SFT, as well as high-quality and
low-quality output pairs for Direct Preference Optimization (DPO). Second, we
perform CPT using large-scale smart contract to enhance the LLM's understanding
of specific security practices in smart contracts. Futhermore, we conduct SFT
with our comprehensive dataset. Finally, we apply DPO, leveraging human
feedback and a specially designed loss function that increases the probability
of preferred explanations while reducing the likelihood of non-preferred
outputs. We evaluate Smart-LLaMA-DPO on four major vulnerability types:
reentrancy, timestamp dependence, integer overflow/underflow, and delegatecall,
as well as machine-unauditable vulnerabilities. Our method significantly
outperforms state-of-the-art baselines, with average improvements of 10.43% in
F1 score and 7.87% in accuracy. Moreover, both LLM evaluation and human
evaluation confirm that our method generates more correct, thorough, and clear
explanations.

</details>


### [415] [Security Assessment of DeepSeek and GPT Series Models against Jailbreak Attacks](https://arxiv.org/abs/2506.18543)
*Xiaodong Wu,Xiangman Li,Jianbing Ni*

Main category: cs.CR

TL;DR: 本文首次系统评估了DeepSeek系列模型在对抗性攻击（jailbreak）下的鲁棒性，并与GPT-3.5和GPT-4进行了比较。研究发现DeepSeek的MoE架构在特定攻击下表现较好，但在其他攻击下更脆弱，而GPT-4 Turbo表现更稳定。


<details>
  <summary>Details</summary>
Motivation: 随着开源大语言模型（如DeepSeek）的广泛应用，其对抗性攻击的脆弱性尚未充分研究，亟需系统评估以揭示潜在风险。

Method: 使用HarmBench基准测试，评估了7种攻击策略对510种有害行为的有效性，对比了DeepSeek、GPT-3.5和GPT-4的表现。

Result: DeepSeek的MoE架构对优化类攻击（如TAP-T）有选择性鲁棒性，但对提示类和手工攻击更脆弱；GPT-4 Turbo表现更稳定。

Conclusion: 开源LLM需在架构效率与安全性之间权衡，建议针对性安全调优和模块化对齐策略以确保安全部署。

Abstract: The widespread deployment of large language models (LLMs) has raised critical
concerns over their vulnerability to jailbreak attacks, i.e., adversarial
prompts that bypass alignment mechanisms and elicit harmful or policy-violating
outputs. While proprietary models like GPT-4 have undergone extensive
evaluation, the robustness of emerging open-source alternatives such as
DeepSeek remains largely underexplored, despite their growing adoption in
real-world applications. In this paper, we present the first systematic
jailbreak evaluation of DeepSeek-series models, comparing them with GPT-3.5 and
GPT-4 using the HarmBench benchmark. We evaluate seven representative attack
strategies across 510 harmful behaviors categorized by both function and
semantic domain. Our analysis reveals that DeepSeek's Mixture-of-Experts (MoE)
architecture introduces routing sparsity that offers selective robustness
against optimization-based attacks such as TAP-T, but leads to significantly
higher vulnerability under prompt-based and manually engineered attacks. In
contrast, GPT-4 Turbo demonstrates stronger and more consistent safety
alignment across diverse behaviors, likely due to its dense Transformer design
and reinforcement learning from human feedback. Fine-grained behavioral
analysis and case studies further show that DeepSeek often routes adversarial
prompts to under-aligned expert modules, resulting in inconsistent refusal
behaviors. These findings highlight a fundamental trade-off between
architectural efficiency and alignment generalization, emphasizing the need for
targeted safety tuning and modular alignment strategies to ensure secure
deployment of open-source LLMs.

</details>


<div id='cs.SE'></div>

# cs.SE [[Back]](#toc)

### [416] [LMR-BENCH: Evaluating LLM Agent's Ability on Reproducing Language Modeling Research](https://arxiv.org/abs/2506.17335)
*Shuo Yan,Ruochen Li,Ziming Luo,Zimu Wang,Daoyang Li,Liqiang Jing,Kaiyu He,Peilin Wu,George Michalopoulos,Yue Zhang,Ziyang Zhang,Mian Zhang,Zhiyu Chen,Xinya Du*

Main category: cs.SE

TL;DR: LMR-BENCH是一个评估LLM代理从NLP研究论文中重现代码能力的基准，揭示了当前先进模型在科学推理和代码合成上的局限性。


<details>
  <summary>Details</summary>
Motivation: 填补LLM代理在从研究论文中重现代码任务上的能力空白，尤其是NLP领域中的复杂推理挑战。

Method: 构建包含28个代码重现任务的LMR-BENCH基准，基于23篇顶级NLP论文，测试模型在标准提示和LLM代理设置下的表现。

Result: 实验显示，即使最先进的模型在科学推理和代码合成上仍存在显著局限性。

Conclusion: LLM代理在自主重现科学研究代码方面仍有重大不足，需进一步改进。

Abstract: Large language model (LLM) agents have demonstrated remarkable potential in
advancing scientific discovery. However, their capability in the fundamental
yet crucial task of reproducing code from research papers, especially in the
NLP domain, remains underexplored. This task includes unique complex reasoning
challenges in the intellectual synthesis of abstract concepts and the
comprehension of code repositories with interdependent files. Motivated by this
gap, we present LMR-BENCH, a benchmark designed to systematically evaluate the
capability of LLM agents on code reproduction from Language Modeling Research.
It consists of 28 code reproduction tasks derived from 23 research papers
published in top-tier NLP venues over the past five years, spanning nine
fundamental categories. Models are provided with a research paper, a code
repository containing one or more masked functions, and instructions for
implementing these functions. We conduct extensive experiments in standard
prompting and LLM agent settings with state-of-the-art LLMs, evaluating the
accuracy of unit tests and performing LLM-based evaluation of code correctness.
Experimental results reveal that even the most advanced models still exhibit
persistent limitations in scientific reasoning and code synthesis, highlighting
critical gaps in LLM agents' ability to autonomously reproduce scientific
research

</details>


### [417] [Re-Evaluating Code LLM Benchmarks Under Semantic Mutation](https://arxiv.org/abs/2506.17369)
*Zhiyuan Pan,Xing Hu,Xin Xia,Xiaohu Yang*

Main category: cs.SE

TL;DR: 论文研究了代码基准测试中提示敏感性对大型语言模型（LLM）性能评估的影响，提出了一种修改提示模板的框架，并通过实验发现轻微提示变化会导致显著性能波动。


<details>
  <summary>Details</summary>
Motivation: 现有代码基准测试通常依赖单一提示模板，容易因提示敏感性导致评估不可靠，而此前研究仅限于传统自然语言处理任务。

Method: 提出一个修改提示模板的框架，保持语义和结构一致，并在8个代码基准任务上对10个开源LLM进行实验，每个任务使用100个语义相似的提示模板。

Result: 实验表明，轻微提示变化会导致性能显著波动，并可能影响不同模型的性能排名。

Conclusion: 未来设计代码基准测试时需考虑提示敏感性，以确保更可靠和准确的LLM能力评估。

Abstract: In the era of large language models (LLMs), code benchmarks have become an
important research area in software engineering and are widely used by
practitioners. These benchmarks evaluate the performance of LLMs on specific
code-related tasks, such as code understanding and generation. A critical step
in constructing code benchmarks is the design of prompts. However, as existing
code benchmarks typically rely on a single prompt template per task, they are
prone to the issue of prompt sensitivity, where minor prompt variations could
result in substantial performance variations, leading to unreliable evaluations
of model capabilities.
  While previous studies have explored prompt sensitivity, their experimental
designs and findings are limited to traditional natural language processing
(NLP) tasks. In this paper, we present an empirical study to investigate prompt
sensitivity in code benchmarks. We first propose a general framework that
modifies prompt templates in a manner that preserves both their semantics and
their structure as much as possible. Based on the framework, we conduct
extensive experiments across eight code benchmark tasks on 10 representative
open-source LLMs, with each task featuring 100 semantically similar prompt
templates. We then analyze the evaluation results using various statistical
metrics, focusing on both absolute and relative model performance. Our findings
suggest that even slight prompt variations can lead to significant shifts in
performance. Additionally, we observe that such variations can introduce
inconsistencies in the performance rankings across different models. These
insights highlight the need for considering prompt sensitivity when designing
future code benchmarks, to ensure more reliable and accurate evaluation of LLM
capabilities.

</details>


### [418] [Software Reuse in the Generative AI Era: From Cargo Cult Towards AI Native Software Engineering](https://arxiv.org/abs/2506.17937)
*Tommi Mikkonen,Antero Taivalsaari*

Main category: cs.SE

TL;DR: 论文探讨了AI辅助生成软件重用对新兴“AI原生”软件工程的影响，并提出了相关问题和研究议程。


<details>
  <summary>Details</summary>
Motivation: 随着AI和生成式软件重用成为软件开发的核心，传统方法正被AI辅助方法取代，这引发了类似“货物崇拜开发”的问题。

Method: 通过讨论AI辅助生成软件重用的影响，提出关键问题，并定义研究议程。

Result: 揭示了AI辅助软件重用可能带来的问题，并呼吁采取行动解决这些问题。

Conclusion: 需要进一步研究和行动以应对AI辅助软件重用带来的挑战。

Abstract: Software development is currently under a paradigm shift in which artificial
intelligence and generative software reuse are taking the center stage in
software creation. Consequently, earlier software reuse practices and methods
are rapidly being replaced by AI-assisted approaches in which developers place
their trust on code that has been generated by artificial intelligence. This is
leading to a new form of software reuse that is conceptually not all that
different from cargo cult development. In this paper we discuss the
implications of AI-assisted generative software reuse in the context of
emerging "AI native" software engineering, bring forth relevant questions, and
define a tentative research agenda and call to action for tackling some of the
central issues associated with this approach.

</details>


### [419] [Call Me Maybe: Enhancing JavaScript Call Graph Construction using Graph Neural Networks](https://arxiv.org/abs/2506.18191)
*Masudul Hasan Masud Bhuiyan,Gianluca De Stefano,Giancarlo Pellegrino,Cristian-Alexandru Staicu*

Main category: cs.SE

TL;DR: GRAPHIA利用图神经网络改进JavaScript调用图构建，通过链接预测提高召回率，减少人工分析工作量。


<details>
  <summary>Details</summary>
Motivation: 现有JavaScript调用图构建工具既不健全也不完整，存在误报和遗漏。

Method: 将问题建模为全程序图上的链接预测，结合句法和语义边表示程序，利用图神经网络建模非局部关系。

Result: 在50个流行JavaScript库上评估，GRAPHIA在42%的未解决调用中将正确目标排名第一，72%的案例中排名前五。

Conclusion: 基于学习的方法可显著提高JavaScript调用图构建的召回率，是首个将GNN应用于多文件程序图进行跨过程分析的工作。

Abstract: Static analysis plays a key role in finding bugs, including security issues.
A critical step in static analysis is building accurate call graphs that model
function calls in a program. However, due to hard-to-analyze language features,
existing call graph construction algorithms for JavaScript are neither sound
nor complete. Prior work shows that even advanced solutions produce false edges
and miss valid ones. In this work, we assist these tools by identifying missed
call edges. Our main idea is to frame the problem as link prediction on full
program graphs, using a rich representation with multiple edge types. Our
approach, GRAPHIA, leverages recent advances in graph neural networks to model
non-local relationships between code elements. Concretely, we propose
representing JavaScript programs using a combination of syntactic- and
semantic-based edges. GRAPHIA can learn from imperfect labels, including static
call edges from existing tools and dynamic edges from tests, either from the
same or different projects. Because call graphs are sparse, standard machine
learning metrics like ROC are not suitable. Instead, we evaluate GRAPHIA by
ranking function definitions for each unresolved call site. We conduct a
large-scale evaluation on 50 popular JavaScript libraries with 163K call edges
(150K static and 13K dynamic). GRAPHIA builds program graphs with 6.6M
structural and 386K semantic edges. It ranks the correct target as the top
candidate in over 42% of unresolved cases and within the top 5 in 72% of cases,
reducing the manual effort needed for analysis. Our results show that
learning-based methods can improve the recall of JavaScript call graph
construction. To our knowledge, this is the first work to apply GNN-based link
prediction to full multi-file program graphs for interprocedural analysis.

</details>


### [420] [Tu(r)ning AI Green: Exploring Energy Efficiency Cascading with Orthogonal Optimizations](https://arxiv.org/abs/2506.18289)
*Saurabhsingh Rajput,Mootez Saad,Tushar Sharma*

Main category: cs.SE

TL;DR: 论文提出将能源效率作为AI设计核心，通过五个阶段的策略组合，显著降低能耗并保持性能。


<details>
  <summary>Details</summary>
Motivation: AI快速发展带来高能耗问题，现有优化方法缺乏系统性，需将能源效率作为首要设计目标。

Method: 在数据、模型、训练、系统和推理五个阶段进行策略性组合优化。

Result: 实验显示能耗降低94.6%，同时保持95.95%的原始F1分数。

Conclusion: 系统性优化框架为可持续AI提供了高效、性能与环保的平衡方案。

Abstract: AI's exponential growth intensifies computational demands and energy
challenges. While practitioners employ various optimization techniques, that we
refer as "knobs" in this paper, to tune model efficiency, these are typically
afterthoughts and reactive ad-hoc changes applied in isolation without
understanding their combinatorial effects on energy efficiency. This paper
emphasizes on treating energy efficiency as the first-class citizen and as a
fundamental design consideration for a compute-intensive pipeline. We show that
strategic selection across five AI pipeline phases (data, model, training,
system, inference) creates cascading efficiency. Experimental validation shows
orthogonal combinations reduce energy consumption by up to $94.6$% while
preserving $95.95$% of the original F1 score of non-optimized pipelines. This
curated approach provides actionable frameworks for informed sustainable AI
that balance efficiency, performance, and environmental responsibility.

</details>


### [421] [Use Property-Based Testing to Bridge LLM Code Generation and Validation](https://arxiv.org/abs/2506.18315)
*Lehan He,Zeren Chen,Zhe Zhang,Jing Shao,Xiang Gao,Lu Sheng*

Main category: cs.SE

TL;DR: 论文提出Property-Generated Solver框架，利用基于属性的测试（PBT）验证程序的高层属性，而非具体输入输出，以提升大语言模型（LLMs）生成代码的功能正确性。


<details>
  <summary>Details</summary>
Motivation: 传统测试驱动开发（TDD）在LLMs代码生成中存在高质量测试用例稀缺和自动生成测试的偏差问题，导致验证效果不佳。

Method: 框架包含两个协作的LLM代理：Generator负责代码生成与迭代优化，Tester管理PBT生命周期并提供语义丰富的反馈。

Result: 实验表明，Property-Generated Solver在多个代码生成基准测试中，pass@1指标相对TDD方法提升了23.1%至37.3%。

Conclusion: 通过PBT作为核心验证机制，该框架显著提升了LLMs生成代码的正确性和泛化能力。

Abstract: Large Language Models (LLMs) excel at code generation, but ensuring their
outputs to be functionally correct, especially in complex programming tasks, is
a persistent challenge. While traditional Test-Driven Development (TDD) offers
a path for code refinement, its efficacy with LLMs is often undermined by the
scarcity of high-quality test cases or the pitfalls of automated test
generation, including biased tests or inaccurate output predictions that can
misdirect the correction process. This paper introduces Property-Generated
Solver, a novel framework that leverages Property-Based Testing (PBT) to
validate high-level program properties or invariants, instead of relying on
specific input-output examples. These properties are often simpler to define
and verify than directly predicting exhaustive test oracles, breaking the
"cycle of self-deception" where tests might share flaws with the code they are
meant to validate. Property-Generated Solver employs two collaborative
LLM-based agents: a Generator dedicated to code generation and iterative
refinement, and a Tester that manages the PBT life-cycle and formulate
semantically rich feedback from property violations. The resulting
comprehensive and actionable feedback then guides the Generator in its
refinement efforts. By establishing PBT as the core validation engine within
this iterative, closed-loop paradigm, Property-Generated Solver provides a
robust mechanism for steering LLMs towards more correct and generalizable code.
Extensive experimental results on multiple code generation benchmarks
demonstrate that Property-Generated Solver achieves substantial pass@1
improvements, ranging from 23.1% to 37.3% relative gains over established TDD
methods.

</details>


### [422] [The Debugging Decay Index: Rethinking Debugging Strategies for Code LLMs](https://arxiv.org/abs/2506.18403)
*Muntasir Adnan,Carlos C. N. Kuhn*

Main category: cs.SE

TL;DR: 论文提出调试衰减指数（DDI），量化AI调试能力衰减，并通过策略性重启提升调试效果。


<details>
  <summary>Details</summary>
Motivation: 现有AI调试能力在2-3次尝试后显著下降，亟需量化框架优化迭代调试策略。

Method: 引入DDI框架，结合策略性重启（从利用转向探索）以提升调试效果。

Result: DDI揭示了当前AI调试的根本限制，并提供了优化代码生成策略的量化工具。

Conclusion: DDI为迭代代码生成策略提供了首个定量优化框架，显著提升调试效率。

Abstract: The effectiveness of AI debugging follows a predictable exponential decay
pattern; most models lose 60-80% of their debugging capability within just 2-3
attempts, despite iterative debugging being a critical capability for practical
code generation systems. We introduce the Debugging Decay Index (DDI), a
mathematical framework that quantifies when debugging becomes ineffective and
predicts intervention points. Our strategic fresh start approach shifts from
exploitation to exploration at strategic points in the debugging process,
demonstrating that well-timed interventions can rescue the effectiveness of
debugging. DDI reveals a fundamental limitation in current AI debugging and
provides the first quantitative framework for optimising iterative code
generation strategies.

</details>


### [423] [Understanding Software Engineering Agents: A Study of Thought-Action-Result Trajectories](https://arxiv.org/abs/2506.18824)
*Islem Bouzenia,Michael Pradel*

Main category: cs.SE

TL;DR: 该论文通过大规模实证研究分析了三种基于LLM的代理（RepairAgent、AutoCodeRover和OpenHands）的决策过程，揭示了成功与失败执行的行为模式和反模式，为代理设计提供了改进建议。


<details>
  <summary>Details</summary>
Motivation: 尽管基于LLM的代理在软件工程任务中广泛应用，但其内部决策过程尚未充分研究，限制了对其运行动态和失败模式的理解。

Method: 研究统一了三种代理的交互日志，分析了120条轨迹和2822次LLM交互，结合定量和定性方法评估了结构特性、动作模式和推理连贯性。

Result: 研究发现成功与失败执行的行为差异，包括迭代次数、令牌消耗和动作序列，并提出了改进代理设计的建议。

Conclusion: 研究为透明和鲁棒的自洽软件工程代理提供了数据集和框架，支持进一步研究。

Abstract: Large Language Model (LLM)-based agents are increasingly employed to automate
complex software engineering tasks such as program repair and issue resolution.
These agents operate by autonomously generating natural language thoughts,
invoking external tools, and iteratively refining their solutions. Despite
their widespread adoption, the internal decision-making processes of these
agents remain largely unexplored, limiting our understanding of their
operational dynamics and failure modes. In this paper, we present a large-scale
empirical study of the thought-action-result trajectories of three
state-of-the-art LLM-based agents: \textsc{RepairAgent},
\textsc{AutoCodeRover}, and \textsc{OpenHands}. We unify their interaction logs
into a common format, capturing 120 trajectories and 2822 LLM interactions
focused on program repair and issue resolution. Our study combines quantitative
analyses of structural properties, action patterns, and token usage with
qualitative assessments of reasoning coherence and feedback integration. We
identify key trajectory characteristics such as iteration counts and token
consumption, recurring action sequences, and the semantic coherence linking
thoughts, actions, and their results. Our findings reveal behavioral motifs and
anti-patterns that distinguish successful from failed executions, providing
actionable insights for improving agent design, including prompting strategies,
failure diagnosis, and anti-pattern detection. We release our dataset and
annotation framework to support further research on transparent and robust
autonomous software engineering agents.

</details>


<div id='math.PR'></div>

# math.PR [[Back]](#toc)

### [424] [Greedy Selection under Independent Increments: A Toy Model Analysis](https://arxiv.org/abs/2506.17941)
*Huitao Yang*

Main category: math.PR

TL;DR: 论文研究了N个独立同分布的离散时间随机过程的迭代选择问题，证明在每阶段贪心选择是获得最终最大值过程的最优策略。


<details>
  <summary>Details</summary>
Motivation: 探讨在多阶段淘汰设置中贪心启发式方法的合理性，为高维应用中的相关算法提供理论支持。

Method: 在每阶段基于观测值保留固定数量的过程，利用强独立性假设分析贪心选择策略。

Result: 证明贪心选择策略在每阶段是最优的，能够确保最终选择最大值过程。

Conclusion: 贪心策略在多阶段淘汰问题中具有理论最优性，可作为高维算法设计的参考模型。

Abstract: We study an iterative selection problem over N i.i.d. discrete-time
stochastic processes with independent increments. At each stage, a fixed number
of processes are retained based on their observed values. Under this simple
model, we prove that the optimal strategy for selecting the final maximum-value
process is to apply greedy selection at each stage. While the result relies on
strong independence assumptions, it offers a clean justification for greedy
heuristics in multi-stage elimination settings and may serve as a toy example
for understanding related algorithms in high-dimensional applications.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [425] [Embedded FPGA Acceleration of Brain-Like Neural Networks: Online Learning to Scalable Inference](https://arxiv.org/abs/2506.18530)
*Muhammad Ihsan Al Hafiz,Naresh Ravichandran,Anders Lansner,Pawel Herman,Artur Podobas*

Main category: cs.AR

TL;DR: 该论文提出了一种基于FPGA的嵌入式加速器，用于实现BCPNN（一种类脑神经网络），在边缘设备上实现高效学习和推理，显著降低延迟和能耗。


<details>
  <summary>Details</summary>
Motivation: 传统深度学习模型在边缘设备上能耗高且依赖云端，而类脑神经网络（如BCPNN）具有稀疏架构和本地学习规则，适合低功耗边缘智能，但现有实现依赖GPU或数据中心FPGA，限制了嵌入式应用。

Method: 采用高级综合技术在Zynq UltraScale+ SoC上实现首个嵌入式FPGA加速器，支持在线学习和推理，并支持可变和混合精度。

Result: 在MNIST、肺炎和乳腺癌数据集上测试，加速器比ARM基线延迟降低17.5倍，能耗节省94%，且不损失准确性。

Conclusion: 该工作填补了类脑学习与实际部署之间的差距，为边缘设备上的实用神经形态计算提供了可能。

Abstract: Edge AI applications increasingly require models that can learn and adapt
on-device with minimal energy budget. Traditional deep learning models, while
powerful, are often overparameterized, energy-hungry, and dependent on cloud
connectivity. Brain-Like Neural Networks (BLNNs), such as the Bayesian
Confidence Propagation Neural Network (BCPNN), propose a neuromorphic
alternative by mimicking cortical architecture and biologically-constrained
learning. They offer sparse architectures with local learning rules and
unsupervised/semi-supervised learning, making them well-suited for low-power
edge intelligence. However, existing BCPNN implementations rely on GPUs or
datacenter FPGAs, limiting their applicability to embedded systems. This work
presents the first embedded FPGA accelerator for BCPNN on a Zynq UltraScale+
SoC using High-Level Synthesis. We implement both online learning and
inference-only kernels with support for variable and mixed precision. Evaluated
on MNIST, Pneumonia, and Breast Cancer datasets, our accelerator achieves up to
17.5x latency and 94% energy savings over ARM baselines, without sacrificing
accuracy. This work enables practical neuromorphic computing on edge devices,
bridging the gap between brain-like learning and real-world deployment.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [426] [PBFT-Backed Semantic Voting for Multi-Agent Memory Pruning](https://arxiv.org/abs/2506.17338)
*Duong Bach*

Main category: cs.DC

TL;DR: 论文提出了一种名为Co-Forgetting Protocol的新框架，用于在多智能体系统中同步修剪共享记忆，结合语义投票、时间衰减函数和PBFT共识机制，显著减少了内存占用并提高了决策准确性。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在复杂动态环境中需要高效管理共享知识，避免过时或无关数据的积累，类似于生物遗忘过程。

Method: 协议包含三个关键组件：基于DistilBERT的语义投票、多尺度时间衰减函数和PBFT共识机制，使用gRPC和Pinecone等技术实现高效通信与存储。

Result: 实验显示，协议在500个周期内减少52%内存占用，遗忘决策准确率达88%，PBFT共识成功率为92%，内存访问缓存命中率为82%。

Conclusion: Co-Forgetting Protocol有效解决了多智能体系统中的记忆同步问题，具有高效性和鲁棒性。

Abstract: The proliferation of multi-agent systems (MAS) in complex, dynamic
environments necessitates robust and efficient mechanisms for managing shared
knowledge. A critical challenge is ensuring that distributed memories remain
synchronized, relevant, and free from the accumulation of outdated or
inconsequential data - a process analogous to biological forgetting. This paper
introduces the Co-Forgetting Protocol, a novel, comprehensive framework
designed to address this challenge by enabling synchronized memory pruning in
MAS. The protocol integrates three key components: (1) context-aware semantic
voting, where agents utilize a lightweight DistilBERT model to assess the
relevance of memory items based on their content and the current operational
context; (2) multi-scale temporal decay functions, which assign diminishing
importance to memories based on their age and access frequency across different
time horizons; and (3) a Practical Byzantine Fault Tolerance (PBFT)-based
consensus mechanism, ensuring that decisions to retain or discard memory items
are agreed upon by a qualified and fault-tolerant majority of agents, even in
the presence of up to f Byzantine (malicious or faulty) agents in a system of N
greater than or equal to 3f+1 agents. The protocol leverages gRPC for efficient
inter-agent communication and Pinecone for scalable vector embedding storage
and similarity search, with SQLite managing metadata. Experimental evaluations
in a simulated MAS environment with four agents demonstrate the protocol's
efficacy, achieving a 52% reduction in memory footprint over 500 epochs, 88%
voting accuracy in forgetting decisions against human-annotated benchmarks, a
92% PBFT consensus success rate under simulated Byzantine conditions, and an
82% cache hit rate for memory access.

</details>


### [427] [Speeding up Local Optimization in Vehicle Routing with Tensor-based GPU Acceleration](https://arxiv.org/abs/2506.17357)
*Zhenyu Lei,Jin-Kao Hao,Qinghua Wu*

Main category: cs.DC

TL;DR: 提出了一种基于张量的GPU加速方法，用于优化车辆路径问题（VRP）中的局部搜索操作，显著提高了计算效率。


<details>
  <summary>Details</summary>
Motivation: 局部搜索在VRP及其变体中计算成本高且耗时，尤其是在大规模或复杂约束问题中。

Method: 采用基于属性的张量表示法，将密集计算完全卸载到GPU，实现低耦合架构和广泛适用性。

Result: 在三种路由问题的基准实例上，该方法比传统CPU实现具有显著计算优势。

Conclusion: 该方法为局部搜索提供了高效解决方案，并指出了未来改进方向。

Abstract: Local search plays a central role in many effective heuristic algorithms for
the vehicle routing problem (VRP) and its variants. However, neighborhood
exploration is known to be computationally expensive and time consuming,
especially for large instances or problems with complex constraints. In this
study, we explore a promising direction to address this challenge by
introducing an original tensor-based GPU acceleration method designed to speed
up the commonly used local search operators in vehicle routing. By using an
attribute-based representation, the method offers broad extensibility, making
it applicable to different VRP variants. Its low-coupling architecture, with
intensive computations completely offloaded to the GPU, ensures seamless
integration in various local search-based algorithms and frameworks, leading to
significant improvements in computational efficiency and potentially improved
solution quality. Through comparative experiments on benchmark instances of
three routing problems, we demonstrate the substantial computational advantages
of the proposed approach over traditional CPU-based implementations. We also
provide a detailed analysis of the strengths and limitations of the method,
providing valuable insights into its performance characteristics and
identifying potential bottlenecks in practical applications. These findings
contribute to a better understanding and suggest directions for future
improvements.

</details>


### [428] [ConsumerBench: Benchmarking Generative AI Applications on End-User Devices](https://arxiv.org/abs/2506.17538)
*Yile Gu,Rohan Kadekodi,Hoang Nguyen,Keisuke Kamahori,Yiyu Liu,Baris Kasikci*

Main category: cs.DC

TL;DR: ConsumerBench是一个用于评估终端设备上GenAI模型系统效率和响应时间的基准测试框架，模拟多应用并发场景，揭示资源分配和调度问题。


<details>
  <summary>Details</summary>
Motivation: 随着GenAI应用从云端转向终端设备，资源管理、系统效率和用户体验面临新挑战，需要更真实的评估工具。

Method: ConsumerBench模拟多应用并发执行场景，支持自定义工作流，捕获应用级和系统级性能指标。

Result: 实验揭示了资源共享的低效性、贪婪分配的不公平性以及静态模型服务器配置的性能问题。

Conclusion: ConsumerBench为模型开发者和系统设计者提供了实用见解，强调定制内核和SLO感知调度策略的重要性。

Abstract: The recent shift in Generative AI (GenAI) applications from cloud-only
environments to end-user devices introduces new challenges in resource
management, system efficiency, and user experience. This paper presents
ConsumerBench, a comprehensive benchmarking framework designed to evaluate the
system efficiency and response time of GenAI models running on end-user
devices. Unlike existing benchmarks that assume exclusive model access on
dedicated GPUs, ConsumerBench simulates realistic multi-application scenarios
executing concurrently on constrained hardware. Furthermore, ConsumerBench
supports customizable workflows that simulate complex tasks requiring
coordination among multiple applications. ConsumerBench captures both
application-level metrics, including latency and Service Level Objective (SLO)
attainment, and system-level metrics like CPU/GPU utilization and memory
bandwidth. Through extensive experiments, ConsumerBench reveals inefficiencies
in resource sharing, unfair scheduling under greedy allocation, and performance
pitfalls of static model server configurations. The paper also provides
practical insights for model developers and system designers, highlighting the
benefits of custom kernels tailored to consumer-grade GPU architectures and the
value of implementing SLO-aware scheduling strategies.

</details>


### [429] [Research on Model Parallelism and Data Parallelism Optimization Methods in Large Language Model-Based Recommendation Systems](https://arxiv.org/abs/2506.17551)
*Haowei Yang,Yu Tian,Zhongheng Yang,Zhao Wang,Chengrui Zhou,Dannier Li*

Main category: cs.DC

TL;DR: 本文研究了在推荐系统中使用大语言模型（LLMs）时的分布式训练优化方法，包括模型并行和数据并行，提出了一种混合并行方案，显著提升了训练效率和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在推荐系统中的广泛应用，其庞大的参数规模和数据量导致了计算和通信瓶颈，亟需优化方法以提高效率。

Method: 研究了模型并行（包括张量并行和流水线并行）和数据并行（同步与异步模式），结合负载均衡、梯度压缩和稀疏化技术，提出了混合并行方案。

Result: 实验表明，混合并行方案比传统单模式并行提高了30%的训练吞吐量和约20%的资源利用率，同时保持了良好的扩展性和鲁棒性。

Conclusion: 讨论了不同并行策略的权衡，并展望了未来方向，如异构硬件集成和自动化调度技术。

Abstract: With the rapid adoption of large language models (LLMs) in recommendation
systems, the computational and communication bottlenecks caused by their
massive parameter sizes and large data volumes have become increasingly
prominent. This paper systematically investigates two classes of optimization
methods-model parallelism and data parallelism-for distributed training of LLMs
in recommendation scenarios. For model parallelism, we implement both tensor
parallelism and pipeline parallelism, and introduce an adaptive load-balancing
mechanism to reduce cross-device communication overhead. For data parallelism,
we compare synchronous and asynchronous modes, combining gradient compression
and sparsification techniques with an efficient aggregation communication
framework to significantly improve bandwidth utilization. Experiments conducted
on a real-world recommendation dataset in a simulated service environment
demonstrate that our proposed hybrid parallelism scheme increases training
throughput by over 30% and improves resource utilization by approximately 20%
compared to traditional single-mode parallelism, while maintaining strong
scalability and robustness. Finally, we discuss trade-offs among different
parallel strategies in online deployment and outline future directions
involving heterogeneous hardware integration and automated scheduling
technologies.

</details>


<div id='cs.DL'></div>

# cs.DL [[Back]](#toc)

### [430] [Mapping the Evolution of Research Contributions using KnoVo](https://arxiv.org/abs/2506.17508)
*Sajratul Y. Rubaiat,Syed N. Sakib,Hasan M. Jamil*

Main category: cs.DL

TL;DR: KnoVo是一个智能框架，用于量化分析科学文献中研究新颖性的演变，通过多维度比较和动态评分，帮助研究者评估原创性、发现研究空白。


<details>
  <summary>Details</summary>
Motivation: 传统引用分析主要衡量影响力，而KnoVo旨在量化研究新颖性，通过动态比较揭示知识的演变。

Method: 利用大型语言模型（LLMs）提取比较维度（如方法、应用、数据集），并通过类似锦标赛选择的方法生成定量新颖性评分。

Result: 通过分析20篇跨领域论文，验证了KnoVo在评估新颖性和发现研究空白方面的能力，并测试了不同开源LLMs的性能。

Conclusion: KnoVo为研究者提供了一种新颖的工具，能够动态跟踪知识演变，促进跨学科研究。

Abstract: This paper presents KnoVo (Knowledge Evolution), an intelligent framework
designed for quantifying and analyzing the evolution of research novelty in the
scientific literature. Moving beyond traditional citation analysis, which
primarily measures impact, KnoVo determines a paper's novelty relative to both
prior and subsequent work within its multilayered citation network. Given a
target paper's abstract, KnoVo utilizes Large Language Models (LLMs) to
dynamically extract dimensions of comparison (e.g., methodology, application,
dataset). The target paper is then compared to related publications along these
same extracted dimensions. This comparative analysis, inspired by tournament
selection, yields quantitative novelty scores reflecting the relative
improvement, equivalence, or inferiority of the target paper in specific
aspects. By aggregating these scores and visualizing their progression, for
instance, through dynamic evolution graphs and comparative radar charts, KnoVo
facilitates researchers not only to assess originality and identify similar
work, but also to track knowledge evolution along specific research dimensions,
uncover research gaps, and explore cross-disciplinary connections. We
demonstrate these capabilities through a detailed analysis of 20 diverse papers
from multiple scientific fields and report on the performance of various
open-source LLMs within the KnoVo framework.

</details>


### [431] [Unfolding the Past: A Comprehensive Deep Learning Approach to Analyzing Incunabula Pages](https://arxiv.org/abs/2506.18069)
*Klaudia Ropel,Krzysztof Kutt,Luiz do Valle Miranda,Grzegorz J. Nalepa*

Main category: cs.DL

TL;DR: 开发了一种自动分析早期印刷书籍页面结构和内容的方法，结合自定义数据集和公开数据集，使用YOLO、OCR和图像分类模型，取得了高精度结果。


<details>
  <summary>Details</summary>
Motivation: 探索机器学习在分析早期印刷书籍（如incunabula）结构和内容中的应用潜力。

Method: 使用YOLO11n和YOLO11s模型进行对象检测，结合Tesseract和Kraken OCR进行文本识别，以及ResNet18和CLIP模型进行图像分类和语义描述。

Result: YOLO11n模型在自定义数据集上表现最佳（F1=0.94），Tesseract OCR在文本识别中优于Kraken，ResNet18图像分类准确率达98.7%。

Conclusion: 机器学习在早期印刷书籍分析中具有潜力，但OCR性能和视觉内容解释仍需改进。

Abstract: We developed a proof-of-concept method for the automatic analysis of the
structure and content of incunabula pages. A custom dataset comprising 500
annotated pages from five different incunabula was created using resources from
the Jagiellonian Digital Library. Each page was manually labeled with five
predefined classes: Text, Title, Picture, Table, and Handwriting. Additionally,
the publicly available DocLayNet dataset was utilized as supplementary training
data. To perform object detection, YOLO11n and YOLO11s models were employed and
trained using two strategies: a combined dataset (DocLayNet and the custom
dataset) and the custom dataset alone. The highest performance (F1 = 0.94) was
achieved by the YOLO11n model trained exclusively on the custom data. Optical
character recognition was then conducted on regions classified as Text, using
both Tesseract and Kraken OCR, with Tesseract demonstrating superior results.
Subsequently, image classification was applied to the Picture class using a
ResNet18 model, achieving an accuracy of 98.7% across five subclasses:
Decorative_letter, Illustration, Other, Stamp, and Wrong_detection.
Furthermore, the CLIP model was utilized to generate semantic descriptions of
illustrations. The results confirm the potential of machine learning in the
analysis of early printed books, while emphasizing the need for further
advancements in OCR performance and visual content interpretation.

</details>


<div id='cs.SD'></div>

# cs.SD [[Back]](#toc)

### [432] [Zero-Shot Cognitive Impairment Detection from Speech Using AudioLLM](https://arxiv.org/abs/2506.17351)
*Mostafa Shahin,Beena Ahmed,Julien Epps*

Main category: cs.SD

TL;DR: 提出了一种基于零样本学习的语音认知障碍检测方法，利用Qwen2-Audio AudioLLM模型，无需手动标注，性能接近监督方法，且在多语言和多任务中表现一致。


<details>
  <summary>Details</summary>
Motivation: 认知障碍早期检测对干预至关重要，传统方法依赖手动标注且泛化性差，因此探索非侵入性、易收集的语音生物标志物。

Method: 使用Qwen2-Audio AudioLLM模型，设计基于提示的指令，对语音样本进行零样本分类。

Result: 在英语和多语言数据集上，零样本方法性能接近监督方法，且在多语言和多任务中表现一致。

Conclusion: 零样本语音检测方法具有潜力，可作为认知障碍早期检测的通用工具。

Abstract: Cognitive impairment (CI) is of growing public health concern, and early
detection is vital for effective intervention. Speech has gained attention as a
non-invasive and easily collectible biomarker for assessing cognitive decline.
Traditional CI detection methods typically rely on supervised models trained on
acoustic and linguistic features extracted from speech, which often require
manual annotation and may not generalise well across datasets and languages. In
this work, we propose the first zero-shot speech-based CI detection method
using the Qwen2- Audio AudioLLM, a model capable of processing both audio and
text inputs. By designing prompt-based instructions, we guide the model in
classifying speech samples as indicative of normal cognition or cognitive
impairment. We evaluate our approach on two datasets: one in English and
another multilingual, spanning different cognitive assessment tasks. Our
results show that the zero-shot AudioLLM approach achieves performance
comparable to supervised methods and exhibits promising generalizability and
consistency across languages, tasks, and datasets.

</details>


### [433] [AI-Generated Song Detection via Lyrics Transcripts](https://arxiv.org/abs/2506.18488)
*Markus Frohmann,Elena V. Epure,Gabriel Meseguer-Brocal,Markus Schedl,Romain Hennequin*

Main category: cs.SD

TL;DR: 论文提出了一种通过自动语音识别（ASR）模型转录歌词来检测AI生成音乐的方法，解决了现有音频检测方法在泛化和抗干扰方面的不足。


<details>
  <summary>Details</summary>
Motivation: AI音乐生成工具的快速发展需要准确的检测方法，但现有音频检测方法难以泛化且易受干扰，而基于完美歌词的方法在实际中不可行。

Method: 使用通用ASR模型转录歌曲，结合多种检测器（如Whisper large-v2和LLM2Vec嵌入）分析歌词。

Result: 在多语言、多流派的歌词上表现出色，尤其在抗干扰和泛化能力上优于现有音频检测方法。

Conclusion: 该方法填补了实际应用中的空白，为AI生成音乐的检测提供了更鲁棒的解决方案。

Abstract: The recent rise in capabilities of AI-based music generation tools has
created an upheaval in the music industry, necessitating the creation of
accurate methods to detect such AI-generated content. This can be done using
audio-based detectors; however, it has been shown that they struggle to
generalize to unseen generators or when the audio is perturbed. Furthermore,
recent work used accurate and cleanly formatted lyrics sourced from a lyrics
provider database to detect AI-generated music. However, in practice, such
perfect lyrics are not available (only the audio is); this leaves a substantial
gap in applicability in real-life use cases. In this work, we instead propose
solving this gap by transcribing songs using general automatic speech
recognition (ASR) models. We do this using several detectors. The results on
diverse, multi-genre, and multi-lingual lyrics show generally strong detection
performance across languages and genres, particularly for our best-performing
model using Whisper large-v2 and LLM2Vec embeddings. In addition, we show that
our method is more robust than state-of-the-art audio-based ones when the audio
is perturbed in different ways and when evaluated on different music
generators. Our code is available at
https://github.com/deezer/robust-AI-lyrics-detection.

</details>


### [434] [Smooth Operators: LLMs Translating Imperfect Hints into Disfluency-Rich Transcripts](https://arxiv.org/abs/2506.18510)
*Duygu Altinok*

Main category: cs.SD

TL;DR: 提出了一种利用大型语言模型（LLM）将口语中的不流畅部分标记为带时间戳的显式令牌的方法，生成完整标注的不流畅转录本。


<details>
  <summary>Details</summary>
Motivation: 提高自动语音和语言处理系统的性能，推动更具包容性的语音和语言技术发展。

Method: 结合音频编码器的声学表征与不同质量的文本输入（如干净转录、时间对齐转录或ASR模型输出），利用LLM处理不完美输入。

Result: 实验表明，即使文本输入不完美，只要包含时间戳相关线索，LLM仍能生成完整的不流畅标注转录本。

Conclusion: LLM在处理不完美提示时表现出鲁棒性，为不流畅检测提供了有效解决方案。

Abstract: Accurate detection of disfluencies in spoken language is crucial for
enhancing the performance of automatic speech and language processing systems,
as well as fostering the development of more inclusive speech and language
technologies. Leveraging the growing trend of large language models (LLMs) as
versatile learners capable of processing both lexical and non-lexical inputs
(e.g., audio and video), we propose a novel approach to transcribing
disfluencies as explicit tokens with timestamps, enabling the generation of
fully annotated disfluency-rich transcripts. Our method integrates acoustic
representations extracted from an audio encoder with textual inputs of varying
quality: clean transcriptions without disfluencies, time-aligned transcriptions
from aligners, or outputs from phoneme-based ASR models -- all of which may
contain imperfections. Importantly, our experiments demonstrate that textual
inputs do not need to be flawless. As long as they include timestamp-related
cues, LLMs can effectively smooth the input and produce fully
disfluency-annotated transcripts, underscoring their robustness in handling
imperfect hints.

</details>


### [435] [USAD: Universal Speech and Audio Representation via Distillation](https://arxiv.org/abs/2506.18843)
*Heng-Jui Chang,Saurabhchand Bhati,James Glass,Alexander H. Liu*

Main category: cs.SD

TL;DR: USAD提出了一种统一的音频表示学习方法，通过层间蒸馏整合多种音频类型，性能接近SOTA。


<details>
  <summary>Details</summary>
Motivation: 解决自监督学习模型通常局限于特定音频领域（如语音或非语音）的问题。

Method: 采用层间蒸馏技术，从领域特定的自监督学习模型中训练一个学生模型，整合多种音频类型。

Result: 在多种任务和数据集上表现优异，包括语音处理、音频标记和声音分类，性能接近SOTA。

Conclusion: USAD提供了一种高效且通用的音频表示学习方法，适用于多种音频任务。

Abstract: Self-supervised learning (SSL) has revolutionized audio representations, yet
models often remain domain-specific, focusing on either speech or non-speech
tasks. In this work, we present Universal Speech and Audio Distillation (USAD),
a unified approach to audio representation learning that integrates diverse
audio types - speech, sound, and music - into a single model. USAD employs
efficient layer-to-layer distillation from domain-specific SSL models to train
a student on a comprehensive audio dataset. USAD offers competitive performance
across various benchmarks and datasets, including frame and instance-level
speech processing tasks, audio tagging, and sound classification, achieving
near state-of-the-art results with a single encoder on SUPERB and HEAR
benchmarks.

</details>


### [436] [From Generality to Mastery: Composer-Style Symbolic Music Generation via Large-Scale Pre-training](https://arxiv.org/abs/2506.17497)
*Mingyang Yao,Ke Chen*

Main category: cs.SD

TL;DR: 论文提出了一种两阶段训练方法，通过预训练和微调提升作曲家风格的音乐生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决作曲家风格音乐生成中的数据稀缺问题，利用广泛音乐知识增强特定风格建模。

Method: 采用两阶段训练：先在大型音乐语料库上预训练REMI模型，再用轻量适配器模块在少量作曲家数据上微调。

Result: 实验表明，该方法在风格准确性和音乐性上优于基线，实现了更精确的风格建模和更好的音乐美学。

Conclusion: 该方法有效结合通用音乐知识和特定风格微调，提升了作曲家风格音乐生成的质量。

Abstract: Despite progress in controllable symbolic music generation, data scarcity
remains a challenge for certain control modalities. Composer-style music
generation is a prime example, as only a few pieces per composer are available,
limiting the modeling of both styles and fundamental music elements (e.g.,
melody, chord, rhythm). In this paper, we investigate how general music
knowledge learned from a broad corpus can enhance the mastery of specific
composer styles, with a focus on piano piece generation. Our approach follows a
two-stage training paradigm. First, we pre-train a REMI-based music generation
model on a large corpus of pop, folk, and classical music. Then, we fine-tune
it on a small, human-verified dataset from four renowned composers, namely
Bach, Mozart, Beethoven, and Chopin, using a lightweight adapter module to
condition the model on style indicators. To evaluate the effectiveness of our
approach, we conduct both objective and subjective evaluations on style
accuracy and musicality. Experimental results demonstrate that our method
outperforms ablations and baselines, achieving more precise composer-style
modeling and better musical aesthetics. Additionally, we provide observations
on how the model builds music concepts from the generality pre-training and
refines its stylistic understanding through the mastery fine-tuning.

</details>


### [437] [CultureMERT: Continual Pre-Training for Cross-Cultural Music Representation Learning](https://arxiv.org/abs/2506.17818)
*Angelos-Nikolaos Kanatas,Charilaos Papaioannou,Alexandros Potamianos*

Main category: cs.SD

TL;DR: CultureMERT-95M是一个多文化适应的音乐基础模型，通过两阶段持续预训练策略提升跨文化音乐表示学习，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有音乐基础模型在多样音乐传统中表现有限，需改进跨文化音乐表示学习。

Method: 采用两阶段持续预训练策略，结合学习率调整，训练于650小时多文化音乐数据。

Result: 在非西方音乐自动标记任务中平均提升4.9%性能，同时保持西方基准性能。

Conclusion: 多文化适应模型表现最佳，支持世界音乐表示学习研究，公开模型以促进发展。

Abstract: Recent advances in music foundation models have improved audio representation
learning, yet their effectiveness across diverse musical traditions remains
limited. We introduce CultureMERT-95M, a multi-culturally adapted foundation
model developed to enhance cross-cultural music representation learning and
understanding. To achieve this, we propose a two-stage continual pre-training
strategy that integrates learning rate re-warming and re-decaying, enabling
stable adaptation even with limited computational resources. Training on a
650-hour multi-cultural data mix, comprising Greek, Turkish, and Indian music
traditions, results in an average improvement of 4.9% in ROC-AUC and AP across
diverse non-Western music auto-tagging tasks, surpassing prior
state-of-the-art, with minimal forgetting on Western-centric benchmarks. We
further investigate task arithmetic, an alternative approach to multi-cultural
adaptation that merges single-culture adapted models in the weight space. Task
arithmetic performs on par with our multi-culturally trained model on
non-Western auto-tagging tasks and shows no regression on Western datasets.
Cross-cultural evaluation reveals that single-culture models transfer with
varying effectiveness across musical traditions, whereas the multi-culturally
adapted model achieves the best overall performance. To support research on
world music representation learning, we publicly release CultureMERT-95M and
CultureMERT-TA-95M, fostering the development of more culturally aware music
foundation models.

</details>


### [438] [TCDiff++: An End-to-end Trajectory-Controllable Diffusion Model for Harmonious Music-Driven Group Choreography](https://arxiv.org/abs/2506.18671)
*Yuqin Dai,Wanlu Zhu,Ronghui Li,Xiu Li,Zhenyu Zhang,Jun Li,Jian Yang*

Main category: cs.SD

TL;DR: TCDiff++是一个音乐驱动的端到端框架，用于生成和谐的群舞，解决了多舞者碰撞、单舞者脚滑和长群舞生成中的突然交换问题。


<details>
  <summary>Details</summary>
Motivation: 现有方法在群舞生成中存在多舞者碰撞、单舞者脚滑和长群舞生成中的突然交换问题，影响了舞蹈的和谐性和质量。

Method: 使用舞者定位嵌入和距离一致性损失减少碰撞；引入交换模式嵌入和Footwork Adaptor减少脚滑；采用长群扩散采样策略和序列解码层优化长群舞生成。

Result: TCDiff++在长时场景中表现优异，实现了高质量且连贯的群舞生成。

Conclusion: TCDiff++通过创新方法有效解决了群舞生成中的关键问题，显著提升了生成质量。

Abstract: Music-driven dance generation has garnered significant attention due to its
wide range of industrial applications, particularly in the creation of group
choreography. During the group dance generation process, however, most existing
methods still face three primary issues: multi-dancer collisions, single-dancer
foot sliding and abrupt swapping in the generation of long group dance. In this
paper, we propose TCDiff++, a music-driven end-to-end framework designed to
generate harmonious group dance. Specifically, to mitigate multi-dancer
collisions, we utilize a dancer positioning embedding to better maintain the
relative positioning among dancers. Additionally, we incorporate a
distance-consistency loss to ensure that inter-dancer distances remain within
plausible ranges. To address the issue of single-dancer foot sliding, we
introduce a swap mode embedding to indicate dancer swapping patterns and design
a Footwork Adaptor to refine raw motion, thereby minimizing foot sliding. For
long group dance generation, we present a long group diffusion sampling
strategy that reduces abrupt position shifts by injecting positional
information into the noisy input. Furthermore, we integrate a Sequence Decoder
layer to enhance the model's ability to selectively process long sequences.
Extensive experiments demonstrate that our TCDiff++ achieves state-of-the-art
performance, particularly in long-duration scenarios, ensuring high-quality and
coherent group dance generation.

</details>


### [439] [Frequency-Weighted Training Losses for Phoneme-Level DNN-based Speech Enhancement](https://arxiv.org/abs/2506.18714)
*Nasser-Eddine Monir,Paul Magron,Romain Serizel*

Main category: cs.SD

TL;DR: 论文提出了一种基于感知的SDR损失函数变体，通过频域加权提升语音增强效果。


<details>
  <summary>Details</summary>
Motivation: 传统SDR损失函数可能无法保留对语音清晰度至关重要的频谱细节，因此需要改进。

Method: 设计了频域加权的SDR损失函数，包括固定和自适应策略，如ANSI频带权重和动态权重。

Result: 实验显示，标准指标（如SDR）提升有限，但感知加权指标显著改善，且辅音重建更好。

Conclusion: 频域加权SDR损失能更好地保留声学线索，提升语音增强的感知效果。

Abstract: Recent advances in deep learning have significantly improved multichannel
speech enhancement algorithms, yet conventional training loss functions such as
the scale-invariant signal-to-distortion ratio (SDR) may fail to preserve
fine-grained spectral cues essential for phoneme intelligibility. In this work,
we propose perceptually-informed variants of the SDR loss, formulated in the
time-frequency domain and modulated by frequency-dependent weighting schemes.
These weights are designed to emphasize time-frequency regions where speech is
prominent or where the interfering noise is particularly strong. We investigate
both fixed and adaptive strategies, including ANSI band-importance weights,
spectral magnitude-based weighting, and dynamic weighting based on the relative
amount of speech and noise. We train the FaSNet multichannel speech enhancement
model using these various losses. Experimental results show that while standard
metrics such as the SDR are only marginally improved, their perceptual
frequency-weighted counterparts exhibit a more substantial improvement.
Besides, spectral and phoneme-level analysis indicates better consonant
reconstruction, which points to a better preservation of certain acoustic cues.

</details>


### [440] [MuseControlLite: Multifunctional Music Generation with Lightweight Conditioners](https://arxiv.org/abs/2506.18729)
*Fang-Duo Tsai,Shih-Lun Wu,Weijaw Lee,Sheng-Ping Yang,Bo-Rui Chen,Hao-Chung Cheng,Yi-Hsuan Yang*

Main category: cs.SD

TL;DR: MuseControlLite是一种轻量级机制，通过时间变化的音乐属性和参考音频信号，优化文本到音乐生成模型的精确控制。


<details>
  <summary>Details</summary>
Motivation: 现有文本到音乐生成模型在时间相关条件控制上表现不佳，且参数效率低。

Method: 在解耦的交叉注意力层中添加旋转位置嵌入，显著提升控制精度并减少可训练参数。

Result: 控制精度从56.6%提升至61.1%，参数仅为同类方法的1/6.75。

Conclusion: MuseControlLite在音乐属性控制、音频修复和扩展方面表现优异，且成本更低。

Abstract: We propose MuseControlLite, a lightweight mechanism designed to fine-tune
text-to-music generation models for precise conditioning using various
time-varying musical attributes and reference audio signals. The key finding is
that positional embeddings, which have been seldom used by text-to-music
generation models in the conditioner for text conditions, are critical when the
condition of interest is a function of time. Using melody control as an
example, our experiments show that simply adding rotary positional embeddings
to the decoupled cross-attention layers increases control accuracy from 56.6%
to 61.1%, while requiring 6.75 times fewer trainable parameters than
state-of-the-art fine-tuning mechanisms, using the same pre-trained diffusion
Transformer model of Stable Audio Open. We evaluate various forms of musical
attribute control, audio inpainting, and audio outpainting, demonstrating
improved controllability over MusicGen-Large and Stable Audio Open ControlNet
at a significantly lower fine-tuning cost, with only 85M trainble parameters.
Source code, model checkpoints, and demo examples are available at: https:
//MuseControlLite.github.io/web/.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [441] [Resolving the Ti-V Phase Diagram Discrepancy with First-Principles Calculations and Bayesian Learning](https://arxiv.org/abs/2506.17719)
*Timofei Miryashkin,Olga Klimanova,Alexander Shapeev*

Main category: cond-mat.mtrl-sci

TL;DR: 论文通过结合主动训练的Moment Tensor Potential和贝叶斯热力学推断，解决了钛-钒合金是否具有BCC混溶间隙的争议，结果表明存在混溶间隙，且与氧污染无关。


<details>
  <summary>Details</summary>
Motivation: 解决钛-钒（Ti-V）二元合金是否具有体心立方（BCC）混溶间隙的实验争议，验证混溶间隙是否由氧污染引起。

Method: 采用ab initio + 机器学习工作流，结合主动训练的Moment Tensor Potential和贝叶斯热力学推断，计算Ti-V二元系统的热力学相图。

Result: 计算结果显示Ti-V合金在T = 980 K和c = 0.67时存在BCC混溶间隙，且该间隙与氧污染无关。

Conclusion: 研究支持Ti-V合金存在BCC混溶间隙的结论，并排除了氧污染的影响，为相关争议提供了明确答案。

Abstract: Conflicting experiments disagree on whether the titanium-vanadium (Ti-V)
binary alloy exhibits a body-centred cubic (BCC) miscibility gap or remains
completely soluble. A leading hypothesis attributes the miscibility gap to
oxygen contamination during alloy preparation. To resolve this controversy, we
use an ab initio + machine-learning workflow that couples an actively-trained
Moment Tensor Potential to Bayesian thermodynamic inference. Using this
workflow, we obtain Ti-V binary system across the entire composition range,
together with confidence intervals in the thermodynamic limit. The resulting
diagram reproduces all experimental features, demonstrating the robustness of
our approach, and clearly favors the variant with a BCC miscibility gap
terminating at T = 980 K and c = 0.67. Because oxygen was excluded from
simulations, the gap cannot be attributed to impurity effects, contradicting
recent CALPHAD reassessments.

</details>


### [442] [Residual Connection-Enhanced ConvLSTM for Lithium Dendrite Growth Prediction](https://arxiv.org/abs/2506.17756)
*Hosung Lee,Byeongoh Hwang,Dasan Kim,Myungjoo Kang*

Main category: cond-mat.mtrl-sci

TL;DR: 提出了一种增强型ConvLSTM模型，通过残差连接改进锂枝晶生长预测的准确性和计算效率。


<details>
  <summary>Details</summary>
Motivation: 锂枝晶生长严重影响电池性能和安全性，需更准确的预测方法。

Method: 在ConvLSTM中集成残差连接，解决梯度消失问题，捕捉局部和宏观动态。

Result: 模型在多种电压条件下比传统ConvLSTM准确率提高7%，MSE显著降低。

Conclusion: 残差连接在时空网络中有效，为电池诊断提供新工具，未来可扩展至其他电池化学体系。

Abstract: The growth of lithium dendrites significantly impacts the performance and
safety of rechargeable batteries, leading to short circuits and capacity
degradation. This study proposes a Residual Connection-Enhanced ConvLSTM model
to predict dendrite growth patterns with improved accuracy and computational
efficiency. By integrating residual connections into ConvLSTM, the model
mitigates the vanishing gradient problem, enhances feature retention across
layers, and effectively captures both localized dendrite growth dynamics and
macroscopic battery behavior. The dataset was generated using a phase-field
model, simulating dendrite evolution under varying conditions. Experimental
results show that the proposed model achieves up to 7% higher accuracy and
significantly reduces mean squared error (MSE) compared to conventional
ConvLSTM across different voltage conditions (0.1V, 0.3V, 0.5V). This
highlights the effectiveness of residual connections in deep spatiotemporal
networks for electrochemical system modeling. The proposed approach offers a
robust tool for battery diagnostics, potentially aiding in real-time monitoring
and optimization of lithium battery performance. Future research can extend
this framework to other battery chemistries and integrate it with real-world
experimental data for further validation

</details>


<div id='eess.AS'></div>

# eess.AS [[Back]](#toc)

### [443] [Enhancing Few-shot Keyword Spotting Performance through Pre-Trained Self-supervised Speech Models](https://arxiv.org/abs/2506.17686)
*Alican Gok,Oguzhan Buyuksolak,Osman Erman Okman,Murat Saraclar*

Main category: eess.AS

TL;DR: 论文提出了一种基于自监督学习的训练方案，用于提升Few-Shot Keyword Spotting（FS-KWS）在资源受限边缘设备上的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统FS-KWS系统在资源受限的边缘环境中表现不佳，特别是在低误接受率下的准确性不足。

Method: 采用自监督学习模型（Wav2Vec 2.0）进行特征提取，结合Sub-center ArcFace损失函数增强类间分离性和类内紧凑性，并通过注意力降维和轻量级ResNet15学生模型实现高效部署。

Result: 在GSC数据集上，10-shot分类准确率从33.4%提升至74.1%，误接受率为1%。

Conclusion: 该方法显著提升了FS-KWS在边缘设备上的实用性，适用于实际场景。

Abstract: Keyword Spotting plays a critical role in enabling hands-free interaction for
battery-powered edge devices. Few-Shot Keyword Spotting (FS-KWS) addresses the
scalability and adaptability challenges of traditional systems by enabling
recognition of custom keywords with only a few examples. However, existing
FS-KWS systems achieve subpar accuracy at desirable false acceptance rates,
particularly in resource-constrained edge environments. To address these
issues, we propose a training scheme that leverages self-supervised learning
models for robust feature extraction, dimensionality reduction, and knowledge
distillation. The teacher model, based on Wav2Vec 2.0 is trained using
Sub-center ArcFace loss, which enhances inter-class separability and
intra-class compactness. To enable efficient deployment on edge devices, we
introduce attention-based dimensionality reduction and train a standard
lightweight ResNet15 student model. We evaluate the proposed approach on the
English portion of the Multilingual Spoken Words Corpus (MSWC) and the Google
Speech Commands (GSC) datasets. Notably, the proposed training method improves
the 10-shot classification accuracy from 33.4% to 74.1% on 11 classes at 1%
false alarm accuracy on the GSC dataset, thus making it significantly
better-suited for a real use case scenario.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [444] [Advanced Game-Theoretic Frameworks for Multi-Agent AI Challenges: A 2025 Outlook](https://arxiv.org/abs/2506.17348)
*Pavel Malinovskiy*

Main category: cs.MA

TL;DR: 本文探讨了如何利用高级博弈论范式作为下一代人工智能（AI）挑战的基础，提出了动态联盟形成、语言效用、破坏风险和部分可观察性等新方法。


<details>
  <summary>Details</summary>
Motivation: 为应对2025年前后AI领域的新挑战，本文旨在提供理论工具，以在不确定和部分对抗的环境中优化多智能体系统的战略交互。

Method: 通过数学形式化、模拟和编码方案，结合重复博弈、贝叶斯更新和道德框架等方法，研究多智能体系统在复杂环境中的适应与协商。

Result: 提出了一套理论工具，帮助AI研究者在不确定和部分对抗的背景下优化战略交互。

Conclusion: 本文为AI研究者提供了应对未来复杂挑战的博弈论基础，强调了动态联盟和道德框架的重要性。

Abstract: This paper presents a substantially reworked examination of how advanced
game-theoretic paradigms can serve as a foundation for the next-generation
challenges in Artificial Intelligence (AI), forecasted to arrive in or around
2025. Our focus extends beyond traditional models by incorporating dynamic
coalition formation, language-based utilities, sabotage risks, and partial
observability. We provide a set of mathematical formalisms, simulations, and
coding schemes that illustrate how multi-agent AI systems may adapt and
negotiate in complex environments. Key elements include repeated games,
Bayesian updates for adversarial detection, and moral framing within payoff
structures. This work aims to equip AI researchers with robust theoretical
tools for aligning strategic interaction in uncertain, partially adversarial
contexts.

</details>


### [445] [Towards Zero-Shot Coordination between Teams of Agents: The N-XPlay Framework](https://arxiv.org/abs/2506.17560)
*Ava Abderezaei,Chi-Hui Lin,Joseph Miceli,Naren Sivagnanadasan,Stéphane Aroca-Ouellette,Jake Brawer,Alessandro Roncone*

Main category: cs.MA

TL;DR: 论文提出了N-XPlay方法，用于解决多智能体系统中的零样本协作问题，并在N-player Overcooked环境中验证了其优于Self-Play的性能。


<details>
  <summary>Details</summary>
Motivation: 现有零样本协作方法仅适用于双智能体场景，无法满足现实多智能体系统中多团队协作的复杂性需求。

Method: 引入N-player Overcooked作为新基准，并提出N-XPlay方法，用于优化多团队环境中的协作能力。

Result: 实验表明，N-XPlay在平衡团队内和团队间协作方面优于Self-Play。

Conclusion: N-XPlay为多智能体系统中的零样本协作提供了更有效的解决方案。

Abstract: Zero-shot coordination (ZSC) -- the ability to collaborate with unfamiliar
partners -- is essential to making autonomous agents effective teammates.
Existing ZSC methods evaluate coordination capabilities between two agents who
have not previously interacted. However, these scenarios do not reflect the
complexity of real-world multi-agent systems, where coordination often involves
a hierarchy of sub-groups and interactions between teams of agents, known as
Multi-Team Systems (MTS). To address this gap, we first introduce N-player
Overcooked, an N-agent extension of the popular two-agent ZSC benchmark,
enabling evaluation of ZSC in N-agent scenarios. We then propose N-XPlay for
ZSC in N-agent, multi-team settings. Comparison against Self-Play across two-,
three- and five-player Overcooked scenarios, where agents are split between an
``ego-team'' and a group of unseen collaborators shows that agents trained with
N-XPlay are better able to simultaneously balance ``intra-team'' and
``inter-team'' coordination than agents trained with SP.

</details>


<div id='cs.MM'></div>

# cs.MM [[Back]](#toc)

### [446] [Can Generated Images Serve as a Viable Modality for Text-Centric Multimodal Learning?](https://arxiv.org/abs/2506.17623)
*Yuesheng Huang,Peng Zhang,Riliang Liu,Jiaqi Liang*

Main category: cs.MM

TL;DR: 研究探讨了利用文本生成图像（T2I）模型为文本中心任务提供补充模态的潜力，发现其性能提升依赖于语义对齐、任务的可视化基础及T2I模型质量。


<details>
  <summary>Details</summary>
Motivation: 解决文本数据丰富但多模态模型能力增强之间的模态差距问题，探索T2I生成图像作为补充模态的可行性。

Method: 通过文本分类任务的综合评估框架，分析T2I模型质量、提示工程策略和多模态融合架构的影响。

Result: 合成感知能显著提升性能，但效果高度依赖于语义对齐、任务的可视化基础和T2I模型的生成保真度。

Conclusion: 研究为这一范式建立了首个严格基准，明确了其潜力与局限，展示了在传统单模态场景中丰富语言理解的可行性。

Abstract: A significant ``modality gap" exists between the abundance of text-only data
and the increasing power of multimodal models. This work systematically
investigates whether images generated on-the-fly by Text-to-Image (T2I) models
can serve as a valuable complementary modality for text-centric tasks. Through
a comprehensive evaluation framework on text classification, we analyze the
impact of critical variables, including T2I model quality, prompt engineering
strategies, and multimodal fusion architectures. Our findings demonstrate that
this``synthetic perception" can yield significant performance gains, even when
augmenting strong large language model baselines. However, we find the
effectiveness of this approach is highly conditional, depending critically on
the semantic alignment between text and the generated image, the inherent
``visual groundability" of the task, and the generative fidelity of the T2I
model. Our work establishes the first rigorous benchmark for this paradigm,
providing a clear analysis of its potential and current limitations, and
demonstrating its viability as a pathway to enrich language understanding in
traditionally unimodal scenarios.

</details>


<div id='stat.ML'></div>

# stat.ML [[Back]](#toc)

### [447] [DRO-Augment Framework: Robustness by Synergizing Wasserstein Distributionally Robust Optimization and Data Augmentation](https://arxiv.org/abs/2506.17874)
*Jiaming Hu,Debarghya Mukherjee,Ioannis Ch. Paschalidis*

Main category: stat.ML

TL;DR: DRO-Augment结合Wasserstein分布鲁棒优化与数据增强，显著提升DNN在多种数据扰动和对抗攻击下的鲁棒性，同时保持干净数据的准确性。


<details>
  <summary>Details</summary>
Motivation: 提升深度神经网络在多种输入扰动（如数据损坏和对抗攻击）下的鲁棒性和稳定性。

Method: 提出DRO-Augment框架，整合Wasserstein分布鲁棒优化（W-DRO）与多种数据增强策略。

Result: 在CIFAR-10-C、CIFAR-100-C等基准数据集上表现优于现有方法，同时保持干净数据的准确性。

Conclusion: DRO-Augment通过理论分析和实验验证，显著提升了模型的鲁棒性，为实际应用提供了可靠解决方案。

Abstract: In many real-world applications, ensuring the robustness and stability of
deep neural networks (DNNs) is crucial, particularly for image classification
tasks that encounter various input perturbations. While data augmentation
techniques have been widely adopted to enhance the resilience of a trained
model against such perturbations, there remains significant room for
improvement in robustness against corrupted data and adversarial attacks
simultaneously. To address this challenge, we introduce DRO-Augment, a novel
framework that integrates Wasserstein Distributionally Robust Optimization
(W-DRO) with various data augmentation strategies to improve the robustness of
the models significantly across a broad spectrum of corruptions. Our method
outperforms existing augmentation methods under severe data perturbations and
adversarial attack scenarios while maintaining the accuracy on the clean
datasets on a range of benchmark datasets, including but not limited to
CIFAR-10-C, CIFAR-100-C, MNIST, and Fashion-MNIST. On the theoretical side, we
establish novel generalization error bounds for neural networks trained using a
computationally efficient, variation-regularized loss function closely related
to the W-DRO problem.

</details>


<div id='q-bio.NC'></div>

# q-bio.NC [[Back]](#toc)

### [448] [PaceLLM: Brain-Inspired Large Language Models for Long-Context Understanding](https://arxiv.org/abs/2506.17310)
*Kangcong Li,Peng Ye,Chongjun Tu,Lin Zhang,Chunfeng Song,Jiamin Wu,Tao Yang,Qihao Zheng,Tao Chen*

Main category: q-bio.NC

TL;DR: PaceLLM通过模拟大脑的工作记忆和皮层模块化，提出持久活动机制和皮层专家聚类，显著提升LLM的长上下文能力。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLM）因瞬态神经激活和未结构化前馈网络权重导致的信息衰减和语义碎片化问题。

Method: 引入持久活动（PA）机制和皮层专家（CE）聚类，分别动态管理关键状态和重组权重为语义模块。

Result: 在多个基准测试中表现优异，如LongBench的多文档QA提升6%，Infinite-Bench任务提升12.5-17.5%，支持20万token的上下文。

Conclusion: PaceLLM为脑启发式LLM优化提供了新思路，可推广至其他模型，无需结构改动即可提升长上下文性能和可解释性。

Abstract: While Large Language Models (LLMs) demonstrate strong performance across
domains, their long-context capabilities are limited by transient neural
activations causing information decay and unstructured feed-forward network
(FFN) weights leading to semantic fragmentation. Inspired by the brain's
working memory and cortical modularity, we propose PaceLLM, featuring two
innovations: (1) a Persistent Activity (PA) Mechanism that mimics prefrontal
cortex (PFC) neurons' persistent firing by introducing an activation-level
memory bank to dynamically retrieve, reuse, and update critical FFN states,
addressing contextual decay; and (2) Cortical Expert (CE) Clustering that
emulates task-adaptive neural specialization to reorganize FFN weights into
semantic modules, establishing cross-token dependencies and mitigating
fragmentation. Extensive evaluations show that PaceLLM achieves 6% improvement
on LongBench's Multi-document QA and 12.5-17.5% performance gains on
Infinite-Bench tasks, while extending measurable context length to 200K tokens
in Needle-In-A-Haystack (NIAH) tests. This work pioneers brain-inspired LLM
optimization and is complementary to other works. Besides, it can be
generalized to any model and enhance their long-context performance and
interpretability without structural overhauls.

</details>


### [449] [Challenges in Grounding Language in the Real World](https://arxiv.org/abs/2506.17375)
*Peter Lindes,Kaoutar Skiker*

Main category: q-bio.NC

TL;DR: 论文探讨了如何通过结合认知机器人和大型语言模型的能力，实现人机自然语言协作的挑战与解决方案。


<details>
  <summary>Details</summary>
Motivation: 长期目标是构建一个能通过自然语言与人类协作的AI系统，尤其是在物理机器人领域。

Method: 提出一种方法，将具备交互式任务学习能力的认知机器人与大型语言模型的语言能力相结合。

Result: 指出了实现这一方法的初步路径。

Conclusion: 通过整合认知机器人和语言模型的能力，有望实现更自然的人机协作。

Abstract: A long-term goal of Artificial Intelligence is to build a language
understanding system that allows a human to collaborate with a physical robot
using language that is natural to the human. In this paper we highlight some of
the challenges in doing this, and propose a solution that integrates the
abilities of a cognitive agent capable of interactive task learning in a
physical robot with the linguistic abilities of a large language model. We also
point the way to an initial implementation of this approach.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [450] [BlenderFusion: 3D-Grounded Visual Editing and Generative Compositing](https://arxiv.org/abs/2506.17450)
*Jiacheng Chen,Ramin Mehran,Xuhui Jia,Saining Xie,Sanghyun Woo*

Main category: cs.GR

TL;DR: BlenderFusion是一个生成式视觉合成框架，通过重新组合物体、相机和背景合成新场景，采用分层-编辑-合成流程，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决复杂场景编辑任务中物体、相机和背景的灵活控制和合成问题。

Method: 采用分层-编辑-合成流程：分层将输入转换为可编辑3D实体，编辑在Blender中进行3D控制，合成通过扩展扩散模型实现。

Result: 在复杂场景编辑任务中表现显著优于现有方法。

Conclusion: BlenderFusion为复杂场景编辑提供了高效且灵活的解决方案。

Abstract: We present BlenderFusion, a generative visual compositing framework that
synthesizes new scenes by recomposing objects, camera, and background. It
follows a layering-editing-compositing pipeline: (i) segmenting and converting
visual inputs into editable 3D entities (layering), (ii) editing them in
Blender with 3D-grounded control (editing), and (iii) fusing them into a
coherent scene using a generative compositor (compositing). Our generative
compositor extends a pre-trained diffusion model to process both the original
(source) and edited (target) scenes in parallel. It is fine-tuned on video
frames with two key training strategies: (i) source masking, enabling flexible
modifications like background replacement; (ii) simulated object jittering,
facilitating disentangled control over objects and camera. BlenderFusion
significantly outperforms prior methods in complex compositional scene editing
tasks.

</details>


### [451] [3D Gaussian Splatting for Fine-Detailed Surface Reconstruction in Large-Scale Scene](https://arxiv.org/abs/2506.17636)
*Shihan Chen,Zhaojin Li,Zeyu Chen,Qingsong Yan,Gaoyang Shen,Ran Duan*

Main category: cs.GR

TL;DR: 论文提出了一种新方法，通过粗到细的策略和自适应场景分割，结合解耦外观模型和瞬态掩模模型，解决了大规模3D高斯泼溅重建的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有3D高斯泼溅方法在大规模场景中因计算复杂和动态外观难以应用，尤其在航测和自动驾驶领域。

Method: 采用粗到细策略，先重建粗略模型，再自适应分割场景并细化；引入解耦外观模型和瞬态掩模模型，扩展多视角约束和单视角正则化。

Result: 在GauU-Scene V2数据集上表现优于现有NeRF和高斯泼溅方法，实现了高保真视觉结果和精确表面重建。

Conclusion: 该方法有效解决了大规模场景重建的挑战，具有实际应用潜力，代码将开源。

Abstract: Recent developments in 3D Gaussian Splatting have made significant advances
in surface reconstruction. However, scaling these methods to large-scale scenes
remains challenging due to high computational demands and the complex dynamic
appearances typical of outdoor environments. These challenges hinder the
application in aerial surveying and autonomous driving. This paper proposes a
novel solution to reconstruct large-scale surfaces with fine details,
supervised by full-sized images. Firstly, we introduce a coarse-to-fine
strategy to reconstruct a coarse model efficiently, followed by adaptive scene
partitioning and sub-scene refining from image segments. Additionally, we
integrate a decoupling appearance model to capture global appearance variations
and a transient mask model to mitigate interference from moving objects.
Finally, we expand the multi-view constraint and introduce a single-view
regularization for texture-less areas. Our experiments were conducted on the
publicly available dataset GauU-Scene V2, which was captured using unmanned
aerial vehicles. To the best of our knowledge, our method outperforms existing
NeRF-based and Gaussian-based methods, achieving high-fidelity visual results
and accurate surface from full-size image optimization. Open-source code will
be available on GitHub.

</details>


### [452] [Auto-Regressive Surface Cutting](https://arxiv.org/abs/2506.18017)
*Yang Li,Victor Cheung,Xinhai Liu,Yuguang Chen,Zhongjin Luo,Biwen Lei,Haohan Weng,Zibo Zhao,Jingwei Huang,Zhuo Chen,Chunchao Guo*

Main category: cs.GR

TL;DR: SeamGPT是一种自回归模型，通过模拟专业工作流程生成切割缝，将表面切割任务转化为下一个标记预测任务，显著提升了UV展开和3D分割的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在表面切割中常产生技术有效但语义不连贯的碎片化图集，需要更智能的解决方案。

Method: 采样网格顶点和边上的点云，将其编码为形状条件，使用GPT风格变换器顺序预测量化3D坐标的切割缝段。

Result: 在包含流形和非流形网格的UV展开基准测试中表现优异，并为3D分割工具提供清晰边界。

Conclusion: SeamGPT通过模拟专业工作流程，显著提升了表面切割的语义连贯性和技术性能。

Abstract: Surface cutting is a fundamental task in computer graphics, with applications
in UV parameterization, texture mapping, and mesh decomposition. However,
existing methods often produce technically valid but overly fragmented atlases
that lack semantic coherence. We introduce SeamGPT, an auto-regressive model
that generates cutting seams by mimicking professional workflows. Our key
technical innovation lies in formulating surface cutting as a next token
prediction task: sample point clouds on mesh vertices and edges, encode them as
shape conditions, and employ a GPT-style transformer to sequentially predict
seam segments with quantized 3D coordinates. Our approach achieves exceptional
performance on UV unwrapping benchmarks containing both manifold and
non-manifold meshes, including artist-created, and 3D-scanned models. In
addition, it enhances existing 3D segmentation tools by providing clean
boundaries for part decomposition.

</details>


### [453] [Collaborative Texture Filtering](https://arxiv.org/abs/2506.17770)
*Tomas Akenine-Möller,Pontus Ebelin,Matt Pharr,Bartlomiej Wronski*

Main category: cs.GR

TL;DR: 该论文提出了一种利用GPU波通信内在特性改进纹理压缩和过滤的新算法，避免了重复的纹理解压缩，并在放大过滤时实现了零误差。


<details>
  <summary>Details</summary>
Motivation: 现有随机纹理过滤（STF）技术在放大时可能导致视觉外观变化和噪声，尽管使用了时空去噪器。

Method: 利用GPU波通信内在特性，在活动执行的着色器中共享解码的纹理值，避免重复解压缩，并提出新的过滤回退方法。

Result: 在足够大的放大因子下，实现了每像素≤1次纹理评估的零误差过滤，其余情况下质量优于先前方法。

Conclusion: 新算法显著提高了纹理过滤的质量和效率，尤其是在放大情况下。

Abstract: Recent advances in texture compression provide major improvements in
compression ratios, but cannot use the GPU's texture units for decompression
and filtering. This has led to the development of stochastic texture filtering
(STF) techniques to avoid the high cost of multiple texel evaluations with such
formats. Unfortunately, those methods can give undesirable visual appearance
changes under magnification and may contain visible noise and flicker despite
the use of spatiotemporal denoisers. Recent work substantially improves the
quality of magnification filtering with STF by sharing decoded texel values
between nearby pixels (Wronski 2025). Using GPU wave communication intrinsics,
this sharing can be performed inside actively executing shaders without memory
traffic overhead. We take this idea further and present novel algorithms that
use wave communication between lanes to avoid repeated texel decompression
prior to filtering. By distributing unique work across lanes, we can achieve
zero-error filtering using <=1 texel evaluations per pixel given a sufficiently
large magnification factor. For the remaining cases, we propose novel filtering
fallback methods that also achieve higher quality than prior approaches.

</details>


### [454] [Morse: Dual-Sampling for Lossless Acceleration of Diffusion Models](https://arxiv.org/abs/2506.18251)
*Chao Li,Jiawei Fan,Anbang Yao*

Main category: cs.GR

TL;DR: Morse是一个无损加速扩散模型的双采样框架，通过跳跃采样和自适应残差反馈策略提升效率。


<details>
  <summary>Details</summary>
Motivation: 加速扩散模型的生成过程，同时保持无损性能。

Method: 使用Dash和Dot两个模型交互，Dash进行跳跃采样，Dot生成残差反馈以匹配Dash的估计。

Result: 在多种任务中实现1.78X到3.31X的无损加速，并适用于Latent Consistency Model。

Conclusion: Morse在提升效率的同时保持性能，具有广泛适用性。

Abstract: In this paper, we present Morse, a simple dual-sampling framework for
accelerating diffusion models losslessly. The key insight of Morse is to
reformulate the iterative generation (from noise to data) process via taking
advantage of fast jump sampling and adaptive residual feedback strategies.
Specifically, Morse involves two models called Dash and Dot that interact with
each other. The Dash model is just the pre-trained diffusion model of any type,
but operates in a jump sampling regime, creating sufficient space for sampling
efficiency improvement. The Dot model is significantly faster than the Dash
model, which is learnt to generate residual feedback conditioned on the
observations at the current jump sampling point on the trajectory of the Dash
model, lifting the noise estimate to easily match the next-step estimate of the
Dash model without jump sampling. By chaining the outputs of the Dash and Dot
models run in a time-interleaved fashion, Morse exhibits the merit of flexibly
attaining desired image generation performance while improving overall runtime
efficiency. With our proposed weight sharing strategy between the Dash and Dot
models, Morse is efficient for training and inference. Our method shows a
lossless speedup of 1.78X to 3.31X on average over a wide range of sampling
step budgets relative to 9 baseline diffusion models on 6 image generation
tasks. Furthermore, we show that our method can be also generalized to improve
the Latent Consistency Model (LCM-SDXL, which is already accelerated with
consistency distillation technique) tailored for few-step text-to-image
synthesis. The code and models are available at
https://github.com/deep-optimization/Morse.

</details>


### [455] [What You Think Is What You Get: Bridge User Intent and Transfer Function Design through Multimodal Large Language Models](https://arxiv.org/abs/2506.18407)
*Yiyao Wang,Bo Pan,Ke Wang,Han Liu,Jinyuan Mao,Yuxin Liu,Minfeng Zhu,Bo Zhang,Weifeng Chen,Xiuqi Huang,Wei Chen*

Main category: cs.GR

TL;DR: 论文提出WYTWYG框架，利用多模态大语言模型（MLLMs）指导基于用户意图的传递函数优化，解决现有方法探索空间大和泛化性弱的问题。


<details>
  <summary>Details</summary>
Motivation: 传递函数（TFs）设计在直接体渲染（DVR）中至关重要，但现有方法存在探索空间大和泛化性弱的挑战。

Method: 提出WYTWYG框架，包含基于进化的TF空间探索器和基于MLLMs的渲染质量评估器，并开发了交互式设计系统。

Result: 通过三个案例研究和实验验证了框架的通用性和各组件的有效性。

Conclusion: WYTWYG框架成功解决了TF设计中的探索和泛化问题，具有广泛适用性。

Abstract: Direct volume rendering (DVR) is a fundamental technique for visualizing
volumetric data, with transfer functions (TFs) playing a crucial role in
extracting meaningful structures. However, designing effective TFs remains
unintuitive due to the semantic gap between user intent and TF parameter space.
Researchers have developed numerous TF optimization methods to bridge this gap.
However, existing methods still face two challenges: large exploration space
and weak generalizability. To address these issues, we propose What You Think
is What You Get (WYTWYG) framework, which leveraging Multi-model Large Language
Models (MLLMs) to guide the TF optimization based on user intent. Specifically,
we first introduce a novel TF optimization approach comprising two core
components: (1) an evolution-based explorer for effective exploration of the TF
space, and (2) a volume rendering quality evaluator based on MLLMs to provide
generalizable visual guidance. We further propose a TF interactive design
system based on this approach. We demonstrate the general applicability of our
framework through three case studies, and validate the effectiveness of each
component through extensive experiments. Our code is available at:
https://github.com/wyysteelhead/TFevolve.

</details>


### [456] [BulletGen: Improving 4D Reconstruction with Bullet-Time Generation](https://arxiv.org/abs/2506.18601)
*Denys Rozumnyi,Jonathon Luiten,Numair Khan,Johannes Schönberger,Peter Kontschieder*

Main category: cs.GR

TL;DR: BulletGen利用生成模型修正高斯动态场景表示中的错误和缺失信息，通过扩散视频生成模型与4D重建对齐，实现沉浸式动态体验。


<details>
  <summary>Details</summary>
Motivation: 解决单目视频重建中的未观测区域和深度估计模糊问题。

Method: 结合生成模型优化4D高斯模型，利用扩散视频生成模型生成帧监督优化。

Result: 在新视角合成和2D/3D跟踪任务中达到最先进效果。

Conclusion: BulletGen成功将生成内容与静态和动态场景组件无缝融合。

Abstract: Transforming casually captured, monocular videos into fully immersive dynamic
experiences is a highly ill-posed task, and comes with significant challenges,
e.g., reconstructing unseen regions, and dealing with the ambiguity in
monocular depth estimation. In this work we introduce BulletGen, an approach
that takes advantage of generative models to correct errors and complete
missing information in a Gaussian-based dynamic scene representation. This is
done by aligning the output of a diffusion-based video generation model with
the 4D reconstruction at a single frozen "bullet-time" step. The generated
frames are then used to supervise the optimization of the 4D Gaussian model.
Our method seamlessly blends generative content with both static and dynamic
scene components, achieving state-of-the-art results on both novel-view
synthesis, and 2D/3D tracking tasks.

</details>


### [457] [DuetGen: Music Driven Two-Person Dance Generation via Hierarchical Masked Modeling](https://arxiv.org/abs/2506.18680)
*Anindita Ghosh,Bing Zhou,Rishabh Dabral,Jian Wang,Vladislav Golyanik,Christian Theobalt,Philipp Slusallek,Chuan Guo*

Main category: cs.GR

TL;DR: DuetGen是一个新颖的框架，用于从音乐生成交互式双人舞蹈。通过两阶段方法（编码和生成）和分层掩码建模，实现了同步和交互性。


<details>
  <summary>Details</summary>
Motivation: 解决双人舞蹈生成中复杂的同步和交互问题，利用运动合成的最新进展。

Method: 1. 使用VQ-VAE将双人动作编码为离散标记；2. 通过分层掩码变换器从音乐生成标记。

Result: 在双人舞蹈数据集上表现出色，实现了动作真实性、音乐舞蹈对齐和伙伴协调的先进性能。

Conclusion: DuetGen通过分层建模和交互表示，成功生成了同步且交互性强的双人舞蹈。

Abstract: We present DuetGen, a novel framework for generating interactive two-person
dances from music. The key challenge of this task lies in the inherent
complexities of two-person dance interactions, where the partners need to
synchronize both with each other and with the music. Inspired by the recent
advances in motion synthesis, we propose a two-stage solution: encoding
two-person motions into discrete tokens and then generating these tokens from
music. To effectively capture intricate interactions, we represent both
dancers' motions as a unified whole to learn the necessary motion tokens, and
adopt a coarse-to-fine learning strategy in both the stages. Our first stage
utilizes a VQ-VAE that hierarchically separates high-level semantic features at
a coarse temporal resolution from low-level details at a finer resolution,
producing two discrete token sequences at different abstraction levels.
Subsequently, in the second stage, two generative masked transformers learn to
map music signals to these dance tokens: the first producing high-level
semantic tokens, and the second, conditioned on music and these semantic
tokens, producing the low-level tokens. We train both transformers to learn to
predict randomly masked tokens within the sequence, enabling them to
iteratively generate motion tokens by filling an empty token sequence during
inference. Through the hierarchical masked modeling and dedicated interaction
representation, DuetGen achieves the generation of synchronized and interactive
two-person dances across various genres. Extensive experiments and user studies
on a benchmark duet dance dataset demonstrate state-of-the-art performance of
DuetGen in motion realism, music-dance alignment, and partner coordination.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [458] [A Digital Twin Framework for Generation-IV Reactors with Reinforcement Learning-Enabled Health-Aware Supervisory Control](https://arxiv.org/abs/2506.17258)
*Jasmin Y. Lim,Dimitrios Pylorof,Humberto E. Garcia,Karthik Duraisamy*

Main category: eess.SY

TL;DR: 本文提出了一种用于第四代核电站的数字孪生框架，通过数据增强方法优化运行和维护策略，同时满足系统约束。


<details>
  <summary>Details</summary>
Motivation: 第四代核电站虽具潜力，但高昂成本阻碍其部署。数字孪生技术可降低成本、提升决策效率和运行性能。

Method: 框架结合替代建模、强化学习和贝叶斯推断，通过闭环设计实现实时调节和自调整。强化学习用于考虑组件健康状态驱动目标功率，约束通过参考调节器控制算法实现。

Result: 三个案例验证了框架的鲁棒性：长期运行维护规划、短期精度优化和实时边界条件调整。

Conclusion: 该数字孪生框架适用于第四代核电站及其他复杂工程系统，展示了健康感知和约束感知的核电站运行潜力。

Abstract: Generation IV (Gen-IV) nuclear power plants are envisioned to replace the
current reactor fleet, bringing improvements in performance, safety,
reliability, and sustainability. However, large cost investments currently
inhibit the deployment of these advanced reactor concepts. Digital twins bridge
real-world systems with digital tools to reduce costs, enhance decision-making,
and boost operational efficiency. In this work, a digital twin framework is
designed to operate the Gen-IV Fluoride-salt-cooled High-temperature Reactor,
utilizing data-enhanced methods to optimize operational and maintenance
policies while adhering to system constraints. The closed-loop framework
integrates surrogate modeling, reinforcement learning, and Bayesian inference
to streamline end-to-end communication for online regulation and
self-adjustment. Reinforcement learning is used to consider component health
and degradation to drive the target power generations, with constraints
enforced through a Reference Governor control algorithm that ensures compliance
with pump flow rate and temperature limits. These input driving modules benefit
from detailed online simulations that are assimilated to measurement data with
Bayesian filtering. The digital twin is demonstrated in three case studies: a
one-year long-term operational period showcasing maintenance planning
capabilities, short-term accuracy refinement with high-frequency measurements,
and system shock capturing that demonstrates real-time recalibration
capabilities when change in boundary conditions. These demonstrations validate
robustness for health-aware and constraint-informed nuclear plant operation,
with general applicability to other advanced reactor concepts and complex
engineering systems.

</details>


### [459] [Conformal Safety Shielding for Imperfect-Perception Agents](https://arxiv.org/abs/2506.17275)
*William Scarbro,Calum Imrie,Sinem Getir Yaman,Kavan Fatehi,Corina S. Pasareanu,Radu Calinescu,Ravi Mangal*

Main category: eess.SY

TL;DR: 提出了一种基于马尔可夫决策过程的屏蔽构造方法，为使用学习组件的自主代理提供运行时安全保证，利用共形预测确保状态估计的安全性。


<details>
  <summary>Details</summary>
Motivation: 解决自主代理在使用学习组件进行高维观测时的安全控制问题，尤其是在感知误差存在的情况下。

Method: 使用共形预测为感知组件提供概率保证，屏蔽构造限制代理的动作以确保局部安全性。

Result: 通过案例研究展示了方法在自动驾驶系统中的应用，成功为飞机滑行提供安全指导。

Conclusion: 提出的屏蔽构造方法能够有效保证自主代理在感知误差下的安全性，同时证明了全局安全性质。

Abstract: We consider the problem of safe control in discrete autonomous agents that
use learned components for imperfect perception (or more generally, state
estimation) from high-dimensional observations. We propose a shield
construction that provides run-time safety guarantees under perception errors
by restricting the actions available to an agent, modeled as a Markov decision
process, as a function of the state estimates. Our construction uses conformal
prediction for the perception component, which guarantees that for each
observation, the predicted set of estimates includes the actual state with a
user-specified probability. The shield allows an action only if it is allowed
for all the estimates in the predicted set, resulting in a local safety
guarantee. We also articulate and prove a global safety property of existing
shield constructions for perfect-perception agents bounding the probability of
reaching unsafe states if the agent always chooses actions prescribed by the
shield. We illustrate our approach with a case-study of an experimental
autonomous system that guides airplanes on taxiways using high-dimensional
perception DNNs.

</details>


### [460] [A Theoretical Framework for Virtual Power Plant Integration with Gigawatt-Scale AI Data Centers: Multi-Timescale Control and Stability Analysis](https://arxiv.org/abs/2506.17284)
*Ali Peivandizadeh*

Main category: eess.SY

TL;DR: 论文提出了一种四层分层控制架构的虚拟电厂框架，以应对AI数据中心极端动态功率波动，证明了传统VPP架构无法满足需求，并提出了新的控制机制和稳定性标准。


<details>
  <summary>Details</summary>
Motivation: AI数据中心的快速增长导致功率波动极大，传统虚拟电厂（VPP）架构无法满足其动态需求，亟需新的理论框架。

Method: 采用四层分层控制架构，涵盖从100微秒到24小时的时间尺度，开发了针对转换器主导系统的控制机制和稳定性标准。

Result: 证明了传统VPP架构在AI数据中心动态下不稳定；提出了新的控制层和稳定性标准，临界清除时间从150毫秒降至83毫秒；量化了灵活性，峰值功率可减少30%。

Conclusion: 该研究为AI基础设施的稳定集成奠定了数学基础，预计到2030年将占数据中心电力消耗的50-70%。

Abstract: The explosive growth of artificial intelligence has created gigawatt-scale
data centers that fundamentally challenge power system operation, exhibiting
power fluctuations exceeding 500 MW within seconds and millisecond-scale
variations of 50-75% of thermal design power. This paper presents a
comprehensive theoretical framework that reconceptualizes Virtual Power Plants
(VPPs) to accommodate these extreme dynamics through a four-layer hierarchical
control architecture operating across timescales from 100 microseconds to 24
hours.
  We develop control mechanisms and stability criteria specifically tailored to
converter-dominated systems with pulsing megawatt-scale loads. We prove that
traditional VPP architectures, designed for aggregating distributed resources
with response times of seconds to minutes, cannot maintain stability when
confronted with AI data center dynamics exhibiting slew rates exceeding 1,000
MW/s at gigawatt scale.
  Our framework introduces: (1) a sub-millisecond control layer that interfaces
with data center power electronics to actively dampen power oscillations; (2)
new stability criteria incorporating protection system dynamics, demonstrating
that critical clearing times reduce from 150 ms to 83 ms for gigawatt-scale
pulsing loads; and (3) quantified flexibility characterization showing that
workload deferability enables 30% peak reduction while maintaining AI service
availability above 99.95%.
  This work establishes the mathematical foundations necessary for the stable
integration of AI infrastructure that will constitute 50-70% of data center
electricity consumption by 2030.

</details>


### [461] [Frequency Control in Microgrids: An Adaptive Fuzzy-Neural-Network Virtual Synchronous Generator](https://arxiv.org/abs/2506.18611)
*Waleed Breesam,Rezvan Alamian,Nima Tashakor,Brahim Elkhalil Youcefa,Stefan M. Goetz*

Main category: eess.SY

TL;DR: 论文提出了一种基于模糊神经网络的动态调整虚拟同步发电机参数的方法，以解决微电网频率调节问题，显著减少了频率偏差和稳定时间。


<details>
  <summary>Details</summary>
Motivation: 随着分布式可再生能源的普及，电力电子设备取代同步发电机导致微电网动态特性变化，系统惯性和阻尼降低，传统固定参数虚拟同步发电机无法满足频率调节需求。

Method: 通过模糊神经网络控制器动态调整虚拟同步发电机的惯性、阻尼和下垂参数，控制器在线学习以选择最佳参数值。

Result: 在MATLAB/Simulink模型和硬件在环实验中验证，相比传统方法，频率偏差降至0.03 Hz以下，稳定时间缩短。

Conclusion: 动态调整虚拟参数的方法能有效提升微电网频率稳定性，适用于高比例可再生能源场景。

Abstract: The reliance on distributed renewable energy has increased recently. As a
result, power electronic-based distributed generators replaced synchronous
generators which led to a change in the dynamic characteristics of the
microgrid. Most critically, they reduced system inertia and damping. Virtual
synchronous generators emulated in power electronics, which mimic the dynamic
behaviour of synchronous generators, are meant to fix this problem. However,
fixed virtual synchronous generator parameters cannot guarantee a frequency
regulation within the acceptable tolerance range. Conversely, a dynamic
adjustment of these virtual parameters promises robust solution with stable
frequency. This paper proposes a method to adapt the inertia, damping, and
droop parameters dynamically through a fuzzy neural network controller. This
controller trains itself online to choose appropriate values for these virtual
parameters. The proposed method can be applied to a typical AC microgrid by
considering the penetration and impact of renewable energy sources. We study
the system in a MATLAB/Simulink model and validate it experimentally in real
time using hardware-in-the-loop based on an embedded ARM system (SAM3X8E,
Cortex-M3). Compared to traditional and fuzzy logic controller methods, the
results demonstrate that the proposed method significantly reduces the
frequency deviation to less than 0.03 Hz and shortens the stabilizing/recovery
time.

</details>


<div id='cs.FL'></div>

# cs.FL [[Back]](#toc)

### [462] [Tutorial: $\varphi$-Transductions in OpenFst via the Gallic Semiring](https://arxiv.org/abs/2506.17942)
*Marco Cognetta,Cyril Allauzen*

Main category: cs.FL

TL;DR: 本文介绍了如何利用OpenFst的Gallic半环功能正确实现φ-转换，并以MaxMatch（WordPiece）分词算法为例进行演示。


<details>
  <summary>Details</summary>
Motivation: OpenFst库虽然支持φ-转换，但由于实现限制，无法直接用于转换器。本文旨在解决这一问题。

Method: 通过OpenFst的Gallic半环功能实现φ-转换，并以MaxMatch算法为例展示其应用。

Result: 成功实现了φ-转换，并提供了自包含的代码示例。

Conclusion: 本文提供了一种有效的方法来克服OpenFst中φ-转换的限制，并展示了其实际应用。

Abstract: OpenFst, a popular finite-state transducer library, supports
$\varphi$-transitions but, due to an implementation constraint, they cannot be
used with transducers in a straightforward way.
  In this short tutorial, we describe how one can use other functionality
provided by OpenFst (namely, the Gallic semiring) to correctly implement
$\varphi$-transductions and demonstrate it by implementing the MaxMatch
(WordPiece) tokenization algorithm (Devlin et al., 2019; Song et al., 2021).
Accompanying self-contained code examples are provided.
https://www.openfst.org/twiki/pub/Contrib/FstContrib/phi_transduction_tutorial_code.tgz

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [463] [Heterogeneous Temporal Hypergraph Neural Network](https://arxiv.org/abs/2506.17312)
*Huan Liu,Pengfei Jiao,Mengzhou Gao,Chaochao Chen,Di Jin*

Main category: cs.SI

TL;DR: 论文提出了一种新的异构时序超图神经网络（HTHGN），用于捕捉复杂异构时序图中的高阶交互关系，解决了现有方法忽略高阶拓扑信息和静态同质图限制的问题。


<details>
  <summary>Details</summary>
Motivation: 现有图表示学习方法主要关注低阶拓扑信息，忽略了高阶群体交互关系，且现有超图方法仅适用于静态同质图，无法有效建模异构时序图中的高阶交互。

Method: 提出异构时序超图的定义和P-均匀超边构建算法，并设计HTHGN模型，通过分层注意力机制和对比学习捕捉高阶交互关系。

Result: 在三个真实异构时序图数据集上的实验验证了HTHGN的有效性，性能显著提升。

Conclusion: HTHGN成功建模了异构时序图中的高阶交互关系，为复杂网络分析提供了新工具。

Abstract: Graph representation learning (GRL) has emerged as an effective technique for
modeling graph-structured data. When modeling heterogeneity and dynamics in
real-world complex networks, GRL methods designed for complex heterogeneous
temporal graphs (HTGs) have been proposed and have achieved successful
applications in various fields. However, most existing GRL methods mainly focus
on preserving the low-order topology information while ignoring higher-order
group interaction relationships, which are more consistent with real-world
networks. In addition, most existing hypergraph methods can only model static
homogeneous graphs, limiting their ability to model high-order interactions in
HTGs. Therefore, to simultaneously enable the GRL model to capture high-order
interaction relationships in HTGs, we first propose a formal definition of
heterogeneous temporal hypergraphs and $P$-uniform heterogeneous hyperedge
construction algorithm that does not rely on additional information. Then, a
novel Heterogeneous Temporal HyperGraph Neural network (HTHGN), is proposed to
fully capture higher-order interactions in HTGs. HTHGN contains a hierarchical
attention mechanism module that simultaneously performs temporal
message-passing between heterogeneous nodes and hyperedges to capture rich
semantics in a wider receptive field brought by hyperedges. Furthermore, HTHGN
performs contrastive learning by maximizing the consistency between low-order
correlated heterogeneous node pairs on HTG to avoid the low-order structural
ambiguity issue. Detailed experimental results on three real-world HTG datasets
verify the effectiveness of the proposed HTHGN for modeling high-order
interactions in HTGs and demonstrate significant performance improvements.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [464] [RoboTwin 2.0: A Scalable Data Generator and Benchmark with Strong Domain Randomization for Robust Bimanual Robotic Manipulation](https://arxiv.org/abs/2506.18088)
*Tianxing Chen,Zanxin Chen,Baijun Chen,Zijian Cai,Yibin Liu,Qiwei Liang,Zixuan Li,Xianliang Lin,Yiheng Ge,Zhenyu Gu,Weiliang Deng,Yubin Guo,Tian Nian,Xuanbing Xie,Qiangyu Chen,Kailun Su,Tianling Xu,Guodong Liu,Mengkang Hu,Huan-ang Gao,Kaixuan Wang,Zhixuan Liang,Yusen Qin,Xiaokang Yang,Ping Luo,Yao Mu*

Main category: cs.RO

TL;DR: RoboTwin 2.0是一个可扩展的仿真框架，用于生成多样且真实的双机械臂操作数据，并通过领域随机化和专家数据合成提升模拟到现实的迁移能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有合成数据集在双机械臂操作中缺乏高效、可扩展的数据生成方法以及仿真环境过于简化的问题。

Method: 构建大规模对象库，结合多模态大语言模型（MLLMs）和仿真循环优化生成任务级执行代码，并通过五轴领域随机化增强数据多样性。

Result: 在50个双机械臂任务中生成超过100,000条专家轨迹，代码生成成功率提升10.9%，并在未见过的真实场景中表现出强泛化能力。

Conclusion: RoboTwin 2.0显著提升了双机械臂操作的鲁棒性和泛化能力，为相关研究提供了可扩展的工具和数据支持。

Abstract: Simulation-based data synthesis has emerged as a powerful paradigm for
enhancing real-world robotic manipulation. However, existing synthetic datasets
remain insufficient for robust bimanual manipulation due to two challenges: (1)
the lack of an efficient, scalable data generation method for novel tasks, and
(2) oversimplified simulation environments that fail to capture real-world
complexity. We present RoboTwin 2.0, a scalable simulation framework that
enables automated, large-scale generation of diverse and realistic data, along
with unified evaluation protocols for dual-arm manipulation. We first construct
RoboTwin-OD, a large-scale object library comprising 731 instances across 147
categories, each annotated with semantic and manipulation-relevant labels.
Building on this foundation, we develop an expert data synthesis pipeline that
combines multimodal large language models (MLLMs) with simulation-in-the-loop
refinement to generate task-level execution code automatically. To improve
sim-to-real transfer, RoboTwin 2.0 incorporates structured domain randomization
along five axes: clutter, lighting, background, tabletop height and language
instructions, thereby enhancing data diversity and policy robustness. We
instantiate this framework across 50 dual-arm tasks spanning five robot
embodiments, and pre-collect over 100,000 domain-randomized expert
trajectories. Empirical results show a 10.9% gain in code generation success
and improved generalization to novel real-world scenarios. A VLA model
fine-tuned on our dataset achieves a 367% relative improvement (42.0% vs. 9.0%)
on unseen scene real-world tasks, while zero-shot models trained solely on our
synthetic data achieve a 228% relative gain, highlighting strong generalization
without real-world supervision. We release the data generator, benchmark,
dataset, and code to support scalable research in robust bimanual manipulation.

</details>


### [465] [General-Purpose Robotic Navigation via LVLM-Orchestrated Perception, Reasoning, and Acting](https://arxiv.org/abs/2506.17462)
*Bernard Lange,Anil Yildiz,Mansur Arief,Shehryar Khattak,Mykel Kochenderfer,Georgios Georgakis*

Main category: cs.RO

TL;DR: ARNA是一个基于大型视觉语言模型（LVLM）的通用导航框架，通过自主定义和执行任务特定工作流，实现未知环境中的鲁棒导航和推理。


<details>
  <summary>Details</summary>
Motivation: 现有导航系统依赖任务特定神经网络和固定数据流，泛化能力有限；而LVLM提供了类似人类的知识嵌入，适合推理和规划。

Method: ARNA结合感知、推理和导航工具库，运行时自主定义工作流，迭代查询机器人模块并选择导航动作。

Result: 在HM-EQA基准测试中，ARNA实现了最先进的性能，无需依赖手工计划或预建地图。

Conclusion: ARNA为机器人堆栈设计提供了新视角，展示了在未知环境中高效导航和推理的潜力。

Abstract: Developing general-purpose navigation policies for unknown environments
remains a core challenge in robotics. Most existing systems rely on
task-specific neural networks and fixed data flows, limiting generalizability.
Large Vision-Language Models (LVLMs) offer a promising alternative by embedding
human-like knowledge suitable for reasoning and planning. Yet, prior LVLM-robot
integrations typically depend on pre-mapped spaces, hard-coded representations,
and myopic exploration. We introduce the Agentic Robotic Navigation
Architecture (ARNA), a general-purpose navigation framework that equips an
LVLM-based agent with a library of perception, reasoning, and navigation tools
available within modern robotic stacks. At runtime, the agent autonomously
defines and executes task-specific workflows that iteratively query the robotic
modules, reason over multimodal inputs, and select appropriate navigation
actions. This approach enables robust navigation and reasoning in previously
unmapped environments, providing a new perspective on robotic stack design.
Evaluated in Habitat Lab on the HM-EQA benchmark, ARNA achieves
state-of-the-art performance, demonstrating effective exploration, navigation,
and embodied question answering without relying on handcrafted plans, fixed
input representations, or pre-existing maps.

</details>


### [466] [Distilling On-device Language Models for Robot Planning with Minimal Human Intervention](https://arxiv.org/abs/2506.17486)
*Zachary Ravichandran,Ignacio Hounie,Fernando Cladera,Alejandro Ribeiro,George J. Pappas,Vijay Kumar*

Main category: cs.RO

TL;DR: PRISM框架通过合成任务和环境数据，从大型语言模型（LLM）中提取小型语言模型（SLM），实现设备端运行的机器人规划器，性能接近GPT-4o的93%。


<details>
  <summary>Details</summary>
Motivation: 当前依赖云端LLM的机器人在通信不可靠的环境（如户外或工业场景）中受限，需要一种本地化解决方案。

Method: PRISM自动合成多样化任务和环境数据，利用LLM生成规划，并从中提取紧凑的SLM作为替代模型。

Result: PRISM将Llama-3.2-3B的性能从GPT-4o的10-20%提升至93%，且能泛化到不同机器人平台和环境。

Conclusion: PRISM提供了一种高效、本地化的机器人规划解决方案，适用于多样化场景。

Abstract: Large language models (LLMs) provide robots with powerful contextual
reasoning abilities and a natural human interface. Yet, current LLM-enabled
robots typically depend on cloud-hosted models, limiting their usability in
environments with unreliable communication infrastructure, such as outdoor or
industrial settings. We present PRISM, a framework for distilling small
language model (SLM)-enabled robot planners that run on-device with minimal
human supervision. Starting from an existing LLM-enabled planner, PRISM
automatically synthesizes diverse tasks and environments, elicits plans from
the LLM, and uses this synthetic dataset to distill a compact SLM as a drop-in
replacement of the source model. We apply PRISM to three LLM-enabled planners
for mapping and exploration, manipulation, and household assistance, and we
demonstrate that PRISM improves the performance of Llama-3.2-3B from 10-20% of
GPT-4o's performance to over 93% - using only synthetic data. We further
demonstrate that the distilled planners generalize across heterogeneous robotic
platforms (ground and aerial) and diverse environments (indoor and outdoor). We
release all software, trained models, and datasets at
https://zacravichandran.github.io/PRISM.

</details>


### [467] [Risk-Guided Diffusion: Toward Deploying Robot Foundation Models in Space, Where Failure Is Not An Option](https://arxiv.org/abs/2506.17601)
*Rohan Thakker,Adarsh Patnaik,Vince Kurtz,Jonas Frey,Jonathan Becktor,Sangwoo Moon,Rob Royce,Marcel Kaufmann,Georgios Georgakis,Pascal Roth,Joel Burdick,Marco Hutter,Shehryar Khattak*

Main category: cs.RO

TL;DR: 提出了一种结合快速学习系统与慢速物理系统的风险引导扩散框架，以提高极端地形导航的安全性和可靠性。


<details>
  <summary>Details</summary>
Motivation: 未来太空探索任务需要机器人在极端、陌生地形中安全可靠地导航，现有生成式AI方法缺乏足够的安全保障。

Method: 融合快速学习的"System-1"与慢速物理基础的"System-2"，通过风险引导扩散框架在训练和推理中共享计算。

Result: 在NASA JPL的Mars Yard实验中，该方法将故障率降低至4倍，同时保持目标达成性能。

Conclusion: 该方法通过推理时计算提升了导航的安全性和适应性，无需额外训练。

Abstract: Safe, reliable navigation in extreme, unfamiliar terrain is required for
future robotic space exploration missions. Recent generative-AI methods learn
semantically aware navigation policies from large, cross-embodiment datasets,
but offer limited safety guarantees. Inspired by human cognitive science, we
propose a risk-guided diffusion framework that fuses a fast, learned "System-1"
with a slow, physics-based "System-2", sharing computation at both training and
inference to couple adaptability with formal safety. Hardware experiments
conducted at the NASA JPL's Mars-analog facility, Mars Yard, show that our
approach reduces failure rates by up to $4\times$ while matching the
goal-reaching performance of learning-based robotic models by leveraging
inference-time compute without any additional training.

</details>


### [468] [RLRC: Reinforcement Learning-based Recovery for Compressed Vision-Language-Action Models](https://arxiv.org/abs/2506.17639)
*Yuxuan Chen,Xiao Li*

Main category: cs.RO

TL;DR: 论文提出RLRC方法，通过三阶段压缩技术显著减少VLA模型的内存占用和推理延迟，同时保持或提升任务成功率。


<details>
  <summary>Details</summary>
Motivation: 解决VLA模型因参数量大和推理延迟高而难以在资源受限的机器人平台上部署的问题。

Method: 采用三阶段恢复方法（结构化剪枝、基于SFT和RL的性能恢复、量化）压缩VLA模型。

Result: RLRC实现内存占用减少8倍，推理吞吐量提升2.3倍，且任务成功率不降反升。

Conclusion: RLRC在压缩VLA模型方面优于现有基线，适合在设备端部署。

Abstract: Vision-Language-Action models (VLA) have demonstrated remarkable capabilities
and promising potential in solving complex robotic manipulation tasks. However,
their substantial parameter sizes and high inference latency pose significant
challenges for real-world deployment, particularly on resource-constrained
robotic platforms. To address this issue, we begin by conducting an extensive
empirical study to explore the effectiveness of model compression techniques
when applied to VLAs. Building on the insights gained from these preliminary
experiments, we propose RLRC, a three-stage recovery method for compressed
VLAs, including structured pruning, performance recovery based on SFT and RL,
and further quantization. RLRC achieves up to an 8x reduction in memory usage
and a 2.3x improvement in inference throughput, while maintaining or even
surpassing the original VLA's task success rate. Extensive experiments show
that RLRC consistently outperforms existing compression baselines,
demonstrating strong potential for on-device deployment of VLAs. Project
website: https://rlrc-vla.github.io

</details>


### [469] [RoboMonkey: Scaling Test-Time Sampling and Verification for Vision-Language-Action Models](https://arxiv.org/abs/2506.17811)
*Jacky Kwok,Christopher Agia,Rohan Sinha,Matt Foutter,Shulu Li,Ion Stoica,Azalia Mirhoseini,Marco Pavone*

Main category: cs.RO

TL;DR: 本文研究了通过采样和验证增强Vision-Language-Action（VLA）模型的鲁棒性和泛化能力，提出了RoboMonkey框架，显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 尽管VLA模型在视觉运动控制中表现出色，但在非结构化现实环境中的鲁棒性仍是一个挑战。

Method: 提出RoboMonkey框架，通过采样、高斯扰动、多数投票和VLM验证来优化动作选择，并设计了合成数据生成流程训练验证器。

Result: RoboMonkey显著提升了性能，在分布外任务上绝对提升25%，分布内任务提升8%，且适应新机器人设置时性能提升7%。

Conclusion: RoboMonkey通过测试时缩放和验证机制有效增强了VLA模型的鲁棒性和适应性。

Abstract: Vision-Language-Action (VLA) models have demonstrated remarkable capabilities
in visuomotor control, yet ensuring their robustness in unstructured real-world
environments remains a persistent challenge. In this paper, we investigate
test-time scaling through the lens of sampling and verification as means to
enhance the robustness and generalization of VLAs. We first demonstrate that
the relationship between action error and the number of generated samples
follows an exponentiated power law across a range of VLAs, indicating the
existence of inference-time scaling laws. Building on these insights, we
introduce RoboMonkey, a test-time scaling framework for VLAs. At deployment,
RoboMonkey samples a small set of actions from a VLA, applies Gaussian
perturbation and majority voting to construct an action proposal distribution,
and then uses a Vision Language Model (VLM)-based verifier to select the
optimal action. We propose a synthetic data generation pipeline for training
such VLM-based action verifiers, and demonstrate that scaling the synthetic
dataset consistently improves verification and downstream accuracy. Through
extensive simulated and hardware experiments, we show that pairing existing
VLAs with RoboMonkey yields significant performance gains, achieving a 25%
absolute improvement on out-of-distribution tasks and 8% on in-distribution
tasks. Additionally, when adapting to new robot setups, we show that
fine-tuning both VLAs and action verifiers yields a 7% performance increase
compared to fine-tuning VLAs alone.

</details>


### [470] [Learning to Dock: A Simulation-based Study on Closing the Sim2Real Gap in Autonomous Underwater Docking](https://arxiv.org/abs/2506.17823)
*Kevin Chang,Rakesh Vivekanandan,Noah Pragin,Sean Bullock,Geoffrey Hollinger*

Main category: cs.RO

TL;DR: 研究通过强化学习减少AUV动态对接中的sim2real差距，探索随机化和历史条件控制器等方法，评估不同负载下的性能。


<details>
  <summary>Details</summary>
Motivation: 解决AUV在动态不确定环境中对接时，强化学习控制器因sim2real差距导致的性能下降问题。

Method: 通过模拟研究，训练多种控制器并在真实扰动下评估，重点关注不同负载下的对接挑战。

Result: 提供了减少sim2real差距的见解，并指出未来研究方向。

Conclusion: 研究为海洋机器人社区提供了减少sim2real差距的方法和未来研究方向。

Abstract: Autonomous Underwater Vehicle (AUV) docking in dynamic and uncertain
environments is a critical challenge for underwater robotics. Reinforcement
learning is a promising method for developing robust controllers, but the
disparity between training simulations and the real world, or the sim2real gap,
often leads to a significant deterioration in performance. In this work, we
perform a simulation study on reducing the sim2real gap in autonomous docking
through training various controllers and then evaluating them under realistic
disturbances. In particular, we focus on the real-world challenge of docking
under different payloads that are potentially outside the original training
distribution. We explore existing methods for improving robustness including
randomization techniques and history-conditioned controllers. Our findings
provide insights into mitigating the sim2real gap when training docking
controllers. Furthermore, our work indicates areas of future research that may
be beneficial to the marine robotics community.

</details>


### [471] [Generative Grasp Detection and Estimation with Concept Learning-based Safety Criteria](https://arxiv.org/abs/2506.17842)
*Al-Harith Farhad,Khalil Abuibaid,Christiane Plociennik,Achim Wagner,Martin Ruskowski*

Main category: cs.RO

TL;DR: 论文提出了一种用于协作机器人抓取算法的透明化AI方法，通过提取学习特征并关联输入类别，提高模型预测的可解释性和安全性。


<details>
  <summary>Details</summary>
Motivation: 神经网络的复杂性使其成为黑盒模型，尤其在安全关键应用中存在问题，因此需要提高透明度和可靠性。

Method: 提出了一种协作机器人抓取算法流程，结合可解释AI方法，提取学习特征并关联输入类别。

Result: 在工业环境中测试，展示了方法的一致性和对交接位置的改进。

Conclusion: 该方法通过透明化AI提高了协作机器人抓取的安全性和可靠性。

Abstract: Neural networks are often regarded as universal equations that can estimate
any function. This flexibility, however, comes with the drawback of high
complexity, rendering these networks into black box models, which is especially
relevant in safety-centric applications. To that end, we propose a pipeline for
a collaborative robot (Cobot) grasping algorithm that detects relevant tools
and generates the optimal grasp. To increase the transparency and reliability
of this approach, we integrate an explainable AI method that provides an
explanation for the underlying prediction of a model by extracting the learned
features and correlating them to corresponding classes from the input. These
concepts are then used as additional criteria to ensure the safe handling of
work tools. In this paper, we show the consistency of this approach and the
criterion for improving the handover position. This approach was tested in an
industrial environment, where a camera system was set up to enable a robot to
pick up certain tools and objects.

</details>


### [472] [A workflow for generating synthetic LiDAR datasets in simulation environments](https://arxiv.org/abs/2506.17378)
*Abhishek Phadke,Shakib Mahmud Dipto,Pratip Rana*

Main category: cs.RO

TL;DR: 该论文提出了一种生成合成LiDAR数据集的仿真工作流程，用于支持自动驾驶感知、机器人研究和传感器安全分析。


<details>
  <summary>Details</summary>
Motivation: 动机是提供一个高保真、多模态的合成数据集，以促进感知研究和传感器安全性的提升。

Method: 方法包括利用CoppeliaSim仿真环境和Python API，集成多种传感器（LiDAR、图像传感器、二维扫描仪）到模拟车辆平台，并自动化数据捕获、存储和标注。

Result: 结果生成了大规模点云及同步的RGB和深度图像，验证了工作流程的有效性，并探讨了LiDAR数据的安全漏洞。

Conclusion: 结论指出该工作流程为感知研究和传感器安全提供了可重复的框架，同时提出了未来改进方向（如天气效果、真实地形模型等）。

Abstract: This paper presents a simulation workflow for generating synthetic LiDAR
datasets to support autonomous vehicle perception, robotics research, and
sensor security analysis. Leveraging the CoppeliaSim simulation environment and
its Python API, we integrate time-of-flight LiDAR, image sensors, and two
dimensional scanners onto a simulated vehicle platform operating within an
urban scenario. The workflow automates data capture, storage, and annotation
across multiple formats (PCD, PLY, CSV), producing synchronized multimodal
datasets with ground truth pose information. We validate the pipeline by
generating large-scale point clouds and corresponding RGB and depth imagery.
The study examines potential security vulnerabilities in LiDAR data, such as
adversarial point injection and spoofing attacks, and demonstrates how
synthetic datasets can facilitate the evaluation of defense strategies.
Finally, limitations related to environmental realism, sensor noise modeling,
and computational scalability are discussed, and future research directions,
such as incorporating weather effects, real-world terrain models, and advanced
scanner configurations, are proposed. The workflow provides a versatile,
reproducible framework for generating high-fidelity synthetic LiDAR datasets to
advance perception research and strengthen sensor security in autonomous
systems. Documentation and examples accompany this framework; samples of
animated cloud returns and image sensor data can be found at this Link.

</details>


### [473] [GeNIE: A Generalizable Navigation System for In-the-Wild Environments](https://arxiv.org/abs/2506.17960)
*Jiaming Wang,Diwen Liu,Jizhuo Chen,Jiaxuan Da,Nuowen Qian,Tram Minh Man,Harold Soh*

Main category: cs.RO

TL;DR: GeNIE是一种通用导航系统，在复杂环境中表现优异，并在ICRA 2025比赛中获得第一名。


<details>
  <summary>Details</summary>
Motivation: 解决非结构化环境中可靠导航的挑战，适应多样化地形、天气和传感器配置。

Method: 结合基于SAM2的可通行性预测模型和路径融合策略，提升规划稳定性。

Result: 在ICRA 2025比赛中以79%的最高分获胜，领先第二名17%，全程无需人工干预。

Conclusion: GeNIE为户外机器人导航设立了新标杆，并将开源代码和数据集支持未来研究。

Abstract: Reliable navigation in unstructured, real-world environments remains a
significant challenge for embodied agents, especially when operating across
diverse terrains, weather conditions, and sensor configurations. In this paper,
we introduce GeNIE (Generalizable Navigation System for In-the-Wild
Environments), a robust navigation framework designed for global deployment.
GeNIE integrates a generalizable traversability prediction model built on SAM2
with a novel path fusion strategy that enhances planning stability in noisy and
ambiguous settings. We deployed GeNIE in the Earth Rover Challenge (ERC) at
ICRA 2025, where it was evaluated across six countries spanning three
continents. GeNIE took first place and achieved 79% of the maximum possible
score, outperforming the second-best team by 17%, and completed the entire
competition without a single human intervention. These results set a new
benchmark for robust, generalizable outdoor robot navigation. We will release
the codebase, pretrained model weights, and newly curated datasets to support
future research in real-world navigation.

</details>


### [474] [EASE: Embodied Active Event Perception via Self-Supervised Energy Minimization](https://arxiv.org/abs/2506.17516)
*Zhou Chen,Sanjoy Kundu,Harsimran S. Baweja,Sathyanarayanan N. Aakur*

Main category: cs.RO

TL;DR: EASE是一个自监督框架，通过自由能最小化统一时空表示学习和具身控制，无需标注或外部奖励即可实现动态事件感知。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预定义动作空间、标注数据集和外部奖励，限制了在动态现实场景中的适应性和可扩展性。

Method: EASE结合生成感知模型和动作驱动控制策略，利用预测误差和熵作为内在信号，动态对齐预测与观察。

Result: EASE在仿真和现实环境中表现出隐私保护、可扩展的事件感知能力，支持隐式记忆和目标连续性等行为。

Conclusion: EASE为无脚本动态任务中的具身系统提供了鲁棒基础。

Abstract: Active event perception, the ability to dynamically detect, track, and
summarize events in real time, is essential for embodied intelligence in tasks
such as human-AI collaboration, assistive robotics, and autonomous navigation.
However, existing approaches often depend on predefined action spaces,
annotated datasets, and extrinsic rewards, limiting their adaptability and
scalability in dynamic, real-world scenarios. Inspired by cognitive theories of
event perception and predictive coding, we propose EASE, a self-supervised
framework that unifies spatiotemporal representation learning and embodied
control through free energy minimization. EASE leverages prediction errors and
entropy as intrinsic signals to segment events, summarize observations, and
actively track salient actors, operating without explicit annotations or
external rewards. By coupling a generative perception model with an
action-driven control policy, EASE dynamically aligns predictions with
observations, enabling emergent behaviors such as implicit memory, target
continuity, and adaptability to novel environments. Extensive evaluations in
simulation and real-world settings demonstrate EASE's ability to achieve
privacy-preserving and scalable event perception, providing a robust foundation
for embodied systems in unscripted, dynamic tasks.

</details>


### [475] [ADA-DPM: A Neural Descriptors-based Adaptive Noise Point Filtering Strategy for SLAM](https://arxiv.org/abs/2506.18016)
*Yongxin Shao,Binrui Wang,Aihong Tan*

Main category: cs.RO

TL;DR: 提出了一种自适应噪声过滤的LiDAR SLAM策略ADA-DPM，通过动态分割头和全局重要性评分头优化特征点选择，结合GLI-GCN模块提升特征判别能力，在多个公开数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR SLAM方法在动态物体干扰、点云噪声和非结构化环境中需在定位精度和系统鲁棒性之间权衡，亟需改进。

Method: 设计了动态分割头（Dynamic Segmentation Head）和全局重要性评分头（Global Importance Scoring Head），结合GLI-GCN模块融合多尺度邻域结构。

Result: 在多个公开数据集上测试，取得了出色的定位精度和鲁棒性表现。

Conclusion: ADA-DPM策略有效解决了动态干扰和噪声问题，提升了LiDAR SLAM的性能。

Abstract: LiDAR SLAM has demonstrated significant application value in various fields,
including mobile robot navigation and high-precision map construction. However,
existing methods often need to make a trade-off between positioning accuracy
and system robustness when faced with dynamic object interference, point cloud
noise, and unstructured environments. To address this challenge, we propose an
adaptive noise filtering SLAM strategy-ADA-DPM, achieving excellent preference
in both aspects. We design the Dynamic Segmentation Head to predict the
category of feature points belonging to dynamic points, to eliminate dynamic
feature points; design the Global Importance Scoring Head to adaptively select
feature points with higher contribution and features while suppressing noise
interference; and construct the Cross Layer Intra-Graph Convolution Module
(GLI-GCN) to fuse multi-scale neighborhood structures, thereby enhancing the
discriminative ability of overlapping features. Finally, to further validate
the effectiveness of our method, we tested it on several publicly available
datasets and achieved outstanding results.

</details>


### [476] [Radar and Event Camera Fusion for Agile Robot Ego-Motion Estimation](https://arxiv.org/abs/2506.18443)
*Yang Lyu,Zhenghao Zou,Yanfeng Li,Chunhui Zhao,Quan Pan*

Main category: cs.RO

TL;DR: 提出了一种无需IMU和特征关联的框架，通过结合事件相机和毫米波雷达，实现高动态场景下机器人平台的激进自我运动速度估计。


<details>
  <summary>Details</summary>
Motivation: 高动态机器人运动时，传统传感器因响应不及时导致测量模糊、失真和延迟，难以实现可靠的自我运动估计。

Method: 利用瞬时原始事件和多普勒测量直接推导旋转和平移速度，后端采用连续时间状态空间模型融合混合时间测量和事件测量。

Result: 在自收集的实验数据集上验证，框架在挑战性环境中实现了可靠且高效的自我运动速度估计。

Conclusion: 该框架无需IMU和复杂关联过程，适用于纹理和结构缺失环境，计算效率高，适合边缘设备。

Abstract: Achieving reliable ego motion estimation for agile robots, e.g., aerobatic
aircraft, remains challenging because most robot sensors fail to respond timely
and clearly to highly dynamic robot motions, often resulting in measurement
blurring, distortion, and delays. In this paper, we propose an IMU-free and
feature-association-free framework to achieve aggressive ego-motion velocity
estimation of a robot platform in highly dynamic scenarios by combining two
types of exteroceptive sensors, an event camera and a millimeter wave radar,
First, we used instantaneous raw events and Doppler measurements to derive
rotational and translational velocities directly. Without a sophisticated
association process between measurement frames, the proposed method is more
robust in texture-less and structureless environments and is more
computationally efficient for edge computing devices. Then, in the back-end, we
propose a continuous-time state-space model to fuse the hybrid time-based and
event-based measurements to estimate the ego-motion velocity in a fixed-lagged
smoother fashion. In the end, we validate our velometer framework extensively
in self-collected experiment datasets. The results indicate that our IMU-free
and association-free ego motion estimation framework can achieve reliable and
efficient velocity output in challenging environments. The source code,
illustrative video and dataset are available at
https://github.com/ZzhYgwh/TwistEstimator.

</details>


### [477] [TDACloud: Point Cloud Recognition Using Topological Data Analysis](https://arxiv.org/abs/2506.18725)
*Anirban Ghosh,Ian Dahlin,Ayan Dutta*

Main category: cs.RO

TL;DR: 提出了一种名为TDACloud的新方法，利用拓扑数据分析（TDA）从点云中提取局部描述符，无需GPU密集型机器学习训练，并在噪声和变换条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 点云识别在自动驾驶等应用中具有重要性，但现有方法在噪声或变换条件下表现不佳，需要高效且鲁棒的解决方案。

Method: 采用ATOL向量化方法生成点云的固定大小TDA描述符向量，直接处理原始点云，避免了体素化的需求。

Result: 在多个真实和模拟数据集上测试，TDACloud在噪声和变换条件下表现出高识别准确率，优于基线方法约14%。

Conclusion: TDACloud是一种高效且鲁棒的点云识别方法，适用于复杂环境下的实际应用。

Abstract: Point cloud-based object/place recognition remains a problem of interest in
applications such as autonomous driving, scene reconstruction, and
localization. Extracting meaningful local descriptors from a query point cloud
that can be matched with the descriptors of the collected point clouds is a
challenging problem. Furthermore, when the query point cloud is noisy or has
been transformed (e.g., rotated), it adds to the complexity. To this end, we
propose a novel methodology, named TDACloud, using Topological Data Analysis
(TDA) for local descriptor extraction from a point cloud, which does not need
resource-intensive GPU-based machine learning training. More specifically, we
used the ATOL vectorization method to generate vectors for point clouds. Unlike
voxelization, our proposed technique can take raw point clouds as inputs and
outputs a fixed-size TDA-descriptor vector. To test the quality of the proposed
TDACloud technique, we have implemented it on multiple real-world (e.g., Oxford
RobotCar, KITTI-360) and realistic (e.g., ShapeNet) point cloud datasets for
object and place recognition. We have also tested TDACloud on noisy and
transformed test cases where the query point cloud has been scaled, translated,
or rotated. Our results demonstrate high recognition accuracies in noisy
conditions and large-scale real-world place recognition while outperforming the
baselines by up to approximately 14%.

</details>


### [478] [Reproducible Evaluation of Camera Auto-Exposure Methods in the Field: Platform, Benchmark and Lessons Learned](https://arxiv.org/abs/2506.18844)
*Olivier Gamache,Jean-Michel Fortin,Matěj Boxan,François Pomerleau,Philippe Giguère*

Main category: cs.RO

TL;DR: 提出一种基于模拟器的方法，利用多曝光数据集BorealHDR及其扩展，离线评估自动曝光方法，实现可重复性实验。


<details>
  <summary>Details</summary>
Motivation: 标准数据集因输入传感器固定，难以比较动态调整传感器参数的方法，如自动曝光（AE）方法。传统在线评估方式不可复现。

Method: 利用模拟器生成任意曝光时间的图像，基于BorealHDR多曝光立体数据集及其扩展，结合离线评估方法。

Result: 模拟图像与真实图像的RMSE低于1.78%，评估了8种AE方法，发现经典方法表现最佳。

Conclusion: 离线方法解决了AE方法评估的可重复性问题，经典AE方法仍是最优选择，并公开了数据集和硬件平台细节。

Abstract: Standard datasets often present limitations, particularly due to the fixed
nature of input data sensors, which makes it difficult to compare methods that
actively adjust sensor parameters to suit environmental conditions. This is the
case with Automatic-Exposure (AE) methods, which rely on environmental factors
to influence the image acquisition process. As a result, AE methods have
traditionally been benchmarked in an online manner, rendering experiments
non-reproducible. Building on our prior work, we propose a methodology that
utilizes an emulator capable of generating images at any exposure time. This
approach leverages BorealHDR, a unique multi-exposure stereo dataset, along
with its new extension, in which data was acquired along a repeated trajectory
at different times of the day to assess the impact of changing illumination. In
total, BorealHDR covers 13.4 km over 59 trajectories in challenging lighting
conditions. The dataset also includes lidar-inertial-odometry-based maps with
pose estimation for each image frame, as well as Global Navigation Satellite
System (GNSS) data for comparison. We demonstrate that by using images acquired
at various exposure times, we can emulate realistic images with a
Root-Mean-Square Error (RMSE) below 1.78% compared to ground truth images.
Using this offline approach, we benchmarked eight AE methods, concluding that
the classical AE method remains the field's best performer. To further support
reproducibility, we provide in-depth details on the development of our backpack
acquisition platform, including hardware, electrical components, and
performance specifications. Additionally, we share valuable lessons learned
from deploying the backpack over more than 25 km across various environments.
Our code and dataset are available online at this link:
https://github.com/norlab-ulaval/TFR24 BorealHDR

</details>


### [479] [GRAND-SLAM: Local Optimization for Globally Consistent Large-Scale Multi-Agent Gaussian SLAM](https://arxiv.org/abs/2506.18885)
*Annika Thomas,Aneesa Sonawalla,Alex Rose,Jonathan P. How*

Main category: cs.RO

TL;DR: GRAND-SLAM是一种多智能体协作的3D高斯泼溅SLAM方法，适用于大规模户外环境，性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有高斯泼溅SLAM方法局限于小规模室内环境，无法满足多智能体户外场景的需求。

Method: 结合局部优化的隐式跟踪模块和多机器人闭环检测的位姿图优化框架。

Result: 在Replica室内数据集上PSNR提升28%，在Kimera-Multi户外数据集上多智能体跟踪误差降低91%。

Conclusion: GRAND-SLAM在大规模多智能体户外环境中表现出色，具有高效跟踪和高质量渲染能力。

Abstract: 3D Gaussian splatting has emerged as an expressive scene representation for
RGB-D visual SLAM, but its application to large-scale, multi-agent outdoor
environments remains unexplored. Multi-agent Gaussian SLAM is a promising
approach to rapid exploration and reconstruction of environments, offering
scalable environment representations, but existing approaches are limited to
small-scale, indoor environments. To that end, we propose Gaussian
Reconstruction via Multi-Agent Dense SLAM, or GRAND-SLAM, a collaborative
Gaussian splatting SLAM method that integrates i) an implicit tracking module
based on local optimization over submaps and ii) an approach to inter- and
intra-robot loop closure integrated into a pose-graph optimization framework.
Experiments show that GRAND-SLAM provides state-of-the-art tracking performance
and 28% higher PSNR than existing methods on the Replica indoor dataset, as
well as 91% lower multi-agent tracking error and improved rendering over
existing multi-agent methods on the large-scale, outdoor Kimera-Multi dataset.

</details>


### [480] [Robots and Children that Learn Together : Improving Knowledge Retention by Teaching Peer-Like Interactive Robots](https://arxiv.org/abs/2506.18365)
*Imene Tarakli,Samuele Vinanzi,Richard Moore,Alessandro Di Nuovo*

Main category: cs.RO

TL;DR: 研究通过交互式强化学习（RL）模型，探索了社交机器人在真实课堂中的教学效果，发现儿童通过教学机器人显著提高了学习效果，尤其是语法任务。


<details>
  <summary>Details</summary>
Motivation: 尽管学习-教学（LbT）范式受到关注，但缺乏关于自主社交机器人在真实课堂中应用的研究。本研究旨在填补这一空白。

Method: 采用交互式强化学习（RL）模型，让儿童教学机器人学习法语词汇和语法规则，并与自主练习组对比。

Result: LbT组的儿童在语法任务上表现显著优于自主练习组，尤其是知识水平较低的儿童。行为数据显示儿童逐渐调整教学策略。

Conclusion: 交互式RL是一种有效的教学模型，社交机器人可作为适应性伙伴提升学习效果，首次展示了多机器人在真实课堂中的可行性。

Abstract: Despite growing interest in Learning-by-Teaching (LbT), few studies have
explored how this paradigm can be implemented with autonomous, peer-like social
robots in real classrooms. Most prior work has relied on scripted or
Wizard-of-Oz behaviors, limiting our understanding of how real-time,
interactive learning can be supported by artificial agents. This study
addresses this gap by introducing Interactive Reinforcement Learning (RL) as a
cognitive model for teachable social robots. We conducted two between-subject
experiments with 58 primary school children, who either taught a robot or
practiced independently on a tablet while learning French vocabulary
(memorization) and grammatical rules (inference). The robot, powered by
Interactive RL, learned from the child's evaluative feedback. Children in the
LbT condition achieved significantly higher retention gains compared to those
in the self-practice condition, especially on the grammar task. Learners with
lower prior knowledge benefited most from teaching the robot. Behavioural
metrics revealed that children adapted their teaching strategies over time and
engaged more deeply during inference tasks. This work makes two contributions:
(1) it introduces Interactive RL as a pedagogically effective and scalable
model for peer-robot learning, and (2) it demonstrates, for the first time, the
feasibility of deploying multiple autonomous robots simultaneously in real
classrooms. These findings extend theoretical understanding of LbT by showing
that social robots can function not only as passive tutees but as adaptive
partners that enhance meta-cognitive engagement and long-term learning
outcomes.

</details>


### [481] [NOVA: Navigation via Object-Centric Visual Autonomy for High-Speed Target Tracking in Unstructured GPS-Denied Environments](https://arxiv.org/abs/2506.18689)
*Alessandro Saviolo,Giuseppe Loianno*

Main category: cs.RO

TL;DR: NOVA是一个完全机载的、以目标为中心的框架，仅使用立体相机和IMU实现鲁棒的目标跟踪和碰撞感知导航，无需依赖全局地图或绝对定位。


<details>
  <summary>Details</summary>
Motivation: 解决在非结构化和GPS缺失环境中自主空中目标跟踪的挑战，避免依赖运动捕捉系统或预映射场景。

Method: 结合轻量级目标检测器、立体深度补全和直方图滤波，通过视觉惯性状态估计器和非线性模型预测控制器实现目标帧内的6-DoF位姿恢复和轨迹规划。

Result: 在复杂场景（如城市迷宫、森林小径）中验证，实现超过50 km/h的敏捷目标跟踪，性能稳定可靠。

Conclusion: NOVA证明了仅依赖机载传感器即可实现高速视觉目标跟踪，无需外部定位或环境假设。

Abstract: Autonomous aerial target tracking in unstructured and GPS-denied environments
remains a fundamental challenge in robotics. Many existing methods rely on
motion capture systems, pre-mapped scenes, or feature-based localization to
ensure safety and control, limiting their deployment in real-world conditions.
We introduce NOVA, a fully onboard, object-centric framework that enables
robust target tracking and collision-aware navigation using only a stereo
camera and an IMU. Rather than constructing a global map or relying on absolute
localization, NOVA formulates perception, estimation, and control entirely in
the target's reference frame. A tightly integrated stack combines a lightweight
object detector with stereo depth completion, followed by histogram-based
filtering to infer robust target distances under occlusion and noise. These
measurements feed a visual-inertial state estimator that recovers the full
6-DoF pose of the robot relative to the target. A nonlinear model predictive
controller (NMPC) plans dynamically feasible trajectories in the target frame.
To ensure safety, high-order control barrier functions are constructed online
from a compact set of high-risk collision points extracted from depth, enabling
real-time obstacle avoidance without maps or dense representations. We validate
NOVA across challenging real-world scenarios, including urban mazes, forest
trails, and repeated transitions through buildings with intermittent GPS loss
and severe lighting changes that disrupt feature-based localization. Each
experiment is repeated multiple times under similar conditions to assess
resilience, showing consistent and reliable performance. NOVA achieves agile
target following at speeds exceeding 50 km/h. These results show that
high-speed vision-based tracking is possible in the wild using only onboard
sensing, with no reliance on external localization or environment assumptions.

</details>


### [482] [MinD: Unified Visual Imagination and Control via Hierarchical World Models](https://arxiv.org/abs/2506.18897)
*Xiaowei Chi,Kuangzhi Ge,Jiaming Liu,Siyuan Zhou,Peidong Jia,Zichen He,Yuzhen Liu,Tingguang Li,Lei Han,Sirui Han,Shanghang Zhang,Yike Guo*

Main category: cs.RO

TL;DR: 论文提出了一种名为MinD的分层扩散世界模型框架，通过双系统设计解决视频生成模型在机器人应用中的速度慢和一致性差问题，实现了低延迟闭环控制和可靠的任务预测。


<details>
  <summary>Details</summary>
Motivation: 视频生成模型（VGMs）在机器人统一世界建模中具有潜力，但实际应用中存在生成速度慢和视频与动作一致性差的问题。

Method: MinD采用分层扩散框架，结合低频视频预测和高频扩散策略，并引入DiffMatcher模块和共训练策略以协调两者。

Result: MinD在RL-Bench中实现了63%以上的操作性能，并在任务预测和风险缓解方面表现出色。

Conclusion: MinD通过双系统设计和DiffMatcher模块，显著提升了VGMs在机器人操作和世界建模中的性能，推动了统一世界建模的前沿。

Abstract: Video generation models (VGMs) offer a promising pathway for unified world
modeling in robotics by integrating simulation, prediction, and manipulation.
However, their practical application remains limited due to (1) slowgeneration
speed, which limits real-time interaction, and (2) poor consistency between
imagined videos and executable actions. To address these challenges, we propose
Manipulate in Dream (MinD), a hierarchical diffusion-based world model
framework that employs a dual-system design for vision-language manipulation.
MinD executes VGM at low frequencies to extract video prediction features,
while leveraging a high-frequency diffusion policy for real-time interaction.
This architecture enables low-latency, closed-loop control in manipulation with
coherent visual guidance. To better coordinate the two systems, we introduce a
video-action diffusion matching module (DiffMatcher), with a novel co-training
strategy that uses separate schedulers for each diffusion model. Specifically,
we introduce a diffusion-forcing mechanism to DiffMatcher that aligns their
intermediate representations during training, helping the fast action model
better understand video-based predictions. Beyond manipulation, MinD also
functions as a world simulator, reliably predicting task success or failure in
latent space before execution. Trustworthy analysis further shows that VGMs can
preemptively evaluate task feasibility and mitigate risks. Extensive
experiments across multiple benchmarks demonstrate that MinD achieves
state-of-the-art manipulation (63%+) in RL-Bench, advancing the frontier of
unified world modeling in robotics.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [483] [When concept-based XAI is imprecise: Do people distinguish between generalisations and misrepresentations?](https://arxiv.org/abs/2506.17936)
*Romy Müller*

Main category: cs.HC

TL;DR: 研究发现，人们难以区分AI概念解释中的泛化与不精确性，尤其是在非相关特征上，这可能影响对AI模型深层理解的判断。


<details>
  <summary>Details</summary>
Motivation: 探讨人们是否能识别并欣赏AI概念解释中的泛化行为，以及是否能区分泛化与不精确性。

Method: 通过实验铁路安全场景，参与者评估AI基于图像片段概念解释的危险性判断。

Result: 参与者对非相关特征的泛化概念评价较低，但对相关特征的不精确性高度敏感。

Conclusion: 人们可能无法自发识别泛化行为，从而难以判断AI是否真正理解复杂情境。

Abstract: Concept-based explainable artificial intelligence (C-XAI) can help reveal the
inner representations of AI models. Understanding these representations is
particularly important in complex tasks like safety evaluation. Such tasks rely
on high-level semantic information (e.g., about actions) to make decisions
about abstract categories (e.g., whether a situation is dangerous). In this
context, it may desirable for C-XAI concepts to show some variability,
suggesting that the AI is capable of generalising beyond the concrete details
of a situation. However, it is unclear whether people recognise and appreciate
such generalisations and can distinguish them from other, less desirable forms
of imprecision. This was investigated in an experimental railway safety
scenario. Participants evaluated the performance of a simulated AI that
evaluated whether traffic scenes involving people were dangerous. To explain
these decisions, the AI provided concepts in the form of similar image
snippets. These concepts differed in their match with the classified image,
either regarding a highly relevant feature (i.e., relation to tracks) or a less
relevant feature (i.e., actions). Contrary to the hypotheses, concepts that
generalised over less relevant features led to ratings that were lower than for
precisely matching concepts and comparable to concepts that systematically
misrepresented these features. Conversely, participants were highly sensitive
to imprecisions in relevant features. These findings cast doubts on whether
people spontaneously recognise generalisations. Accordingly, they might not be
able to infer from C-XAI concepts whether AI models have gained a deeper
understanding of complex situations.

</details>


### [484] [Conceptualization, Operationalization, and Measurement of Machine Companionship: A Scoping Review](https://arxiv.org/abs/2506.18119)
*Jaime Banks,Zhixin Li*

Main category: cs.HC

TL;DR: 本文通过PRISMA指导的范围综述，系统分析了71篇关于机器伴侣（MC）的学术文献，提出了MC的定义：一种随时间展开、主观积极、自主且协调的人机连接。


<details>
  <summary>Details</summary>
Motivation: 机器伴侣的概念在社会技术想象中长期存在，但缺乏对其作为正式概念或测量变量的深入研究。本文旨在填补这一空白。

Method: 采用PRISMA指导的范围综述方法，系统筛选、调查和综合2017-2025年的71篇相关文献。

Result: 文献对MC的定义和测量变量差异较大，最终提出MC为一种自主、协调、随时间展开且主观积极的人机连接。

Conclusion: 本文为MC提供了文献支持的定义，为未来研究奠定了基础。

Abstract: The notion of machine companions has long been embedded in
social-technological imaginaries. Recent advances in AI have moved those media
musings into believable sociality manifested in interfaces, robotic bodies, and
devices. Those machines are often referred to colloquially as "companions" yet
there is little careful engagement of machine companionship (MC) as a formal
concept or measured variable. This PRISMA-guided scoping review systematically
samples, surveys, and synthesizes current scholarly works on MC (N = 71;
2017-2025), to that end. Works varied widely in considerations of MC according
to guiding theories, dimensions of a-priori specified properties (subjectively
positive, sustained over time, co-active, autotelic), and in measured concepts
(with more than 50 distinct measured variables). WE ultimately offer a
literature-guided definition of MC as an autotelic, coordinated connection
between human and machine that unfolds over time and is subjectively positive.

</details>


### [485] [AI Harmonizer: Expanding Vocal Expression with a Generative Neurosymbolic Music AI System](https://arxiv.org/abs/2506.18143)
*Lancelot Blanchard,Cameron Holt,Joseph A. Paradiso*

Main category: cs.HC

TL;DR: AI Harmonizer利用生成式AI技术，自动为独唱旋律生成四部和声，无需用户输入和声信息。


<details>
  <summary>Details</summary>
Motivation: 传统和声生成工具需要用户具备音乐专业知识，而AI Harmonizer旨在简化这一过程，让更多人能够轻松使用。

Method: 结合先进的音高检测、声部建模和符号音乐模型，系统自动生成和声。

Result: 系统能够生成丰富的合唱纹理，适用于表演和作曲。

Conclusion: AI Harmonizer是AI辅助音乐表演的重要一步，未来将探索实时实现。代码已在GitHub发布。

Abstract: Vocals harmonizers are powerful tools to help solo vocalists enrich their
melodies with harmonically supportive voices. These tools exist in various
forms, from commercially available pedals and software to custom-built systems,
each employing different methods to generate harmonies. Traditional harmonizers
often require users to manually specify a key or tonal center, while others
allow pitch selection via an external keyboard-both approaches demanding some
degree of musical expertise. The AI Harmonizer introduces a novel approach by
autonomously generating musically coherent four-part harmonies without
requiring prior harmonic input from the user. By integrating state-of-the-art
generative AI techniques for pitch detection and voice modeling with
custom-trained symbolic music models, our system arranges any vocal melody into
rich choral textures. In this paper, we present our methods, explore potential
applications in performance and composition, and discuss future directions for
real-time implementations. While our system currently operates offline, we
believe it represents a significant step toward AI-assisted vocal performance
and expressive musical augmentation. We release our implementation on GitHub.

</details>


### [486] [Two Sonification Methods for the MindCube](https://arxiv.org/abs/2506.18196)
*Fangzheng Liu,Lancelot Blanchard,Don D. Haddad,Joseph A. Paradiso*

Main category: cs.HC

TL;DR: 探索MindCube作为音乐界面的潜力，用于情绪调节，提出两种映射方式（含AI与不含AI），并讨论结果与未来方向。


<details>
  <summary>Details</summary>
Motivation: 研究MindCube作为音乐界面的适用性，帮助用户调节情绪。

Method: 设计两种MindCube映射方式（含AI与不含AI），利用生成式AI在潜在空间中导航。

Result: 提出了一种结合AI的映射方法，能够有效调节情绪。

Conclusion: MindCube在情绪调节音乐系统中具有潜力，未来可进一步优化AI映射。

Abstract: In this work, we explore the musical interface potential of the MindCube, an
interactive device designed to study emotions. Embedding diverse sensors and
input devices, this interface resembles a fidget cube toy commonly used to help
users relieve their stress and anxiety. As such, it is a particularly
well-suited controller for musical systems that aim to help with emotion
regulation. In this regard, we present two different mappings for the MindCube,
with and without AI. With our generative AI mapping, we propose a way to infuse
meaning within a latent space and techniques to navigate through it with an
external controller. We discuss our results and propose directions for future
work.

</details>


### [487] [BRAVE: Brain-Controlled Prosthetic Arm with Voice Integration and Embodied Learning for Enhanced Mobility](https://arxiv.org/abs/2506.18749)
*Abdul Basit,Maha Nawaz,Muhammad Shafique*

Main category: cs.HC

TL;DR: BRAVE是一种结合EEG和语音控制的假肢系统，通过集成集成学习和人机交互校正框架，提高了响应速度和分类准确性，适用于无创假肢控制。


<details>
  <summary>Details</summary>
Motivation: 解决现有EEG控制假肢系统在信号噪声、分类准确性和实时适应性方面的挑战。

Method: 结合LSTM、CNN和随机森林的集成学习框架，使用ICA和CSP预处理EEG信号，并集成语音识别进行模式切换。

Result: 分类准确率达到96%，响应延迟150毫秒，适用于多用户和低功耗嵌入式部署。

Conclusion: BRAVE为非侵入式假肢控制提供了稳健、实时的解决方案。

Abstract: Non-invasive brain-computer interfaces (BCIs) have the potential to enable
intuitive control of prosthetic limbs for individuals with upper limb
amputations. However, existing EEG-based control systems face challenges
related to signal noise, classification accuracy, and real-time adaptability.
In this work, we present BRAVE, a hybrid EEG and voice-controlled prosthetic
system that integrates ensemble learning-based EEG classification with a
human-in-the-loop (HITL) correction framework for enhanced responsiveness.
Unlike traditional electromyography (EMG)-based prosthetic control, BRAVE aims
to interpret EEG-driven motor intent, enabling movement control without
reliance on residual muscle activity. To improve classification robustness,
BRAVE combines LSTM, CNN, and Random Forest models in an ensemble framework,
achieving a classification accuracy of 96% across test subjects. EEG signals
are preprocessed using a bandpass filter (0.5-45 Hz), Independent Component
Analysis (ICA) for artifact removal, and Common Spatial Pattern (CSP) feature
extraction to minimize contamination from electromyographic (EMG) and
electrooculographic (EOG) signals. Additionally, BRAVE incorporates automatic
speech recognition (ASR) to facilitate intuitive mode switching between
different degrees of freedom (DOF) in the prosthetic arm. The system operates
in real time, with a response latency of 150 ms, leveraging Lab Streaming Layer
(LSL) networking for synchronized data acquisition. The system is evaluated on
an in-house fabricated prosthetic arm and on multiple participants highlighting
the generalizability across users. The system is optimized for low-power
embedded deployment, ensuring practical real-world application beyond
high-performance computing environments. Our results indicate that BRAVE offers
a promising step towards robust, real-time, non-invasive prosthetic control.

</details>


<div id='cs.IR'></div>

# cs.IR [[Back]](#toc)

### [488] [QUST_NLP at SemEval-2025 Task 7: A Three-Stage Retrieval Framework for Monolingual and Crosslingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2506.17272)
*Youzheng Liu,Jiyan Liu,Xiaoman Xu,Taihang Wang,Yimin Wang,Ye Jiang*

Main category: cs.IR

TL;DR: QUST_NLP团队在SemEval-2025 Task 7中提出了一种三阶段检索框架，用于事实核查声明检索，最终在单语和多语赛道中分别获得第5和第7名。


<details>
  <summary>Details</summary>
Motivation: 设计一个高效的检索框架，以提升事实核查声明的检索效果。

Method: 三阶段框架：1) 评估并选择最佳检索模型；2) 使用多个重排序模型优化候选结果；3) 加权投票确定最终结果。

Result: 在单语赛道排名第5，多语赛道排名第7。

Conclusion: 提出的三阶段框架在事实核查声明检索中表现良好，代码已开源。

Abstract: This paper describes the participation of QUST_NLP in the SemEval-2025 Task
7. We propose a three-stage retrieval framework specifically designed for
fact-checked claim retrieval. Initially, we evaluate the performance of several
retrieval models and select the one that yields the best results for candidate
retrieval. Next, we employ multiple re-ranking models to enhance the candidate
results, with each model selecting the Top-10 outcomes. In the final stage, we
utilize weighted voting to determine the final retrieval outcomes. Our approach
achieved 5th place in the monolingual track and 7th place in the crosslingual
track. We release our system code at:
https://github.com/warmth27/SemEval2025_Task7

</details>


### [489] [Chunk Twice, Embed Once: A Systematic Study of Segmentation and Representation Trade-offs in Chemistry-Aware Retrieval-Augmented Generation](https://arxiv.org/abs/2506.17277)
*Mahmoud Amiri,Thomas Bocklitz*

Main category: cs.IR

TL;DR: 本文系统评估了化学领域检索增强生成（RAG）系统的文档分块策略和嵌入模型，发现递归基于标记的分块（R100-0）和优化检索的嵌入模型（如Nomic和Intfloat E5）表现最佳。


<details>
  <summary>Details</summary>
Motivation: 化学领域文献快速增长，但RAG系统在领域特定设计（如文档分块和表示）上缺乏深入研究。

Method: 评估了25种分块策略和48种嵌入模型，使用三个化学特定基准（包括新数据集QuestChemRetrieval）。

Result: 递归基于标记的分块（R100-0）表现最优，检索优化嵌入模型（如Nomic和Intfloat E5）优于领域专用模型（如SciBERT）。

Conclusion: 提供了构建高效化学RAG系统的实用指南，并公开了数据集和评估框架。

Abstract: Retrieval-Augmented Generation (RAG) systems are increasingly vital for
navigating the ever-expanding body of scientific literature, particularly in
high-stakes domains such as chemistry. Despite the promise of RAG, foundational
design choices -- such as how documents are segmented and represented -- remain
underexplored in domain-specific contexts. This study presents the first
large-scale, systematic evaluation of chunking strategies and embedding models
tailored to chemistry-focused RAG systems. We investigate 25 chunking
configurations across five method families and evaluate 48 embedding models on
three chemistry-specific benchmarks, including the newly introduced
QuestChemRetrieval dataset. Our results reveal that recursive token-based
chunking (specifically R100-0) consistently outperforms other approaches,
offering strong performance with minimal resource overhead. We also find that
retrieval-optimized embeddings -- such as Nomic and Intfloat E5 variants --
substantially outperform domain-specialized models like SciBERT. By releasing
our datasets, evaluation framework, and empirical benchmarks, we provide
actionable guidelines for building effective and efficient chemistry-aware RAG
systems.

</details>


### [490] [CORONA: A Coarse-to-Fine Framework for Graph-based Recommendation with Large Language Models](https://arxiv.org/abs/2506.17281)
*Junze Chen,Xinjie Yang,Cheng Yang,Junfei Bao,Zeyuan Guo,Yawen Li,Chuan Shi*

Main category: cs.IR

TL;DR: 论文提出了一种名为CORONA的框架，利用大语言模型（LLMs）的推理能力在候选筛选过程中，结合图神经网络（GNNs）提升推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现有研究未能充分利用LLMs在候选筛选阶段的潜力，导致性能不佳。作者希望通过结合LLMs的推理能力和GNNs的高阶交互捕捉能力，优化推荐效果。

Method: CORONA框架分三步：1）LLM基于用户画像进行偏好推理，提取相关用户和物品；2）LLM结合历史数据进一步缩小候选范围；3）GNN从子图中捕获高阶信息生成最终推荐。

Result: 实验表明，CORONA在召回率和NDCG上平均相对提升了18.6%和18.4%，达到最优性能。

Conclusion: CORONA通过结合LLMs和GNNs的优势，显著提升了推荐系统的性能，证明了其在候选筛选阶段的潜力。

Abstract: Recommender systems (RSs) are designed to retrieve candidate items a user
might be interested in from a large pool. A common approach is using graph
neural networks (GNNs) to capture high-order interaction relationships. As
large language models (LLMs) have shown strong capabilities across domains,
researchers are exploring their use to enhance recommendation. However, prior
work limits LLMs to re-ranking results or dataset augmentation, failing to
utilize their power during candidate filtering - which may lead to suboptimal
performance. Instead, we propose to leverage LLMs' reasoning abilities during
the candidate filtering process, and introduce Chain Of Retrieval ON grAphs
(CORONA) to progressively narrow down the range of candidate items on
interaction graphs with the help of LLMs: (1) First, LLM performs preference
reasoning based on user profiles, with the response serving as a query to
extract relevant users and items from the interaction graph as
preference-assisted retrieval; (2) Then, using the information retrieved in the
previous step along with the purchase history of target user, LLM conducts
intent reasoning to help refine an even smaller interaction subgraph as
intent-assisted retrieval; (3) Finally, we employ a GNN to capture high-order
collaborative filtering information from the extracted subgraph, performing
GNN-enhanced retrieval to generate the final recommendation results. The
proposed framework leverages the reasoning capabilities of LLMs during the
retrieval process, while seamlessly integrating GNNs to enhance overall
recommendation performance. Extensive experiments on various datasets and
settings demonstrate that our proposed CORONA achieves state-of-the-art
performance with an 18.6% relative improvement in recall and an 18.4% relative
improvement in NDCG on average.

</details>


### [491] [SlimRAG: Retrieval without Graphs via Entity-Aware Context Selection](https://arxiv.org/abs/2506.17288)
*Jiale Zhang,Jiaxiang Chen,Zhucong Li,Jie Ding,Kui Zhao,Zenglin Xu,Xin Pang,Yinghui Xu*

Main category: cs.IR

TL;DR: SlimRAG是一种轻量级框架，通过实体感知机制替代图结构，提高检索效率，减少无关内容。


<details>
  <summary>Details</summary>
Motivation: 图基础的RAG系统存在结构开销大和检索不精确的问题，语义相似性不等于语义相关性。

Method: SlimRAG构建实体到块的紧凑表，查询时识别关键实体并检索相关块，避免图遍历。

Result: 实验表明SlimRAG在准确性和检索效率上优于基线方法，减少了索引大小和RITU。

Conclusion: SlimRAG证明了无结构、以实体为中心的检索方法的有效性。

Abstract: Retrieval-Augmented Generation (RAG) enhances language models by
incorporating external knowledge at inference time. However, graph-based RAG
systems often suffer from structural overhead and imprecise retrieval: they
require costly pipelines for entity linking and relation extraction, yet
frequently return subgraphs filled with loosely related or tangential content.
This stems from a fundamental flaw -- semantic similarity does not imply
semantic relevance. We introduce SlimRAG, a lightweight framework for retrieval
without graphs. SlimRAG replaces structure-heavy components with a simple yet
effective entity-aware mechanism. At indexing time, it constructs a compact
entity-to-chunk table based on semantic embeddings. At query time, it
identifies salient entities, retrieves and scores associated chunks, and
assembles a concise, contextually relevant input -- without graph traversal or
edge construction. To quantify retrieval efficiency, we propose Relative Index
Token Utilization (RITU), a metric measuring the compactness of retrieved
content. Experiments across multiple QA benchmarks show that SlimRAG
outperforms strong flat and graph-based baselines in accuracy while reducing
index size and RITU (e.g., 16.31 vs. 56+), highlighting the value of
structure-free, entity-centric context selection. The code will be released
soon. https://github.com/continue-ai-company/SlimRAG

</details>


### [492] [Enhancing Document Retrieval in COVID-19 Research: Leveraging Large Language Models for Hidden Relation Extraction](https://arxiv.org/abs/2506.18311)
*Hoang-An Trieu,Dinh-Truong Do,Chau Nguyen,Vu Tran,Minh Le Nguyen*

Main category: cs.IR

TL;DR: 提出了一种利用大语言模型（LLMs）改进COVID-19相关文献检索系统的方法，以提取未标注文献中的隐藏关系。


<details>
  <summary>Details</summary>
Motivation: COVID-19疫情期间文献数量庞大，现有检索系统效率不足，需要更高效的工具支持突发疫情的研究。

Method: 利用大语言模型（LLMs）提取未标注文献中的隐藏关系，改进Covrelex-SE检索系统。

Result: 系统能够提供更高质量的检索结果，增强检索过程中的信息有用性。

Conclusion: 该方法显著提升了检索系统的效率，为突发疫情研究提供了更好的支持。

Abstract: In recent years, with the appearance of the COVID-19 pandemic, numerous
publications relevant to this disease have been issued. Because of the massive
volume of publications, an efficient retrieval system is necessary to provide
researchers with useful information if an unexpected pandemic happens so
suddenly, like COVID-19. In this work, we present a method to help the
retrieval system, the Covrelex-SE system, to provide more high-quality search
results. We exploited the power of the large language models (LLMs) to extract
the hidden relationships inside the unlabeled publication that cannot be found
by the current parsing tools that the system is using. Since then, help the
system to have more useful information during retrieval progress.

</details>


### [493] [Team LA at SCIDOCA shared task 2025: Citation Discovery via relation-based zero-shot retrieval](https://arxiv.org/abs/2506.18316)
*Trieu An,Long Nguyen,Minh Le Nguyen*

Main category: cs.IR

TL;DR: 论文提出了一种结合关系特征提取和大型语言模型（LLM）的方法，用于从候选池中预测正确的引用。


<details>
  <summary>Details</summary>
Motivation: 解决因摘要段落长度和候选摘要高度相似性导致的引用预测困难。

Method: 首先基于提取的关系特征检索最相似的top-k摘要，然后利用LLM精确识别最相关引用。

Result: 在SCIDOCA 2025提供的训练数据集上验证了方法的有效性。

Conclusion: 该方法能够有效解决引用预测问题。

Abstract: The Citation Discovery Shared Task focuses on predicting the correct citation
from a given candidate pool for a given paragraph. The main challenges stem
from the length of the abstract paragraphs and the high similarity among
candidate abstracts, making it difficult to determine the exact paper to cite.
To address this, we develop a system that first retrieves the top-k most
similar abstracts based on extracted relational features from the given
paragraph. From this subset, we leverage a Large Language Model (LLM) to
accurately identify the most relevant citation. We evaluate our framework on
the training dataset provided by the SCIDOCA 2025 organizers, demonstrating its
effectiveness in citation prediction.

</details>


### [494] [Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models](https://arxiv.org/abs/2506.17580)
*Sajratul Y. Rubaiat,Hasan M. Jamil*

Main category: cs.IR

TL;DR: WISE是一个智能科学知识提取系统，通过结构化工作流程解决传统搜索引擎和LLMs的局限性，提供详细、有序的答案。


<details>
  <summary>Details</summary>
Motivation: 科学文献的快速增长使知识提取和合成变得困难，传统搜索引擎和LLMs在深度和完整性上存在不足。

Method: WISE采用基于LLM的树状架构，动态评分和排名，结合自适应停止标准，提取和精炼查询相关知识。

Result: 实验显示WISE在HBB基因相关疾病任务中减少了80%的文本处理量，召回率显著高于基线方法，且输出更具独特性和深度。

Conclusion: WISE在多个领域（如药物发现、材料科学）中展示了高效的知识提取和合成能力。

Abstract: The exponential growth of scientific literature challenges researchers
extracting and synthesizing knowledge. Traditional search engines return many
sources without direct, detailed answers, while general-purpose LLMs may offer
concise responses that lack depth or omit current information. LLMs with search
capabilities are also limited by context window, yielding short, incomplete
answers. This paper introduces WISE (Workflow for Intelligent Scientific
Knowledge Extraction), a system addressing these limits by using a structured
workflow to extract, refine, and rank query-specific knowledge. WISE uses an
LLM-powered, tree-based architecture to refine data, focusing on query-aligned,
context-aware, and non-redundant information. Dynamic scoring and ranking
prioritize unique contributions from each source, and adaptive stopping
criteria minimize processing overhead. WISE delivers detailed, organized
answers by systematically exploring and synthesizing knowledge from diverse
sources. Experiments on HBB gene-associated diseases demonstrate WISE reduces
processed text by over 80% while achieving significantly higher recall over
baselines like search engines and other LLM-based approaches. ROUGE and BLEU
metrics reveal WISE's output is more unique than other systems, and a novel
level-based metric shows it provides more in-depth information. We also explore
how the WISE workflow can be adapted for diverse domains like drug discovery,
material science, and social science, enabling efficient knowledge extraction
and synthesis from unstructured scientific papers and web sources.

</details>


### [495] [Reinforcing User Interest Evolution in Multi-Scenario Learning for recommender systems](https://arxiv.org/abs/2506.17682)
*Zhijian Feng,Wenhao Zheng,Xuanji Xiao*

Main category: cs.IR

TL;DR: 提出了一种基于强化学习的方法，通过建模用户兴趣在多场景中的演化，提升多场景推荐系统的性能。


<details>
  <summary>Details</summary>
Motivation: 现实推荐系统中，用户在不同场景的兴趣可能不一致，导致统一建模困难。

Method: 采用Double Q-learning和对比学习损失优化，建模用户兴趣在多场景中的演化。

Result: 实验表明，该方法在多场景推荐任务中优于现有方法。

Conclusion: 为多场景建模提供了新视角，并指出了未来研究方向。

Abstract: In real-world recommendation systems, users would engage in variety
scenarios, such as homepages, search pages, and related recommendation pages.
Each of these scenarios would reflect different aspects users focus on.
However, the user interests may be inconsistent in different scenarios, due to
differences in decision-making processes and preference expression. This
variability complicates unified modeling, making multi-scenario learning a
significant challenge. To address this, we propose a novel reinforcement
learning approach that models user preferences across scenarios by modeling
user interest evolution across multiple scenarios. Our method employs Double
Q-learning to enhance next-item prediction accuracy and optimizes contrastive
learning loss using Q-value to make model performance better. Experimental
results demonstrate that our approach surpasses state-of-the-art methods in
multi-scenario recommendation tasks. Our work offers a fresh perspective on
multi-scenario modeling and highlights promising directions for future
research.

</details>


### [496] [CARTS: Collaborative Agents for Recommendation Textual Summarization](https://arxiv.org/abs/2506.17765)
*Jiao Chen,Kehui Yao,Reza Yousefi Maragheh,Kai Zhao,Jianpeng Xu,Jason Cho,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Main category: cs.IR

TL;DR: CARTS是一个多代理LLM框架，用于推荐系统中的文本摘要生成，通过分阶段协作显著提升标题相关性和用户参与度。


<details>
  <summary>Details</summary>
Motivation: 当前推荐系统中的文本摘要方法无法直接满足对核心特征相关性和字数限制的严格要求。

Method: CARTS采用三阶段多代理框架（GAG、细化循环和仲裁），分别负责特征提取、迭代优化和最终标题选择。

Result: 实验表明，CARTS在标题相关性和用户参与度上显著优于单次和链式思维LLM基线。

Conclusion: CARTS为推荐系统中的文本摘要提供了一种高效且协作的解决方案。

Abstract: Current recommendation systems often require some form of textual data
summarization, such as generating concise and coherent titles for product
carousels or other grouped item displays. While large language models have
shown promise in NLP domains for textual summarization, these approaches do not
directly apply to recommendation systems, where explanations must be highly
relevant to the core features of item sets, adhere to strict word limit
constraints. In this paper, we propose CARTS (Collaborative Agents for
Recommendation Textual Summarization), a multi-agent LLM framework designed for
structured summarization in recommendation systems. CARTS decomposes the task
into three stages-Generation Augmented Generation (GAG), refinement circle, and
arbitration, where successive agent roles are responsible for extracting
salient item features, iteratively refining candidate titles based on relevance
and length feedback, and selecting the final title through a collaborative
arbitration process. Experiments on large-scale e-commerce data and live A/B
testing show that CARTS significantly outperforms single-pass and
chain-of-thought LLM baselines, delivering higher title relevance and improved
user engagement metrics.

</details>


### [497] [Expanding Relevance Judgments for Medical Case-based Retrieval Task with Multimodal LLMs](https://arxiv.org/abs/2506.17782)
*Catarina Pires,Sérgio Nunes,Luís Filipe Teixeira*

Main category: cs.IR

TL;DR: 该论文探讨了使用多模态大语言模型（MLLM）扩展信息检索系统中的相关性标注，以减少对人工标注的依赖，并在医学案例检索任务中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 信息检索系统评估依赖高质量的人工相关性标注，但成本高且耗时。多模态大语言模型（MLLM）提供了一种替代方案，尤其在需要分析文本和视觉信息的复杂领域（如医学案例检索）中具有潜力。

Method: 使用Gemini 1.5 Pro模型，通过结构化提示策略（包括二元评分、基于指令的评估和少样本学习）模拟人类标注，扩展了ImageCLEFmed 2013任务中的相关性标注数据集。

Result: MLLM与人工标注的一致性达到Cohen's Kappa 0.6，扩展了数据集37倍（从15,028标注增至558,653），并将相关标注从4.72%增至5,950。

Conclusion: MLLM能够显著扩展相关性标注规模，为医学和多模态信息检索任务提供了一种高效的评估支持方法。

Abstract: Evaluating Information Retrieval (IR) systems relies on high-quality manual
relevance judgments (qrels), which are costly and time-consuming to obtain.
While pooling reduces the annotation effort, it results in only partially
labeled datasets. Large Language Models (LLMs) offer a promising alternative to
reducing reliance on manual judgments, particularly in complex domains like
medical case-based retrieval, where relevance assessment requires analyzing
both textual and visual information. In this work, we explore using a
Multimodal Large Language Model (MLLM) to expand relevance judgments, creating
a new dataset of automated judgments. Specifically, we employ Gemini 1.5 Pro on
the ImageCLEFmed 2013 case-based retrieval task, simulating human assessment
through an iteratively refined, structured prompting strategy that integrates
binary scoring, instruction-based evaluation, and few-shot learning. We
systematically experimented with various prompt configurations to maximize
agreement with human judgments. To evaluate agreement between the MLLM and
human judgments, we use Cohen's Kappa, achieving a substantial agreement score
of 0.6, comparable to inter-annotator agreement typically observed in
multimodal retrieval tasks. Starting from the original 15,028 manual judgments
(4.72% relevant) across 35 topics, our MLLM-based approach expanded the dataset
by over 37x to 558,653 judgments, increasing relevant annotations to 5,950. On
average, each medical case query received 15,398 new annotations, with
approximately 99% being non-relevant, reflecting the high sparsity typical in
this domain. Our results demonstrate the potential of MLLMs to scale relevance
judgment collection, offering a promising direction for supporting retrieval
evaluation in medical and multimodal IR tasks.

</details>


### [498] [A GenAI System for Improved FAIR Independent Biological Database Integration](https://arxiv.org/abs/2506.17934)
*Syed N. Sakib,Kallol Naha,Sajratul Y. Rubaiat,Hasan M. Jamil*

Main category: cs.IR

TL;DR: FAIRBridge是一个基于自然语言的查询处理系统，旨在帮助科学家发现、访问和查询生物数据库，即使这些数据库不符合FAIR原则。


<details>
  <summary>Details</summary>
Motivation: 生命科学研究需要从不断变化的Linked Open Data (LOD)网络中高效处理数据，但现有方法通常耗时、易错且昂贵。FAIRBridge旨在解决这些问题。

Method: FAIRBridge利用AI技术解释查询意图，将其映射到科学文献中描述的数据库，并通过智能资源访问计划生成可执行查询。系统还包含工具以应对低质量查询处理。

Result: FAIRBridge提供了一个用户友好的自动化平台，支持探索替代数据源，并在每一步提供信息选择，显著提高了科学数据的集成和处理效率。

Conclusion: FAIRBridge为研究人员提供了一个强大的新工具，能够显著提升科学数据的整合和查询能力。

Abstract: Life sciences research increasingly requires identifying, accessing, and
effectively processing data from an ever-evolving array of information sources
on the Linked Open Data (LOD) network. This dynamic landscape places a
significant burden on researchers, as the quality of query responses depends
heavily on the selection and semantic integration of data sources --processes
that are often labor-intensive, error-prone, and costly. While the adoption of
FAIR (Findable, Accessible, Interoperable, and Reusable) data principles has
aimed to address these challenges, barriers to efficient and accurate
scientific data processing persist.
  In this paper, we introduce FAIRBridge, an experimental natural
language-based query processing system designed to empower scientists to
discover, access, and query biological databases, even when they are not
FAIR-compliant. FAIRBridge harnesses the capabilities of AI to interpret query
intents, map them to relevant databases described in scientific literature, and
generate executable queries via intelligent resource access plans. The system
also includes robust tools for mitigating low-quality query processing,
ensuring high fidelity and responsiveness in the information delivered.
  FAIRBridge's autonomous query processing framework enables users to explore
alternative data sources, make informed choices at every step, and leverage
community-driven crowd curation when needed. By providing a user-friendly,
automated hypothesis-testing platform in natural English, FAIRBridge
significantly enhances the integration and processing of scientific data,
offering researchers a powerful new tool for advancing their inquiries.

</details>


### [499] [LLM-Enhanced Multimodal Fusion for Cross-Domain Sequential Recommendation](https://arxiv.org/abs/2506.17966)
*Wangyu Wu,Zhenhong Chen,Xianglin Qiu,Siqi Song,Xiaowei Huang,Fei Ma,Jimin Xiao*

Main category: cs.IR

TL;DR: LLM-EMF是一种跨域序列推荐方法，通过融合多模态数据和LLM知识提升推荐性能。


<details>
  <summary>Details</summary>
Motivation: 解决跨域用户行为预测中单模态数据的局限性，利用多模态数据提升推荐准确性。

Method: 使用冻结的CLIP模型生成图像和文本嵌入，结合多重注意力机制学习单域和跨域偏好。

Result: 在四个电商数据集上表现优于现有方法，验证了多模态数据融合的有效性。

Conclusion: LLM-EMF通过多模态数据整合显著提升了跨域序列推荐系统的性能。

Abstract: Cross-Domain Sequential Recommendation (CDSR) predicts user behavior by
leveraging historical interactions across multiple domains, focusing on
modeling cross-domain preferences and capturing both intra- and inter-sequence
item relationships. We propose LLM-Enhanced Multimodal Fusion for Cross-Domain
Sequential Recommendation (LLM-EMF), a novel and advanced approach that
enhances textual information with Large Language Models (LLM) knowledge and
significantly improves recommendation performance through the fusion of visual
and textual data. Using the frozen CLIP model, we generate image and text
embeddings, thereby enriching item representations with multimodal data. A
multiple attention mechanism jointly learns both single-domain and cross-domain
preferences, effectively capturing and understanding complex user interests
across diverse domains. Evaluations conducted on four e-commerce datasets
demonstrate that LLM-EMF consistently outperforms existing methods in modeling
cross-domain user preferences, thereby highlighting the effectiveness of
multimodal data integration and its advantages in enhancing sequential
recommendation systems. Our source code will be released.

</details>


### [500] [LettinGo: Explore User Profile Generation for Recommendation System](https://arxiv.org/abs/2506.18309)
*Lu Wang,Di Zhang,Fangkai Yang,Pu Zhao,Jianfeng Liu,Yuefeng Zhan,Hao Sun,Qingwei Lin,Weiwei Deng,Dongmei Zhang,Feng Sun,Qi Zhang*

Main category: cs.IR

TL;DR: LettinGo是一个新颖的框架，利用LLMs生成多样化和自适应的用户画像，通过DPO优化任务性能，显著提升推荐系统的准确性、灵活性和上下文感知能力。


<details>
  <summary>Details</summary>
Motivation: 传统基于嵌入的用户画像缺乏可解释性和适应性，而现有文本画像方法受限于固定格式，无法捕捉用户行为的多样性。

Method: LettinGo通过三个阶段实现：1) 多LLMs探索多样化画像，2) 基于推荐任务评估画像质量，3) 利用任务性能的偏好数据对齐画像生成。

Result: 实验表明，LettinGo显著提升了推荐准确性、灵活性和上下文感知能力。

Conclusion: LettinGo为下一代推荐系统提供了创新的用户画像生成方法。

Abstract: User profiling is pivotal for recommendation systems, as it transforms raw
user interaction data into concise and structured representations that drive
personalized recommendations. While traditional embedding-based profiles lack
interpretability and adaptability, recent advances with large language models
(LLMs) enable text-based profiles that are semantically richer and more
transparent. However, existing methods often adhere to fixed formats that limit
their ability to capture the full diversity of user behaviors. In this paper,
we introduce LettinGo, a novel framework for generating diverse and adaptive
user profiles. By leveraging the expressive power of LLMs and incorporating
direct feedback from downstream recommendation tasks, our approach avoids the
rigid constraints imposed by supervised fine-tuning (SFT). Instead, we employ
Direct Preference Optimization (DPO) to align the profile generator with
task-specific performance, ensuring that the profiles remain adaptive and
effective. LettinGo operates in three stages: (1) exploring diverse user
profiles via multiple LLMs, (2) evaluating profile quality based on their
impact in recommendation systems, and (3) aligning the profile generation
through pairwise preference data derived from task performance. Experimental
results demonstrate that our framework significantly enhances recommendation
accuracy, flexibility, and contextual awareness. This work enhances profile
generation as a key innovation for next-generation recommendation systems.

</details>


### [501] [Bias vs Bias -- Dawn of Justice: A Fair Fight in Recommendation Systems](https://arxiv.org/abs/2506.18327)
*Tahsin Alamgir Kheya,Mohamed Reda Bouadjenek,Sunil Aryal*

Main category: cs.IR

TL;DR: 提出了一种公平感知的重新排序方法，用于缓解推荐系统中的多类别偏见问题。


<details>
  <summary>Details</summary>
Motivation: 推荐系统在多个领域影响用户体验，但现有研究忽视了某些类别的偏见，且多集中于二元敏感属性。

Method: 利用现有偏见信息，设计了一种重新排序方法，可处理多敏感属性（如性别、年龄、职业）。

Result: 在三个真实数据集上验证了方法的有效性，能在不显著降低性能的情况下缓解社会偏见。

Conclusion: 该方法为推荐系统中的多类别偏见问题提供了有效解决方案。

Abstract: Recommendation systems play a crucial role in our daily lives by impacting
user experience across various domains, including e-commerce, job
advertisements, entertainment, etc. Given the vital role of such systems in our
lives, practitioners must ensure they do not produce unfair and imbalanced
recommendations. Previous work addressing bias in recommendations overlooked
bias in certain item categories, potentially leaving some biases unaddressed.
Additionally, most previous work on fair re-ranking focused on binary-sensitive
attributes. In this paper, we address these issues by proposing a
fairness-aware re-ranking approach that helps mitigate bias in different
categories of items. This re-ranking approach leverages existing biases to
correct disparities in recommendations across various demographic groups. We
show how our approach can mitigate bias on multiple sensitive attributes,
including gender, age, and occupation. We experimented on three real-world
datasets to evaluate the effectiveness of our re-ranking scheme in mitigating
bias in recommendations. Our results show how this approach helps mitigate
social bias with little to no degradation in performance.

</details>


### [502] [PERSCEN: Learning Personalized Interaction Pattern and Scenario Preference for Multi-Scenario Matching](https://arxiv.org/abs/2506.18382)
*Haotong Du,Yaqing Wang,Fei Xiong,Lei Shao,Ming Liu,Hao Gu,Quanming Yao,Zhen Wang*

Main category: cs.IR

TL;DR: PERSCEN是一种创新的多场景匹配方法，通过用户特定建模和图神经网络捕捉跨场景共享偏好，同时利用向量量化技术提取场景感知偏好，显著提升推荐效果。


<details>
  <summary>Details</summary>
Motivation: 随着在线平台业务规模和范围的扩大，多场景匹配成为降低维护成本和缓解数据稀疏的主流解决方案。现有方法常忽略用户特定建模，限制了个性化用户表示生成。

Method: PERSCEN构建用户特定特征图，使用轻量图神经网络捕捉高阶交互模式，并通过向量量化技术从行为序列中提取场景感知偏好，结合渐进式场景感知门控线性单元实现信息融合。

Result: 实验表明PERSCEN优于现有方法，且能有效平衡性能与计算成本。

Conclusion: PERSCEN通过用户特定和场景感知建模，为工业系统提供了高效且实用的多场景推荐解决方案。

Abstract: With the expansion of business scales and scopes on online platforms,
multi-scenario matching has become a mainstream solution to reduce maintenance
costs and alleviate data sparsity. The key to effective multi-scenario
recommendation lies in capturing both user preferences shared across all
scenarios and scenario-aware preferences specific to each scenario. However,
existing methods often overlook user-specific modeling, limiting the generation
of personalized user representations. To address this, we propose PERSCEN, an
innovative approach that incorporates user-specific modeling into
multi-scenario matching. PERSCEN constructs a user-specific feature graph based
on user characteristics and employs a lightweight graph neural network to
capture higher-order interaction patterns, enabling personalized extraction of
preferences shared across scenarios. Additionally, we leverage vector
quantization techniques to distil scenario-aware preferences from users'
behavior sequence within individual scenarios, facilitating user-specific and
scenario-aware preference modeling. To enhance efficient and flexible
information transfer, we introduce a progressive scenario-aware gated linear
unit that allows fine-grained, low-latency fusion. Extensive experiments
demonstrate that PERSCEN outperforms existing methods. Further efficiency
analysis confirms that PERSCEN effectively balances performance with
computational cost, ensuring its practicality for real-world industrial
systems.

</details>


<div id='cs.CY'></div>

# cs.CY [[Back]](#toc)

### [503] [Distinguishing Predictive and Generative AI in Regulation](https://arxiv.org/abs/2506.17347)
*Jennifer Wang,Andrew Selbst,Solon Barocas,Suresh Venkatasubramanian*

Main category: cs.CY

TL;DR: 本文探讨了生成式AI对现有政策工具的挑战，提出了四点独特之处，并给出三条政策建议。


<details>
  <summary>Details</summary>
Motivation: 生成式AI的出现使得原有针对预测式AI的监管假设不再适用，需要重新评估政策工具的有效性。

Method: 通过分析生成式AI的四个独特方面（通用性、评估难度、法律问题和分布式结构），提出政策调整建议。

Result: 识别出生成式AI的四个关键特征，并建议政策制定者重新评估现有工具，设计针对性的新政策。

Conclusion: 政策制定者需结合生成式AI的特点，调整监管目标，利用生态系统约束更有效地治理生成式AI。

Abstract: Over the past decade, policymakers have developed a set of regulatory tools
to ensure AI development aligns with key societal goals. Many of these tools
were initially developed in response to concerns with predictive AI and
therefore encode certain assumptions about the nature of AI systems and the
utility of certain regulatory approaches. With the advent of generative AI,
however, some of these assumptions no longer hold, even as policymakers attempt
to maintain a single regulatory target that covers both types of AI.
  In this paper, we identify four distinct aspects of generative AI that call
for meaningfully different policy responses. These are the generality and
adaptability of generative AI that make it a poor regulatory target, the
difficulty of designing effective evaluations, new legal concerns that change
the ecosystem of stakeholders and sources of expertise, and the distributed
structure of the generative AI value chain.
  In light of these distinctions, policymakers will need to evaluate where the
past decade of policy work remains relevant and where new policies, designed to
address the unique risks posed by generative AI, are necessary. We outline
three recommendations for policymakers to more effectively identify regulatory
targets and leverage constraints across the broader ecosystem to govern
generative AI.

</details>


### [504] [The Democratic Paradox in Large Language Models' Underestimation of Press Freedom](https://arxiv.org/abs/2506.18045)
*I. Loaiza,R. Vestrelli,A. Fronzetti Colladon,R. Rigobon*

Main category: cs.CY

TL;DR: 研究发现六种流行大语言模型（LLMs）在评估180个国家新闻自由时存在系统性偏差，与专家评估相比，LLMs普遍低估新闻自由，且对本国存在正面偏见。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在全球信息获取中的核心作用，其偏见可能影响公众对民主制度的理解与信任，尤其是新闻自由。

Method: 比较六种LLMs对180个国家新闻自由的评估与世界新闻自由指数（WPFI）的专家评估。

Result: LLMs普遍低估新闻自由（71%-93%国家），在新闻自由较强的国家低估更严重，且五种LLMs对本国存在正面偏见（7%-260%）。

Conclusion: 若LLMs成为下一代搜索引擎和文化工具，必须确保其对全球人权和公民权利的准确表征。

Abstract: As Large Language Models (LLMs) increasingly mediate global information
access for millions of users worldwide, their alignment and biases have the
potential to shape public understanding and trust in fundamental democratic
institutions, such as press freedom. In this study, we uncover three systematic
distortions in the way six popular LLMs evaluate press freedom in 180 countries
compared to expert assessments of the World Press Freedom Index (WPFI). The six
LLMs exhibit a negative misalignment, consistently underestimating press
freedom, with individual models rating between 71% to 93% of countries as less
free. We also identify a paradoxical pattern we term differential misalignment:
LLMs disproportionately underestimate press freedom in countries where it is
strongest. Additionally, five of the six LLMs exhibit positive home bias,
rating their home countries' press freedoms more favorably than would be
expected given their negative misalignment with the human benchmark. In some
cases, LLMs rate their home countries between 7% to 260% more positively than
expected. If LLMs are set to become the next search engines and some of the
most important cultural tools of our time, they must ensure accurate
representations of the state of our human and civic rights globally.

</details>


### [505] [Automatic Large Language Models Creation of Interactive Learning Lessons](https://arxiv.org/abs/2506.17356)
*Jionghao Lin,Jiarui Rao,Yiyang Zhao,Yuting Wang,Ashish Gurung,Amanda Barany,Jaclyn Ocumpaugh,Ryan S. Baker,Kenneth R. Koedinger*

Main category: cs.CY

TL;DR: 论文探讨了利用GPT-4o和任务分解提示策略自动生成交互式、基于场景的数学辅导培训课程，结果显示该方法优于单步生成。


<details>
  <summary>Details</summary>
Motivation: 研究旨在通过AI辅助生成高效的辅导培训课程，以提升新手数学辅导员的在线教学能力。

Method: 采用检索增强生成（RAG）和任务分解提示策略，生成三个主题的课程，并通过人工评估验证效果。

Result: 任务分解策略生成的课程评分更高，内容结构良好且节省时间，但也存在反馈泛化和部分指令不清晰的问题。

Conclusion: 混合人机方法在生成高效辅导培训课程方面具有潜力。

Abstract: We explore the automatic generation of interactive, scenario-based lessons
designed to train novice human tutors who teach middle school mathematics
online. Employing prompt engineering through a Retrieval-Augmented Generation
approach with GPT-4o, we developed a system capable of creating structured
tutor training lessons. Our study generated lessons in English for three key
topics: Encouraging Students' Independence, Encouraging Help-Seeking Behavior,
and Turning on Cameras, using a task decomposition prompting strategy that
breaks lesson generation into sub-tasks. The generated lessons were evaluated
by two human evaluators, who provided both quantitative and qualitative
evaluations using a comprehensive rubric informed by lesson design research.
Results demonstrate that the task decomposition strategy led to higher-rated
lessons compared to single-step generation. Human evaluators identified several
strengths in the LLM-generated lessons, including well-structured content and
time-saving potential, while also noting limitations such as generic feedback
and a lack of clarity in some instructional sections. These findings underscore
the potential of hybrid human-AI approaches for generating effective lessons in
tutor training.

</details>


### [506] [A Large-Scale Real-World Evaluation of LLM-Based Virtual Teaching Assistant](https://arxiv.org/abs/2506.17363)
*Sunjun Kweon,Sooyohn Nam,Hyunseung Lim,Hwajung Hong,Edward Choi*

Main category: cs.CY

TL;DR: 研究评估了基于大语言模型的虚拟助教（VTA）在真实课堂中的效果和接受度，通过大规模实证研究和互动分析，探讨了其可行性和挑战。


<details>
  <summary>Details</summary>
Motivation: 尽管虚拟助教有潜力提升学习效果，但其在实际课堂中的效果和接受度缺乏实证研究。

Method: 开发了基于LLM的VTA，并在477名研究生的AI编程课程中部署，通过三轮调查和3,869次互动分析评估效果。

Result: 研究发现VTA在课堂中的可行性和挑战，并比较了其与人类教师的互动差异。

Conclusion: 研究为AI驱动的教育提供了实证支持，并开源了VTA系统代码以促进未来发展。

Abstract: Virtual Teaching Assistants (VTAs) powered by Large Language Models (LLMs)
have the potential to enhance student learning by providing instant feedback
and facilitating multi-turn interactions. However, empirical studies on their
effectiveness and acceptance in real-world classrooms are limited, leaving
their practical impact uncertain. In this study, we develop an LLM-based VTA
and deploy it in an introductory AI programming course with 477 graduate
students. To assess how student perceptions of the VTA's performance evolve
over time, we conduct three rounds of comprehensive surveys at different stages
of the course. Additionally, we analyze 3,869 student--VTA interaction pairs to
identify common question types and engagement patterns. We then compare these
interactions with traditional student--human instructor interactions to
evaluate the VTA's role in the learning process. Through a large-scale
empirical study and interaction analysis, we assess the feasibility of
deploying VTAs in real-world classrooms and identify key challenges for broader
adoption. Finally, we release the source code of our VTA system, fostering
future advancements in AI-driven education:
\texttt{https://github.com/sean0042/VTA}.

</details>


### [507] [AI-based Multimodal Biometrics for Detecting Smartphone Distractions: Application to Online Learning](https://arxiv.org/abs/2506.17364)
*Alvaro Becerra,Roberto Daza,Ruth Cobos,Aythami Morales,Mutlu Cukurova,Julian Fierrez*

Main category: cs.CY

TL;DR: 研究通过多模态生物识别技术检测在线学习中因手机使用导致的注意力分散，提出基于AI的方法，结合生理信号和头部姿态数据，多模态模型准确率达91%。


<details>
  <summary>Details</summary>
Motivation: 在线学习中，学习者面临内部、系统和上下文因素的干扰，传统平台缺乏详细行为数据，多模态学习分析和生物传感器为注意力研究提供新视角。

Method: 利用生理信号（如脑波、心率）和头部姿态数据，构建AI模型检测手机使用行为。

Result: 单一生物信号准确率有限（如脑波或心率），头部姿态单独准确率为87%，多模态模型结合所有信号后准确率提升至91%。

Conclusion: 多模态模型在在线学习环境中具有实时支持潜力，但也需考虑部署的限制和影响。

Abstract: This work investigates the use of multimodal biometrics to detect
distractions caused by smartphone use during tasks that require sustained
attention, with a focus on computer-based online learning. Although the methods
are applicable to various domains, such as autonomous driving, we concentrate
on the challenges learners face in maintaining engagement amid internal (e.g.,
motivation), system-related (e.g., course design) and contextual (e.g.,
smartphone use) factors. Traditional learning platforms often lack detailed
behavioral data, but Multimodal Learning Analytics (MMLA) and biosensors
provide new insights into learner attention. We propose an AI-based approach
that leverages physiological signals and head pose data to detect phone use.
Our results show that single biometric signals, such as brain waves or heart
rate, offer limited accuracy, while head pose alone achieves 87%. A multimodal
model combining all signals reaches 91% accuracy, highlighting the benefits of
integration. We conclude by discussing the implications and limitations of
deploying these models for real-time support in online learning environments.

</details>


### [508] [AI based Content Creation and Product Recommendation Applications in E-commerce: An Ethical overview](https://arxiv.org/abs/2506.17370)
*Aditi Madhusudan Jain,Ayush Jain*

Main category: cs.CY

TL;DR: 本文探讨了AI在电子商务内容创作和产品推荐中的伦理挑战，提出了消除偏见和确保包容性的最佳实践，并讨论了保护数据隐私和增强消费者自主权的伦理框架。


<details>
  <summary>Details</summary>
Motivation: 随着AI在电子商务中的广泛应用，其带来的伦理问题（如数据隐私、算法偏见和消费者自主权）日益凸显，亟需建立公平、透明的伦理标准。

Method: 通过定期算法审计、多样化训练数据和引入公平性指标等方法，提出消除偏见的实践建议，并构建保护隐私和透明决策的伦理框架。

Result: 研究提供了确保AI在电子商务中既高效又符合伦理的实用指南，强调了公平性和透明性的重要性。

Conclusion: 通过实施提出的伦理框架和最佳实践，可以确保AI在电子商务中的应用既有效又符合伦理标准。

Abstract: As e-commerce rapidly integrates artificial intelligence for content creation
and product recommendations, these technologies offer significant benefits in
personalization and efficiency. AI-driven systems automate product
descriptions, generate dynamic advertisements, and deliver tailored
recommendations based on consumer behavior, as seen in major platforms like
Amazon and Shopify. However, the widespread use of AI in e-commerce raises
crucial ethical challenges, particularly around data privacy, algorithmic bias,
and consumer autonomy. Bias -- whether cultural, gender-based, or socioeconomic
-- can be inadvertently embedded in AI models, leading to inequitable product
recommendations and reinforcing harmful stereotypes. This paper examines the
ethical implications of AI-driven content creation and product recommendations,
emphasizing the need for frameworks to ensure fairness, transparency, and need
for more established and robust ethical standards. We propose actionable best
practices to remove bias and ensure inclusivity, such as conducting regular
audits of algorithms, diversifying training data, and incorporating fairness
metrics into AI models. Additionally, we discuss frameworks for ethical
conformance that focus on safeguarding consumer data privacy, promoting
transparency in decision-making processes, and enhancing consumer autonomy. By
addressing these issues, we provide guidelines for responsibly utilizing AI in
e-commerce applications for content creation and product recommendations,
ensuring that these technologies are both effective and ethically sound.

</details>


### [509] [Multimodal Political Bias Identification and Neutralization](https://arxiv.org/abs/2506.17372)
*Cedric Bernard,Xavier Pleimling,Amun Kharel,Chase Vickery*

Main category: cs.CY

TL;DR: 提出了一种结合文本和图像偏见的去偏见模型，通过四个步骤（图像文本对齐、图像偏见评分、文本去偏见和最终去偏见）实现，初步结果良好但需更多资源优化。


<details>
  <summary>Details</summary>
Motivation: 政治回音室现象使得去除政治文章中的主观偏见和情绪化语言变得重要，但现有研究仅关注文本部分，忽略了图像的偏见影响。

Method: 模型包括四个步骤：1) 图像文本对齐（CLIP模型）；2) 图像偏见评分（ViT分类器）；3) 文本去偏见（BERT模型）；4) 最终去偏见（替换为中性内容）。

Result: 文本去偏见能有效识别偏见词汇，ViT模型训练效果良好，语义对齐高效，但需更多资源和时间优化。

Conclusion: 模型初步表现良好，但需进一步优化和人工评估以确保语义一致性。

Abstract: Due to the presence of political echo chambers, it becomes imperative to
detect and remove subjective bias and emotionally charged language from both
the text and images of political articles. However, prior work has focused on
solely the text portion of the bias rather than both the text and image
portions. This is a problem because the images are just as powerful of a medium
to communicate information as text is. To that end, we present a model that
leverages both text and image bias which consists of four different steps.
Image Text Alignment focuses on semantically aligning images based on their
bias through CLIP models. Image Bias Scoring determines the appropriate bias
score of images via a ViT classifier. Text De-Biasing focuses on detecting
biased words and phrases and neutralizing them through BERT models. These three
steps all culminate to the final step of debiasing, which replaces the text and
the image with neutralized or reduced counterparts, which for images is done by
comparing the bias scores. The results so far indicate that this approach is
promising, with the text debiasing strategy being able to identify many
potential biased words and phrases, and the ViT model showcasing effective
training. The semantic alignment model also is efficient. However, more time,
particularly in training, and resources are needed to obtain better results. A
human evaluation portion was also proposed to ensure semantic consistency of
the newly generated text and images.

</details>


### [510] [Optimizing Mastery Learning by Fast-Forwarding Over-Practice Steps](https://arxiv.org/abs/2506.17577)
*Meng Xia,Robin Schmucker,Conrad Borchers,Vincent Aleven*

Main category: cs.CY

TL;DR: Fast-Forwarding技术通过跳过已掌握的步骤减少过度练习，提升学习效率，但对高难度问题的选择效果最佳。


<details>
  <summary>Details</summary>
Motivation: 解决辅导系统中学生过度练习已掌握技能的问题，避免资源密集的课程重新设计。

Method: 提出Fast-Forwarding技术，基于学习者模型和问题解决路径模拟研究，跳过已完全掌握的步骤。

Result: Fast-Forwarding可减少高达三分之一的过度练习，尤其适用于偏好选择高难度问题的算法。

Conclusion: Fast-Forwarding能提升练习效率，但实际效果取决于学生在高难度任务中的动机和参与度。

Abstract: Mastery learning improves learning proficiency and efficiency. However, the
overpractice of skills--students spending time on skills they have already
mastered--remains a fundamental challenge for tutoring systems. Previous
research has reduced overpractice through the development of better problem
selection algorithms and the authoring of focused practice tasks. However, few
efforts have concentrated on reducing overpractice through step-level
adaptivity, which can avoid resource-intensive curriculum redesign. We propose
and evaluate Fast-Forwarding as a technique that enhances existing problem
selection algorithms. Based on simulation studies informed by learner models
and problem-solving pathways derived from real student data, Fast-Forwarding
can reduce overpractice by up to one-third, as it does not require students to
complete problem-solving steps if all remaining pathways are fully mastered.
Fast-Forwarding is a flexible method that enhances any problem selection
algorithm, though its effectiveness is highest for algorithms that
preferentially select difficult problems. Therefore, our findings suggest that
while Fast-Forwarding may improve student practice efficiency, the size of its
practical impact may also depend on students' ability to stay motivated and
engaged at higher levels of difficulty.

</details>


### [511] [MAARTA:Multi-Agentic Adaptive Radiology Teaching Assistant](https://arxiv.org/abs/2506.17320)
*Akash Awasthi,Brandon V. Chang,Anh M. Vu,Ngan Le,Rishi Agrawal,Zhigang Deng,Carol Wu,Hien Van Nguyen*

Main category: cs.CY

TL;DR: MAARTA是一个多智能体框架，通过分析视线模式和放射学报告提供个性化反馈，帮助学生改进视觉搜索和诊断推理。


<details>
  <summary>Details</summary>
Motivation: 放射学学生因缺乏专家指导时间而难以培养感知能力，导致视觉搜索和诊断解释错误，现有AI系统未能有效解决此类问题。

Method: MAARTA通过动态选择智能体分析视线行为差异，使用结构化图比较专家与学生行为，并通过逐步提示帮助学生理解错误。

Result: 系统能够识别遗漏发现并分析差异，提供个性化反馈以改进学生的诊断推理能力。

Conclusion: MAARTA推动了AI驱动的放射学教育，通过多智能体框架有效解决学生感知错误问题。

Abstract: Radiology students often struggle to develop perceptual expertise due to
limited expert mentorship time, leading to errors in visual search and
diagnostic interpretation. These perceptual errors, such as missed fixations,
short dwell times, or misinterpretations, are not adequately addressed by
current AI systems, which focus on diagnostic accuracy but fail to explain how
and why errors occur. To address this gap, we introduce MAARTA (Multi-Agentic
Adaptive Radiology Teaching Assistant), a multi-agent framework that analyzes
gaze patterns and radiology reports to provide personalized feedback. Unlike
single-agent models, MAARTA dynamically selects agents based on error
complexity, enabling adaptive and efficient reasoning. By comparing expert and
student gaze behavior through structured graphs, the system identifies missed
findings and assigns Perceptual Error Teacher agents to analyze discrepancies.
MAARTA then uses step-by-step prompting to help students understand their
errors and improve diagnostic reasoning, advancing AI-driven radiology
education.

</details>


<div id='q-bio.BM'></div>

# q-bio.BM [[Back]](#toc)

### [512] [OmniESI: A unified framework for enzyme-substrate interaction prediction with progressive conditional deep learning](https://arxiv.org/abs/2506.17963)
*Zhiwei Nie,Hongyu Zhang,Hao Jiang,Yutian Liu,Xiansong Huang,Fan Xu,Jie Fu,Zhixiang Ren,Yonghong Tian,Wen-Bin Zhang,Jie Chen*

Main category: q-bio.BM

TL;DR: OmniESI是一个两阶段渐进式框架，通过条件深度学习预测酶-底物相互作用，整合催化特异性与关键相互作用，显著提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 现有预测方法未充分利用酶催化的先验知识，无法合理调整与催化模式不符的通用蛋白-分子特征。

Method: OmniESI采用两阶段条件网络，分别强调酶反应特异性和催化相关相互作用，逐步从通用特征过渡到催化感知特征。

Result: 在七个基准测试中，OmniESI在分布内和分布外设置下均优于现有方法，且参数增加极少（0.16%）。

Conclusion: OmniESI为酶-底物相互作用提供了统一预测方法，具有强泛化能力和广泛应用性。

Abstract: Understanding and modeling enzyme-substrate interactions is crucial for
catalytic mechanism research, enzyme engineering, and metabolic engineering.
Although a large number of predictive methods have emerged, they do not
incorporate prior knowledge of enzyme catalysis to rationally modulate general
protein-molecule features that are misaligned with catalytic patterns. To
address this issue, we introduce a two-stage progressive framework, OmniESI,
for enzyme-substrate interaction prediction through conditional deep learning.
By decomposing the modeling of enzyme-substrate interactions into a two-stage
progressive process, OmniESI incorporates two conditional networks that
respectively emphasize enzymatic reaction specificity and crucial
catalysis-related interactions, facilitating a gradual feature modulation in
the latent space from general protein-molecule domain to catalysis-aware
domain. On top of this unified architecture, OmniESI can adapt to a variety of
downstream tasks, including enzyme kinetic parameter prediction,
enzyme-substrate pairing prediction, enzyme mutational effect prediction, and
enzymatic active site annotation. Under the multi-perspective performance
evaluation of in-distribution and out-of-distribution settings, OmniESI
consistently delivered superior performance than state-of-the-art specialized
methods across seven benchmarks. More importantly, the proposed conditional
networks were shown to internalize the fundamental patterns of catalytic
efficiency while significantly improving prediction performance, with only
negligible parameter increases (0.16%), as demonstrated by ablation studies on
key components. Overall, OmniESI represents a unified predictive approach for
enzyme-substrate interactions, providing an effective tool for catalytic
mechanism cracking and enzyme engineering with strong generalization and broad
applicability.

</details>


<div id='eess.IV'></div>

# eess.IV [[Back]](#toc)

### [513] [Can Common VLMs Rival Medical VLMs? Evaluation and Strategic Insights](https://arxiv.org/abs/2506.17337)
*Yuan Zhong,Ruinan Jin,Xiaoxiao Li,Qi Dou*

Main category: eess.IV

TL;DR: 研究比较了通用视觉语言模型（VLMs）和医学专用VLMs在医学影像任务中的表现，发现轻量级微调后通用VLMs可媲美或超越医学专用模型。


<details>
  <summary>Details</summary>
Motivation: 探讨通用VLMs是否通过微调能在特定医学影像任务中与医学专用VLMs竞争，以降低计算和数据资源需求。

Method: 系统评估CLIP和LLaVA模型在疾病诊断和视觉问答任务中的表现，包括域内和域外任务。

Result: 医学专用预训练在域内任务中有优势，但通用VLMs经微调后表现相当或更好，且在域外任务中展现强适应性。

Conclusion: 通用VLMs结合微调是一种可扩展且经济高效的替代方案，挑战了医学专用预训练的必要性。

Abstract: Medical vision-language models (VLMs) leverage large-scale pretraining for
diverse imaging tasks but require substantial computational and data resources.
Meanwhile, common or general-purpose VLMs (e.g., CLIP, LLaVA), though not
trained for medical use, show promise with fine-tuning. This raises a key
question: Can efficient fine-tuned common VLMs rival generalist medical VLMs
for solving specific medical imaging tasks? This study systematically evaluates
common and medical VLMs across disease diagnosis and visual question answering
(VQA). Using CLIP-based and LLaVA-based models, we examine (1) off-the-shelf
performance gaps in in-domain (ID) settings, (2) whether fine-tuning bridges
these gaps, and (3) generalization to out-of-domain (OOD) tasks on unseen
medical modalities. While medical-specific pretraining provides advantages in
ID settings, common VLMs match or surpass medical-specific models after
lightweight fine-tuning, with LoRA-based adaptation proving highly effective
among different tasks. In OOD tasks, common VLMs demonstrate strong
adaptability in some tasks, challenging the assumption that medical-specific
pre-training is essential. These findings suggest that leveraging common VLMs
with fine-tuning offers a scalable and cost-effective alternative to developing
large-scale medical VLMs, providing crucial insights for future research in the
medical imaging field.

</details>


### [514] [DSA-NRP: No-Reflow Prediction from Angiographic Perfusion Dynamics in Stroke EVT](https://arxiv.org/abs/2506.17501)
*Shreeram Athreya,Carlos Olivares,Ameera Ismail,Kambiz Nael,William Speier,Corey Arnold*

Main category: eess.IV

TL;DR: 该研究提出了一种基于机器学习的框架，利用术中数字减影血管造影（DSA）序列和临床变量，在急性缺血性卒中（AIS）患者接受血管内血栓切除术（EVT）后立即预测无复流现象。


<details>
  <summary>Details</summary>
Motivation: 无复流现象是EVT后常见的并发症，会导致微血管低灌注，影响组织恢复和临床结果。目前依赖术后24小时内的灌注MRI进行诊断，存在延迟。

Method: 研究回顾性分析了UCLA医学中心的AIS患者数据，提取DSA序列的统计和时序灌注特征，结合临床变量训练机器学习分类器。

Result: 该方法显著优于基于临床特征的基线模型（AUC: 0.7703 vs. 0.5728；准确率: 0.8125 vs. 0.6331），表明DSA灌注动态能实时反映微血管完整性。

Conclusion: 该研究为无复流现象的即时预测奠定了基础，有助于临床医生及时干预高风险患者，减少对延迟成像的依赖。

Abstract: Following successful large-vessel recanalization via endovascular
thrombectomy (EVT) for acute ischemic stroke (AIS), some patients experience a
complication known as no-reflow, defined by persistent microvascular
hypoperfusion that undermines tissue recovery and worsens clinical outcomes.
Although prompt identification is crucial, standard clinical practice relies on
perfusion magnetic resonance imaging (MRI) within 24 hours post-procedure,
delaying intervention. In this work, we introduce the first-ever machine
learning (ML) framework to predict no-reflow immediately after EVT by
leveraging previously unexplored intra-procedural digital subtraction
angiography (DSA) sequences and clinical variables. Our retrospective analysis
included AIS patients treated at UCLA Medical Center (2011-2024) who achieved
favorable mTICI scores (2b-3) and underwent pre- and post-procedure MRI.
No-reflow was defined as persistent hypoperfusion (Tmax > 6 s) on
post-procedural imaging. From DSA sequences (AP and lateral views), we
extracted statistical and temporal perfusion features from the target
downstream territory to train ML classifiers for predicting no-reflow. Our
novel method significantly outperformed a clinical-features baseline(AUC:
0.7703 $\pm$ 0.12 vs. 0.5728 $\pm$ 0.12; accuracy: 0.8125 $\pm$ 0.10 vs. 0.6331
$\pm$ 0.09), demonstrating that real-time DSA perfusion dynamics encode
critical insights into microvascular integrity. This approach establishes a
foundation for immediate, accurate no-reflow prediction, enabling clinicians to
proactively manage high-risk patients without reliance on delayed imaging.

</details>


### [515] [MTSIC: Multi-stage Transformer-based GAN for Spectral Infrared Image Colorization](https://arxiv.org/abs/2506.17540)
*Tingting Liu,Yuan Liu,Jinhui Tang,Liyin Yuan,Chengyu Liu,Chunlai Li,Xiubao Sui,Qian Chen*

Main category: eess.IV

TL;DR: 提出了一种基于GAN的多波段红外图像彩色化框架MTSIC，通过自注意力机制和多尺度小波块提升语义准确性和视觉质量。


<details>
  <summary>Details</summary>
Motivation: TIR图像缺乏颜色和纹理信息，现有方法依赖单波段图像导致失真和语义模糊，多波段红外图像提供了更丰富的光谱数据。

Method: 使用多阶段光谱自注意力Transformer网络（MTSIC）作为生成器，结合空间-光谱注意力残差块（SARB）和多尺度小波块（MSWB）优化特征提取。

Result: 实验表明，该方法显著优于传统技术，有效提升了红外图像的视觉质量。

Conclusion: MTSIC框架通过多波段特征映射和语义对齐，显著改善了红外图像的彩色化效果。

Abstract: Thermal infrared (TIR) images, acquired through thermal radiation imaging,
are unaffected by variations in lighting conditions and atmospheric haze.
However, TIR images inherently lack color and texture information, limiting
downstream tasks and potentially causing visual fatigue. Existing colorization
methods primarily rely on single-band images with limited spectral information
and insufficient feature extraction capabilities, which often result in image
distortion and semantic ambiguity. In contrast, multiband infrared imagery
provides richer spectral data, facilitating the preservation of finer details
and enhancing semantic accuracy. In this paper, we propose a generative
adversarial network (GAN)-based framework designed to integrate spectral
information to enhance the colorization of infrared images. The framework
employs a multi-stage spectral self-attention Transformer network (MTSIC) as
the generator. Each spectral feature is treated as a token for self-attention
computation, and a multi-head self-attention mechanism forms a spatial-spectral
attention residual block (SARB), achieving multi-band feature mapping and
reducing semantic confusion. Multiple SARB units are integrated into a
Transformer-based single-stage network (STformer), which uses a U-shaped
architecture to extract contextual information, combined with multi-scale
wavelet blocks (MSWB) to align semantic information in the spatial-frequency
dual domain. Multiple STformer modules are cascaded to form MTSIC,
progressively optimizing the reconstruction quality. Experimental results
demonstrate that the proposed method significantly outperforms traditional
techniques and effectively enhances the visual quality of infrared images.

</details>


### [516] [LVPNet: A Latent-variable-based Prediction-driven End-to-end Framework for Lossless Compression of Medical Images](https://arxiv.org/abs/2506.17983)
*Chenyue Song,Chen Hui,Qing Lin,Wei Zhang,Siqiao Li,Shengping Zhang,Haiqi Zhu,Zhixuan Li,Shaohui Liu,Feng Jiang,Xiang Li*

Main category: eess.IV

TL;DR: LVPNet提出了一种基于预测的无损医学图像压缩方法，通过全局潜在变量预测像素值，并编码预测概率以实现高效压缩。


<details>
  <summary>Details</summary>
Motivation: 现有方法中，图像分割导致潜在变量信息均匀分布，引发后验崩溃和低效利用。

Method: 引入全局多尺度感知模块（GMSM）提取紧凑潜在表示，并提出量化补偿模块（QCM）减少量化误差。

Result: 在多个基准测试中，LVPNet的压缩效率优于现有方法，同时保持推理速度。

Conclusion: LVPNet通过全局潜在变量和量化补偿，显著提升了无损医学图像压缩的性能。

Abstract: Autoregressive Initial Bits is a framework that integrates sub-image
autoregression and latent variable modeling, demonstrating its advantages in
lossless medical image compression. However, in existing methods, the image
segmentation process leads to an even distribution of latent variable
information across each sub-image, which in turn causes posterior collapse and
inefficient utilization of latent variables. To deal with these issues, we
propose a prediction-based end-to-end lossless medical image compression method
named LVPNet, leveraging global latent variables to predict pixel values and
encoding predicted probabilities for lossless compression. Specifically, we
introduce the Global Multi-scale Sensing Module (GMSM), which extracts compact
and informative latent representations from the entire image, effectively
capturing spatial dependencies within the latent space. Furthermore, to
mitigate the information loss introduced during quantization, we propose the
Quantization Compensation Module (QCM), which learns the distribution of
quantization errors and refines the quantized features to compensate for
quantization loss. Extensive experiments on challenging benchmarks demonstrate
that our method achieves superior compression efficiency compared to
state-of-the-art lossless image compression approaches, while maintaining
competitive inference speed. The code is at
https://github.com/Anonymity00000/Anonymity-repository/.

</details>


### [517] [Multimodal Medical Image Binding via Shared Text Embeddings](https://arxiv.org/abs/2506.18072)
*Yunhao Liu,Suyang Xi,Shiqi Liu,Hong Ding,Chicheng Jin,Chenxi Yang,Junjun He,Yiqing Shen*

Main category: eess.IV

TL;DR: M³Bind是一种新型预训练框架，通过共享文本表示空间实现多模态医学图像对齐，无需显式配对数据。


<details>
  <summary>Details</summary>
Motivation: 医学图像分析需要多模态整合以提高诊断准确性，但现有方法（如CLIP）需要显式配对数据，难以获取。

Method: M³Bind通过微调预训练的CLIP-like模型，对齐模态特定文本嵌入空间，并蒸馏为统一模型。

Result: 在X射线、CT等多种医学图像任务中，M³Bind在零样本、少样本分类和跨模态检索任务中表现最佳。

Conclusion: M³Bind有效实现了医学图像的跨模态对齐，为多模态医学分析提供了新方法。

Abstract: Medical image analysis increasingly relies on the integration of multiple
imaging modalities to capture complementary anatomical and functional
information, enabling more accurate diagnosis and treatment planning. Achieving
aligned feature representations across these diverse modalities is therefore
important for effective multimodal analysis. While contrastive language-image
pre-training (CLIP) and its variant have enabled image-text alignments, they
require explicitly paired data between arbitrary two modalities, which is
difficult to acquire in medical contexts. To address the gap, we present
Multimodal Medical Image Binding with Text (M\textsuperscript{3}Bind), a novel
pre-training framework that enables seamless alignment of multiple medical
imaging modalities through a shared text representation space without requiring
explicit paired data between any two medical image modalities. Specifically,
based on the insight that different images can naturally bind with text,
M\textsuperscript{3}Bind first fine-tunes pre-trained CLIP-like image-text
models to align their modality-specific text embedding space while preserving
their original image-text alignments. Subsequently, we distill these
modality-specific text encoders into a unified model, creating a shared text
embedding space. Experiments on X-ray, CT, retina, ECG, and pathological images
on multiple downstream tasks demonstrate that M\textsuperscript{3}Bind achieves
state-of-the-art performance in zero-shot, few-shot classification and
cross-modal retrieval tasks compared to its CLIP-like counterparts. These
results validate M\textsuperscript{3}Bind's effectiveness in achieving
cross-image-modal alignment for medical analysis.

</details>


### [518] [Transforming H&E images into IHC: A Variance-Penalized GAN for Precision Oncology](https://arxiv.org/abs/2506.18371)
*Sara Rehmat,Hafeez Ur Rehman*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习的图像转换框架，从H&E染色样本生成高保真IHC图像，用于HER2评估，解决了现有方法的局限性。


<details>
  <summary>Details</summary>
Motivation: HER2阳性乳腺癌需要精确诊断，但传统IHC方法成本高且依赖抗体选择，而H&E染色虽普及但缺乏特异性。

Method: 改进金字塔pix2pix的损失函数，引入基于方差的惩罚项，解决GAN中的模式崩溃问题，增强生成图像的结构多样性。

Result: 在BCI数据集上表现优于现有方法，尤其在HER2阳性（IHC 3+）图像转换中，PSNR、SSIM和FID指标显著提升。

Conclusion: 该模型为HER2诊断提供了高效可靠的替代方案，并在通用图像转换任务中展现出潜力。

Abstract: The overexpression of the human epidermal growth factor receptor 2 (HER2) in
breast cells is a key driver of HER2-positive breast cancer, a highly
aggressive subtype requiring precise diagnosis and targeted therapy.
Immunohistochemistry (IHC) is the standard technique for HER2 assessment but is
costly, labor-intensive, and highly dependent on antibody selection. In
contrast, hematoxylin and eosin (H&E) staining, a routine histopathological
procedure, offers broader accessibility but lacks HER2 specificity. This study
proposes an advanced deep learning-based image translation framework to
generate highfidelity IHC images from H&E-stained tissue samples, enabling
cost-effective and scalable HER2 assessment. By modifying the loss function of
pyramid pix2pix, we mitigate mode collapse, a fundamental limitation in
generative adversarial networks (GANs), and introduce a novel variance-based
penalty that enforces structural diversity in generated images. Our model
particularly excels in translating HER2-positive (IHC 3+) images, which have
remained challenging for existing methods due to their complex morphological
variations. Extensive evaluations on the BCI histopathological dataset
demonstrate that our model surpasses state-of-the-art methods in terms of peak
signal-tonoise ratio (PSNR), structural similarity index (SSIM), and Frechet
Inception Distance (FID), particularly in accurately translating HER2-positive
(IHC 3+) images. Beyond medical imaging, our model exhibits superior
performance in general image-to-image translation tasks, showcasing its
potential across multiple domains. This work marks a significant step toward
AI-driven precision oncology, offering a reliable and efficient alternative to
traditional HER2 diagnostics.

</details>


### [519] [Taming Vision-Language Models for Medical Image Analysis: A Comprehensive Review](https://arxiv.org/abs/2506.18378)
*Haoneng Lin,Cheng Xu,Jing Qin*

Main category: eess.IV

TL;DR: 本文综述了视觉语言模型（VLMs）在医学图像分析中的适应策略、挑战及未来方向。


<details>
  <summary>Details</summary>
Motivation: 医学领域对多模态整合的需求促使VLMs成为医学图像分析的有力工具，但适应过程中存在领域差距和任务多样性等挑战。

Method: 总结了医学VLMs的核心学习策略（预训练、微调和提示学习），并分类了五种主要适应策略，分析了其在11个医学成像任务中的应用。

Result: 揭示了VLMs在医学图像分析中的潜力，但也指出了当前的技术障碍和挑战。

Conclusion: 本文旨在帮助研究人员更好地理解VLMs的能力与局限，推动其在临床实践中的创新、稳健和安全应用。

Abstract: Modern Vision-Language Models (VLMs) exhibit unprecedented capabilities in
cross-modal semantic understanding between visual and textual modalities. Given
the intrinsic need for multi-modal integration in clinical applications, VLMs
have emerged as a promising solution for a wide range of medical image analysis
tasks. However, adapting general-purpose VLMs to medical domain poses numerous
challenges, such as large domain gaps, complicated pathological variations, and
diversity and uniqueness of different tasks. The central purpose of this review
is to systematically summarize recent advances in adapting VLMs for medical
image analysis, analyzing current challenges, and recommending promising yet
urgent directions for further investigations. We begin by introducing core
learning strategies for medical VLMs, including pretraining, fine-tuning, and
prompt learning. We then categorize five major VLM adaptation strategies for
medical image analysis. These strategies are further analyzed across eleven
medical imaging tasks to illustrate their current practical implementations.
Furthermore, we analyze key challenges that impede the effective adaptation of
VLMs to clinical applications and discuss potential directions for future
research. We also provide an open-access repository of related literature to
facilitate further research, available at
https://github.com/haonenglin/Awesome-VLM-for-MIA. It is anticipated that this
article can help researchers who are interested in harnessing VLMs in medical
image analysis tasks have a better understanding on their capabilities and
limitations, as well as current technical barriers, to promote their
innovative, robust, and safe application in clinical practice.

</details>


### [520] [A Deep Convolutional Neural Network-Based Novel Class Balancing for Imbalance Data Segmentation](https://arxiv.org/abs/2506.18474)
*Atifa Kalsoom,M. A. Iftikhar,Amjad Ali,Zubair Shah,Shidin Balakrishnan,Hazrat Ali*

Main category: eess.IV

TL;DR: 提出了一种基于深度学习和双层次类别平衡方案（BLCB-CNN）的新方法，用于视网膜血管分割，解决了数据分布不平衡和血管厚度变化的问题，取得了优异的性能指标。


<details>
  <summary>Details</summary>
Motivation: 视网膜血管分割的挑战在于数据分布不平衡和血管厚度变化，需要一种有效的方法来平衡类别分布并提升分割精度。

Method: 使用CNN架构和双层次类别平衡方案（Level-I平衡血管/非血管，Level-II平衡厚/薄血管），并结合GCN、CLAHE和伽马校正进行预处理。

Result: 在标准数据集上表现优异，ROC曲线下面积为98.23%，准确率为96.22%，灵敏度为81.57%，特异性为97.65%，并通过STARE图像验证了泛化能力。

Conclusion: BLCB-CNN方法在视网膜血管分割中表现出色，具有高精度和良好的泛化能力。

Abstract: Retinal fundus images provide valuable insights into the human eye's interior
structure and crucial features, such as blood vessels, optic disk, macula, and
fovea. However, accurate segmentation of retinal blood vessels can be
challenging due to imbalanced data distribution and varying vessel thickness.
In this paper, we propose BLCB-CNN, a novel pipeline based on deep learning and
bi-level class balancing scheme to achieve vessel segmentation in retinal
fundus images. The BLCB-CNN scheme uses a Convolutional Neural Network (CNN)
architecture and an empirical approach to balance the distribution of pixels
across vessel and non-vessel classes and within thin and thick vessels. Level-I
is used for vessel/non-vessel balancing and Level-II is used for thick/thin
vessel balancing. Additionally, pre-processing of the input retinal fundus
image is performed by Global Contrast Normalization (GCN), Contrast Limited
Adaptive Histogram Equalization (CLAHE), and gamma corrections to increase
intensity uniformity as well as to enhance the contrast between vessels and
background pixels. The resulting balanced dataset is used for
classification-based segmentation of the retinal vascular tree. We evaluate the
proposed scheme on standard retinal fundus images and achieve superior
performance measures, including an area under the ROC curve of 98.23%, Accuracy
of 96.22%, Sensitivity of 81.57%, and Specificity of 97.65%. We also
demonstrate the method's efficacy through external cross-validation on STARE
images, confirming its generalization ability.

</details>


### [521] [Temporal Neural Cellular Automata: Application to modeling of contrast enhancement in breast MRI](https://arxiv.org/abs/2506.18720)
*Daniel M. Lang,Richard Osuala,Veronika Spieker,Karim Lekadir,Rickmer Braren,Julia A. Schnabel*

Main category: eess.IV

TL;DR: 论文提出了一种名为TeNCA的方法，用于合成对比增强图像，解决了现有方法在时间演化一致性上的不足，并在乳腺MRI数据上取得了优于现有方法的效果。


<details>
  <summary>Details</summary>
Motivation: 乳腺MRI的长时间采集和高成本限制了其广泛应用，合成对比增强技术可以快速获取图像且无需注射对比剂，但现有方法在时间演化一致性上表现不足。

Method: 提出TeNCA（Temporal Neural Cellular Automata），扩展了神经细胞自动机（NCA）以建模稀疏、非均匀采样的时间数据，通过自适应损失计算和物理时间模拟改进训练策略。

Result: 在乳腺MRI数据集上验证了TeNCA的有效性，生成的图像与真实对比增强序列更一致，性能优于现有方法。

Conclusion: TeNCA能够学习生理上合理的对比增强演化，为合成对比增强提供了一种高效且可靠的解决方案。

Abstract: Synthetic contrast enhancement offers fast image acquisition and eliminates
the need for intravenous injection of contrast agent. This is particularly
beneficial for breast imaging, where long acquisition times and high cost are
significantly limiting the applicability of magnetic resonance imaging (MRI) as
a widespread screening modality. Recent studies have demonstrated the
feasibility of synthetic contrast generation. However, current state-of-the-art
(SOTA) methods lack sufficient measures for consistent temporal evolution.
Neural cellular automata (NCA) offer a robust and lightweight architecture to
model evolving patterns between neighboring cells or pixels. In this work we
introduce TeNCA (Temporal Neural Cellular Automata), which extends and further
refines NCAs to effectively model temporally sparse, non-uniformly sampled
imaging data. To achieve this, we advance the training strategy by enabling
adaptive loss computation and define the iterative nature of the method to
resemble a physical progression in time. This conditions the model to learn a
physiologically plausible evolution of contrast enhancement. We rigorously
train and test TeNCA on a diverse breast MRI dataset and demonstrate its
effectiveness, surpassing the performance of existing methods in generation of
images that align with ground truth post-contrast sequences.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [522] [Spiffy: Efficient Implementation of CoLaNET for Raspberry Pi](https://arxiv.org/abs/2506.18306)
*Andrey Derzhavin,Denis Larionov*

Main category: cs.NE

TL;DR: 论文提出了一种轻量级的软件方法，无需专用神经形态硬件即可运行脉冲神经网络（SNN），并在Raspberry Pi上实现了92%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决依赖专用硬件或框架运行SNN的问题，提供一种更通用的解决方案。

Method: 使用Rust语言实现特定SNN架构（CoLaNET），并优化以适配常见计算平台。

Result: 在MNIST数据集上，Spiffy实现92%准确率，训练步骤延迟0.9毫秒，推理步骤延迟0.45毫秒。

Conclusion: 该方法证明了轻量级软件实现SNN的可行性，代码已开源。

Abstract: This paper presents a lightweight software-based approach for running spiking
neural networks (SNNs) without relying on specialized neuromorphic hardware or
frameworks. Instead, we implement a specific SNN architecture (CoLaNET) in Rust
and optimize it for common computing platforms. As a case study, we demonstrate
our implementation, called Spiffy, on a Raspberry Pi using the MNIST dataset.
Spiffy achieves 92% accuracy with low latency - just 0.9 ms per training step
and 0.45 ms per inference step. The code is open-source.

</details>


<div id='cs.CE'></div>

# cs.CE [[Back]](#toc)

### [523] [A Study of Dynamic Stock Relationship Modeling and S&P500 Price Forecasting Based on Differential Graph Transformer](https://arxiv.org/abs/2506.18717)
*Linyue Hu,Qi Wang*

Main category: cs.CE

TL;DR: 论文提出了一种差分图变换器（DGT）框架，用于动态关系建模和股票价格预测，优于传统方法，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 股票价格预测对投资决策和风险管理至关重要，但由于市场的非线性动态和时变相关性，传统静态模型难以捕捉动态关系。

Method: DGT框架通过差分图机制将序列图结构变化整合到多头自注意力中，并评估了不同相关性度量作为空间注意力先验。

Result: DGT在S&P 500数据上表现优于GRU基线（RMSE: 0.24 vs. 0.87），Kendall's Tau全局矩阵效果最佳（MAE: 0.11）。聚类分析揭示了不同股票类别的预测误差差异。

Conclusion: DGT通过动态建模和跨资产交互分析，推动了金融时间序列预测的发展，并为量化策略提供了支持。

Abstract: Stock price prediction is vital for investment decisions and risk management,
yet remains challenging due to markets' nonlinear dynamics and time-varying
inter-stock correlations. Traditional static-correlation models fail to capture
evolving stock relationships. To address this, we propose a Differential Graph
Transformer (DGT) framework for dynamic relationship modeling and price
prediction. Our DGT integrates sequential graph structure changes into
multi-head self-attention via a differential graph mechanism, adaptively
preserving high-value connections while suppressing noise. Causal temporal
attention captures global/local dependencies in price sequences. We further
evaluate correlation metrics (Pearson, Mutual Information, Spearman, Kendall's
Tau) across global/local/dual scopes as spatial-attention priors. Using 10
years of S&P 500 closing prices (z-score normalized; 64-day sliding windows),
DGT with spatial priors outperformed GRU baselines (RMSE: 0.24 vs. 0.87).
Kendall's Tau global matrices yielded optimal results (MAE: 0.11). K-means
clustering revealed "high-volatility growth" and "defensive blue-chip" stocks,
with the latter showing lower errors (RMSE: 0.13) due to stable correlations.
Kendall's Tau and Mutual Information excelled in volatile sectors. This study
innovatively combines differential graph structures with Transformers,
validating dynamic relationship modeling and identifying optimal correlation
metrics/scopes. Clustering analysis supports tailored quantitative strategies.
Our framework advances financial time-series prediction through dynamic
modeling and cross-asset interaction analysis.

</details>
